{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/monitoring.html",
        "title": "Monitoring - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Monitoring[\uf0c1](#monitoring \"Permalink to this heading\")\n\n## Why Monitoring?[\uf0c1](#why-monitoring \"Permalink to this heading\")\n\nWhen developing your LLM application, it can be helpful to keep track of production data such as:\n\n*   Pipeline performance (latency/token count/throughput of various stages)\n    \n*   Resource usage (LLM/Embedding Inference Cost, CPU/GPU utilization)\n    \n*   Evaluation metrics (accuracy, precision, recall, qualitative eval and drift)\n    \n*   Pipeline versioning (which versions of sub-components e.g. LLM/embedding & artifacts e.g. prompts were used in the pipeline at a given time)\n    \n\nWe will share more on how to set up monitoring in the future."
}