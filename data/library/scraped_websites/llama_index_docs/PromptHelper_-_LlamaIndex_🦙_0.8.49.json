{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html",
        "title": "PromptHelper - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## PromptHelper[\uf0c1](#module-llama_index.indices.prompt_helper \"Permalink to this heading\")\n\nGeneral prompt helper that can help deal with LLM context window token limitations.\n\nAt its core, it calculates available context size by starting with the context window size of an LLM and reserve token space for the prompt template, and the output.\n\nIt provides utility for \u201crepacking\u201d text chunks (retrieved from index) to maximally make use of the available context window (and thereby reducing the number of LLM calls needed), or truncating them so that they fit in a single LLM call.\n\n_pydantic model_ llama\\_index.indices.prompt\\_helper.PromptHelper[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper \"Permalink to this definition\")\n\nPrompt helper.\n\nGeneral prompt helper that can help deal with LLM context window token limitations.\n\nAt its core, it calculates available context size by starting with the context window size of an LLM and reserve token space for the prompt template, and the output.\n\nIt provides utility for \u201crepacking\u201d text chunks (retrieved from index) to maximally make use of the available context window (and thereby reducing the number of LLM calls needed), or truncating them so that they fit in a single LLM call.\n\nParameters\n\n*   **context\\_window** (_int_) \u2013 Context window for the LLM.\n    \n*   **num\\_output** (_int_) \u2013 Number of outputs for the LLM.\n    \n*   **chunk\\_overlap\\_ratio** (_float_) \u2013 Chunk overlap as a ratio of chunk size\n    \n*   **chunk\\_size\\_limit** (_Optional__\\[__int__\\]_) \u2013 Maximum chunk size to use.\n    \n*   **tokenizer** (_Optional__\\[__Callable__\\[__\\[__str__\\]__,_ _List__\\]__\\]_) \u2013 Tokenizer to use.\n    \n*   **separator** (_str_) \u2013 Separator for text splitter\n    \n\nShow JSON schema\n\n{\n   \"title\": \"PromptHelper\",\n   \"description\": \"Prompt helper.\\\\n\\\\nGeneral prompt helper that can help deal with LLM context window token limitations.\\\\n\\\\nAt its core, it calculates available context size by starting with the context\\\\nwindow size of an LLM and reserve token space for the prompt template, and the\\\\noutput.\\\\n\\\\nIt provides utility for \\\\\"repacking\\\\\" text chunks (retrieved from index) to maximally\\\\nmake use of the available context window (and thereby reducing the number of LLM\\\\ncalls needed), or truncating them so that they fit in a single LLM call.\\\\n\\\\nArgs:\\\\n    context\\_window (int):                   Context window for the LLM.\\\\n    num\\_output (int):                       Number of outputs for the LLM.\\\\n    chunk\\_overlap\\_ratio (float):            Chunk overlap as a ratio of chunk size\\\\n    chunk\\_size\\_limit (Optional\\[int\\]):         Maximum chunk size to use.\\\\n    tokenizer (Optional\\[Callable\\[\\[str\\], List\\]\\]): Tokenizer to use.\\\\n    separator (str):                        Separator for text splitter\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"context\\_window\": {\n         \"title\": \"Context Window\",\n         \"description\": \"The maximum context size that will get sent to the LLM.\",\n         \"default\": 3900,\n         \"type\": \"integer\"\n      },\n      \"num\\_output\": {\n         \"title\": \"Num Output\",\n         \"description\": \"The amount of token-space to leave in input for generation.\",\n         \"default\": 256,\n         \"type\": \"integer\"\n      },\n      \"chunk\\_overlap\\_ratio\": {\n         \"title\": \"Chunk Overlap Ratio\",\n         \"description\": \"The percentage token amount that each chunk should overlap.\",\n         \"default\": 0.1,\n         \"type\": \"number\"\n      },\n      \"chunk\\_size\\_limit\": {\n         \"title\": \"Chunk Size Limit\",\n         \"description\": \"The maximum size of a chunk.\",\n         \"type\": \"integer\"\n      },\n      \"separator\": {\n         \"title\": \"Separator\",\n         \"description\": \"The separator when chunking tokens.\",\n         \"default\": \" \",\n         \"type\": \"string\"\n      }\n   }\n}\n\nFields\n\n*   [`chunk_overlap_ratio (float)`](#llama_index.indices.prompt_helper.PromptHelper.chunk_overlap_ratio \"llama_index.indices.prompt_helper.PromptHelper.chunk_overlap_ratio\")\n    \n*   [`chunk_size_limit (Optional[int])`](#llama_index.indices.prompt_helper.PromptHelper.chunk_size_limit \"llama_index.indices.prompt_helper.PromptHelper.chunk_size_limit\")\n    \n*   [`context_window (int)`](#llama_index.indices.prompt_helper.PromptHelper.context_window \"llama_index.indices.prompt_helper.PromptHelper.context_window\")\n    \n*   [`num_output (int)`](#llama_index.indices.prompt_helper.PromptHelper.num_output \"llama_index.indices.prompt_helper.PromptHelper.num_output\")\n    \n*   [`separator (str)`](#llama_index.indices.prompt_helper.PromptHelper.separator \"llama_index.indices.prompt_helper.PromptHelper.separator\")\n    \n\n_field_ chunk\\_overlap\\_ratio_: float_ _\\= 0.1_[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.chunk_overlap_ratio \"Permalink to this definition\")\n\nThe percentage token amount that each chunk should overlap.\n\n_field_ chunk\\_size\\_limit_: Optional\\[int\\]_ _\\= None_[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.chunk_size_limit \"Permalink to this definition\")\n\nThe maximum size of a chunk.\n\n_field_ context\\_window_: int_ _\\= 3900_[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.context_window \"Permalink to this definition\")\n\nThe maximum context size that will get sent to the LLM.\n\n_field_ num\\_output_: int_ _\\= 256_[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.num_output \"Permalink to this definition\")\n\nThe amount of token-space to leave in input for generation.\n\n_field_ separator_: str_ _\\= ' '_[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.separator \"Permalink to this definition\")\n\nThe separator when chunking tokens.\n\n_classmethod_ class\\_name() \u2192 str[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.class_name \"Permalink to this definition\")\n\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\n_classmethod_ construct(_\\_fields\\_set: Optional\\[SetStr\\] \\= None_, _\\*\\*values: Any_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.construct \"Permalink to this definition\")\n\nCreates a new model setting \\_\\_dict\\_\\_ and \\_\\_fields\\_set\\_\\_ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _update: Optional\\[DictStrAny\\] \\= None_, _deep: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.copy \"Permalink to this definition\")\n\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nParameters\n\n*   **include** \u2013 fields to include in new model\n    \n*   **exclude** \u2013 fields to exclude from new model, as with values this takes precedence over include\n    \n*   **update** \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n    \n*   **deep** \u2013 set to True to make a deep copy of the model\n    \n\nReturns\n\nnew model instance\n\ndict(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_) \u2192 DictStrAny[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.dict \"Permalink to this definition\")\n\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n_classmethod_ from\\_dict(_data: Dict\\[str, Any\\]_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.from_dict \"Permalink to this definition\")\n\n_classmethod_ from\\_json(_data\\_str: str_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.from_json \"Permalink to this definition\")\n\n_classmethod_ from\\_llm\\_metadata(_llm\\_metadata: [LLMMetadata](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLMMetadata \"llama_index.llms.base.LLMMetadata\")_, _chunk\\_overlap\\_ratio: float \\= 0.1_, _chunk\\_size\\_limit: Optional\\[int\\] \\= None_, _tokenizer: Optional\\[Callable\\[\\[str\\], List\\]\\] \\= None_, _separator: str \\= ' '_) \u2192 [PromptHelper](#llama_index.indices.prompt_helper.PromptHelper \"llama_index.indices.prompt_helper.PromptHelper\")[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.from_llm_metadata \"Permalink to this definition\")\n\nCreate from llm predictor.\n\nThis will autofill values like context\\_window and num\\_output.\n\n_classmethod_ from\\_orm(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.from_orm \"Permalink to this definition\")\n\nget\\_text\\_splitter\\_given\\_prompt(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _num\\_chunks: int \\= 1_, _padding: int \\= 5_) \u2192 TokenTextSplitter[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.get_text_splitter_given_prompt \"Permalink to this definition\")\n\nGet text splitter configured to maximally pack available context window, taking into account of given prompt, and desired number of chunks.\n\njson(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_, _encoder: Optional\\[Callable\\[\\[Any\\], Any\\]\\] \\= None_, _models\\_as\\_dict: bool \\= True_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.json \"Permalink to this definition\")\n\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\n_classmethod_ parse\\_file(_path: Union\\[str, Path\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.parse_file \"Permalink to this definition\")\n\n_classmethod_ parse\\_obj(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.parse_obj \"Permalink to this definition\")\n\n_classmethod_ parse\\_raw(_b: Union\\[str, bytes\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.parse_raw \"Permalink to this definition\")\n\nrepack(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _text\\_chunks: Sequence\\[str\\]_, _padding: int \\= 5_) \u2192 List\\[str\\][\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.repack \"Permalink to this definition\")\n\nRepack text chunks to fit available context window.\n\nThis will combine text chunks into consolidated chunks that more fully \u201cpack\u201d the prompt template given the context\\_window.\n\n_classmethod_ schema(_by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_) \u2192 DictStrAny[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.schema \"Permalink to this definition\")\n\n_classmethod_ schema\\_json(_\\*_, _by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.schema_json \"Permalink to this definition\")\n\nto\\_dict(_\\*\\*kwargs: Any_) \u2192 Dict\\[str, Any\\][\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.to_dict \"Permalink to this definition\")\n\nto\\_json(_\\*\\*kwargs: Any_) \u2192 str[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.to_json \"Permalink to this definition\")\n\ntruncate(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _text\\_chunks: Sequence\\[str\\]_, _padding: int \\= 5_) \u2192 List\\[str\\][\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.truncate \"Permalink to this definition\")\n\nTruncate text chunks to fit available context window.\n\n_classmethod_ update\\_forward\\_refs(_\\*\\*localns: Any_) \u2192 None[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.update_forward_refs \"Permalink to this definition\")\n\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\n\n_classmethod_ validate(_value: Any_) \u2192 Model[\uf0c1](#llama_index.indices.prompt_helper.PromptHelper.validate \"Permalink to this definition\")"
}