{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PII.html",
        "title": "PII Masking - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## PII Masking[\uf0c1](#pii-masking \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index.indices.postprocessor import (\n    PIINodePostprocessor,\n    NERPIINodePostprocessor,\n)\nfrom llama\\_index.llms import HuggingFaceLLM\nfrom llama\\_index import ServiceContext, Document, VectorStoreIndex\nfrom llama\\_index.schema import TextNode\n\nINFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNote: NumExpr detected 16 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\nNumExpr defaulting to 8 threads.\n\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n\n\\# load documents\ntext \\= \"\"\"\nHello Paulo Santos. The latest statement for your credit card account \\\\\n1111-0000-1111-0000 was mailed to 123 Any Street, Seattle, WA 98109.\n\"\"\"\nnode \\= TextNode(text\\=text)\n\n## Option 1: Use NER Model for PII Masking[\uf0c1](#option-1-use-ner-model-for-pii-masking \"Permalink to this heading\")\n\nUse a Hugging Face NER model for PII Masking\n\nservice\\_context \\= ServiceContext.from\\_defaults()\nprocessor \\= NERPIINodePostprocessor(service\\_context\\=service\\_context)\n\nfrom llama\\_index.schema import NodeWithScore\n\nnew\\_nodes \\= processor.postprocess\\_nodes(\\[NodeWithScore(node\\=node)\\])\n\nNo model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/transformers/pipelines/token\\_classification.py:169: UserWarning: \\`grouped\\_entities\\` is deprecated and will be removed in version v5.0.0, defaulted to \\`aggregation\\_strategy=\"AggregationStrategy.SIMPLE\"\\` instead.\n  warnings.warn(\n\n\\# view redacted text\nnew\\_nodes\\[0\\].node.get\\_text()\n\n'Hello \\[ORG\\_6\\]. The latest statement for your credit card account 1111-0000-1111-0000 was mailed to 123 \\[ORG\\_108\\] \\[LOC\\_112\\], \\[LOC\\_120\\], \\[LOC\\_129\\] 98109.'\n\n\\# get mapping in metadata\n\\# NOTE: this is not sent to the LLM!\nnew\\_nodes\\[0\\].node.metadata\\[\"\\_\\_pii\\_node\\_info\\_\\_\"\\]\n\n{'\\[ORG\\_6\\]': 'Paulo Santos',\n '\\[ORG\\_108\\]': 'Any',\n '\\[LOC\\_112\\]': 'Street',\n '\\[LOC\\_120\\]': 'Seattle',\n '\\[LOC\\_129\\]': 'WA'}\n\n## Option 2: Use LLM for PII Masking[\uf0c1](#option-2-use-llm-for-pii-masking \"Permalink to this heading\")\n\nNOTE: You should be using a _local_ LLM model for PII masking. The example shown is using OpenAI, but normally you\u2019d use an LLM running locally, possibly from huggingface. Examples for local LLMs are [here](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-huggingface-llm).\n\nservice\\_context \\= ServiceContext.from\\_defaults()\nprocessor \\= PIINodePostprocessor(service\\_context\\=service\\_context)\n\nfrom llama\\_index.schema import NodeWithScore\n\nnew\\_nodes \\= processor.postprocess\\_nodes(\\[NodeWithScore(node\\=node)\\])\n\n\\# view redacted text\nnew\\_nodes\\[0\\].node.get\\_text()\n\n'Hello \\[NAME\\]. The latest statement for your credit card account \\[CREDIT\\_CARD\\_NUMBER\\] was mailed to \\[ADDRESS\\].'\n\n\\# get mapping in metadata\n\\# NOTE: this is not sent to the LLM!\nnew\\_nodes\\[0\\].node.metadata\\[\"\\_\\_pii\\_node\\_info\\_\\_\"\\]\n\n{'NAME': 'Paulo Santos',\n 'CREDIT\\_CARD\\_NUMBER': '1111-0000-1111-0000',\n 'ADDRESS': '123 Any Street, Seattle, WA 98109'}\n\n## Feed Nodes to Index[\uf0c1](#feed-nodes-to-index \"Permalink to this heading\")\n\n\\# feed into index\nindex \\= VectorStoreIndex(\\[n.node for n in new\\_nodes\\])\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 30 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 30 tokens\n\nresponse \\= index.as\\_query\\_engine().query(\n    \"What address was the statement mailed to?\"\n)\nprint(str(response))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 71 tokens\n> \\[get\\_response\\] Total LLM token usage: 71 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n\\[ADDRESS\\]"
}