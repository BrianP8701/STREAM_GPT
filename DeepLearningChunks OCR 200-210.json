["CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\nthe fraction of counts of each outcome observed in the training set:\n\nm\njt LOzie Dan\nj=l tae O=x\n\nBecause maximum likelihood is a consistent estimator, this is guaranteed to happen\nso long as the model family is capable of representing the training distribution. In\npractice, limited model capacity and imperfect optimization will mean that the\nmodel is only able to approximate these fractions.\n\n(6.31)\n\nsoftmax(z(a; 0)); &\n\nMany objective functions other than the log-likelihood do not work as well\nwith the softmax function. Specifically, objective functions that do not use a log to\nundo the exp of the softmax fail to learn when the argument to the exp becomes\nvery negative, causing the gradient to vanish. In particular, squared error is a\npoor loss function for softmax units, and can fail to train the model to change its\noutput, even when the model makes highly confident incorrect predictions (Bridle,\n1990). To understand why these other loss functions can fail, we need to examine\nthe softmax function itself.\n\nLike the sigmoid, the softmax activation can saturate. The sigmoid function has\na single output that saturates when its input is extremely negative or extremely\npositive. In the case of the softmax, there are multiple output values. These\noutput values can saturate when the differences between input values become\nextreme. When the softmax saturates, many cost functions based on the softmax\nalso saturate, unless they are able to invert the saturating activating function.\n\nTo see that the softmax function responds to the difference between its inputs,\nobserve that the softmax output is invariant to adding the same scalar to all of its\ninputs:\n\nsoftmax(z) = softmax(z + c). (6.32)\n\nUsing this property, we can derive a numerically stable variant of the softmax:\nsoftmax(z) = softmax(z \u2014 max x). (6.33)\na\n\nThe reformulated version allows us to evaluate softmax with only small numerical\nerrors even when z contains extremely large or extremely negative numbers. Ex-\namining the numerically stable variant, we see that the softmax function is driven\nby the amount that its arguments deviate from max; 2;.\n\nAn output softmax(z); saturates to 1 when the corresponding input is maximal\n(zi = maxjz;) and y% is much greater than all of the other inputs. The output\nsoftmax(z); can also saturate to 0 when % is not maximal and the maximum is\nmuch greater. This is a generalization of the way that sigmoid units saturate, and\n\n186\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\ncan cause similar difficulties for learning if the loss function is not designed to\ncompensate for it.\n\nThe argument z to the softmax function can be produced in two different ways.\nThe most common is simply to have an earlier layer of the neural network output\nevery element of z, as described above using the linear layer z = W 'h+b. While\nstraightforward, this approach actually overparametrizes the distribution. The\nconstraint that the n outputs must sum to 1 means that only n \u20141 parameters are\nnecessary; the probability of the n-th value may be obtained by subtracting the\nfirst n\u2014 1 probabilities from 1. We can thus impose a requirement that one element\nof z be fixed. For example, we can require that z, = 0. Indeed, this is exactly\nwhat the sigmoid unit does. Defining P(y=1 | \u00ab) = o(z) is equivalent to defining\nP(y=1| x) = softmax(z): with a two-dimensional z and z = 0. Both the n\u20141\nargument and the n argument approaches to the softmax can describe the same\nset of probability distributions, but have different learning dynamics. In practice,\nthere is rarely much difference between using the overparametrized version or the\nrestricted version, and it is simpler to implement the overparametrized version.\n\nFrom a neuroscientific point of view, it is interesting to think of the softmax as\na way to create a form of competition between the units that participate in it: the\nsoftmax outputs always sum to | so an increase in the value of one unit necessarily\ncorresponds to a decrease in the value of others. This is analogous to the lateral\ninhibition that is believed to exist between nearby neurons in the cortex. At the\nextreme (when the difference between the maximal a; and the others is large in\nmagnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1\nand the others are nearly 0).\n\nThe name \u201csoftmax\u201d can be somewhat confusing. The function is more closely\nrelated to the arg max function than the max function. The term \u201csoft\u201d derives\nfrom the fact that the softmax function is continuous and differentiable. The\narg max function, with its result represented as a one-hot vector, is not continuous\nor differentiable. The softmax function thus provides a \u201csoftened\u201d version of the\narg max. The corresponding soft version of the maximum function is softmax(z) ' z.\nIt would perhaps be better to call the softmax function \u201csoftargmax,\u201d but the\ncurrent name is an entrenched convention.\n\n6.2.2.4 Other Output Types\n\nThe linear, sigmoid, and softmax output units described above are the most\ncommon. Neural networks can generalize to almost any kind of output layer that\nwe wish. The principle of maximum likelihood provides a guide for how to design\n\n187\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\na good cost function for nearly any kind of output layer.\n\nIn general, if we define a conditional distribution p(y | x; 0), the principle of\nmaximum likelihood suggests we use \u2014 log p(y | #;@) as our cost function.\n\nIn general, we can think of the neural network as representing a function f(a;6).\nThe outputs of this function are not direct predictions of the value y. Instead,\nf (@;0) = w provides the parameters for a distribution over y. Our loss function\ncan then be interpreted as \u2014 log p(y; w(a)).\n\nFor example, we may wish to learn the variance of a conditional Gaussian for y,\ngiven x. In the simple case, where the variance o? is a constant, there is a closed\nform expression because the maximum likelihood estimator of variance is simply the\nempirical mean of the squared difference between observations y and their expected\nvalue. A computationally more expensive approach that does not require writing\nspecial-case code is to simply include the variance as one of the properties of the\ndistribution p(y | z) that is controlled by w = f(a; 0). The negative log-likelihood\n\u2014 log p(y;w(ax)) will then provide a cost function with the appropriate terms\nnecessary to make our optimization procedure incrementally learn the variance. In\nthe simple case where the standard deviation does not depend on the input, we\ncan make a new parameter in the network that is copied directly into w. This new\nparameter might be o itself or could be a parameter y representing o? or it could\nbe a parameter 6 representing >, depending on how we choose to parametrize\nthe distribution. We may wish our model to predict a different amount of variance\nin y for different values of x. This is called a heteroscedastic model. In the\nheteroscedastic case, we simply make the specification of the variance be one of\nthe values output by f(x;@). A typical way to do this is to formulate the Gaussian\ndistribution using precision, rather than variance, as described in equation 3.22.\nIn the multivariate case it is most common to use a diagonal precision matrix\n\ndiag(@). (6.34)\n\nThis formulation works well with gradient descent because the formula for the\nlog-likelihood of the Gaussian distribution parametrized by @ involves only mul-\ntiplication by 6; and addition of log G@;. The gradient of multiplication, addition,\nand logarithm operations is well-behaved. By comparison, if we parametrized the\noutput in terms of variance, we would need to use division. The division function\nbecomes arbitrarily steep near zero. While large gradients can help learning,\narbitrarily large gradients usually result in instability. If we parametrized the\noutput in terms of standard deviation, the log-likelihood would still involve division,\nand would also involve squaring. The gradient through the squaring operation\ncan vanish near zero, making it difficult to learn parameters that are squared.\n\n188\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\nRegardless of whether we use standard deviation, variance, or precision, we must\nensure that the covariance matrix of the Gaussian is positive definite. Because\nthe eigenvalues of the precision matrix are the reciprocals of the eigenvalues of\nthe covariance matrix, this is equivalent to ensuring that the precision matrix is\npositive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix,\nthen the only condition we need to enforce on the output of the model is positivity.\nIf we suppose that a is the raw activation of the model used to determine the\ndiagonal precision, we can use the softplus function to obtain a positive precision\nvector: 3 = \u00a2(a). This same strategy applies equally if using variance or standard\ndeviation rather than precision or if using a scalar times identity rather than\ndiagonal matrix.\n\nIt is rare to learn a covariance or precision matrix with richer structure than\ndiagonal. If the covariance is full and conditional, then a parametrization must\nbe chosen that guarantees positive-definiteness of the predicted covariance matrix.\nThis can be achieved by writing =(a#) = B(a)B' (a), where B is an unconstrained\nsquare matrix. One practical issue if the matrix is full rank is that computing the\nlikelihood is expensive, with a d x d matrix requiring O(d\u00b0 ) computation for the\ndeterminant and inverse of \u00a9(a) (or equivalently, and more commonly done, its\neigendecomposition or that of B(a)).\n\nWe often want to perform multimodal regression, that is, to predict real values\nthat come from a conditional distribution p(y | x) that can have several different\npeaks in y space for the same value of w. In this case, a Gaussian mixture is\na natural representation for the output (Jacobs et al., 1991; Bishop, 1994).\nNeural networks with Gaussian mixtures as their output are often called mixture\ndensity networks. A Gaussian mixture output with n components is defined by\nthe conditional probability distribution\n\nn\np(y | @) = $7 ple =i] @)N (ys w(x), 20 (a). (6.35)\ni=1\nThe neural network must have three outputs: a vector defining p(c =i |x), a\nmatrix providing yw\u201c (a) for all i, and a tensor providing \u00a9\u201c(\u00ab) for all i. These\noutputs must satisfy different constraints:\n\n1. Mixture components p(c = i | x): these form a multinoulli distribution\nover the n different components associated with latent variable! c, and can\n\nWe consider c to be latent because we do not observe it in the data: given input x and target\ny, it is not possible to know with certainty which Gaussian component was responsible for y, but\nwe can imagine that y was generated by picking one of them, and make that unobserved choice a\nrandom variable.\n\n189\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\ntypically be obtained by a softmax over an n-dimensional vector, to guarantee\nthat these outputs are positive and sum to 1.\n\n2. Means p)(a): these indicate the center or mean associated with the i-th\nGaussian component, and are unconstrained (typically with no nonlinearity\nat all for these output units). If y is a dvector, then the network must output\nann x d matrix containing all n of these d-dimensional vectors. Learning\nthese means with maximum likelihood is slightly more complicated than\nlearning the means of a distribution with only one output mode. We only\nwant to update the mean for the component that actually produced the\nobservation. In practice, we do not know which component produced each\nobservation. The expression for the negative log-likelihood naturally weights\neach example\u2019s contribution to the loss for each component by the probability\nthat the component produced the example.\n\n3. Covariances SO(ax): these specify the covariance matrix for each component\ni. As when learning a single Gaussian component, we typically use a diagonal\nmatrix to avoid needing to compute determinants. As with learning the means\nof the mixture, maximum likelihood is complicated by needing to assign\npartial responsibility for each point to each mixture component. Gradient\ndescent will automatically follow the correct process if given the correct\nspecification of the negative log-likelihood under the mixture model.\n\nIt has been reported that gradient-based optimization of conditional Gaussian\nmixtures (on the output of neural networks) can be unreliable, in part because one\ngets divisions (by the variance) which can be numerically unstable (when some\nvariance gets to be small for a particular example, yielding very large gradients).\nOne solution is to clip gradients (see section 10.11.1) while another is to scale\nthe gradients heuristically (Murray and Larochelle, 2014).\n\nGaussian mixture outputs are particularly effective in generative models of\nspeech (Schuster, 1999) or movements of physical objects (Graves, 2013). The\nmixture density strategy gives a way for the network to represent multiple output\nmodes and to control the variance of its output, which is crucial for obtaining\na high degree of quality in these real-valued domains. An example of a mixture\ndensity network is shown in figure 6.4.\n\nIn general, we may wish to continue to model larger vectors y containing more\nvariables, and to impose richer and richer structures on these output variables. For\nexample, we may wish for our neural network to output a sequence of characters\nthat forms a sentence. In these cases, we may continue to use the principle\nof maximum likelihood applied to our model p(y; w(x)), but the model we use\n\n190\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\nFigure 6.4: Samples drawn from a neural network with a mixture density output layer.\nThe input x is sampled from a uniform distribution and the output y is sampled from\nPmodel(y | z). The neural network is able to learn nonlinear mappings from the input to\nthe parameters of the output distribution. These parameters include the probabilities\ngoverning which of three mixture components will generate the output as well as the\nparameters for each mixture component. Each mixture component is Gaussian with\npredicted mean and variance. All of these aspects of the output distribution are able to\nvary with respect to the input x, and to do so in nonlinear ways.\n\nto describe y becomes complex enough to be beyond the scope of this chapter.\nChapter 10 describes how to use recurrent neural networks to define such models\nover sequences, and part III describes advanced techniques for modeling arbitrary\nprobability distributions.\n\n6.3 Hidden Units\n\nSo far we have focused our discussion on design choices for neural networks that\nare common to most parametric machine learning models trained with gradient-\nbased optimization. Now we turn to an issue that is unique to feedforward neural\nnetworks: how to choose the type of hidden unit to use in the hidden layers of the\nmodel.\n\nThe design of hidden units is an extremely active area of research and does not\nyet have many definitive guiding theoretical principles.\n\nRectified linear units are an excellent default choice of hidden unit. Many other\ntypes of hidden units are available. It can be difficult to determine when to use\nwhich kind (though rectified linear units are usually an acceptable choice). We\n\n191\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\ndescribe here some of the basic intuitions motivating each type of hidden units.\nThese intuitions can help decide when to try out each of these units. It is usually\nimpossible to predict in advance which will work best. The design process consists\nof trial and error, intuiting that a kind of hidden unit may work well, and then\ntraining a network with that kind of hidden unit and evaluating its performance\non a validation set.\n\nSome of the hidden units included in this list are not actually differentiable at\nall input points. For example, the rectified linear function g(z) = max{0, z} is not\ndifferentiable at z = 0. This may seem like it invalidates g for use with a gradient-\nbased learning algorithm. In practice, gradient descent still performs well enough\nfor these models to be used for machine learning tasks. This is in part because\nneural network training algorithms do not usually arrive at a local minimum of\nthe cost function, but instead merely reduce its value significantly, as shown in\nfigure 4.3. These ideas will be described further in chapter 8. Because we do not\nexpect training to actually reach a point where the gradient is 0, it is acceptable\nfor the minima of the cost function to correspond to points with undefined gradient.\nHidden units that are not differentiable are usually non-differentiable at only a\nsmall number of points. In general, a function g(z) has a left derivative defined\nby the slope of the function immediately to the left of z and a right derivative\ndefined by the slope of the function immediately to the right of z. A function\nis differentiable at z only if both the left derivative and the right derivative are\ndefined and equal to each other. The functions used in the context of neural\nnetworks usually have defined left derivatives and defined right derivatives. In the\ncase of g(z) = max{0, z}, the left derivative at z = 0 is 0 and the right derivative\nis 1. Software implementations of neural network training usually return one of\nthe one-sided derivatives rather than reporting that the derivative is undefined or\nraising an error. This may be heuristically justified by observing that gradient-\nbased optimization on a digital computer is subject to numerical error anyway.\nWhen a function is asked to evaluate g(0), it is very unlikely that the underlying\nvalue truly was 0. Instead, it was likely to be some small value \u00a2\u20ac that was rounded\nto 0. In some contexts, more theoretically pleasing justifications are available, but\nthese usually do not apply to neural network training. The important point is tha\nin practice one can safely disregard the non-differentiability of the hidden uni\nactivation functions described below.\n\nUnless indicated otherwise, most hidden units can be described as accepting\na vector of inputs 2, computing an affine transformation z = W'a+ b, and\nthen applying an element-wise nonlinear function g(z). Most hidden units are\ndistinguished from each other only by the choice of the form of the activation\nfunction g(z).\n\n192\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\n6.3.1 Rectified Linear Units and Their Generalizations\n\nRectified linear units use the activation function g(z) = max{0, z}.\n\nRectified linear units are easy to optimize because they are so similar to linear\nunits. The only difference between a linear unit and a rectified linear unit is\nthat a rectified linear unit outputs zero across half its domain. This makes the\nderivatives through a rectified linear unit remain large whenever the unit is active.\nThe gradients are not only large but also consistent. The second derivative of the\nrectifying operation is 0 almost everywhere, and the derivative of the rectifying\noperation is 1 everywhere that the unit is active. This means that the gradient\ndirection is far more useful for learning than it would be with activation functions\nthat introduce second-order effects.\n\nRectified linear units are typically used on top of an affine transformation:\nh=g(W'ax +b). (6.36)\n\nWhen initializing the parameters of the affine transformation, it can be a good\npractice to set all elements of b to a small, positive value, such as 0.1. This makes\nit very likely that the rectified linear units will be initially active for most inputs\nin the training set and allow the derivatives to pass through.\n\nSeveral generalizations of rectified linear units exist. Most of these general-\nizations perform comparably to rectified linear units and occasionally perform\nbetter.\n\nOne drawback to rectified linear units is that they cannot learn via gradient-\nbased methods on examples for which their activation is zero. A variety of\ngeneralizations of rectified linear units guarantee that they receive gradient every-\nwhere.\n\nThree generalizations of rectified linear units are based on using a non-zero\nslope a; when % <0: hi = g(z, a); = max(0, %) + aj min(0, 2). Absolute value\nrectification fixes a; = \u20141 to obtain g(z) = |z|. It is used for object recognition\nfrom images (Jarrett et al., 2009), where it makes sense to seek features that are\ninvariant under a polarity reversal of the input illumination. Other generalizations\nof rectified linear units are more broadly applicable. A leaky ReLU (Maas ec? ai.,\n2013) fixes a; to a small value like 0.01 while a parametric ReLU or PReLU\ntreats a; as a learnable parameter (He ei al., 2015).\n\nMaxout units (Goodfellow ef al., 2013a) generalize rectified linear units\nfurther. Instead of applying an element-wise function g(z), maxout units divide z\ninto groups of k values. Each maxout unit then outputs the maximum element of\n\n193\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\none of these groups:\ng(2)i = max % (6.37)\njeG@\nwhere G\u201d is the set of indices into the inputs for group i, { (i \u2014 1)k+1,..., ik}.\nThis provides a way of learning a piecewise linear function that responds to multiple\ndirections in the input a space.\n\nA maxout unit can learn a piecewise linear, convex function with up to k pieces.\nMaxout units can thus be seen as learning the activation function itself rather\nthan just the relationship between units. With large enough k, a maxout unit can\nlearn to approximate any convex function with arbitrary fidelity. In particular,\na maxout layer with two pieces can learn to implement the same function of the\ninput x as a traditional layer using the rectified linear activation function, absolute\nvalue rectification function, or the leaky or parametric ReLU, or can learn to\nimplement a totally different function altogether. The maxout layer will of course\nbe parametrized differently from any of these other layer types, so the learning\ndynamics will be different even in the cases where maxout learns to implement the\nsame function of # as one of the other layer types.\n\nEach maxout unit is now parametrized by k weight vectors instead of just one,\nso maxout units typically need more regularization than rectified linear units. They\ncan work well without regularization if the training set is large and the number of\npieces per unit is kept low (Cai et al., 2013).\n\nMaxout units have a few other benefits. In some cases, one can gain some sta-\ntistical and computational advantages by requiring fewer parameters. Specifically,\nif the features captured by n different linear filters can be summarized without\nlosing information by taking the max over each group of k features, then the next\nlayer can get by with k times fewer weights.\n\nBecause each unit is driven by multiple filters, maxout units have some redun-\ndancy that helps them to resist a phenomenon called catastrophic forgetting\nin which neural networks forget how to perform tasks that they were trained on in\nthe past (Goodfellow et al., 2014a).\n\nRectified linear units and all of these generalizations of them are based on the\nprinciple that models are easier to optimize if their behavior is closer to linear.\nThis same general principle of using linear behavior to obtain easier optimization\nalso applies in other contexts besides deep linear networks. Recurrent networks can\nlearn from sequences and produce a sequence of states and outputs. When training\nthem, one needs to propagate information through several time steps, which is much\neasier when some linear computations (with some directional derivatives being of\nmagnitude near 1) are involved. One of the best-performing recurrent network\n\n194\n", "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n\narchitectures, the LSTM, propagates information through time via summation\u2014a\nparticular straightforward kind of such linear activation. This is discussed further\nin section 10.10.\n\n6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n\nPrior to the introduction of rectified linear units, most neural networks used the\nlogistic sigmoid activation function\n\n(2) = o(2) (6.38)\nor the hyperbolic tangent activation function\ng(z) = tanh(z). (6.39)\n\nThese activation functions are closely related because tanh(z) = 20(2z) \u2014 1.\n\nWe have already seen sigmoid units as output units, used to predict the\nprobability that a binary variable is 1. Unlike piecewise linear units, sigmoidal\nunits saturate across most of their domain\u2014they saturate to a high value when\nz is very positive, saturate to a low value when z is very negative, and are only\nstrongly sensitive to their input when z is near 0. The widespread saturation of\nsigmoidal units can make gradient-based learning very difficult. For this reason,\ntheir use as hidden units in feedforward networks is now discouraged. Their use\nas output units is compatible with the use of gradient-based learning when an\nappropriate cost function can undo the saturation of the sigmoid in the output\nlayer.\n\nWhen a sigmoidal activation function must be used, the hyperbolic tangent\nactivation function typically performs better than the logistic sigmoid. It resembles\nthe identity function more closely, in the sense that tanh(0) = 0 while o(0) = 3.\nBecause tanh is similar to the identity function near 0, training a deep neural\nnetwork \u00a2 = w! tanh(U' tanh(V'\u00ab)) resembles training a linear model g =\nw'U'V'z so long as the activations of the network can be kept small. This\n\nmakes training the tanh network easier.\n\nSigmoidal activation functions are more common in settings other than feed-\nforward networks. Recurrent networks, many probabilistic models, and some\nautoencoders have additional requirements that rule out the use of piecewise\nlinear activation functions and make sigmoidal units more appealing despite the\ndrawbacks of saturation.\n"]