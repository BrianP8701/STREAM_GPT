{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html",
        "title": "Custom Embeddings - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Custom Embeddings[\uf0c1](#custom-embeddings \"Permalink to this heading\")\n\nLlamaIndex supports embeddings from OpenAI, Azure, and Langchain. But if this isn\u2019t enough, you can also implement any embeddings model!\n\nThe example below uses Instructor Embeddings ([install/setup details here](https://huggingface.co/hkunlp/instructor-large)), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \u201cinstructions\u201d on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.\n\n\\# Install dependencies\n\\# !pip install InstructorEmbedding torch transformers sentence-transformers\n\nimport openai\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"YOUR\\_API\\_KEY\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n## Custom Embeddings Implementation[\uf0c1](#custom-embeddings-implementation \"Permalink to this heading\")\n\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\n\nfrom llama\\_index.bridge.pydantic import PrivateAttr\nfrom llama\\_index.embeddings.base import BaseEmbedding\n\nclass InstructorEmbeddings(BaseEmbedding):\n    \\_model: INSTRUCTOR \\= PrivateAttr()\n    \\_instruction: str \\= PrivateAttr()\n\n    def \\_\\_init\\_\\_(\n        self,\n        instructor\\_model\\_name: str \\= \"hkunlp/instructor-large\",\n        instruction: str \\= \"Represent a document for semantic search:\",\n        \\*\\*kwargs: Any,\n    ) \\-> None:\n        self.\\_model \\= INSTRUCTOR(instructor\\_model\\_name)\n        self.\\_instruction \\= instruction\n        super().\\_\\_init\\_\\_(\\*\\*kwargs)\n\n    @classmethod\n    def class\\_name(cls) \\-> str:\n        return \"instructor\"\n\n    async def \\_aget\\_query\\_embedding(self, query: str) \\-> List\\[float\\]:\n        return self.\\_get\\_query\\_embedding(query)\n\n    async def \\_aget\\_text\\_embedding(self, text: str) \\-> List\\[float\\]:\n        return self.\\_get\\_text\\_embedding(text)\n\n    def \\_get\\_query\\_embedding(self, query: str) \\-> List\\[float\\]:\n        embeddings \\= self.\\_model.encode(\\[\\[self.\\_instruction, query\\]\\])\n        return embeddings\\[0\\]\n\n    def \\_get\\_text\\_embedding(self, text: str) \\-> List\\[float\\]:\n        embeddings \\= self.\\_model.encode(\\[\\[self.\\_instruction, text\\]\\])\n        return embeddings\\[0\\]\n\n    def \\_get\\_text\\_embeddings(self, texts: List\\[str\\]) \\-> List\\[List\\[float\\]\\]:\n        embeddings \\= self.\\_model.encode(\n            \\[\\[self.\\_instruction, text\\] for text in texts\\]\n        )\n        return embeddings\n\n## Usage Example[\uf0c1](#usage-example \"Permalink to this heading\")\n\nfrom llama\\_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    embed\\_model\\=InstructorEmbeddings(embed\\_batch\\_size\\=2), chunk\\_size\\=512\n)\n\n\\# if running for the first time, will download model weights first!\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nload INSTRUCTOR\\_Transformer\nmax\\_seq\\_length  512\n\nresponse \\= index.as\\_query\\_engine().query(\"What did the author do growing up?\")\nprint(response)\n\nThe author wrote short stories and also worked on programming, specifically on an IBM 1401 computer in 9th grade. They used an early version of Fortran and had to type programs on punch cards. Later on, they got a microcomputer, a TRS-80, and started programming more extensively, writing simple games and a word processor. They initially planned to study philosophy in college but eventually switched to AI."
}