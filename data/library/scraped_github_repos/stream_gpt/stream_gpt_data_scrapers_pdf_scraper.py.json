{
    "metadata": {
        "type": "repo",
        "path": "stream_gpt/data_scrapers/pdf_scraper.py"
    },
    "text": "'''\nThis class contains functions that scrape text from PDFs.\n\nI want this to be an automatic part of the pipeline, just pass in\na path to a pdf, and get back strings.\n\nAfter using each a little bit, I found, annoyingly, that each of these\nsometimes might not work well on some pdfs for no seemingly good reason.\nIssues, like mashing together words, interpreting pictures or math equations\nas a bunch of weird characters, adding spaces between each letter, etc.\n\nTo get around this, I just use all three of them on a pdf, and then\nask ChatGPT to look at samples from each of them and pick the one that\nlooks the best. Of course, in the rare case where all three of them\nbreak down, your going to have some ugly data.\n'''\nimport pdfplumber\nfrom pypdf import PdfReader\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom stream_gpt.utils import inference\nimport random\nimport json\nfrom pdfminer.layout import LTTextBoxHorizontal\nfrom pdfminer.high_level import extract_pages\nfrom stream_gpt.types.document import Document\nfrom stream_gpt.interfaces.scraper_interface import Scraper\n\nclass PDFScraper(Scraper):\n    def __init__(self):\n        self.scrapers = [\n            self.pdfplumber_scraper,\n            self.pypdf2_scraper,\n            self.pytesseract_scraper,\n            self.pdfminer_scraper\n        ]\n\n    def scrape(self, path, metadata={'title':''}) -> Document:\n        '''\n        Extract text from a PDF using all available scrapers and have ChatGPT pick the best extraction.\n        '''\n        all_text = []  # Store results from each scraper\n        for scraper in self.scrapers:\n            try:\n                extracted_text = scraper(path)\n                all_text.append(extracted_text)\n                \n                # Save the extracted text for review\n                scraper_name = scraper.__name__\n                with open(f'data/raw_library/{len(all_text)}{scraper_name}.json', 'w') as f:\n                    json.dump(extracted_text, f)\n            except Exception as e:\n                raise Exception(f'{scraper} failed: {e}')\n\n        # Collect random samples from each scraper result for comparison\n        samples = []\n        rand_chunk_indices = [random.randint(0, len(all_text[0])-1) for _ in range(3)]\n        for scraper_index, scraped_chunks in enumerate(all_text, 1):\n            sample = ' '.join([scraped_chunks[i][:100] for i in rand_chunk_indices])\n            sample = f'<Sample {scraper_index}>: {sample}\\n'\n            samples.append(sample)\n\n        # Ask ChatGPT to determine the best sample\n        best_sample_index = inference.choose_best_scraped_text(samples)\n        print(best_sample_index)\n        return self.convert_scraped_data_to_documents(all_text[best_sample_index], metadata)\n\n    def pdfminer_scraper(self, path):\n        extracted_text = []\n        \n        for page_layout in extract_pages(path):\n            page_text = []  # Temporary list to store text from each element of the current page\n            for element in page_layout:\n                if isinstance(element, LTTextBoxHorizontal):\n                    page_text.append(element.get_text())\n                    \n            # Concatenate all the text elements of the current page and append to the main list\n            extracted_text.append(' '.join(page_text))\n                    \n        return extracted_text\n\n    def pdfplumber_scraper(self, path):\n        all_text = []\n        with pdfplumber.open(path) as pdf:\n            # Loop through each page\n            for page_num in range(len(pdf.pages)):\n                # Get the specific page\n                page = pdf.pages[page_num]\n                \n                # Extract text from the page\n                text = page.extract_text()      \n                all_text.append(text)\n\n        return all_text\n\n    def pypdf2_scraper(self, path):\n        pdf_reader = PdfReader(open(path, 'rb'))\n        num_pages = len(pdf_reader.pages)\n        \n        text_list = []  \n        \n        for page_num in range(num_pages):\n            page = pdf_reader.pages[page_num]\n            text_content = page.extract_text()\n            text_list.append(text_content)\n        return text_list\n\n    def pytesseract_scraper(self, path):\n        imgs = convert_from_path(path)\n        chunks = []\n        for img in imgs:\n            chunks.append(pytesseract.image_to_string(img))\n        return chunks\n    \n    @staticmethod\n    def convert_scraped_data_to_documents(text, metadata):\n        metadata['type'] = 'pdf'\n        return Document(metadata=metadata, text=text)"
}