{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/llms.html",
        "title": "LLMs - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## LLMs[\uf0c1](#llms \"Permalink to this heading\")\n\nA large language model (LLM) is a reasoning engine that can complete text, chat with users, and follow instructions.\n\n## LLM Implementations[\uf0c1](#llm-implementations \"Permalink to this heading\")\n\n## LLM Interface[\uf0c1](#llm-interface \"Permalink to this heading\")\n\n_class_ llama\\_index.llms.base.LLM(_\\*_, _callback\\_manager: [CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\") \\= None_)[\uf0c1](#llama_index.llms.base.LLM \"Permalink to this definition\")\n\nLLM interface.\n\n_abstract async_ achat(_messages: Sequence\\[[ChatMessage](#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 [ChatResponse](#llama_index.llms.base.ChatResponse \"llama_index.llms.base.ChatResponse\")[\uf0c1](#llama_index.llms.base.LLM.achat \"Permalink to this definition\")\n\nAsync chat endpoint for LLM.\n\n_abstract async_ acomplete(_prompt: str_, _\\*\\*kwargs: Any_) \u2192 [CompletionResponse](#llama_index.llms.base.CompletionResponse \"llama_index.llms.base.CompletionResponse\")[\uf0c1](#llama_index.llms.base.LLM.acomplete \"Permalink to this definition\")\n\nAsync completion endpoint for LLM.\n\n_abstract async_ astream\\_chat(_messages: Sequence\\[[ChatMessage](#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 AsyncGenerator\\[[ChatResponse](#llama_index.llms.base.ChatResponse \"llama_index.llms.base.ChatResponse\"), None\\][\uf0c1](#llama_index.llms.base.LLM.astream_chat \"Permalink to this definition\")\n\nAsync streaming chat endpoint for LLM.\n\n_abstract async_ astream\\_complete(_prompt: str_, _\\*\\*kwargs: Any_) \u2192 AsyncGenerator\\[[CompletionResponse](#llama_index.llms.base.CompletionResponse \"llama_index.llms.base.CompletionResponse\"), None\\][\uf0c1](#llama_index.llms.base.LLM.astream_complete \"Permalink to this definition\")\n\nAsync streaming completion endpoint for LLM.\n\n_abstract_ chat(_messages: Sequence\\[[ChatMessage](#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 [ChatResponse](#llama_index.llms.base.ChatResponse \"llama_index.llms.base.ChatResponse\")[\uf0c1](#llama_index.llms.base.LLM.chat \"Permalink to this definition\")\n\nChat endpoint for LLM.\n\n_abstract classmethod_ class\\_name() \u2192 str[\uf0c1](#llama_index.llms.base.LLM.class_name \"Permalink to this definition\")\n\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\n_abstract_ complete(_prompt: str_, _\\*\\*kwargs: Any_) \u2192 [CompletionResponse](#llama_index.llms.base.CompletionResponse \"llama_index.llms.base.CompletionResponse\")[\uf0c1](#llama_index.llms.base.LLM.complete \"Permalink to this definition\")\n\nCompletion endpoint for LLM.\n\n_classmethod_ construct(_\\_fields\\_set: Optional\\[SetStr\\] \\= None_, _\\*\\*values: Any_) \u2192 Model[\uf0c1](#llama_index.llms.base.LLM.construct \"Permalink to this definition\")\n\nCreates a new model setting \\_\\_dict\\_\\_ and \\_\\_fields\\_set\\_\\_ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _update: Optional\\[DictStrAny\\] \\= None_, _deep: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llms.base.LLM.copy \"Permalink to this definition\")\n\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nParameters\n\n*   **include** \u2013 fields to include in new model\n    \n*   **exclude** \u2013 fields to exclude from new model, as with values this takes precedence over include\n    \n*   **update** \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n    \n*   **deep** \u2013 set to True to make a deep copy of the model\n    \n\nReturns\n\nnew model instance\n\ndict(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_) \u2192 DictStrAny[\uf0c1](#llama_index.llms.base.LLM.dict \"Permalink to this definition\")\n\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\njson(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_, _encoder: Optional\\[Callable\\[\\[Any\\], Any\\]\\] \\= None_, _models\\_as\\_dict: bool \\= True_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.llms.base.LLM.json \"Permalink to this definition\")\n\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\n_abstract property_ metadata_: [LLMMetadata](#llama_index.llms.base.LLMMetadata \"llama_index.llms.base.LLMMetadata\")_[\uf0c1](#llama_index.llms.base.LLM.metadata \"Permalink to this definition\")\n\nLLM metadata.\n\n_abstract_ stream\\_chat(_messages: Sequence\\[[ChatMessage](#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 Generator\\[[ChatResponse](#llama_index.llms.base.ChatResponse \"llama_index.llms.base.ChatResponse\"), None, None\\][\uf0c1](#llama_index.llms.base.LLM.stream_chat \"Permalink to this definition\")\n\nStreaming chat endpoint for LLM.\n\n_abstract_ stream\\_complete(_prompt: str_, _\\*\\*kwargs: Any_) \u2192 Generator\\[[CompletionResponse](#llama_index.llms.base.CompletionResponse \"llama_index.llms.base.CompletionResponse\"), None, None\\][\uf0c1](#llama_index.llms.base.LLM.stream_complete \"Permalink to this definition\")\n\nStreaming completion endpoint for LLM.\n\n_classmethod_ update\\_forward\\_refs(_\\*\\*localns: Any_) \u2192 None[\uf0c1](#llama_index.llms.base.LLM.update_forward_refs \"Permalink to this definition\")\n\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\n\n## Schemas[\uf0c1](#schemas \"Permalink to this heading\")\n\n_class_ llama\\_index.llms.base.MessageRole(_value_, _names\\=None_, _\\*_, _module\\=None_, _qualname\\=None_, _type\\=None_, _start\\=1_, _boundary\\=None_)[\uf0c1](#llama_index.llms.base.MessageRole \"Permalink to this definition\")\n\nMessage role.\n\ncapitalize()[\uf0c1](#llama_index.llms.base.MessageRole.capitalize \"Permalink to this definition\")\n\nReturn a capitalized version of the string.\n\nMore specifically, make the first character have upper case and the rest lower case.\n\ncasefold()[\uf0c1](#llama_index.llms.base.MessageRole.casefold \"Permalink to this definition\")\n\nReturn a version of the string suitable for caseless comparisons.\n\ncenter(_width_, _fillchar\\=' '_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.center \"Permalink to this definition\")\n\nReturn a centered string of length width.\n\nPadding is done using the specified fill character (default is a space).\n\ncount(_sub_\\[, _start_\\[, _end_\\]\\]) \u2192 int[\uf0c1](#llama_index.llms.base.MessageRole.count \"Permalink to this definition\")\n\nReturn the number of non-overlapping occurrences of substring sub in string S\\[start:end\\]. Optional arguments start and end are interpreted as in slice notation.\n\nencode(_encoding\\='utf-8'_, _errors\\='strict'_)[\uf0c1](#llama_index.llms.base.MessageRole.encode \"Permalink to this definition\")\n\nEncode the string using the codec registered for encoding.\n\nencoding\n\nThe encoding in which to encode the string.\n\nerrors\n\nThe error handling scheme to use for encoding errors. The default is \u2018strict\u2019 meaning that encoding errors raise a UnicodeEncodeError. Other possible values are \u2018ignore\u2019, \u2018replace\u2019 and \u2018xmlcharrefreplace\u2019 as well as any other name registered with codecs.register\\_error that can handle UnicodeEncodeErrors.\n\nendswith(_suffix_\\[, _start_\\[, _end_\\]\\]) \u2192 bool[\uf0c1](#llama_index.llms.base.MessageRole.endswith \"Permalink to this definition\")\n\nReturn True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try.\n\nexpandtabs(_tabsize\\=8_)[\uf0c1](#llama_index.llms.base.MessageRole.expandtabs \"Permalink to this definition\")\n\nReturn a copy where all tab characters are expanded using spaces.\n\nIf tabsize is not given, a tab size of 8 characters is assumed.\n\nfind(_sub_\\[, _start_\\[, _end_\\]\\]) \u2192 int[\uf0c1](#llama_index.llms.base.MessageRole.find \"Permalink to this definition\")\n\nReturn the lowest index in S where substring sub is found, such that sub is contained within S\\[start:end\\]. Optional arguments start and end are interpreted as in slice notation.\n\nReturn -1 on failure.\n\nformat(_\\*args_, _\\*\\*kwargs_) \u2192 str[\uf0c1](#llama_index.llms.base.MessageRole.format \"Permalink to this definition\")\n\nReturn a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces (\u2018{\u2018 and \u2018}\u2019).\n\nformat\\_map(_mapping_) \u2192 str[\uf0c1](#llama_index.llms.base.MessageRole.format_map \"Permalink to this definition\")\n\nReturn a formatted version of S, using substitutions from mapping. The substitutions are identified by braces (\u2018{\u2018 and \u2018}\u2019).\n\nindex(_sub_\\[, _start_\\[, _end_\\]\\]) \u2192 int[\uf0c1](#llama_index.llms.base.MessageRole.index \"Permalink to this definition\")\n\nReturn the lowest index in S where substring sub is found, such that sub is contained within S\\[start:end\\]. Optional arguments start and end are interpreted as in slice notation.\n\nRaises ValueError when the substring is not found.\n\nisalnum()[\uf0c1](#llama_index.llms.base.MessageRole.isalnum \"Permalink to this definition\")\n\nReturn True if the string is an alpha-numeric string, False otherwise.\n\nA string is alpha-numeric if all characters in the string are alpha-numeric and there is at least one character in the string.\n\nisalpha()[\uf0c1](#llama_index.llms.base.MessageRole.isalpha \"Permalink to this definition\")\n\nReturn True if the string is an alphabetic string, False otherwise.\n\nA string is alphabetic if all characters in the string are alphabetic and there is at least one character in the string.\n\nisascii()[\uf0c1](#llama_index.llms.base.MessageRole.isascii \"Permalink to this definition\")\n\nReturn True if all characters in the string are ASCII, False otherwise.\n\nASCII characters have code points in the range U+0000-U+007F. Empty string is ASCII too.\n\nisdecimal()[\uf0c1](#llama_index.llms.base.MessageRole.isdecimal \"Permalink to this definition\")\n\nReturn True if the string is a decimal string, False otherwise.\n\nA string is a decimal string if all characters in the string are decimal and there is at least one character in the string.\n\nisdigit()[\uf0c1](#llama_index.llms.base.MessageRole.isdigit \"Permalink to this definition\")\n\nReturn True if the string is a digit string, False otherwise.\n\nA string is a digit string if all characters in the string are digits and there is at least one character in the string.\n\nisidentifier()[\uf0c1](#llama_index.llms.base.MessageRole.isidentifier \"Permalink to this definition\")\n\nReturn True if the string is a valid Python identifier, False otherwise.\n\nCall keyword.iskeyword(s) to test whether string s is a reserved identifier, such as \u201cdef\u201d or \u201cclass\u201d.\n\nislower()[\uf0c1](#llama_index.llms.base.MessageRole.islower \"Permalink to this definition\")\n\nReturn True if the string is a lowercase string, False otherwise.\n\nA string is lowercase if all cased characters in the string are lowercase and there is at least one cased character in the string.\n\nisnumeric()[\uf0c1](#llama_index.llms.base.MessageRole.isnumeric \"Permalink to this definition\")\n\nReturn True if the string is a numeric string, False otherwise.\n\nA string is numeric if all characters in the string are numeric and there is at least one character in the string.\n\nisprintable()[\uf0c1](#llama_index.llms.base.MessageRole.isprintable \"Permalink to this definition\")\n\nReturn True if the string is printable, False otherwise.\n\nA string is printable if all of its characters are considered printable in repr() or if it is empty.\n\nisspace()[\uf0c1](#llama_index.llms.base.MessageRole.isspace \"Permalink to this definition\")\n\nReturn True if the string is a whitespace string, False otherwise.\n\nA string is whitespace if all characters in the string are whitespace and there is at least one character in the string.\n\nistitle()[\uf0c1](#llama_index.llms.base.MessageRole.istitle \"Permalink to this definition\")\n\nReturn True if the string is a title-cased string, False otherwise.\n\nIn a title-cased string, upper- and title-case characters may only follow uncased characters and lowercase characters only cased ones.\n\nisupper()[\uf0c1](#llama_index.llms.base.MessageRole.isupper \"Permalink to this definition\")\n\nReturn True if the string is an uppercase string, False otherwise.\n\nA string is uppercase if all cased characters in the string are uppercase and there is at least one cased character in the string.\n\njoin(_iterable_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.join \"Permalink to this definition\")\n\nConcatenate any number of strings.\n\nThe string whose method is called is inserted in between each given string. The result is returned as a new string.\n\nExample: \u2018.\u2019.join(\\[\u2018ab\u2019, \u2018pq\u2019, \u2018rs\u2019\\]) -> \u2018ab.pq.rs\u2019\n\nljust(_width_, _fillchar\\=' '_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.ljust \"Permalink to this definition\")\n\nReturn a left-justified string of length width.\n\nPadding is done using the specified fill character (default is a space).\n\nlower()[\uf0c1](#llama_index.llms.base.MessageRole.lower \"Permalink to this definition\")\n\nReturn a copy of the string converted to lowercase.\n\nlstrip(_chars\\=None_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.lstrip \"Permalink to this definition\")\n\nReturn a copy of the string with leading whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.\n\n_static_ maketrans()[\uf0c1](#llama_index.llms.base.MessageRole.maketrans \"Permalink to this definition\")\n\nReturn a translation table usable for str.translate().\n\nIf there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters to Unicode ordinals, strings or None. Character keys will be then converted to ordinals. If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.\n\npartition(_sep_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.partition \"Permalink to this definition\")\n\nPartition the string into three parts using the given separator.\n\nThis will search for the separator in the string. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it.\n\nIf the separator is not found, returns a 3-tuple containing the original string and two empty strings.\n\nremoveprefix(_prefix_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.removeprefix \"Permalink to this definition\")\n\nReturn a str with the given prefix string removed if present.\n\nIf the string starts with the prefix string, return string\\[len(prefix):\\]. Otherwise, return a copy of the original string.\n\nremovesuffix(_suffix_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.removesuffix \"Permalink to this definition\")\n\nReturn a str with the given suffix string removed if present.\n\nIf the string ends with the suffix string and that suffix is not empty, return string\\[:-len(suffix)\\]. Otherwise, return a copy of the original string.\n\nreplace(_old_, _new_, _count\\=\\-1_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.replace \"Permalink to this definition\")\n\nReturn a copy with all occurrences of substring old replaced by new.\n\n> count\n> \n> Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences.\n\nIf the optional argument count is given, only the first count occurrences are replaced.\n\nrfind(_sub_\\[, _start_\\[, _end_\\]\\]) \u2192 int[\uf0c1](#llama_index.llms.base.MessageRole.rfind \"Permalink to this definition\")\n\nReturn the highest index in S where substring sub is found, such that sub is contained within S\\[start:end\\]. Optional arguments start and end are interpreted as in slice notation.\n\nReturn -1 on failure.\n\nrindex(_sub_\\[, _start_\\[, _end_\\]\\]) \u2192 int[\uf0c1](#llama_index.llms.base.MessageRole.rindex \"Permalink to this definition\")\n\nReturn the highest index in S where substring sub is found, such that sub is contained within S\\[start:end\\]. Optional arguments start and end are interpreted as in slice notation.\n\nRaises ValueError when the substring is not found.\n\nrjust(_width_, _fillchar\\=' '_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.rjust \"Permalink to this definition\")\n\nReturn a right-justified string of length width.\n\nPadding is done using the specified fill character (default is a space).\n\nrpartition(_sep_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.rpartition \"Permalink to this definition\")\n\nPartition the string into three parts using the given separator.\n\nThis will search for the separator in the string, starting at the end. If the separator is found, returns a 3-tuple containing the part before the separator, the separator itself, and the part after it.\n\nIf the separator is not found, returns a 3-tuple containing two empty strings and the original string.\n\nrsplit(_sep\\=None_, _maxsplit\\=\\-1_)[\uf0c1](#llama_index.llms.base.MessageRole.rsplit \"Permalink to this definition\")\n\nReturn a list of the substrings in the string, using sep as the separator string.\n\n> sep\n> \n> The separator used to split the string.\n> \n> When set to None (the default value), will split on any whitespace character (including n r t f and spaces) and will discard empty strings from the result.\n> \n> maxsplit\n> \n> Maximum number of splits (starting from the left). -1 (the default value) means no limit.\n\nSplitting starts at the end of the string and works to the front.\n\nrstrip(_chars\\=None_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.rstrip \"Permalink to this definition\")\n\nReturn a copy of the string with trailing whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.\n\nsplit(_sep\\=None_, _maxsplit\\=\\-1_)[\uf0c1](#llama_index.llms.base.MessageRole.split \"Permalink to this definition\")\n\nReturn a list of the substrings in the string, using sep as the separator string.\n\n> sep\n> \n> The separator used to split the string.\n> \n> When set to None (the default value), will split on any whitespace character (including n r t f and spaces) and will discard empty strings from the result.\n> \n> maxsplit\n> \n> Maximum number of splits (starting from the left). -1 (the default value) means no limit.\n\nNote, str.split() is mainly useful for data that has been intentionally delimited. With natural text that includes punctuation, consider using the regular expression module.\n\nsplitlines(_keepends\\=False_)[\uf0c1](#llama_index.llms.base.MessageRole.splitlines \"Permalink to this definition\")\n\nReturn a list of the lines in the string, breaking at line boundaries.\n\nLine breaks are not included in the resulting list unless keepends is given and true.\n\nstartswith(_prefix_\\[, _start_\\[, _end_\\]\\]) \u2192 bool[\uf0c1](#llama_index.llms.base.MessageRole.startswith \"Permalink to this definition\")\n\nReturn True if S starts with the specified prefix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. prefix can also be a tuple of strings to try.\n\nstrip(_chars\\=None_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.strip \"Permalink to this definition\")\n\nReturn a copy of the string with leading and trailing whitespace removed.\n\nIf chars is given and not None, remove characters in chars instead.\n\nswapcase()[\uf0c1](#llama_index.llms.base.MessageRole.swapcase \"Permalink to this definition\")\n\nConvert uppercase characters to lowercase and lowercase characters to uppercase.\n\ntitle()[\uf0c1](#llama_index.llms.base.MessageRole.title \"Permalink to this definition\")\n\nReturn a version of the string where each word is titlecased.\n\nMore specifically, words start with uppercased characters and all remaining cased characters have lower case.\n\ntranslate(_table_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.translate \"Permalink to this definition\")\n\nReplace each character in the string using the given translation table.\n\n> table\n> \n> Translation table, which must be a mapping of Unicode ordinals to Unicode ordinals, strings, or None.\n\nThe table must implement lookup/indexing via \\_\\_getitem\\_\\_, for instance a dictionary or list. If this operation raises LookupError, the character is left untouched. Characters mapped to None are deleted.\n\nupper()[\uf0c1](#llama_index.llms.base.MessageRole.upper \"Permalink to this definition\")\n\nReturn a copy of the string converted to uppercase.\n\nzfill(_width_, _/_)[\uf0c1](#llama_index.llms.base.MessageRole.zfill \"Permalink to this definition\")\n\nPad a numeric string with zeros on the left, to fill a field of the given width.\n\nThe string is never truncated.\n\n_pydantic model_ llama\\_index.llms.base.ChatMessage[\uf0c1](#llama_index.llms.base.ChatMessage \"Permalink to this definition\")\n\nChat message.\n\nShow JSON schema\n\n{\n   \"title\": \"ChatMessage\",\n   \"description\": \"Chat message.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"role\": {\n         \"default\": \"user\",\n         \"allOf\": \\[\n            {\n               \"$ref\": \"#/definitions/MessageRole\"\n            }\n         \\]\n      },\n      \"content\": {\n         \"title\": \"Content\",\n         \"default\": \"\",\n         \"type\": \"string\"\n      },\n      \"additional\\_kwargs\": {\n         \"title\": \"Additional Kwargs\",\n         \"type\": \"object\"\n      }\n   },\n   \"definitions\": {\n      \"MessageRole\": {\n         \"title\": \"MessageRole\",\n         \"description\": \"Message role.\",\n         \"enum\": \\[\n            \"system\",\n            \"user\",\n            \"assistant\",\n            \"function\"\n         \\],\n         \"type\": \"string\"\n      }\n   }\n}\n\nFields\n\n*   [`additional_kwargs (dict)`](#llama_index.llms.base.ChatMessage.additional_kwargs \"llama_index.llms.base.ChatMessage.additional_kwargs\")\n    \n*   [`content (Optional[str])`](#llama_index.llms.base.ChatMessage.content \"llama_index.llms.base.ChatMessage.content\")\n    \n*   [`role (llama_index.llms.base.MessageRole)`](#llama_index.llms.base.ChatMessage.role \"llama_index.llms.base.ChatMessage.role\")\n    \n\n_field_ additional\\_kwargs_: dict_ _\\[Optional\\]_[\uf0c1](#llama_index.llms.base.ChatMessage.additional_kwargs \"Permalink to this definition\")\n\n_field_ content_: Optional\\[str\\]_ _\\= ''_[\uf0c1](#llama_index.llms.base.ChatMessage.content \"Permalink to this definition\")\n\n_field_ role_: [MessageRole](#llama_index.llms.base.MessageRole \"llama_index.llms.base.MessageRole\")_ _\\= MessageRole.USER_[\uf0c1](#llama_index.llms.base.ChatMessage.role \"Permalink to this definition\")\n\n_pydantic model_ llama\\_index.llms.base.ChatResponse[\uf0c1](#llama_index.llms.base.ChatResponse \"Permalink to this definition\")\n\nChat response.\n\nShow JSON schema\n\n{\n   \"title\": \"ChatResponse\",\n   \"description\": \"Chat response.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"message\": {\n         \"$ref\": \"#/definitions/ChatMessage\"\n      },\n      \"raw\": {\n         \"title\": \"Raw\",\n         \"type\": \"object\"\n      },\n      \"delta\": {\n         \"title\": \"Delta\",\n         \"type\": \"string\"\n      },\n      \"additional\\_kwargs\": {\n         \"title\": \"Additional Kwargs\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": \\[\n      \"message\"\n   \\],\n   \"definitions\": {\n      \"MessageRole\": {\n         \"title\": \"MessageRole\",\n         \"description\": \"Message role.\",\n         \"enum\": \\[\n            \"system\",\n            \"user\",\n            \"assistant\",\n            \"function\"\n         \\],\n         \"type\": \"string\"\n      },\n      \"ChatMessage\": {\n         \"title\": \"ChatMessage\",\n         \"description\": \"Chat message.\",\n         \"type\": \"object\",\n         \"properties\": {\n            \"role\": {\n               \"default\": \"user\",\n               \"allOf\": \\[\n                  {\n                     \"$ref\": \"#/definitions/MessageRole\"\n                  }\n               \\]\n            },\n            \"content\": {\n               \"title\": \"Content\",\n               \"default\": \"\",\n               \"type\": \"string\"\n            },\n            \"additional\\_kwargs\": {\n               \"title\": \"Additional Kwargs\",\n               \"type\": \"object\"\n            }\n         }\n      }\n   }\n}\n\nFields\n\n*   [`additional_kwargs (dict)`](#llama_index.llms.base.ChatResponse.additional_kwargs \"llama_index.llms.base.ChatResponse.additional_kwargs\")\n    \n*   [`delta (Optional[str])`](#llama_index.llms.base.ChatResponse.delta \"llama_index.llms.base.ChatResponse.delta\")\n    \n*   [`message (llama_index.llms.base.ChatMessage)`](#llama_index.llms.base.ChatResponse.message \"llama_index.llms.base.ChatResponse.message\")\n    \n*   [`raw (Optional[dict])`](#llama_index.llms.base.ChatResponse.raw \"llama_index.llms.base.ChatResponse.raw\")\n    \n\n_field_ additional\\_kwargs_: dict_ _\\[Optional\\]_[\uf0c1](#llama_index.llms.base.ChatResponse.additional_kwargs \"Permalink to this definition\")\n\n_field_ delta_: Optional\\[str\\]_ _\\= None_[\uf0c1](#llama_index.llms.base.ChatResponse.delta \"Permalink to this definition\")\n\n_field_ message_: [ChatMessage](#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")_ _\\[Required\\]_[\uf0c1](#llama_index.llms.base.ChatResponse.message \"Permalink to this definition\")\n\n_field_ raw_: Optional\\[dict\\]_ _\\= None_[\uf0c1](#llama_index.llms.base.ChatResponse.raw \"Permalink to this definition\")\n\n_pydantic model_ llama\\_index.llms.base.CompletionResponse[\uf0c1](#llama_index.llms.base.CompletionResponse \"Permalink to this definition\")\n\nCompletion response.\n\nFields:\n\ntext: Text content of the response if not streaming, or if streaming,\n\nthe current extent of streamed text.\n\nadditional\\_kwargs: Additional information on the response(i.e. token\n\ncounts, function calling information).\n\nraw: Optional raw JSON that was parsed to populate text, if relevant. delta: New text that just streamed in (only relevant when streaming).\n\nShow JSON schema\n\n{\n   \"title\": \"CompletionResponse\",\n   \"description\": \"Completion response.\\\\n\\\\nFields:\\\\n    text: Text content of the response if not streaming, or if streaming,\\\\n        the current extent of streamed text.\\\\n    additional\\_kwargs: Additional information on the response(i.e. token\\\\n        counts, function calling information).\\\\n    raw: Optional raw JSON that was parsed to populate text, if relevant.\\\\n    delta: New text that just streamed in (only relevant when streaming).\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"text\": {\n         \"title\": \"Text\",\n         \"type\": \"string\"\n      },\n      \"additional\\_kwargs\": {\n         \"title\": \"Additional Kwargs\",\n         \"type\": \"object\"\n      },\n      \"raw\": {\n         \"title\": \"Raw\",\n         \"type\": \"object\"\n      },\n      \"delta\": {\n         \"title\": \"Delta\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": \\[\n      \"text\"\n   \\]\n}\n\nFields\n\n*   [`additional_kwargs (dict)`](#llama_index.llms.base.CompletionResponse.additional_kwargs \"llama_index.llms.base.CompletionResponse.additional_kwargs\")\n    \n*   [`delta (Optional[str])`](#llama_index.llms.base.CompletionResponse.delta \"llama_index.llms.base.CompletionResponse.delta\")\n    \n*   [`raw (Optional[dict])`](#llama_index.llms.base.CompletionResponse.raw \"llama_index.llms.base.CompletionResponse.raw\")\n    \n*   [`text (str)`](#llama_index.llms.base.CompletionResponse.text \"llama_index.llms.base.CompletionResponse.text\")\n    \n\n_field_ additional\\_kwargs_: dict_ _\\[Optional\\]_[\uf0c1](#llama_index.llms.base.CompletionResponse.additional_kwargs \"Permalink to this definition\")\n\n_field_ delta_: Optional\\[str\\]_ _\\= None_[\uf0c1](#llama_index.llms.base.CompletionResponse.delta \"Permalink to this definition\")\n\n_field_ raw_: Optional\\[dict\\]_ _\\= None_[\uf0c1](#llama_index.llms.base.CompletionResponse.raw \"Permalink to this definition\")\n\n_field_ text_: str_ _\\[Required\\]_[\uf0c1](#llama_index.llms.base.CompletionResponse.text \"Permalink to this definition\")\n\n_pydantic model_ llama\\_index.llms.base.LLMMetadata[\uf0c1](#llama_index.llms.base.LLMMetadata \"Permalink to this definition\")\n\nShow JSON schema\n\n{\n   \"title\": \"LLMMetadata\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"context\\_window\": {\n         \"title\": \"Context Window\",\n         \"description\": \"Total number of tokens the model can be input and output for one response.\",\n         \"default\": 3900,\n         \"type\": \"integer\"\n      },\n      \"num\\_output\": {\n         \"title\": \"Num Output\",\n         \"description\": \"Number of tokens the model can output when generating a response.\",\n         \"default\": 256,\n         \"type\": \"integer\"\n      },\n      \"is\\_chat\\_model\": {\n         \"title\": \"Is Chat Model\",\n         \"description\": \"Set True if the model exposes a chat interface (i.e. can be passed a sequence of messages, rather than text), like OpenAI's /v1/chat/completions endpoint.\",\n         \"default\": false,\n         \"type\": \"boolean\"\n      },\n      \"is\\_function\\_calling\\_model\": {\n         \"title\": \"Is Function Calling Model\",\n         \"description\": \"Set True if the model supports function calling messages, similar to OpenAI's function calling API. For example, converting 'Email Anya to see if she wants to get coffee next Friday' to a function call like \\`send\\_email(to: string, body: string)\\`.\",\n         \"default\": false,\n         \"type\": \"boolean\"\n      },\n      \"model\\_name\": {\n         \"title\": \"Model Name\",\n         \"description\": \"The model's name used for logging, testing, and sanity checking. For some models this can be automatically discerned. For other models, like locally loaded models, this must be manually specified.\",\n         \"default\": \"unknown\",\n         \"type\": \"string\"\n      }\n   }\n}\n\nFields\n\n*   [`context_window (int)`](#llama_index.llms.base.LLMMetadata.context_window \"llama_index.llms.base.LLMMetadata.context_window\")\n    \n*   [`is_chat_model (bool)`](#llama_index.llms.base.LLMMetadata.is_chat_model \"llama_index.llms.base.LLMMetadata.is_chat_model\")\n    \n*   [`is_function_calling_model (bool)`](#llama_index.llms.base.LLMMetadata.is_function_calling_model \"llama_index.llms.base.LLMMetadata.is_function_calling_model\")\n    \n*   [`model_name (str)`](#llama_index.llms.base.LLMMetadata.model_name \"llama_index.llms.base.LLMMetadata.model_name\")\n    \n*   [`num_output (int)`](#llama_index.llms.base.LLMMetadata.num_output \"llama_index.llms.base.LLMMetadata.num_output\")\n    \n\n_field_ context\\_window_: int_ _\\= 3900_[\uf0c1](#llama_index.llms.base.LLMMetadata.context_window \"Permalink to this definition\")\n\nTotal number of tokens the model can be input and output for one response.\n\n_field_ is\\_chat\\_model_: bool_ _\\= False_[\uf0c1](#llama_index.llms.base.LLMMetadata.is_chat_model \"Permalink to this definition\")\n\nSet True if the model exposes a chat interface (i.e. can be passed a sequence of messages, rather than text), like OpenAI\u2019s /v1/chat/completions endpoint.\n\n_field_ is\\_function\\_calling\\_model_: bool_ _\\= False_[\uf0c1](#llama_index.llms.base.LLMMetadata.is_function_calling_model \"Permalink to this definition\")\n\nSet True if the model supports function calling messages, similar to OpenAI\u2019s function calling API. For example, converting \u2018Email Anya to see if she wants to get coffee next Friday\u2019 to a function call like send\\_email(to: string, body: string).\n\n_field_ model\\_name_: str_ _\\= 'unknown'_[\uf0c1](#llama_index.llms.base.LLMMetadata.model_name \"Permalink to this definition\")\n\nThe model\u2019s name used for logging, testing, and sanity checking. For some models this can be automatically discerned. For other models, like locally loaded models, this must be manually specified.\n\n_field_ num\\_output_: int_ _\\= 256_[\uf0c1](#llama_index.llms.base.LLMMetadata.num_output \"Permalink to this definition\")\n\nNumber of tokens the model can output when generating a response."
}