{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/getting_started/installation.html",
        "title": "Installation and Setup - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Installation and Setup[\uf0c1](#installation-and-setup \"Permalink to this heading\")\n\n## Installation from Pip[\uf0c1](#installation-from-pip \"Permalink to this heading\")\n\nYou can simply do:\n\n**NOTE:** LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, \u2026). Use the environment variable \u201cLLAMA\\_INDEX\\_CACHE\\_DIR\u201d to control where these files are saved.\n\n## Installation from Source[\uf0c1](#installation-from-source \"Permalink to this heading\")\n\nGit clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do the following:\n\n*   [Install poetry](https://python-poetry.org/docs/#installation) - this will help you manage package dependencies\n    \n*   `poetry shell` - this command creates a virtual environment, which keeps installed packages contained to this project\n    \n*   `poetry install` - this will install the core package requirements\n    \n*   (Optional) `poetry install --with dev,docs` - this will install all dependencies needed for most local development\n    \n\n## OpenAI Environment Setup[\uf0c1](#openai-environment-setup \"Permalink to this heading\")\n\nBy default, we use the OpenAI `gpt-3.5-turbo` model for text generation and `text-embedding-ada-002` for retrieval and embeddings. In order to use this, you must have an OPENAI\\_API\\_KEY setup. You can register an API key by logging into [OpenAI\u2019s page and creating a new API token](https://beta.openai.com/account/api-keys).\n\n## Local Environment Setup[\uf0c1](#local-environment-setup \"Permalink to this heading\")\n\nIf you don\u2019t wish to use OpenAI, the environment will automatically fallback to using `LlamaCPP` and `llama2-chat-13B` for text generation and `BAAI/bge-small-en` for retrieval and embeddings. These models will all run locally.\n\nIn order to use `LlamaCPP`, follow the installation guide [here](https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html). You\u2019ll need to install the `llama-cpp-python` package, preferably compiled to support your GPU. This will use aronund 11.5GB of memory across the CPU and GPU.\n\nIn order to use the local embeddings, simply run `pip install sentence-transformers`. The local embedding model uses about 500MB of memory."
}