{
    "chunk0": [
        "Deep Learning",
        "Tan Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville",
        "Website",
        "Acknowledgments",
        "Notation",
        "Introduction",
        "Historical Trends in Deep Learning",
        "Applied Math and Machine Learning Basics",
        "Linear Algebra",
        "Scalars, Vectors, Matrices and Tensors",
        "Multiplying Matrices and Vectors",
        "Identity and Inverse Matrices",
        "Linear Dependence and Span",
        "Norms",
        "Special Kinds of Matrices and Vectors",
        "Eigendecomposition",
        "Singular Value Decomposition",
        "The Moore-Penrose Pseudoinverse",
        "The Trace Operator",
        "The Determinant",
        "Example: Principal Components Analysis",
        "Probability and Information Theory",
        "Random Variables",
        "Probability Distributions",
        "Marginal Probability",
        "Conditional Probability",
        "The Chain Rule of Conditional Probabilities",
        "Independence and Conditional Independence",
        "Expectation, Variance and Covariance",
        "Common Probability Distributions",
        "Useful Properties of Common Functions",
        "Bayes\u2019 Rule",
        "Technical Details of Continuous Variables",
        "Information Theory",
        "Structured Probabilistic Models",
        "Numerical Computation",
        "Overflow and Underflow",
        "Poor Conditioning",
        "Gradient-Based Optimization",
        "Constrained Optimization",
        "Example: Linear Least Squares",
        "Machine Learning Basics",
        "Learning Algorithms",
        "Capacity, Overfitting and Underfitting",
        "Hyperparameters and Validation Sets",
        "Estimators, Bias and Variance",
        "Maximum Likelihood Estimation",
        "Bayesian Statistics",
        "Supervised Learning Algorithms",
        "Unsupervised Learning Algorithms",
        "Stochastic Gradient Descent",
        "Building a Machine Learning Algorithm",
        "Challenges Motivating Deep Learning",
        "Deep Networks: Modern Practices",
        "Deep Feedforward Networks",
        "Example: Learning XOR",
        "Gradient-Based Learning",
        "Hidden Units",
        "Architecture Design",
        "Back-Propagation and Other Differentiation Algorithms",
        "Historical Notes",
        "Regularization for Deep Learning",
        "Parameter Norm Penalties",
        "Norm Penalties as Constrained Optimization",
        "Regularization and Under-Constrained Problems",
        "Dataset Augmentation",
        "Noise Robustness",
        "Semi-Supervised Learning",
        "Multi-Task Learning",
        "Early Stopping",
        "Parameter Tying and Parameter Sharing",
        "Sparse Representations",
        "Bagging and Other Ensemble Methods",
        "Dropout",
        "Adversarial Training",
        "Tangent Distance, Tangent Prop, and Manifold Tangent Classifier",
        "Optimization for Training Deep Models",
        "How Learning Differs from Pure Optimization",
        "Challenges in Neural Network Optimization",
        "Basic Algorithms",
        "Parameter Initialization Strategies",
        "Algorithms with Adaptive Learning Rates",
        "Approximate Second-Order Methods",
        "Optimization Strategies and Meta-Algorithms",
        "Convolutional Networks",
        "The Convolution Operation",
        "Motivation",
        "Pooling",
        "Convolution and Pooling as an Infinitely Strong Prior",
        "Variants of the Basic Convolution Function",
        "Structured Outputs",
        "Data Types",
        "Efficient Convolution Algorithms",
        "Random or Unsupervised Features",
        "The Neuroscientific Basis for Convolutional Networks",
        "Convolutional Networks and the History of Deep Learning",
        "Sequence Modeling: Recurrent and Recursive Nets",
        "Unfolding Computational Graphs",
        "Recurrent Neural Networks",
        "Bidirectional RNNs",
        "Encoder-Decoder Sequence-to-Sequence Architectures",
        "Deep Recurrent Networks",
        "Recursive Neural Networks",
        "The Challenge of Long-Term Dependencies",
        "Echo State Networks",
        "Leaky Units and Other Strategies for Multiple Time Scales",
        "The Long Short-Term Memory and Other Gated RNNs",
        "Optimization for Long-Term Dependencies",
        "Explicit Memory",
        "Practical Methodology",
        "Performance Metrics",
        "Default Baseline Models",
        "Determining Whether to Gather More Data",
        "Selecting Hyperparameters",
        "Debugging Strategies",
        "Example: Multi-Digit Number Recognition",
        "Applications",
        "Large-Scale Deep Learning",
        "Computer Vision",
        "Speech Recognition",
        "Natural Language Processing",
        "Other Applications",
        "Deep Learning Research",
        "Linear Factor Models",
        "Probabilistic PCA and Factor Analysis",
        "Independent Component Analysis (ICA)",
        "Slow Feature Analysis",
        "Sparse Coding",
        "Autoencoders",
        "Representation Learning",
        "Structured Probabilistic Models for Deep Learning",
        "Monte Carlo Methods",
        "Confronting the Partition Function",
        "Manifold Interpretation of PCA",
        "Undercomplete Autoencoders",
        "Regularized Autoencoders",
        "Representational Power, Layer Size and Depth",
        "Stochastic Encoders and Decoders",
        "Denoising Autoencoders",
        "Learning Manifolds with Autoencoders",
        "Contractive Autoencoders",
        "Predictive Sparse Decomposition",
        "Applications of Autoencoders",
        "Greedy Layer-Wise Unsupervised Pretraining",
        "Transfer Learning and Domain Adaptation",
        "Semi-Supervised Disentangling of Causal Factors",
        "Distributed Representation",
        "Exponential Gains from Depth",
        "Providing Clues to Discover Underlying Causes",
        "The Challenge of Unstructured Modeling",
        "Using Graphs to Describe Model Structure",
        "Sampling from Graphical Models",
        "Advantages of Structured Modeling",
        "Learning about Dependencies",
        "Inference and Approximate Inference",
        "The Deep Learning Approach to Structured Probabilistic Models",
        "Sampling and Monte Carlo Methods",
        "Importance Sampling",
        "Markov Chain Monte Carlo Methods",
        "Gibbs Sampling",
        "The Challenge of Mixing between Separated Modes",
        "The Log-Likelihood Gradient",
        "Stochastic Maximum Likelihood and Contrastive Divergence",
        "Pseudolikelihood",
        "Score Matching and Ratio Matching",
        "Denoising Score Matching",
        "Noise-Contrastive Estimation",
        "Estimating the Partition Function",
        "Inference as Optimization",
        "Expectation Maximization",
        "MAP Inference and Sparse Coding",
        "Variational Inference and Learning",
        "Learned Approximate Inference",
        "Approximate Inference",
        "Deep Generative Models",
        "Boltzmann Machines",
        "Restricted Boltzmann Machines",
        "Deep Belief Networks",
        "Deep Boltzmann Machines",
        "Boltzmann Machines for Real-Valued Data",
        "Convolutional Boltzmann Machines",
        "Boltzmann Machines for Structured or Sequential Outputs",
        "Other Boltzmann Machines",
        "Back-Propagation through Random Operations",
        "Directed Generative Nets",
        "Drawing Samples from Autoencoders",
        "Generative Stochastic Networks",
        "Other Generation Schemes",
        "Evaluating Generative Models",
        "Conclusion",
        "Bibliography",
        "Index"
    ],
    "chunk1": [
        "Acknowledgments",
        "Introduction",
        "Inventors have long dreamed of creating machines that think",
        "Desire for machines that think dates back to ancient Greece",
        "Mythical figures Pygmalion, Daedalus, and Hephaestus as legendary inventors",
        "Galatea, Talos, and Pandora as artificial life",
        "Programmable computers and the possibility of intelligence",
        "Artificial intelligence as a thriving field with practical applications",
        "AI used for automating routine labor, understanding speech or images, making diagnoses, and supporting scientific research",
        "Early days of AI focused on solving intellectually difficult problems",
        "True challenge of AI is solving tasks that are easy for people but hard to describe formally",
        "Solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts",
        "Hierarchy of concepts allows computers to learn complicated concepts by building them out of simpler ones",
        "Avoids the need for human operators to specify all the knowledge the computer needs",
        "Graph showing how concepts are related"
    ],
    "chunk2": [
        "AI approach called deep learning",
        "Early successes of AI in formal environments",
        "IBM's Deep Blue chess-playing system",
        "Difficulty of describing chess to a computer",
        "Computers matching human abilities in object recognition and speech",
        "Difficulty of capturing informal knowledge in a computer",
        "Knowledge base approach to AI",
        "Cyc project and its limitations",
        "Importance of machine learning in AI",
        "Examples of machine learning algorithms",
        "Importance of data representation in machine learning",
        "Challenges in extracting features from raw data",
        "Importance of representation learning",
        "Autoencoder as an example of representation learning",
        "Factors of variation in data",
        "Difficulty in disentangling factors of variation",
        "Deep learning as a solution to representation learning",
        "Different perspectives on depth in deep learning models",
        "Target audiences for the book"
    ],
    "chunk3": [
        "Mapping from features to more abstract features",
        "Hand-designed features",
        "Simple program",
        "Input",
        "Deep learning",
        "Classic machine learning",
        "Rule-based systems",
        "Representation learning",
        "Historical trends in deep learning",
        "Long and rich history of deep learning",
        "Deep learning becoming more useful with more training data",
        "Deep learning models growing in size over time",
        "Deep learning solving increasingly complicated applications with increasing accuracy",
        "Three parts of the book: Applied Math and Machine Learning Basics, Deep Networks: Modern Practices, Deep Learning Research",
        "Readers can skip parts that are not relevant to them",
        "Assumption that readers come from a computer science background",
        "Assumption of familiarity with programming, computational performance issues, complexity theory, calculus, and graph theory",
        "Three waves of development of deep learning: cybernetics, connectionism, and current resurgence",
        "Deep learning previously known as artificial neural networks (ANNs)",
        "Deep learning models inspired by the biological brain",
        "Deep learning models not necessarily neurally inspired",
        "Deep learning models learn multiple levels of composition",
        "Deep learning models inspired by applied math fundamentals",
        "Deep learning models not an attempt to simulate the brain",
        "Deep learning researchers more likely to cite the brain as an influence",
        "Deep learning researchers draw inspiration from many fields",
        "Deep learning and computational neuroscience are separate fields",
        "Connectionism movement in the 1980s",
        "Key concepts from connectionism: distributed representation, parallel distributed processing"
    ],
    "chunk4": [
        "Learn about redness from images of cars, trucks, and birds",
        "Concept of distributed representation",
        "Use of back-propagation to train deep neural networks",
        "Introduction of the back-propagation algorithm",
        "Advances in modeling sequences with neural networks",
        "Introduction of the long short-term memory (LSTM) network",
        "Use of LSTM for sequence modeling tasks",
        "Decline in popularity of neural networks in the 1990s",
        "Advances in kernel machines and graphical models",
        "Continued impressive performance of neural networks",
        "Role of the Canadian Institute for Advanced Research (CIFAR)",
        "Difficulty in training deep networks",
        "Breakthrough in training deep belief networks",
        "Introduction of greedy layer-wise pre-training",
        "Popularization of the term 'deep learning'",
        "Improvement in dataset sizes",
        "Increase in model sizes",
        "Improvement in accuracy, complexity, and real-world impact",
        "Success of deep learning in object recognition and speech recognition",
        "Impact of deep learning on pedestrian detection and image segmentation",
        "Success of deep learning in traffic sign classification",
        "Increase in complexity of tasks solved by deep networks",
        "Introduction of neural Turing machines",
        "Extension of deep learning to reinforcement learning",
        "Profitability and adoption of deep learning by top technology companies",
        "Contributions of deep learning to other sciences",
        "Advances in software infrastructure for deep learning"
    ],
    "chunk5": [
        "Deep learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics, and applied math.",
        "Deep learning provides useful tools for processing massive amounts of data and making useful predictions in scientific fields.",
        "Deep learning has been successfully used in various scientific fields, such as predicting molecular interactions, searching for subatomic particles, and constructing 3-D maps of the human brain.",
        "Deep learning is expected to be used in more scientific fields in the future.",
        "Deep neural networks have been doubling in size roughly every 2.4 years since the introduction of hidden units.",
        "There have been various neural network architectures developed over the years, such as the perceptron, neocognitron, LeNet-5, deep belief network, and GoogLeNet.",
        "Deep networks have consistently won the ImageNet Large Scale Visual Recognition Challenge and achieved lower error rates each year.",
        "Linear algebra is a branch of mathematics widely used in science and engineering.",
        "Linear algebra is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms.",
        "Scalars, vectors, matrices, and tensors are the types of mathematical objects studied in linear algebra.",
        "Scalars are single numbers, vectors are arrays of numbers, matrices are 2-D arrays of numbers, and tensors are arrays with more than two axes.",
        "The transpose of a matrix is the mirror image of the matrix across the main diagonal."
    ],
    "chunk6": [
        "Define a vector by writing out its elements in the text inline as a row matrix, then using the transpose operator to turn it into a standard column vector",
        "A scalar can be thought of as a matrix with only a single entry",
        "We can add matrices to each other, as long as they have the same shape, just by adding their corresponding elements",
        "We can also add a scalar to a matrix or multiply a matrix by a scalar, just by performing that operation on each element of a matrix",
        "In the context of deep learning, we allow the addition of matrix and a vector, yielding another matrix",
        "The matrix product of matrices A and B is a third matrix C",
        "The dot product between two vectors x and y of the same dimensionality is the matrix product a\u2019 y",
        "Matrix product operations have many useful properties",
        "We can write a system of linear equations using matrix-vector product notation",
        "Linear algebra offers a powerful tool called matrix inversion that allows us to analytically solve equations",
        "An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix",
        "The matrix inverse of A is denoted as A^(-1), and it is defined as the matrix such that A * A^(-1) = I",
        "Determining whether Ax = b has a solution amounts to testing whether b is in the span of the columns of A",
        "A set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors",
        "In order for the matrix to have an inverse, we additionally need to ensure that equation Ax = b has at most one solution for each value of b",
        "Norms are functions mapping vectors to non-negative values that measure the size of a vector",
        "The L2 norm is commonly used in machine learning and is known as the Euclidean norm",
        "The squared L2 norm is more convenient to work with mathematically and computationally than the L2 norm itself",
        "The L1 norm is used when the difference between zero and nonzero elements is very important",
        "The L0 norm is sometimes used to measure the number of nonzero elements in a vector",
        "The max norm measures the size of a vector as the absolute value of the element with the largest magnitude",
        "The Frobenius norm is commonly used to measure the size of a matrix",
        "Diagonal matrices consist mostly of zeros and have non-zero entries only along the main diagonal",
        "Symmetric matrices are matrices that are equal to their own transpose",
        "A unit vector is a vector with unit norm",
        "Orthogonal matrices are square matrices whose rows and columns are mutually orthonormal"
    ],
    "chunk7": [
        "Orthogonal matrices are of interest because their inverse is very cheap to compute.",
        "The rows of orthogonal matrices are not merely orthogonal but fully orthonormal.",
        "Eigendecomposition is a widely used matrix decomposition that decomposes a matrix into a set of eigenvectors and eigenvalues.",
        "An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of v.",
        "The scalar \u03bb is known as the eigenvalue corresponding to the eigenvector.",
        "The eigendecomposition of A is given by A = Vdiag(A)V^(-1), where V is a matrix of eigenvectors and diag(A) is a diagonal matrix of eigenvalues.",
        "Not every matrix can be decomposed into eigenvalues and eigenvectors.",
        "Every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues.",
        "The singular value decomposition (SVD) provides another way to factorize a matrix into singular vectors and singular values.",
        "The SVD is more generally applicable than the eigendecomposition.",
        "The Moore-Penrose pseudoinverse allows us to make some headway in cases where matrix inversion is not defined.",
        "The pseudoinverse of A is defined as At+ = VD^(-1)U^T, where U, D, and V are the singular value decomposition of A.",
        "The trace operator gives the sum of all the diagonal entries of a matrix.",
        "The determinant of a square matrix is equal to the product of all the eigenvalues of the matrix.",
        "Principal components analysis (PCA) is a machine learning algorithm that can be derived using basic linear algebra.",
        "PCA is used for lossy compression of data by representing the data in a lower-dimensional space.",
        "PCA uses the singular value decomposition to find the optimal code for each input point."
    ],
    "chunk8": [
        "Efficient encoding using matrix-vector operation",
        "Encoder function f(x) = D'e",
        "PCA reconstruction operation r(@) = g(f(x)) = DD'z",
        "Minimizing L2 distance between inputs and reconstructions",
        "Minimizing Frobenius norm of matrix of errors",
        "Deriving algorithm for finding optimal encoding matrix D*",
        "Case where | = 1 and D is a single vector d",
        "Optimization problem for finding d*",
        "Rewriting problem in terms of design matrix X",
        "Simplifying Frobenius norm portion of the problem",
        "Re-introducing the constraint",
        "Optimization problem may be solved using eigendecomposition",
        "Derivation specific to case of | = 1 and first principal component",
        "Matrix D is given by the k eigenvectors corresponding to the largest eigenvalues",
        "Linear algebra is fundamental to understand deep learning",
        "Probability theory is necessary in machine learning",
        "Probability theory provides means of quantifying uncertainty",
        "Probability theory used in AI systems reasoning and analysis",
        "Information theory quantifies uncertainty in probability distribution",
        "Probability theory and information theory are key in machine learning",
        "Probability theory developed to analyze frequencies of events",
        "Probability theory used to study repeatable events",
        "Probability theory used to represent degree of belief",
        "Bayesian probability represents qualitative levels of certainty",
        "Probability extends logic to deal with uncertainty",
        "Random variable is a variable that can take on different values randomly",
        "Random variable can be discrete or continuous",
        "Probability distribution describes likelihood of random variable's states",
        "Probability mass function (PMF) for discrete variables",
        "PMF maps state of random variable to probability of that state",
        "Joint probability distribution for multiple variables",
        "Properties of probability mass function",
        "Example of uniform distribution on a single discrete random variable"
    ],
    "chunk9": [
        "Probability density function (PDF) is used to describe probability distributions of continuous random variables.",
        "Properties of a probability density function: domain must be the set of all possible states of x, p(x) > 0 for all x, and the integral of p(x) over the entire domain must be 1.",
        "Probability density function gives the probability of landing inside an infinitesimal region with volume dx.",
        "The probability that x lies in a set S is given by the integral of p(x) over that set.",
        "Uniform distribution on an interval [a, b] is denoted as x ~ U(a, b).",
        "Marginal probability distribution is the probability distribution over a subset of variables.",
        "Sum rule is used to find the marginal probability distribution for discrete random variables.",
        "Integration is used to find the marginal probability distribution for continuous variables.",
        "Conditional probability is the probability of an event given that another event has happened.",
        "Conditional probability is denoted as P(y = y | x = a).",
        "Chain rule of conditional probabilities states that any joint probability distribution can be decomposed into conditional distributions over one variable.",
        "Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y.",
        "Two random variables x and y are conditionally independent given a random variable z if the conditional probability distribution over x and y factorizes for every value of z.",
        "Expectation or expected value of a function f(x) with respect to a probability distribution P(x) is the average value that f takes on when x is drawn from P.",
        "Variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution.",
        "Covariance gives a measure of how much two values are linearly related to each other, as well as the scale of these variables.",
        "Common probability distributions include Bernoulli, Multinoulli, Gaussian, Exponential, Laplace, Dirac, and Empirical distributions."
    ],
    "chunk10": [
        "Dirac delta function",
        "Empirical distribution",
        "Mixture distribution",
        "Latent variable",
        "Gaussian mixture model",
        "Logistic sigmoid function",
        "Softplus function",
        "Bayes' rule",
        "Measure theory",
        "Information theory"
    ],
    "chunk11": [
        "Information theory deals with the measurement and quantification of information gained from observing events of different probabilities.",
        "Shannon entropy is a measure of the uncertainty or information content in a probability distribution.",
        "The Kullback-Leibler (KL) divergence measures the difference between two probability distributions.",
        "The KL divergence is non-negative and can be used to measure the distance between two distributions.",
        "The cross-entropy is a related quantity to the KL divergence and is used to measure the difference between two distributions.",
        "Structured probabilistic models are used to represent probability distributions over multiple random variables.",
        "Directed models use graphs with directed edges and represent factorizations into conditional probability distributions.",
        "Undirected models use graphs with undirected edges and represent factorizations into a set of functions.",
        "Numerical computation in machine learning algorithms often involves solving mathematical problems iteratively and dealing with approximation errors.",
        "Overflow and underflow are common numerical errors that can occur when representing real numbers on a digital computer.",
        "Underflow occurs when numbers near zero are rounded to zero, while overflow occurs when numbers with large magnitude are approximated as infinity.",
        "The softmax function, commonly used in predicting probabilities, needs to be stabilized against underflow and overflow.",
        "Log softmax can be stabilized using the same technique as softmax."
    ],
    "chunk12": [
        "Theano is a software package that automatically detects and stabilizes numerically unstable expressions in deep learning.",
        "Conditioning refers to how rapidly a function changes with respect to small changes in its inputs.",
        "Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation.",
        "The condition number of a matrix is the ratio of the magnitude of the largest and smallest eigenvalue.",
        "Matrix inversion is particularly sensitive to error in the input when the condition number is large.",
        "Poorly conditioned matrices amplify pre-existing errors when multiplied by the true matrix inverse.",
        "Gradient-based optimization is commonly used in deep learning algorithms.",
        "Optimization refers to the task of minimizing or maximizing a function by altering its inputs.",
        "The function to be minimized or maximized is called the objective function or criterion.",
        "The terms cost function, loss function, and error function are often used interchangeably with objective function.",
        "The value that minimizes or maximizes a function is denoted with a superscript *.",
        "The derivative of a function gives the slope of the function at a given point.",
        "The derivative is useful for minimizing a function because it tells us how to change the input to make a small improvement in the output.",
        "Gradient descent is an optimization algorithm that uses the derivatives of a function to find a minimum.",
        "Critical points are points where the derivative of a function is zero.",
        "Local minima are points where the function is lower than at all neighboring points.",
        "Local maxima are points where the function is higher than at all neighboring points.",
        "Saddle points are points where the function has neighbors that are both higher and lower than the point itself.",
        "A global minimum is a point that obtains the absolute lowest value of the function.",
        "Functions with multiple inputs require the use of partial derivatives.",
        "The gradient of a function is the vector containing all of its partial derivatives.",
        "The directional derivative in a specific direction is the slope of the function in that direction.",
        "Steepest descent is an optimization algorithm that uses the gradient to find a minimum.",
        "The learning rate determines the size of the step taken in steepest descent.",
        "The Hessian matrix contains all the second partial derivatives of a function.",
        "The Hessian matrix can be used to determine the curvature of a function.",
        "The second derivative test can be used to determine whether a critical point is a local maximum, local minimum, or saddle point.",
        "The condition number of the Hessian measures how much the second derivatives differ from each other."
    ],
    "chunk13": [
        "A saddle point is a point in a function where there is both positive and negative curvature.",
        "Saddle points can be found in functions with both positive and negative eigenvalues.",
        "Gradient descent fails to exploit the curvature information contained in the Hessian matrix.",
        "Newton's method is a second-order optimization algorithm that uses the Hessian matrix.",
        "Optimization algorithms that use only the gradient are called first-order optimization algorithms.",
        "Optimization algorithms that use the Hessian matrix are called second-order optimization algorithms.",
        "Deep learning algorithms tend to lack guarantees because the family of functions used in deep learning is complicated.",
        "Lipschitz continuity is a property of functions where the rate of change is bounded by a Lipschitz constant.",
        "Convex optimization algorithms are applicable only to convex functions.",
        "Constrained optimization involves finding the maximal or minimal value of a function within a set of constraints.",
        "The Karush-Kuhn-Tucker (KKT) approach provides a general solution to constrained optimization.",
        "The KKT conditions are necessary conditions for a point to be optimal in constrained optimization.",
        "Linear least squares is a problem of finding the value of x that minimizes a quadratic function.",
        "The Moore-Penrose pseudoinverse can be used to find the smallest-norm solution to the unconstrained least squares problem."
    ],
    "chunk14": [
        "Deep learning is a specific kind of machine learning.",
        "Understanding deep learning requires a solid understanding of the basic principles of machine learning.",
        "Novice readers or those who want a wider perspective are encouraged to consider machine learning textbooks with a more comprehensive coverage of the fundamentals.",
        "Machine learning basics are covered in this chapter.",
        "Section 5.11 covers traditional machine learning techniques that have influenced the development of deep learning algorithms.",
        "The chapter begins with a definition of a learning algorithm and presents an example of the linear regression algorithm.",
        "The challenge of fitting the training data is different from finding patterns that generalize to new data.",
        "Machine learning algorithms have hyperparameters that must be determined external to the learning algorithm itself.",
        "Setting hyperparameters can be done using additional data.",
        "Machine learning is a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions.",
        "The two central approaches to statistics are frequentist estimators and Bayesian inference.",
        "Machine learning algorithms can be divided into supervised learning and unsupervised learning.",
        "Deep learning algorithms are based on the stochastic gradient descent optimization algorithm.",
        "Various algorithm components, such as an optimization algorithm, a cost function, a model, and a dataset, can be combined to build a machine learning algorithm.",
        "Traditional machine learning has limitations in generalization, which has led to the development of deep learning algorithms.",
        "A learning algorithm is an algorithm that is able to learn from data.",
        "Learning is the means of attaining the ability to perform a task.",
        "Machine learning tasks are usually described in terms of how the system should process an example.",
        "Common machine learning tasks include classification, regression, transcription, machine translation, structured output, anomaly detection, synthesis and sampling, imputation of missing values, and denoising.",
        "The performance measure of a machine learning algorithm depends on the task being performed.",
        "Accuracy and error rate are common performance measures for classification tasks.",
        "Density estimation tasks require a different performance metric, such as average log-probability.",
        "Machine learning algorithms can be unsupervised or supervised.",
        "Unsupervised learning involves learning the structure of a dataset, while supervised learning involves learning to predict a target value or vector."
    ],
    "chunk15": [
        "Supervised learning problem of learning p(y | x) can be solved using traditional unsupervised learning technologies to learn the joint distribution p(x, y) and inferring p(y | x)",
        "Regression, classification, and structured output problems are considered supervised learning",
        "Density estimation in support of other tasks is considered unsupervised learning",
        "Semi-supervised learning includes examples with and without supervision targets",
        "Multi-instance learning labels entire collections of examples instead of individual members",
        "Reinforcement learning algorithms interact with an environment and have a feedback loop",
        "Machine learning algorithms experience a dataset, which is a collection of examples",
        "A design matrix is a matrix containing a different example in each row and each column corresponds to a different feature",
        "In cases where examples cannot be described as vectors of the same size, the dataset is described as a set containing elements of different sizes",
        "In supervised learning, examples contain labels or targets in addition to features",
        "Linear regression is an example of a machine learning algorithm that solves a regression problem",
        "Linear regression predicts the value of a scalar output as a linear function of the input",
        "The parameters in linear regression control the behavior of the system",
        "The performance measure in linear regression is the mean squared error",
        "The goal of a machine learning algorithm is to minimize the generalization error, which is the expected error on new inputs",
        "Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set",
        "Overfitting occurs when the gap between the training error and test error is too large",
        "The capacity of a learning algorithm determines its ability to fit a wide variety of functions",
        "The hypothesis space of a learning algorithm is the set of functions it can select as the solution",
        "Changing the number of input features and parameters can change the capacity of a model",
        "Representational capacity refers to the family of functions that the learning algorithm can choose from"
    ],
    "chunk16": [
        "The imperfection of the optimization algorithm may result in the learning algorithm's effective capacity being less than the representational capacity of the model family.",
        "The principle of parsimony, also known as Occam's razor, states that among competing hypotheses that explain known observations equally well, one should choose the simplest one.",
        "Statistical learning theory provides various means of quantifying model capacity, with the most well-known being the Vapnik-Chervonenkis dimension (VC dimension).",
        "Quantifying the capacity of the model allows statistical learning theory to make quantitative predictions about the discrepancy between training error and generalization error.",
        "The bounds provided by statistical learning theory show that the discrepancy between training error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases.",
        "Determining the capacity of a deep learning model is difficult due to the limitations of the optimization algorithm and the lack of theoretical understanding of non-convex optimization problems in deep learning.",
        "Simpler functions are more likely to generalize well, but a sufficiently complex hypothesis is still needed to achieve low training error.",
        "The relationship between capacity and error typically follows a U-shaped curve, with underfitting occurring when capacity is too low and overfitting occurring when capacity is too high.",
        "Non-parametric models have no limitation on the size of the parameter vector and can be designed to have complexity that depends on the training set size.",
        "Non-parametric models can be theoretical abstractions or practical models like nearest neighbor regression.",
        "Non-parametric learning algorithms can be created by wrapping a parametric learning algorithm inside another algorithm that increases the number of parameters as needed.",
        "The ideal model is an oracle that knows the true probability distribution that generates the data, but it still incurs some error due to noise or other variables.",
        "Training and generalization error vary as the size of the training set varies, with expected generalization error never increasing as the number of training examples increases.",
        "The no free lunch theorem states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points.",
        "Regularization is any modification made to a learning algorithm to reduce its generalization error without reducing its training error.",
        "Regularization can be achieved by modifying the hypothesis space, expressing preferences for certain solutions, or adding penalties to the cost function.",
        "Hyperparameters are settings that control the behavior of a machine learning algorithm and are not adapted by the learning algorithm itself.",
        "A validation set is used to estimate the generalization error during or after training and guide the selection of hyperparameters.",
        "The validation set is constructed from the training data by splitting it into two disjoint subsets, with one subset used for learning the parameters and the other subset used for validation.",
        "Repeatedly using the same test set for evaluating performance can lead to optimistic evaluations, and it is recommended to use new benchmark datasets to reflect the true field performance of a trained system."
    ],
    "chunk17": [
        "Cross-Validation",
        "Problem with small test set",
        "Alternative procedures for small datasets",
        "k-fold cross-validation",
        "Estimators, Bias, and Variance",
        "Parameter estimation",
        "Bias and variance",
        "Point estimation",
        "Function estimation",
        "Unbiased estimators",
        "Bias of an estimator",
        "Variance and standard error",
        "Trade-off between bias and variance",
        "Mean squared error"
    ],
    "chunk18": [
        "MSE measures the overall expected deviation between the estimator and the true value of the parameter",
        "Evaluating the MSE incorporates both the bias and the variance",
        "Desirable estimators have small MSE and manage to keep both their bias and variance in check",
        "Capacity, underfitting, and overfitting are tightly linked to the relationship between bias and variance",
        "Increasing capacity tends to increase variance and decrease bias",
        "Consistency refers to the convergence of point estimates to the true value of the parameter as the number of data points increases",
        "Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows",
        "Asymptotic unbiasedness does not imply consistency",
        "Maximum likelihood principle is a common principle used to derive specific functions that are good estimators for different models",
        "Maximum likelihood estimator is defined as the value of the parameter that maximizes the likelihood function",
        "Maximum likelihood estimation can be interpreted as minimizing the dissimilarity between the empirical distribution and the model distribution",
        "Conditional maximum likelihood estimator is used to estimate a conditional probability in order to make predictions",
        "Linear regression can be justified as a maximum likelihood procedure",
        "Maximum likelihood estimator is consistent and statistically efficient",
        "Bayesian statistics uses probability to reflect degrees of certainty of states of knowledge",
        "Bayesian estimation makes predictions using a full distribution over the parameter",
        "The prior distribution in Bayesian estimation reflects a high degree of uncertainty about the parameter",
        "Bayesian estimation incorporates the prior distribution to shift probability mass density towards preferred regions of the parameter space",
        "Bayesian methods typically generalize well with limited training data but suffer from high computational cost with large training examples"
    ],
    "chunk19": [
        "Prior distribution over the model parameters",
        "Determining the posterior distribution over the model parameters",
        "Maximum A Posteriori (MAP) Estimation",
        "Supervised Learning Algorithms",
        "Probabilistic Supervised Learning",
        "Support Vector Machines",
        "Other Simple Supervised Learning Algorithms",
        "Nearest Neighbor Regression",
        "Decision Trees"
    ],
    "chunk20": [
        "Decision tree is a type of learning algorithm that breaks the input space into regions and has separate parameters for each region.",
        "Decision trees are trained with specialized algorithms and can be considered non-parametric if allowed to learn a tree of arbitrary size.",
        "Decision trees struggle to solve problems that are easy for logistic regression, such as non-axis-aligned decision boundaries.",
        "Nearest neighbor predictors and decision trees are useful learning algorithms when computational resources are constrained.",
        "Unsupervised learning algorithms do not have a supervision signal and are used to extract information from a distribution without human labor.",
        "Unsupervised learning includes tasks such as density estimation, drawing samples from a distribution, denoising data, finding a manifold, and clustering.",
        "A classic unsupervised learning task is finding the best representation of the data, which can mean preserving information while keeping the representation simpler or more accessible.",
        "Simpler representations can be achieved through lower dimensional representations, sparse representations, and independent representations.",
        "Principal Components Analysis (PCA) is an unsupervised learning algorithm that learns a lower dimensional representation of the data.",
        "PCA learns a representation with lower dimensionality and no linear correlation between elements, which is a step towards learning statistically independent representations.",
        "PCA transforms the data by projecting it onto a linear transformation that aligns the direction of greatest variance with the axes of the new space.",
        "k-means clustering is an unsupervised learning algorithm that divides the training set into k different clusters of examples.",
        "k-means clustering provides a k-dimensional one-hot code vector representing an input, where each entry is mostly zero except for the cluster it belongs to.",
        "Stochastic Gradient Descent (SGD) is an optimization algorithm used in nearly all deep learning.",
        "SGD is an extension of gradient descent that estimates the gradient using a minibatch of examples, making it computationally efficient.",
        "SGD is used to train large linear models on very large datasets and is the main way to learn nonlinear models in deep learning.",
        "Deep learning algorithms combine a dataset, a cost function, an optimization procedure, and a model."
    ],
    "chunk21": [
        "Cost function J(w,b) = \u2014Ex,y~Baata 108 Pmodel (Y | 2), (5.100)",
        "Model specification pmoaea(y | \u00a9) =N (y;a' w+ 6,1)",
        "Optimization algorithm defined by solving for where the gradient of the cost is zero using the normal equations",
        "Ability to replace components independently to obtain a variety of algorithms",
        "Cost function typically includes negative log-likelihood for maximum likelihood estimation",
        "Cost function may include additional terms such as regularization terms",
        "Closed-form optimization still possible with nonlinear models",
        "Iterative numerical optimization required for nonlinear models",
        "Recipe for constructing a learning algorithm supports both supervised and unsupervised learning",
        "Supervised learning supported by defining dataset with X and appropriate cost and model",
        "Unsupervised learning supported by defining dataset with only X and appropriate unsupervised cost and model",
        "Example of unsupervised learning using PCA vector and loss function T(w) = Exemfgass lle \u2014 7(a; w)||3",
        "Cost function may be a function that cannot be evaluated directly, requiring approximation of gradients",
        "Most machine learning algorithms can be described using the recipe of models, costs, and optimization algorithms",
        "Challenges motivating deep learning include failure of traditional algorithms to solve central problems in AI",
        "Generalizing to new examples becomes exponentially more difficult with high-dimensional data",
        "Curse of dimensionality: number of possible configurations increases exponentially with number of variables",
        "Statistical challenge arises due to large number of possible configurations compared to number of training examples",
        "Local constancy and smoothness regularization as priors for generalization",
        "Local constancy prior assumes function does not change much within a small region",
        "Many simpler algorithms rely on local constancy prior but fail to scale to AI-level tasks",
        "Deep learning introduces additional priors to reduce generalization error on sophisticated tasks",
        "Local constancy approach used in k-nearest neighbors algorithm and kernel machines",
        "Decision trees suffer from limitations of smoothness-based learning",
        "Representing complex functions efficiently and generalizing well requires additional assumptions",
        "Deep learning introduces assumptions about composition of factors or features at multiple levels in a hierarchy",
        "Deep learning algorithms provide exponential gains in distinguishing regions with limited examples",
        "Manifold learning as an important concept in machine learning",
        "Manifold is a connected region with local Euclidean structure",
        "Manifold learning allows approximation of data with a small number of dimensions"
    ],
    "chunk22": [
        "Manifold learning algorithms assume that most of R\u201d consists of invalid inputs and interesting inputs occur only along a collection of manifolds containing a small subset of points.",
        "Manifold learning was introduced in the case of continuous-valued data and the unsupervised learning setting.",
        "The assumption that the data lies along a low-dimensional manifold may not always be correct or useful.",
        "The probability distribution over images, text strings, and sounds that occur in real life is highly concentrated.",
        "Uniform noise essentially never resembles structured inputs from these domains.",
        "The distribution of natural language sequences occupies a very small volume in the total space of sequences of letters.",
        "The examples we encounter are connected to each other by other examples, with each example surrounded by other highly similar examples that may be reached by applying transformations to traverse the manifold.",
        "There are multiple manifolds involved in most applications.",
        "The manifold hypothesis is supported by rigorous experiments.",
        "When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the manifold.",
        "Deep learning is a powerful framework for supervised learning.",
        "Deep feedforward networks are the quintessential deep learning models.",
        "Feedforward networks define a mapping from input to output and learn the parameters that result in the best function approximation.",
        "Feedforward networks do not have feedback connections.",
        "Feedforward networks are composed of many different functions connected in a chain.",
        "The length of the chain gives the depth of the model.",
        "The final layer of a feedforward network is called the output layer.",
        "The behavior of the hidden layers in a feedforward network is not directly specified by the training data.",
        "Feedforward networks are called neural because they are loosely inspired by neuroscience.",
        "Each hidden layer of a feedforward network is typically vector-valued.",
        "The dimensionality of the hidden layers determines the width of the model.",
        "Each unit in a hidden layer represents a vector-to-scalar function.",
        "Feedforward networks are function approximation machines designed to achieve statistical generalization.",
        "Linear models have limitations in representing nonlinear functions.",
        "To represent nonlinear functions, linear models can be applied to a transformed input."
    ],
    "chunk23": [
        "Nonlinear transformation",
        "Choosing the mapping \u00a2",
        "Using a generic \u00a2",
        "Manually engineering \u00a2",
        "Learning \u00a2",
        "Parametrizing the representation",
        "Capturing the benefits of generic and manual engineering approaches",
        "Improving models by learning features",
        "Applying the principle of learning features to different models",
        "Design decisions for training a feedforward network",
        "Choosing the optimizer, cost function, and output units",
        "Choosing activation functions for the hidden layer",
        "Designing the architecture of the network",
        "Computing gradients in deep neural networks",
        "Back-propagation algorithm",
        "Historical perspective",
        "Example: Learning XOR",
        "XOR function",
        "Training a feedforward network",
        "Choosing the model form",
        "Linear model",
        "Affine transformation",
        "Activation function",
        "Rectified linear unit (ReLU)",
        "Solution to the XOR problem",
        "Gradient-based learning",
        "Non-convex loss functions",
        "Iterative, gradient-based optimizers",
        "Parameter initialization"
    ],
    "chunk24": [
        "Deep feedforward networks are often improvements of the stochastic gradient descent algorithm",
        "Training neural networks is similar to training other models with gradient descent",
        "The gradient of the cost function must be large and predictable for effective learning",
        "Choosing a cost function and output representation is important in neural network design",
        "Cost functions for neural networks are similar to those for other parametric models",
        "Maximum likelihood is commonly used as the cost function for neural networks",
        "Specialized loss functions can be used to train predictors of conditional statistics",
        "Regularization techniques, such as weight decay, can be applied to neural networks",
        "The cross-entropy cost function is popular due to its avoidance of saturation",
        "The cross-entropy cost function does not have a minimum value for many models",
        "Linear units are used for Gaussian output distributions",
        "Sigmoid units are used for Bernoulli output distributions",
        "Softmax units are used for multinoulli output distributions"
    ],
    "chunk25": [
        "Fraction of counts of each outcome observed in the training set",
        "Maximum likelihood is a consistent estimator",
        "Limited model capacity and imperfect optimization can lead to approximation of observed fractions",
        "Softmax function and its limitations with certain loss functions",
        "Saturating behavior of softmax function",
        "Numerically stable variant of softmax function",
        "Softmax function responds to the difference between its inputs",
        "Saturating behavior of softmax function can cause difficulties for learning",
        "Different ways to produce the argument to the softmax function",
        "Overparametrized and restricted versions of softmax",
        "Softmax function as a way to create competition between units",
        "Relationship between softmax and arg max functions",
        "Different output types for neural networks",
        "Principle of maximum likelihood for designing cost functions",
        "Conditional Gaussian distribution and learning the variance",
        "Heteroscedastic model and predicting different amounts of variance",
        "Learning covariance or precision matrix with richer structure",
        "Gaussian mixture outputs and mixture density networks",
        "Choosing the type of hidden unit for feedforward neural networks",
        "Rectified linear units and their generalizations",
        "Generalizations of rectified linear units to guarantee gradient everywhere",
        "Maxout units as a generalization of rectified linear units"
    ],
    "chunk26": [
        "Maxout units can learn a piecewise linear, convex function with up to k pieces.",
        "Maxout units learn the activation function itself rather than just the relationship between units.",
        "Maxout units can approximate any convex function with arbitrary fidelity.",
        "Maxout layer with two pieces can learn to implement the same function as a traditional layer using different activation functions.",
        "Maxout units need more regularization than rectified linear units.",
        "Maxout units can resist catastrophic forgetting.",
        "Logistic sigmoid and hyperbolic tangent are commonly used activation functions.",
        "Sigmoidal units saturate across most of their domain, making gradient-based learning difficult.",
        "Hyperbolic tangent activation function performs better than the logistic sigmoid.",
        "Other hidden unit types include linear units, softmax units, radial basis function units, softplus units, and hard tanh units.",
        "Linear units can reduce the number of parameters in a network.",
        "Softmax units can represent a probability distribution over a discrete variable.",
        "There are many other possible hidden unit types.",
        "Architecture refers to the overall structure of the network.",
        "Most neural networks are organized into layers.",
        "The main architectural considerations are the depth and width of the network.",
        "Deeper networks often generalize better but are harder to optimize.",
        "Feedforward networks with hidden layers provide a universal approximation framework.",
        "The universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer can approximate any function with any desired non-zero amount of error.",
        "The number of hidden units required by a shallow model may be exponential in the number of inputs.",
        "Choosing a deep model encodes a belief that the function involves composition of several simpler functions.",
        "Greater depth often results in better generalization.",
        "Neural networks can have specialized architectures for specific tasks.",
        "Skip connections can make it easier for the gradient to flow through the network."
    ],
    "chunk27": [
        "Deeper networks generalize better when used to transcribe multi-digit numbers from photographs of addresses",
        "The test set accuracy consistently increases with increasing depth",
        "Other increases to the model size do not yield the same effect",
        "Architecture design involves how to connect layers to each other",
        "Default neural network layer connects every input unit to every output unit",
        "Specialized networks have fewer connections, connecting each unit in the input layer to only a small subset of units in the output layer",
        "Reducing the number of connections reduces the number of parameters and computation required",
        "Convolutional networks use specialized patterns of sparse connections that are effective for computer vision problems",
        "Specific advice on architecture of a generic neural network is difficult to give in this chapter",
        "Subsequent chapters develop architectural strategies that work well for different application domains",
        "Deeper models tend to perform better",
        "Increasing the number of parameters in layers of convolutional networks without increasing their depth is not as effective at increasing test set performance",
        "Using a deep model expresses a useful preference over the space of functions the model can learn",
        "Deep models believe that the function should consist of many simpler functions composed together",
        "Back-propagation allows information from the cost to flow backwards through the network to compute the gradient",
        "Back-propagation is a method for computing the gradient, while stochastic gradient descent is used for learning",
        "Back-propagation can compute derivatives of any function",
        "The gradient most often required is the gradient of the cost function with respect to the parameters",
        "Computational graphs formalize computation as graphs",
        "An operation is a simple function of one or more variables",
        "Computational graphs consist of nodes and edges",
        "The chain rule of calculus is used to compute the derivatives of functions formed by composing other functions",
        "Back-propagation computes the chain rule with a specific order of operations",
        "The gradient of a variable can be obtained by multiplying a Jacobian matrix by a gradient",
        "Back-propagation can be applied to tensors of arbitrary dimensionality",
        "The back-propagation algorithm computes the gradient for each operation in the graph",
        "The back-propagation algorithm reduces the number of common subexpressions"
    ],
    "chunk28": [
        "Simplified version of the back-propagation algorithm for computing the derivatives of u\u2122 with respect to the variables in the graph",
        "Computational cost of this algorithm is proportional to the number of edges in the graph",
        "Forward propagation to obtain the activations of the network",
        "Initialize grad_table, a data structure that will store the computed derivatives",
        "Compute the derivatives of all nodes in the graph",
        "Return the computed derivatives",
        "Back-propagation avoids the exponential explosion in repeated subexpressions",
        "Other algorithms may be able to avoid more subexpressions or conserve memory",
        "Back-propagation computation in fully-connected MLP",
        "Forward propagation through a typical deep neural network and computation of the cost function",
        "Backward computation for the deep neural network",
        "Symbol-to-Symbol derivatives",
        "Algebraic expressions and computational graphs operate on symbols or variables",
        "Assign specific values to symbols when using or training a neural network",
        "Backward computation for the deep neural network",
        "Symbol-to-Symbol approach to computing derivatives",
        "Symbol-to-Number differentiation approach",
        "General Back-Propagation algorithm",
        "Compute the gradient of a scalar z with respect to one of its ancestors in the graph",
        "Compute the gradient with respect to each parent of z in the graph",
        "Multiply the current gradient by the Jacobian of the operation that produced z",
        "Continue multiplying by Jacobians traveling backwards through the graph",
        "Sum the gradients arriving from different paths at a node",
        "Each node in the graph corresponds to a variable",
        "Each variable is associated with get_operation, get_consumers, and get_inputs subroutines",
        "Each operation is associated with a bprop operation that computes a Jacobian-vector product",
        "Back-propagation algorithm does not need to know any differentiation rules",
        "Back-propagation algorithm calls each operation's bprop rules with the right arguments",
        "Back-propagation algorithm is able to achieve great generality",
        "Back-propagation algorithm is formally described in algorithm 6.5",
        "Computational cost of back-propagation algorithm is at most O(n^2)",
        "Most neural network cost functions are differentiable"
    ],
    "chunk29": [
        "Algorithm 6.6: The inner loop subroutine build_grad(V,G,G\u2019,grad_table) of the back-propagation algorithm",
        "The back-propagation algorithm has O(n) cost, which is better than the naive approach",
        "Back-propagation is a table-filling algorithm that avoids recomputation",
        "Example: Back-Propagation for MLP Training",
        "Training a multilayer perceptron using minibatch stochastic gradient descent",
        "Computational graph for the gradient of the MLP",
        "Complications in back-propagation implementation",
        "Differentiation outside the Deep Learning Community",
        "Differentiation techniques in deep learning",
        "Higher-order derivatives and Krylov methods",
        "Historical notes on feedforward networks and back-propagation"
    ],
    "chunk30": [
        "Approaches to gradient descent are still in use",
        "Improvement in neural network performance from 1986 to 2015 can be attributed to larger datasets and more powerful computers",
        "Algorithmic changes have improved the performance of neural networks",
        "Mean squared error was replaced by cross-entropy loss functions",
        "Cross-entropy losses improved the performance of models with sigmoid and softmax outputs",
        "Sigmoid hidden units were replaced by rectified linear units",
        "Rectified linear units greatly improved the performance of feedforward networks",
        "Rectified linear units were avoided in the early 2000s due to a belief that activation functions with non-differentiable points should be avoided",
        "Using rectifying nonlinearity is the single most important factor in improving the performance of a recognition system",
        "Rectified linear units were motivated by biological considerations",
        "Feedforward networks had a bad reputation from 2006-2012",
        "Feedforward networks now perform well with the right resources and engineering practices",
        "Gradient-based learning in feedforward networks is used to develop probabilistic models",
        "Feedforward networks have unfulfilled potential",
        "Regularization is a strategy to reduce test error",
        "There are many forms of regularization available in deep learning",
        "Regularization strategies have been a major research effort in the field",
        "Regularization is used to reduce generalization error",
        "Regularization can involve adding constraints or penalties to the objective function",
        "Regularization can encode prior knowledge or promote generalization",
        "Regularization can be used to make an underdetermined problem determined",
        "Regularization can involve ensemble methods",
        "Regularization in deep learning is based on regularizing estimators",
        "Regularization trades increased bias for reduced variance",
        "Weight decay is a common form of regularization",
        "Weight decay rescales the weights of the model",
        "Weight decay shrinks the weights towards zero",
        "Weight decay affects the weights along the axes defined by the eigenvectors of the Hessian matrix",
        "Weight decay preserves directions that contribute significantly to reducing the objective function",
        "Weight decay decays away components of the weight vector corresponding to unimportant directions"
    ],
    "chunk31": [
        "Sum of squared errors",
        "Objective function with L2 regularization",
        "Objective function with L1 regularization",
        "Effect of L1 regularization on linear regression model",
        "Effect of L2 regularization on linear regression model",
        "Sparsity induced by L1 regularization",
        "LASSO model and feature selection",
        "L1 regularization as MAP Bayesian inference with Laplace prior",
        "Norm penalties as constrained optimization",
        "Effect of constraints on weights",
        "Dataset augmentation for machine learning",
        "Effectiveness of dataset augmentation for object recognition",
        "Effectiveness of dataset augmentation for speech recognition",
        "Effectiveness of dataset augmentation for neural networks",
        "Comparison of machine learning algorithms with dataset augmentation"
    ],
    "chunk32": [
        "Noise injection can be a powerful regularization technique, especially when applied to hidden units.",
        "Noise applied to the weights can be interpreted as a stochastic implementation of Bayesian inference.",
        "Noise applied to the weights can also be interpreted as equivalent to a traditional form of regularization.",
        "Injecting noise at the output targets can prevent maximizing log p(y | x) when y is a mistake.",
        "Semi-supervised learning uses both labeled and unlabeled examples to estimate P(y | x) or predict y from x.",
        "Multi-task learning improves generalization by pooling examples from multiple tasks.",
        "Early stopping is a regularization strategy that involves stopping training when validation set error starts to rise.",
        "Early stopping is a very unobtrusive form of regularization and can be used in conjunction with other strategies.",
        "Early stopping reduces the computational cost of training and provides regularization without additional penalty terms."
    ],
    "chunk33": [
        "Algorithm 7.3: Meta-algorithm using early stopping to determine at what objective value we start to overfit, then continue training until that value is reached.",
        "Splitting the training set into subtrain and valid sets.",
        "Running early stopping algorithm 7.1 on the subtrain and valid sets to update the parameters.",
        "Using early stopping to restrict the optimization procedure to a small volume of parameter space.",
        "Early stopping as a form of regularization.",
        "Equivalence of early stopping and L2 regularization in a simple linear model.",
        "Parameter tying and parameter sharing as ways to express dependencies between model parameters.",
        "Parameter sharing in convolutional neural networks (CNNs).",
        "Sparse representations as a strategy to impose a penalty on the activations of units in a neural network.",
        "Different methods to achieve representational sparsity.",
        "Bagging as a technique for reducing generalization error by combining several models.",
        "Ensemble methods and model averaging."
    ],
    "chunk34": [
        "Ensemble to make partially independent errors",
        "Model averaging is a powerful method for reducing generalization error",
        "Machine learning contests are usually won by methods using model averaging",
        "Not all techniques for constructing ensembles are designed to make the ensemble more regularized",
        "Boosting constructs an ensemble with higher capacity than the individual models",
        "Dropout is a method of making bagging practical for ensembles of large neural networks",
        "Dropout trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from an underlying base network",
        "Dropout uses a minibatch-based learning algorithm and randomly samples a binary mask for each unit in the network",
        "Dropout approximates the inference with sampling or using the geometric mean of the ensemble members' predicted distributions",
        "The weight scaling inference rule is used in dropout to approximate the ensemble predictor",
        "Dropout is computationally cheap and works well with various types of models",
        "The cost of using dropout in a complete system can be significant",
        "Dropout is less effective when very few labeled training examples are available",
        "Unsupervised feature learning can gain an advantage over dropout when additional unlabeled data is available"
    ],
    "chunk35": [
        "Weight decay coefficient is determined by the variance of each feature",
        "Dropout is a means of approximating the sum over all sub-models",
        "Fast dropout is a faster convergence time method than traditional dropout",
        "Dropout boosting lacks the regularizing effect of traditional dropout",
        "DropConnect is a special case of dropout where each product between a weight and a hidden unit state can be dropped",
        "Stochastic pooling is a form of randomized pooling for building ensembles of convolutional networks",
        "Dropout trains an ensemble of models that share hidden units",
        "Adversarial training can be used to probe the level of understanding of a neural network",
        "Adversarial examples are examples intentionally constructed to cause a model to misclassify",
        "Excessive linearity in neural networks can cause adversarial examples",
        "Adversarial training encourages the network to be locally constant in the neighborhood of the training data",
        "Tangent distance algorithm uses a metric derived from knowledge of the manifolds near which probability concentrates",
        "Tangent prop algorithm trains a neural net classifier to be locally invariant to known factors of variation",
        "Manifold tangent classifier estimates the manifold tangent vectors using an autoencoder"
    ],
    "chunk36": [
        "Optimization algorithms are used in deep learning for tasks such as PCA and neural network training.",
        "Specialized optimization techniques have been developed for solving the neural network training problem.",
        "Chapter 4 provides an overview of gradient-based optimization.",
        "This chapter focuses on finding the parameters of a neural network that reduce a cost function.",
        "Optimization for machine learning differs from pure optimization in several ways.",
        "Machine learning optimizes a cost function in the hope of improving a performance measure, while pure optimization aims to minimize the cost function itself.",
        "Machine learning objective functions often involve an average over the training set.",
        "Empirical risk minimization is a common approach in machine learning, where the goal is to minimize the expected generalization error.",
        "Surrogate loss functions are used when the desired loss function is not easily optimized.",
        "Early stopping is a technique used to halt the training algorithm based on a convergence criterion.",
        "Batch and minibatch algorithms are commonly used in machine learning optimization.",
        "Batch algorithms process all training examples simultaneously, while minibatch algorithms use a subset of examples.",
        "Stochastic gradient descent is a popular minibatch algorithm.",
        "Minibatch sizes are determined by factors such as accuracy, hardware limitations, and regularization effects.",
        "Randomly selecting minibatches is important for unbiased estimation of the gradient.",
        "Minibatch stochastic gradient descent follows the gradient of the true generalization error when examples are not repeated."
    ],
    "chunk37": [
        "Additional epochs provide benefit by decreasing training error",
        "Training examples are used only once or with incomplete passes through the training set",
        "Underfitting and computational efficiency are concerns with large training sets",
        "Optimization for training deep models is challenging",
        "Ill-conditioning of the Hessian matrix is a problem in optimization",
        "Ill-conditioning can cause SGD to get stuck and slow down learning",
        "Local minima are a challenge in optimization for deep models",
        "Deep models have multiple local minima due to model identifiability problem",
        "Local minima with high cost can be problematic for optimization algorithms",
        "Saddle points and other flat regions are common in high-dimensional non-convex functions",
        "Saddle points can be a problem for Newton's method",
        "Gradient descent can escape saddle points in many cases",
        "Cliffs can occur in neural networks with many layers",
        "Gradient clipping can help avoid the consequences of cliffs",
        "Long-term dependencies can be a challenge in deep neural networks"
    ],
    "chunk38": [
        "Repeated application of the same parameters in a computational graph can lead to difficulties, such as vanishing and exploding gradients.",
        "The power method algorithm used to find the largest eigenvalue of a matrix is similar to the repeated multiplication by a matrix in a computational graph.",
        "Recurrent networks use the same matrix at each time step, while feedforward networks do not, which helps avoid the vanishing and exploding gradient problem.",
        "Optimization algorithms often rely on noisy or biased estimates of the gradient or Hessian matrix.",
        "Sampling-based estimates are commonly used to compute the gradient when the objective function is intractable.",
        "Poor correspondence between local and global structure can make optimization difficult.",
        "The length of the learning trajectory is an important factor in optimization.",
        "Theoretical limits exist on the performance of optimization algorithms for neural networks.",
        "Stochastic gradient descent (SGD) and its variants are commonly used optimization algorithms for deep learning.",
        "SGD uses a learning rate that is gradually decreased over time.",
        "Momentum is an optimization strategy that accelerates learning by accumulating an exponentially decaying moving average of past gradients.",
        "The momentum algorithm helps overcome problems such as poor conditioning of the Hessian matrix and variance in the stochastic gradient."
    ],
    "chunk39": [
        "The size of the step in optimization depends on the alignment of gradients.",
        "The momentum algorithm accelerates in the direction of the negative gradient until reaching a terminal velocity.",
        "The momentum hyperparameter determines the maximum speed of the momentum algorithm.",
        "Common values of the momentum hyperparameter include 0.5, 0.9, and 0.99.",
        "The momentum hyperparameter can be adapted over time.",
        "The momentum algorithm simulates a particle subject to continuous-time Newtonian dynamics.",
        "The momentum algorithm uses forces proportional to the negative gradient and the negative velocity.",
        "Nesterov momentum is a variant of the momentum algorithm that evaluates the gradient after applying the current velocity.",
        "Nesterov momentum does not improve the rate of convergence in the stochastic gradient case.",
        "The choice of initialization for deep learning models is important and can affect convergence and generalization.",
        "The initial parameters need to break symmetry between different units.",
        "Random initialization of weights is commonly used.",
        "The scale of the initial weights affects optimization and generalization.",
        "Larger initial weights help to break symmetry and avoid losing signal during propagation.",
        "However, large initial weights can cause exploding values and saturation.",
        "The initial scale of the weights can be treated as a hyperparameter.",
        "Sparse initialization can help achieve diversity among units.",
        "The biases are usually set to zero, but may be set to non-zero values in certain cases.",
        "The biases for output units can be initialized to match the marginal statistics of the output.",
        "The biases can be set to avoid saturation at initialization.",
        "The biases can be set to control whether other units participate in a function."
    ],
    "chunk40": [
        "Set bias for h to ensure h \u00a9 1 most of the time at initialization",
        "Initialize variance or precision parameters to 1",
        "Initialize biases to produce the correct marginal mean of the output",
        "Initialize variance parameters to the marginal variance of the output in the training set",
        "Initialize model parameters using machine learning",
        "Initialize supervised model with parameters learned by an unsupervised model trained on the same inputs",
        "Perform supervised training on a related task to initialize model parameters",
        "Perform supervised training on an unrelated task to initialize model parameters",
        "Algorithms with adaptive learning rates",
        "Delta-bar-delta algorithm for adapting individual learning rates for model parameters",
        "AdaGrad algorithm for individually adapting learning rates of model parameters",
        "RMSProp algorithm for modifying AdaGrad to perform better in the non-convex setting",
        "Adam algorithm as an adaptive learning rate optimization algorithm",
        "Choosing the right optimization algorithm",
        "Approximate second-order methods for training deep networks",
        "Newton's method for optimization",
        "Conjugate gradients as an efficient method to avoid calculating the inverse Hessian"
    ],
    "chunk41": [
        "Method of steepest descent",
        "Conjugate gradient method",
        "BFGS algorithm",
        "Limited Memory BFGS (L-BFGS)",
        "Batch normalization",
        "Coordinate descent"
    ],
    "chunk42": [
        "Optimization strategy of alternating between optimizing W with H fixed and optimizing AT with W fixed",
        "Coordinate descent is not effective when one variable strongly influences the optimal value of another variable",
        "Polyak averaging algorithm for averaging points in the trajectory through parameter space",
        "Exponentially decaying running average for Polyak averaging in non-convex problems",
        "Supervised pretraining as a strategy to train simpler models before training the desired model",
        "Greedy algorithms for breaking a problem into components and solving each component in isolation",
        "Greedy supervised pretraining as a strategy to break supervised learning problems into simpler problems",
        "Different variations of greedy supervised pretraining algorithms",
        "Designing models to aid optimization by choosing activation functions and model families that are easy to optimize",
        "Using linear transformations between layers and differentiable activation functions",
        "Other model design strategies to aid optimization, such as skip connections and auxiliary heads",
        "Continuation methods as a strategy to make optimization easier by choosing initial points in well-behaved regions of space",
        "Curriculum learning as a continuation method to plan a learning process from simple to complex concepts",
        "Stochastic curriculum learning as a variation of curriculum learning with a random mix of easy and difficult examples"
    ],
    "chunk43": [
        "Convolutional networks are a specialized kind of neural network for processing data that has a known, grid-like topology.",
        "Convolutional networks use convolution in place of general matrix multiplication in at least one of their layers.",
        "Convolution is a specialized kind of linear operation.",
        "Convolutional networks employ an operation called pooling.",
        "Convolutional networks use variants of the convolution function that are widely used in practice for neural networks.",
        "Convolution can be applied to many kinds of data with different numbers of dimensions.",
        "Convolutional networks make use of neuroscientific principles.",
        "Convolutional networks have played a significant role in the history of deep learning.",
        "The chapter does not address how to choose the architecture of a convolutional network.",
        "Convolution is an operation on two functions of a real-valued argument.",
        "Convolution can be used to obtain a less noisy estimate of a measurement by averaging together several measurements.",
        "Convolution is typically denoted with an asterisk.",
        "Convolution in a convolutional network is often referred to as the input and the kernel.",
        "Convolution can be discrete when working with data on a computer.",
        "Convolution can be applied to multidimensional arrays of data.",
        "Convolution can be commutative or non-commutative depending on whether the kernel is flipped or not.",
        "Convolution can be implemented as multiplication by a matrix.",
        "Convolution leverages sparse interactions, parameter sharing, and equivariant representations.",
        "Convolution allows for working with inputs of variable size.",
        "Convolutional networks have sparse interactions, meaning the kernel is smaller than the input.",
        "Convolutional networks use parameter sharing, where the same parameter is used for more than one function in the model.",
        "Convolutional networks use equivariant representations, allowing for efficient description of complicated interactions between variables."
    ],
    "chunk44": [
        "Parameter sharing in convolutional networks",
        "Efficiency of convolution compared to dense matrix multiplication",
        "Equivariance to translation in convolutional networks",
        "Pooling in convolutional networks",
        "Invariance to translation in pooling",
        "Pooling with downsampling",
        "Different types of pooling functions",
        "Pooling in different neural network architectures",
        "Convolution and pooling as an infinitely strong prior"
    ],
    "chunk45": [
        "Output of softmax: 1,000 class probabilities",
        "Output of matrix multiply: 1,000 units",
        "Output of reshape to vector: 16,384 units",
        "Output of pooling with stride 4: 16x16x64",
        "Output of convolution + ReLU: 64x64x64",
        "Output of pooling with stride 4: 64x64x64",
        "Output of convolution + ReLU: 256x256x64",
        "Input image: 256x256x3",
        "Output of softmax: 1,000 class probabilities",
        "Output of matrix multiply: 1,000 units",
        "Output of reshape to vector: 576 units",
        "Output of pooling to 3x3 grid: 3x3x64",
        "Output of convolution + ReLU: 64x64x64",
        "Output of pooling with stride 4: 64x64x64",
        "Output of convolution + ReLU: 256x256x64",
        "Output of softmax: 1,000 class probabilities",
        "Output of average pooling: 1x1x1,000",
        "Output of convolution: 16x16x1,000",
        "Output of pooling with stride 4: 16x16x64",
        "Output of convolution + ReLU: 64x64x64",
        "Output of pooling with stride 4: 64x64x64",
        "Output of convolution + ReLU: 256x256x64",
        "Input image: 256x256x3",
        "Convolutional network processes fixed image size",
        "Convolutional network processes variable-sized image with fully connected section",
        "Convolutional network does not have any fully connected weight layer",
        "Convolution and pooling can cause underfitting",
        "Convolutional models should be compared to other convolutional models",
        "Variants of the basic convolution function",
        "Convolution in neural networks consists of many applications of convolution in parallel",
        "Input and output of convolutional networks are 3-D tensors",
        "Linear operations in convolutional networks are not commutative",
        "Convolutional networks use zero-padding to control kernel width and output size",
        "Downsampled convolution function samples only every s pixels in each direction",
        "Locally connected layers are useful when each feature should be a function of a small part of space",
        "Connectivity in convolutional layers can be further restricted to reduce memory consumption",
        "Tiled convolution offers a compromise between convolutional and locally connected layers"
    ],
    "chunk46": [
        "Channel coordinates",
        "Spatial coordinates",
        "Convolutional network with specific channel connections",
        "Comparison of locally connected layers, tiled convolution, and standard convolution",
        "Tiled convolution algebraically defined",
        "Interaction between locally connected layers and max-pooling",
        "Other operations necessary for convolutional networks",
        "Convolution as a linear operation",
        "Transpose convolution",
        "Operations needed to train convolutional networks",
        "Structured outputs in convolutional networks",
        "Different strategies for handling output plane size",
        "Recurrent convolutional networks for pixel labeling",
        "Processing predictions for image segmentation",
        "Data types used with convolutional networks",
        "Convolutional networks for inputs with varying spatial extents"
    ],
    "chunk47": [
        "Convolution only makes sense for inputs with variable size that contain varying amounts of observation of the same kind of thing",
        "Convolution does not make sense for inputs with variable size that include different kinds of observations",
        "Efficient convolution algorithms are important for modern convolutional network applications",
        "Convolution can be performed by converting the input and kernel to the frequency domain, performing point-wise multiplication, and converting back to the time domain",
        "Separable convolution is a more efficient approach when the kernel can be expressed as the outer product of vectors",
        "Devising faster convolution algorithms is an active area of research",
        "Random or unsupervised features can be used to reduce the cost of convolutional network training",
        "Three strategies for obtaining convolution kernels without supervised training are random initialization, manual design, and unsupervised learning",
        "Random filters can work well in convolutional networks",
        "Greedy layer-wise pretraining can be used to learn features in convolutional networks",
        "Convolutional networks are inspired by the mammalian vision system",
        "Convolutional networks capture properties of the primary visual cortex (V1) such as spatial mapping and simple and complex cells",
        "Convolutional networks are similar to the inferotemporal cortex (IT) in terms of object recognition",
        "Convolutional networks have differences from the mammalian vision system such as resolution, integration with other senses, and processing of scenes and 3D information",
        "Neuroscience has provided insights into the function of individual cells in convolutional networks",
        "Simple cells in V1 respond to specific spatial frequencies and orientations",
        "Complex cells in V1 respond to combinations of simple cell responses",
        "Complex cells in V1 are invariant to small translations and phase offsets"
    ],
    "chunk48": [
        "Gabor functions with different parameter settings",
        "Correspondences between neuroscience and machine learning",
        "Sparse coding algorithm learns features with receptive fields similar to simple cells",
        "Many learning algorithms learn Gabor-like functions when applied to natural images",
        "Edge detectors are an important part of the statistical structure of natural images",
        "Convolutional networks played an important role in the history of deep learning",
        "Convolutional networks were some of the first deep models to perform well",
        "Convolutional networks were used to win many contests",
        "Convolutional networks were some of the first working deep networks trained with back-propagation",
        "Recurrent neural networks are specialized for processing sequential data",
        "Recurrent networks can scale to longer sequences than networks without sequence-based specialization",
        "Recurrent networks can process sequences of variable length",
        "Recurrent networks share parameters across different parts of the model",
        "Recurrent networks use the same update rule applied to previous outputs",
        "Recurrent networks can be used for tasks that require predicting the future from the past",
        "Recurrent networks learn to use the state as a lossy summary of the past sequence",
        "Unfolding a recurrent computation into a computational graph results in parameter sharing",
        "Unfolding allows the use of the same transition function with the same parameters at every time step"
    ],
    "chunk49": [
        "Information flow forward and backward in time",
        "Recurrent neural networks can be designed using graph unrolling and parameter sharing",
        "Different design patterns for recurrent neural networks",
        "The computational graph for computing the training loss of a recurrent network",
        "The universal property of recurrent neural networks",
        "Forward propagation equations for a recurrent neural network",
        "The total loss for a given sequence",
        "The gradient computation for the loss function",
        "Teacher forcing and networks with output recurrence",
        "Computing the gradient in a recurrent neural network"
    ],
    "chunk50": [
        "Obtaining gradients on the parameter nodes",
        "Using dummy variables to resolve ambiguity in calculus operations",
        "Equations for computing the gradient on the remaining parameters",
        "Different loss functions for recurrent networks",
        "Interpreting the output of an RNN as a probability distribution",
        "Decomposing the joint probability over the sequence of y values",
        "Graphical model interpretation of RNNs",
        "Efficient parametrization of the joint distribution over the observations",
        "Challenges in predicting missing values in the middle of the sequence",
        "Difficulties in optimizing the parameters of RNNs",
        "Parameter sharing in recurrent networks",
        "Determining the length of the sequence in RNNs",
        "Different ways to provide an extra input to an RNN",
        "Conditional distribution over a sequence given another sequence"
    ],
    "chunk51": [
        "Remove the restriction of having sequences of the same length",
        "Bidirectional recurrent neural networks (RNNs)",
        "Combining forward and backward RNNs",
        "Applications of bidirectional RNNs",
        "Encoder-decoder sequence-to-sequence architectures",
        "Training an RNN to map input sequence to output sequence of different lengths",
        "Context representation in encoder-decoder architecture",
        "Deep recurrent networks",
        "Decomposing RNN into multiple layers",
        "Adding depth to RNN operations",
        "Recursive neural networks",
        "Computational graph of recursive neural networks",
        "Advantages of recursive nets over recurrent nets",
        "Structuring the tree in recursive nets",
        "Variants of recursive nets",
        "The challenge of long-term dependencies in recurrent networks"
    ],
    "chunk52": [
        "Projection of output",
        "Recurrent networks involve the composition of the same function multiple times",
        "Function composition employed by recurrent neural networks resembles matrix multiplication",
        "Recurrent neural networks lacking a nonlinear activation function and inputs",
        "Recurrence relation describes the power method",
        "Recurrence relation may be simplified",
        "Recurrence may be simplified further",
        "Eigenvalues with magnitude less than one decay to zero and eigenvalues with magnitude greater than one explode",
        "Vanishing and exploding gradient problem for RNNs",
        "RNN must enter a region of parameter space where gradients vanish",
        "Learning long-term dependencies becomes increasingly difficult",
        "Various approaches to reduce the difficulty of learning long-term dependencies",
        "Echo State Networks",
        "Recurrent weights and input weights are difficult parameters to learn",
        "ESNs and liquid state machines are reservoir computing",
        "Reservoir computing maps an arbitrary length sequence into a fixed-length vector",
        "Training criterion for reservoir computing is convex",
        "Set input and recurrent weights such that the dynamical system is near the edge of stability",
        "Setting the time constants used by leaky units",
        "Adding skip connections through time",
        "Leaky units and a spectrum of different time scales",
        "Removing connections to handle long-term dependencies",
        "Gated RNNs are the most effective sequence models used in practical applications",
        "Gated RNNs create paths through time with derivatives that neither vanish nor explode",
        "LSTM and GRU are examples of gated RNNs"
    ],
    "chunk53": [
        "Introducing self-loops in LSTM to allow gradient flow for long durations",
        "Making the weight on self-loops conditioned on the context",
        "Using gated self-loops to dynamically change the time scale of integration",
        "LSTM has been successful in various applications",
        "LSTM block diagram and forward propagation equations",
        "Other gated RNNs and their differences from LSTM",
        "Optimization for long-term dependencies",
        "Clipping gradients to avoid exploding gradients",
        "Regularizing to encourage information flow and address vanishing gradients",
        "Explicit memory in neural networks"
    ],
    "chunk54": [
        "Neural networks lack a working memory system",
        "Memory networks and neural Turing machines introduced to address the lack of working memory",
        "Memory cells in NTMs are an extension of memory cells in LSTMs and GRUs",
        "NTMs read from and write to multiple memory cells simultaneously",
        "Memory addressing coefficients are chosen via a softmax function",
        "Vector-valued memory cells allow for content-based addressing",
        "Content-based addressing retrieves a complete vector-valued memory based on a pattern",
        "Location-based addressing refers to the memory location without considering the content",
        "Memory cells propagate information and gradients forward and backward in time",
        "Explicit memory approach allows models to learn tasks that ordinary RNNs cannot",
        "Memory addressing coefficients can be interpreted as probabilities",
        "Attention mechanism is used to choose an address in memory",
        "Precision and recall are used as performance metrics in some applications",
        "Coverage is a performance metric for systems that can refuse to make a decision",
        "Default baseline models for different types of problems",
        "Recommendations for optimization algorithms and regularization techniques"
    ],
    "chunk55": [
        "Copying the model and algorithm from a similar task that has been studied extensively",
        "Using features from a trained model to solve other tasks",
        "Considering whether to use unsupervised learning based on the domain",
        "Including unsupervised learning in the first end-to-end baseline if it is known to be important",
        "Determining whether to gather more data based on the performance on the training set",
        "Increasing the size of the model or improving the learning algorithm if the training set performance is poor",
        "Considering the quality of the training data if large models and tuned optimization algorithms do not work well",
        "Measuring the performance on a test set to determine whether to gather more data",
        "Considering the cost and feasibility of gathering more data and reducing test error by other means",
        "Plotting curves to predict how much additional training data is needed to achieve a certain level of performance",
        "Considering the cost and feasibility of gathering more data and improving the learning algorithm",
        "Selecting hyperparameters manually by understanding their effect on model capacity",
        "Adjusting the effective capacity of the model to match the complexity of the task",
        "Finding the lowest generalization error subject to runtime and memory budget",
        "Understanding the relationship between hyperparameters, training error, generalization error, and computational resources",
        "Plotting curves to determine the optimal model capacity",
        "Considering the effect of hyperparameters on model capacity",
        "Tuning the learning rate as it is the most important hyperparameter",
        "Monitoring training and test error to diagnose overfitting or underfitting",
        "Increasing capacity if the error on the training set is higher than the target error rate",
        "Reducing the gap between train and test error to reduce generalization error",
        "Adding regularization to reduce effective model capacity",
        "Collecting more training data to reduce generalization error",
        "Automated algorithms for hyperparameter optimization",
        "Optimization algorithms that choose hyperparameters to optimize an objective function",
        "Grid search as a common practice for hyperparameter tuning",
        "Performing grid search when there are three or fewer hyperparameters",
        "Training a model for every joint specification of hyperparameter values in the Cartesian product",
        "Random search as an alternative to grid search",
        "Sampling joint hyperparameter configurations randomly and running training with each of them"
    ],
    "chunk56": [
        "Grid search involves picking values approximately on a logarithmic scale",
        "Random search is an alternative to grid search that converges faster to good values of hyperparameters",
        "Model-based hyperparameter optimization uses a Bayesian regression model to estimate the expected value of the validation set error",
        "Debugging strategies include visualizing the model in action, visualizing the worst mistakes, reasoning about software using train and test error, fitting a tiny dataset, comparing back-propagated derivatives to numerical derivatives, and monitoring histograms of activations and gradients",
        "The Street View transcription system aimed for human-level, 98% accuracy",
        "The baseline system for the Street View transcription system was a convolutional network with rectified linear units",
        "The output layer of the Street View transcription system was refined to improve coverage",
        "Train and test set performance were instrumented to determine if the problem was underfitting or overfitting"
    ],
    "chunk57": [
        "Debugging strategy: visualize model's worst errors",
        "Problem with training data: cropped images",
        "Solution: expand width of crop region",
        "Adjusting hyperparameters to improve performance",
        "Transcription project success: faster and lower cost",
        "Design principles for success",
        "Deep learning based on connectionism",
        "Importance of large neural networks",
        "High performance hardware and software infrastructure",
        "Fast CPU implementations",
        "GPU implementations",
        "Large-scale distributed implementations",
        "Model compression for reducing inference cost",
        "Dynamic structure in neural networks",
        "Cascade strategy for accelerating inference"
    ],
    "chunk58": [
        "Train a decision tree with each node using a neural network to make the splitting decision",
        "Use a neural network called the gater to select which expert network to use for computing the output",
        "Mixture of experts: gater outputs probabilities or weights for each expert, final output is weighted combination of expert outputs",
        "Hard mixture of experts: gater chooses a single expert for each example, accelerates training and inference time",
        "Train combinatorial gaters using gradient estimators or reinforcement learning techniques",
        "Dynamic structure: switch allows a hidden unit to receive input from different units depending on the context",
        "Switch can be interpreted as an attention mechanism",
        "Specialized hardware implementations of deep networks have been developed to speed up training and inference",
        "Different forms of specialized hardware: ASICs, digital, analog, hybrid, FPGA",
        "Low-precision implementations of backprop-based neural nets can use 8-16 bits of precision",
        "Computer vision is a popular application area for deep learning",
        "Computer vision tasks include object recognition, optical character recognition, image synthesis, and image restoration",
        "Preprocessing for computer vision typically involves standardizing image pixels and resizing images",
        "Dataset augmentation can reduce generalization error in computer vision models",
        "Contrast normalization is a preprocessing technique to remove variation in image contrast",
        "Global contrast normalization subtracts the mean and rescales the standard deviation of image pixels",
        "Local contrast normalization normalizes contrast across small windows of an image",
        "Local contrast normalization can be implemented efficiently using separable convolution",
        "Regularization is important for contrast normalization to avoid division by zero"
    ],
    "chunk59": [
        "Dataset augmentation can improve the generalization of a classifier by increasing the size of the training set and adding modified examples.",
        "Object recognition is a classification task that can benefit from dataset augmentation.",
        "Random translations, rotations, and flips of the input can be used to augment the dataset for object recognition.",
        "Specialized computer vision applications use more advanced transformations for dataset augmentation, such as random perturbation of colors and nonlinear geometric distortions.",
        "Speech recognition is the task of mapping an acoustic signal containing spoken language into the corresponding sequence of words.",
        "Speech recognition systems preprocess the input using specialized hand-designed features, but some deep learning systems learn features from raw input.",
        "The automatic speech recognition (ASR) task involves creating a function that computes the most probable linguistic sequence given the acoustic sequence.",
        "Historically, speech recognition systems combined hidden Markov models (HMMs) and Gaussian mixture models (GMMs).",
        "Neural networks were applied to speech recognition in the late 1980s and early 1990s, achieving performance comparable to GMM-HMM systems.",
        "With larger models and datasets, deep learning improved recognition accuracy in speech recognition.",
        "Unsupervised pretraining using restricted Boltzmann machines (RBMs) was used to build deep feedforward networks for speech recognition.",
        "Deep networks for speech recognition shifted from pretraining and RBMs to techniques like rectified linear units and dropout.",
        "Convolutional networks were used in speech recognition to improve over time-delay neural networks.",
        "End-to-end deep learning speech recognition systems that remove the HMM have been developed.",
        "Neural language models (NLMs) use distributed representations of words to overcome the curse of dimensionality problem in language modeling.",
        "NLMs share statistical strength between similar words and contexts, allowing for generalization.",
        "NLMs use word embeddings to represent words as points in a lower-dimensional feature space.",
        "NLMs can produce high-dimensional outputs, which can be computationally expensive for large vocabularies."
    ],
    "chunk60": [
        "Loss functions and difficulties with standard cross-entropy loss",
        "Use of a short list to limit vocabulary size",
        "Hierarchical softmax to reduce computational burden",
        "Importance sampling to speed up training",
        "Noise-contrastive estimation and ranking loss",
        "Combining neural language models with n-grams",
        "Neural machine translation"
    ],
    "chunk61": [
        "Encoder-decoder architecture for translation between different modalities",
        "Use of RNN to accommodate variable length inputs and outputs",
        "Context summary in encoder-decoder framework",
        "Attention mechanism to focus on specific parts of input sequence",
        "Representation learning for sentences with similar meaning",
        "Alignment of word embeddings in different languages",
        "Distributed representations for symbols and words",
        "Transition in representation of input to neural networks",
        "Neural language models and word embeddings",
        "Applications of neural language models in NLP tasks",
        "Visualization of embeddings using t-SNE algorithm",
        "Recommender systems and collaborative filtering",
        "Content-based recommender systems using deep learning",
        "Use of convolutional networks for music recommendation",
        "Exploration versus exploitation in recommender systems"
    ],
    "chunk62": [
        "Factors that determine the extent of exploration or exploitation",
        "Time scale as a factor in exploration and exploitation",
        "Supervised learning and the absence of tradeoff between exploration and exploitation",
        "Difficulty of evaluating and comparing different policies in reinforcement learning",
        "Techniques for evaluating contextual bandits",
        "Success of deep learning in language modeling, machine translation, and natural language processing",
        "Use of embeddings for symbols and words in deep learning",
        "Research frontier in developing embeddings for phrases and relations",
        "Use of machine learning in search engines for improving representations",
        "Importance of representing relations and reasoning about them",
        "Training data for inferring relations between entities",
        "Structured databases and relational databases as knowledge bases",
        "Learning representations for entities and relations from knowledge bases",
        "Different parametrization approaches for modeling relations between entities",
        "Link prediction as a short-term application of knowledge bases and distributed representations",
        "Challenges in evaluating the performance of a model on a link prediction task",
        "Word-sense disambiguation as an application of knowledge bases and distributed representations",
        "Building a general question answering system using knowledge of relations and natural language understanding",
        "Memory networks for remembering and retrieving specific declarative facts",
        "Applications of deep learning beyond the ones described",
        "Introduction to Part III: Deep Learning Research",
        "Speculative approaches to reducing the amount of labeled data necessary for existing models",
        "Unsupervised learning and its challenges",
        "Statistical and computational challenges in high-dimensional random variables",
        "Approximating intractable computations in probabilistic models",
        "Generative models as an alternative to intractable computations",
        "Importance of Part III for researchers in deep learning",
        "Introduction to Chapter 13: Linear Factor Models",
        "Linear factor models as probabilistic models with latent variables",
        "Use of a stochastic, linear decoder function in linear factor models",
        "Discovery of explanatory factors in linear factor models"
    ],
    "chunk63": [
        "Linear factor models",
        "Probabilistic PCA and Factor Analysis",
        "Independent Component Analysis (ICA)",
        "Slow Feature Analysis",
        "Sparse Coding"
    ],
    "chunk64": [
        "Sparse coding approach combined with non-parametric encoder can minimize reconstruction error and log-prior better than specific parametric encoder",
        "No generalization error to the encoder in sparse coding approach",
        "Sparse coding features generalize better for object recognition tasks than features of a related model based on a parametric encoder",
        "Non-parametric encoder requires more time to compute h given a",
        "Back-propagation through non-parametric encoder is difficult",
        "Sparse coding often produces poor samples",
        "Linear factor models can be interpreted as learning a manifold",
        "Autoencoders are neural networks trained to copy their input to their output",
        "Undercomplete autoencoders learn the most salient features of the data distribution",
        "Regularized autoencoders use a loss function that encourages other properties besides input copying",
        "Sparse autoencoders involve a sparsity penalty on the code layer"
    ],
    "chunk65": [
        "Autoencoders are generative models with latent variables.",
        "The log-likelihood of an autoencoder can be decomposed into the prior distribution over the latent variables and the conditional distribution of the visible variables given the latent variables.",
        "Autoencoders approximate the sum of the joint distribution with a point estimate for one highly likely value of the latent variables.",
        "The log-prior term in the log-likelihood can induce sparsity in the latent variables.",
        "Sparse autoencoders impose a sparsity penalty on the latent variables to prevent low reconstruction error everywhere.",
        "Denoising autoencoders are trained to reconstruct the original, uncorrupted data point from a corrupted version.",
        "Regularizing by penalizing derivatives can be used to enforce that the autoencoder's function does not change much when the input changes slightly.",
        "Deep autoencoders with multiple hidden layers offer advantages such as increased representational power and decreased computational cost.",
        "Stochastic autoencoders use a distribution for the encoder and decoder instead of simple functions.",
        "Denoising autoencoders can estimate the score of the data distribution."
    ],
    "chunk66": [
        "Training autoencoders with the squared error criterion",
        "Using corruption with noise variance to train autoencoders",
        "Illustration of how denoising autoencoders work",
        "Generalization of results for specific parametrizations",
        "Using autoencoders as generative models",
        "Historical perspective on using MLPs for denoising",
        "Introducing the concept of denoising autoencoders",
        "Motivation for denoising autoencoders",
        "Comparison of denoising autoencoders with other approaches",
        "Learning manifolds with autoencoders",
        "Important characteristics of manifolds",
        "Compromise between learning a representation and satisfying a constraint",
        "The role of autoencoders in capturing variations on the manifold",
        "Comparison of autoencoders with other manifold learning approaches",
        "Introduction to contractive autoencoders",
        "Connection between denoising autoencoders and contractive autoencoders",
        "Explanation of the name 'contractive autoencoders'",
        "Local contraction of input neighborhoods in contractive autoencoders",
        "Effect of applying the contractive penalty to sigmoidal units",
        "Approximating the encoder as a linear operator in contractive autoencoders"
    ],
    "chunk67": [
        "Contractive autoencoder",
        "Regularized autoencoders learn manifolds by balancing two opposing forces: reconstruction error and the contractive penalty",
        "The goal of the contractive autoencoder is to learn the manifold structure of the data",
        "The CAE penalizes the Frobenius norm of the local linear approximation of f(a) at every training point x",
        "The CAE encourages each local linear operator to become a contraction",
        "The CAE compromises between reconstruction error and the contractive penalty to yield mostly tiny derivatives",
        "The CAE learns tangent vectors that approximate the tangent planes of the manifold",
        "The CAE interprets the largest singular values of J as the tangent directions it has learned",
        "The CAE should ideally learn tangent directions that correspond to real variations in the data",
        "The CAE regularization criterion can be expensive to compute for deeper autoencoders",
        "Rifai et al. separately train a series of single-layer autoencoders and compose them to form a deep autoencoder",
        "The contraction penalty of the CAE can obtain useless results if the decoder does not impose some sort of scale",
        "The CAE prevents useless results by tying the weights of the encoder and decoder",
        "Predictive sparse decomposition (PSD) is a hybrid model of sparse coding and parametric autoencoders",
        "PSD consists of an encoder and a decoder that are both parametric",
        "PSD is trained to predict the output of iterative inference",
        "PSD has been applied to unsupervised feature learning for object recognition in images and video",
        "PSD has also been applied to audio feature learning",
        "PSD minimizes the reconstruction error and a penalty term on the difference between the hidden code and the output of the encoder",
        "PSD is an example of learned approximate inference",
        "PSD can be interpreted as training a directed sparse coding probabilistic model",
        "The iterative optimization in PSD is only used during training, and the encoder is used for inference during deployment",
        "PSD models can be stacked and used to initialize a deep network for another task",
        "Autoencoders have been successfully applied to dimensionality reduction and information retrieval tasks",
        "Dimensionality reduction was one of the first applications of representation learning and deep learning",
        "Autoencoders can yield lower-dimensional representations that improve performance on tasks like classification",
        "Autoencoders can place semantically related examples near each other in the lower-dimensional space",
        "Information retrieval benefits from dimensionality reduction, especially when using binary codes",
        "Semantic hashing is an approach to information retrieval via dimensionality reduction and binarization",
        "Semantic hashing uses binary codes and a hash table to efficiently perform information retrieval",
        "Semantic hashing has been applied to textual input and images",
        "Binary codes for semantic hashing can be learned by injecting noise during training",
        "Learning a hashing function has been explored further, optimizing a loss more directly linked to the task of finding nearby examples",
        "Representation learning is the process of learning useful representations of data",
        "A good representation makes a subsequent learning task easier",
        "Feedforward networks trained by supervised learning can be seen as performing representation learning",
        "Unsupervised pretraining is a form of representation learning that initializes a deep network for a subsequent supervised learning task",
        "Unsupervised pretraining can leverage unlabeled data to improve performance on supervised tasks",
        "Humans and animals are able to learn from very few labeled examples, possibly by leveraging unsupervised or semi-supervised learning",
        "Greedy layer-wise unsupervised pretraining is a procedure that trains each layer of a deep network independently using unsupervised learning",
        "Greedy layer-wise unsupervised pretraining can be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks",
        "Greedy layer-wise unsupervised pretraining is called greedy because it optimizes each layer independently",
        "Greedy layer-wise unsupervised pretraining is called layer-wise because it trains one layer at a time",
        "Greedy layer-wise unsupervised pretraining is called unsupervised because it uses unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining is called pretraining because it is a first step before joint training of all layers",
        "Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can be used for supervised pretraining",
        "Greedy layer-wise unsupervised pretraining is used as a regularizer and parameter initialization",
        "Greedy layer-wise unsupervised pretraining is not required to train fully connected deep architectures, but it was the first method to succeed",
        "Greedy layer-wise unsupervised pretraining can be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can be used for supervised pretraining",
        "Greedy layer-wise unsupervised pretraining is used as a regularizer and parameter initialization",
        "Greedy layer-wise unsupervised pretraining is not required to train fully connected deep architectures, but it was the first method to succeed",
        "Greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks",
        "Greedy layer-wise unsupervised pretraining is called greedy because it optimizes each layer independently",
        "Greedy layer-wise unsupervised pretraining is called layer-wise because it trains one layer at a time",
        "Greedy layer-wise unsupervised pretraining is called unsupervised because it uses unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining is called pretraining because it is a first step before joint training of all layers",
        "Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can be used for supervised pretraining",
        "Greedy layer-wise unsupervised pretraining is used as a regularizer and parameter initialization",
        "Greedy layer-wise unsupervised pretraining is not required to train fully connected deep architectures, but it was the first method to succeed",
        "Greedy layer-wise unsupervised pretraining can be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can be used for supervised pretraining",
        "Greedy layer-wise unsupervised pretraining is used as a regularizer and parameter initialization",
        "Greedy layer-wise unsupervised pretraining is not required to train fully connected deep architectures, but it was the first method to succeed",
        "Greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks",
        "Greedy layer-wise unsupervised pretraining is called greedy because it optimizes each layer independently",
        "Greedy layer-wise unsupervised pretraining is called layer-wise because it trains one layer at a time",
        "Greedy layer-wise unsupervised pretraining is called unsupervised because it uses unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining is called pretraining because it is a first step before joint training of all layers",
        "Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms",
        "Greedy layer-wise unsupervised pretraining can be used for supervised pretraining",
        "Greedy layer-wise unsupervised pretraining is used as a regularizer and parameter initialization",
        "Greedy layer-wise unsupervised pretraining is not required to train fully connected deep architectures, but it was the first method to succeed"
    ],
    "chunk68": [
        "Algorithm 15.1 Greedy layer-wise unsupervised pretraining protocol.",
        "Unsupervised pretraining combines two different ideas: the choice of initial parameters for a deep neural network and learning about the input distribution.",
        "The choice of initial parameters for a deep neural network can have a regularizing effect on the model and improve optimization.",
        "Learning about the input distribution can help to learn about the mapping from inputs to outputs.",
        "Unsupervised pretraining is sometimes helpful but often harmful, so it is important to understand when and why it works.",
        "Most of the discussion is restricted to greedy unsupervised pretraining.",
        "There are other paradigms for performing semi-supervised learning with neural networks, such as virtual adversarial training and training an autoencoder or generative model at the same time as the supervised model.",
        "The choice of initial parameters for a deep neural network is the least well understood idea in unsupervised pretraining.",
        "Unsupervised pretraining initializes the model in a location that would cause it to approach one local minimum rather than another.",
        "Learning about the input distribution can help to learn about the mapping from inputs to outputs.",
        "The effect of unsupervised pretraining on the supervised training stage is limited.",
        "Unsupervised pretraining can help to discover features that relate to the underlying causes that generate the observed data.",
        "Unsupervised pretraining can act as a regularizer by encouraging the learning algorithm to discover useful features.",
        "Unsupervised pretraining is more effective when the initial representation is poor.",
        "Unsupervised pretraining is most helpful when the number of labeled examples is very small and the number of unlabeled examples is very large.",
        "Unsupervised pretraining is most useful when the function to be learned is extremely complicated.",
        "Unsupervised pretraining has declined in popularity but remains important in natural language processing.",
        "Transfer learning and domain adaptation involve using what has been learned in one setting to improve generalization in another setting.",
        "Transfer learning involves performing two or more different tasks, but many factors that explain the variations in one task are relevant to the variations in another task.",
        "Domain adaptation involves the same task but with a slightly different input distribution.",
        "Concept drift is a form of transfer learning due to gradual changes in the data distribution over time."
    ],
    "chunk69": [
        "Multi-task learning",
        "Transfer learning",
        "Representation learning",
        "Unsupervised deep learning for transfer learning",
        "One-shot learning",
        "Zero-shot learning",
        "Zero-data learning",
        "Joint distribution of random variables",
        "Distributed representation of object categories",
        "Machine translation",
        "Multi-modal learning",
        "Semi-supervised learning",
        "Disentangling of causal factors",
        "Ideal representation",
        "Factors of variation",
        "Generative adversarial networks",
        "Determining salient factors",
        "Robustness to changes in distribution"
    ],
    "chunk70": [
        "Distributed representations are powerful because they can use n features with k values to describe k\u201d different concepts.",
        "Many deep learning algorithms are motivated by the assumption that the hidden units can learn to represent the underlying causal factors that explain the data.",
        "Distributed representations are natural for this approach, because each direction in representation space can correspond to the value of a different underlying configuration variable.",
        "A distributed representation is a vector of n binary features that can take 2^n configurations, each potentially corresponding to a different region in input space.",
        "A symbolic representation is a non-distributed representation that associates the input with a single symbol or category.",
        "Non-distributed representations may contain many entries but without significant meaningful separate control over each entry.",
        "Examples of learning algorithms based on non-distributed representations include clustering methods, k-nearest neighbors algorithms, decision trees, Gaussian mixtures and mixtures of experts, kernel machines with a Gaussian kernel, and language or translation models based on n-grams.",
        "Distributed representations generalize due to shared attributes between different concepts.",
        "Distributed representations have a statistical advantage when a complicated structure can be compactly represented using a small number of parameters.",
        "The number of regions that a distributed representation can distinguish is exponential in the input size and polynomial in the number of hidden units.",
        "Distributed representations can generalize better than non-distributed representations because they can represent exponentially more regions with fewer parameters.",
        "Models based on distributed representations have limited capacity despite being able to encode many different regions.",
        "Deep neural networks can learn features that are interpretable and correspond to meaningful labels.",
        "Deep networks can represent functions more efficiently than shallow networks.",
        "Deep architectures can give an exponential boost to statistical efficiency compared to shallow architectures.",
        "Many kinds of networks with a single hidden layer can be shown to be universal approximators.",
        "Structured probabilistic models with a single hidden layer of latent variables are universal approximators of probability distributions."
    ],
    "chunk71": [
        "Sufficiently deep feedforward network can have an exponential advantage over a network that is too shallow",
        "Probabilistic models like sum-product network (SPN) use polynomial circuits to compute probability distribution",
        "Minimum depth of SPN is required for certain probability distributions",
        "Significant differences between every two finite depths of SPN",
        "Constraints used to make SPNs tractable may limit their representational power",
        "Deep circuits have exponential advantage over shallow circuits even when the shallow circuit approximates the function computed by the deep circuit",
        "Previous theoretical work only made claims regarding exact replication of functions by shallow circuits",
        "An ideal representation disentangles the underlying causal factors of variation",
        "Supervised learning provides a strong clue for representation learning",
        "Unlabeled data can provide hints about the underlying factors",
        "No free lunch theorem shows that regularization strategies are necessary for good generalization",
        "Goal of deep learning is to find generic regularization strategies applicable to a wide variety of AI tasks",
        "List of generic regularization strategies: smoothness, linearity, multiple explanatory factors, causal factors, depth, shared factors across tasks, manifolds, natural clustering, temporal and spatial coherence, sparsity, simplicity of factor dependencies",
        "Representation learning ties together all forms of deep learning",
        "Structured probabilistic models use a graph to describe the interaction between random variables",
        "Graphical models are also referred to as structured probabilistic models",
        "Graphical models have different structures, learning algorithms, and inference algorithms",
        "Graphical models are a key ingredient in deep learning research",
        "Graphical models describe the structure of a probability distribution",
        "Graphical models help overcome challenges in building large-scale probabilistic models",
        "Graphical modeling requires understanding which variables need to interact directly",
        "Two approaches to learning dependencies in graphical models",
        "Deep learning practitioners use different model structures, learning algorithms, and inference procedures compared to the rest of the graphical models research community",
        "Deep learning places emphasis on specific approaches to graphical modeling",
        "Deep learning aims to scale machine learning to solve artificial intelligence challenges",
        "Classification algorithms summarize high-dimensional data with a categorical label",
        "Probabilistic models can perform tasks like density estimation, denoising, missing value imputation, and sampling",
        "Modeling a rich distribution over many random variables is computationally and statistically challenging",
        "Naive approach of storing a lookup table for every possible outcome is not feasible"
    ],
    "chunk72": [
        "Memory: the cost of storing the representation",
        "Statistical efficiency: As the number of parameters in a model increases, so does the amount of training data needed to choose the values of those parameters using a statistical estimator",
        "Runtime: the cost of inference",
        "Runtime: the cost of sampling",
        "The problem with the table-based approach is that we are explicitly modeling every possible kind of interaction between every possible subset of variables",
        "Structured probabilistic models provide a formal framework for modeling only direct interactions between random variables",
        "Structured probabilistic models use graphs to represent interactions between random variables",
        "Directed graphical models are called 'directed' because their edges are directed, that is, they point from one vertex to another",
        "Directed graphical models are defined by a directed acyclic graph and a set of local conditional probability distributions",
        "Undirected models use graphs whose edges are undirected",
        "Undirected graphical models are defined by an undirected graph and a set of factors that measure the affinity of the variables in each clique",
        "The partition function is the value that results in the probability distribution summing or integrating to 1",
        "Undirected models are often intractable to compute exactly and require approximations",
        "The choice of factors in undirected models can determine whether the probability distribution is defined",
        "Energy-based models are a type of undirected model where the probability distribution is defined in terms of an energy function"
    ],
    "chunk73": [
        "p(a,b,c,d,e,f) can be written as F ba, (a, b)bv,c(b, \u00a2)ba,a(a, d)p,e(b, \u20ac) de,e(e, f) for an appropriate choice of the \u00a2 functions",
        "E(x) is known as the energy function",
        "Learning the energy function allows for unconstrained optimization",
        "Energy-based models can have probabilities approaching zero but never reaching it",
        "Boltzmann distribution is an example of a distribution of the form given by equation 16.7",
        "Energy-based models are often called Boltzmann machines",
        "Boltzmann machines can have both binary and real-valued variables",
        "Boltzmann machines with latent variables are more often called Markov random fields or log-linear models",
        "Cliques in an undirected graph correspond to factors of the unnormalized probability function",
        "Energy-based models are a special kind of Markov network",
        "Energy-based models can be viewed as a product of experts",
        "The definition of an energy-based model includes a negative sign for compatibility with physics literature",
        "Free energy is the negative logarithm of the partition function",
        "Separation and d-separation are used to determine conditional independences in graphical models",
        "Separation is used for undirected models, while d-separation is used for directed models",
        "Conditional independence is determined by the presence or absence of active paths in the graph",
        "Graphs may not imply all independences that are present in the distribution",
        "Graphs may fail to encode some independences",
        "Undirected and directed graphs can be converted to each other",
        "Directed models can represent immorality substructures",
        "Undirected models can represent loops of length greater than three, but with chords"
    ],
    "chunk74": [
        "Converting directed models to undirected models by constructing moralized graphs",
        "Moralization adds many edges to the graph and loses implied independences",
        "Converting undirected models to directed models by triangulating the graph and assigning directions to the edges",
        "Factor graphs resolve ambiguity in the interpretation of undirected networks",
        "Sampling from graphical models using ancestral sampling in directed models",
        "Sampling from undirected models using Gibbs sampling",
        "Advantages of structured modeling: reduced cost of representing probability distributions, learning, and inference",
        "Separation of representation of knowledge from learning and inference",
        "Learning about dependencies by introducing latent variables to capture indirect interactions between visible variables",
        "Inference and approximate inference in structured probabilistic models",
        "Deep learning approach to structured probabilistic models: use of distributed representations, more latent variables, complicated nonlinear interactions, and latent variables without specific semantics"
    ],
    "chunk75": [
        "Latent variables in traditional graphical models have specific semantics in mind",
        "Deep graphical models have large groups of units connected to other groups",
        "Deep models connect each visible unit to many hidden units",
        "Deep models use distributed representations",
        "Deep models do not use loopy belief propagation",
        "Deep models use Gibbs sampling or variational inference algorithms",
        "Deep models have a large number of latent variables",
        "Deep models group units into layers with interaction matrices",
        "Deep models tolerate unknowns and approximate solutions",
        "Restricted Boltzmann Machine (RBM) is an example of deep graphical models",
        "RBM has a single layer of latent variables",
        "RBM organizes units into layers and uses matrix connectivity",
        "RBM allows efficient Gibbs sampling",
        "RBM can be used to build deeper models",
        "Monte Carlo methods are used in machine learning",
        "Monte Carlo algorithms provide approximate answers with random error",
        "Sampling is used to approximate sums and integrals",
        "Monte Carlo sampling approximates expectations by averaging",
        "Importance sampling is used when sampling from the base distribution is not feasible",
        "Optimal importance sampling minimizes the variance of the estimator"
    ],
    "chunk76": [
        "Biased importance sampling does not require normalized p or q",
        "Biased importance sampling estimator for discrete variables",
        "Importance sampling can improve efficiency of Monte Carlo estimation",
        "Poor choice of g can make efficiency worse in importance sampling",
        "Importance sampling collects useless samples when g(a) >> p(a)|f(a)|",
        "Importance sampling yields large ratio when g(a) < p(a)|f(a)|",
        "Importance sampling has been used in machine learning algorithms",
        "Markov chain Monte Carlo methods are used when exact samples cannot be drawn",
        "Markov chain Monte Carlo methods use Markov chains to sample from a distribution",
        "Markov chain Monte Carlo methods are applicable to energy-based models",
        "Markov chain Monte Carlo methods converge to a stationary distribution",
        "Gibbs sampling is a simple and effective approach for Markov chain sampling",
        "MCMC methods have a tendency to mix poorly",
        "MCMC samples become highly correlated",
        "Mixing between separated modes is a challenge in MCMC methods",
        "Gibbs sampling can have difficulty in flipping signs of variables",
        "Updating groups of highly dependent units simultaneously can help with mixing",
        "Alternating between sampling from Pmodel(x | h) and Pmodel(h | x) in models with latent variables"
    ],
    "chunk77": [
        "Illustration of the slow mixing problem in deep probabilistic models",
        "Distribution with sharp peaks and low probability regions",
        "Tempering to mix between modes",
        "Parallel tempering",
        "Depth may help mixing",
        "Gradient of the log-likelihood with respect to the parameters",
        "Stochastic Maximum Likelihood and Contrastive Divergence",
        "Approximations to the negative phase",
        "Role of positive and negative phase in learning",
        "Designing a less expensive alternative to MCMC algorithm"
    ],
    "chunk78": [
        "Contrastive divergence (CD) algorithm initializes the Markov chain at each step with samples from the data distribution",
        "CD is an approximation to the correct negative phase",
        "CD fails to suppress regions of high probability that are far from actual training examples",
        "Spurious modes are regions that have high probability under the model but low probability under the data generating distribution",
        "CD is biased for RBMs and fully visible Boltzmann machines",
        "CD can be used as an inexpensive way to initialize a model that could later be fine-tuned via more expensive MCMC methods",
        "CD is useful for training shallow models like RBMs",
        "CD does not provide much help for training deeper models directly",
        "Persistent contrastive divergence (PCD) initializes the Markov chains at each gradient step with their states from the previous gradient step",
        "PCD is more resistant to forming models with spurious modes than CD",
        "PCD can train deep models efficiently",
        "Pseudolikelihood is based on the observation that conditional probabilities can be computed without knowledge of the partition function",
        "Pseudolikelihood can perform better than maximum likelihood for tasks that require only the conditional distributions used during training",
        "Generalized pseudolikelihood techniques are especially powerful if the data has regular structure",
        "Score matching minimizes the expected squared difference between the derivatives of the model's log density and the derivatives of the data's log density"
    ],
    "chunk79": [
        "Score matching requires taking derivatives with respect to x",
        "Score matching is not applicable to models of discrete data",
        "Score matching can be used to pretrain the first hidden layer of a larger model",
        "Generalized score matching (GSM) does not work in high dimensional discrete spaces",
        "Ratio matching applies specifically to binary data",
        "Ratio matching avoids the partition function using the same trick as the pseudolikelihood estimator",
        "Ratio matching outperforms SML, pseudolikelihood and GSM in terms of denoising test set images",
        "Ratio matching requires n evaluations of p per data point",
        "Ratio matching can be useful for dealing with high-dimensional sparse data",
        "Denoising score matching is useful when we do not have access to the true data distribution",
        "Noise-contrastive estimation (NCE) estimates the partition function by treating it as another parameter",
        "NCE reduces the unsupervised learning problem to a supervised learning problem",
        "NCE is most successful when applied to problems with few random variables",
        "NCE does not work if only a lower bound on p is available",
        "Estimating the partition function is important for evaluating and comparing models",
        "Estimating the partition function can be done using importance sampling",
        "Annealed importance sampling bridges the gap between two distributions by introducing intermediate distributions"
    ],
    "chunk80": [
        "Intractable inference problems in deep learning usually arise from interactions between latent variables in a structured graphical model.",
        "Exact inference requires an exponential amount of time in models with multiple layers of hidden variables or even in some models with only a single layer.",
        "Approximate inference techniques can be used to address intractable inference problems.",
        "Inference can be formulated as an optimization problem.",
        "The evidence lower bound (ELBO) is a lower bound on the log probability of the observed data.",
        "The ELBO can be computed by maximizing the difference between the log probability of the observed data and the KL divergence between the approximate distribution and the true posterior distribution.",
        "The ELBO is always less than or equal to the log probability of the observed data.",
        "The ELBO can be rearranged into a more convenient form.",
        "The evidence lower bound is tractable to compute for an appropriate choice of the approximate distribution.",
        "The evidence lower bound provides a lower bound on the likelihood."
    ],
    "chunk81": [
        "Approximations of p(h | v) can be tighter or looser depending on the lower bound \u00a3",
        "Inference is the procedure for finding the g that maximizes L",
        "Exact inference maximizes \u00a3 perfectly by searching over a family of functions q",
        "Approximate inference can be derived by using approximate optimization to find g",
        "The optimization procedure can be less expensive by restricting the family of distributions q or using an imperfect optimization procedure",
        "\u00a3 is a lower bound and can be tighter or looser depending on the choice of q",
        "The EM algorithm is a popular training algorithm for models with latent variables",
        "The EM algorithm consists of alternating between the E-step and the M-step",
        "The E-step sets q(h | v) = p(h | v;@) for all training examples",
        "The M-step maximizes DF! ,6,q) with respect to @",
        "Stochastic gradient ascent on latent variable models is a special case of the EM algorithm",
        "MAP inference computes the single most likely value of the missing variables",
        "MAP inference can be seen as a form of approximate inference",
        "Variational inference maximizes \u00a3 over a restricted family of distributions g",
        "The mean field approach is a common approach to variational learning",
        "Variational learning can be applied to models with discrete latent variables",
        "Variational learning with discrete latent variables involves optimizing the parameters of the distribution g",
        "Variational learning with discrete latent variables can be done using fixed point equations",
        "Variational learning with discrete latent variables is typically fast",
        "Variational learning with continuous latent variables involves optimization over a space of functions",
        "Variational learning with continuous latent variables uses calculus of variations",
        "Variational learning with continuous latent variables removes much of the responsibility from the human designer of the model",
        "Variational learning with continuous latent variables simplifies the required expectations",
        "Variational learning with continuous latent variables uses the opposite direction of the KL divergence compared to maximum likelihood learning",
        "Variational learning with continuous latent variables is computationally efficient"
    ],
    "chunk82": [
        "Binary sparse coding using unrestricted vector of variational parameters z",
        "Relation between sigmoid and softplus: log o(a%) = \u2014\u00a2(\u2014z)",
        "Mean field approximation makes learning tractable",
        "Evidence lower bound (L) equation",
        "Use of fixed point equations to estimate mean field parameters",
        "Advantages of using fixed point equations over gradient descent",
        "Connection between recurrent neural networks and inference in graphical models",
        "Derivation of updates for binary sparse coding model",
        "Heuristic technique called damping for block updates",
        "Introduction to calculus of variations",
        "Optimizing a functional using calculus of variations",
        "Functional derivatives and variational derivatives",
        "Example of finding probability distribution with maximal entropy",
        "Applying calculus of variations to continuous latent variables",
        "General equation for mean field fixed point updates",
        "Example of applying calculus of variations to a simple probabilistic model",
        "Functional form of the optimal solution for the simple probabilistic model"
    ],
    "chunk83": [
        "Approximate inference affects the learning process and the accuracy of the inference algorithm.",
        "Training the parameters using variational learning increases Enxg log p(v, h).",
        "Approximating assumptions become self-fulfilling prophecies.",
        "Estimating log p(v) after training the model can measure the accuracy of the variational approximation.",
        "The true amount of harm induced by the variational approximation is difficult to measure.",
        "Learned approximate inference is an optimization procedure that increases the value of a function.",
        "The optimization process can be approximated with a neural network.",
        "Wake-sleep algorithm resolves the problem of training a model to infer h from v.",
        "Dream sleep provides negative phase samples for Monte Carlo training algorithms.",
        "Other forms of learned inference include single pass inference network, predictive sparse decomposition model, and variational autoencoder.",
        "Boltzmann machines are general connectionist approach to learning probability distributions.",
        "Restricted Boltzmann machines are undirected graphical models with visible and hidden layers.",
        "Deep belief networks are hybrid graphical models involving both directed and undirected connections.",
        "Deep Boltzmann machines are undirected graphical models with multiple layers of latent variables."
    ],
    "chunk84": [
        "Binary version of the restricted Boltzmann machine",
        "Extensions to other types of visible and hidden units",
        "Energy-based model with joint probability distribution specified by energy function",
        "Partition function and its intractability",
        "Conditional distributions P(h | v) and P(v | h)",
        "Training restricted Boltzmann machines",
        "Deep belief networks (DBNs)",
        "Generative models with several layers of latent variables",
        "Intractability of inference and evaluating log-likelihood",
        "Training DBNs with RBMs and generative fine-tuning",
        "Using DBNs for classification",
        "Approximating log-likelihood with AIS",
        "Definition and structure of deep Boltzmann machines (DBMs)",
        "Energy-based model with joint probability distribution specified by energy function",
        "Bipartite graph structure and conditional distributions in DBMs",
        "Efficient Gibbs sampling in DBMs",
        "Interesting properties of DBMs"
    ],
    "chunk85": [
        "DBMs have hidden units that are conditionally independent given the other layers",
        "DBMs capture the influence of top-down feedback interactions",
        "DBMs have been used as computational models of real neuroscientific phenomena",
        "Sampling from DBMs is relatively difficult",
        "DBM Mean Field Inference",
        "Mean field approximation is a simple form of variational inference",
        "Mean field equations capture the bidirectional interactions between layers",
        "Iterative approximate inference procedure for DBMs",
        "DBM Parameter Learning",
        "Learning in DBMs requires dealing with intractable partition function and posterior distribution",
        "Variational inference allows the construction of an approximating distribution",
        "Learning proceeds by maximizing the variational lower bound on the intractable log-likelihood",
        "Layer-Wise Pretraining is a technique for training DBMs",
        "Greedy layer-wise pretraining trains each layer of the DBM as an RBM",
        "Jointly Training Deep Boltzmann Machines",
        "Centered deep Boltzmann machine reparametrizes the model to avoid greedy layer-wise pretraining",
        "Multi-prediction deep Boltzmann machine uses an alternative training criterion",
        "Centering trick for the Boltzmann machine introduces a vector that is subtracted from all states"
    ],
    "chunk86": [
        "Conditioning of the Hessian matrix improves with the centering trick",
        "Centering trick is equivalent to enhanced gradient technique",
        "Improved conditioning of the Hessian matrix allows learning to succeed in difficult cases",
        "Multi-prediction deep Boltzmann machine (MP-DBM) uses mean field equations to solve inference problems",
        "MP-DBM trains the inference network to predict values of remaining units",
        "Back-propagation through the inference graph trains the model with approximate inference",
        "Back-propagation computes the exact gradient of the loss",
        "MP-DBM performs well as a classifier without special modifications",
        "Back-propagating through approximate inference provides a heuristic approximation of the generalized pseudolikelihood",
        "MP-DBM inspired the NADE-k extension to the NADE framework",
        "MP-DBM has connections to dropout",
        "Gaussian-Bernoulli RBMs can be used for real-valued data",
        "Gaussian-Bernoulli RBMs model the conditional distribution of visible units as a Gaussian distribution",
        "Gaussian-Bernoulli RBMs can use a covariance matrix or a precision matrix for the Gaussian distribution",
        "Gaussian-Bernoulli RBMs can include bias terms on the visible units",
        "Undirected models of conditional covariance include mcRBM, mPoT, and ssRBM",
        "mcRBM models the conditional mean and covariance of observed units",
        "mPoT extends the PoT model to include nonzero Gaussian means",
        "ssRBM models the covariance structure of real-valued data using spike and slab hidden units"
    ],
    "chunk87": [
        "Coin is positive definite",
        "Gating by the spike variables",
        "Comparing the ssRBM to the mcRBM and the mPoT models",
        "ssRBM parametrizes the conditional covariance of the observation",
        "ssRBM specifies the conditional covariance of the observations using the hidden spike activations",
        "ssRBM conditional covariance is similar to that given by a different model: the product of probabilistic principal components analysis (PoPPCA)",
        "sparse activations with the ssRBM parametrization permit significant variance only in the selected directions of the sparsely activated h;",
        "mcRBM or mPoT models are less well suited to the overcomplete setting",
        "primary disadvantage of the spike and slab restricted Boltzmann machine is that some settings of the parameters can correspond to a covariance matrix that is not positive definite",
        "qualitatively, convolutional variants of the ssRBM produce excellent samples of natural images",
        "ssRBM allows for several extensions including higher-order interactions and average-pooling of the slab variables",
        "term to the energy function that prevents the partition function from becoming undefined results in a sparse coding model, spike and slab sparse coding",
        "deep convolutional networks usually require a pooling operation",
        "probabilistic max pooling is a solution to the pooling operation in energy-based models",
        "probabilistic max pooling constrains the detector units so at most one may be active at a time",
        "probabilistic max pooling does not support overlapping pooling regions",
        "probabilistic max pooling greatly reduces the performance of convolutional Boltzmann machines",
        "probabilistic max pooling can be used to build convolutional deep Boltzmann machines",
        "traditional convolutional neural networks can use a fixed number of pooling units and dynamically increase the size of their pooling regions",
        "Boltzmann machines have been used for structured or sequential outputs",
        "Boltzmann machines can model conditional distributions for structured output tasks",
        "Boltzmann machines can model conditional distributions for sequence modeling tasks",
        "Boltzmann machines can model joint angles of skeletons used to render 3-D characters",
        "Boltzmann machines can model the distribution over sequences of musical notes used to compose songs",
        "many other variants of Boltzmann machines are possible",
        "Boltzmann machines may be extended with different training criteria",
        "Boltzmann machines can have higher-order interactions in their energy functions",
        "back-propagation through random operations allows for stochastic transformations of input variables",
        "back-propagation through random operations can be achieved using the reparametrization trick",
        "back-propagation through discrete stochastic operations can be achieved using the REINFORCE algorithm"
    ],
    "chunk88": [
        "REINFORCE algorithm for gradient estimation",
        "Variance reduction methods for REINFORCE",
        "Directed graphical models",
        "Sigmoid belief nets",
        "Differentiable generator nets",
        "Variational autoencoders"
    ],
    "chunk89": [
        "VAE models tend to use only a small subset of the dimensions of z",
        "VAEs work well with a diverse family of differentiable operators",
        "DRAW model uses a recurrent encoder and decoder combined with an attention mechanism",
        "VAEs can be extended to generate sequences by defining variational RNNs",
        "Importance weighted autoencoder objective is a lower bound on log pmodei (a@)",
        "Variational autoencoders have connections to the MP-DBM and other approaches that involve back-propagation through the approximate inference graph",
        "Variational autoencoder increases a bound on the log-likelihood of the model",
        "Variational autoencoder learns a predictable coordinate system",
        "Generative adversarial networks (GANs) are another generative modeling approach based on differentiable generator networks",
        "GANs are based on a game theoretic scenario where the generator network competes against an adversary",
        "GANs can fit probability distributions that assign zero probability to the training points",
        "Generative moment matching networks are based on differentiable generator networks and use moment matching to train the generator",
        "Generative moment matching networks can be trained by minimizing the maximum mean discrepancy (MMD) cost function",
        "Convolutional generator networks use the transpose of the convolution operator and un-pooling to generate images",
        "Auto-regressive networks are directed probabilistic models with no latent random variables"
    ],
    "chunk90": [
        "Introduce a form of parameter sharing in deep generative models",
        "Parameter sharing brings statistical and computational advantages",
        "Linear Auto-Regressive Networks have no hidden units and no parameter sharing",
        "Linear Auto-Regressive Networks use linear regression for real-valued data",
        "Linear Auto-Regressive Networks use logistic regression for binary data",
        "Linear Auto-Regressive Networks use softmax regression for discrete data",
        "Neural Auto-Regressive Networks have the same graphical model as logistic auto-regressive networks",
        "Neural Auto-Regressive Networks have a different parametrization of the conditional distributions",
        "Neural Auto-Regressive Networks have increased capacity and improved generalization",
        "Neural Auto-Regressive Networks use a left-to-right connectivity",
        "Neural Auto-Regressive Networks use hidden units that can be reused for predicting subsequent variables",
        "Neural Auto-Regressive Networks use a neural network to estimate conditional probabilities",
        "Neural Auto-Regressive Networks can represent conditional distributions using a neural network",
        "NADE is a form of neural auto-regressive network with additional parameter sharing",
        "NADE shares parameters of hidden units among different groups",
        "NADE uses a particular weight sharing pattern",
        "NADE is similar to mean field inference in RBMs",
        "NADE can be extended to mimic multiple time steps of mean field recurrent inference",
        "RNADE extends NADE to handle continuous-valued data",
        "RNADE uses a Gaussian mixture parametrization",
        "RNADE uses a softmax unit to produce mixture weight probabilities",
        "RNADE uses a pseudo-gradient to handle ill-behaved stochastic gradient descent",
        "Autoencoders learn the data distribution",
        "Variational autoencoders explicitly represent a probability distribution",
        "Most other autoencoders require MCMC sampling",
        "Contractive autoencoders recover an estimate of the tangent plane of the data manifold",
        "Contractive autoencoders induce a random walk along the surface of the manifold",
        "Markov chain can be associated with any denoising autoencoder",
        "Markov chain for denoising autoencoders consists of injecting noise, encoding, decoding, and sampling",
        "Markov chain associated with denoising autoencoders forms a consistent estimator of the data generating distribution",
        "Autoencoders can be used to sample from conditional distributions",
        "Clamping observed units and resampling free units allows sampling from conditional distributions",
        "Detailed balance property should be satisfied for the transition operator of the Markov chain"
    ],
    "chunk91": [
        "Walk-Back Training Procedure",
        "Generative Stochastic Networks",
        "Discriminant GSNs",
        "Other Generation Schemes",
        "Evaluating Generative Models",
        "Conclusion"
    ],
    "chunk92": [
        "Neural machine translation by jointly learning to align and translate",
        "Speech recognition with continuous-parameter hidden Markov models",
        "Neural networks and principal component analysis: Learning from examples without local minima",
        "Exploiting the past and the future in protein secondary structure prediction",
        "Searching for exotic particles in high-energy physics with deep learning",
        "Parallel vision computation",
        "Unsupervised learning",
        "Universal approximation bounds for superpositions of a sigmoidal function",
        "Latent variable models and factor analysis",
        "Statistical Factor Analysis and Related Methods: Theory and Applications",
        "Theano: new features and speed improvements",
        "Teaching classification boundaries to humans",
        "Learning internal representations",
        "Learning stochastic recurrent networks",
        "A self-organizing neural network that discovers surfaces in random-dot stereograms",
        "Learning iterative image reconstruction in the neural abstraction pyramid",
        "VLSI implementations of threshold logic-a comprehensive survey",
        "Laplacian eigenmaps and spectral techniques for embedding and clustering",
        "Laplacian eigenmaps for dimensionality reduction and data representation",
        "Conditional computation in neural networks for faster models",
        "Taking on the curse of dimensionality in joint distributions using neural networks",
        "Scheduled sampling for sequence prediction with recurrent neural networks",
        "Artificial Neural Networks and their Application to Sequence Recognition",
        "Gradient-based optimization of hyperparameters",
        "New distributed probabilistic language models",
        "Learning deep architectures for AI",
        "Deep learning of representations: looking forward",
        "Early inference in energy-based models approximates back-propagation",
        "Modeling high-dimensional discrete data with multi-layer neural networks",
        "Justifying and generalizing contrastive divergence",
        "No unbiased estimator of the variance of k-fold cross-validation",
        "Scaling learning algorithms towards AI",
        "Non-local manifold tangent learning",
        "Quick training of probabilistic neural nets by importance sampling",
        "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
        "Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks",
        "Neural network-Gaussian mixture hybrid for speech recognition or density estimation",
        "The problem of learning long-term dependencies in recurrent networks",
        "Learning long-term dependencies with gradient descent is difficult",
        "Gradient-based learning of hyper-parameters",
        "A neural probabilistic language model",
        "Convex neural networks",
        "The curse of highly variable functions for local kernel machines",
        "Non-local manifold Parzen windows",
        "Greedy layer-wise training of deep networks",
        "Curriculum learning",
        "Better mixing via deep representations",
        "Estimating or propagating gradients through stochastic neurons for conditional computation",
        "Generalized denoising auto-encoders as generative models",
        "Representation learning: A review and new perspectives",
        "Deep generative stochastic networks trainable by backprop",
        "Efficient estimation of free energy differences from Monte Carlo data",
        "The Netflix prize",
        "A maximum entropy approach to natural language processing",
        "Slow, decorrelated features for pretraining complex cell-like networks",
        "Training a 3-node neural network is NP-complete",
        "Alphanets: a recurrent 'neural' network architecture with a hidden Markov model interpretation",
        "Back-propagation fails to separate where perceptrons succeed",
        "Bagging predictors",
        "Classification and Regression Trees",
        "Auto-association by multilayer perceptrons and singular value decomposition",
        "Speech pattern discrimination and multi-layered perceptrons",
        "An empirical study of smoothing techniques for language modeling",
        "Anomaly detection: A survey",
        "Cluster kernels for semi-supervised learning",
        "Semi-Supervised Learning",
        "High Performance Convolutional Neural Networks for Document Processing",
        "On contrastive divergence learning",
        "Training energy-based models for time-series imputation",
        "Charting a manifold",
        "A training algorithm for optimal margin classifiers",
        "Slow feature analysis yields a rich repertoire of complex cell properties",
        "Incorporating Complex Cells into Neural Networks for Pattern Classification",
        "Random search for hyper-parameter optimization",
        "Algorithms for hyper-parameter optimization",
        "Maximin affinity learning of image segmentation",
        "A statistical approach to machine translation",
        "Class-based n-gram models of natural language",
        "Convex Optimization",
        "Algorithms for manifold learning",
        "Training with noise is equivalent to Tikhonov regularization",
        "From machine learning to machine reasoning",
        "Multitask connectionist learning",
        "An empirical study of feature pooling in vision algorithms",
        "Ask the locals: multi-way local pooling for image recognition",
        "Auto-association by multilayer perceptrons and singular value decomposition",
        "A theoretical analysis of feature pooling in vision algorithms",
        "A semantic matching energy function for learning with multi-relational data",
        "Translating embeddings for modeling multi-relational data",
        "Learning temporal dependencies in high-dimensional sequences",
        "Model compression",
        "Training bidirectional Helmholtz machines",
        "Reweighted wake-sleep",
        "Deep maxout neural networks for speech recognition",
        "A small-footprint high-throughput accelerator for ubiquitous machine-learning",
        "DaDianNao: A machine-learning supercomputer",
        "Project Adam: Building an efficient and scalable deep learning training system"
    ],
    "chunk93": [
        "Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines",
        "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
        "On the properties of neural machine translation: Encoder-decoder approaches",
        "The loss surface of multilayer networks",
        "End-to-end continuous speech recognition using attention-based recurrent NN: First results",
        "Automatic Hessians by reverse accumulation",
        "Learning language through pictures",
        "Empirical evaluation of gated recurrent neural networks on sequence modeling",
        "Gated feedback recurrent neural networks",
        "A recurrent latent variable model for sequential data",
        "Multi-column deep neural network for traffic sign classification",
        "Deep big simple neural nets for handwritten digit recognition",
        "The importance of encoding versus training with sparse coding and vector quantization",
        "An analysis of single-layer networks in unsupervised feature learning",
        "Deep learning with COTS HPC systems",
        "On the expressive power of deep learning: A tensor analysis",
        "Large Scale Machine Learning",
        "Deep learning for efficient discriminative parsing",
        "A unified architecture for natural language processing: Deep neural networks with multitask learning",
        "A parallel mixture of SVMs for very large scale problems",
        "Parallel mixture of SVMs for very large scale problems",
        "Natural language processing (almost) from scratch",
        "Torch7: A Matlab-like environment for machine learning",
        "Independent component analysis - a new concept?",
        "Support vector networks",
        "Indoor semantic segmentation using depth information",
        "Low precision arithmetic for deep learning",
        "Unsupervised models of images by spike-and-slab RBMs",
        "Stochastic ratio matching of RBMs for sparse high-dimensional inputs",
        "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
        "The visual microphone: Passive recovery of sound from video",
        "Reinforcement comparison",
        "Varieties of Helmholtz machine",
        "The Helmholtz machine",
        "Large scale distributed deep networks",
        "A model for reasoning about persistence and causation",
        "Indexing by latent semantic analysis",
        "Shallow vs. deep sum-product networks",
        "Improving deep neural networks for LVCSR using rectified linear units and dropout",
        "Multi-task neural networks for QSAR predictions",
        "Learning where to attend with deep architectures for image tracking",
        "Tempered Markov chain Monte Carlo for training of restricted Boltzmann machines",
        "On tracking the partition function",
        "Natural neural networks",
        "Deep generative image models using a Laplacian pyramid of adversarial networks",
        "Mechanisms underlying visual object recognition: Humans vs. neurons vs. machines",
        "NICE: Non-linear independent components estimation",
        "Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data",
        "Learning to generate chairs with convolutional neural networks",
        "Bifurcations of recurrent neural networks in gradient descent learning",
        "The numerical solution of variational problems",
        "Dreyfus, S. E. (1973). The computational solution of optimal control problems with time lag. IEEE Transactions on Automatic Control, 18(4), 383-385.",
        "Drucker, H. and LeCun, Y. (1992). Improving generalisation performance using double back-propagation. IEEE Transactions on Neural Networks, 3(6), 991-997.",
        "Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research.",
        "Dudik, M., Langford, J., and Li, L. (2011). Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine learning, ICML \u201911.",
        "Dugas, C., Bengio, Y., B\u00e9lisle, F., and Nadeau, C. (2001). Incorporating second-order functional knowledge for better option pricing. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13 (NIPS\u201900), pages 472-478. MIT Press.",
        "Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. (2015). Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906.",
        "El Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term dependencies. In NIPS\u20191995.",
        "Elkahky, A. M., Song, Y., and He, X. (2015). A multi-view deep learning approach for cross domain user modeling in recommendation systems. In Proceedings of the 24th International Conference on World Wide Web, pages 278-288.",
        "Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48, 781-799.",
        "Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The difficulty of training deep architectures and the effect of unsupervised pre-training. In Proceedings of AISTATS\u20192009.",
        "Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? J. Machine Learning Res.",
        "Fahlman, S. E., Hinton, G. E., and Sejnowski, T. J. (1983). Massively parallel architectures for AI: NETL, thistle, and Boltzmann machines. In Proceedings of the National Conference on Artificial Intelligence AAAI-83.",
        "Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X., Mitchell, M., Platt, J. C., Zitnick, C. L., and Zweig, G. (2015). From captions to visual concepts and back. arXiv:1411.4952.",
        "Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and Talay, S. (2011). Large-scale FPGA-based convolutional networks. In R. Bekkerman, M. Bilenko, and J. Langford, editors, Scaling up Machine Learning: Parallel and Distributed Approaches.",
        "Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features for scene labeling. JEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1915-1929.",
        "Fei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4), 594-611.",
        "Finn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. (2015). Learning visual feature spaces for robotic manipulation with deep spatial autoencoders. arXiv preprint arXiv:1509.06118.",
        "Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7, 179-188.",
        "Foldiak, P. (1989). Adaptive network for optimal linear feature extraction. In International Joint Conference on Neural Networks (IJCNN), volume 1, pages 401-405, Washington 1989. IEEE, New York.",
        "Franzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place, head-direction, and spatial-view cells.",
        "Franzius, M., Wilbert, N., and Wiskott, L. (2008). Invariant object recognition with slow feature analysis. In Artificial Neural Networks-ICANN 2008, pages 961-970. Springer.",
        "Frasconi, P., Gori, M., and Sperduti, A. (1997). On the efficient classification of data structures by neural networks. In Proc. Int. Joint Conf. on Artificial Intelligence.",
        "Frasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive processing of data structures. [EEE Transactions on Neural Networks, 9(5), 768-786.",
        "Freund, Y. and Schapire, R. E. (1996a). Experiments with a new boosting algorithm. In Machine Learning: Proceedings of Thirteenth International Conference, pages 148-156, USA. ACM.",
        "Freund, Y. and Schapire, R. E. (1996b). Game theory, on-line prediction and boosting. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages 325-332.",
        "Frey, B. J. (1998). Graphical models for machine learning and digital communication. MIT Press.",
        "Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators? In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems 8 (NIPS\u201995), pages 661-670. MIT Press, Cambridge, MA.",
        "Frobenius, G. (1908). Uber matrizen aus positiven elementen, s. B. Preuss. Akad. Wiss. Berlin, Germany.",
        "Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network. Biological Cybernetics, 20, 121-136.",
        "Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193-202.",
        "Gal, Y. and Ghahramani, Z. (2015). Bayesian convolutional neural networks with Bernoulli approximate variational inference. arXiv preprint arXiv:1506.02158.",
        "Gallinari, P., LeCun, Y., Thiria, $., and Fogelman-Soulie, F. (1987). Memoires associatives distribuees. In Proceedings of COGNITIVA 87, Paris, La Villette.",
        "Garcia-Duran, A., Bordes, A., Usunier, N., and Grandvalet, Y. (2015). Combining two and three-way embeddings models for link prediction in knowledge bases. arXiv preprint arXiv:1506.00999.",
        "Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., and Pallett, D. S. (1993). Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon Technical Report N, 93, 27403.",
        "Garson, J. (1900). The metric system of identification of criminals, as used in Great Britain and Ireland. The Journal of the Anthropological Institute of Great Britain and Ireland, (2), 177-227.",
        "Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural computation, 12(10), 2451-2471.",
        "Ghahramani, Z. and Hinton, G. E. (1996). The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, Dpt. of Comp. Sci., Univ. of Toronto.",
        "Gillick, D., Brunk, C., Vinyals, O., and Subramanya, A. (2015). Multilingual language processing from bytes. arXiv preprint arXiv:1512.00108.",
        "Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2015). Region-based convolutional networks for accurate object detection and segmentation.",
        "Giudice, M. D., Manera, V., and Keysers, C. (2009). Programmed to learn? The ontogeny of mirror neurons. Dev. Sci., 12(2), 350\u2014363.",
        "Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In AISTATS\u20192010.",
        "Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In AISTATS\u20192011."
    ],
    "chunk94": [
        "Domain adaptation for large-scale sentiment classification: A deep learning approach",
        "Neighbourhood components analysis",
        "Dynamic Vision: From Images to Face Recognition",
        "Measuring invariances in deep networks",
        "Interfaces for personal robots",
        "Multidimensional, downsampled convolution for autoencoders",
        "Distinguishability criteria for estimating generative models",
        "Spike-and-slab sparse coding for unsupervised feature discovery",
        "Maxout networks",
        "Multi-prediction deep Boltzmann machines",
        "Pylearn2: a machine learning research library",
        "Scaling up spike-and-slab models for unsupervised feature learning",
        "Catastrophic forgetting in gradient-based neural networks",
        "Explaining and harnessing adversarial examples",
        "Generative adversarial networks",
        "Multi-digit number recognition from Street View imagery using deep convolutional neural networks",
        "Qualitatively characterizing neural network optimization problems",
        "Classes for fast maximum entropy training",
        "On the problem of local minima in backpropagation",
        "The probable error of a mean",
        "BilBOWA: Fast bilingual distributed representations without word alignments",
        "Analog electronic neural network circuits",
        "Practical variational inference for neural networks",
        "Supervised Sequence Labelling with Recurrent Neural Networks",
        "Generating sequences with recurrent neural networks",
        "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "Offline handwriting recognition with multidimensional recurrent neural networks",
        "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
        "Unconstrained online handwriting recognition with recurrent neural networks",
        "A novel connectionist system for unconstrained handwriting recognition",
        "Speech recognition with deep recurrent neural networks",
        "Neural Turing machines",
        "Learning to transduce with unbounded memory",
        "LSTM: a search space odyssey",
        "Emergence of complex-like cells in a temporal product network with local receptive fields",
        "Learning fast approximations of sparse coding",
        "Deep autoregressive networks",
        "DRAW: A recurrent neural network for image generation",
        "A kernel two-sample test",
        "Knowledge matters: Importance of prior information for optimization",
        "Classification trees with neural network feature extraction",
        "Deep learning with limited numerical precision",
        "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
        "Online learning for offroad robots: Spatial label propagation to learn long-range traversability",
        "Threshold circuits of bounded depth",
        "Almost optimal lower bounds for small depth circuits",
        "The elements of statistical learning: data mining, inference and prediction",
        "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
        "Connectionist learning procedures",
        "Mapping part-whole hierarchies into connectionist networks",
        "Products of experts",
        "Training products of experts by minimizing contrastive divergence",
        "To recognize shapes, first learn to generate images",
        "How to do backpropagation in a brain",
        "Learning multiple layers of representation",
        "A practical guide to training restricted Boltzmann machines",
        "Generative models for discovering sparse distributed representations",
        "Learning representations by recirculation",
        "Stochastic neighbor embedding",
        "Reducing the dimensionality of data with neural networks",
        "Learning and relearning in Boltzmann machines",
        "Unsupervised learning: foundations of neural computation",
        "Lesioning an attractor network: investigations of acquired dyslexia",
        "Autoencoders, minimum description length, and Helmholtz free energy",
        "Boltzmann machines: Constraint satisfaction networks that learn",
        "Distributed representations",
        "Recognizing handwritten digits using mixtures of linear models",
        "The wake-sleep algorithm for unsupervised neural networks",
        "Modelling the manifolds of images of handwritten digits",
        "A new view of ICA",
        "A fast learning algorithm for deep belief nets",
        "Deep neural networks for acoustic modeling in speech recognition",
        "Improving neural networks by preventing co-adaptation of feature detectors",
        "Dark knowledge",
        "Untersuchungen zu dynamischen neuronalen Netzen",
        "Simplifying neural nets by discovering flat minima",
        "Long short-term memory",
        "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
        "Finite precision error analysis of neural network hardware implementations",
        "Back propagation simulations using limited precision calculations",
        "Multilayer feedforward networks are universal approximators",
        "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks",
        "Behind Deep Blue: Building the Computer That Defeated the World Chess Champion",
        "Generalized pseudo-likelihood estimates for Markov random fields on lattice",
        "Learning deep structured semantic models for web search using clickthrough data",
        "Receptive fields and functional architecture of monkey striate cortex",
        "Receptive fields of single neurons in the cat's striate cortex",
        "Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex",
        "How (not) to train your generative model: schedule sampling, likelihood, adversary?",
        "Sequential model-based optimization for general algorithm configuration",
        "Turing machines are recurrent neural networks",
        "Survey on independent component analysis",
        "Estimation of non-normalized statistical models using score matching",
        "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables",
        "Some extensions of score matching",
        "Emergence of topography and complex cell properties from natural images using extensions of ica",
        "Nonlinear independent component analysis: Existence and uniqueness results",
        "Independent Component Analysis",
        "Topographic independent component analysis",
        "Natural Image Statistics: A probabilistic approach to early computational vision",
        "Extended ensemble Monte Carlo"
    ],
    "chunk95": [
        "Improved generalization by adding both auto-association and hidden-layer noise to neural-network-based-classifiers",
        "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "Increased rates of convergence through learning rate adaptation",
        "Adaptive mixtures of local experts",
        "Adaptive nonlinear system identification with echo state networks",
        "Discovering multiscale dynamical features with hierarchical echo state networks",
        "Echo state network",
        "Long short-term memory in echo state networks: Details of a simulation study",
        "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
        "Optimization and applications of echo state networks with leaky- integrator neurons",
        "Supervised learning of image restoration with convolutional networks",
        "Learning a better representation of speech soundwaves using restricted Boltzmann machines",
        "Vocal tract length perturbation (VTLP) improves speech recognition",
        "What is the best multi-stage architecture for object recognition?",
        "Nonequilibrium equality for free energy differences",
        "Probability Theory: The Logic of Science",
        "On using very large target vocabulary for neural machine translation",
        "Interpolated estimation of Markov source parameters from sparse data",
        "Caffe: An open source convolutional architecture for fast feature embedding",
        "Beyond spatial pyramids: Receptive field learning for pooled image features",
        "An analysis of noise in recurrent neural networks: convergence and generalization",
        "Learning in Graphical Models",
        "Inferring algorithmic patterns with stack-augmented recurrent nets",
        "An empirical evaluation of recurrent network architectures",
        "Neural Network Design and the Complexity of Learning",
        "Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture",
        "Combining modality specific deep neural networks for emotion recognition in video",
        "Recurrent continuous translation models",
        "Grid long short-term memory",
        "The potential energy of an autoencoder",
        "Deep visual-semantic alignments for generating image descriptions",
        "Large-scale video classification with convolutional neural networks",
        "Minima of Functions of Several Variables with Inequalities as Side Constraints",
        "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
        "Fast inference in sparse coding algorithms with applications to object recognition",
        "Learning invariant features through topographic filter maps",
        "Learning convolutional feature hierarchies for visual recognition",
        "Gradient theory of optimal flight paths",
        "How do humans teach: On curriculum learning and teaching dimension",
        "A highly scalable restricted Boltzmann machine FPGA implementation",
        "Markov Random Fields and Their Applications",
        "Adam: A method for stochastic optimization",
        "Regularized estimation of image statistics by score matching",
        "Semi-supervised learning with deep generative models",
        "Fast gradient-based inference with continuous latent variable models in auxiliary form",
        "Auto-encoding variational bayes",
        "Efficient gradient-based inference through transformations between bayes nets and neural nets",
        "Optimization by simulated annealing",
        "Multimodal neural language models",
        "Unifying visual-semantic embeddings with multimodal neural language models",
        "Inducing crosslingual distributed representations of words",
        "Deep learning for the connectome",
        "Probabilistic Graphical Models: Principles and Techniques",
        "REMAP: Recursive estimation and maximization of a posteriori probabilities \u2014 application to transition-based connectionist speech recognition",
        "The BellKor solution to the Netflix grand prize",
        "From group to individual labels using deep features",
        "A clockwork RNN",
        "Learning Bilingual Word Representations by Marginalizing Alignments",
        "Convolutional deep belief networks on CIFAR-10",
        "Learning multiple layers of features from tiny images",
        "Using very deep autoencoders for content-based image retrieval",
        "ImageNet classification with deep convolutional neural networks",
        "Lappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear independent component analysis using ensemble learning: Experiments and discussion.",
        "Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines.",
        "Larochelle, H. and Hinton, G. E. (2010). Learning to combine foveal glimpses with a third-order Boltzmann machine.",
        "Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator.",
        "Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks.",
        "Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009). Exploring strategies for training deep neural networks.",
        "Lasserre, J. A., Bishop, C. M., and Minka, T. P. (2006). Principled hybrids of generative and discriminative models.",
        "Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010). Tiled convolutional neural networks.",
        "Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. (2011). On optimization methods for deep learning.",
        "Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. (2012). Building high-level features using large scale unsupervised learning.",
        "Le Roux, N. and Bengio, Y. (2008). Representational power of restricted Boltzmann machines and deep belief networks.",
        "Le Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approximators.",
        "LeCun, Y. (1985). Une proc\u00e9dure d\u2019apprentissage pour R\u00e9seau a seuil assym\u00e9trique.",
        "LeCun, Y. (1986). Learning processes in an asymmetric threshold network.",
        "LeCun, Y. (1987). Mod\u00e9les connexionistes de V\u2019apprentissage.",
        "LeCun, Y. (1989). Generalization and network design strategies.",
        "LeCun, Y., Jackel, L. D., Boser, B., Denker, J. S., Graf, H. P., Guyon, I., Henderson, D., Howard, R. E., and Hubbard, W. (1989). Handwritten digit recognition: Applications of neural network chips and automatic learning.",
        "LeCun, Y., Bottou, L., Orr, G. B., and Miiller, K.-R. (1998a). Efficient backprop.",
        "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient based learning applied to document recognition.",
        "LeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010). Convolutional networks and applications in vision.",
        "L\u2019Ecuyer, P. (1994). Efficiency improvement and variance reduction.",
        "Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2014). Deeply-supervised nets.",
        "Lee, H., Battle, A., Raina, R., and Ng, A. (2007). Efficient sparse coding algorithms.",
        "Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area V2.",
        "Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.",
        "Lee, Y. J. and Grauman, K. (2011). Learning the easy things first: self-paced visual category discovery.",
        "Leibniz, G. W. (1676). Memoir using the chain rule.",
        "Lenat, D. B. and Guha, R. V. (1989). Building large knowledge-based systems; representation and inference in the Cyc project.",
        "Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. (1993). Multilayer feedforward networks with a nonpolynomial activation function can approximate any function.",
        "Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares.",
        "L\u2019H6pital, G. F. A. (1696). Analyse des infiniment petits, pour Vintelligence des lignes courbes.",
        "Li, Y., Swersky, K., and Zemel, R. S. (2015). Generative moment matching networks.",
        "Lin, T., Horne, B. G., Tino, P., and Giles, C. L. (1996). Learning long-term dependencies is not as difficult with NARX recurrent neural networks.",
        "Lin, Y., Liu, Z., Sun, M., Liu, Y., and Zhu, X. (2015). Learning entity and relation embeddings for knowledge graph completion.",
        "Linde, N. (1992). The machine that changed the world, episode 3.",
        "Lindsey, C. and Lindblad, T. (1994). Review of hardware neural networks: a user\u2019s perspective.",
        "Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error.",
        "LISA (2008). Deep learning tutorials: Restricted Boltzmann machines.",
        "Long, P. M. and Servedio, R. A. (2010). Restricted Boltzmann machines are hard to approximately evaluate or simulate.",
        "Lotter, W., Kreiman, G., and Cox, D. (2015). Unsupervised learning of visual structure using predictive generative networks.",
        "Lovelace, A. (1842). Notes upon L. F. Menabrea\u2019s \u201cSketch of the Analytical Engine invented by Charles Babbage\u201d.",
        "Lu, L., Zhang, X., Cho, K., and Renals, S. (2015). A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition.",
        "Lu, T., Pal, D., and Pal, M. (2010). Contextual multi-armed bandits.",
        "Luenberger, D. G. (1984). Linear and Nonlinear Programming.",
        "LukoSevicius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent neural network training."
    ],
    "chunk96": [
        "Learning class-relevant features and class-irrelevant features via a hybrid third-order RBM",
        "Texture modeling with convolutional spike-and-slab RBMs and deep extensions",
        "Interpretation and generalization of score matching",
        "Deep neural nets as a method for quantitative structure \u2014 activity relationships",
        "Rectifier nonlinearities improve neural network acoustic models",
        "Bounds for the computational power and learning complexity of analog neural nets",
        "A comparison of the computational power of sigmoid and Boolean threshold circuits",
        "Real-time computing without stable states: A new framework for neural computation based on perturbations",
        "Information Theory, Inference and Learning Algorithms",
        "Gradient-based hyperparameter optimization through reversible learning",
        "Deep captioning with multimodal recurrent neural networks",
        "Novel approaches to the discrimination problem",
        "Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood",
        "Inductive principles for restricted Boltzmann machine learning",
        "An algorithm for least-squares estimation of non-linear parameters",
        "Cooperative computation of stereo disparity",
        "Deep learning via Hessian-free optimization",
        "On the expressive efficiency of sum product networks",
        "Learning recurrent neural networks with Hessian-free optimization",
        "Consistency of the maximum pseudo-likelihood estimator of continuous state space Gibbsian processes",
        "The appeal of parallel distributed processing",
        "A logical calculus of ideas immanent in nervous activity",
        "Analog VLSI implementation of neural systems",
        "How to center binary deep Boltzmann machines",
        "Unsupervised learning of image transformations",
        "Learning to represent spatial transformations with factored higher-order Boltzmann machines",
        "Unsupervised and transfer learning challenge: a deep learning approach",
        "Surfing on the manifold",
        "Natural language processing with modular PDP networks and distributed lexicon",
        "Statistical Language Models based on Neural Networks",
        "Empirical evaluation and combination of advanced language modeling techniques",
        "Strategies for training large scale neural network language models",
        "Efficient estimation of word representations in vector space",
        "Exploiting similarities among languages for machine translation",
        "Divergence measures and message passing",
        "Perceptrons",
        "Conditional generative adversarial nets",
        "All you need is a good init",
        "Artificial neural networks in hardware: A survey of two decades of progress",
        "Machine Learning",
        "Distributional smoothing with virtual adversarial training",
        "Neural variational inference and learning in belief networks",
        "Three new graphical models for statistical language modelling",
        "A scalable hierarchical distributed language model",
        "Learning word embeddings efficiently with noise-contrastive estimation",
        "A fast and simple algorithm for training neural probabilistic language models",
        "Learning to detect roads in high-resolution aerial images",
        "Conditional restricted Boltzmann machines for structure output prediction",
        "Playing Atari with deep reinforcement learning",
        "Recurrent models of visual attention",
        "Human-level control through deep reinforcement learning",
        "A theoretical analysis of optimization by Gaussian continuation",
        "Deep learning from temporal coherence in video",
        "Deep belief networks for phone recognition",
        "Deep belief networks using discriminative features for phone recognition",
        "Acoustic modeling using deep belief networks",
        "Understanding how deep belief networks perform acoustic modelling",
        "A scaled conjugate gradient algorithm for fast supervised learning",
        "Deep Boltzmann machines and the centering trick",
        "Universal approximation depth and errors of narrow belief networks with discrete units",
        "Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines",
        "On the number of linear regions of deep neural networks",
        "The induction of multiscale temporal structure",
        "J. M. S. Hanson and R. Lippmann edited Advances in Neural Information Processing Systems 4 (NIPS\u201991), published by Morgan Kaufmann.",
        "K. P. Murphy wrote the book Machine Learning: a Probabilistic Perspective, published by MIT Press.",
        "B. U. I. Aurray and H. Larochelle presented a paper on A deep and tractable density estimator at ICML\u20192014.",
        "V. Nair and G. Hinton published a paper on how Rectified linear units improve restricted Boltzmann machines at ICML\u20192010.",
        "V. Nair and G. E. Hinton published a paper on 3d object recognition with deep belief nets in Advances in Neural Information Processing Systems 22.",
        "H. Narayanan and S. Mitter presented a paper on the sample complexity of testing the manifold hypothesis at NIPS\u20192010.",
        "U. Naumann published a paper on optimal Jacobian accumulation in Mathematical Programming.",
        "R. Navigli and P. Velardi published a paper on structural semantic interconnections and word sense disambiguation in IEEE Trans. Pattern Analysis and Machine Intelligence.",
        "R. Neal and G. Hinton published a paper on a view of the EM algorithm that justifies incremental, sparse, and other variants in the book Learning in Graphical Models.",
        "R. Neal published a technical report on learning stochastic feedforward networks.",
        "R. Neal published a technical report on probabilistic inference using Markov chain Monte-Carlo methods.",
        "R. Neal published a technical report on sampling from multimodal distributions using tempered transitions.",
        "R. Neal published a book on Bayesian Learning for Neural Networks.",
        "R. Neal published a paper on annealed importance sampling in Statistics and Computing.",
        "R. Neal published a paper on estimating ratios of normalizing constants using linked importance sampling.",
        "Y. Nesterov published a paper on a method of solving a convex programming problem with convergence rate O(1/k?) in Soviet Mathematics Doklady.",
        "Y. Nesterov published a book on introductory lectures on convex optimization.",
        "Y. Netzer et al. presented a paper on reading digits in natural images with unsupervised feature learning at the Deep Learning and Unsupervised Feature Learning Workshop.",
        "H. Ney and R. Kneser presented a paper on improved clustering techniques for class-based statistical language modelling at the European Conference on Speech Communication and Technology.",
        "A. Ng wrote a document titled Advice for applying machine learning.",
        "T. R. Niesler et al. presented a paper on comparison of part-of-speech and automatically derived category-based language models for speech recognition at the International Conference on Acoustics, Speech and Signal Processing.",
        "F. Ning et al. presented a paper on automatic phenotyping of developing embryos from videos in IEEE Transactions on Image Processing.",
        "J. Nocedal and S. Wright wrote the book Numerical Optimization.",
        "M. Norouzi and D. J. Fleet presented a paper on minimal loss hashing for compact binary codes at ICML\u20192011.",
        "S. J. Nowlan published a technical report on competing experts: An experimental investigation of associative mixture models.",
        "S. J. Nowlan and G. E. Hinton published a paper on simplifying neural networks by soft weight-sharing in Neural Computation.",
        "B. Olshausen and D. J. Field published a paper on how close we are to understanding V1 in Neural Computation.",
        "B. A. Olshausen and D. J. Field published a paper on the emergence of simple-cell receptive field properties by learning a sparse code for natural images in Nature.",
        "B. A. Olshausen et al. published a paper on a neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information in J. Neurosci.",
        "M. Opper and C. Archambeau published a paper on the variational Gaussian approximation revisited in Neural Computation.",
        "M. Oquab et al. presented a paper on learning and transferring mid-level image representations using convolutional neural networks at the IEEE Conference on Computer Vision and Pattern Recognition.",
        "S. Osindero and G. E. Hinton presented a paper on modeling image patches with a directed hierarchy of Markov random fields in Advances in Neural Information Processing Systems 20.",
        "Ovid and C. Martin wrote the book Metamorphoses.",
        "A. Paccanaro and G. E. Hinton presented a paper on extracting distributed representations of concepts and relations from positive and negative propositions at the International Joint Conference on Neural Networks.",
        "T. L. Paine et al. published an analysis of unsupervised pre-training in light of recent advances.",
        "M. Palatucci et al. presented a paper on zero-shot learning with semantic output codes in Advances in Neural Information Processing Systems 22.",
        "D. B. Parker published a technical report on learning-logic.",
        "R. Pascanu et al. published a paper on the difficulty of training recurrent neural networks at ICML\u20192013.",
        "R. Pascanu et al. published a paper on how to construct deep recurrent neural networks at ICLR\u20192014.",
        "R. Pascanu et al. published a paper on the number of inference regions of deep feed forward networks with piece-wise linear activations at ICLR\u20192014.",
        "Y. Pati et al. presented a paper on orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition at the Annual Asilomar Conference on Signals, Systems, and Computers.",
        "J. Pearl presented a paper on Bayesian networks: A model of self-activated memory for evidential reasoning at the Conference of the Cognitive Science Society.",
        "J. Pearl wrote the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.",
        "O. Perron published a paper on Zur theorie der matrices in Mathematische Annalen.",
        "K. B. Petersen and M. S. Pedersen wrote the Matrix Cookbook.",
        "G. B. Peterson published a paper on B. F. Skinner\u2019s discovery of shaping in the Journal of the Experimental Analysis of Behavior.",
        "D.-T. Pham et al. presented a paper on separation of a mixture of independent sources through a maximum likelihood approach at EUSIPCO."
    ],
    "chunk97": [
        "NeuFlow: dataflow vision processing system-on-a-chip",
        "Recurrent convolutional neural networks for scene labeling",
        "From image-level to pixel-level labeling with convolutional networks",
        "Why is real-world visual object recognition hard?",
        "Scaling up biologically-inspired computer vision: A case study in unconstrained face recognition on facebook",
        "Recursive distributed representations",
        "Acceleration of stochastic approximation by averaging",
        "Some methods of speeding up the convergence of iteration methods",
        "Analyzing noise in autoencoders and deep networks",
        "Sum-product networks: A new deep architecture",
        "A fixed point implementation of the backpropagation learning algorithm",
        "A useful theorem for nonlinear devices having Gaussian inputs",
        "Invariant visual representation by single neurons in the human brain",
        "Unsupervised representation learning with deep convolutional generative adversarial networks",
        "Iterative neural autoregressive distribution estimator (NADE-k)",
        "Large-scale deep unsupervised learning using graphics processors",
        "Truth and probability",
        "Modeling pixel means and covariances using factorized third-order Boltzmann machines",
        "Efficient learning of sparse representations with an energy-based model",
        "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
        "Sparse feature learning for deep belief networks",
        "Factored 3-way restricted Boltzmann machines for modeling natural images",
        "Generating more realistic images using gated MRFs",
        "Information and the accuracy attainable in the estimation of statistical parameters",
        "Semi-supervised learning with ladder network",
        "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
        "Neuronal adaptation for sampling-based probabilistic inference in perceptual bistability",
        "Stochastic backpropagation and approximate inference in deep generative models",
        "Contractive autoencoders: Explicit invariance during feature extraction",
        "Higher order contractive auto-encoder",
        "The manifold tangent classifier",
        "A generative process for sampling contractive auto-encoders",
        "Reverse correlation in neurophysiology",
        "Independent component analysis: principles and practice",
        "A recurrent error propagation network speech recognition system",
        "Convex analysis",
        "Learning in Markov random fields using tempered transitions",
        "Deep Boltzmann machines",
        "Semantic hashing",
        "Learning a nonlinear embedding by preserving class neighbourhood structure",
        "Using deep belief nets to learn covariance kernels for Gaussian processes",
        "Probabilistic matrix factorization",
        "On the quantitative analysis of deep belief networks",
        "Restricted Boltzmann machines for collaborative filtering",
        "Neural network learning control of robot manipulators using gradually increasing task difficulty",
        "Exploiting tractable substructures in intractable networks",
        "Mean field theory for sigmoid belief networks",
        "The impact of arithmetic representation on implementing mlp-bp on fpgas: A study",
        "On random weights and unsupervised feature learning",
        "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "Unit tests for stochastic optimization",
        "Learning complex, extended sequences using the principle of history compression",
        "Sequential neural text compression",
        "Selfdelimiting neural networks",
        "Learning with kernels: Support vector machines, regularization, optimization, and beyond",
        "Nonlinear component analysis as a kernel eigenvalue problem",
        "Advances in Kernel Methods \u2014 Support Vector Learning",
        "On causal and anticausal learning",
        "On supervised learning from sequential data with applications for speech recognition",
        "Bidirectional recurrent neural networks",
        "Continuous space language models",
        "Continuous space language models for statistical machine translation",
        "Cleaned subset of WMT \u201914 dataset",
        "Training methods for adaptive boosting of neural networks",
        "Connectionist language modeling for large vocabulary continuous speech recognition",
        "Continuous space language models for the IWSLT 2006 task",
        "Conversational speech transcription using context-dependent deep neural networks",
        "Higher-order Boltzmann machines",
        "Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model",
        "Convolutional neural networks applied to house numbers digit classification",
        "Pedestrian detection with unsupervised multi-stage feature learning",
        "Linear Algebra",
        "Computation beyond the Turing limit",
        "Turing computability with neural nets",
        "On the computational power of neural nets",
        "Creating artificial neural networks that generalize",
        "Best practices for convolutional neural networks",
        "Backpropagation without multiplication",
        "Tangent prop - A formalism for specifying selected invariances in an adaptive network",
        "Efficient pattern recognition using a new transformation distance",
        "Transformation invariance in pattern recognition \u2014 tangent distance and tangent propagation",
        "Failure to detect changes to people during a real-world interaction",
        "Very deep convolutional networks for large-scale image recognition",
        "Overtraining, regularization and searching for a minimum, with application to neural networks",
        "Reinforcement today",
        "Information processing in dynamical systems: Foundations of harmony theory",
        "Practical Bayesian optimization of machine learning algorithms",
        "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
        "Parsing natural scenes and natural language with recursive neural networks",
        "Semi-supervised recursive autoencoders for predicting sentiment distributions",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Zero-shot learning through cross-modal transfer",
        "Deep unsupervised learning using nonequilibrium thermodynamics",
        "Learning and selecting features jointly with point-wise gated Boltzmann machines",
        "A system for incremental learning based on algorithmic probability",
        "VC dimension of neural networks",
        "Backpropagation can give rise to spurious local minima even for networks without hidden layers"
    ],
    "chunk98": [
        "Sparkes, B. (1996). The Red and the Black: Studies in Greek Pottery. Routledge. 1",
        "Spitkovsky, V. L, Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog: how \u201cless is more\u201d in unsupervised dependency parsing. In HLT\u201910. 328",
        "Squire, W. and Trapp, G. (1998). Using complex variables to estimate derivatives of real functions. STAM Rev., 40(1), 110\u2014112. 439",
        "Srebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. In Proceedings of the 18th Annual Conference on Learning Theory, pages 545-560. Springer-Verlag. 238",
        "Srivastava, N. (2013). Improving Neural Networks With Dropout. Master\u2019s thesis, U. Toronto. 535",
        "Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann machines. In NJPS\u20192012. 541",
        "Srivastava, N., Salakhutdinov, R. R., and Hinton, G. E. (2013). Modeling documents with deep Boltzmann machines. arXiv preprint arXiv:1309.6865. 663",
        "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15, 1929-1958. 258, 265, 267, 672",
        "Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway networks. arXiv:1505.00387. 326",
        "Steinkrau, D., Simard, P. Y., and Buck, I. (2005). Using GPUs for machine learning algorithms. 2013 12th International Conference on Document Analysis and Recognition, 0, 1115-1119. 445",
        "Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 15 of JMLR Workshop and Conference Proceedings, pages 725-733, Fort Lauderdale. Supplementary material (4 pages) also available. 674, 698",
        "Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015). Weakly supervised memory networks. arXiv preprint arXiv:1503.08895. 418",
        "Supancic, J. and Ramanan, D. (2013). Self-paced learning for long-term tracking. In CVPR\u20192018. 328",
        "Sussillo, D. (2014). Random walks: Training very deep nonlinear feed-forward networks with smart initialization. CoRR, abs/1412.6558. 290, 303, 305, 403",
        "Sutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, Department of computer science, University of Toronto. 406, 413",
        "Sutskever, I. and Hinton, G. E. (2008). Deep narrow sigmoid belief networks are universal approximators. Neural Computation, 20(11), 2629-2636. 693",
        "Sutskever, I. and Tieleman, T. (2010). On the Convergence Properties of Contrastive Divergence. In Y. W. Teh and M. Titterington, editors, Proc. of the International Conference on Artificial Intelligence and Statistics (AISTATS), volume 9, pages 789-795. 612",
        "Sutskever, I., Hinton, G., and Taylor, G. (2009). The recurrent temporal restricted Boltzmann machine. In NIPS\u20192008. 685",
        "Sutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent neural networks. In [CML \u20192011, pages 1017-1024. 477",
        "Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In ICML. 300, 406, 413",
        "Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In NIPS\u20192014, arXiv:1409.8215. 25, 101, 397, 410, 411, 474, 475",
        "Sutton, R. and Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press. 06",
        "Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In NIPS\u20191999, pages 1057\u20141063. MIT Press. 691",
        "Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On autoencoders and score matching for energy based models. In [CML\u20192011. ACM. 513",
        "Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization. arXiv preprint arXiw:1406.38896. 436",
        "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014a). Going deeper with convolutions. Technical report, arXiv:1409.4842. 24, 27, 201, 258, 269, 326, 347",
        "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. (2014b). Intriguing properties of neural networks. CLR, abs/1312.6199. 268, 271",
        "Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2015). Rethinking the Inception Architecture for Computer Vision. ArXiv e-prints. 243, 322",
        "Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). DeepFace: Closing the gap to human-level performance in face verification. In CVPR\u20192014. 100",
        "Tandy, D. W. (1997). Works and Days: A Translation and Commentary for the Social Sciences. University of California Press. 1",
        "Tang, Y. and Eliasmith, C. (2010). Deep networks for robust visual recognition. In Proceedings of the 27th International Conference on Machine Learning, June 21-24, 2010, Haifa, Israel. 241",
        "Tang, Y., Salakhutdinov, R., and Hinton, G. (2012). Deep mixtures of factor analysers. arXiv preprint arXiw:1206.4685. 489",
        "Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion style. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-sitth International Conference on Machine Learning (ICML\u201909), pages 1025-1032, Montreal, Quebec, Canada. ACM. 685",
        "Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In B. Sch\u00e9lkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS\u201906), pages 1345-1352. MIT Press, Cambridge, MA. 685",
        "Teh, Y., Welling, M., Osindero, S., and Hinton, G. E. (2003). Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4, 1235-1260. 491",
        "Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500), 2319-2323. 164, 518, 533",
        "Theis, L., van den Oord, A., and Bethge, M. (2015). A note on the evaluation of generative models. arXiv:1511.01844. 698, 719",
        "Thompson, J., Jain, A., LeCun, Y., and Bregler, C. (2014). Joint training of a convolutional network and a graphical model for human pose estimation. In NIPS\u20192014. 360",
        "Thrun, S. (1995). Learning to play the game of chess. In NIPS\u20191994. 271",
        "Tibshirani, R. J. (1995). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society B, 58, 267-288. 236",
        "Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, Pro- ceedings of the Twenty-fifth International Conference on Machine Learning (ICML\u201908), pages 1064-1071. ACM. 612",
        "Tieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive divergence. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-sixth International Conference on Machine Learning (ICML\u201909), pages 1033-1040. ACM. 614",
        "Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis. Journal of the Royal Statistical Society B, 61(3), 611-622. 491",
        "Torralba, A., Fergus, R., and Weiss, Y. (2008). Small codes and large databases for recognition. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR\u201908), pages 1-8. 525",
        "Touretzky, D. S. and Minton, G. E. (1985). Symbols among the neurons: Details of a connectionist inference architecture. In Proceedings of the 9th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI\u201985, pages 238-243, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. 17",
        "Tu, K. and Honavar, V. (2011). On the utility of curricula in unsupervised learning of probabilistic grammars. In IJCAI\u20192011. 328",
        "Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S. (2010). Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2), 511-538. 360",
        "Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: A simple and general method for semi-supervised learning. In Proc. AC'L\u20192010, pages 384-394. 535",
        "To\u00e9scher, A., Jahrer, M., and Bell, R. M. (2009). The BigChaos solution to the Netflix grand prize. 480",
        "Uria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregressive density-estimator. In NJPS\u20192013. 709, 710",
        "van den Oo\u00e9rd, A., Dieleman, S., and Schrauwen, B. (2013). Deep content-based music recommendation. In NIPS\u20192013. 480",
        "van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-SNE. J. Machine Learning Res., 9. 477, 519",
        "Vanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks on CPUs. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop. 444, 452",
        "Vapnik, V. N. (1982). Estimation of Dependences Based on Empirical Data. Springer-Verlag, Berlin. 114",
        "Vapnik, V. N. (1995). The Nature of Statistical Learning Theory. Springer, New York. 114",
        "Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16, 264-280. 114",
        "Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7). 513, 515, 712",
        "Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In NIPS\u20192002. MIT Press. 520",
        "Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008. 241, 515",
        "Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res., 11. 515",
        "Vincent, P., de Br\u00e9bisson, A., and Bouthillier, X. (2015). Efficient exact gradient update for training deep networks with very large sparse targets. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1108-1116. Curran Associates, Inc. 466",
        "Vinyals, O., Kaiser, L., Koo, T., Petrov, $., Sutskever, I., and Hinton, G. (2014a). Grammar as a foreign language. Technical report, arXiv:1412.7449. 410",
        "Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014b). Show and tell: a neural image caption generator. arXiv 1411.4555. 410",
        "Vinyals, O., Fortunato, M., and Jaitly, N. (2015a). Pointer networks. arXiv preprint arXiv:1506.03134. 418",
        "Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b). Show and tell: a neural image caption generator. In CVPR\u20192015. arXiv:1411.4555. 102",
        "Viola, P. and Jones, M. (2001). Robust real-time object detection. In International Journal of Computer Vision. 449",
        "Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., and Bengio, Y. (2015). ReNet: A recurrent neural network based alternative to convolutional networks. arXiv preprint arXiv:1505.00393. 395",
        "Von Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by retinal projections directed to the auditory pathway. Nature, 404(6780), 871-876. 16",
        "Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems 26, pages 351-359. 265",
        "Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. (1989). Phoneme recognition using time-delay neural networks. [EEE Transactions on Acoustics, Speech, and Signal Processing, 37, 328-339. 374, 453, 459",
        "Wan, L., Zeiler, M., Zhang, $., LeCun, Y., and Fergus, R. (2013). Regularization of neural networks using dropconnect. In [CML\u20192013. 266",
        "Wang, S. and Manning, C. (2013). Fast dropout training. In ICML\u20192013. 266",
        "Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014a). Knowledge graph and text jointly embedding. In Proc. EMNLP\u20192014. 484",
        "Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014b). Knowledge graph embedding by translating on hyperplanes. In Proc. AAAI\u20192014. 484",
        "Warde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. (2014). An empirical analysis of dropout in piecewise linear networks. In ICLR\u20192014. 262, 266, 267",
        "Wawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N. (1996). Spert-II: A vector microprocessor system. Computer, 29(3), 79-86. 451",
        "Weaver, L. and Tao, N. (2001). The optimal reward baseline for gradient-based reinforcement learning. In Proc. UAI\u20192001, pages 538-545. 691",
        "Weinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of image manifolds by semidefinite programming. In CVPR\u20192004, pages 988-995. 164, 519",
        "Weiss, Y., Torralba, A., and Fergus, R. (2008). Spectral hashing. In NIPS, pages 1753-1760. 525",
        "Welling, M., Zemel, R. S., and Hinton, G. E. (2002). Self supervised boosting. In Advances in Neural Information Processing Systems, pages 665-672. 703",
        "Welling, M., Hinton, G. E., and Osindero, S. (2003a). Learning sparse topographic representations with products of Student-t distributions. In NIPS\u20192002. 680",
        "Welling, M., Zemel, R., and Hinton, G. E. (2003b). Self-supervised boosting. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15 (NIPS\u201902), pages 665-672. MIT Press. 622",
        "Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums with an application to information retrieval. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17 (NIPS\u201904), volume 17, Cambridge, MA. MIT Press. 676",
        "Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC, pages 762-770. 225",
        "Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to rank with joint word-image embeddings. Machine Learning, 81(1), 21-35. 401",
        "Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv preprint arXiv:1410.8916. 418, 485",
        "Widrow, B. and Hoff, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON Convention Record, volume 4, pages 96-104. IRE, New York. 15, 21, 24, 27",
        "Wikipedia (2015). List of animals by number of neurons \u2014 Wikipedia, the free encyclopedia. [Online; accessed 4-March-2015]. 24, 27",
        "Williams, C. K. I. and Agakov, F. V. (2002). Products of Gaussians and Probabilistic Minor Component Analysis. Neural Computation, 14(5), 1169-1182. 682",
        "Williams, C. K. I. and Rasmussen, C. E. (1996). Gaussian processes for regression. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems 8 (NIPS\u201995), pages 514-520. MIT Press, Cambridge, MA. 142",
        "Wi a ams, R. J. (1992). Simple statistical gradient-following algorithms connectionist einforcement learning. Machine Learning, 8, 229-256. 688, 689",
        "K Williams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1, 270-280. 223",
        "Wilson, D. R. and Martinez, T. R. (2003). The general inefficiency of batch training for gradient descent learning. Neural Networks, 16(10), 1429-1451. 279",
        "Wilson, J. R. (1984). Variance reduction techniques for digital simulation. American Journal of Mathematical and Management Sciences, 4(3), 277\u2014312. 690",
        "Wiskott, L. and Sejnowski, T. J. (2002). Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4), 715-770. 494",
        "Wolpert, D. and MacReady, W. (1997). No free lunch theorems for optimization. [EEE Transactions on Evolutionary Computation, 1, 67-82. 293",
        "Wolpert, D. H. (1996). The lack of a priori distinction between learning algorithms. Neural Computation, 8(7), 1341-1390. 116",
        "Wu, R., Yan, S., Shan, Y., Dang, Q., and Sun, G. (2015). Deep image: Scaling up image recognition. arXiv:1501.02876. 447",
        "Wu, Z. (1997). Global continuation for distance geometry problems. SIAM Journal of Optimization, 7, 814-836. 327",
        "Xiong, H. Y., Barash, Y., and Frey, B. J. (2011). Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context. Bioinformatics, 2'7(18), 2554-2562. 265",
        "Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. In ICML\u20192015, arXiv:1502.03044. 102, 410, 691",
        "Yildiz, I. B., Jaeger, H., and Kiebel, S. J. (2012). Re-visiting the echo state property. Neural networks, 35, 1-9. 405",
        "Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In NIPS\u20192014. 325, 536",
        "Younes, L. (1998). On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates. In Stochastics and Stochastics Models, pages 177-228. 612",
        "Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields. IEEE Journal of Selected Topics in Signal Processing. 323",
        "Zaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv 1410.4615. 329",
        "Zaremba, W. and Sutskever, I. (2015). Reinforcement learning neural Turing machines. arXiv:1505.00521. 419",
        "Zaslavsky, T. (1975). Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes. Number no. 154 in Memoirs of the American Mathematical Society. American Mathematical Society. 550",
        "Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks. In ECCV\u201914. 6",
        "Zeiler, M. D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior, A., Vanhoucke, V., Dean, J., and Hinton, G. E. (2013). On rectified linear units for speech processing. In ICASSP 2018. 460",
        "Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors emerge in deep scene CNNs. ICLR\u20192015, arXiv:1412.6856. 551",
        "Zhou, J. and Troyanskaya, O. G. (2014). Deep supervised and convolutional generative stochastic network for protein secondary structure prediction. In ICML\u20192014. 715",
        "Zhou, Y. and Chellappa, R. (1988). Computation of optical flow using a neural network. In Neural Networks, 1988., IEEE International Conference on, pages 71-78. IEEE. 339",
        "Zohrer, M. and Pernkopf, F. (2014). General stochastic networks for classification. In NIPS\u20192014. 716"
    ],
    "chunk99": [
        "0-1 loss",
        "Absolute value rectification",
        "Accuracy",
        "Activation function",
        "Active constraint",
        "AdaGrad",
        "ADALINE",
        "Adam",
        "Adaptive linear element",
        "Adversarial example",
        "Adversarial training",
        "Affine",
        "AIS",
        "Almost everywhere",
        "Almost sure convergence",
        "Ancestral sampling",
        "ANN",
        "Annealed importance sampling",
        "Approximate Bayesian computation",
        "Approximate inference",
        "Artificial intelligence",
        "Artificial neural network",
        "ASR",
        "Asymptotically unbiased",
        "Audio",
        "Autoencoder",
        "Automatic speech recognition",
        "Back-propagation",
        "Back-propagation through time",
        "Backprop",
        "Bag of words",
        "Bagging",
        "Batch normalization",
        "Bayes error",
        "Bayes\u2019 rule",
        "Bayesian hyperparameter optimization",
        "Bayesian network",
        "Bayesian probability",
        "Bayesian statistics",
        "Belief network",
        "Bernoulli distribution",
        "BFGS",
        "Bias",
        "Bias parameter",
        "Biased importance sampling",
        "Bigram",
        "Binary relation",
        "Block Gibbs sampling",
        "Boltzmann distribution",
        "Boltzmann machine",
        "BPTT",
        "Broadcasting",
        "Burn-in",
        "CAE",
        "Calculus of variations",
        "Categorical distribution",
        "CD",
        "Centering trick (DBM)",
        "Central limit theorem",
        "Chain rule (calculus)",
        "Chain rule of probability",
        "Chess",
        "Chord",
        "Chordal graph",
        "Class-based language models",
        "Classical dynamical system",
        "Classification",
        "Clique potential",
        "CNN",
        "Collaborative Filtering",
        "Collider",
        "Color images",
        "Complex cell",
        "Computational graph",
        "Computer vision",
        "Concept drift",
        "Condition number",
        "Conditional computation",
        "Conditional independence",
        "Conditional probability",
        "Conditional RBM",
        "Connectionism",
        "Connectionist temporal classification",
        "Consistency",
        "Constrained optimization",
        "Content-based addressing",
        "Content-based recommender systems",
        "Context-specific independence",
        "Contextual bandits",
        "Continuation methods",
        "Contractive autoencoder",
        "Contrast",
        "Contrastive divergence",
        "Convex optimization",
        "Convolution",
        "Convolutional network",
        "Convolutional neural network",
        "Coordinate descent",
        "Correlation",
        "Cost function",
        "Covariance",
        "Covariance matrix",
        "Coverage",
        "Critical temperature",
        "Cross-correlation",
        "Cross-entropy",
        "Cross-validation",
        "CTC",
        "Curriculum learning",
        "Curse of dimensionality",
        "Cyc",
        "D-separation",
        "DAE",
        "Data generating distribution",
        "Data generating process",
        "Data parallelism",
        "Dataset",
        "Dataset augmentation",
        "DBM",
        "DCGAN",
        "Decision tree",
        "Decoder",
        "Deep belief network",
        "Deep Blue",
        "Deep Boltzmann machine",
        "Deep feedforward network",
        "learning",
        "2",
        "5",
        "Denoising autoencoder",
        "506",
        "683",
        "Denoising score matching",
        "615",
        "Density estimation",
        "102",
        "Derivative",
        "xiii",
        "82",
        "Design matrix",
        "105",
        "Detector layer",
        "336",
        "Determinant",
        "xii",
        "Diagonal matrix",
        "40",
        "Differential entropy",
        "73",
        "641",
        "Dirac delta function",
        "64",
        "Directed graphical model",
        "76",
        "503",
        "559",
        "685",
        "Directional derivative",
        "84",
        "Discriminative fine-tuning",
        "see supervised fine-tuning",
        "Discriminative RBM",
        "680",
        "Distributed representation",
        "17",
        "149",
        "542",
        "Domain adaptation",
        "532",
        "779",
        "INDEX",
        "Dot product",
        "33",
        "139",
        "Double backprop",
        "268",
        "Doubly block circulant matrix",
        "330",
        "Dream sleep",
        "605",
        "647",
        "DropConnect",
        "263",
        "Dropout",
        "255",
        "422",
        "427",
        "428",
        "666",
        "683",
        "Dynamic structure",
        "445",
        "E-step",
        "629",
        "Early stopping",
        "244",
        "246",
        "270",
        "271",
        "422",
        "EBM",
        "see energy-based model",
        "Echo state network",
        "23",
        "26",
        "401",
        "Effective capacity",
        "113",
        "Eigendecomposition",
        "41",
        "Eigenvalue",
        "41",
        "Eigenvector",
        "41",
        "ELBO",
        "see evidence lower bound",
        "Element-wise product",
        "see Hadamard product",
        "EM",
        "see expectation maximization",
        "Embedding",
        "512",
        "Empirical distribution",
        "65",
        "Empirical risk",
        "274",
        "Empirical risk minimization",
        "274",
        "Encoder",
        "4",
        "Energy function",
        "565",
        "Energy-based model",
        "565",
        "591",
        "648",
        "657",
        "Ensemble methods",
        "252",
        "Epoch",
        "244",
        "Equality constraint",
        "93",
        "Equivariance",
        "335",
        "Error function",
        "see objective function",
        "ESN",
        "see echo state network",
        "Euclidean norm",
        "38",
        "Euler-Lagrange equation",
        "641",
        "Evidence lower bound",
        "628",
        "655",
        "Example",
        "98",
        "Expectation",
        "59",
        "Expectation maximization",
        "629",
        "Expected value",
        "see expectation",
        "Explaining away",
        "570",
        "626",
        "639",
        "Exploitation",
        "477",
        "Exploration",
        "477",
        "Exponential distribution",
        "64",
        "F-score",
        "420",
        "Factor (graphical model)",
        "563",
        "Factor analysis",
        "486",
        "Factor graph",
        "575",
        "Factors of variation",
        "4",
        "Feature",
        "98",
        "Feature selection",
        "234",
        "Feedforward neural network",
        "166",
        "Fine-tuning",
        "321",
        "Finite differences",
        "436",
        "Forget gate",
        "304",
        "Forward propagation",
        "201",
        "Fourier transform",
        "357",
        "359",
        "Fovea",
        "363",
        "FPCD",
        "610",
        "Free energy",
        "567",
        "674",
        "Freebase",
        "479",
        "Frequentist probability",
        "54",
        "Frequentist statistics",
        "134",
        "Frobenius norm",
        "45",
        "Fully-visible Bayes network",
        "699",
        "Functional derivatives",
        "640",
        "FVBN",
        "see fully-visible Bayes network",
        "Gabor function",
        "365",
        "GANs",
        "see generative adversarial networks",
        "Gated recurrent unit",
        "422",
        "Gaussian distribution",
        "see normal distribution",
        "Gaussian kernel",
        "140",
        "Gaussian mixture",
        "66",
        "187",
        "GCN",
        "see global contrast normalization",
        "GeneOntology",
        "479",
        "Generalization",
        "109",
        "General",
        "zed Lagrange function, see general- ized Lagrangian",
        "Generalized Lagrangian, 93",
        "Generative adversarial networks, 683, 693",
        "Generative moment matching networks, 696",
        "Generator network, 687",
        "Gibbs distribution, 564",
        "Gibbs sampling, 577, 595",
        "Global contrast normalization, 451",
        "GPU, see graphics processing unit",
        "Gradient, 83",
        "Gradient clipping, 287, 411",
        "Gradient descent, 82, 84",
        "Graph, xii",
        "Graphical model, see structured probabilis- tic model",
        "Graphics processing unit, 441",
        "Greedy algorithm, 321",
        "Greedy layer-wise unsupervised pretraining, 524",
        "Greedy supervised pretraining, 321",
        "Grid search, 429",
        "Hadamard product, xii, 33",
        "Hard tanh, 195",
        "Harmonium, see restricted Boltzmann ma- chine",
        "Harmony theory, 567",
        "Helmholtz free energy, see evidence lower bound",
        "Hessian, 221",
        "Hessian matrix, xiii, 86",
        "Heteroscedastic, 186",
        "Hidden layer, 6, 166",
        "Hill climbing, 85",
        "Hyperparameter optimization, 429",
        "Hyperparameters, 119, 427",
        "Hypothesis space, 111, 117",
        "iid. assumptions, 110, 121, 265",
        "Identity matrix, 35",
        "ILSVRC, see ImageNet Large Scale Visual Recognition Challenge",
        "ImageNet Large Scale Visual Recognition Challenge, 22",
        "Immorality, 573",
        "Importance sampling, 588, 620, 691",
        "Importance weighted autoencoder, 691",
        "Independence, xiii, 59",
        "Independent and identically distributed, see iid. assumptions",
        "Independent component analysis, 487",
        "Independent subspace analysis, 489",
        "Inequality constraint, 93",
        "Inference, 558, 579, 626, 628, 630, 633, 643, 646",
        "Information retrieval, 520",
        "Initialization, 298",
        "Integral, xiii",
        "Invariance, 339",
        "Isotropic, 64",
        "Jacobian matrix, xiii, 71, 85",
        "Joint probability, 56",
        "k-means, 361, 542",
        "k-nearest neighbors, 141, 544",
        "Karush-Kuhn-Tucker conditions, 94, 235",
        "Karush\u2014Kuhn\u2014Tucker, 93",
        "Kernel (convolution), 328, 329",
        "Kernel machine, 544",
        "Kernel trick, 139",
        "KKT, see Karush\u2014Kuhn\u2014Tucker",
        "KKT conditions, see Karush-Kuhn-Tucker conditions",
        "KL divergence, see Kullback-Leibler divergence",
        "Knowledge base, 2, 479",
        "Krylov methods, 222",
        "Kullback-Leibler divergence, xiii, 73",
        "Label smoothing, 241",
        "Lagrange multipliers, 93, 641",
        "Lagrangian, see generalized Lagrangian",
        "LAPGAN, 695",
        "Laplace distribution, 64, 492",
        "Latent variable, 66",
        "Layer (neural network), 166",
        "LCN, see local contrast normalization",
        "Leaky ReLU, 191",
        "Leaky units, 404",
        "Learning rate, 84",
        "Line search, 84, 85, 92",
        "Linear combination, 36",
        "Linear dependence, 37",
        "Linear factor models, 485",
        "Linear regression, 106, 109, 138",
        "Link prediction, 480",
        "Lipschitz constant, 91",
        "Lipschitz continuous, 91",
        "Liquid state machine, 401",
        "Local conditional probability distribution, 560",
        "Local contrast normalization, 452",
        "Logistic regression, 3, 138, 139",
        "Logistic sigmoid, 7, 66",
        "Long short-term memory, 18, 24, 304, 407, 422",
        "Loop, 575",
        "Loopy belief propagation, 581",
        "Loss function, see objective function",
        "ZL? norm, 38",
        "LSTM, see long short-term memory",
        "[-step, 629",
        "Machine learning, 2",
        "Machine translation, 100",
        "fain diagonal, 32",
        "Manifold, 159",
        "Manifold hypothesis, 160",
        "Manifold learning, 160",
        "Manifold tangent classifier, 268",
        "IAP approximation, 137, 501",
        "Marginal probability, 57",
        "Markov chain, 591",
        "Markov chain Monte Carlo, 591",
        "Markov network, see undirected model",
        "Markov random field, see undirected model",
        "Matrix, xi, xii, 31",
        "Matrix inverse, 35",
        "Matrix product, 33",
        "Jax norm, 39",
        "Jax pooling, 336",
        "Jaximum likelihood, 130",
        "Maxout, 191, 422",
        "CMC, see Markov chain Monte Carlo",
        "lean field, 633, 634, 666",
        "lean squared error, 107",
        "leasure theory, 70",
        "leasure zero, 70",
        "emory network, 413, 415",
        "Aethod of steepest descent, see gradient descent",
        "Minibatch, 277",
        "Jissing inputs, 99",
        "fixing (Markov chain), 597",
        "Jixture density networks, 187",
        "Aixture distribution, 65",
        "Aixture model, 187, 506",
        "ixture of experts, 446, 544",
        "ALP, see multilayer perception",
        "ANIST, 20, 21, 666",
        "Jodel averaging, 252",
        "{odel compression, 444",
        "Nodel identifiability, 282",
        "Nodel parallelism, 444",
        "Joment matching, 696",
        "Joore-Penrose pseudoinverse, 44, 237",
        "Joralized graph, 573",
        "[P-DBM, see multi-prediction DBM",
        "(MRF (Markov Random",
        "Undirected model",
        "Mean squared error",
        "Multimodal learning",
        "Multiprediction DBM",
        "Multitask learning",
        "Multilayer perception",
        "Multilayer perceptron",
        "Multinomial distribution",
        "Multinoulli distribution",
        "N-gram",
        "NADE",
        "Naive Bayes",
        "Nat",
        "Natural image",
        "Natural language processing",
        "Nearest neighbor regression",
        "Negative definite",
        "Negative phase",
        "Neocognitron",
        "Nesterov momentum",
        "Netflix Grand Prize",
        "Neural language model",
        "Neural network",
        "Neural Turing machine",
        "Neuroscience",
        "Newton's method",
        "NLM",
        "NLP",
        "No free lunch theorem",
        "Noise-contrastive estimation",
        "Non-parametric model",
        "Norm",
        "Normal distribution",
        "Normal equations",
        "Normalized initialization",
        "Numerical differentiation",
        "Object detection",
        "Object recognition",
        "Objective function",
        "OMP-k",
        "One-shot learning",
        "Operation",
        "Optimization",
        "Orthodox statistics",
        "Orthogonal matching pursuit",
        "Orthogonal matrix",
        "Orthogonality",
        "Output layer",
        "Parallel distributed processing",
        "Parameter initialization",
        "Parameter sharing",
        "Parameter tying",
        "Parametric model",
        "Parametric ReLU",
        "Partial derivative",
        "Partition function",
        "PCA",
        "PCD",
        "Perceptron",
        "Persistent contrastive divergence",
        "Perturbation analysis",
        "Point estimator",
        "Policy",
        "Pooling",
        "Positive definite",
        "Positive phase",
        "Precision",
        "Precision (of a normal distribution)",
        "Predictive sparse decomposition",
        "Preprocessing",
        "Pretraining",
        "Primary visual cortex",
        "Principal components analysis",
        "Prior probability distribution",
        "Probabilistic max pooling",
        "Probabilistic PCA",
        "Probability density function",
        "Probability distribution",
        "Probability mass function",
        "Probability mass function estimation",
        "Product of experts",
        "Product rule of probability",
        "PSD",
        "Pseudolikelihood",
        "Quadrature pair",
        "Quasi-Newton methods",
        "Radial basis function",
        "Random search",
        "Random variable",
        "Ratio matching",
        "RBF",
        "RBM",
        "Recall",
        "Receptive field",
        "Recommender Systems",
        "Rectified linear unit",
        "Recurrent network",
        "Recurrent neural network",
        "Regression",
        "Regularization",
        "Regularizer",
        "REINFORCE",
        "Reinforcement learning",
        "Relational database",
        "Relations",
        "Reparametrization trick",
        "Representation learning",
        "Representational capacity",
        "Restricted Boltzmann machine",
        "Risk",
        "RNN-RBM",
        "Saddle points",
        "Sample mean",
        "Scalar",
        "Score matching",
        "Second derivative",
        "Second derivative test",
        "Self-information",
        "Semantic hashing",
        "Semi-supervised learning",
        "Separable convolution",
        "Separation (probabilistic modeling)",
        "Set",
        "SGD",
        "Shannon entropy",
        "Shortlist",
        "Sigmoid",
        "Sigmoid belief network",
        "Simple cell",
        "Singular value",
        "Singular value decomposition",
        "Singular vector",
        "Slow feature analysis",
        "SML",
        "Softmax",
        "Softplus",
        "Spam detection",
        "Sparse coding",
        "Sparse initialization",
        "Sparse representation",
        "Spearmint",
        "Spectral radius",
        "Speech recognition",
        "Sphering",
        "Spike and slab restricted Boltzmann machine",
        "SPN",
        "Square matrix",
        "ssRBM",
        "Standard deviation",
        "Standard error",
        "Standard error of the mean",
        "Statistic",
        "Statistical learning theory",
        "Steepest descent",
        "Stochastic back-propagation",
        "Stochastic gradient descent",
        "Stochastic maximum likelihood",
        "Stochastic pooling",
        "Structure learning",
        "Structured output",
        "Structured probabilistic model",
        "Sum rule of probability",
        "Sum-product network",
        "Supervised fine-tuning",
        "Supervised learning",
        "Support vector machine",
        "Surrogate loss function",
        "SVD",
        "Symmetric matrix",
        "Tangent distance",
        "Tangent plane",
        "Tangent prop",
        "TDNN",
        "Teacher forcing",
        "Tempering",
        "Template matching",
        "Tensor",
        "Test set",
        "Tikhonov regularization",
        "Tiled convolution",
        "Time-delay neural network",
        "Toeplitz matrix",
        "Topographic ICA",
        "Trace operator",
        "Training error",
        "Transcription",
        "Transfer learning",
        "Transpose",
        "Triangle inequality",
        "Zero-data learning",
        "Zero-shot learning"
    ]
}