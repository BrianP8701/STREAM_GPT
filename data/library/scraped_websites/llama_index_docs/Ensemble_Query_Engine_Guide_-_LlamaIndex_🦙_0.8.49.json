{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval.html",
        "title": "Ensemble Query Engine Guide - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Ensemble Query Engine Guide[\uf0c1](#ensemble-query-engine-guide \"Permalink to this heading\")\n\nOftentimes when building a RAG applications there are many retreival parameters/strategies to decide from (from chunk size to vector vs. keyword vs. hybrid search, for instance).\n\nThought: what if we could try a bunch of strategies at once, and have any AI/reranker/LLM prune the results?\n\nThis achieves two purposes:\n\n*   Better (albeit more costly) retrieved results by pooling results from multiple strategies, assuming the reranker is good\n    \n*   A way to benchmark different retrieval strategies against each other (w.r.t reranker)\n    \n\nThis guide showcases this over the Llama 2 paper. We do ensemble retrieval over different chunk sizes and also different indices.\n\n**NOTE**: A closely related guide is our [Ensemble Retrievers Guide](https://gpt-index.readthedocs.io/en/stable/examples/retrievers/ensemble_retrieval.html) - make sure to check it out!\n\n%load\\_ext autoreload\n%autoreload 2\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nHere we define the necessary imports.\n\n\\# NOTE: This is ONLY necessary in jupyter notebook.\n\\# Details: Jupyter runs an event-loop behind the scenes.\n\\#          This results in nested event-loops when we start an event-loop to make async queries.\n\\#          This is normally not allowed, we use nest\\_asyncio to allow it for convenience.\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().handlers \\= \\[\\]\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SummaryIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n    StorageContext,\n)\nfrom llama\\_index.response.notebook\\_utils import display\\_response\nfrom llama\\_index.llms import OpenAI\n\nNote: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNumExpr defaulting to 8 threads.\n\n## Load Data[\uf0c1](#load-data \"Permalink to this heading\")\n\nIn this section we first load in the Llama 2 paper as a single document. We then chunk it multiple times, according to different chunk sizes. We build a separate vector index corresponding to each chunk size.\n\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\n\\--2023-09-28 12:56:38--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 128.84.21.199\nConnecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) \\[application/pdf\\]\nSaving to: \u2018data/llama2.pdf\u2019\n\ndata/llama2.pdf     100%\\[===================>\\]  13.03M   521KB/s    in 42s     \n\n2023-09-28 12:57:20 (320 KB/s) - \u2018data/llama2.pdf\u2019 saved \\[13661300/13661300\\]\n\nfrom pathlib import Path\nfrom llama\\_index import Document\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocs0 \\= loader.load(file\\_path\\=Path(\"./data/llama2.pdf\"))\ndoc\\_text \\= \"\\\\n\\\\n\".join(\\[d.get\\_content() for d in docs0\\])\ndocs \\= \\[Document(text\\=doc\\_text)\\]\n\nHere we try out different chunk sizes: 128, 256, 512, and 1024.\n\n\\# initialize service context (set chunk size)\nllm \\= OpenAI(model\\=\"gpt-4\")\nchunk\\_sizes \\= \\[128, 256, 512, 1024\\]\nservice\\_contexts \\= \\[\\]\nnodes\\_list \\= \\[\\]\nvector\\_indices \\= \\[\\]\nquery\\_engines \\= \\[\\]\nfor chunk\\_size in chunk\\_sizes:\n    print(f\"Chunk Size: {chunk\\_size}\")\n    service\\_context \\= ServiceContext.from\\_defaults(\n        chunk\\_size\\=chunk\\_size, llm\\=llm\n    )\n    service\\_contexts.append(service\\_context)\n    nodes \\= service\\_context.node\\_parser.get\\_nodes\\_from\\_documents(docs)\n\n    \\# add chunk size to nodes to track later\n    for node in nodes:\n        node.metadata\\[\"chunk\\_size\"\\] \\= chunk\\_size\n        node.excluded\\_embed\\_metadata\\_keys \\= \\[\"chunk\\_size\"\\]\n        node.excluded\\_llm\\_metadata\\_keys \\= \\[\"chunk\\_size\"\\]\n\n    nodes\\_list.append(nodes)\n\n    \\# build vector index\n    vector\\_index \\= VectorStoreIndex(nodes)\n    vector\\_indices.append(vector\\_index)\n\n    \\# query engines\n    query\\_engines.append(vector\\_index.as\\_query\\_engine())\n\nChunk Size: 128\nChunk Size: 256\nChunk Size: 512\nChunk Size: 1024\n\n## Define Ensemble Retriever[\uf0c1](#define-ensemble-retriever \"Permalink to this heading\")\n\nWe setup an \u201censemble\u201d retriever primarily using our recursive retrieval abstraction. This works like the following:\n\n*   Define a separate `IndexNode` corresponding to the vector retriever for each chunk size (retriever for chunk size 128, retriever for chunk size 256, and more)\n    \n*   Put all IndexNodes into a single `SummaryIndex` - when the corresponding retriever is called, _all_ nodes are returned.\n    \n*   Define a Recursive Retriever, with the root node being the summary index retriever. This will first fetch all nodes from the summary index retriever, and then recursively call the vector retriever for each chunk size.\n    \n*   Rerank the final results.\n    \n\nThe end result is that all vector retrievers are called when a query is run.\n\n\\# try ensemble retrieval\n\nfrom llama\\_index.tools import RetrieverTool\nfrom llama\\_index.schema import IndexNode\n\n\\# retriever\\_tools = \\[\\]\nretriever\\_dict \\= {}\nretriever\\_nodes \\= \\[\\]\nfor chunk\\_size, vector\\_index in zip(chunk\\_sizes, vector\\_indices):\n    node\\_id \\= f\"chunk\\_{chunk\\_size}\"\n    node \\= IndexNode(\n        text\\=(\n            \"Retrieves relevant context from the Llama 2 paper (chunk size\"\n            f\" {chunk\\_size})\"\n        ),\n        index\\_id\\=node\\_id,\n    )\n    retriever\\_nodes.append(node)\n    retriever\\_dict\\[node\\_id\\] \\= vector\\_index.as\\_retriever()\n\nDefine recursive retriever.\n\nfrom llama\\_index.selectors.pydantic\\_selectors import PydanticMultiSelector\n\n\\# from llama\\_index.retrievers import RouterRetriever\nfrom llama\\_index.retrievers import RecursiveRetriever\nfrom llama\\_index import SummaryIndex\n\n\\# the derived retriever will just retrieve all nodes\nsummary\\_index \\= SummaryIndex(retriever\\_nodes)\n\nretriever \\= RecursiveRetriever(\n    root\\_id\\=\"root\",\n    retriever\\_dict\\={\"root\": summary\\_index.as\\_retriever(), \\*\\*retriever\\_dict},\n)\n\nLet\u2019s test the retriever on a sample query.\n\nnodes \\= await retriever.aretrieve(\n    \"Tell me about the main aspects of safety fine-tuning\"\n)\n\nprint(f\"Number of nodes: {len(nodes)}\")\nfor node in nodes:\n    print(node.node.metadata\\[\"chunk\\_size\"\\])\n    print(node.node.get\\_text())\n\nDefine reranker to process the final retrieved set of nodes.\n\n\\# define reranker\nfrom llama\\_index.indices.postprocessor import (\n    LLMRerank,\n    SentenceTransformerRerank,\n    CohereRerank,\n)\n\n\\# reranker = LLMRerank()\n\\# reranker = SentenceTransformerRerank(top\\_n=10)\nreranker \\= CohereRerank(top\\_n\\=10)\n\nDefine retriever query engine to integrate the recursive retriever + reranker together.\n\n\\# define RetrieverQueryEngine\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\nquery\\_engine \\= RetrieverQueryEngine(retriever, node\\_postprocessors\\=\\[reranker\\])\n\nresponse \\= query\\_engine.query(\n    \"Tell me about the main aspects of safety fine-tuning\"\n)\n\ndisplay\\_response(\n    response, show\\_source\\=True, source\\_length\\=500, show\\_source\\_metadata\\=True\n)\n\n### Analyzing the Relative Importance of each Chunk[\uf0c1](#analyzing-the-relative-importance-of-each-chunk \"Permalink to this heading\")\n\nOne interesting property of ensemble-based retrieval is that through reranking, we can actually use the ordering of chunks in the final retrieved set to determine the importance of each chunk size. For instance, if certain chunk sizes are always ranked near the top, then those are probably more relevant to the query.\n\n\\# compute the average precision for each chunk size based on positioning in combined ranking\nfrom collections import defaultdict\nimport pandas as pd\n\ndef mrr\\_all(metadata\\_values, metadata\\_key, source\\_nodes):\n    \\# source nodes is a ranked list\n    \\# go through each value, find out positioning in source\\_nodes\n    value\\_to\\_mrr\\_dict \\= {}\n    for metadata\\_value in metadata\\_values:\n        mrr \\= 0\n        for idx, source\\_node in enumerate(source\\_nodes):\n            if source\\_node.node.metadata\\[metadata\\_key\\] \\== metadata\\_value:\n                mrr \\= 1 / (idx + 1)\n                break\n            else:\n                continue\n\n        \\# normalize AP, set in dict\n        value\\_to\\_mrr\\_dict\\[metadata\\_value\\] \\= mrr\n\n    df \\= pd.DataFrame(value\\_to\\_mrr\\_dict, index\\=\\[\"MRR\"\\])\n    df.style.set\\_caption(\"Mean Reciprocal Rank\")\n    return df\n\n\\# Compute the Mean Reciprocal Rank for each chunk size (higher is better)\n\\# we can see that chunk size of 256 has the highest ranked results.\nprint(\"Mean Reciprocal Rank for each Chunk Size\")\nmrr\\_all(chunk\\_sizes, \"chunk\\_size\", response.source\\_nodes)\n\nMean Reciprocal Rank for each Chunk Size\n\n|     | 128 | 256 | 512 | 1024 |\n| --- | --- | --- | --- | --- |\n| MRR | 0.333333 | 1.0 | 0.5 | 0.25 |\n\n## Evaluation[\uf0c1](#evaluation \"Permalink to this heading\")\n\nWe more rigorously evaluate how well an ensemble retriever works compared to the \u201cbaseline\u201d retriever.\n\nWe define/load an eval benchmark dataset and then run different evaluations over it.\n\n**WARNING**: This can be _expensive_, especially with GPT-4. Use caution and tune the sample size to fit your budget.\n\nfrom llama\\_index.evaluation import (\n    DatasetGenerator,\n    QueryResponseDataset,\n)\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import OpenAI\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\n\\# NOTE: run this if the dataset isn't already saved\neval\\_service\\_context \\= ServiceContext.from\\_defaults(llm\\=OpenAI(model\\=\"gpt-4\"))\n\\# generate questions from the largest chunks (1024)\ndataset\\_generator \\= DatasetGenerator(\n    nodes\\_list\\[\\-1\\],\n    service\\_context\\=eval\\_service\\_context,\n    show\\_progress\\=True,\n    num\\_questions\\_per\\_chunk\\=2,\n)\n\neval\\_dataset \\= await dataset\\_generator.agenerate\\_dataset\\_from\\_nodes(num\\=60)\n\neval\\_dataset.save\\_json(\"data/llama2\\_eval\\_qr\\_dataset.json\")\n\n\\# optional\neval\\_dataset \\= QueryResponseDataset.from\\_json(\n    \"data/llama2\\_eval\\_qr\\_dataset.json\"\n)\n\n### Compare Results[\uf0c1](#compare-results \"Permalink to this heading\")\n\nimport asyncio\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nfrom llama\\_index.evaluation import (\n    CorrectnessEvaluator,\n    SemanticSimilarityEvaluator,\n    RelevancyEvaluator,\n    FaithfulnessEvaluator,\n    PairwiseComparisonEvaluator,\n)\n\n\\# NOTE: can uncomment other evaluators\nevaluator\\_c \\= CorrectnessEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_s \\= SemanticSimilarityEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_r \\= RelevancyEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_f \\= FaithfulnessEvaluator(service\\_context\\=eval\\_service\\_context)\n\npairwise\\_evaluator \\= PairwiseComparisonEvaluator(\n    service\\_context\\=eval\\_service\\_context\n)\n\nfrom llama\\_index.evaluation.eval\\_utils import get\\_responses, get\\_results\\_df\nfrom llama\\_index.evaluation import BatchEvalRunner\n\nmax\\_samples \\= 60\n\neval\\_qs \\= eval\\_dataset.questions\nqr\\_pairs \\= eval\\_dataset.qr\\_pairs\nref\\_response\\_strs \\= \\[r for (\\_, r) in qr\\_pairs\\]\n\n\\# resetup base query engine and ensemble query engine\n\\# base query engine\nbase\\_query\\_engine \\= vector\\_indices\\[\\-1\\].as\\_query\\_engine(similarity\\_top\\_k\\=2)\n\\# ensemble query engine\nreranker \\= CohereRerank(top\\_n\\=4)\nquery\\_engine \\= RetrieverQueryEngine(retriever, node\\_postprocessors\\=\\[reranker\\])\n\nbase\\_pred\\_responses \\= get\\_responses(\n    eval\\_qs\\[:max\\_samples\\], base\\_query\\_engine, show\\_progress\\=True\n)\n\npred\\_responses \\= get\\_responses(\n    eval\\_qs\\[:max\\_samples\\], query\\_engine, show\\_progress\\=True\n)\n\nimport numpy as np\n\npred\\_response\\_strs \\= \\[str(p) for p in pred\\_responses\\]\nbase\\_pred\\_response\\_strs \\= \\[str(p) for p in base\\_pred\\_responses\\]\n\nevaluator\\_dict \\= {\n    \"correctness\": evaluator\\_c,\n    \"faithfulness\": evaluator\\_f,\n    \\# \"relevancy\": evaluator\\_r,\n    \"semantic\\_similarity\": evaluator\\_s,\n}\nbatch\\_runner \\= BatchEvalRunner(evaluator\\_dict, workers\\=1, show\\_progress\\=True)\n\neval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    queries\\=eval\\_qs\\[:max\\_samples\\],\n    responses\\=pred\\_responses\\[:max\\_samples\\],\n    reference\\=ref\\_response\\_strs\\[:max\\_samples\\],\n)\n\nbase\\_eval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    queries\\=eval\\_qs\\[:max\\_samples\\],\n    responses\\=base\\_pred\\_responses\\[:max\\_samples\\],\n    reference\\=ref\\_response\\_strs\\[:max\\_samples\\],\n)\n\nresults\\_df \\= get\\_results\\_df(\n    \\[eval\\_results, base\\_eval\\_results\\],\n    \\[\"Ensemble Retriever\", \"Base Retriever\"\\],\n    \\[\"correctness\", \"faithfulness\", \"semantic\\_similarity\"\\],\n)\ndisplay(results\\_df)\n\n|     | names | correctness | faithfulness | semantic\\_similarity |\n| --- | --- | --- | --- | --- |\n| 0   | Ensemble Retriever | 4.375000 | 0.983333 | 0.964546 |\n| 1   | Base Retriever | 4.066667 | 0.983333 | 0.956692 |\n\nbatch\\_runner \\= BatchEvalRunner(\n    {\"pairwise\": pairwise\\_evaluator}, workers\\=3, show\\_progress\\=True\n)\n\npairwise\\_eval\\_results \\= await batch\\_runner.aevaluate\\_response\\_strs(\n    queries\\=eval\\_qs\\[:max\\_samples\\],\n    response\\_strs\\=pred\\_response\\_strs\\[:max\\_samples\\],\n    reference\\=base\\_pred\\_response\\_strs\\[:max\\_samples\\],\n)\n\nresults\\_df \\= get\\_results\\_df(\n    \\[eval\\_results, base\\_eval\\_results\\],\n    \\[\"Ensemble Retriever\", \"Base Retriever\"\\],\n    \\[\"pairwise\"\\],\n)\ndisplay(results\\_df)\n\n|     | names | pairwise |\n| --- | --- | --- |\n| 0   | Pairwise Comparison | 0.5 |"
}