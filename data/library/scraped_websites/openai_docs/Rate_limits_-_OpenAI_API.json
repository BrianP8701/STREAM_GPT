{
    "metadata": {
        "type": "web",
        "url": "https://platform.openai.com/docs/guides/rate-limits/i-ve-implemented-exponential-backoff-for-my-text-code-apis-but-i-m-still-hitting-this-error-how-do-i-increase-my-rate-limit",
        "title": "Rate limits - OpenAI API",
        "description": "Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform."
    },
    "text": "[](https://platform.openai.com/docs/guides/rate-limits/rate-limits)\n\n## [Rate limits](https://platform.openai.com/docs/guides/rate-limits/rate-limits)\n\n[](https://platform.openai.com/docs/guides/rate-limits/overview)\n\n## [Overview](https://platform.openai.com/docs/guides/rate-limits/overview)\n\n[](https://platform.openai.com/docs/guides/rate-limits/what-are-rate-limits)\n\n### [What are rate limits?](https://platform.openai.com/docs/guides/rate-limits/what-are-rate-limits)\n\nA rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.\n\n[](https://platform.openai.com/docs/guides/rate-limits/why-do-we-have-rate-limits)\n\n### [Why do we have rate limits?](https://platform.openai.com/docs/guides/rate-limits/why-do-we-have-rate-limits)\n\nRate limits are a common practice for APIs, and they're put in place for a few different reasons:\n\n*   **They help protect against abuse or misuse of the API.** For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n*   **Rate limits help ensure that everyone has fair access to the API.** If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.\n*   **Rate limits can help OpenAI manage the aggregate load on its infrastructure.** If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n\nPlease work through this document in its entirety to better understand how OpenAI\u2019s rate limit system works. We include code examples and possible solutions to handle common issues. It is recommended to follow this guidance before filling out the [Rate Limit Increase Request form](https://docs.google.com/forms/d/e/1FAIpQLSc6gSL3zfHFlL6gNIyUcjkEv29jModHGxg5_XGyr-PrE2LaHw/viewform) with details regarding how to fill it out in the last section.\n\n[](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n\n### [What are the rate limits for our API?](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n\nYou can view the rate limits for your organization under the [rate limits](https://platform.openai.com/account/rate-limits) section of the account management page. Rate limits are automatically adjusted based on history of good use.\n\nWe enforce rate limits at the [organization level](https://platform.openai.com/docs/guides/production-best-practices), not user level, based on the specific endpoint used as well as the type of account you have. Rate limits are measured in three ways: **RPM** (requests per minute), **RPD** (requests per day), and **TPM** (tokens per minute). Rate limit can be hit by any of the three options depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the Completions endpoint and that would fill your limit (if your RPM was 20), even if you did not send 150k tokens (if your TPM limit was 150k) within those 20 requests.\n\n[](https://platform.openai.com/docs/guides/rate-limits/usage-tiers)\n\n### [Usage tiers](https://platform.openai.com/docs/guides/rate-limits/usage-tiers)\n\nYour rate limit and spending limit (quota) are automatically adjusted based on a number of factors. As your usage of the OpenAI API goes up and you successfully pay the bill, we automatically increase your usage tier. Below is a breakdown of the first 3 usage tiers.\n\n| Tier | Qualification | Max credits | Request limits | Token limits |\n| --- | --- | --- | --- | --- |\n| Free | User must be in an [allowed geography](https://platform.openai.com/docs/supported-countries) | $100 | 3 RPM, 200 RPD | 20K TPM (GPT-3.5), 4K TPM (GPT-4) |\n| Tier\u00a01 | $5 paid | $100 | 500 RPM, 10K RPD | 40K TPM (GPT-3.5), 10K TPM (GPT-4) |\n| Tier\u00a02 | $50 paid and 7+ days since first successful payment | $250 | 5000 RPM | 80K TPM (GPT-3.5), 20K TPM (GPT-4) |\n\nWe plan to expose additional usage tiers over time and will adjust these accordingly in response to capacity and fraud activity. Our main goal with usage tiers is to automatically increase rate limits and spending limits for customers who are successfully paying their bill.\n\nAs your usage tier increases, we may also move your account onto lower latency models behind the scenes.\n\n[](https://platform.openai.com/docs/guides/rate-limits/how-do-rate-limits-work)\n\n### [How do rate limits work?](https://platform.openai.com/docs/guides/rate-limits/how-do-rate-limits-work)\n\nIf your rate limit is 60 requests per minute and 150k tokens per minute, you\u2019ll be limited either by reaching the requests/min cap or running out of tokens\u2014whichever happens first. For example, if your max requests/min is 60, you should be able to send 1 request per second. If you send 1 request every 800ms, once you hit your rate limit, you\u2019d only need to make your program sleep 200ms in order to send one more request otherwise subsequent requests would fail. With the default of 3,000 requests/min, customers can effectively send 1 request every 20ms, or every .02 seconds.\n\n[](https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers)\n\n### [Rate limits in headers](https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers)\n\nIn addition to seeing your rate limit on your [account page](https://platform.openai.com/account/rate-limits), you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.\n\nYou can expect to see the following header fields:\n\n| Field | Sample Value | Description |\n| --- | --- | --- |\n| x-ratelimit-limit-requests | 60  | The maximum number of requests that are permitted before exhausting the rate limit. |\n| x-ratelimit-limit-tokens | 150000 | The maximum number of tokens that are permitted before exhausting the rate limit. |\n| x-ratelimit-remaining-requests | 59  | The remaining number of requests that are permitted before exhausting the rate limit. |\n| x-ratelimit-remaining-tokens | 149984 | The remaining number of tokens that are permitted before exhausting the rate limit. |\n| x-ratelimit-reset-requests | 1s  | The time until the rate limit (based on requests) resets to its initial state. |\n| x-ratelimit-reset-tokens | 6m0s | The time until the rate limit (based on tokens) resets to its initial state. |\n\n[](https://platform.openai.com/docs/guides/rate-limits/what-happens-if-i-hit-a-rate-limit-error)\n\n### [What happens if I hit a rate limit error?](https://platform.openai.com/docs/guides/rate-limits/what-happens-if-i-hit-a-rate-limit-error)\n\nRate limit errors look like this:\n\n> Rate limit reached for text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.\n\nIf you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.\n\n[](https://platform.openai.com/docs/guides/rate-limits/rate-limits-vs-max_tokens)\n\n### [Rate limits vs `max_tokens`](https://platform.openai.com/docs/guides/rate-limits/rate-limits-vs-max_tokens)\n\nEach [model we offer](https://platform.openai.com/docs/models/overview) has a limited number of tokens that can be passed in as input when making a request. You cannot increase the maximum number of tokens a model takes in. For example, if you are using `text-ada-001`, the maximum number of tokens you can send to this model is 2,048 tokens per request.\n\n[](https://platform.openai.com/docs/guides/rate-limits/error-mitigation)\n\n## [Error Mitigation](https://platform.openai.com/docs/guides/rate-limits/error-mitigation)\n\n[](https://platform.openai.com/docs/guides/rate-limits/what-are-some-steps-i-can-take-to-mitigate-this)\n\n### [What are some steps I can take to mitigate this?](https://platform.openai.com/docs/guides/rate-limits/what-are-some-steps-i-can-take-to-mitigate-this)\n\nThe OpenAI Cookbook has a [Python notebook](https://cookbook.openai.com/examples/how_to_handle_rate_limits) that explains how to avoid rate limit errors, as well an example [Python script](https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py) for staying under rate limits while batch processing API requests.\n\nYou should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.\n\nTo protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.\n\n[](https://platform.openai.com/docs/guides/rate-limits/retrying-with-exponential-backoff)\n\n#### [Retrying with exponential backoff](https://platform.openai.com/docs/guides/rate-limits/retrying-with-exponential-backoff)\n\nOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:\n\n*   Automatic retries means you can recover from rate limit errors without crashes or missing data\n*   Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n*   Adding random jitter to the delay helps retries from all hitting at the same time.\n\nNote that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won\u2019t work.\n\nBelow are a few example solutions **for Python** that use exponential backoff.\n\nExample #1: Using the Tenacity library\n\nTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. To add exponential backoff to your requests, you can use the `tenacity.retry` decorator. The below example uses the `tenacity.wait_random_exponential` function to add random exponential backoff to a request.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n \n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletion_with_backoff(model=\"gpt-3.5-turbo-instruct\", prompt=\"Once upon a time,\")\n```\n\nNote that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\n\nExample #2: Using the backoff library\n\nAnother python library that provides function decorators for backoff and retry is [backoff](https://pypi.org/project/backoff/):\n\n```\n1\n2\n3\n4\n5\n6\n7\nimport backoff \nimport openai \n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n \ncompletions_with_backoff(model=\"gpt-3.5-turbo-instruct\", prompt=\"Once upon a time,\")\n```\n\nLike Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\n\nExample 3: Manual backoff implementation\n\nIf you don't want to use third-party libraries, you can implement your own backoff logic following this example:\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n# imports\nimport random\nimport time\n \nimport openai\n \n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n \n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n \n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n \n            # Retry on specific errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n \n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n \n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n \n                # Sleep for the delay\n                time.sleep(delay)\n \n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n \n    return wrapper\n    \n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.Completion.create(**kwargs)\n```\n\nAgain, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.\n\n[](https://platform.openai.com/docs/guides/rate-limits/reduce-the-max_tokens-to-match-the-size-of-your-completions)\n\n#### [Reduce the `max_tokens` to match the size of your completions](https://platform.openai.com/docs/guides/rate-limits/reduce-the-max_tokens-to-match-the-size-of-your-completions)\n\nYour rate limit is calculated as the maximum of `max_tokens` and the estimated number of tokens based on the character count of your request. Try to set the `max_tokens` value as close to your expected response size as possible.\n\n[](https://platform.openai.com/docs/guides/rate-limits/batching-requests)\n\n#### [Batching requests](https://platform.openai.com/docs/guides/rate-limits/batching-requests)\n\nThe OpenAI API has separate limits for requests per minute and tokens per minute.\n\nIf you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.\n\nSending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport openai\n \nnum_stories = 10\nprompt = \"Once upon a time,\"\n \n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = openai.Completion.create(\n        model=\"curie\",\n        prompt=prompt,\n        max_tokens=20,\n    )\n    # print story\n    print(prompt + response.choices[0].text)\n```\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport openai  # for making OpenAI API requests\n \n \nnum_stories = 10\nprompts = [\"Once upon a time,\"] * num_stories\n \n# batched example, with 10 story completions per request\nresponse = openai.Completion.create(\n    model=\"curie\",\n    prompt=prompts,\n    max_tokens=20,\n)\n \n# match completions to prompts by index\nstories = [\"\"] * len(prompts)\nfor choice in response.choices:\n    stories[choice.index] = prompts[choice.index] + choice.text\n \n# print stories\nfor story in stories:\n    print(story)\n```\n\nWarning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.\n\n[](https://platform.openai.com/docs/guides/rate-limits/request-increase)\n\n## [Request Increase](https://platform.openai.com/docs/guides/rate-limits/request-increase)\n\n[](https://platform.openai.com/docs/guides/rate-limits/when-should-i-consider-applying-for-a-rate-limit-increase)\n\n### [When should I consider applying for a rate limit increase?](https://platform.openai.com/docs/guides/rate-limits/when-should-i-consider-applying-for-a-rate-limit-increase)\n\nOur default rate limits help us maximize stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. Large rate limit increase requests without supporting data are not likely to be approved. If you're gearing up for a product launch, please obtain the relevant data through a phased release over 10 days.\n\nKeep in mind that rate limit increases can sometimes take 7-10 days so it makes sense to try and plan ahead and submit early if there is data to support you will reach your rate limit given your current growth numbers.\n\n[](https://platform.openai.com/docs/guides/rate-limits/will-my-rate-limit-increase-request-be-rejected)\n\n### [Will my rate limit increase request be rejected?](https://platform.openai.com/docs/guides/rate-limits/will-my-rate-limit-increase-request-be-rejected)\n\nA rate limit increase request is most often rejected because it lacks the data needed to justify the increase. We have provided numerical examples below that show how to best support a rate limit increase request and try our best to approve all requests that align with our safety policy and show supporting data. We are committed to enabling developers to scale and be successful with our API.\n\n[](https://platform.openai.com/docs/guides/rate-limits/i-ve-implemented-exponential-backoff-for-my-text-code-apis-but-i-m-still-hitting-this-error-how-do-i-increase-my-rate-limit)\n\n### [I\u2019ve implemented exponential backoff for my text/code APIs, but I\u2019m still hitting this error. How do I increase my rate limit?](https://platform.openai.com/docs/guides/rate-limits/i-ve-implemented-exponential-backoff-for-my-text-code-apis-but-i-m-still-hitting-this-error-how-do-i-increase-my-rate-limit)\n\nWe understand the frustration that limited rate limits can cause, and we would love to raise the defaults for everyone. However, due to shared capacity constraints, we can only approve rate limit increases for paid customers who have demonstrated a need through our [Rate Limit Increase Request form](https://docs.google.com/forms/d/e/1FAIpQLSc6gSL3zfHFlL6gNIyUcjkEv29jModHGxg5_XGyr-PrE2LaHw/viewform). To help us evaluate your needs properly, we ask that you please provide statistics on your current usage or projections based on historic user activity in the 'Share evidence of need' section of the form. If this information is not available, we recommend a phased release approach. Start by releasing the service to a subset of users at your current rate limits, gather usage data for 10 business days, and then submit a formal rate limit increase request based on that data for our review and approval.\n\nWe will review your request and if it is approved, we will notify you of the approval within a period of 7-10 business days.\n\nHere are some examples of how you might fill out this form:\n\n| Model | Estimate Tokens/Minute | Estimate Requests/Minute | \\# of users | Evidence of need | 1 hour max throughput cost |\n| --- | --- | --- | --- | --- | --- |\n| DALL-E API | N/A | 50  | 1000 | Our app is currently in production and based on our past traffic, we make about 10 requests per minute. | $60 |\n| DALL-E API | N/A | 150 | 10,000 | Our app is gaining traction in the App Store and we\u2019re starting to hit rate limits. Can we get triple the default limit of 50 img/min? If we need more we\u2019ll submit a new form. Thanks! | $180 |\n\n| Model | Estimate Tokens/Minute | Estimate Requests/Minute | \\# of users | Evidence of need | 1 hour max throughput cost |\n| --- | --- | --- | --- | --- | --- |\n| gpt-3.5-turbo | 325,000 | 4,000 | 50  | We\u2019re releasing to an initial group of alpha testers and need a higher limit to accommodate their initial usage. We have a link here to our google drive which shows analytics and api usage. | $390 |\n| gpt-4 | 750,000 | 10,000 | 10,000 | Our application is receiving a lot of interest; we have 50,000 people on our waitlist. We\u2019d like to roll out to groups of 1,000 people/day until we reach 50,000 users. Please see this link of our current token/minute traffic over the past 30 days. This is for 500 users, and based on their usage, we think 750,000 tokens/minute and 10,000 requests/minute will work as a good starting point. | $900 |\n\n| Model | Estimate Tokens/Minute | Estimate Requests/Minute | \\# of users | Evidence of need | 1 hour max throughput cost |\n| --- | --- | --- | --- | --- | --- |\n| code-davinci-002 | 150,000 | 1,000 | 15  | We are a group of researchers working on a paper. We estimate that we will need a higher rate limit on code-davinci-002 in order to complete our research before the end of the month. These estimates are based on the following calculation \\[...\\] | Codex models are now deprecated and no longer accessible. |\n\nPlease note that these examples are just general use case scenarios, the actual usage rate will vary depending on the specific implementation and usage."
}