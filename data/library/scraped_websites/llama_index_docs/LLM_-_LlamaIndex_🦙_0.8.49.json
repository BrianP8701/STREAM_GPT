{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/root.html",
        "title": "LLM - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## LLM[\uf0c1](#llm \"Permalink to this heading\")\n\n## Concept[\uf0c1](#concept \"Permalink to this heading\")\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it\u2019s from OpenAI, Hugging Face, or LangChain, so that you don\u2019t have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n\n*   Support for **text completion** and **chat** endpoints (details below)\n    \n*   Support for **streaming** and **non-streaming** endpoints\n    \n*   Support for **synchronous** and **asynchronous** endpoints\n    \n\n## LLM Compatibility Tracking[\uf0c1](#llm-compatibility-tracking \"Permalink to this heading\")\n\nWhile LLMs are powerful, not every LLM is easy to set up. Furthermore, even with proper setup, some LLMs have trouble performning tasks that require strict instruction following.\n\nLlamaIndex offers integrations with nearly every LLM, but it can be often unclear if the LLM will work well out of the box, or if further customization is needed.\n\nThe tables below attempt to validate the **initial** experience with various LlamaIndex features for various LLMs. These notebooks serve as a best attempt to gauge performance, as well as how much effort and tweaking is needed to get things to function properly.\n\nGenerally, paid APIs such as OpenAI or Anthropic are viewed as more reliable. However, local open-source models have been gaining popularity due to their customizability and approach to transparency.\n\n**Contributing:** Anyone is welcome to contribute new LLMs to the documentation. Simply copy an existing notebook, setup and test your LLM, and open a PR with your resutls.\n\nIf you have ways to improve the setup for existing notebooks, contributions to change this are welcome!\n\n**Legend**\n\n*   \u2705 = should work fine\n    \n*   \u26a0\ufe0f = sometimes unreliable, may need prompt engineering to improve\n    \n*   \ud83d\uded1 = usually unreliable, would need prompt engineering/fine-tuning to improve\n    \n\n### Paid LLM APIs[\uf0c1](#paid-llm-apis \"Permalink to this heading\")\n\n| Model Name | Basic Query Engines | Router Query Engine | Sub Question Query Engine | Text2SQL | Pydantic Programs | Data Agents | Notes |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| [gpt-3.5-turbo](https://colab.research.google.com/drive/1oVqUAkn0GCBG5OCs3oMUPlNQDdpDTH_c?usp=sharing) (openai) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   |     |\n| [gpt-3.5-turbo-instruct](https://colab.research.google.com/drive/1DrVdx-VZ3dXwkwUVZQpacJRgX7sOa4ow?usp=sharing) (openai) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u26a0\ufe0f  | Tool usage in data-agents seems flakey. |\n| [gpt-4](https://colab.research.google.com/drive/1RsBoT96esj1uDID-QE8xLrOboyHKp65L?usp=sharing) (openai) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   |     |\n| [claude-2](https://colab.research.google.com/drive/1os4BuDS3KcI8FCcUM_2cJma7oI2PGN7N?usp=sharing) (anthropic) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u26a0\ufe0f  | Prone to hallucinating tool inputs. |\n| [claude-instant-1.2](https://colab.research.google.com/drive/1wt3Rt2OWBbqyeRYdiLfmB0_OIUOGit_D?usp=sharing) (anthropic) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u26a0\ufe0f  | Prone to hallucinating tool inputs. |\n\n### Open Source LLMs[\uf0c1](#open-source-llms \"Permalink to this heading\")\n\nSince open source LLMs require large amounts of resources, the quantization is reported. Quantization is just a method for reducing the size of an LLM by shrinking the accuracy of calculations within the model. Research has shown that up to 4Bit quantization can be achieved for large LLMs without impacting performance too severely.\n\n| Model Name | Basic Query Engines | Router Query Engine | SubQuestion Query Engine | Text2SQL | Pydantic Programs | Data Agents | Notes |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| [llama2-chat-7b 4bit](https://colab.research.google.com/drive/14N-hmJ87wZsFqHktrw40OU6sVcsiSzlQ?usp=sharing) (huggingface) | \u2705   | \ud83d\uded1  | \ud83d\uded1  | \ud83d\uded1  | \ud83d\uded1  | \u26a0\ufe0f  | Llama2 seems to be quite chatty, which makes parsing structured outputs difficult. Fine-tuning and prompt engineering likely required for better performance on structured outputs. |\n| [Mistral-7B-instruct-v0.1 4bit](https://colab.research.google.com/drive/1ZAdrabTJmZ_etDp10rjij_zME2Q3umAQ?usp=sharing) (huggingface) | \u2705   | \ud83d\uded1  | \ud83d\uded1  | \u26a0\ufe0f  | \u26a0\ufe0f  | \u26a0\ufe0f  | Mistral seems slightly more reliable for structured outputs compared to Llama2. Likely with some prompt engineering, it may do better. |\n| [zephyr-7b-alpha](https://colab.research.google.com/drive/16Ygf2IyGNkb725ZqtRmFQjwWBuzFX_kl?usp=sharing) (huggingface) | \u2705   | \u2705   | \u2705   | \u2705   | \u2705   | \u26a0\ufe0f  | Overall, `zyphyr-7b` is appears to be more reliable than other open-source models of this size. Although it still hallucinates a bit, especially as an agent. |\n\n## Modules[\uf0c1](#modules \"Permalink to this heading\")\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\n*   [Modules](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html)\n    *   [OpenAI](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#openai)\n    *   [AI21](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#ai21)\n    *   [Anthropic](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#anthropic)\n    *   [Gradient](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#gradient)\n    *   [Hugging Face](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#hugging-face)\n    *   [EverlyAI](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#everlyai)\n    *   [LiteLLM](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#litellm)\n    *   [PaLM](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#palm)\n    *   [Predibase](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#predibase)\n    *   [Replicate](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#replicate)\n    *   [LangChain](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#langchain)\n    *   [Llama API](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#llama-api)\n    *   [Llama CPP](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#llama-cpp)\n    *   [Xorbits Inference](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#xorbits-inference)\n    *   [MonsterAPI](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#monsterapi)\n    *   [RunGPT](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#rungpt)\n    *   [Portkey](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#portkey)\n    *   [AnyScale](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#anyscale)\n    *   [Ollama](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#ollama)\n    *   [Konko](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#konko)\n    *   [Clarifai](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#clarifai)\n    *   [Bedrock](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/modules.html#bedrock)"
}