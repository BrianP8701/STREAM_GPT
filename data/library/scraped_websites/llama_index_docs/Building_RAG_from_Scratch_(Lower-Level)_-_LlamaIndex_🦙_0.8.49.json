{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/low_level/root.html",
        "title": "Building RAG from Scratch (Lower-Level) - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nThis doc is a hub for showing how you can build RAG and agent-based apps using only lower-level abstractions (e.g. LLMs, prompts, embedding models), and without using more \u201cpackaged\u201d out of the box abstractions.\n\nOut of the box abstractions include:\n\n*   High-level ingestion code e.g. `VectorStoreIndex.from_documents`\n    \n*   High-level query and retriever code e.g. `VectorStoreIndex.as_retriever()` and `VectorStoreIndex.as_query_engine()`\n    \n*   High-level agent abstractions e.g. `OpenAIAgent`\n    \n\nInstead of using these, the goal here is to educate users on what\u2019s going on under the hood. By showing you the underlying algorithms for constructing RAG and agent pipelines, you can then be empowered to create your own custom LLM workflows (while still using LlamaIndex abstractions at any level of granularity that makes sense).\n\nWe show how to build an app from scratch, component by component. For the sake of focus, each tutorial will show how to build a specific component from scratch while using out-of-the-box abstractions for other components. **NOTE**: This is a WIP document, we\u2019re in the process of fleshing this out!\n\n## Building a (Very Simple) Vector Store from Scratch[\uf0c1](#building-a-very-simple-vector-store-from-scratch \"Permalink to this heading\")\n\nIf you want to learn more about how vector stores work, here\u2019s a tutorial showing you how to build a very simple vector store capable of dense search + metadata filtering.\n\nObviously not a replacement for production databases.\n\n*   [Building a (Very Simple) Vector Store from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/vector_store.html)\n\n## Building Response Synthesis from Scratch[\uf0c1](#building-response-synthesis-from-scratch \"Permalink to this heading\")\n\nThis tutorial shows you how to use the LLM to synthesize results given a set of retrieved context. Deals with context overflows, async calls, and source citations!\n\n*   [Building Response Synthesis from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/response_synthesis.html)\n\n## Building Evaluation from Scratch[\uf0c1](#building-evaluation-from-scratch \"Permalink to this heading\")\n\nLearn how to build common LLM-based eval modules (correctness, faithfulness) using LLMs and prompt modules; this will help you define your own custom evals!\n\n*   [Building Evaluation from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/evaluation.html)\n\n## Building Advanced RAG from Scratch[\uf0c1](#building-advanced-rag-from-scratch \"Permalink to this heading\")\n\nThese tutorials will show you how to build advanced functionality beyond the basic RAG pipeline. Especially helpful for advanced users with custom workflows / production needs.\n\n### Building a Router from Scratch[\uf0c1](#building-a-router-from-scratch \"Permalink to this heading\")\n\nBeyond the standard RAG pipeline, this takes you one step towards automated decision making with LLMs by showing you how to build a router module from scratch.\n\n*   [Building a Router from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/router.html)"
}