{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/llm/portkey.html",
        "title": "Portkey - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Portkey[\uf0c1](#portkey \"Permalink to this heading\")\n\n**Portkey** is a full-stack LLMOps platform that productionizes your Gen AI app reliably and securely.\n\n## Key Features of Portkey\u2019s Integration with Llamaindex:[\uf0c1](#key-features-of-portkey-s-integration-with-llamaindex \"Permalink to this heading\")\n\n![header](https://3798672042-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FeWEp2XRBGxs7C1jgAdk7%2Fuploads%2FjDGBQvw5aFOCqctr0xwp%2FColab%20Version%202.png?alt=media&token=16057c99-b86c-416c-932e-c2b71549c506)\n\n1.  **\ud83d\udeaa AI Gateway**:\n    \n    *   **Automated Fallbacks & Retries**: Ensure your application remains functional even if a primary service fails.\n        \n    *   **Load Balancing**: Efficiently distribute incoming requests among multiple models.\n        \n    *   **Semantic Caching**: Reduce costs and latency by intelligently caching results.\n        \n2.  **\ud83d\udd2c Observability**:\n    \n    *   **Logging**: Keep track of all requests for monitoring and debugging.\n        \n    *   **Requests Tracing**: Understand the journey of each request for optimization.\n        \n    *   **Custom Tags**: Segment and categorize requests for better insights.\n        \n3.  **\ud83d\udcdd Continuous Improvement with User Feedback**:\n    \n    *   **Feedback Collection**: Seamlessly gather feedback on any served request, be it on a generation or conversation level.\n        \n    *   **Weighted Feedback**: Obtain nuanced information by attaching weights to user feedback values.\n        \n    *   **Feedback Metadata**: Incorporate custom metadata with the feedback to provide context, allowing for richer insights and analyses.\n        \n4.  **\ud83d\udd11 Secure Key Management**:\n    \n    *   **Virtual Keys**: Portkey transforms original provider keys into virtual keys, ensuring your primary credentials remain untouched.\n        \n    *   **Multiple Identifiers**: Ability to add multiple keys for the same provider or the same key under different names for easy identification without compromising security.\n        \n\nTo harness these features, let\u2019s start with the setup:\n\n[![\\\"Open](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/portkey.ipynb)\n\n\\# Installing Llamaindex & Portkey SDK\n!pip install \\-U llama\\_index\n!pip install \\-U portkey-ai\n\n\\# Importing necessary libraries and modules\nfrom llama\\_index.llms import Portkey, ChatMessage\nimport portkey as pk\n\nYou do not need to install **any** other SDKs or import them in your Llamaindex app.\n\n## **Step 1\ufe0f\u20e3: Get your Portkey API Key and your Virtual Keys for OpenAI, Anthropic, and more**[\uf0c1](#step-1-get-your-portkey-api-key-and-your-virtual-keys-for-openai-anthropic-and-more \"Permalink to this heading\")\n\n**[Portkey API Key](https://app.portkey.ai/)**: Log into [Portkey here](https://app.portkey.ai/), then click on the profile icon on top left and \u201cCopy API Key\u201d.\n\nimport os\n\nos.environ\\[\"PORTKEY\\_API\\_KEY\"\\] \\= \"PORTKEY\\_API\\_KEY\"\n\n**[Virtual Keys](https://docs.portkey.ai/key-features/ai-provider-keys)**\n\n1.  Navigate to the \u201cVirtual Keys\u201d page on [Portkey dashboard](https://app.portkey.ai/) and hit the \u201cAdd Key\u201d button located at the top right corner.\n    \n2.  Choose your AI provider (OpenAI, Anthropic, Cohere, HuggingFace, etc.), assign a unique name to your key, and, if needed, jot down any relevant usage notes. Your virtual key is ready!\n    \n\n![header](https://3798672042-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FeWEp2XRBGxs7C1jgAdk7%2Fuploads%2F66S1ik16Gle8jS1u6smr%2Fvirtual_keys.png?alt=media&token=2fec1c39-df4e-4c93-9549-7445a833321c) 3. Now copy and paste the keys below - you can use them anywhere within the Portkey ecosystem and keep your original key secure and untouched.\n\nopenai\\_virtual\\_key\\_a \\= \"\"\nopenai\\_virtual\\_key\\_b \\= \"\"\n\nanthropic\\_virtual\\_key\\_a \\= \"\"\nanthropic\\_virtual\\_key\\_b \\= \"\"\n\ncohere\\_virtual\\_key\\_a \\= \"\"\ncohere\\_virtual\\_key\\_b \\= \"\"\n\nIf you don\u2019t want to use Portkey\u2019s Virtual keys, you can also use your AI provider keys directly.\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"\"\nos.environ\\[\"ANTHROPIC\\_API\\_KEY\"\\] \\= \"\"\n\n## **Step 2\ufe0f\u20e3: Configure Portkey Features**[\uf0c1](#step-2-configure-portkey-features \"Permalink to this heading\")\n\nTo harness the full potential of Portkey\u2019s integration with Llamaindex, you can configure various features as illustrated above. Here\u2019s a guide to all Portkey features and the expected values:\n\n| Feature | Config Key | Value(Type) | Required |\n| --- | --- | --- | --- |\n| API Key | `api_key` | `string` | \u2705 Required (can be set externally) |\n| Mode | `mode` | `fallback`, `loadbalance`, `single` | \u2705 Required |\n| Cache Type | `cache_status` | `simple`, `semantic` | \u2754 Optional |\n| Force Cache Refresh | `cache_force_refresh` | `True`, `False` | \u2754 Optional |\n| Cache Age | `cache_age` | `integer` (in seconds) | \u2754 Optional |\n| Trace ID | `trace_id` | `string` | \u2754 Optional |\n| Retries | `retry` | `integer` \\[0,5\\] | \u2754 Optional |\n| Metadata | `metadata` | `json object` [More info](https://docs.portkey.ai/key-features/custom-metadata) | \u2754 Optional |\n| Base URL | `base_url` | `url` | \u2754 Optional |\n\n*   `api_key` and `mode` are required values.\n    \n*   You can set your Portkey API key using the Portkey constructor or you can also set it as an environment variable.\n    \n*   There are **3** modes - Single, Fallback, Loadbalance.\n    \n    *   **Single** - This is the standard mode. Use it if you do not want Fallback OR Loadbalance features.\n        \n    *   **Fallback** - Set this mode if you want to enable the Fallback feature. Check out the guide here.\n        \n    *   **Loadbalance** - Set this mode if you want to enable the Loadbalance feature. Check out the guide here.\n        \n\nHere\u2019s an example of how to set up some of these features:\n\nportkey\\_client \\= Portkey(\n    mode\\=\"single\",\n)\n\n\\# Since we have defined the Portkey API Key with os.environ, we do not need to set api\\_key again here\n\n## **Step 3\ufe0f\u20e3: Constructing the LLM**[\uf0c1](#step-3-constructing-the-llm \"Permalink to this heading\")\n\nWith the Portkey integration, constructing an LLM is simplified. Use the `LLMOptions` function for all providers, with the exact same keys you\u2019re accustomed to in your OpenAI or Anthropic constructors. The only new key is `weight`, essential for the load balancing feature.\n\nopenai\\_llm \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-4\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n)\n\nThe above code illustrates how to utilize the `LLMOptions` function to set up an LLM with the OpenAI provider and the GPT-4 model. This same function can be used for other providers as well, making the integration process streamlined and consistent across various providers.\n\n## **Step 4\ufe0f\u20e3: Activate the Portkey Client**[\uf0c1](#step-4-activate-the-portkey-client \"Permalink to this heading\")\n\nOnce you\u2019ve constructed the LLM using the `LLMOptions` function, the next step is to activate it with Portkey. This step is essential to ensure that all the Portkey features are available for your LLM.\n\nportkey\\_client.add\\_llms(openai\\_llm)\n\nAnd, that\u2019s it! In just 4 steps, you have infused your Llamaindex app with sophisticated production capabilities.\n\n## **\ud83d\udd27 Testing the Integration**[\uf0c1](#testing-the-integration \"Permalink to this heading\")\n\nLet\u2019s ensure that everything is set up correctly. Below, we create a simple chat scenario and pass it through our Portkey client to see the response.\n\nmessages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"What can you do?\"),\n\\]\nprint(\"Testing Portkey Llamaindex integration:\")\nresponse \\= portkey\\_client.chat(messages)\nprint(response)\n\nHere\u2019s how your logs will appear on your [Portkey dashboard](https://app.portkey.ai/):\n\n![Logs](https://portkey.ai/blog/content/images/2023/09/Log-1.png)\n\n## **\u23e9 Streaming Responses**[\uf0c1](#streaming-responses \"Permalink to this heading\")\n\nWith Portkey, streaming responses has never been more straightforward. Portkey has 4 response functions:\n\n1.  `.complete(prompt)`\n    \n2.  `.stream_complete(prompt)`\n    \n3.  `.chat(messages)`\n    \n4.  `.stream_chat(messages)`\n    \n\nWhile the `complete` function expects a string input(`str`), the `chat` function works with an array of `ChatMessage` objects.\n\n**Example usage:**\n\n\\# Let's set up a prompt and then use the stream\\_complete function to obtain a streamed response.\n\nprompt \\= \"Why is the sky blue?\"\n\nprint(\"\\\\nTesting Stream Complete:\\\\n\")\nresponse \\= portkey\\_client.stream\\_complete(prompt)\nfor i in response:\n    print(i.delta, end\\=\"\", flush\\=True)\n\n\\# Let's prepare a set of chat messages and then utilize the stream\\_chat function to achieve a streamed chat response.\n\nmessages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"What can you do?\"),\n\\]\n\nprint(\"\\\\nTesting Stream Chat:\\\\n\")\nresponse \\= portkey\\_client.stream\\_chat(messages)\nfor i in response:\n    print(i.delta, end\\=\"\", flush\\=True)\n\n## **\ud83d\udd0d Recap and References**[\uf0c1](#recap-and-references \"Permalink to this heading\")\n\nCongratulations! \ud83c\udf89 You\u2019ve successfully set up and tested the Portkey integration with Llamaindex. To recap the steps:\n\n1.  pip install portkey-ai\n    \n2.  from llama\\_index.llms import Portkey\n    \n3.  Grab your Portkey API Key and create your virtual provider keys from [here](https://app.portkey.ai/).\n    \n4.  Construct your Portkey client and set mode: `portkey_client=Portkey(mode=\"fallback\")`\n    \n5.  Construct your provider LLM with LLMOptions: `openai_llm = pk.LLMOptions(provider=\"openai\", model=\"gpt-4\", virtual_key=openai_key_a)`\n    \n6.  Add the LLM to Portkey with `portkey_client.add_llms(openai_llm)`\n    \n7.  Call the Portkey methods regularly like you would any other LLM, with `portkey_client.chat(messages)`\n    \n\nHere\u2019s the guide to all the functions and their params:\n\n*   Portkey LLM Constructor\n    \n*   [LLMOptions Constructor](https://github.com/Portkey-AI/rubeus-python-sdk/blob/4cf3e17b847225123e92f8e8467b41d082186d60/rubeus/api_resources/utils.py#L179)\n    \n*   List of Portkey + Llamaindex Features\n    \n\n[![\\\"Open](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/portkey.ipynb)\n\n## **\ud83d\udd01 Implementing Fallbacks and Retries with Portkey**[\uf0c1](#implementing-fallbacks-and-retries-with-portkey \"Permalink to this heading\")\n\nFallbacks and retries are essential for building resilient AI applications. With Portkey, implementing these features is straightforward:\n\n*   **Fallbacks**: If a primary service or model fails, Portkey will automatically switch to a backup model.\n    \n*   **Retries**: If a request fails, Portkey can be configured to retry the request multiple times.\n    \n\nBelow, we demonstrate how to set up fallbacks and retries using Portkey:\n\nportkey\\_client \\= Portkey(mode\\=\"fallback\")\nmessages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"What can you do?\"),\n\\]\n\nllm1 \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-4\",\n    retry\\_settings\\={\"on\\_status\\_codes\": \\[429, 500\\], \"attempts\": 2},\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n)\n\nllm2 \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-3.5-turbo\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_b,\n)\n\nportkey\\_client.add\\_llms(llm\\_params\\=\\[llm1, llm2\\])\n\nprint(\"Testing Fallback & Retry functionality:\")\nresponse \\= portkey\\_client.chat(messages)\nprint(response)\n\n## **\u2696\ufe0f Implementing Load Balancing with Portkey**[\uf0c1](#implementing-load-balancing-with-portkey \"Permalink to this heading\")\n\nLoad balancing ensures that incoming requests are efficiently distributed among multiple models. This not only enhances the performance but also provides redundancy in case one model fails.\n\nWith Portkey, implementing load balancing is simple. You need to:\n\n*   Define the `weight` parameter for each LLM. This weight determines how requests are distributed among the LLMs.\n    \n*   Ensure that the sum of weights for all LLMs equals 1.\n    \n\nHere\u2019s an example of setting up load balancing with Portkey:\n\nportkey\\_client \\= Portkey(mode\\=\"ab\\_test\")\n\nmessages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"What can you do?\"),\n\\]\n\nllm1 \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-4\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n    weight\\=0.2,\n)\n\nllm2 \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-3.5-turbo\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n    weight\\=0.8,\n)\n\nportkey\\_client.add\\_llms(llm\\_params\\=\\[llm1, llm2\\])\n\nprint(\"Testing Loadbalance functionality:\")\nresponse \\= portkey\\_client.chat(messages)\nprint(response)\n\n## **\ud83e\udde0 Implementing Semantic Caching with Portkey**[\uf0c1](#implementing-semantic-caching-with-portkey \"Permalink to this heading\")\n\nSemantic caching is a smart caching mechanism that understands the context of a request. Instead of caching based solely on exact input matches, semantic caching identifies similar requests and serves cached results, reducing redundant requests and improving response times as well as saving money.\n\nLet\u2019s see how to implement semantic caching with Portkey:\n\nimport time\n\nportkey\\_client \\= Portkey(mode\\=\"single\")\n\nopenai\\_llm \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-3.5-turbo\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n    cache\\_status\\=\"semantic\",\n)\n\nportkey\\_client.add\\_llms(openai\\_llm)\n\ncurrent\\_messages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"What are the ingredients of a pizza?\"),\n\\]\n\nprint(\"Testing Portkey Semantic Cache:\")\n\nstart \\= time.time()\nresponse \\= portkey\\_client.chat(current\\_messages)\nend \\= time.time() \\- start\n\nprint(response)\nprint(f\"{'-'\\*50}\\\\nServed in {end} seconds.\\\\n{'-'\\*50}\")\n\nnew\\_messages \\= \\[\n    ChatMessage(role\\=\"system\", content\\=\"You are a helpful assistant\"),\n    ChatMessage(role\\=\"user\", content\\=\"Ingredients of pizza\"),\n\\]\n\nprint(\"Testing Portkey Semantic Cache:\")\n\nstart \\= time.time()\nresponse \\= portkey\\_client.chat(new\\_messages)\nend \\= time.time() \\- start\n\nprint(response)\nprint(f\"{'-'\\*50}\\\\nServed in {end} seconds.\\\\n{'-'\\*50}\")\n\nPortkey\u2019s cache supports two more cache-critical functions - Force Refresh and Age.\n\n`cache_force_refresh`: Force-send a request to your provider instead of serving it from a cache. `cache_age`: Decide the interval at which the cache store for this particular string should get automatically refreshed. The cache age is set in seconds.\n\nHere\u2019s how you can use it:\n\n\\# Setting the cache status as \\`semantic\\` and cache\\_age as 60s.\nopenai\\_llm \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-3.5-turbo\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n    cache\\_force\\_refresh\\=True,\n    cache\\_age\\=60,\n)\n\n## **\ud83d\udd2c Observability with Portkey**[\uf0c1](#observability-with-portkey \"Permalink to this heading\")\n\nHaving insight into your application\u2019s behavior is paramount. Portkey\u2019s observability features allow you to monitor, debug, and optimize your AI applications with ease. You can track each request, understand its journey, and segment them based on custom tags. This level of detail can help in identifying bottlenecks, optimizing costs, and enhancing the overall user experience.\n\nHere\u2019s how to set up observability with Portkey:\n\nmetadata \\= {\n    \"\\_environment\": \"production\",\n    \"\\_prompt\": \"test\",\n    \"\\_user\": \"user\",\n    \"\\_organisation\": \"acme\",\n}\n\ntrace\\_id \\= \"llamaindex\\_portkey\"\n\nportkey\\_client \\= Portkey(mode\\=\"single\")\n\nopenai\\_llm \\= pk.LLMOptions(\n    provider\\=\"openai\",\n    model\\=\"gpt-3.5-turbo\",\n    virtual\\_key\\=openai\\_virtual\\_key\\_a,\n    metadata\\=metadata,\n    trace\\_id\\=trace\\_id,\n)\n\nportkey\\_client.add\\_llms(openai\\_llm)\n\nprint(\"Testing Observability functionality:\")\nresponse \\= portkey\\_client.chat(messages)\nprint(response)\n\n## **\ud83c\udf09 Open Source AI Gateway**[\uf0c1](#open-source-ai-gateway \"Permalink to this heading\")\n\nPortkey\u2019s AI Gateway uses the [open source project Rubeus](https://github.com/portkey-ai/rubeus) internally. Rubeus powers features like interoperability of LLMs, load balancing, fallbacks, and acts as an intermediary, ensuring that your requests are processed optimally.\n\nOne of the advantages of using Portkey is its flexibility. You can easily customize its behavior, redirect requests to different providers, or even bypass logging to Portkey altogether.\n\nHere\u2019s an example of customizing the behavior with Portkey:\n\nportkey\\_client.base\\_url\\=None\n\n## **\ud83d\udcdd Feedback with Portkey**[\uf0c1](#feedback-with-portkey \"Permalink to this heading\")\n\nContinuous improvement is a cornerstone of AI. To ensure your models and applications evolve and serve users better, feedback is vital. Portkey\u2019s Feedback API offers a straightforward way to gather weighted feedback from users, allowing you to refine and improve over time.\n\nHere\u2019s how to utilize the Feedback API with Portkey:\n\nRead more about [Feedback here](https://docs.portkey.ai/key-features/feedback-api).\n\nimport requests\nimport json\n\n\\# Endpoint URL\nurl \\= \"https://api.portkey.ai/v1/feedback\"\n\n\\# Headers\nheaders \\= {\n    \"x-portkey-api-key\": os.environ.get(\"PORTKEY\\_API\\_KEY\"),\n    \"Content-Type\": \"application/json\",\n}\n\n\\# Data\ndata \\= {\"trace\\_id\": \"llamaindex\\_portkey\", \"value\": 1}\n\n\\# Making the request\nresponse \\= requests.post(url, headers\\=headers, data\\=json.dumps(data))\n\n\\# Print the response\nprint(response.text)\n\nAll the feedback with `weight` and `value` for each trace id is available on the Portkey dashboard:\n\n![Feedback](https://portkey.ai/blog/content/images/2023/09/feedback.png)\n\n## **\u2705 Conclusion**[\uf0c1](#conclusion \"Permalink to this heading\")\n\nIntegrating Portkey with Llamaindex simplifies the process of building robust and resilient AI applications. With features like semantic caching, observability, load balancing, feedback, and fallbacks, you can ensure optimal performance and continuous improvement.\n\nBy following this guide, you\u2019ve set up and tested the Portkey integration with Llamaindex. As you continue to build and deploy AI applications, remember to leverage the full potential of this integration!\n\nFor further assistance or questions, reach out to the developers \u27a1\ufe0f  \n[![Twitter](https://img.shields.io/twitter/follow/portkeyai?style=social&logo=twitter)](https://twitter.com/intent/follow?screen_name=portkeyai)\n\nJoin our community of practitioners putting LLMs into production \u27a1\ufe0f  \n[![Discord](https://img.shields.io/discord/1143393887742861333?logo=discord)](https://discord.gg/sDk9JaNfK8)"
}