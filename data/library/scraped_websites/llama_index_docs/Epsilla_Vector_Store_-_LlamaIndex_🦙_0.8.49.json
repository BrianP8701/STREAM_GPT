{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/EpsillaIndexDemo.html",
        "title": "Epsilla Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Epsilla Vector Store[\uf0c1](#epsilla-vector-store \"Permalink to this heading\")\n\nIn this notebook we are going to show how to use [Epsilla](https://www.epsilla.com/) to perform vector searches in LlamaIndex.\n\nAs a prerequisite, you need to have a running Epsilla vector database (for example, through our docker image), and install the `pyepsilla` package. View full docs at [docs](https://epsilla-inc.gitbook.io/epsilladb/quick-start)\n\n!pip/pip3 install pyepsilla\n\nimport logging\nimport sys\n\n\\# Uncomment to see debug logs\n\\# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama\\_index import SimpleDirectoryReader, Document, StorageContext\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.vector\\_stores import EpsillaVectorStore\nimport textwrap\n\n## Setup OpenAI[\uf0c1](#setup-openai \"Permalink to this heading\")\n\nLets first begin by adding the openai api key. It will be used to created embeddings for the documents loaded into the index.\n\nimport openai\nimport getpass\n\nOPENAI\\_API\\_KEY \\= getpass.getpass(\"OpenAI API Key:\")\nopenai.api\\_key \\= OPENAI\\_API\\_KEY\n\n## Loading documents[\uf0c1](#loading-documents \"Permalink to this heading\")\n\nLoad documents stored in the `/data/paul_graham` folder using the SimpleDirectoryReader.\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\nprint(f\"Total documents: {len(documents)}\")\nprint(f\"First document, id: {documents\\[0\\].doc\\_id}\")\nprint(f\"First document, hash: {documents\\[0\\].hash}\")\n\nTotal documents: 1\nFirst document, id: ac7f23f0-ce15-4d94-a0a2-5020fa87df61\nFirst document, hash: 4c702b4df575421e1d1af4b1fd50511b226e0c9863dbfffeccb8b689b8448f35\n\n## Create the index[\uf0c1](#create-the-index \"Permalink to this heading\")\n\nHere we create an index backed by Epsilla using the documents loaded previously. EpsillaVectorStore takes a few arguments.\n\n*   client (Any): Epsilla client to connect to.\n    \n*   collection\\_name (str, optional): Which collection to use. Defaults to \u201cllama\\_collection\u201d.\n    \n*   db\\_path (str, optional): The path where the database will be persisted. Defaults to \u201c/tmp/langchain-epsilla\u201d.\n    \n*   db\\_name (str, optional): Give a name to the loaded database. Defaults to \u201clangchain\\_store\u201d.\n    \n*   dimension (int, optional): The dimension of the embeddings. If not provided, collection creation will be done on first insert. Defaults to None.\n    \n*   overwrite (bool, optional): Whether to overwrite existing collection with same name. Defaults to False.\n    \n\nEpsilla vectordb is running with default host \u201clocalhost\u201d and port \u201c8888\u201d.\n\n\\# Create an index over the documnts\nfrom pyepsilla import vectordb\n\nclient \\= vectordb.Client()\nvector\\_store \\= EpsillaVectorStore(client\\=client, db\\_path\\=\"/tmp/llamastore\")\n\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\n\\[INFO\\] Connected to localhost:8888 successfully.\n\n## Query the data[\uf0c1](#query-the-data \"Permalink to this heading\")\n\nNow we have our document stored in the index, we can ask questions against the index.\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"Who is the author?\")\nprint(textwrap.fill(str(response), 100))\n\nThe author of the given context information is Paul Graham.\n\nresponse \\= query\\_engine.query(\"How did the author learn about AI?\")\nprint(textwrap.fill(str(response), 100))\n\nThe author learned about AI through various sources. One source was a novel called \"The Moon is a\nHarsh Mistress\" by Heinlein, which featured an intelligent computer called Mike. Another source was\na PBS documentary that showed Terry Winograd using SHRDLU, a program that could understand natural\nlanguage. These experiences sparked the author's interest in AI and motivated them to start learning\nabout it, including teaching themselves Lisp, which was regarded as the language of AI at the time.\n\nNext, let\u2019s try to overwrite the previous data.\n\nvector\\_store \\= EpsillaVectorStore(client\\=client, overwrite\\=True)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nsingle\\_doc \\= Document(text\\=\"Epsilla is the vector database we are using.\")\nindex \\= VectorStoreIndex.from\\_documents(\n    \\[single\\_doc\\],\n    storage\\_context\\=storage\\_context,\n)\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"Who is the author?\")\nprint(textwrap.fill(str(response), 100))\n\nThere is no information provided about the author in the given context.\n\nresponse \\= query\\_engine.query(\"What vector database is being used?\")\nprint(textwrap.fill(str(response), 100))\n\nEpsilla is the vector database being used.\n\nNext, let\u2019s add more data to existing collection.\n\nvector\\_store \\= EpsillaVectorStore(client\\=client, overwrite\\=False)\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store\\=vector\\_store)\nfor doc in documents:\n    index.insert(document\\=doc)\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"Who is the author?\")\nprint(textwrap.fill(str(response), 100))\n\nThe author of the given context information is Paul Graham.\n\nresponse \\= query\\_engine.query(\"What vector database is being used?\")\nprint(textwrap.fill(str(response), 100))\n\nEpsilla is the vector database being used."
}