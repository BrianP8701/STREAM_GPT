{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/RecencyPostprocessorDemo.html",
        "title": "Recency Filtering - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Recency Filtering[\uf0c1](#recency-filtering \"Permalink to this heading\")\n\nShowcase capabilities of recency-weighted node postprocessor\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama\\_index.indices.postprocessor import (\n    FixedRecencyPostprocessor,\n    EmbeddingRecencyPostprocessor,\n)\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom llama\\_index.storage.docstore import SimpleDocumentStore\nfrom llama\\_index.response.notebook\\_utils import display\\_response\n\n/Users/jerryliu/Programming/llama\\_index/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n\n## Parse Documents into Nodes, add to Docstore[\uf0c1](#parse-documents-into-nodes-add-to-docstore \"Permalink to this heading\")\n\nIn this example, there are 3 different versions of PG\u2019s essay. They are largely identical **except** for one specific section, which details the amount of funding they raised for Viaweb.\n\nV1: 50k, V2: 30k, V3: 10K\n\nV1: 2020-01-01, V2: 2020-02-03, V3: 2022-04-12\n\nThe idea is to encourage index to fetch the most recent info (which is V3)\n\n\\# load documents\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\ndef get\\_file\\_metadata(file\\_name: str):\n    \"\"\"Get file metadata.\"\"\"\n    if \"v1\" in file\\_name:\n        return {\"date\": \"2020-01-01\"}\n    elif \"v2\" in file\\_name:\n        return {\"date\": \"2020-02-03\"}\n    elif \"v3\" in file\\_name:\n        return {\"date\": \"2022-04-12\"}\n    else:\n        raise ValueError(\"invalid file\")\n\ndocuments \\= SimpleDirectoryReader(\n    input\\_files\\=\\[\n        \"test\\_versioned\\_data/paul\\_graham\\_essay\\_v1.txt\",\n        \"test\\_versioned\\_data/paul\\_graham\\_essay\\_v2.txt\",\n        \"test\\_versioned\\_data/paul\\_graham\\_essay\\_v3.txt\",\n    \\],\n    file\\_metadata\\=get\\_file\\_metadata,\n).load\\_data()\n\n\\# define service context (wrapper container around current classes)\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=512)\n\n\\# use node parser in service context to parse into nodes\nnodes \\= service\\_context.node\\_parser.get\\_nodes\\_from\\_documents(documents)\n\n\\# add to docstore\ndocstore \\= SimpleDocumentStore()\ndocstore.add\\_documents(nodes)\n\nstorage\\_context \\= StorageContext.from\\_defaults(docstore\\=docstore)\n\nprint(documents\\[2\\].get\\_text())\n\n## Build Index[\uf0c1](#build-index \"Permalink to this heading\")\n\n\\# build index\nindex \\= VectorStoreIndex(nodes, storage\\_context\\=storage\\_context)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 84471 tokens\n\n## Define Recency Postprocessors[\uf0c1](#define-recency-postprocessors \"Permalink to this heading\")\n\nnode\\_postprocessor \\= FixedRecencyPostprocessor(service\\_context\\=service\\_context)\n\nnode\\_postprocessor\\_emb \\= EmbeddingRecencyPostprocessor(\n    service\\_context\\=service\\_context\n)\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\n\\# naive query\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=3,\n)\nresponse \\= query\\_engine.query(\n    \"How much did the author raise in seed funding from Idelle's husband\"\n    \" (Julian) for Viaweb?\",\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 1813 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 22 tokens\n\n\\# query using fixed recency node postprocessor\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=3, node\\_postprocessors\\=\\[node\\_postprocessor\\]\n)\nresponse \\= query\\_engine.query(\n    \"How much did the author raise in seed funding from Idelle's husband\"\n    \" (Julian) for Viaweb?\",\n)\n\n\\# query using embedding-based node postprocessor\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=3, node\\_postprocessors\\=\\[node\\_postprocessor\\_emb\\]\n)\nresponse \\= query\\_engine.query(\n    \"How much did the author raise in seed funding from Idelle's husband\"\n    \" (Julian) for Viaweb?\",\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 541 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 22 tokens\n\n## Query Index (Lower-Level Usage)[\uf0c1](#query-index-lower-level-usage \"Permalink to this heading\")\n\nIn this example we first get the full set of nodes from a query call, and then send to node postprocessor, and then finally synthesize response through a summary index.\n\nfrom llama\\_index import SummaryIndex\n\nquery\\_str \\= (\n    \"How much did the author raise in seed funding from Idelle's husband\"\n    \" (Julian) for Viaweb?\"\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=3, response\\_mode\\=\"no\\_text\"\n)\ninit\\_response \\= query\\_engine.query(\n    query\\_str,\n)\nresp\\_nodes \\= \\[n.node for n in init\\_response.source\\_nodes\\]\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 22 tokens\n\nsummary\\_index \\= SummaryIndex(resp\\_nodes)\nquery\\_engine \\= summary\\_index.as\\_query\\_engine(\n    node\\_postprocessors\\=\\[node\\_postprocessor\\]\n)\nresponse \\= query\\_engine.query(query\\_str)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 541 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 0 tokens"
}