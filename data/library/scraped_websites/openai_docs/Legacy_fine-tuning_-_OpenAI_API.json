{
    "metadata": {
        "type": "web",
        "url": "https://platform.openai.com/docs/guides/legacy-fine-tuning/classification-specific-metrics",
        "title": "Legacy fine-tuning - OpenAI API",
        "description": "Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform."
    },
    "text": "Learn how to customize a model for your application.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/introduction)\n\n## [Introduction](https://platform.openai.com/docs/guides/legacy-fine-tuning/introduction)\n\nOn August 22nd, 2023, we [announced](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) the deprecation of the /v1/fine-tunes API will take place on January 4th, 2024. We recommend all users migrate to the new /v1/fine\\_tuning/jobs API.\n\nFine-tuning lets you get more out of the models available through the API by providing:\n\n1.  Higher quality results than prompt design\n2.  Ability to train on more examples than can fit in a prompt\n3.  Token savings due to shorter prompts\n4.  Lower latency requests\n\nGPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"\n\nFine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. **Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore.** This saves costs and enables lower-latency requests.\n\nAt a high level, fine-tuning involves the following steps:\n\n1.  Prepare and upload training data\n2.  Train a new fine-tuned model\n3.  Use your fine-tuned model\n\nVisit our [pricing page](https://openai.com/api/pricing) to learn more about how fine-tuned model training and usage are billed.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/what-models-can-be-fine-tuned)\n\n## [What models can be fine-tuned?](https://platform.openai.com/docs/guides/legacy-fine-tuning/what-models-can-be-fine-tuned)\n\nThe now deprecated fine-tunes endpoint only supports the following base models: `davinci`, `curie`, `babbage`, and `ada`. These are the original models that do not have any instruction following training (like `text-davinci-003` does for example). You are also able to [continue fine-tuning a fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/continue-fine-tuning-from-a-fine-tuned-model) to add additional data without having to start from scratch.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/installation)\n\n## [Installation](https://platform.openai.com/docs/guides/legacy-fine-tuning/installation)\n\nWe recommend using our OpenAI command-line interface (CLI). To install this, run\n\n```\npip install --upgrade openai\n```\n\n(The following instructions work for version **0.9.4** and up. Additionally, the OpenAI CLI requires python 3.)\n\nSet your `OPENAI_API_KEY` environment variable by adding the following line into your shell initialization script (e.g. .bashrc, zshrc, etc.) or running it in the command line before the fine-tuning command:\n\n```\nexport OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/prepare-training-data)\n\n## [Prepare training data](https://platform.openai.com/docs/guides/legacy-fine-tuning/prepare-training-data)\n\nTraining data is how you teach GPT-3 what you'd like it to say.\n\nYour data must be a [JSONL](https://jsonlines.org/) document, where each line is a prompt-completion pair corresponding to a training example. You can use our [CLI data preparation tool](https://platform.openai.com/docs/guides/legacy-fine-tuning/cli-data-preparation-tool) to easily convert your data into this file format.\n\n```\n1\n2\n3\n4\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n...\n```\n\nDesigning your prompts and completions for fine-tuning is different from designing your prompts for use with our base models (Davinci, Curie, Babbage, Ada). In particular, while prompts for base models often consist of multiple examples (\"few-shot learning\"), for fine-tuning, each training example generally consists of a single input example and its associated output, without the need to give detailed instructions or include multiple examples in the same prompt.\n\nFor more detailed guidance on how to prepare training data for various tasks, please refer to our [preparing your dataset](https://platform.openai.com/docs/guides/legacy-fine-tuning/preparing-your-dataset) best practices.\n\nThe more training examples you have, the better. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/cli-data-preparation-tool)\n\n### [CLI data preparation tool](https://platform.openai.com/docs/guides/legacy-fine-tuning/cli-data-preparation-tool)\n\nWe developed a tool which validates, gives suggestions and reformats your data:\n\n```\nopenai tools fine_tunes.prepare_data -f <LOCAL_FILE>\n```\n\nThis tool accepts different formats, with the only requirement that they contain a prompt and a completion column/key. You can pass a **CSV, TSV, XLSX, JSON** or **JSONL** file, and it will save the output into a JSONL file ready for fine-tuning, after guiding you through the process of suggested changes.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/create-a-fine-tuned-model)\n\n## [Create a fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/create-a-fine-tuned-model)\n\nThe following assumes you've already prepared training data following the [above instructions](https://platform.openai.com/docs/guides/legacy-fine-tuning/prepare-training-data).\n\nStart your fine-tuning job using the OpenAI CLI:\n\n```\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>\n```\n\nWhere `BASE_MODEL` is the name of the base model you're starting from (ada, babbage, curie, or davinci). You can customize your fine-tuned model's name using the [suffix parameter](https://platform.openai.com/docs/guides/legacy-fine-tuning/customize-your-model-name).\n\nRunning the above command does several things:\n\n1.  Uploads the file using the [files API](https://platform.openai.com/docs/api-reference/files) (or uses an already-uploaded file)\n2.  Creates a fine-tune job\n3.  Streams events until the job is done (this often takes minutes, but can take hours if there are many jobs in the queue or your dataset is large)\n\nEvery fine-tuning job starts from a base model, which defaults to curie. The choice of model influences both the performance of the model and the cost of running your fine-tuned model. Your model can be one of: `ada`, `babbage`, `curie`, or `davinci`. Visit our [pricing page](https://openai.com/api/pricing/#faq-fine-tuning-pricing-calculation) for details on fine-tune rates.\n\nAfter you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size. If the event stream is interrupted for any reason, you can resume it by running:\n\n```\nopenai api fine_tunes.follow -i <YOUR_FINE_TUNE_JOB_ID>\n```\n\nWhen the job is done, it should display the name of the fine-tuned model.\n\nIn addition to creating a fine-tune job, you can also list existing jobs, retrieve the status of a job, or cancel a job.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# List all created fine-tunes\nopenai api fine_tunes.list\n\n# Retrieve the state of a fine-tune. The resulting object includes\n# job status (which can be one of pending, running, succeeded, or failed)\n# and other information\nopenai api fine_tunes.get -i <YOUR_FINE_TUNE_JOB_ID>\n\n# Cancel a job\nopenai api fine_tunes.cancel -i <YOUR_FINE_TUNE_JOB_ID>\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/use-a-fine-tuned-model)\n\n## [Use a fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/use-a-fine-tuned-model)\n\nWhen a job has succeeded, the `fine_tuned_model` field will be populated with the name of the model. You may now specify this model as a parameter to our [Completions API](https://platform.openai.com/docs/api-reference/completions), and make requests to it using the [Playground](https://platform.openai.com/playground).\n\nAfter your job first completes, it may take several minutes for your model to become ready to handle requests. If completion requests to your model time out, it is likely because your model is still being loaded. If this happens, try again in a few minutes.\n\nYou can start making requests by passing the model name as the `model` parameter of a completion request:\n\nOpenAI CLI:\n\n```\nopenai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>\n```\n\ncURL:\n\n```\n1\n2\n3\n4\ncurl https://api.openai.com/v1/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": YOUR_PROMPT, \"model\": FINE_TUNED_MODEL}'\n```\n\nPython:\n\n```\n1\n2\n3\n4\nimport openai\nopenai.Completion.create(\n    model=FINE_TUNED_MODEL,\n    prompt=YOUR_PROMPT)\n```\n\nNode.js:\n\n```\n1\n2\n3\n4\nconst response = await openai.createCompletion({\n  model: FINE_TUNED_MODEL\n  prompt: YOUR_PROMPT,\n});\n```\n\nYou may continue to use all the other [Completions](https://platform.openai.com/docs/api-reference/completions) parameters like `temperature`, `frequency_penalty`, `presence_penalty`, etc, on these requests to fine-tuned models.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/delete-a-fine-tuned-model)\n\n## [Delete a fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/delete-a-fine-tuned-model)\n\nTo delete a fine-tuned model, you must be designated an \"owner\" within your organization.\n\nOpenAI CLI:\n\n```\nopenai api models.delete -i <FINE_TUNED_MODEL>\n```\n\ncURL:\n\n```\ncurl -X \"DELETE\" https://api.openai.com/v1/models/<FINE_TUNED_MODEL> \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n```\n\nPython:\n\n```\nimport openai\nopenai.Model.delete(FINE_TUNED_MODEL)\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/preparing-your-dataset)\n\n## [Preparing your dataset](https://platform.openai.com/docs/guides/legacy-fine-tuning/preparing-your-dataset)\n\nFine-tuning is a powerful technique to create a new model that's specific to your use case. **Before fine-tuning your model, we strongly recommend reading these best practices and [specific guidelines](https://platform.openai.com/docs/guides/legacy-fine-tuning/specific-guidelines) for your use case below.**\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/data-formatting)\n\n## [Data formatting](https://platform.openai.com/docs/guides/legacy-fine-tuning/data-formatting)\n\nTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our base models, where you might input detailed instructions or multiple examples in a single prompt.\n\n*   Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is `\\n\\n###\\n\\n`. The separator should not appear elsewhere in any prompt.\n    \n*   Each completion should start with a whitespace due to our [tokenization](https://platform.openai.com/tokenizer), which tokenizes most words with a preceding whitespace.\n    \n*   Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be `\\n`, `###`, or any other token that does not appear in any completion.\n    \n*   For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.\n    \n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/general-best-practices)\n\n## [General best practices](https://platform.openai.com/docs/guides/legacy-fine-tuning/general-best-practices)\n\nFine-tuning performs better with more high-quality examples. To fine-tune a model that performs better than using a high-quality prompt with our base models, you should provide at least a few hundred high-quality examples, ideally vetted by human experts. From there, performance tends to linearly increase with every doubling of the number of examples. Increasing the number of examples is usually the best and most reliable way of improving performance.\n\nClassifiers are the easiest models to get started with. For classification problems we suggest using ada, which generally tends to perform only very slightly worse than more capable models once fine-tuned, whilst being significantly faster and cheaper.\n\nIf you are fine-tuning on a pre-existing dataset rather than writing prompts from scratch, be sure to manually review your data for offensive or inaccurate content if possible, or review as many random samples of the dataset as possible if it is large.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/specific-guidelines)\n\n## [Specific guidelines](https://platform.openai.com/docs/guides/legacy-fine-tuning/specific-guidelines)\n\nFine-tuning can solve a variety of problems, and the optimal way to use it may depend on your specific use case. Below, we've listed the most common use cases for fine-tuning and corresponding guidelines.\n\n*   [Classification](https://platform.openai.com/docs/guides/legacy-fine-tuning/classification)\n    *   [Is the model making untrue statements?](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-is-the-model-making-untrue-statements)\n    *   [Sentiment analysis](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-sentiment-analysis)\n    *   [Categorization for email triage](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-categorization-for-email-triage)\n*   [Conditional generation](https://platform.openai.com/docs/guides/legacy-fine-tuning/conditional-generation)\n    *   [Write an engaging ad based on a Wikipedia article](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-write-an-engaging-ad-based-on-a-wikipedia-article)\n    *   [Entity extraction](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-entity-extraction)\n    *   [Customer support chatbot](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-customer-support-chatbot)\n    *   [Product description based on a technical list of properties](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-product-description-based-on-a-technical-list-of-properties)\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/classification)\n\n### [Classification](https://platform.openai.com/docs/guides/legacy-fine-tuning/classification)\n\nIn classification problems, each input in the prompt should be classified into one of the predefined classes. For this type of problem, we recommend:\n\n*   Use a separator at the end of the prompt, e.g. `\\n\\n###\\n\\n`. Remember to also append this separator when you eventually make requests to your model.\n*   Choose classes that map to a single [token](https://platform.openai.com/tokenizer). At inference time, specify `max_tokens=1` since you only need the first token for classification.\n*   Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n*   Aim for at least ~100 examples per class\n*   To get class log probabilities you can specify `logprobs=5` (for 5 classes) when using your model\n*   Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-is-the-model-making-untrue-statements)\n\n#### [Case study: Is the model making untrue statements?](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-is-the-model-making-untrue-statements)\n\nLet's say you'd like to ensure that the text of the ads on your website mention the correct product and company. In other words, you want to ensure the model isn't making things up. You may want to fine-tune a classifier which filters out incorrect ads.\n\nThe dataset might look something like the following:\n\n```\n{\"prompt\":\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop shop for all your insurance needs!\\nSupported:\", \"completion\":\" yes\"}\n{\"prompt\":\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight teeth in weeks!\\nSupported:\", \"completion\":\" no\"}\n```\n\nIn the example above, we used a structured input containing the name of the company, the product, and the associated ad. As a separator we used `\\nSupported:` which clearly separated the prompt from the completion. With a sufficient number of examples, the separator doesn't make much of a difference (usually less than 0.4%) as long as it doesn't appear within the prompt or the completion.\n\nFor this use case we fine-tuned an ada model since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.\n\nNow we can query our model by making a Completion request.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax help\\nAd:Best advice in town!\\nSupported:\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'\n```\n\nWhich will return either `yes` or `no`.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-sentiment-analysis)\n\n#### [Case study: Sentiment analysis](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-sentiment-analysis)\n\nLet's say you'd like to get a degree to which a particular tweet is positive or negative. The dataset might look something like the following:\n\n```\n{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}\n{\"prompt\":\"@lakers disappoint for a third straight night https://t.co/38EFe43 ->\", \"completion\":\" negative\"}\n```\n\nOnce the model is fine-tuned, you can get back the log probabilities for the first completion token by setting `logprobs=2` on the completion request. The higher the probability for positive class, the higher the relative sentiment.\n\nNow we can query our model by making a Completion request.\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"https://t.co/f93xEd2 Excited to share my latest blog post! ->\",\n    \"max_tokens\": 1,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\"\n  }'\n```\n\nWhich will return:\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n    \"id\": \"cmpl-COMPLETION_ID\",\n    \"object\": \"text_completion\",\n    \"created\": 1589498378,\n    \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\",\n    \"choices\": [\n        {\n            \"logprobs\": {\n                \"text_offset\": [19],\n                \"token_logprobs\": [-0.03597255],\n                \"tokens\": [\" positive\"],\n                \"top_logprobs\": [\n                    {\n                        \" negative\": -4.9785037,\n                        \" positive\": -0.03597255\n                    }\n                ]\n            },\n\n            \"text\": \" positive\",\n            \"index\": 0,\n            \"finish_reason\": \"length\"\n        }\n    ]\n}\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-categorization-for-email-triage)\n\n#### [Case study: Categorization for Email triage](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-categorization-for-email-triage)\n\nLet's say you'd like to categorize incoming email into one of a large number of predefined categories. For classification into a large number of categories, we recommend you convert those categories into numbers, which will work well up to ~500 categories. We've observed that adding a space before the number sometimes slightly helps the performance, due to tokenization. You may want to structure your training data as follows:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:<date>\\nContent:<email_body>\\n\\n###\\n\\n\",\n    \"completion\": \" <numerical_category>\"\n}\n```\n\nFor example:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Subject: Update my address\\nFrom:Joe Doe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like to update my billing address to match my delivery address.\\n\\nPlease let me know once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\",\n    \"completion\": \" 4\"\n}\n```\n\nIn the example above we used an incoming email capped at 2043 tokens as input. (This allows for a 4 token separator and a one token completion, summing up to 2048.) As a separator we used `\\n\\n###\\n\\n` and we removed any occurrence of `###` within the email.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/conditional-generation)\n\n### [Conditional generation](https://platform.openai.com/docs/guides/legacy-fine-tuning/conditional-generation)\n\nConditional generation is a problem where the content needs to be generated given some kind of input. This includes paraphrasing, summarizing, entity extraction, product description writing given specifications, chatbots and many others. For this type of problem we recommend:\n\n*   Use a separator at the end of the prompt, e.g. `\\n\\n###\\n\\n`. Remember to also append this separator when you eventually make requests to your model.\n*   Use an ending token at the end of the completion, e.g. `END`\n*   Remember to add the ending token as a stop sequence during inference, e.g. `stop=[\" END\"]`\n*   Aim for at least ~500 examples\n*   Ensure that the prompt + completion doesn't exceed 2048 tokens, including the separator\n*   Ensure the examples are of high quality and follow the same desired format\n*   Ensure that the dataset used for finetuning is very similar in structure and type of task as what the model will be used for\n*   Using Lower learning rate and only 1-2 epochs tends to work better for these use cases\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-write-an-engaging-ad-based-on-a-wikipedia-article)\n\n#### [Case study: Write an engaging ad based on a Wikipedia article](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-write-an-engaging-ad-based-on-a-wikipedia-article)\n\nThis is a generative use case so you would want to ensure that the samples you provide are of the highest quality, as the fine-tuned model will try to imitate the style (and mistakes) of the given examples. A good starting point is around 500 examples. A sample dataset might look like this:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\",\n    \"completion\": \" <engaging ad> END\"\n}\n```\n\nFor example:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android smartphone developed by Samsung Electronics exclusively for the Japanese market. The phone was released in June 2017 and was sold by NTT Docomo. It runs on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh battery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but can be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel has a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-A53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of internal storage, expandable to 256GB via microSD. Aside from its software and hardware specifications, Samsung also introduced a unique a hole in the phone's shell to accommodate the Japanese perceived penchant for personalizing their mobile phones. The Galaxy Feel's battery was also touted as a major selling point since the market favors handsets with longer battery life. The device is also waterproof and supports 1seg digital broadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\",\n    \"completion\": \"Looking for a smartphone that can do it all? Look no further than Samsung Galaxy Feel! With a slim and sleek design, our latest smartphone features high-quality picture and video capabilities, as well as an award winning battery life. END\"\n}\n```\n\nHere we used a multi line separator, as Wikipedia articles contain multiple paragraphs and headings. We also used a simple end token, to ensure that the model knows when the completion should finish.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-entity-extraction)\n\n#### [Case study: Entity extraction](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-entity-extraction)\n\nThis is similar to a language transformation task. To improve the performance, it is best to either sort different extracted entities alphabetically or in the same order as they appear in the original text. This will help the model to keep track of all the entities which need to be generated in order. The dataset could look as follows:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"<any text, for example news article>\\n\\n###\\n\\n\",\n    \"completion\": \" <list of entities, separated by a newline> END\"\n}\n```\n\nFor example:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Portugal will be removed from the UK's green travel list from Tuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation of the so-called Indian variant\\\". It will join the amber list, meaning holidaymakers should not visit and returnees must isolate for 10 days...\\n\\n###\\n\\n\",\n    \"completion\": \" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"\n}\n```\n\nA multi-line separator works best, as the text will likely contain multiple lines. Ideally there will be a high diversity of the types of input prompts (news articles, Wikipedia pages, tweets, legal documents), which reflect the likely texts which will be encountered when extracting entities.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-customer-support-chatbot)\n\n#### [Case study: Customer support chatbot](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-customer-support-chatbot)\n\nA chatbot will normally contain relevant context about the conversation (order details), summary of the conversation so far as well as most recent messages. For this use case the same past conversation can generate multiple rows in the dataset, each time with a slightly different context, for every agent generation as a completion. This use case will require a few thousand examples, as it will likely deal with different types of requests, and customer issues. To ensure the performance is of high quality we recommend vetting the conversation samples to ensure the quality of agent messages. The summary can be generated with a separate text transformation fine tuned model. The dataset could look as follows:\n\n```\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent:\", \"completion\":\" <response2>\\n\"}\n{\"prompt\":\"Summary: <summary of the interaction so far>\\n\\nSpecific information:<for example order details in natural language>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer: <message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\", \"completion\":\" <response3>\\n\"}\n```\n\nHere we purposefully separated different types of input information, but maintained Customer Agent dialog in the same format between a prompt and a completion. All the completions should only be by the agent, and we can use `\\n` as a stop sequence when doing inference.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-product-description-based-on-a-technical-list-of-properties)\n\n#### [Case study: Product description based on a technical list of properties](https://platform.openai.com/docs/guides/legacy-fine-tuning/case-study-product-description-based-on-a-technical-list-of-properties)\n\nHere it is important to convert the input data into a natural language, which will likely lead to superior performance. For example, the following format:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Item=handbag, Color=army_green, price=$99, size=S->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}\n```\n\nWon't work as well as:\n\n```\n1\n2\n3\n4\n{\n    \"prompt\": \"Item is a handbag. Colour is army green. Price is midrange. Size is small.->\",\n    \"completion\": \" This stylish small green handbag will add a unique touch to your look, without costing you a fortune.\"\n}\n```\n\nFor high performance ensure that the completions were based on the description provided. If external content is often consulted, then adding such content in an automated way would improve the performance. If the description is based on images, it may help to use an algorithm to extract a textual description of the image. Since completions are only one sentence long, we can use `.` as the stop sequence during inference.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/advanced-usage)\n\n## [Advanced usage](https://platform.openai.com/docs/guides/legacy-fine-tuning/advanced-usage)\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/customize-your-model-name)\n\n## [Customize your model name](https://platform.openai.com/docs/guides/legacy-fine-tuning/customize-your-model-name)\n\nYou can add a suffix of up to 40 characters to your fine-tuned model name using the [suffix](https://platform.openai.com/docs/api-reference/fine-tunes/create#fine-tunes/create-suffix) parameter.\n\nOpenAI CLI:\n\n```\nopenai api fine_tunes.create -t test.jsonl -m ada --suffix \"custom model name\"\n```\n\nThe resulting name would be:\n\n```\nada:ft-your-org:custom-model-name-2022-02-15-04-21-04\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model)\n\n## [Analyzing your fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model)\n\nWe attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files:\n\nOpenAI CLI:\n\n```\nopenai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n```\n\nCURL:\n\n```\ncurl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > results.csv\n```\n\nThe `_results.csv` file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. In addition to the step number, each row contains the following fields corresponding to that step:\n\n*   **elapsed\\_tokens**: the number of tokens the model has seen so far (including repeats)\n*   **elapsed\\_examples**: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if `batch_size = 4`, each step will increase `elapsed_examples` by 4.\n*   **training\\_loss**: loss on the training batch\n*   **training\\_sequence\\_accuracy**: the percentage of **completions** in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a `batch_size` of 3, if your data contains the completions \\[\\[1, 2\\], \\[0, 5\\], \\[4, 2\\]\\] and the model predicted \\[\\[1, 1\\], \\[0, 5\\], \\[4, 2\\]\\], this accuracy will be 2/3 = 0.67\n*   **training\\_token\\_accuracy**: the percentage of **tokens** in the training batch that were correctly predicted by the model. For example, with a `batch_size` of 3, if your data contains the completions \\[\\[1, 2\\], \\[0, 5\\], \\[4, 2\\]\\] and the model predicted \\[\\[1, 1\\], \\[0, 5\\], \\[4, 2\\]\\], this accuracy will be 5/6 = 0.83\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/classification-specific-metrics)\n\n### [Classification specific metrics](https://platform.openai.com/docs/guides/legacy-fine-tuning/classification-specific-metrics)\n\nWe also provide the option of generating additional classification-specific metrics in the results file, such as accuracy and weighted F1 score. These metrics are periodically calculated against the full validation set and at the end of fine-tuning. You will see them as additional columns in your results file.\n\nTo enable this, set the parameter `--compute_classification_metrics`. Additionally, you must provide a validation file, and set either the `classification_n_classes` parameter, for multiclass classification, or `classification_positive_class`, for binary classification.\n\nOpenAI CLI:\n\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n# For multiclass classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes <N_CLASSES>\n\n# For binary classification\nopenai api fine_tunes.create \\\n  -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_OR_PATH> \\\n  -m <MODEL> \\\n  --compute_classification_metrics \\\n  --classification_n_classes 2 \\\n  --classification_positive_class <POSITIVE_CLASS_FROM_DATASET>\n```\n\nThe following metrics will be displayed in your [results file](https://platform.openai.com/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model) if you set `--compute_classification_metrics`:\n\n##### For multiclass classification\n\n*   **classification/accuracy**: accuracy\n*   **classification/weighted\\_f1\\_score**: weighted F-1 score\n\n##### For binary classification\n\nThe following metrics are based on a classification threshold of 0.5 (i.e. when the probability is > 0.5, an example is classified as belonging to the positive class.)\n\n*   **classification/accuracy**\n*   **classification/precision**\n*   **classification/recall**\n*   **classification/f{beta}**\n*   **classification/auroc** - AUROC\n*   **classification/auprc** - AUPRC\n\nNote that these evaluations assume that you are using text labels for classes that tokenize down to a single token, as described above. If these conditions do not hold, the numbers you get will likely be wrong.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/validation)\n\n### [Validation](https://platform.openai.com/docs/guides/legacy-fine-tuning/validation)\n\nYou can reserve some of your data for validation. A validation file has exactly the same format as a train file, and your train and validation data should be mutually exclusive.\n\nIf you include a validation file when creating your fine-tune job, the generated results file will include evaluations on how well the fine-tuned model performs against your validation data at periodic intervals during training.\n\nOpenAI CLI:\n\n```\n1\n2\n3\nopenai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> \\\n  -v <VALIDATION_FILE_ID_OR_PATH> \\\n  -m <MODEL>\n```\n\nIf you provided a validation file, we periodically calculate metrics on batches of validation data during training time. You will see the following additional metrics in your results file:\n\n*   **validation\\_loss**: loss on the validation batch\n*   **validation\\_sequence\\_accuracy**: the percentage of completions in the validation batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a `batch_size` of 3, if your data contains the completion \\[\\[1, 2\\], \\[0, 5\\], \\[4, 2\\]\\] and the model predicted \\[\\[1, 1\\], \\[0, 5\\], \\[4, 2\\]\\], this accuracy will be 2/3 = 0.67\n*   **validation\\_token\\_accuracy**: the percentage of tokens in the validation batch that were correctly predicted by the model. For example, with a `batch_size` of 3, if your data contains the completion \\[\\[1, 2\\], \\[0, 5\\], \\[4, 2\\]\\] and the model predicted \\[\\[1, 1\\], \\[0, 5\\], \\[4, 2\\]\\], this accuracy will be 5/6 = 0.83\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/hyperparameters)\n\n## [Hyperparameters](https://platform.openai.com/docs/guides/legacy-fine-tuning/hyperparameters)\n\nWe've picked default hyperparameters that work well across a range of use cases. The only required parameter is the training file.\n\nThat said, tweaking the hyperparameters used for fine-tuning can often lead to a model that produces higher quality output. In particular, you may want to configure the following:\n\n*   `model`: The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\", or \"davinci\". To learn more about these models, see the [Models](https://platform.openai.com/docs/models) documentation.\n*   `n_epochs` - defaults to 4. The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.\n*   `batch_size` - defaults to ~0.2% of the number of examples in the training set, capped at 256. The batch size is the number of training examples used to train a single forward and backward pass. In general, we've found that larger batch sizes tend to work better for larger datasets.\n*   `learning_rate_multiplier` - defaults to 0.05, 0.1, or 0.2 depending on final `batch_size`. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this multiplier. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. Empirically, we've found that larger learning rates often perform better with larger batch sizes.\n*   `compute_classification_metrics` - defaults to False. If True, for fine-tuning for classification tasks, computes classification-specific metrics (accuracy, F-1 score, etc) on the validation set at the end of every epoch.\n\nTo configure these additional hyperparameters, pass them in via command line flags on the OpenAI CLI, for example:\n\n```\n1\n2\n3\n4\nopenai api fine_tunes.create \\\n  -t file-JD89ePi5KMsB3Tayeli5ovfW \\\n  -m ada \\\n  --n_epochs 1\n```\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/continue-fine-tuning-from-a-fine-tuned-model)\n\n## [Continue fine-tuning from a fine-tuned model](https://platform.openai.com/docs/guides/legacy-fine-tuning/continue-fine-tuning-from-a-fine-tuned-model)\n\nIf you have already fine-tuned a model for your task and now have additional training data that you would like to incorporate, you can continue fine-tuning from the model. This creates a model that has learned from all of the training data without having to re-train from scratch.\n\nTo do this, pass in the fine-tuned model name when creating a new fine-tuning job (e.g. `-m curie:ft-<org>-<date>`). Other training parameters do not have to be changed, however if your new training data is much smaller than your previous training data, you may find it useful to reduce `learning_rate_multiplier` by a factor of 2 to 4.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/weights-biases)\n\n## [Weights & Biases](https://platform.openai.com/docs/guides/legacy-fine-tuning/weights-biases)\n\nYou can sync your fine-tunes with [Weights & Biases](https://wandb.me/openai-docs) to track experiments, models, and datasets.\n\nTo get started, you will need a [Weights & Biases](https://wandb.me/openai-docs) account and a paid OpenAI plan. To make sure you are using the lastest version of `openai` and `wandb`, run:\n\n```\npip install --upgrade openai wandb\n```\n\nTo sync your fine-tunes with Weights & Biases, run:\n\nYou can read the [Weights & Biases documentation](https://wandb.me/openai-docs) for more information on this integration.\n\n[](https://platform.openai.com/docs/guides/legacy-fine-tuning/example-notebooks)\n\n## [Example notebooks](https://platform.openai.com/docs/guides/legacy-fine-tuning/example-notebooks)\n\n[finetuning-classification.ipynb](https://cookbook.openai.com/examples/Fine-tuned_classification.ipynb)\n\nThis notebook will demonstrate how to fine-tune a model that can classify whether a piece of input text is related to Baseball or Hockey. We will perform this task in four steps in the [notebook](https://cookbook.openai.com/examples/fine-tuned_classification):\n\n1.  **Data exploration** will give an overview of the data source and what an example looks like\n2.  **Data preparation** will turn our data source into a jsonl file that can be used for fine-tuning\n3.  **Fine-tuning** will kick off the fine-tuning job and explain the resulting model's performance\n4.  **Using the model** will demonstrate making requests to the fine-tuned model to get predictions.\n\nThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.\n\nTo create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".\n\nWe will perform this task in three notebooks:\n\n1.  [The first notebook](https://cookbook.openai.com/examples/fine-tuned_qa/olympics-1-collect-data) focuses on collecting recent data, which GPT-3 didn't see during it's pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.\n2.  [The second notebook](https://cookbook.openai.com/examples/fine-tuned_qa/olympics-2-create-qa) will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.\n3.  [The third notebook](https://cookbook.openai.com/examples/fine-tuned_qa/olympics-3-train-qa) will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not."
}