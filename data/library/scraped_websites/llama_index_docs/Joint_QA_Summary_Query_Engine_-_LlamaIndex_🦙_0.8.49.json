{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/query_engine/JointQASummary.html",
        "title": "Joint QA Summary Query Engine - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index.composability.joint\\_qa\\_summary import (\n    QASummaryQueryEngineBuilder,\n)\nfrom llama\\_index import SimpleDirectoryReader, ServiceContext, LLMPredictor\nfrom llama\\_index.response.notebook\\_utils import display\\_response\nfrom llama\\_index.llms import OpenAI\n\nreader \\= SimpleDirectoryReader(\"../paul\\_graham\\_essay/data\")\ndocuments \\= reader.load\\_data()\n\ngpt4 \\= OpenAI(temperature\\=0, model\\=\"gpt-4\")\nservice\\_context\\_gpt4 \\= ServiceContext.from\\_defaults(llm\\=gpt4, chunk\\_size\\=1024)\n\nchatgpt \\= OpenAI(temperature\\=0, model\\=\"gpt-3.5-turbo\")\nservice\\_context\\_chatgpt \\= ServiceContext.from\\_defaults(\n    llm\\=chatgpt, chunk\\_size\\=1024\n)\n\nWARNING:llama\\_index.llm\\_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\nUnknown max input size for gpt-3.5-turbo, using defaults.\n\n\\# NOTE: can also specify an existing docstore, service context, summary text, qa\\_text, etc.\nquery\\_engine\\_builder \\= QASummaryQueryEngineBuilder(\n    service\\_context\\=service\\_context\\_gpt4\n)\nquery\\_engine \\= query\\_engine\\_builder.build\\_from\\_documents(documents)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 0 tokens\n\nresponse \\= query\\_engine.query(\n    \"Can you give me a summary of the author's life?\",\n)\n\nINFO:llama\\_index.query\\_engine.router\\_query\\_engine:Selecting query engine 1 because: This choice is relevant because it is specifically for summarization queries, which matches the request for a summary of the author's life..\nSelecting query engine 1 because: This choice is relevant because it is specifically for summarization queries, which matches the request for a summary of the author's life..\nINFO:llama\\_index.indices.common\\_tree.base:> Building index from nodes: 6 chunks\n> Building index from nodes: 6 chunks\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1012 tokens\n> \\[get\\_response\\] Total LLM token usage: 1012 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 23485 tokens\n> \\[get\\_response\\] Total LLM token usage: 23485 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\nresponse \\= query\\_engine.query(\n    \"What did the author do growing up?\",\n)\n\nINFO:llama\\_index.query\\_engine.router\\_query\\_engine:Selecting query engine 0 because: This choice is relevant because it involves retrieving specific context from documents, which is needed to answer the question about the author's activities growing up..\nSelecting query engine 0 because: This choice is relevant because it involves retrieving specific context from documents, which is needed to answer the question about the author's activities growing up..\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1893 tokens\n> \\[get\\_response\\] Total LLM token usage: 1893 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\nresponse \\= query\\_engine.query(\n    \"What did the author do during his time in art school?\",\n)\n\nINFO:llama\\_index.query\\_engine.router\\_query\\_engine:Selecting query engine 0 because: This choice is relevant because it involves retrieving specific context from documents, which is needed to answer the question about the author's activities in art school..\nSelecting query engine 0 because: This choice is relevant because it involves retrieving specific context from documents, which is needed to answer the question about the author's activities in art school..\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 12 tokens\n> \\[retrieve\\] Total embedding token usage: 12 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1883 tokens\n> \\[get\\_response\\] Total LLM token usage: 1883 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens"
}