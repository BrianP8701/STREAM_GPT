{
    "metadata": {
        "type": "repo",
        "path": "stream_gpt/utils/inference.py"
    },
    "text": "import openai\nimport os\nimport json\nimport warnings\nfrom stream_gpt.constants import prompts, function_schemas\nfrom stream_gpt.constants.keys import OPENAI_API_KEY\nfrom stream_gpt.utils import helpers\n\nopenai.api_key = OPENAI_API_KEY\n\ndef chat_with_gpt3_turbo(messages, temperature=0.0):\n    if type(messages) == str: # In case someone accidentally passes in a string instead of a list of messages\n        warnings.warn(\"chat_with_gpt3_turbo() expects a list of messages, not a string.\")\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    for message in messages:\n        message[\"content\"] = message[\"content\"].encode('latin-1', errors='ignore').decode('latin-1')\n    completion = openai.ChatCompletion.create(model='gpt-3.5-turbo-16k',messages=messages,temperature=temperature)\n    return completion\n\ndef function_call_with_gpt3_turbo(messages, functions, function_call='auto', temperature=0.0):\n    if type(messages) == str: # In case someone accidentally passes in a string instead of a list of messages\n        warnings.warn(\"chat_with_gpt3_turbo() expects a list of messages, not a string.\")\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    for message in messages:\n        message[\"content\"] = message[\"content\"].encode('latin-1', errors='ignore').decode('latin-1')\n    completion = openai.ChatCompletion.create(model='gpt-3.5-turbo-16k',messages=messages,temperature=temperature,functions=functions, function_call=function_call)\n    return completion\n\ndef chat_with_gpt3_instruct(prompt, temperature=0.0):\n    if type(prompt) == list: # In case someone accidentally passes in a list of messages instead of a prompt\n        warnings.warn(\"chat_with_gpt3_instruct() expects a prompt, not a list of messages.\")\n        prompt = '\\n'.join(f'{message[\"role\"]}: {message[\"content\"]}' for message in prompt)\n    response = openai.Completion.create(model=\"gpt-3.5-turbo-instruct\",prompt=prompt, temperature=temperature)\n    return response\n\ndef summarize(user_prompt, text, model=\"gpt-3.5-turbo-instruct\"):\n    if model == \"gpt-3.5-turbo-instruct\":\n        prompt = f'{prompts.KEYWORD_SUMMARIZATION} {user_prompt}\\n{text}'\n        response = chat_with_gpt3_instruct(prompt).choices[0].text\n    if model == \"gpt-3.5-turbo-16k\":\n        messages=[\n            {\"role\": \"system\", \"content\": f'{prompts.SUMMARIZATION} {user_prompt}'},\n            {\"role\": \"user\", \"content\": text}\n        ]\n        response = chat_with_gpt3_turbo(messages).choices[0]['content']\n    return response\n\ndef rank_categories(user_prompt, categories, model='gpt-3.5-turbo-16k'):\n    '''\n        Compare and rank a list of categories based on how well they match the user's prompt.\n        Must use a model that supports function calling.\n        \n        Args:\n        - user_prompt (string): Prompt from user\n        - categories  (list): List of categories to compare and rank\n        - model       (string): Model to use for inference\n        \n        Returns:\n        - ranked_categories (list): List of categories ranked by relevance\n    '''\n    messages = [{\"role\": \"user\", \"content\": user_prompt},\n                {\"role\": \"user\", \"content\": helpers.concatenate_with_indices(categories)}]\n    response = function_call_with_gpt3_turbo(messages, function_schemas.RANK_CATEGORIES, function_call={'name':'rank_categories'}).choices[0]['message']['function_call']['arguments']\n    return(json.loads(response))\n    \ndef choose_best_scraped_text(samples):\n    '''\n        When using pdf scrapers, sometimes noise can happen. Here we ask ChatGPT \n        to choose the best sample from a list of samples.\n        \n        Args:\n        - samples (list): List of sample from each scraper. Each sample is a string.\n    '''\n    user_prompt = ''\n    index = 1\n    for sample in samples:\n        user_prompt += f'{index}: {sample}\\n'\n    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n    response = function_call_with_gpt3_turbo(messages, function_schemas.CHOOSE_BEST_SAMPLE, function_call={'name':'choose_best_sample'}).choices[0]['message']['function_call']['arguments']\n    return(json.loads(response)['best_sample'])\n\n\n"
}