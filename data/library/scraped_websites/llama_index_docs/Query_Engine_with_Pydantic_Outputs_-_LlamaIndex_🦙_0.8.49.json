{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html",
        "title": "Query Engine with Pydantic Outputs - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nEvery query engine has support for integrated structured responses using the following `response_mode`s in `RetrieverQueryEngine`:\n\n*   `refine`\n    \n*   `compact`\n    \n*   `tree_summarize`\n    \n*   `accumulate` (beta, requires extra parsing to convert to objects)\n    \n*   `compact_accumulate` (beta, requires extra parsing to convert to objects)\n    \n\nIn this notebook, we walk through a small example demonstrating the usage.\n\nUnder the hood, every LLM response will be a pydantic object. If that response needs to be refined or summarized, it is converted into a JSON string for the next response. Then, the final response is returned as a pydantic object.\n\n**NOTE:** This can technically work with any LLM, but non-openai is support is still in development and considered beta.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nimport os\nimport openai\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-...\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\nfrom llama\\_index import SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\n\n### Create our Pydanitc Output Object[\uf0c1](#create-our-pydanitc-output-object \"Permalink to this heading\")\n\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Biography(BaseModel):\n    \"\"\"Data model for a biography.\"\"\"\n\n    name: str\n    best\\_known\\_for: List\\[str\\]\n    extra\\_info: str\n\n## Create the Index + Query Engine (OpenAI)[\uf0c1](#create-the-index-query-engine-openai \"Permalink to this heading\")\n\nWhen using OpenAI, the function calling API will be leveraged for reliable structured outputs.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\", temperature\\=0.1)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    output\\_cls\\=Biography, response\\_mode\\=\"compact\"\n)\n\nresponse \\= query\\_engine.query(\"Who is Paul Graham?\")\n\nprint(response.name)\nprint(response.best\\_known\\_for)\nprint(response.extra\\_info)\n\nPaul Graham\n\\['working on Bel', 'co-founding Viaweb', 'creating the programming language Arc'\\]\nPaul Graham is a computer scientist, entrepreneur, and writer. He is best known for his work on Bel, a programming language, and for co-founding Viaweb, an early web application company that was later acquired by Yahoo. Graham also created the programming language Arc. He has written numerous essays on topics such as startups, programming, and life.\n\n\\# get the full pydanitc object\nprint(type(response.response))\n\n<class '\\_\\_main\\_\\_.Biography'>\n\n## Create the Index + Query Engine (Non-OpenAI, Beta)[\uf0c1](#create-the-index-query-engine-non-openai-beta \"Permalink to this heading\")\n\nWhen using an LLM that does not support function calling, we rely on the LLM to write the JSON itself, and we parse the JSON into the proper pydantic object.\n\nimport os\n\nos.environ\\[\"ANTHROPIC\\_API\\_KEY\"\\] \\= \"sk-...\"\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import Anthropic\n\nllm \\= Anthropic(model\\=\"claude-instant-1.2\", temperature\\=0.1)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    output\\_cls\\=Biography, response\\_mode\\=\"tree\\_summarize\"\n)\n\nresponse \\= query\\_engine.query(\"Who is Paul Graham?\")\n\nprint(response.name)\nprint(response.best\\_known\\_for)\nprint(response.extra\\_info)\n\nPaul Graham\n\\['Co-founder of Y Combinator', 'Essayist and programmer'\\]\nHe is known for creating Viaweb, one of the first web application builders, and for founding Y Combinator, one of the world's top startup accelerators. Graham has also written extensively about technology, investing, and philosophy.\n\n\\# get the full pydanitc object\nprint(type(response.response))\n\n<class '\\_\\_main\\_\\_.Biography'>\n\n## Accumulate Examples (Beta)[\uf0c1](#accumulate-examples-beta \"Permalink to this heading\")\n\nAccumulate with pydantic objects requires some extra parsing. This is still a beta feature, but it\u2019s still possible to get accumulate pydantic objects.\n\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Company(BaseModel):\n    \"\"\"Data model for a companies mentioned.\"\"\"\n\n    company\\_name: str\n    context\\_info: str\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\", temperature\\=0.1)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    output\\_cls\\=Company, response\\_mode\\=\"accumulate\"\n)\n\nresponse \\= query\\_engine.query(\"What companies are mentioned in the text?\")\n\nIn accumulate, responses are separated by a default separator, and prepended with a prefix.\n\ncompanies \\= \\[\\]\n\n\\# split by the default separator\nfor response\\_str in str(response).split(\"\\\\n\\---------------------\\\\n\"):\n    \\# remove the prefix --  every response starts like \\`Response 1: {...}\\`\n    \\# so, we find the first bracket and remove everything before it\n    response\\_str \\= response\\_str\\[response\\_str.find(\"{\") :\\]\n    companies.append(Company.parse\\_raw(response\\_str))\n\n\\[Company(company\\_name='Yahoo', context\\_info='Yahoo bought us'), Company(company\\_name='Yahoo', context\\_info=\"I'd been meaning to since Yahoo bought us\")\\]"
}