{
    "metadata": {
        "type": "web",
        "url": "https://platform.openai.com/docs/guides/safety-best-practices/allow-users-to-report-issues",
        "title": "Safety best practices - OpenAI API",
        "description": "Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform."
    },
    "text": "[](https://platform.openai.com/docs/guides/safety-best-practices/safety-best-practices)\n\n## [Safety best practices](https://platform.openai.com/docs/guides/safety-best-practices/safety-best-practices)\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/use-our-free-moderation-api)\n\n### [Use our free Moderation API](https://platform.openai.com/docs/guides/safety-best-practices/use-our-free-moderation-api)\n\nOpenAI's [Moderation API](https://platform.openai.com/docs/guides/moderation) is free-to-use and can help reduce the frequency of unsafe content in your completions. Alternatively, you may wish to develop your own content filtration system tailored to your use case.\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/adversarial-testing)\n\n### [Adversarial testing](https://platform.openai.com/docs/guides/safety-best-practices/adversarial-testing)\n\nWe recommend \u201cred-teaming\u201d your application to ensure it's robust to adversarial input. Test your product over a wide range of inputs and user behaviors, both a representative set and those reflective of someone trying to \u2018break' your application. Does it wander off topic? Can someone easily redirect the feature via prompt injections, e.g. \u201cignore the previous instructions and do this instead\u201d?\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/human-in-the-loop-hitl)\n\n### [Human in the loop (HITL)](https://platform.openai.com/docs/guides/safety-best-practices/human-in-the-loop-hitl)\n\nWherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation. Humans should be aware of the limitations of the system, and have access to any information needed to verify the outputs (for example, if the application summarizes notes, a human should have easy access to the original notes to refer back).\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/prompt-engineering)\n\n### [Prompt engineering](https://platform.openai.com/docs/guides/safety-best-practices/prompt-engineering)\n\n\u201cPrompt engineering\u201d can help constrain the topic and tone of output text. This reduces the chance of producing undesired content, even if a user tries to produce it. Providing additional context to the model (such as by giving a few high-quality examples of desired behavior prior to the new input) can make it easier to steer model outputs in desired directions.\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/know-your-customer-kyc)\n\n### [\u201cKnow your customer\u201d (KYC)](https://platform.openai.com/docs/guides/safety-best-practices/know-your-customer-kyc)\n\nUsers should generally need to register and log-in to access your service. Linking this service to an existing account, such as a Gmail, LinkedIn, or Facebook log-in, may help, though may not be appropriate for all use-cases. Requiring a credit card or ID card reduces risk further.\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/constrain-user-input-and-limit-output-tokens)\n\n### [Constrain user input and limit output tokens](https://platform.openai.com/docs/guides/safety-best-practices/constrain-user-input-and-limit-output-tokens)\n\nLimiting the amount of text a user can input into the prompt helps avoid prompt injection. Limiting the number of output tokens helps reduce the chance of misuse.\n\nNarrowing the ranges of inputs or outputs, especially drawn from trusted sources, reduces the extent of misuse possible within an application.\n\nAllowing user inputs through validated dropdown fields (e.g., a list of movies on Wikipedia) can be more secure than allowing open-ended text inputs.\n\nReturning outputs from a validated set of materials on the backend, where possible, can be safer than returning novel generated content (for instance, routing a customer query to the best-matching existing customer support article, rather than attempting to answer the query from-scratch).\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/allow-users-to-report-issues)\n\n### [Allow users to report issues](https://platform.openai.com/docs/guides/safety-best-practices/allow-users-to-report-issues)\n\nUsers should generally have an easily-available method for reporting improper functionality or other concerns about application behavior (listed email address, ticket submission method, etc). This method should be monitored by a human and responded to as appropriate.\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/understand-and-communicate-limitations)\n\n### [Understand and communicate limitations](https://platform.openai.com/docs/guides/safety-best-practices/understand-and-communicate-limitations)\n\nFrom hallucinating inaccurate information, to offensive outputs, to bias, and much more, language models may not be suitable for every use case without significant modifications. Consider whether the model is fit for your purpose, and evaluate the performance of the API on a wide range of potential inputs in order to identify cases where the API's performance might drop. Consider your customer base and the range of inputs that they will be using, and ensure their expectations are calibrated appropriately.\n\nSafety and security are very important to us at OpenAI.\n\nIf in the course of your development you do notice any safety or security issues with the API or anything else related to OpenAI, please submit these through our [Coordinated Vulnerability Disclosure Program](https://openai.com/security/disclosure/).\n\n[](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)\n\n## [End-user IDs](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)\n\nSending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application.\n\nThe IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. If you offer a preview of your product to non-logged in users, you can send a session ID instead.\n\nYou can include end-user IDs in your API requests via the `user` parameter as follows:\n\n```\n1\n2\n3\n4\n5\n6\nresponse = openai.Completion.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=\"This is a test\",\n  max_tokens=5,\n  user=\"user123456\"\n)\n```"
}