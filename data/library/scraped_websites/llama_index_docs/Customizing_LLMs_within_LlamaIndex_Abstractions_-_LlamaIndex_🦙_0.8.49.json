{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/usage_custom.html",
        "title": "Customizing LLMs within LlamaIndex Abstractions - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\nBy default, we use OpenAI\u2019s `gpt-3.5-turbo` model. But you may choose to customize the underlying LLM being used.\n\nBelow we show a few examples of LLM customization. This includes\n\n*   changing the underlying LLM\n    \n*   changing the number of output tokens (for OpenAI, Cohere, or AI21)\n    \n*   having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n    \n\n## Example: Changing the underlying LLM[\uf0c1](#example-changing-the-underlying-llm \"Permalink to this heading\")\n\nAn example snippet of customizing the LLM being used is shown below. In this example, we use `gpt-4` instead of `gpt-3.5-turbo`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-instruct`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.\n\nNote that you may also plug in any LLM shown on Langchain\u2019s [LLM](https://python.langchain.com/docs/integrations/llms/) page.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\\# alternatively\n\\# from langchain.llms import ...\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\n\n\\# define LLM\nllm \\= OpenAI(temperature\\=0.1, model\\=\"gpt-4\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build index\nindex \\= KeywordTableIndex.from\\_documents(documents, service\\_context\\=service\\_context)\n\n\\# get response from query\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do after his time at Y Combinator?\")\n\n## Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)[\uf0c1](#example-changing-the-number-of-output-tokens-for-openai-cohere-ai21 \"Permalink to this heading\")\n\nThe number of output tokens is usually set to some low number by default (for instance, with OpenAI the default is 256).\n\nFor OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter (or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\n\n\\# define LLM\nllm \\= OpenAI(temperature\\=0, model\\=\"text-davinci-002\", max\\_tokens\\=512)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n## Example: Explicitly configure `context_window` and `num_output`[\uf0c1](#example-explicitly-configure-context-window-and-num-output \"Permalink to this heading\")\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n\nfrom llama\\_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama\\_index.llms import OpenAI\n\\# alternatively\n\\# from langchain.llms import ...\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\n\n\\# set context window\ncontext\\_window \\= 4096\n\\# set number of output tokens\nnum\\_output \\= 256\n\n\\# define LLM\nllm \\= OpenAI(\n    temperature\\=0,\n    model\\=\"text-davinci-002\",\n    max\\_tokens\\=num\\_output,\n)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm,\n    context\\_window\\=context\\_window,\n    num\\_output\\=num\\_output,\n)\n\n## Example: Using a HuggingFace LLM[\uf0c1](#example-using-a-huggingface-llm \"Permalink to this heading\")\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.\n\nMany open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\nBelow, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found [here](https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b).\n\nfrom llama\\_index.prompts import PromptTemplate\n\nsystem\\_prompt \\= \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n\\- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n\\- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n\\- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n\\- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\n\\# This will wrap the default prompts that are internal to llama-index\nquery\\_wrapper\\_prompt \\= PromptTemplate(\"<|USER|>{query\\_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama\\_index.llms import HuggingFaceLLM\nllm \\= HuggingFaceLLM(\n    context\\_window\\=4096,\n    max\\_new\\_tokens\\=256,\n    generate\\_kwargs\\={\"temperature\": 0.7, \"do\\_sample\": False},\n    system\\_prompt\\=system\\_prompt,\n    query\\_wrapper\\_prompt\\=query\\_wrapper\\_prompt,\n    tokenizer\\_name\\=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model\\_name\\=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device\\_map\\=\"auto\",\n    stopping\\_ids\\=\\[50278, 50279, 50277, 1, 0\\],\n    tokenizer\\_kwargs\\={\"max\\_length\": 4096},\n    \\# uncomment this if using CUDA to reduce memory usage\n    \\# model\\_kwargs={\"torch\\_dtype\": torch.float16}\n)\nservice\\_context \\= ServiceContext.from\\_defaults(\n    chunk\\_size\\=1024,\n    llm\\=llm,\n)\n\nSome models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:\n\nHuggingFaceLLM(\n    ...\n    tokenizer\\_outputs\\_to\\_remove\\=\\[\"token\\_type\\_ids\"\\]\n)\n\nA full API reference can be found [here](https://docs.llamaindex.ai/en/stable/api_reference/llms/huggingface.html).\n\nSeveral example notebooks are also listed below:\n\n*   [StableLM](https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html)\n    \n*   [Camel](https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.html)\n    \n\n## Example: Using a Custom LLM Model - Advanced[\uf0c1](#example-using-a-custom-llm-model-advanced \"Permalink to this heading\")\n\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface) You will be responsible for passing the text to the model and returning the newly generated tokens.\n\nNote that for a completely private experience, also setup a local embedding model (example here).\n\nHere is a small example using locally running facebook/OPT model and Huggingface\u2019s pipeline abstraction:\n\nimport torch\nfrom transformers import pipeline\nfrom typing import Optional, List, Mapping, Any\n\nfrom llama\\_index import (\n    ServiceContext,\n    SimpleDirectoryReader,\n    SummaryIndex\n)\nfrom llama\\_index.callbacks import CallbackManager\nfrom llama\\_index.llms import (\n    CustomLLM,\n    CompletionResponse,\n    CompletionResponseGen,\n    LLMMetadata,\n)\nfrom llama\\_index.llms.base import llm\\_completion\\_callback\n\n\\# set context window size\ncontext\\_window \\= 2048\n\\# set number of output tokens\nnum\\_output \\= 256\n\n\\# store the pipeline/model outside of the LLM class to avoid memory issues\nmodel\\_name \\= \"facebook/opt-iml-max-30b\"\npipeline \\= pipeline(\"text-generation\", model\\=model\\_name, device\\=\"cuda:0\", model\\_kwargs\\={\"torch\\_dtype\":torch.bfloat16})\n\nclass OurLLM(CustomLLM):\n\n    @property\n    def metadata(self) \\-> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context\\_window\\=context\\_window,\n            num\\_output\\=num\\_output,\n            model\\_name\\=model\\_name\n        )\n\n    @llm\\_completion\\_callback()\n    def complete(self, prompt: str, \\*\\*kwargs: Any) \\-> CompletionResponse:\n        prompt\\_length \\= len(prompt)\n        response \\= pipeline(prompt, max\\_new\\_tokens\\=num\\_output)\\[0\\]\\[\"generated\\_text\"\\]\n\n        \\# only return newly generated tokens\n        text \\= response\\[prompt\\_length:\\]\n        return CompletionResponse(text\\=text)\n\n    @llm\\_completion\\_callback()\n    def stream\\_complete(self, prompt: str, \\*\\*kwargs: Any) \\-> CompletionResponseGen:\n        raise NotImplementedError()\n\n\\# define our LLM\nllm \\= OurLLM()\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm,\n    embed\\_model\\=\"local:BAAI/bge-base-en-v1.5\",\n    context\\_window\\=context\\_window,\n    num\\_output\\=num\\_output\n)\n\n\\# Load the your data\ndocuments \\= SimpleDirectoryReader('./data').load\\_data()\nindex \\= SummaryIndex.from\\_documents(documents, service\\_context\\=service\\_context)\n\n\\# Query and print response\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"<query\\_text>\")\nprint(response)\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it\u2019s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\nA list of all default internal prompts is available [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py), and chat-specific prompts are listed [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py). You can also implement your own custom prompts, as described here."
}