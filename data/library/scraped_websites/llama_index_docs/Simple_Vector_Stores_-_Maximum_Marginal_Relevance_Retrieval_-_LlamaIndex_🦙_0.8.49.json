{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoMMR.html",
        "title": "Simple Vector Stores - Maximum Marginal Relevance Retrieval - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nThis notebook explores the use of MMR retrieval \\[[1](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)\\]. By using maximum marginal relevance, one can iteratively find documents that are dissimilar to previous results. It has been shown to improve performance for LLM retrievals \\[[2](https://arxiv.org/pdf/2211.13892.pdf)\\].\n\nThe maximum marginal relevance algorithm is as follows: $$ \\\\text{{MMR}} = \\\\arg\\\\max\\_{d\\_i \\\\in D \\\\setminus R} \\[ \\\\lambda \\\\cdot Sim\\_1(d\\_i, q) - (1 - \\\\lambda) \\\\cdot \\\\max\\_{d\\_j \\\\in R} Sim\\_2(d\\_i, d\\_j) \\] $$\n\nHere, D is the set of all candidate documents, R is the set of already selected documents, q is the query, $Sim\\_1$ is the similarity function between a document and the query, and $Sim\\_2$ is the similarity function between two documents. $d\\_i$ and $d\\_j$ are documents in D and R respectively.\n\nThe parameter \u03bb (mmr\\_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr\\_threshold is close to 1, more emphasis is put on relevance, while a mmr\\_threshold close to 0 puts more emphasis on diversity.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\n\\# llama\\_index/docs/examples/data/paul\\_graham\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n\\# To use mmr, set it as a vector\\_store\\_query\\_mode\nquery\\_engine \\= index.as\\_query\\_engine(vector\\_store\\_query\\_mode\\=\"mmr\")\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nThe author grew up writing essays on topics they had stacked up, exploring other things they could work on, and learning Italian. They lived in Florence, Italy and experienced the city at street level in all conditions. They also studied art and painting, and became familiar with the signature style seekers at RISD. They later moved to Cambridge, Massachusetts and got an apartment that was rent-stabilized. They worked on software, including a code editor and an online store builder, and wrote essays about their experiences. They also founded Y Combinator, a startup accelerator, and created the Summer Founders Program to give undergrads an alternative to working at tech companies.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n\\# To set the threshold, set it in vector\\_store\\_kwargs\nquery\\_engine\\_with\\_threshold \\= index.as\\_query\\_engine(\n    vector\\_store\\_query\\_mode\\=\"mmr\", vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.2}\n)\n\nresponse \\= query\\_engine\\_with\\_threshold.query(\n    \"What did the author do growing up?\"\n)\nprint(response)\n\nThe author grew up writing essays on topics they had stacked up, exploring other things they could work on, and learning Italian. They lived in Florence, Italy and experienced the city at street level in all conditions. They also studied art and painting, and became familiar with the signature style seekers at RISD. They later moved to Cambridge, Massachusetts and got an apartment that was rent-stabilized. They worked on software, including a code editor and an online store builder, and wrote essays about their experiences. They also founded Y Combinator, a startup accelerator, and developed the batch model of funding startups.\n\nNote that the node score will be scaled with the threshold and will additionally be penalized for the similarity to previous nodes. As the threshold goes to 1, the scores will become equal and similarity to previous nodes will be ignored, turning off the impact of MMR. By lowering the threshold, the algorithm will prefer more diverse documents.\n\nindex1 \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine\\_no\\_mrr \\= index1.as\\_query\\_engine()\nresponse\\_no\\_mmr \\= query\\_engine\\_no\\_mrr.query(\n    \"What did the author do growing up?\"\n)\n\nindex2 \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine\\_with\\_high\\_threshold \\= index2.as\\_query\\_engine(\n    vector\\_store\\_query\\_mode\\=\"mmr\", vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.8}\n)\nresponse\\_low\\_threshold \\= query\\_engine\\_with\\_low\\_threshold.query(\n    \"What did the author do growing up?\"\n)\n\nindex3 \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine\\_with\\_low\\_threshold \\= index3.as\\_query\\_engine(\n    vector\\_store\\_query\\_mode\\=\"mmr\", vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.2}\n)\nresponse\\_high\\_threshold \\= query\\_engine\\_with\\_high\\_threshold.query(\n    \"What did the author do growing up?\"\n)\n\nprint(\n    \"Scores without MMR \",\n    \\[node.score for node in response\\_no\\_mmr.source\\_nodes\\],\n)\nprint(\n    \"Scores with MMR and a threshold of 0.8 \",\n    \\[node.score for node in response\\_high\\_threshold.source\\_nodes\\],\n)\nprint(\n    \"Scores with MMR and a threshold of 0.2 \",\n    \\[node.score for node in response\\_low\\_threshold.source\\_nodes\\],\n)\n\nScores without MMR  \\[0.8139363671956625, 0.8110763805571549\\]\nScores with MMR and a threshold of 0.8  \\[0.6511610127407832, 0.4716293734403398\\]\nScores with MMR and a threshold of 0.2  \\[0.16278861260228436, -0.4745776806511904\\]\n\n## Retrieval-Only Demonstration[\uf0c1](#retrieval-only-demonstration \"Permalink to this heading\")\n\nBy setting a small chunk size and adjusting the \u201cmmr\\_threshold\u201d parameter, we can see how the retrieved results change from very diverse (and less relevant) to less diverse (and more relevant/redundant).\n\nWe try the following values: 0.1, 0.5, 0.8, 1.0\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n    LLMPredictor,\n)\nfrom llama\\_index.response.notebook\\_utils import display\\_source\\_node\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(temperature\\=0, model\\=\"gpt-3.5-turbo\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm, chunk\\_size\\_limit\\=64)\n\n\\# llama\\_index/docs/examples/data/paul\\_graham\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nretriever \\= index.as\\_retriever(\n    vector\\_store\\_query\\_mode\\=\"mmr\",\n    similarity\\_top\\_k\\=3,\n    vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.1},\n)\nnodes \\= retriever.retrieve(\n    \"What did the author do during his time in Y Combinator?\"\n)\n\nfor n in nodes:\n    display\\_source\\_node(n, source\\_length\\=1000)\n\n**Document ID:** 40d925c0-67fb-47eb-84f7-51728b224a6d  \n**Similarity:** 0.08476292699394482  \n**Text:** initial set of customers almost entirely from among their batchmates.\n\nI had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited\u2026  \n\n**Document ID:** 72651e88-62cc-4d99-baf8-222c05b5e129  \n**Similarity:** -0.5616228896922558  \n**Text:** and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can\u2019t move. People can\u2019t sit for more than about 15 minutes at\u2026  \n\n**Document ID:** 0328e711-c8f7-4a91-a0c1-a372068e3f1c  \n**Similarity:** -0.5230344987656315  \n**Text:** alternative to the Turing machine. If you want to write an interpreter for a language in itself, what\u2019s the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question\u2026.  \n\nretriever \\= index.as\\_retriever(\n    vector\\_store\\_query\\_mode\\=\"mmr\",\n    similarity\\_top\\_k\\=3,\n    vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.5},\n)\nnodes \\= retriever.retrieve(\n    \"What did the author do during his time in Y Combinator?\"\n)\n\nfor n in nodes:\n    display\\_source\\_node(n, source\\_length\\=1000)\n\n**Document ID:** 40d925c0-67fb-47eb-84f7-51728b224a6d  \n**Similarity:** 0.42381204797542626  \n**Text:** initial set of customers almost entirely from among their batchmates.\n\nI had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited\u2026  \n\n**Document ID:** 0328e711-c8f7-4a91-a0c1-a372068e3f1c  \n**Similarity:** 0.018193356482163803  \n**Text:** alternative to the Turing machine. If you want to write an interpreter for a language in itself, what\u2019s the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question\u2026.  \n\n**Document ID:** fbefd791-308a-4438-b6ec-353c2f05867b  \n**Similarity:** 0.05669398537137432  \n**Text:** and partly because I was focused on my mother, whose cancer had returned.\n\nShe died on January 15, 2014. We knew this was coming, but it was still hard when it did.\n\nI kept working on YC till March, to help get that batch of startups through\u2026  \n\nretriever \\= index.as\\_retriever(\n    vector\\_store\\_query\\_mode\\=\"mmr\",\n    similarity\\_top\\_k\\=3,\n    vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 0.8},\n)\nnodes \\= retriever.retrieve(\n    \"What did the author do during his time in Y Combinator?\"\n)\n\nfor n in nodes:\n    display\\_source\\_node(n, source\\_length\\=1000)\n\n**Document ID:** 40d925c0-67fb-47eb-84f7-51728b224a6d  \n**Similarity:** 0.6781190611335854  \n**Text:** initial set of customers almost entirely from among their batchmates.\n\nI had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited\u2026  \n\n**Document ID:** 7a8189bc-ccb6-402d-8ce5-49587b13878e  \n**Similarity:** 0.49504062407907184  \n**Text:** next several years I wrote lots of essays about all kinds of different topics. O\u2019Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting\u2026.  \n\n**Document ID:** 3ed4c422-a297-40b9-9510-68cc8f18e2c9  \n**Similarity:** 0.5017248860360811  \n**Text:** Y Combinator was not the original name. At first we were called Cambridge Seed. But we didn\u2019t want a regional name, in case someone copied us in Silicon Valley, so we renamed ourselves after one of the coolest tricks in the lambda calculus, the Y\u2026  \n\nretriever \\= index.as\\_retriever(\n    vector\\_store\\_query\\_mode\\=\"mmr\",\n    similarity\\_top\\_k\\=3,\n    vector\\_store\\_kwargs\\={\"mmr\\_threshold\": 1.0},\n)\nnodes \\= retriever.retrieve(\n    \"What did the author do during his time in Y Combinator?\"\n)\n\nfor n in nodes:\n    display\\_source\\_node(n, source\\_length\\=1000)\n\n**Document ID:** 40d925c0-67fb-47eb-84f7-51728b224a6d  \n**Similarity:** 0.8476240959508525  \n**Text:** initial set of customers almost entirely from among their batchmates.\n\nI had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited\u2026  \n\n**Document ID:** 1a8b0250-9b62-418c-a1df-6af4454a77e7  \n**Similarity:** 0.8252174449518838  \n**Text:** already helped write the RSS spec and would a few years later become a martyr for open access, and Sam Altman, who would later become the second president of YC. I don\u2019t think it was entirely luck that the first batch was so good. You had to be pretty bold\u2026  \n\n**Document ID:** 7d571ed4-0f23-41cd-a2fd-8a590c9e8f11  \n**Similarity:** 0.8227484107217059  \n**Text:** announcement on my site, inviting undergrads to apply. I had never imagined that writing essays would be a way to get \u201cdeal flow,\u201d as investors call it, but it turned out to be the perfect source. \\[15\\] We got 225 applications for the Summer Founders\u2026"
}