{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/callbacks/TokenCountingHandler.html",
        "title": "Token Counting Handler - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Token Counting Handler[\uf0c1](#token-counting-handler \"Permalink to this heading\")\n\nThis notebook walks through how to use the TokenCountingHandler and how it can be used to track your prompt, completion, and embedding token usage over time.\n\nimport tiktoken\nfrom llama\\_index.llms import Anthropic\n\nfrom llama\\_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n    set\\_global\\_service\\_context,\n)\nfrom llama\\_index.callbacks import CallbackManager, TokenCountingHandler\n\nimport os\n\nos.environ\\[\"ANTHROPIC\\_API\\_KEY\"\\] \\= \"YOUR\\_API\\_KEY\"\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nHere, we setup the callback and the serivce context. We set a global service context so that we don\u2019t have to worry about passing it into indexes and queries.\n\ntoken\\_counter \\= TokenCountingHandler(\n    tokenizer\\=tiktoken.encoding\\_for\\_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback\\_manager \\= CallbackManager(\\[token\\_counter\\])\n\nllm \\= Anthropic()\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm, callback\\_manager\\=callback\\_manager, embed\\_model\\=\"local\"\n)\n\n\\# set the global default!\nset\\_global\\_service\\_context(service\\_context)\n\n/Users/loganmarkewich/llama\\_index/llama-index/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n\n## Token Counting[\uf0c1](#token-counting \"Permalink to this heading\")\n\nThe token counter will track embedding, prompt, and completion token usage. The token counts are **cummulative** and are only reset when you choose to do so, with `token_counter.reset_counts()`.\n\n### Embedding Token Usage[\uf0c1](#embedding-token-usage \"Permalink to this heading\")\n\nNow that the service context is setup, let\u2019s track our embedding token usage.\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\nprint(token\\_counter.total\\_embedding\\_token\\_count)\n\nThat looks right! Before we go any further, lets reset the counts\n\ntoken\\_counter.reset\\_counts()\n\n### LLM + Embedding Token Usage[\uf0c1](#llm-embedding-token-usage \"Permalink to this heading\")\n\nNext, let\u2019s test a query and see what the counts look like.\n\nquery\\_engine \\= index.as\\_query\\_engine(similarity\\_top\\_k\\=4)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using \\`tokenizers\\` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS\\_PARALLELISM=(true | false)\n\nprint(\n    \"Embedding Tokens: \",\n    token\\_counter.total\\_embedding\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Prompt Tokens: \",\n    token\\_counter.prompt\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Completion Tokens: \",\n    token\\_counter.completion\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"Total LLM Token Count: \",\n    token\\_counter.total\\_llm\\_token\\_count,\n    \"\\\\n\",\n)\n\nEmbedding Tokens:  8 \n LLM Prompt Tokens:  3527 \n LLM Completion Tokens:  214 \n Total LLM Token Count:  3741 \n\n### Token Counting + Streaming![\uf0c1](#token-counting-streaming \"Permalink to this heading\")\n\nThe token counting handler also handles token counting during streaming.\n\nHere, token counting will only happen once the stream is completed.\n\ntoken\\_counter.reset\\_counts()\n\nquery\\_engine \\= index.as\\_query\\_engine(similarity\\_top\\_k\\=4, streaming\\=True)\nresponse \\= query\\_engine.query(\"What happened at Interleaf?\")\n\n\\# finish the stream\nfor token in response.response\\_gen:\n    \\# print(token, end=\"\", flush=True)\n    continue\n\nprint(\n    \"Embedding Tokens: \",\n    token\\_counter.total\\_embedding\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Prompt Tokens: \",\n    token\\_counter.prompt\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Completion Tokens: \",\n    token\\_counter.completion\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"Total LLM Token Count: \",\n    token\\_counter.total\\_llm\\_token\\_count,\n    \"\\\\n\",\n)\n\nEmbedding Tokens:  6 \n LLM Prompt Tokens:  3631 \n LLM Completion Tokens:  214 \n Total LLM Token Count:  3845 \n\n## Advanced Usage[\uf0c1](#advanced-usage \"Permalink to this heading\")\n\nThe token counter tracks each token usage event in an object called a `TokenCountingEvent`. This object has the following attributes:\n\n*   prompt -> The prompt string sent to the LLM or Embedding model\n    \n*   prompt\\_token\\_count -> The token count of the LLM prompt\n    \n*   completion -> The string completion received from the LLM (not used for embeddings)\n    \n*   completion\\_token\\_count -> The token count of the LLM completion (not used for embeddings)\n    \n*   total\\_token\\_count -> The total prompt + completion tokens for the event\n    \n*   event\\_id -> A string ID for the event, which aligns with other callback handlers\n    \n\nThese events are tracked on the token counter in two lists:\n\n*   llm\\_token\\_counts\n    \n*   embedding\\_token\\_counts\n    \n\nLet\u2019s explore what these look like!\n\nprint(\"Num LLM token count events: \", len(token\\_counter.llm\\_token\\_counts))\nprint(\n    \"Num Embedding token count events: \",\n    len(token\\_counter.embedding\\_token\\_counts),\n)\n\nNum LLM token count events:  1\nNum Embedding token count events:  1\n\nThis makes sense! The previous query embedded the query text, and then made 2 LLM calls (since the top k was 4, and the default chunk size is 1024, two seperate calls need to be made so the LLM can read all the retrieved text).\n\nNext, let\u2019s quickly see what these events look like for a single event.\n\nprint(\"prompt: \", token\\_counter.llm\\_token\\_counts\\[0\\].prompt\\[:100\\], \"...\\\\n\")\nprint(\n    \"prompt token count: \",\n    token\\_counter.llm\\_token\\_counts\\[0\\].prompt\\_token\\_count,\n    \"\\\\n\",\n)\n\nprint(\n    \"completion: \", token\\_counter.llm\\_token\\_counts\\[0\\].completion\\[:100\\], \"...\\\\n\"\n)\nprint(\n    \"completion token count: \",\n    token\\_counter.llm\\_token\\_counts\\[0\\].completion\\_token\\_count,\n    \"\\\\n\",\n)\n\nprint(\"total token count\", token\\_counter.llm\\_token\\_counts\\[0\\].total\\_token\\_count)\n\nprompt:  user: Context information is below.\n---------------------\na web app, is common now, but at the time  ...\n\nprompt token count:  3631 \n\ncompletion:  assistant:  Based on the context, a few key things happened at Interleaf:\n\n- It was a software compa ...\n\ncompletion token count:  199 \n\ntotal token count 3830"
}