{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/evaluation/usage_pattern.html",
        "title": "Usage Pattern (Response Evaluation) - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Usage Pattern (Response Evaluation)[\uf0c1](#usage-pattern-response-evaluation \"Permalink to this heading\")\n\n## Using `BaseEvaluator`[\uf0c1](#using-baseevaluator \"Permalink to this heading\")\n\nAll of the evaluation modules in LlamaIndex implement the `BaseEvaluator` class, with two main methods:\n\n1.  The `evaluate` method takes in `query`, `contexts`, `response`, and additional keyword arguments.\n    \n\n    def evaluate(\n        self,\n        query: Optional\\[str\\] \\= None,\n        contexts: Optional\\[Sequence\\[str\\]\\] \\= None,\n        response: Optional\\[str\\] \\= None,\n        \\*\\*kwargs: Any,\n    ) \\-> EvaluationResult:\n\n2.  The `evaluate_response` method provide an alternative interface that takes in a llamaindex `Response` object (which contains response string and source nodes) instead of separate `contexts` and `response`.\n    \n\ndef evaluate\\_response(\n    self,\n    query: Optional\\[str\\] \\= None,\n    response: Optional\\[Response\\] \\= None,\n    \\*\\*kwargs: Any,\n) \\-> EvaluationResult:\n\nIt\u2019s functionally the same as `evaluate`, just simpler to use when working with llamaindex objects directly.\n\n## Using `EvaluationResult`[\uf0c1](#using-evaluationresult \"Permalink to this heading\")\n\nEach evaluator outputs a `EvaluationResult` when executed:\n\neval\\_result \\= evaluator.evaluate(query\\=..., contexts\\=..., response\\=...)\neval\\_result.passing  \\# binary pass/fail\neval\\_result.score  \\# numerical score\neval\\_result.feedback  \\# string feedback\n\nDifferent evaluators may populate a subset of the result fields.\n\n## Evaluating Response Faithfulness (i.e. Hallucination)[\uf0c1](#evaluating-response-faithfulness-i-e-hallucination \"Permalink to this heading\")\n\nThe `FaithfulnessEvaluator` evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there\u2019s hallucination).\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import FaithfulnessEvaluator\n\n\\# build service context\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0.0)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build index\n...\n\n\\# define evaluator\nevaluator \\= FaithfulnessEvaluator(service\\_context\\=service\\_context)\n\n\\# query index\nquery\\_engine \\= vector\\_index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval\\_result \\= evaluator.evaluate\\_response(response\\=response)\nprint(str(eval\\_result.passing))\n\n![](https://docs.llamaindex.ai/en/stable/_images/eval_response_context.png)\n\nYou can also choose to evaluate each source context individually:\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import FaithfulnessEvaluator\n\n\\# build service context\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0.0)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build index\n...\n\n\\# define evaluator\nevaluator \\= FaithfulnessEvaluator(service\\_context\\=service\\_context)\n\n\\# query index\nquery\\_engine \\= vector\\_index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What battles took place in New York City in the American Revolution?\")\nresponse\\_str \\= response.response\nfor source\\_node in response.source\\_nodes:\n    eval\\_result \\= evaluator.evaluate(response\\=response\\_str, contexts\\=\\[source\\_node.get\\_content()\\])\n    print(str(eval\\_result.passing))\n\nYou\u2019ll get back a list of results, corresponding to each source node in `response.source_nodes`.\n\n## Evaluating Query + Response Relevancy[\uf0c1](#evaluating-query-response-relevancy \"Permalink to this heading\")\n\nThe `RelevancyEvaluator` evaluates if the retrieved context and the answer is relevant and consistent for the given query.\n\nNote that this evaluator requires the `query` to be passed in, in addition to the `Response` object.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import RelevancyEvaluator\n\n\\# build service context\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0.0)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build index\n...\n\n\\# define evaluator\nevaluator \\= RelevancyEvaluator(service\\_context\\=service\\_context)\n\n\\# query index\nquery\\_engine \\= vector\\_index.as\\_query\\_engine()\nquery \\= \"What battles took place in New York City in the American Revolution?\"\nresponse \\= query\\_engine.query(query)\neval\\_result \\= evaluator.evaluate\\_response(query\\=query, response\\=response)\nprint(str(eval\\_result))\n\n![](https://docs.llamaindex.ai/en/stable/_images/eval_query_response_context.png)\n\nSimilarly, you can also evaluate on a specific source node.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import RelevancyEvaluator\n\n\\# build service context\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0.0)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build index\n...\n\n\\# define evaluator\nevaluator \\= RelevancyEvaluator(service\\_context\\=service\\_context)\n\n\\# query index\nquery\\_engine \\= vector\\_index.as\\_query\\_engine()\nquery \\= \"What battles took place in New York City in the American Revolution?\"\nresponse \\= query\\_engine.query(query)\nresponse\\_str \\= response.response\nfor source\\_node in response.source\\_nodes:\n    eval\\_result \\= evaluator.evaluate(query\\=query, response\\=response\\_str, contexts\\=\\[source\\_node.get\\_content()\\])\n    print(str(eval\\_result.passing))\n\n![](https://docs.llamaindex.ai/en/stable/_images/eval_query_sources.png)\n\n## Question Generation[\uf0c1](#question-generation \"Permalink to this heading\")\n\nLlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.\n\nfrom llama\\_index import SimpleDirectoryReader, ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import DatasetGenerator\n\n\\# build service context\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0.0)\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# build documents\ndocuments \\= SimpleDirectoryReader(\"./data\").load\\_data()\n\n\\# define generator, generate questions\ndata\\_generator \\= DatasetGenerator.from\\_documents(documents)\n\neval\\_questions \\= data\\_generator.generate\\_questions\\_from\\_nodes()\n\n## Batch Evaluation[\uf0c1](#batch-evaluation \"Permalink to this heading\")\n\nWe also provide a batch evaluation runner for running a set of evaluators across many questions.\n\nfrom llama\\_index.evaluation import BatchEvalRunner\n\nrunner \\= BatchEvalRunner(\n    {\n        \"faithfulness\": faithfulness\\_evaluator, \"\n        \"relevancy\": relevancy\\_evaluator\n    },\n    workers\\=8,\n)\n\neval\\_results \\= await runner.aevaluate\\_queries(\n    vector\\_index.as\\_query\\_engine(), queries\\=questions\n)\n\n## Integrations[\uf0c1](#integrations \"Permalink to this heading\")\n\nWe also integrate with community evaluation tools.\n\n*   [DeepEval](https://docs.llamaindex.ai/en/stable/community/integrations/deepeval.html)\n    \n*   [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/howtos/integrations/llamaindex.ipynb)"
}