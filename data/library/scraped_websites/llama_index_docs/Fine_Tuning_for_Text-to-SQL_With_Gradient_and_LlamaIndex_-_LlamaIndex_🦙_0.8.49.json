{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/finetuning/gradient/gradient_text2sql.html",
        "title": "Fine Tuning for Text-to-SQL With Gradient and LlamaIndex - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\nIn this notebook we show you how to fine-tune llama2-7b on the [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset to be better at Text-to-SQL.\n\nWe do this by using [gradient.ai](https://gradient.ai/)\n\n**NOTE**: This is an alternative to our repo/guide on fine-tuning llama2-7b with Modal: https://github.com/run-llama/modal\\_finetune\\_sql\n\n!pip install llama-index gradientai \\-q\n\nimport os\nfrom llama\\_index.llms import GradientBaseModelLLM\nfrom llama\\_index.finetuning.gradient.base import GradientFinetuneEngine\n\nos.environ\\[\"GRADIENT\\_ACCESS\\_TOKEN\"\\] \\= os.getenv(\"GRADIENT\\_API\\_KEY\")\nos.environ\\[\"GRADIENT\\_WORKSPACE\\_ID\"\\] \\= \"\"\n\n## Prepare Data[\uf0c1](#prepare-data \"Permalink to this heading\")\n\nWe load sql-create-context from Hugging Face datasets. The dataset is a mix of WikiSQL and Spider, and is organized in the format of input query, context, and ground-truth SQL statement. The context is a CREATE TABLE statement.\n\n### Load Data, Save to a Directory[\uf0c1](#load-data-save-to-a-directory \"Permalink to this heading\")\n\nfrom datasets import load\\_dataset\nfrom pathlib import Path\nimport json\n\ndef load\\_jsonl(data\\_dir):\n    data\\_path \\= Path(data\\_dir).as\\_posix()\n    data \\= load\\_dataset(\"json\", data\\_files\\=data\\_path)\n    return data\n\ndef save\\_jsonl(data\\_dicts, out\\_path):\n    with open(out\\_path, \"w\") as fp:\n        for data\\_dict in data\\_dicts:\n            fp.write(json.dumps(data\\_dict) + \"\\\\n\")\n\ndef load\\_data\\_sql(data\\_dir: str \\= \"data\\_sql\"):\n    dataset \\= load\\_dataset(\"b-mc2/sql-create-context\")\n\n    dataset\\_splits \\= {\"train\": dataset\\[\"train\"\\]}\n    out\\_path \\= Path(data\\_dir)\n\n    out\\_path.parent.mkdir(parents\\=True, exist\\_ok\\=True)\n\n    for key, ds in dataset\\_splits.items():\n        with open(out\\_path, \"w\") as f:\n            for item in ds:\n                newitem \\= {\n                    \"input\": item\\[\"question\"\\],\n                    \"context\": item\\[\"context\"\\],\n                    \"output\": item\\[\"answer\"\\],\n                }\n                f.write(json.dumps(newitem) + \"\\\\n\")\n\n\\# dump data to data\\_sql\nload\\_data\\_sql(data\\_dir\\=\"data\\_sql\")\n\n### Split into Training/Validation Splits[\uf0c1](#split-into-training-validation-splits \"Permalink to this heading\")\n\nfrom math import ceil\n\ndef get\\_train\\_val\\_splits(\n    data\\_dir: str \\= \"data\\_sql\",\n    val\\_ratio: float \\= 0.1,\n    seed: int \\= 42,\n    shuffle: bool \\= True,\n):\n    data \\= load\\_jsonl(data\\_dir)\n    num\\_samples \\= len(data\\[\"train\"\\])\n    val\\_set\\_size \\= ceil(val\\_ratio \\* num\\_samples)\n\n    train\\_val \\= data\\[\"train\"\\].train\\_test\\_split(\n        test\\_size\\=val\\_set\\_size, shuffle\\=shuffle, seed\\=seed\n    )\n    return train\\_val\\[\"train\"\\].shuffle(), train\\_val\\[\"test\"\\].shuffle()\n\nraw\\_train\\_data, raw\\_val\\_data \\= get\\_train\\_val\\_splits(data\\_dir\\=\"data\\_sql\")\nsave\\_jsonl(raw\\_train\\_data, \"train\\_data\\_raw.jsonl\")\nsave\\_jsonl(raw\\_val\\_data, \"val\\_data\\_raw.jsonl\")\n\n{'input': 'If the record is 5-5, what is the game maximum?',\n 'context': 'CREATE TABLE table\\_23285805\\_4 (game INTEGER, record VARCHAR)',\n 'output': 'SELECT MAX(game) FROM table\\_23285805\\_4 WHERE record = \"5-5\"'}\n\n### Map Training/Dataset Dictionaries to Prompts[\uf0c1](#map-training-dataset-dictionaries-to-prompts \"Permalink to this heading\")\n\nHere we define functions to map the dataset dictionaries to a prompt format, that we can then feed to gradient.ai\u2019s fine-tuning endpoint.\n\n\\### Format is similar to the nous-hermes LLMs\n\ntext\\_to\\_sql\\_tmpl\\_str \\= \"\"\"\\\\\n<s>### Instruction:\\\\n{system\\_message}{user\\_message}\\\\n\\\\n\\### Response:\\\\n{response}</s>\"\"\"\n\ntext\\_to\\_sql\\_inference\\_tmpl\\_str \\= \"\"\"\\\\\n<s>### Instruction:\\\\n{system\\_message}{user\\_message}\\\\n\\\\n\\### Response:\\\\n\"\"\"\n\n\\### Alternative Format\n\\### Recommended by gradient.ai docs, but empirically we found worse results here\n\n\\# text\\_to\\_sql\\_tmpl\\_str = \"\"\"\\\\\n\\# <s>\\[INST\\] SYS\\\\n{system\\_message}\\\\n<</SYS>>\\\\n\\\\n{user\\_message} \\[/INST\\] {response} </s>\"\"\"\n\n\\# text\\_to\\_sql\\_inference\\_tmpl\\_str = \"\"\"\\\\\n\\# <s>\\[INST\\] SYS\\\\n{system\\_message}\\\\n<</SYS>>\\\\n\\\\n{user\\_message} \\[/INST\\] \"\"\"\n\ndef \\_generate\\_prompt\\_sql(input, context, dialect\\=\"sqlite\", output\\=\"\"):\n    system\\_message \\= f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n    \n    \"\"\"\n    user\\_message \\= f\"\"\"### Dialect:\n{dialect}\n\n\\### Input:\n{input}\n\n\\### Context:\n{context}\n\n\\### Response:\n\"\"\"\n    if output:\n        return text\\_to\\_sql\\_tmpl\\_str.format(\n            system\\_message\\=system\\_message,\n            user\\_message\\=user\\_message,\n            response\\=output,\n        )\n    else:\n        return text\\_to\\_sql\\_inference\\_tmpl\\_str.format(\n            system\\_message\\=system\\_message, user\\_message\\=user\\_message\n        )\n\ndef generate\\_prompt(data\\_point):\n    full\\_prompt \\= \\_generate\\_prompt\\_sql(\n        data\\_point\\[\"input\"\\],\n        data\\_point\\[\"context\"\\],\n        dialect\\=\"sqlite\",\n        output\\=data\\_point\\[\"output\"\\],\n    )\n    return {\"inputs\": full\\_prompt}\n\ntrain\\_data \\= \\[\n    {\"inputs\": d\\[\"inputs\"\\] for d in raw\\_train\\_data.map(generate\\_prompt)}\n\\]\nsave\\_jsonl(train\\_data, \"train\\_data.jsonl\")\nval\\_data \\= \\[{\"inputs\": d\\[\"inputs\"\\] for d in raw\\_val\\_data.map(generate\\_prompt)}\\]\nsave\\_jsonl(val\\_data, \"val\\_data.jsonl\")\n\nprint(train\\_data\\[0\\]\\[\"inputs\"\\])\n\n<s>### Instruction:\nYou are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n    \n    ### Dialect:\nsqlite\n\n### Input:\nWho had the fastest lap in bowmanville, ontario?\n\n### Context:\nCREATE TABLE table\\_30134667\\_2 (fastest\\_lap VARCHAR, location VARCHAR)\n\n### Response:\n\n\n### Response:\nSELECT fastest\\_lap FROM table\\_30134667\\_2 WHERE location = \"Bowmanville, Ontario\"</s>\n\n## Run Fine-tuning with gradient.ai[\uf0c1](#run-fine-tuning-with-gradient-ai \"Permalink to this heading\")\n\nHere we call Gradient\u2019s fine-tuning endpoint with the `GradientFinetuneEngine`.\n\nWe limit the steps for example purposes, but feel free to modify the parameters as you wish.\n\nAt the end we fetch our fine-tuned LLM.\n\n\\# base\\_model\\_slug = \"nous-hermes2\"\nbase\\_model\\_slug \\= \"llama2-7b-chat\"\nbase\\_llm \\= GradientBaseModelLLM(\n    base\\_model\\_slug\\=base\\_model\\_slug, max\\_tokens\\=300\n)\n\n\\# step max steps to 20 just for testing purposes\n\\# NOTE: can only specify one of base\\_model\\_slug or model\\_adapter\\_id\nfinetune\\_engine \\= GradientFinetuneEngine(\n    base\\_model\\_slug\\=base\\_model\\_slug,\n    \\# model\\_adapter\\_id='805c6fd6-daa8-4fc8-a509-bebb2f2c1024\\_model\\_adapter',\n    name\\=\"text\\_to\\_sql\",\n    data\\_path\\=\"train\\_data.jsonl\",\n    verbose\\=True,\n    max\\_steps\\=200,\n    batch\\_size\\=4,\n)\n\nfinetune\\_engine.model\\_adapter\\_id\n\n'805c6fd6-daa8-4fc8-a509-bebb2f2c1024\\_model\\_adapter'\n\nepochs \\= 1\nfor i in range(epochs):\n    print(f\"\\*\\* EPOCH {i} \\*\\*\")\n    finetune\\_engine.finetune()\n\nft\\_llm \\= finetune\\_engine.get\\_finetuned\\_model(max\\_tokens\\=300)\n\n## Evaluation[\uf0c1](#evaluation \"Permalink to this heading\")\n\nThis is two parts:\n\n1.  We evaluate on some sample datapoints in the validation dataset.\n    \n2.  We evaluate on a new toy SQL dataset, and plug the fine-tuned LLM into our `NLSQLTableQueryEngine` to run a full text-to-SQL workflow.\n    \n\n### Part 1: Evaluation on Validation Dataset Datapoints[\uf0c1](#part-1-evaluation-on-validation-dataset-datapoints \"Permalink to this heading\")\n\nfrom llama\\_index import ServiceContext\n\ndef get\\_text2sql\\_completion(llm, raw\\_datapoint):\n    service\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n    text2sql\\_tmpl\\_str \\= \\_generate\\_prompt\\_sql(\n        raw\\_datapoint\\[\"input\"\\],\n        raw\\_datapoint\\[\"context\"\\],\n        dialect\\=\"sqlite\",\n        output\\=None,\n    )\n\n    response \\= llm.complete(text2sql\\_tmpl\\_str)\n    return str(response)\n\ntest\\_datapoint \\= raw\\_val\\_data\\[2\\]\ndisplay(test\\_datapoint)\n\n{'input': ' how many\\\\xa0reverse\\\\xa0with\\\\xa0series\\\\xa0being iii series',\n 'context': 'CREATE TABLE table\\_12284476\\_8 (reverse VARCHAR, series VARCHAR)',\n 'output': 'SELECT COUNT(reverse) FROM table\\_12284476\\_8 WHERE series = \"III series\"'}\n\n\\# run base llama2-7b-chat model\nget\\_text2sql\\_completion(base\\_llm, test\\_datapoint)\n\n\\# run fine-tuned llama2-7b-chat model\nget\\_text2sql\\_completion(ft\\_llm, test\\_datapoint)\n\n'SELECT MIN(year) FROM table\\_name\\_35 WHERE venue = \"barcelona, spain\"'\n\n### Part 2: Evaluation on a Toy Dataset[\uf0c1](#part-2-evaluation-on-a-toy-dataset \"Permalink to this heading\")\n\nHere we create a toy table of cities and their populations.\n\n#### Create Table[\uf0c1](#create-table \"Permalink to this heading\")\n\n\\# create sample\nfrom sqlalchemy import (\n    create\\_engine,\n    MetaData,\n    Table,\n    Column,\n    String,\n    Integer,\n    select,\n    column,\n)\nfrom llama\\_index import SQLDatabase\n\nengine \\= create\\_engine(\"sqlite:///:memory:\")\nmetadata\\_obj \\= MetaData()\n\n\\# create city SQL table\ntable\\_name \\= \"city\\_stats\"\ncity\\_stats\\_table \\= Table(\n    table\\_name,\n    metadata\\_obj,\n    Column(\"city\\_name\", String(16), primary\\_key\\=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable\\=False),\n)\nmetadata\\_obj.create\\_all(engine)\n\n\\# This context is used later on\nfrom sqlalchemy.schema import CreateTable\n\ntable\\_create\\_stmt \\= str(CreateTable(city\\_stats\\_table))\nprint(table\\_create\\_stmt)\n\nCREATE TABLE city\\_stats (\n\tcity\\_name VARCHAR(16) NOT NULL, \n\tpopulation INTEGER, \n\tcountry VARCHAR(16) NOT NULL, \n\tPRIMARY KEY (city\\_name)\n)\n\nsql\\_database \\= SQLDatabase(engine, include\\_tables\\=\\[\"city\\_stats\"\\])\n\n#### Populate with Test Datapoints[\uf0c1](#populate-with-test-datapoints \"Permalink to this heading\")\n\n\\# insert sample rows\nfrom sqlalchemy import insert\n\nrows \\= \\[\n    {\"city\\_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n    {\"city\\_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n    {\n        \"city\\_name\": \"Chicago\",\n        \"population\": 2679000,\n        \"country\": \"United States\",\n    },\n    {\"city\\_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n\\]\nfor row in rows:\n    stmt \\= insert(city\\_stats\\_table).values(\\*\\*row)\n    with engine.connect() as connection:\n        cursor \\= connection.execute(stmt)\n        connection.commit()\n\n#### Get Text2SQL Query Engine[\uf0c1](#get-text2sql-query-engine \"Permalink to this heading\")\n\nfrom llama\\_index.query\\_engine import NLSQLTableQueryEngine\nfrom llama\\_index import ServiceContext, PromptTemplate\n\ndef get\\_text2sql\\_query\\_engine(llm, table\\_context, sql\\_database):\n    service\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n    \\# we essentially swap existing template variables for new template variables\n    \\# to put into our \\`NLSQLTableQueryEngine\\`\n    text2sql\\_tmpl\\_str \\= \\_generate\\_prompt\\_sql(\n        \"{query\\_str}\", \"{schema}\", dialect\\=\"{dialect}\", output\\=\"\"\n    )\n    sql\\_prompt \\= PromptTemplate(text2sql\\_tmpl\\_str)\n    \\# Here we explicitly set the table context to be the CREATE TABLE string\n    \\# So we set \\`tables\\` to empty, and hard fix \\`context\\_str\\` prefix\n\n    query\\_engine \\= NLSQLTableQueryEngine(\n        sql\\_database,\n        tables\\=\\[\\],\n        context\\_str\\_prefix\\=table\\_context,\n        text\\_to\\_sql\\_prompt\\=sql\\_prompt,\n        service\\_context\\=service\\_context,\n        synthesize\\_response\\=False,\n    )\n    return query\\_engine\n\n\\# query = \"Which cities have populations less than 10 million people?\"\nquery \\= \"What is the population of Tokyo? (make sure cities/countries are capitalized)\"\n\\# query = \"What is the average population and total population of the cities?\"\n\n#### Results with base llama2 model[\uf0c1](#results-with-base-llama2-model \"Permalink to this heading\")\n\nThe base llama2 model appends a bunch of text to the SQL statement that breaks our parser (and has minor capitalization mistakes)\n\nbase\\_query\\_engine \\= get\\_text2sql\\_query\\_engine(\n    base\\_llm, table\\_create\\_stmt, sql\\_database\n)\n\nbase\\_response \\= base\\_query\\_engine.query(query)\n\nprint(str(base\\_response))\n\nError: You can only execute one statement at a time.\n\nbase\\_response.metadata\\[\"sql\\_query\"\\]\n\n\"SELECT population FROM city\\_stats WHERE country = 'JAPAN';\\\\n\\\\nThis will return the population of Tokyo, which is the only city in the table with a population value.\"\n\n#### Results with fine-tuned model[\uf0c1](#results-with-fine-tuned-model \"Permalink to this heading\")\n\nft\\_query\\_engine \\= get\\_text2sql\\_query\\_engine(\n    ft\\_llm, table\\_create\\_stmt, sql\\_database\n)\n\nft\\_response \\= ft\\_query\\_engine.query(query)\n\nft\\_response.metadata\\[\"sql\\_query\"\\]\n\n'SELECT population FROM city\\_stats WHERE country = \"Japan\" AND city\\_name = \"Tokyo\"'"
}