{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/fusion_retriever.html",
        "title": "Building an Advanced Fusion Retriever from Scratch - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nIn this tutorial, we show you how to build an advanced retriever from scratch.\n\nSpecifically, we show you how to build our `QueryFusionRetriever` from scratch.\n\nThis is heavily inspired from the RAG-fusion repo here: https://github.com/Raudaschl/rag-fusion.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe load documents and build a simple vector index.\n\n!pip install rank-bm25 pymupdf\n\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\n### Load Documents[\uf0c1](#load-documents \"Permalink to this heading\")\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocuments \\= loader.load(file\\_path\\=\"./data/llama2.pdf\")\n\n### Load into Vector Store[\uf0c1](#load-into-vector-store \"Permalink to this heading\")\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\n\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=1024)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\n### Define LLMs[\uf0c1](#define-llms \"Permalink to this heading\")\n\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\")\n\n## Define Advanced Retriever[\uf0c1](#define-advanced-retriever \"Permalink to this heading\")\n\nWe define an advanced retriever that performs the following steps:\n\n1.  Query generation/rewriting: generate multiple queries given the original user query\n    \n2.  Perform retrieval for each query over an ensemble of retrievers.\n    \n3.  Reranking/fusion: fuse results from all queries, and apply a reranking step to \u201cfuse\u201d the top relevant results!\n    \n\nThen in the next section we\u2019ll plug this into our response synthesis module.\n\n### Step 1: Query Generation/Rewriting[\uf0c1](#step-1-query-generation-rewriting \"Permalink to this heading\")\n\nThe first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n\nWe can do this by prompting ChatGPT.\n\nfrom llama\\_index import PromptTemplate\n\nquery\\_str \\= \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n\nquery\\_gen\\_prompt\\_str \\= (\n    \"You are a helpful assistant that generates multiple search queries based on a \"\n    \"single input query. Generate {num\\_queries} search queries, one on each line, \"\n    \"related to the following input query:\\\\n\"\n    \"Query: {query}\\\\n\"\n    \"Queries:\\\\n\"\n)\nquery\\_gen\\_prompt \\= PromptTemplate(query\\_gen\\_prompt\\_str)\n\ndef generate\\_queries(llm, query\\_str: str, num\\_queries: int \\= 4):\n    fmt\\_prompt \\= query\\_gen\\_prompt.format(\n        num\\_queries\\=num\\_queries \\- 1, query\\=query\\_str\n    )\n    response \\= llm.complete(fmt\\_prompt)\n    queries \\= response.text.split(\"\\\\n\")\n    return queries\n\nqueries \\= generate\\_queries(llm, query\\_str, num\\_queries\\=4)\n\n\\['1. What are the benchmarks used to evaluate open-source chat models?', '2. Can you provide a comparison between the models developed in this work and existing open-source chat models?', '3. Are there any notable differences in performance between the models developed in this work and open-source chat models based on the benchmarks tested?'\\]\n\n### Step 2: Perform Vector Search for Each Query[\uf0c1](#step-2-perform-vector-search-for-each-query \"Permalink to this heading\")\n\nNow we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store.\n\n**NOTE**: We can also have multiple retrievers. Then the total number of queries we run is N_M, where N is number of retrievers and M is number of generated queries. Hence there will also be N_M retrieved lists.\n\nHere we\u2019ll use the retriever provided from our vector store. If you want to see how to build this from scratch please see [our tutorial on this](https://docs.llamaindex.ai/en/latest/examples/low_level/retrieval.html#put-this-into-a-retriever).\n\nfrom tqdm.asyncio import tqdm\n\nasync def run\\_queries(queries, retrievers):\n    \"\"\"Run queries against retrievers.\"\"\"\n    tasks \\= \\[\\]\n    for query in queries:\n        for i, retriever in enumerate(retrievers):\n            tasks.append(retriever.aretrieve(query))\n\n    task\\_results \\= await tqdm.gather(\\*tasks)\n\n    results\\_dict \\= {}\n    for i, (query, query\\_result) in enumerate(zip(queries, task\\_results)):\n        results\\_dict\\[(query, i)\\] \\= query\\_result\n\n    return results\\_dict\n\n\\# get retrievers\nfrom llama\\_index.retrievers import BM25Retriever\n\n\\## vector retriever\nvector\\_retriever \\= index.as\\_retriever(similarity\\_top\\_k\\=2)\n\n\\## bm25 retriever\nbm25\\_retriever \\= BM25Retriever.from\\_defaults(\n    docstore\\=index.docstore, similarity\\_top\\_k\\=2\n)\n\nresults\\_dict \\= await run\\_queries(queries, \\[vector\\_retriever, bm25\\_retriever\\])\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 \\[00:00<00:00, 22.59it/s\\]\n\n### Step 3: Perform Fusion[\uf0c1](#step-3-perform-fusion \"Permalink to this heading\")\n\nThe next step here is to perform fusion: combining the results from several retrievers into one and re-ranking.\n\nNote that a given node might be retrieved multiple times from different retrievers, so there needs to be a way to de-dup and rerank the node given the multiple retrievals.\n\nWe\u2019ll show you how to perform \u201creciprocal rank fusion\u201d: for each node, add up its reciprocal rank in every list where it\u2019s retrieved.\n\nThen reorder nodes by highest score to least.\n\nFull paper here: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n\ndef fuse\\_results(results\\_dict, similarity\\_top\\_k: int \\= 2):\n    \"\"\"Fuse results.\"\"\"\n    k \\= 60.0  \\# \\`k\\` is a parameter used to control the impact of outlier rankings.\n    fused\\_scores \\= {}\n    text\\_to\\_node \\= {}\n\n    \\# compute reciprocal rank scores\n    for nodes\\_with\\_scores in results.values():\n        for rank, node\\_with\\_score in enumerate(\n            sorted(\n                nodes\\_with\\_scores, key\\=lambda x: x.score or 0.0, reverse\\=True\n            )\n        ):\n            text \\= node\\_with\\_score.node.get\\_content()\n            text\\_to\\_node\\[text\\] \\= node\\_with\\_score\n            if text not in fused\\_scores:\n                fused\\_scores\\[text\\] \\= 0.0\n            fused\\_scores\\[text\\] += 1.0 / (rank + k)\n\n    \\# sort results\n    reranked\\_results \\= dict(\n        sorted(fused\\_scores.items(), key\\=lambda x: x\\[1\\], reverse\\=True)\n    )\n\n    \\# adjust node scores\n    reranked\\_nodes: List\\[NodeWithScore\\] \\= \\[\\]\n    for text, score in reranked\\_results.items():\n        reranked\\_nodes.append(text\\_to\\_node\\[text\\])\n        reranked\\_nodes\\[\\-1\\].score \\= score\n\n    return reranked\\_nodes\\[:similarity\\_top\\_k\\]\n\nfinal\\_results \\= fuse\\_results(results\\_dict)\n\nfrom llama\\_index.response.notebook\\_utils import display\\_source\\_node\n\nfor n in final\\_results:\n    display\\_source\\_node(n, source\\_length\\=500)\n\n**Node ID:** d92e53b7-1f27-4129-8d5d-dd06638b1f2d  \n**Similarity:** 0.04972677595628415  \n**Text:** Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models across ~4,000 helpfulness prompts with three raters per prompt. The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of 36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7. Inter-Rater Reliability (\u2026  \n\n**Node ID:** 20d32df8-e16e-45fb-957a-e08175e188e8  \n**Similarity:** 0.016666666666666666  \n**Text:** Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% confidence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, s\u2026  \n\n**Analysis**: The above code has a few straightforward components.\n\n1.  Go through each node in each retrieved list, and add it\u2019s reciprocal rank to the node\u2019s ID. The node\u2019s ID is the hash of it\u2019s text for dedup purposes.\n    \n2.  Sort results by highest-score to lowest.\n    \n3.  Adjust node scores.\n    \n\n## Plug into RetrieverQueryEngine[\uf0c1](#plug-into-retrieverqueryengine \"Permalink to this heading\")\n\nNow we\u2019re ready to define this as a custom retriever, and plug it into our `RetrieverQueryEngine` (which does retrieval and synthesis).\n\nfrom llama\\_index import QueryBundle\nfrom llama\\_index.retrievers import BaseRetriever\nfrom typing import Any, List\nfrom llama\\_index.schema import NodeWithScore\n\nclass FusionRetriever(BaseRetriever):\n    \"\"\"Ensemble retriever with fusion.\"\"\"\n\n    def \\_\\_init\\_\\_(\n        self,\n        llm,\n        retrievers: List\\[BaseRetriever\\],\n        similarity\\_top\\_k: int \\= 2,\n    ) \\-> None:\n        \"\"\"Init params.\"\"\"\n        self.\\_retrievers \\= retrievers\n        self.\\_similarity\\_top\\_k \\= similarity\\_top\\_k\n\n    def \\_retrieve(self, query\\_bundle: QueryBundle) \\-> List\\[NodeWithScore\\]:\n        \"\"\"Retrieve.\"\"\"\n        queries \\= generate\\_queries(llm, query\\_str, num\\_queries\\=4)\n        results \\= run\\_queries(queries, \\[vector\\_retriever, bm25\\_retriever\\])\n        final\\_results \\= fuse\\_results(\n            results\\_dict, similarity\\_top\\_k\\=self.\\_similarity\\_top\\_k\n        )\n\n        return final\\_results\n\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\nfusion\\_retriever \\= FusionRetriever(\n    llm, \\[vector\\_retriever, bm25\\_retriever\\], similarity\\_top\\_k\\=2\n)\n\nquery\\_engine \\= RetrieverQueryEngine(fusion\\_retriever)\n\nresponse \\= query\\_engine.query(query\\_str)\n\n/Users/jerryliu/Programming/gpt\\_index/llama\\_index/indices/base\\_retriever.py:22: RuntimeWarning: coroutine 'run\\_queries' was never awaited\n  return self.\\_retrieve(str\\_or\\_query\\_bundle)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\nThe models developed in this work, specifically the Llama 2-Chat models, are competitive with open-source chat models based on the benchmarks tested. The largest Llama 2-Chat model has a win rate of 36% and a tie rate of 31.5% relative to ChatGPT, which indicates that it performs well in comparison. Additionally, the Llama 2-Chat 70B model outperforms the PaLM-bison chat model by a large percentage on the prompt set used for evaluation. While it is important to note the limitations of the benchmarks and the subjective nature of human evaluations, the results suggest that the Llama 2-Chat models are on par with or even outperform open-source chat models in certain aspects."
}