{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/service_context.html",
        "title": "Service Context - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Service Context[\uf0c1](#service-context \"Permalink to this heading\")\n\nThe service context container is a utility container for LlamaIndex index and query classes. The container contains the following objects that are commonly used for configuring every index and query, such as the LLMPredictor (for configuring the LLM), the PromptHelper (for configuring input size/chunk size), the BaseEmbedding (for configuring the embedding model), and more.\n\n* * *\n\n_class_ llama\\_index.indices.service\\_context.ServiceContext(_llm\\_predictor: BaseLLMPredictor_, _prompt\\_helper: [PromptHelper](https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html#llama_index.indices.prompt_helper.PromptHelper \"llama_index.indices.prompt_helper.PromptHelper\")_, _embed\\_model: BaseEmbedding_, _node\\_parser: [NodeParser](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html#llama_index.node_parser.NodeParser \"llama_index.node_parser.interface.NodeParser\")_, _llama\\_logger: LlamaLogger_, _callback\\_manager: [CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")_)[\uf0c1](#llama_index.indices.service_context.ServiceContext \"Permalink to this definition\")\n\nService Context container.\n\nThe service context container is a utility container for LlamaIndex index and query classes. It contains the following: - llm\\_predictor: BaseLLMPredictor - prompt\\_helper: PromptHelper - embed\\_model: BaseEmbedding - node\\_parser: NodeParser - llama\\_logger: LlamaLogger (deprecated) - callback\\_manager: CallbackManager\n\n_classmethod_ from\\_defaults(_llm\\_predictor: Optional\\[BaseLLMPredictor\\] \\= None_, _llm: Optional\\[Union\\[str, [LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\"), BaseLanguageModel\\]\\] \\= 'default'_, _prompt\\_helper: Optional\\[[PromptHelper](https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html#llama_index.indices.prompt_helper.PromptHelper \"llama_index.indices.prompt_helper.PromptHelper\")\\] \\= None_, _embed\\_model: Optional\\[Union\\[BaseEmbedding, Embeddings, str\\]\\] \\= 'default'_, _node\\_parser: Optional\\[[NodeParser](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html#llama_index.node_parser.NodeParser \"llama_index.node_parser.interface.NodeParser\")\\] \\= None_, _llama\\_logger: Optional\\[LlamaLogger\\] \\= None_, _callback\\_manager: Optional\\[[CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")\\] \\= None_, _system\\_prompt: Optional\\[str\\] \\= None_, _query\\_wrapper\\_prompt: Optional\\[[BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")\\] \\= None_, _chunk\\_size: Optional\\[int\\] \\= None_, _chunk\\_overlap: Optional\\[int\\] \\= None_, _context\\_window: Optional\\[int\\] \\= None_, _num\\_output: Optional\\[int\\] \\= None_, _chunk\\_size\\_limit: Optional\\[int\\] \\= None_) \u2192 [ServiceContext](#llama_index.indices.service_context.ServiceContext \"llama_index.indices.service_context.ServiceContext\")[\uf0c1](#llama_index.indices.service_context.ServiceContext.from_defaults \"Permalink to this definition\")\n\nCreate a ServiceContext from defaults. If an argument is specified, then use the argument value provided for that parameter. If an argument is not specified, then use the default value.\n\nYou can change the base defaults by setting llama\\_index.global\\_service\\_context to a ServiceContext object with your desired settings.\n\nParameters\n\n*   **llm\\_predictor** (_Optional__\\[__BaseLLMPredictor__\\]_) \u2013 LLMPredictor\n    \n*   **prompt\\_helper** (_Optional__\\[_[_PromptHelper_](https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html#llama_index.indices.prompt_helper.PromptHelper \"llama_index.indices.prompt_helper.PromptHelper\")_\\]_) \u2013 PromptHelper\n    \n*   **embed\\_model** (_Optional__\\[__BaseEmbedding__\\]_) \u2013 BaseEmbedding or \u201clocal\u201d (use local model)\n    \n*   **node\\_parser** (_Optional__\\[_[_NodeParser_](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html#llama_index.node_parser.NodeParser \"llama_index.node_parser.NodeParser\")_\\]_) \u2013 NodeParser\n    \n*   **llama\\_logger** (_Optional__\\[__LlamaLogger__\\]_) \u2013 LlamaLogger (deprecated)\n    \n*   **chunk\\_size** (_Optional__\\[__int__\\]_) \u2013 chunk\\_size\n    \n*   **callback\\_manager** (_Optional__\\[_[_CallbackManager_](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.CallbackManager\")_\\]_) \u2013 CallbackManager\n    \n*   **system\\_prompt** (_Optional__\\[__str__\\]_) \u2013 System-wide prompt to be prepended to all input prompts, used to guide system \u201cdecision making\u201d\n    \n*   **query\\_wrapper\\_prompt** (_Optional__\\[_[_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_\\]_) \u2013 A format to wrap passed-in input queries.\n    \n\nDeprecated Args:\n\nchunk\\_size\\_limit (Optional\\[int\\]): renamed to chunk\\_size\n\n_classmethod_ from\\_service\\_context(_service\\_context: [ServiceContext](#llama_index.indices.service_context.ServiceContext \"llama_index.indices.service_context.ServiceContext\")_, _llm\\_predictor: Optional\\[BaseLLMPredictor\\] \\= None_, _llm: Optional\\[Union\\[str, [LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\"), BaseLanguageModel\\]\\] \\= 'default'_, _prompt\\_helper: Optional\\[[PromptHelper](https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html#llama_index.indices.prompt_helper.PromptHelper \"llama_index.indices.prompt_helper.PromptHelper\")\\] \\= None_, _embed\\_model: Optional\\[Union\\[BaseEmbedding, Embeddings, str\\]\\] \\= 'default'_, _node\\_parser: Optional\\[[NodeParser](https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html#llama_index.node_parser.NodeParser \"llama_index.node_parser.interface.NodeParser\")\\] \\= None_, _llama\\_logger: Optional\\[LlamaLogger\\] \\= None_, _callback\\_manager: Optional\\[[CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")\\] \\= None_, _system\\_prompt: Optional\\[str\\] \\= None_, _query\\_wrapper\\_prompt: Optional\\[[BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")\\] \\= None_, _chunk\\_size: Optional\\[int\\] \\= None_, _chunk\\_overlap: Optional\\[int\\] \\= None_, _context\\_window: Optional\\[int\\] \\= None_, _num\\_output: Optional\\[int\\] \\= None_, _chunk\\_size\\_limit: Optional\\[int\\] \\= None_) \u2192 [ServiceContext](#llama_index.indices.service_context.ServiceContext \"llama_index.indices.service_context.ServiceContext\")[\uf0c1](#llama_index.indices.service_context.ServiceContext.from_service_context \"Permalink to this definition\")\n\nInstantiate a new service context using a previous as the defaults.\n\nto\\_dict() \u2192 dict[\uf0c1](#llama_index.indices.service_context.ServiceContext.to_dict \"Permalink to this definition\")\n\nConvert service context to dict.\n\nllama\\_index.indices.service\\_context.set\\_global\\_service\\_context(_service\\_context: Optional\\[[ServiceContext](#llama_index.indices.service_context.ServiceContext \"llama_index.indices.service_context.ServiceContext\")\\]_) \u2192 None[\uf0c1](#llama_index.indices.service_context.set_global_service_context \"Permalink to this definition\")\n\nHelper function to set the global service context."
}