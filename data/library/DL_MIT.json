["", "DEEP LEARNING", "The MIT Press Essential Knowledge Series\nA complete list of the titles in this series appears at the back of this book.", "DEEP LEARNING\nJOHN D. KELLEHER\nThe MIT Press | Cambridge, Massachusetts | London, England", "\u00a9 2019 The Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by\nany electronic or mechanical means (including photocopying, recording,\nor information storage and retrieval) without permission in writing from\nthe publisher.\nThis book was set in Chaparral Pro by Toppan Best-set Premedia Limited.\nPrinted and bound in the United States of America.\nLibrary of Congress Cataloging-in-Publication Data\nNames: Kelleher, John D., 1974- author.\nTitle: Deep learning / John D. Kelleher.\nDescription: Cambridge, MA : The MIT Press, [2019] | Series:\nThe MIT press essential knowledge series | Includes bibliographical\nreferences and index.\nIdentifiers: LCCN 2018059550 | ISBN 9780262537551 (pbk. : alk. paper)\nSubjects: LCSH: Machine learning. | Artificial intelligence.\nClassification: LCC Q325.5 .K454 2019 | DDC 006.3/1\u2014dc23 LC record\navailable at https://lccn.loc.gov/2018059550\n10 9 8 7 6 5 4 3 2 1", "CONTENTS\nSeries Foreword vii\nPreface ix\nAcknowledgments xi\n1 Introduction to Deep Learning 1\n2 Conceptual Foundations 39\n3 Neural Networks: The Building Blocks of Deep\nLearning 65\n4 A Brief History of Deep Learning 101\n5 Convolutional and Recurrent Neural Networks 159\n6 Learning Functions 185\n7 The Future of Deep Learning 231\nGlossary 251\nNotes 257\nReferences 261\nFurther Readings 267\nIndex 269", "", "SERIES FOREWORD\nThe MIT Press Essential Knowledge series offers acces-\nsible, concise, beautifully produced pocket- size books on\ntopics of current interest. Written by leading thinkers,\nthe books in this series deliver expert overviews of sub-\njects that range from the cultural and the historical to the\nscientific and the technical.\nIn today\u2019s era of instant information gratification, we\nhave ready access to opinions, rationalizations, and super-\nficial descriptions. Much harder to come by is the founda-\ntional knowledge that informs a principled understanding\nof the world. Essential Knowledge books fill that need.\nSynthesizing specialized subject matter for nonspecialists\nand engaging critical topics through fundamentals, each\nof these compact volumes offers readers a point of access\nto complex ideas.\nBruce Tidor\nProfessor of Biological Engineering and Computer Science\nMassachusetts Institute of Technology", "", "PREFACE\nDeep learning is enabling innovation and change across\nall aspects of our modern lives. Most of the artificial intel-\nligence breakthroughs that you hear about in the media\nare based on deep learning. As a result, whether you are\na business person interested in improving the efficiency\nof your organization, a policymaker concerned with eth-\nics and privacy in a Big Data world, a researcher working\nwith complex data, or a curious citizen who wants a better\nsense of the potential of artificial intelligence and how it\nwill change your life, it is important for you to have an\nunderstanding of deep learning.\nThe goal of this book is to enable the general reader\nto gain an understanding of what deep learning is, where\nit has come from, how it works, what it makes possible\n(and what it doesn\u2019t), and how the field is likely to develop\nin the next ten years. The fact that deep learning is a set\nof algorithms and models means that understanding deep\nlearning requires understanding how these algorithms\nand models process data. As a result, this book is not\npurely descriptive and definitional; it also includes expla-\nnations of algorithms. I have attempted to present the\ntechnical material in an accessible way. From my teaching\nexperience, I have found that for technical topics the most", "accessible presentation is to explain the fundamental con-\ncepts in a step-b y- step manner. So, although I have tried\nto keep the mathematical content to a minimum, where\nI felt it was necessary to include it I have endeavored to\nwalk you through the mathematical equations in as clear\nand direct a manner as I can. I have supplemented these\nexplanations with examples and illustrations.\nWhat is really wondrous about deep learning is not\nthe complexity of the math it is built on, but rather, that it\ncan perform such a diverse set of exciting and impressive\ntasks using such simple calculations. Don\u2019t be surprised to\nfind yourself saying: \u201cIs that all it\u2019s doing?\u201d In fact, a deep\nlearning model really is just a lot (admittedly, an awful\nlot) of multiplications and additions with a few nonlinear\nmappings (which I will explain) added in. Yet, despite this\nsimplicity, these models can, among other achievements,\nbeat the Go world champion, define the state- of- the- art\nin computer vision and machine translation, and drive a\ncar. This book is an introductory text about deep learning,\nbut I hope that it is an introduction that has enough depth\nthat you will come back to the book as your confidence\nwith the material grows.\nx PREFACE", "ACKNOWLEDGMENTS\nThis book would not have been possible without the sacri-\nfices made by my wife, Aphra, and my family, in particular\nmy parents John and Betty Kelleher. I have also received\na huge amount of support from friends, especially Alan\nMcDonnell, Ionela Lungu, Simon Dobnik, Lorraine Byrne,\nNoel Fitzpatrick, and Josef van Genabith.\nI would also like to acknowledge the help I have re-\nceived from the staff at the MIT Press, and from a number\nof people who have read sections of the book and provided\nfeedback. MIT Press organized three anonymous review-\ners who read and commented on a draft of the book. I\nthank these reviewers for their time and helpful feedback.\nAlso a number of people read draft chapters from the book\nand I wish to take this opportunity to publicly acknowl-\nedge their help, so my thanks to: Mike Dillinger, Magda-\nlena Kacmajor, Elizabeth Kelleher, John Bernard Kelleher,\nAphra Kerr, Filip Klubi\u010dka, and Abhijit Mahalunkar. This\nbook has been informed by the many conversations I have\nhad with colleagues and students about deep learning, in\nparticular those with Robert Ross and Giancarlo Salton.\nThis book is dedicated to my sister Elizabeth (Liz)\nKelleher in recognition of her love and support, and her\npatience with a brother who can\u2019t stop explaining things.", "", "1\nINTRODUCTION TO\nDEEP LEARNING\nDeep learning is the subfield of artificial intelligence that\nfocuses on creating large neural network models that are\ncapable of making accurate data- driven decisions. Deep\nlearning is particularly suited to contexts where the data is\ncomplex and where there are large datasets available. To-\nday most online companies and high-e nd consumer tech-\nnologies use deep learning. Among other things, Facebook\nuses deep learning to analyze text in online conversations.\nGoogle, Baidu, and Microsoft all use deep learning for im-\nage search, and also for machine translation. All modern\nsmart phones have deep learning systems running on\nthem; for example, deep learning is now the standard\ntechnology for speech recognition, and also for face de-\ntection on digital cameras. In the healthcare sector, deep\nlearning is used to process medical images (X-r ays, CT, and\nMRI scans) and diagnose health conditions. Deep learning\nis also at the core of self-d riving cars, where it is used for", "localization and mapping, motion planning and steering,\nand environment perception, as well as tracking driver\nstate.\nPerhaps the best- known example of deep learning is\nDeepMind\u2019s AlphaGo.1 Go is a board game similar to Chess.\nAlphaGo was the first computer program to beat a profes-\nsional Go player. In March 2016, it beat the top Korean\nprofessional, Lee Sedol, in a match watched by more than\ntwo hundred million people. The following year, in 2017,\nAlphaGo beat the world\u2019s No. 1 ranking player, China\u2019s\nKe Jie.\nIn 2016 AlphaGo\u2019s success was very surprising. At\nthe time, most people expected that it would take many\nmore years of research before a computer would be able\nto compete with top level human Go players. It had been\nknown for a long time that programming a computer to\nplay Go was much more difficult than programming it to\nplay Chess. There are many more board configurations\npossible in Go than there are in Chess. This is because Go\nhas a larger board and simpler rules than Chess. There are,\nin fact, more possible board configurations in Go than\nthere are atoms in the universe. This massive search space\nand Go\u2019s large branching factor (the number of board\nconfigurations that can be reached in one move) makes\nGo an incredibly challenging game for both humans and\ncomputers.\nOne way of illustrating the relative difficulty Go\nand Chess presented to computer programs is through\n2 ChAPtER 1", "a historical comparison of how Go and Chess programs\ncompeted with human players. In 1967, MIT\u2019s MacHack- 6\nChess program could successfully compete with humans\nand had an Elo rating2 well above novice level, and, by May\n1997, DeepBlue was capable of beating the Chess world\nchampion Gary Kasparov. In comparison, the first com-\nplete Go program wasn\u2019t written until 1968 and strong\nhuman players were still able to easily beat the best Go\nprograms in 1997.\nThe time lag between the development of Chess and\nGo computer programs reflects the difference in compu-\ntational difficulty between these two games. However, a\nsecond historic comparison between Chess and Go illus-\ntrates the revolutionary impact that deep learning has\nhad on the ability of computer programs to compete with\nhumans at Go. It took thirty years for Chess programs to\nprogress from human level competence in 1967 to world\nchampion level in 1997. However, with the development\nof deep learning it took only seven years for computer Go\nprograms to progress from advanced amateur to world\nchampion; as recently as 2009 the best Go program in\nthe world was rated at the low-e nd of advanced amateur.\nThis acceleration in performance through the use of deep\nlearning is nothing short of extraordinary, but it is also\nindicative of the types of progress that deep learning has\nenabled in a number of fields.\nIntRoduCtIon to dEEP LEARnIng 3", "AlphaGo uses deep learning to evaluate board configu-\nrations and to decide on the next move to make. The fact\nthat AlphaGo used deep learning to decide what move to\nmake next is a clue to understanding why deep learning\nis useful across so many different domains and applica-\ntions. Decision- making is a crucial part of life. One way\nto make decisions is to base them on your \u201cintuition\u201d or\nyour \u201cgut feeling.\u201d However, most people would agree that\nthe best way to make decisions is to base them on the rel-\nevant data. Deep learning enables data- driven decisions by\nidentifying and extracting patterns from large datasets\nthat accurately map from sets of complex inputs to good\ndecision outcomes.\nArtificial Intelligence, Machine Learning, and\nDeep Learning\nDeep learning has emerged from research in artificial\nintelligence and machine learning. Figure 1.1 illustrates\nthe relationship between artificial intelligence, machine\nlearning, and deep learning.\nThe field of artificial intelligence was born at a\nworkshop at Dartmouth College in the summer of 1956.\nResearch on a number of topics was presented at the\nworkshop including mathematical theorem proving, nat-\nural language processing, planning for games, computer\n4 ChAPtER 1", "Deep learning enables\ndata- driven decisions\nby identifying and\nextracting patterns\nfrom large datasets that\naccurately map from\nsets of complex inputs\nto good decision\noutcomes.", "Artificial\nintelligence\nMachine\nlearning\nDeep\nlearning\nFigure 1.1 The relationship between artificial intelligence, machine\nlearning, and deep learning.\nprograms that could learn from examples, and neural net-\nworks. The modern field of machine learning draws on the\nlast two topics: computers that could learn from examples,\nand neural network research.\nMachine learning involves the development and eval-\nuation of algorithms that enable a computer to extract (or\nlearn) functions from a dataset (sets of examples). To un-\nderstand what machine learning means we need to under-\nstand three terms: dataset, algorithm, and function.\nIn its simplest form, a dataset is a table where each row\ncontains the description of one example from a domain,\n6 ChAPtER 1", "Table 1.1. A dataset of loan applicants and their known\ncredit solvency ratings\nID Annual Income Current Debt Credit Solvency\n1 $150 - $100 100\n2 $250 - $300 - 50\n3 $450 - $250 400\n4 $200 - $350 - 300\nand each column contains the information for one of the\nfeatures in a domain. For example, table 1.1 illustrates\nan example dataset for a loan application domain. This\ndataset lists the details of four example loan applications.\nExcluding the ID feature, which is only for ease of refer-\nence, each example is described using three features: the\napplicant\u2019s annual income, their current debt, and their\ncredit solvency.\nAn algorithm is a process (or recipe, or program) that\na computer can follow. In the context of machine learning,\nan algorithm defines a process to analyze a dataset and\nidentify recurring patterns in the data. For example, the\nalgorithm might find a pattern that relates a person\u2019s an-\nnual income and current debt to their credit solvency rat-\ning. In mathematics, relationships of this type are referred\nto as functions.\nA function is a deterministic mapping from a set of\ninput values to one or more output values. The fact that\nIntRoduCtIon to dEEP LEARnIng 7", "the mapping is deterministic means that for any specific\nset of inputs a function will always return the same out-\nputs. For example, addition is a deterministic mapping,\nand so 2+2 is always equal to 4. As we will discuss later,\nwe can create functions for domains that are more com-\nplex than basic arithmetic, we can for example define a\nfunction that takes a person\u2019s income and debt as inputs\nand returns their credit solvency rating as the output\nvalue. The concept of a function is very important to\ndeep learning so it is worth repeating the definition for\nemphasis: a function is simply a mapping from inputs to\noutputs. In fact, the goal of machine learning is to learn\nfunctions from data. A function can be represented in\nmany different ways: it can be as simple as an arithmetic\noperation (e.g., addition or subtraction are both functions\nthat take inputs and return a single output), a sequence\nof if- then- else rules, or it can have a much more complex\nrepresentation.\nOne way to represent a function is to use a neural\nnetwork. Deep learning is the subfield of machine learn-\ning that focuses on deep neural network models. In fact,\nthe patterns that deep learning algorithms extract from\ndatasets are functions that are represented as neural\nnetworks. Figure 1.2 illustrates the structure of a neural\nnetwork. The boxes on the left of the figure represent the\nmemory locations where inputs are presented to the net-\nwork. Each of the circles in this figure is called a neuron\n8 ChAPtER 1", "A function is a\ndeterministic mapping\nfrom a set of input\nvalues to one or more\noutput values.", "and each neuron implements a function: it takes a number\nof values as input and maps them to an output value. The\narrows in the network show how the outputs of each neu-\nron are passed as inputs to other neurons. In this network,\ninformation flows from left to right. For example, if this\nnetwork were trained to predict a person\u2019s credit solvency,\nbased on their income and debt, it would receive the in-\ncome and debt as inputs on the left of the network and\noutput the credit solvency score through the neuron on\nthe right.\nA neural network uses a divide-a nd- conquer strategy\nto learn a function: each neuron in the network learns a\nsimple function, and the overall (more complex) function,\ndefined by the network, is created by combining these\nsimpler functions. Chapter 3 will describe how a neural\nnetwork processes information.\nFigure 1.2 Schematic illustration of a neural network.\n10 ChAPtER 1", "What Is Machine Learning?\nA machine learning algorithm is a search process designed\nto choose the best function, from a set of possible func-\ntions, to explain the relationships between features in a\ndataset. To get an intuitive understanding of what is in-\nvolved in extracting, or learning, a function from data, ex-\namine the following set of sample inputs to an unknown\nfunction and the outputs it returns. Given these examples,\ndecide which arithmetic operation (addition, subtraction,\nmultiplication, or division) is the best choice to explain\nthe mapping the unknown function defines between its\ninputs and output:\nfunction(Inputs)=Output\nfunction(5,5)=25\nfunction(2,6)=12\nfunction(4,4)=16\nfunction(2,2)=04\nMost people would agree that multiplication is the best\nchoice because it provides the best match to the observed\nrelationship, or mapping, from the inputs to the outputs:\nIntRoduCtIon to dEEP LEARnIng 11", "5\u00d75=25\n2\u00d76=12\n4\u00d77=28\n2\u00d72=04\nIn this particular instance, choosing the best func-\ntion is relatively straightforward, and a human can do it\nwithout the aid of a computer. However, as the number\nof inputs to the unknown function increases (perhaps\nto hundreds or thousands of inputs), and the variety of\npotential functions to be considered gets larger, the task\nbecomes much more difficult. It is in these contexts that\nharnessing the power of machine learning to search for\nthe best function, to match the patterns in the dataset,\nbecomes necessary.\nMachine learning involves a two- step process: train-\ning and inference. During training, a machine learning\nalgorithm processes a dataset and chooses the function\nthat best matches the patterns in the data. The extracted\nfunction will be encoded in a computer program in a par-\nticular form (such as if- then- else rules or parameters of\na specified equation). The encoded function is known as\na model, and the analysis of the data in order to extract\nthe function is often referred to as training the model.\n12 ChAPtER 1", "Essentially, models are functions encoded as computer\nprograms. However, in machine learning the concepts of\nfunction and model are so closely related that the distinc-\ntion is often skipped over and the terms may even be used\ninterchangeably.\nIn the context of deep learning, the relationship be-\ntween functions and models is that the function extracted\nfrom a dataset during training is represented as a neural\nnetwork model, and conversely a neural network model\nencodes a function as a computer program. The standard\nprocess used to train a neural network is to begin train-\ning with a neural network where the parameters of the\nnetwork are randomly initialized (we will explain network\nparameters later; for now just think of them as values that\ncontrol how the function the network encodes works).\nThis randomly initialized network will be very inaccurate\nin terms of its ability to match the relationship between\nthe various input values and target outputs for the ex-\namples in the dataset. The training process then proceeds\nby iterating through the examples in the dataset, and,\nfor each example, presenting the input values to the net-\nwork and then using the difference between the output\nreturned by the network and the correct output for the ex-\nample listed in the dataset to update the network\u2019s param-\neters so that it matches the data more closely. Once the\nmachine learning algorithm has found a function that is\nsufficiently accurate (in terms of the outputs it generates\nIntRoduCtIon to dEEP LEARnIng 13", "matching the correct outputs listed in the dataset) for\nthe problem we are trying to solve, the training process\nis completed, and the final model is returned by the algo-\nrithm. This is the point at which the learning in machine\nlearning stops.\nOnce training has finished, the model is fixed. The sec-\nond stage in machine learning is inference. This is when\nthe model is applied to new examples\u2014 examples for\nwhich we do not know the correct output value, and there-\nfore we want the model to generate estimates of this value\nfor us. Most of the work in machine learning is focused on\nhow to train accurate models (i.e., extracting an accurate\nfunction from data). This is because the skills and meth-\nods required to deploy a trained machine learning model\ninto production, in order to do inference on new examples\nat scale, are different from those that a typical data scien-\ntist will possess. There is a growing recognition within the\nindustry of the distinctive skills needed to deploy artifi-\ncial intelligence systems at scale, and this is reflected in a\ngrowing interest in the field known as DevOps, a term de-\nscribing the need for collaboration between development\nand operations teams (the operations team being the\nteam responsible for deploying a developed system into\nproduction and ensuring that these systems are stable and\nscalable). The terms MLOps, for machine learning opera-\ntions, and AIOps, for artificial intelligence operations, are\nalso used to describe the challenges of deploying a trained\n14 ChAPtER 1", "model. The questions around model deployment are be-\nyond the scope of this book, so we will instead focus on\ndescribing what deep learning is, what it can be used for,\nhow it has evolved, and how we can train accurate deep\nlearning models.\nOne relevant question here is: why is extracting a\nfunction from data useful? The reason is that once a func-\ntion has been extracted from a dataset it can be applied\nto unseen data, and the values returned by the function\nin response to these new inputs can provide insight into\nthe correct decisions for these new problems (i.e., it can\nbe used for inference). Recall that a function is simply a\ndeterministic mapping from inputs to outputs. The sim-\nplicity of this definition, however, hides the variety that\nexists within the set of functions. Consider the following\nexamples:\n\u2022 Spam filtering is a function that takes an email as\ninput and returns a value that classifies the email as\nspam (or not).\n\u2022 Face recognition is a function that takes an image as\ninput and returns a labeling of the pixels in the image\nthat demarcates the face in the image.\n\u2022 Gene prediction is a function that takes a genomic DNA\nsequence as input and returns the regions of the DNA\nthat encode a gene.\nIntRoduCtIon to dEEP LEARnIng 15", "\u2022 Speech recognition is a function that takes an audio\nspeech signal as input and returns a textual transcription\nof the speech.\n\u2022 Machine translation is a function that takes a sentence\nin one language as input and returns the translation of\nthat sentence in another language.\nIt is because the solutions to so many problems across so\nmany domains can be framed as functions that machine\nlearning has become so important in recent years.\nWhy Is Machine Learning Difficult?\nThere are a number of factors that make the machine\nlearning task difficult, even with the help of a computer.\nFirst, most datasets will include noise3 in the data, so\nsearching for a function that matches the data exactly is\nnot necessarily the best strategy to follow, as it is equiva-\nlent to learning the noise. Second, it is often the case that\nthe set of possible functions is larger than the set of ex-\namples in the dataset. This means that machine learning\nis an ill-p osed problem: the information given in the prob-\nlem is not sufficient to find a single best solution; instead\nmultiple possible solutions will match the data. We can\nuse the problem of selecting the arithmetic operation (ad-\ndition, subtraction, multiplication, or division) that best\n16 ChAPtER 1", "matches a set of example input- output mappings for an\nunknown function to illustrate the concept of an ill- posed\nproblem. Here are the example mappings for this function\nselection problem:\nfunction(Inputs)=Output\nfunction(1,1)=1\nfunction(2,1)=2\nfunction(3,1)=3\nGiven these examples, multiplication and division are bet-\nter matches for the unknown function than addition and\nsubtraction. However, it is not possible to decide whether\nthe unknown function is actually multiplication or divi-\nsion using this sample of data, because both operations\nare consistent with all the examples provided. Conse-\nquently, this is an ill-p osed problem: it is not possible to\nselect a single best answer given the information provided\nin the problem.\nOne strategy to solve an ill-p osed problem is to col-\nlect more data (more examples) in the hope that the new\nexamples will help us to discriminate between the cor-\nrect underlying function and the remaining alternatives.\nFrequently, however, this strategy is not feasible, either\nIntRoduCtIon to dEEP LEARnIng 17", "because the extra data is not available or is too expensive\nto collect. Instead, machine learning algorithms overcome\nthe ill-p osed nature of the machine learning task by sup-\nplementing the information provided by the data with a\nset of assumptions about the characteristics of the best\nfunction, and use these assumptions to influence the pro-\ncess used by the algorithm that selects the best function\n(or model). These assumptions are known as the inductive\nbias of the algorithm because in logic a process that infers\na general rule from a set of specific examples is known as\ninductive reasoning. For example, if all the swans that you\nhave seen in your life are white, you might induce from\nthese examples the general rule that all swans are white.\nThis concept of inductive reasoning relates to machine\nlearning because a machine learning algorithm induces (or\nextracts) a general rule (a function) from a set of specific\nexamples (the dataset). Consequently, the assumptions\nthat bias a machine learning algorithm are, in effect, bias-\ning an inductive reasoning process, and this is why they\nare known as the inductive bias of the algorithm.\nSo, a machine learning algorithm uses two sources of\ninformation to select the best function: one is the dataset,\nand the other (the inductive bias) is the assumptions that\nbias the algorithm to prefer some functions over others,\nirrespective of the patterns in the dataset. The inductive\nbias of a machine learning algorithm can be understood\nas providing the algorithm with a perspective on a dataset.\n18 ChAPtER 1", "However, just as in the real world, where there is no single\nbest perspective that works in all situations, there is no\nsingle best inductive bias that works well for all datasets.\nThis is why there are so many different machine learning\nalgorithms: each algorithm encodes a different inductive\nbias. The assumptions encoded in the design of a machine\nleanring algorithm can vary in strength. The stronger the\nassumptions the less freedom the algorithm is given in se-\nlecting a function that fits the patterns in the dataset. In a\nsense, the dataset and inductive bias counterbalance each\nother: machine learning algorithms that have a strong in-\nductive bias pay less attention to the dataset when selecting\na function. For example, if a machine learning algorithm\nis coded to prefer a very simple function, no matter how\ncomplex the patterns in the data, then it has a very strong\ninductive bias.\nIn chapter 2 we will explain how we can use the equa-\ntion of a line as a template structure to define a function.\nThe equation of the line is a very simple type of mathemat-\nical function. Machine learning algorithms that use the\nequation of a line as the template structure for the func-\ntions they fit to a dataset make the assumption that the\nmodel they generate should encode a simple linear map-\nping from inputs to output. This assumption is an exam-\nple of an inductive bias. It is, in fact, an example of a strong\ninductive bias, as no matter how complex (or nonlinear)\nIntRoduCtIon to dEEP LEARnIng 19", "the patterns in the data are the algorithm will be restricted\n(or biased) to fit a linear model to it.\nOne of two things can go wrong if we choose a machine\nlearning algorithm with the wrong bias. First, if the in-\nductive bias of a machine learning algorithm is too strong,\nthen the algorithm will ignore important information in\nthe data and the returned function will not capture the\nnuances of the true patterns in the data. In other words,\nthe returned function will be too simple for the domain,4\nand the outputs it generates will not be accurate. This\noutcome is known as the function underfitting the data.\nAlternatively, if the bias is too weak (or permissive), the\nalgorithm is allowed too much freedom to find a function\nthat closely fits the data. In this case, the returned func-\ntion is likely to be too complex for the domain, and, more\nproblematically, the function is likely to fit to the noise in\nthe sample of the data that was supplied to the algorithm\nduring training. Fitting to the noise in the training data\nwill reduce the function\u2019s ability to generalize to new data\n(data that is not in the training sample). This outcome is\nknown as overfitting the data. Finding a machine learning\nalgorithm that balances data and inductive bias appropri-\nately for a given domain is the key to learning a function\nthat neither underfits or overfits the data, and that, there-\nfore, generalizes successfully in that domain (i.e., that is\naccurate at inference, or processing new examples that\nwere not in the training data).\n20 ChAPtER 1", "However, in domains that are complex enough to war-\nrant the use of machine learning, it is not possible in ad-\nvance to know what are the correct assumptions to use\nto bias the selection of the correct model from the data.\nConsequently, data scientists must use their intuition (i.e.,\nmake informed guesses) and also use trial- and- error ex-\nperimentation in order to find the best machine learning\nalgorithm to use in a given domain.\nNeural networks have a relatively weak inductive bias.\nAs a result, generally, the danger with deep learning is that\nthe neural network model will overfit, rather than under-\nfit, the data. It is because neural networks pay so much\nattention to the data that they are best suited to contexts\nwhere there are very large datasets. The larger the dataset,\nthe more information the data provides, and therefore\nit becomes more sensible to pay more attention to the\ndata. Indeed, one of the most important factors driving\nthe emergence of deep learning over the last decade has\nbeen the emergence of Big Data. The massive datasets\nthat have become available through online social plat-\nforms and the proliferation of sensors have combined to\nprovide the data necessary to train neural network mod-\nels to support new applications in a range of domains. To\ngive a sense of the scale of the big data used in deep learn-\ning research, Facebook\u2019s face recognition software, Deep-\nFace, was trained on a dataset of four million facial images\nIntRoduCtIon to dEEP LEARnIng 21", "belonging to more than four thousand identities (Taigman\net al. 2014).\nThe Key Ingredients of Machine Learning\nThe above example of deciding which arithmetic opera-\ntion best explains the relationship between inputs and\noutputs in a set of data illustrates the three key ingredi-\nents in machine learning:\n1. Data (a set of historical examples).\n2. A set of functions that the algorithm will search\nthrough to find the best match with the data.\n3. Some measure of fitness that can be used to evaluate\nhow well each candidate function matches the data.\nAll three of these ingredients must be correct if a machine\nlearning project is to succeed; below we describe each of\nthese ingredients in more detail.\nWe have already introduced the concept of a dataset\nas a two- dimensional table (or n \u00d7 m matrix),5 where each\nrow contains the information for one example, and each\ncolumn contains the information for one of the features\nin the domain. For example, table 1.2 illustrates how the\nsample inputs and outputs of the first unknown arithmetic\n22 ChAPtER 1", "function problem in the chapter can be represented as a\ndataset. This dataset contains four examples (also known\nas instances), and each example is represented using two\ninput features and one output (or target) feature. De-\nsigning and selecting the features to represent the ex-\namples is a very important step in any machine learning\nproject.\nAs is so often the case in computer science, and ma-\nchine learning, there is a tradeoff in feature selection. If\nwe choose to include only a minimal number of features\nin the dataset, then it is likely that a very informative\nfeature will be excluded from the data, and the function\nreturned by the machine learning algorithm will not work\nwell. Conversely, if we choose to include as many features\nas possible in the domain, then it is likely that irrelevant\nor redundant features will be included, and this will also\nlikely result in the function not working well. One reason\nfor this is that the more redundant or irrelevant features\nthat are included, the greater the probability for the ma-\nchine learning algorithm to extract patterns that are based\non spurious correlations between these features. In these\ncases, the algorithm gets confused between the real pat-\nterns in the data and the spurious patterns that only ap-\npear in the data due to the particular sample of examples\nthat have been included in the dataset.\nFinding the correct set of features to include in a\ndataset involves engaging with experts who understand\nIntRoduCtIon to dEEP LEARnIng 23", "the domain, using statistical analysis of the distribution\nof individual features and also the correlations between\npairs of features, and a trial-a nd- error process of building\nmodels and checking the performance of the models when\nparticular features are included or excluded. This process\nof dataset design is a labor- intensive task that often takes\nup a significant portion of the time and effort expended\non a machine learning project. It is, however, a critical task\nif the project is to succeed. Indeed, identifying which fea-\ntures are informative for a given task is frequently where\nthe real value of machine learning projects emerge.\nThe second ingredient in a machine learning project is\nthe set of candidate functions that the algorithm will con-\nsider as the potential explanation of the patterns in the\ndata. In the unknown arithmetic function scenario previ-\nously given, the set of considered functions was explicitly\nspecified and restricted to four: addition, subtraction, mul-\ntiplication, or division. More generally, the set of functions\nis implicitly defined through the inductive bias of the\nTable 1.2. A simple tabular dataset\nInput 1 Input 2 Target\n5 5 25\n2 6 12\n4 4 16\n2 2 04\n24 ChAPtER 1", "machine learning algorithm and the function representa-\ntion (or model) that is being used. For example, a neural\nnetwork model is a very flexible function representation.\nThe third and final ingredient to machine learning is\nthe measure of fitness. The measure of fitness is a function\nthat takes the outputs from a candidate function, gener-\nated when the machine learning algorithm applies the can-\ndidate function to the data, and compares these outputs\nwith the data, in some way. The result of this comparison\nis a value that describes the fitness of the candidate func-\ntion relative to the data. A fitness function that would\nwork for our unknown arithmetic function scenario is to\ncount in how many of the examples a candidate function\nreturns a value that exactly matches the target specified\nin the data. Multiplication would score four out of four\non this fitness measure, addition would score one out of\nfour, and division and subtraction would both score zero\nout of four. There are a large variety of fitness functions\nthat can be used in machine learning, and the selection of\nthe correct fitness function is crucial to the success of a\nmachine learning project. The design of new fitness func-\ntions is a rich area of research in machine learning. Vary-\ning how the dataset is represented, and how the candidate\nfunctions and the fitness function are defined, results in\nthree different categories of machine learning: supervised,\nunsupervised, and reinforcement learning.\nIntRoduCtIon to dEEP LEARnIng 25", "Supervised, Unsupervised, and Reinforcement Learning\nSupervised machine learning is the most common type of\nmachine learning. In supervised machine learning, each\nexample in the dataset is labeled with the expected output\n(or target) value. For example, if we were using the dataset\nin table 1.1 to learn a function that maps from the inputs\nof annual income and debt to a credit solvency score, the\ncredit solvency feature in the dataset would be the target\nfeature. In order to use supervised machine learning, our\ndataset must list the value of the target feature for every\nexample in the dataset. These target feature values can\nsometimes be very difficult, and expensive, to collect. In\nsome cases, we must pay human experts to label each ex-\nample in a dataset with the correct target value. However,\nthe benefit of having these target values in the dataset is\nthat the machine learning algorithm can use these values\nto help the learning process. It does this by comparing the\noutputs a function produces with the target outputs speci-\nfied in the dataset, and using the difference (or error) to\nevaluate the fitness of the candidate function, and use the\nfitness evaluation to guide the search for the best func-\ntion. It is because of this feedback from the target labels\nin the dataset to the algorithm that this type of machine\nlearning is considered supervised. This is the type of ma-\nchine learning that was demonstrated by the example of\n26 ChAPtER 1", "choosing between different arithmetic functions to ex-\nplain the behavior of an unknown function.\nUnsupervised machine learning is generally used for\nclustering data. For example, this type of data analysis\nis useful for customer segmentation, where a company\nwishes to segment its customer base into coherent groups\nso that it can target marketing campaigns and/or product\ndesigns to each group. In unsupervised machine learning,\nthere are no target values in the dataset. Consequently,\nthe algorithm cannot directly evaluate the fitness of a\ncandidate function against the target values in the dataset.\nInstead, the machine learning algorithm tries to identify\nfunctions that map similar examples into clusters, such\nthat the examples in a cluster are more similar to the other\nexamples in the same cluster than they are to examples in\nother clusters. Note that the clusters are not prespecified,\nor at most they are initially very underspecified. For ex-\nample, the data scientist might provide the algorithm with\na target number of clusters, based on some intuition about\nthe domain, without providing explicit information on\nrelative sizes of the clusters or regarding the characteris-\ntics of examples that belong in each cluster. Unsupervised\nmachine learning algorithms often begin by guessing an\ninitial clustering of the examples and then iteratively\nadjusting the clusters (by dropping instances from one\ncluster and adding them to another) so as to improve the\nfitness of the cluster set. The fitness functions used in\nIntRoduCtIon to dEEP LEARnIng 27", "unsupervised machine learning generally reward candi-\ndate functions that result in higher similarity within in-\ndividual clusters and, also, high diversity between clusters.\nReinforcement learning is most relevant for online\ncontrol tasks, such as robot control and game playing. In\nthese scenarios, an agent needs to learn a policy for how it\nshould act in an environment in order to be rewarded. In\nreinforcement learning, the goal of the agent is to learn\na mapping from its current observation of the environ-\nment and its own internal state (its memory) to what\naction it should take: for instance, should the robot move\nforward or backward or should the computer program move\nthe pawn or take the queen. The output of this policy (func-\ntion) is the action that the agent should take next, given\nthe current context. In these types of scenarios, it is dif-\nficult to create historic datasets, and so reinforcement\nlearning is often carried out in situ: an agent is released\ninto an environment where it experiments with different\npolicies (starting with a potentially random policy) and\nover time updates its policy in response to the rewards it\nreceives from the environment. If an action results in a\npositive reward, the mapping from the relevant observa-\ntions and state to that action is reinforced in the policy,\nwhereas if an action results in a negative reward, the map-\nping is weakened. Unlike in supervised and unsupervised\nmachine learning, in reinforcement learning, the fact\nthat learning is done in situ means that the training and\n28 ChAPtER 1", "inference stages are interleaved and ongoing. The agent\ninfers what action it should do next and uses the feedback\nfrom the environment to learn how to update its policy.\nA distinctive aspect of reinforcement learning is that the\ntarget output of the learned function (the agent\u2019s actions)\nis decoupled from the reward mechanism. The reward\nmay be dependent on multiple actions and there may be\nno reward feedback, either positive or negative, available\ndirectly after an action has been performed. For example,\nin a chess scenario, the reward may be +1 if the agent wins\nthe game and - 1 if the agent loses. However, this reward\nfeedback will not be available until the last move of the\ngame has been completed. So, one of the challenges in re-\ninforcement learning is designing training mechanisms\nthat can distribute the reward appropriately back through\na sequence of actions so that the policy can be updated\nappropriately. Google\u2019s DeepMind Technologies gener-\nated a lot of interest by demonstrating how reinforcement\nlearning could be used to train a deep learning model to\nlearn control policies for seven different Atari computer\ngames (Mnih et al. 2013). The input to the system was\nthe raw pixel values from the screen, and the control poli-\ncies specified what joystick action the agent should take at\neach point in the game. Computer game environments are\nparticularly suited to reinforcement learning as the agent\ncan be allowed to play many thousands of games against\nthe computer game system in order to learn a successful\nIntRoduCtIon to dEEP LEARnIng 29", "policy, without incurring the cost of creating and labeling\na large dataset of example situations with correct joystick\nactions. The DeepMind system got so good at the games\nthat it outperformed all previous computer systems on six\nof the seven games, and outperformed human experts on\nthree of the games.\nDeep learning can be applied to all three machine\nlearning scenarios: supervised, unsupervised, and rein-\nforcement. Supervised machine learning is, however, the\nmost common type of machine learning. Consequently,\nthe majority of this book will focus on deep learning in a\nsupervised learning context. However, most of the deep\nlearning concerns and principles introduced in the super-\nvised learning context also apply to unsupervised and re-\ninforcement learning.\nWhy Is Deep Learning So Successful?\nIn any data-d riven process the primary determinant of\nsuccess is knowing what to measure and how to measure it.\nThis is why the processes of feature selection and feature\ndesign are so important to machine learning. As discussed\nabove, these tasks can require domain expertise, statis-\ntical analysis of the data, and iterations of experiments\nbuilding models with different feature sets. Consequently,\ndataset design and preparation can consume a significant\n30 ChAPtER 1", "In any data- driven\nprocess the primary\ndeterminant of success\nis knowing what to\nmeasure and how to\nmeasure it.", "portion of time and resources expended in the project, in\nsome cases approaching up to 80% of the total budget of\na project (Kelleher and Tierney 2018). Feature design is\none task in which deep learning can have a significant ad-\nvantage over traditional machine learning. In traditional\nmachine learning, the design of features often requires a\nlarge amount of human effort. Deep learning takes a dif-\nferent approach to feature design, by attempting to auto-\nmatically learn the features that are most useful for the\ntask from the raw data.\nTo give an example of feature design, a person\u2019s body\nmass index (BMI) is the ratio of a person\u2019s weight (in ki-\nlograms) divided by their height (in meters squared). In a\nmedical setting, BMI is used to categorize people as under-\nweight, normal, overweight, or obese. Categorizing people\nin this way can be useful in predicting the likelihood of\na person developing a weight- related medical condition,\nsuch as diabetes. BMI is used for this categorization be-\ncause it enables doctors to categorize people in a manner\nthat is relevant to these weight-r elated medical condi-\ntions. Generally, as people get taller they also get heavier.\nHowever, most weight- related medical conditions (such as\ndiabetes) are not affected by a person\u2019s height but rather\nthe amount they are overweight compared to other peo-\nple of a similar stature. BMI is a useful feature to use for\nthe medical categorization of a person\u2019s weight because\nit takes the effect of height on weight into account. BMI\n32 ChAPtER 1", "is an example of a feature that is derived (or calculated)\nfrom raw features; in this case the raw features are weight\nand height. BMI is also an example of how a derived fea-\nture can be more useful in making a decision than the raw\nfeatures that it is derived from. BMI is a hand- designed\nfeature: Adolphe Quetelet designed it in the eighteenth\ncentury.\nAs mentioned above, during a machine learning proj-\nect a lot of time and effort is spent on identifying, or de-\nsigning, (derived) features that are useful for the task the\nproject is trying to solve. The advantage of deep learn-\ning is that it can learn useful derived features from data\nautomatically (we will discuss how it does this in later\nchapters). Indeed, given large enough datasets, deep\nlearning has proven to be so effective in learning fea-\ntures that deep learning models are now more accurate\nthan many of the other machine learning models that use\nhand- engineered features. This is also why deep learning\nis so effective in domains where examples are described\nwith very large numbers of features. Technically datasets\nthat contain large numbers of features are called high-\ndimensional. For example, a dataset of photos with a fea-\nture for each pixel in a photo would be high-d imensional.\nIn complex high-d imensional domains, it is extremely\ndifficult to hand- engineer features: consider the chal-\nlenges of hand- engineering features for face recognition\nor machine translation. So, in these complex domains,\nIntRoduCtIon to dEEP LEARnIng 33", "adopting a strategy whereby the features are automati-\ncally learned from a large dataset makes sense. Related\nto this ability to automatically learn useful features, deep\nlearning also has the ability to learn complex nonlinear\nmappings between inputs and outputs; we will explain\nthe concept of a nonlinear mapping in chapter 3, and in\nchapter 6 we will explain how these mappings are learned\nfrom data.\nSummary and the Road Ahead\nThis chapter has focused on positioning deep learning\nwithin the broader field of machine learning. Consequently,\nmuch of this chapter has been devoted to introducing ma-\nchine learning. In particular, the concept of a function as a\ndeterministic mapping from inputs to outputs was intro-\nduced, and the goal of machine learning was explained as\nfinding a function that matches the mappings from input\nfeatures to the output features that are observed in the\nexamples in the dataset.\nWithin this machine learning context, deep learn-\ning was introduced as the subfield of machine learning\nthat focuses on the design and evaluation of training\nalgorithms and model architectures for modern neural\nnetworks. One of the distinctive aspects of deep learn-\ning within machine learning is the approach it takes to\n34 ChAPtER 1", "feature design. In most machine learning projects, feature\ndesign is a human-i ntensive task that can require deep\ndomain expertise and consume a lot of time and project\nbudget. Deep learning models, on the other hand, have\nthe ability to learn useful features from low-l evel raw\ndata, and complex nonlinear mappings from inputs to\noutputs. This ability is dependent on the availability of\nlarge datasets; however, when such datasets are available,\ndeep learning can frequently outperform other machine\nlearning approaches. Furthermore, this ability to learn\nuseful features from large datasets is why deep learning\ncan often generate highly accurate models for complex do-\nmains, be it in machine translation, speech processing, or\nimage or video processing. In a sense, deep learning has\nunlocked the potential of big data. The most noticeable\nimpact of this development has been the integration of\ndeep learning models into consumer devices. However,\nthe fact that deep learning can be used to analyze massive\ndatasets also has implications for our individual privacy\nand civil liberty (Kelleher and Tierney 2018). This is why\nunderstanding what deep learning is, how it works, and\nwhat it can and can\u2019t be used for, is so important. The\nroad ahead is as follows:\n\u2022 Chapter 2 introduces some of the foundational\nconcepts of deep learning, including what a model is,\nhow the parameters of a model can be set using data, and\nIntRoduCtIon to dEEP LEARnIng 35", "how we can create complex models by combining simple\nmodels.\n\u2022 Chapter 3 explains what neural networks are, how\nthey work, and what we mean by a deep neural\nnetwork.\n\u2022 Chapter 4 presents a history of deep learning. This\nhistory focuses on the major conceptual and technical\nbreakthroughs that have contributed to the development\nof the field of machine learning. In particular, it provides\na context and explanation for why deep learning has seen\nsuch rapid development in recent years.\n\u2022 Chapter 5 describes the current state of the field, by\nintroducing the two deep neural architectures that are\nthe most popular today: convolutional neural networks\nand recurrent neural networks. Convolutional neural\nnetworks are ideally suited to processing image and\nvideo data. Recurrent neural networks are ideally suited\nto processing sequential data such as speech, text, or\ntime- series data. Understanding the differences and\ncommonalities across these two architectures will give\nyou an awareness of how a deep neural network can be\ntailored to the characteristics of a specific type of data,\nand also an appreciation of the breadth of the design\nspace of possible network architectures.\n36 ChAPtER 1", "\u2022 Chapter 6 explains how deep neural networks\nmodels are trained, using the gradient descent and\nbackpropagation algorithms. Understanding these two\nalgorithms will give you a real insight into the state\nof artificial intelligence. For example, it will help you\nto understand why, given enough data, it is currently\npossible to train a computer to do a specific task within a\nwell- defined domain at a level beyond human capabilities,\nbut also why a more general form of intelligence is still an\nopen research challenge for artificial intelligence.\n\u2022 Chapter 7 looks to the future in the field of deep\nlearning. It reviews the major trends driving the\ndevelopment of deep learning at present, and how they\nare likely to contribute to the development of the field\nin the coming years. The chapter also discusses some of\nthe challenges the field faces, in particular the challenge\nof understanding and interpreting how a deep neural\nnetwork works.\nIntRoduCtIon to dEEP LEARnIng 37", "", "2\nCONCEPTUAL FOUNDATIONS\nThis chapter introduces some of the foundational concepts\nthat underpin deep learning. The basis of this chapter is\nto decouple the initial presentation of these concepts from\nthe technical terminology used in deep learning, which is\nintroduced in subsequent chapters.\nA deep learning network is a mathematical model that\nis (loosely) inspired by the structure of the brain. Conse-\nquently, in order to understand deep learning it is helpful\nto have an intuitive understanding of what a mathemati-\ncal model is, how the parameters of a model can be set,\nhow we can combine (or compose) models, and how we\ncan use geometry to understand how a model processes\ninformation.", "What Is a Mathematical Model?\nIn its simplest form, a mathematical model is an equa-\ntion that describes how one or more input variables are\nrelated to an output variable. In this form a mathematical\nmodel is the same as a function: a mapping from inputs\nto outputs.\nIn any discussion relating to models, it is important\nto remember the statement by George Box that all models\nare wrong but some are useful! For a model to be useful it\nmust have a correspondence with the real world. This cor-\nrespondence is most obvious in terms of the meaning that\ncan be associated with a variable. For example, in isola-\ntion a value such as 78,000 has no meaning because it has\nno correspondence with concepts in the real world. But\nyearly income=$78,000 tells us how the number describes\nan aspect of the real world. Once the variables in a model\nhave a meaning, we can understand the model as describ-\ning a process through which different aspects of the world\ninteract and cause new events. The new events are then\ndescribed by the outputs of the model.\nA very simple template for a model is the equation of\na line:\ny=mx+c\nIn this equation y is the output variable, x is the input\nvariable, and m and c are two parameters of the model\n40 ChAPtER 2", "that we can set to adjust the relationship the model de-\nfines between the input and the output.\nImagine we have a hypothesis that yearly income af-\nfects a person\u2019s happiness and we wish to describe the\nrelationship between these two variables.1 Using the equa-\ntion of a line, we could define a model to describe this\nrelationship as follows:\nhappiness=m\u00d7income+c\nThis model has a meaning because the variables in the\nmodel (as distinct from the parameters of the model)\nhave a correspondence with concepts from the real world.\nTo complete our model, we have to set the values of the\nmodel\u2019s parameters: m and c. Figure 2.1 illustrates how\nvarying the values of each of these parameters changes\nthe relationship defined by the model between income and\nhappiness.\nOne important thing to notice in this figure is that no\nmatter what values we set the model parameters to, the re-\nlationship defined by the model between the input and the\noutput variable can be plotted as a line. This is not surpris-\ning because we used the equation of a line as the template\nto define our model, and this is why mathematical models\nthat are based on the equation of a line are known as linear\nmodels. The other important thing to notice in the figure\nConCEPtuAL FoundAtIons 41", "01\n8\n)01\nfo\ntuo( 6\nssenippaH\n4\n2\nc = 1,m = 0.08\nc = 1,m = 0.06\nc = 4,m = 0.02\n0\n0 20 40 60 80 100\nIncome ($1,000s)\nFigure 2.1 Three different linear models of how income affects happiness.\nis how changing the parameters of the model changes the\nrelationship between income and happiness.\n(c=1,m=0.08)\nThe solid steep line, with parameters ,\nis a model of the world in which people with zero income\nhave a happiness level of 1, and increases in income have\na significant effect on people\u2019s happiness. The dashed line,\nwith parameters (c=1,m=0.06), is a model in which peo-\nple with zero income have a happiness level of 1 and in-\ncreased income increases happiness, but at the slower rate\ncompared to the world modeled by the solid line. Finally,\n42 ChAPtER 2", "the dotted line, parameters (c=4,m=0.02), is a model\nof the world where no one is particularly unhappy\u2014 even\npeople with zero income have a happiness of 4 out of 10\u2014\nand although increases in income do affect happiness, the\neffect is moderate. This third model assumes that income\nhas a relatively weak effect on happiness.\nMore generally, the differences between the three\nmodels in figure 2.1 show how making changes to the\nparameters of a linear model changes the model. Chang-\ning c causes the line to move up and done. This is most\nclearly seen if we focus on the y- axis: notice that the line\ndefined by a model always crosses (or intercepts) the\ny- axis at the value that c is set to. This is why the c pa-\nrameter in a linear model is known as the intercept. The\nintercept can be understood as specifying the value of the\noutput variable when the input variable is zero. Chang-\ning the m parameter changes the angle (or slope) of the\nline. The slope parameter controls how quickly changes in\nincome effect changes in happiness. In a sense, the slope\nvalue is a measure of how important income is to happi-\nness. If income is very important (i.e., if small changes in\nincome result in big changes in happiness), then the slope\nparameter of our model should be set to a large value. An-\nother way of understanding this is to think of a slope pa-\nrameter of a linear model as describing the importance, or\nweight, of the input variable in determining the value of\nthe output.\nConCEPtuAL FoundAtIons 43", "Linear Models with Multiple Inputs\nThe equation of a line can be used as a template for math-\nematical models that have more than one input variable.\nFor example, imagine yourself in a scenario where you\nhave been hired by a financial institution to act as a loan\nofficer and your job involves deciding whether or not a\nloan application should be granted. From interviewing\ndomain experts you come up with a hypothesis that a use-\nful way to model a person\u2019s credit solvency is to consider\nboth their yearly income and their current debts. If we as-\nsume that there is a linear relationship between these two\ninput variables and a person\u2019s credit solvency, then the\nappropriate mathematical model, written out in English\nwould be:\nsolvency=(income\u00d7weight for income)\n+(debt\u00d7weight for debt)++intercept\nNotice that in this model the m parameter has been re-\nplaced by a separate weight for each input variable, with\neach weight representing the importance of its associated\ninput in determining the output. In mathematical nota-\ntion this model would be written as:\ny=(input \u00d7weight )+(input \u00d7weight )+c\n1 1 2 2\n44 ChAPtER 2", "where y represents the credit solvency output, input rep-\n1\nresents the income variable, input represents the debt\n2\nvariable, and c represents the intercept. Using the idea of\nadding a new weight for each new input to the model al-\nlows us to scale the equation of a line to as many inputs as\nwe like. All the models defined in this way are still linear\nwithin the dimensions defined by the number of inputs\nand the output. What this means is that a linear model\nwith two inputs and one output defines a flat plane rather\nthan a line because that is what a two-d imensional line\nthat has been extruded to three dimensions looks like.\nIt can become tedious to write out a mathematical\nmodel that has a lot of inputs, so mathematicians like to\nwrite things in as compact a form as possible. With this\nin mind, the above equation is sometimes written in the\nshort form:\nn\ny=\u2211(input \u00d7weight )+c\ni i\ni=1\nThis notation tells us that to calculate the output variable\ny we must first go through all n inputs and multiple each\ninput by its corresponding weight, then we should sum\ntogether the results of these n multiplications, and finally\nwe add the c intercept parameter to the result of the sum-\nmation. The \u2211 symbol tells us that we use addition to\ncombine the results of the multiplications, and the index i\nConCEPtuAL FoundAtIons 45", "tells us that we multiply each input by the weight with the\nsame index. We can make our notation even more compact\nby treating the intercept as a weight. One way to do this is\nto assume an input that is always equal to 1 and to treat\n0\nthe intercept as the weight on this input, that is, weight .\n0\nDoing this allows us to write out the model as follows:\nn\ny=\u2211(input \u00d7weight )\ni i\ni=0\nNotice that the index now starts at 0, rather than 1, be-\ncause we are now assuming an extra input, input =1, and\n0\nwe have relabeled the intercept weight .\n0\nAlthough we can write down a linear model in a num-\nber of different ways, the core of a linear model is that the\noutput is calculated as the sum of the n input values mul-\ntiplied by their corresponding weights. Consequently, this\ntype of model defines a calculation known as a weighted\nsum, because we weight each input and sum the results.\nAlthough a weighted sum is easy to calculate, it turns out\nto be very useful in many situations, and it is the basic cal-\nculation used in every neuron in a neural network.\nSetting the Parameters of a Linear Model\nLet us return to our working scenario where we wish\nto create a model that enables us to calculate the credit\n46 ChAPtER 2", "The multiplication of\ninputs by weights,\nfollowed by a\nsummation, is known\nas a weighted sum.", "solvency of individuals who have applied for a financial\nloan. For simplicity in presentation we will ignore the\nintercept parameter in this discussion as it is treated the\nsame as the other parameters (i.e., the weights on the in-\nputs). So, dropping the intercept parameter, we have the\nfollowing linear model (or weighted sum) of the relation-\nship between a person\u2019s income and debt to their credit\nsolvency:\nsolvency=(income\u00d7weight for income)\n+(debt\u00d7weight for debt)\nIn order to complete our model, we need to specify the pa-\nrameters of the model; that is, we need to specify the value\nof the weight for each input. One way to do this would be\nto use our domain expertise to come up with values for\neach of the parameters.\nFor example, if we assume that an increase in a per-\nson\u2019s income has a bigger impact on their credit solvency\nthan a similar increase in their debt, we should set the\nweighting for income to be larger than that of the debt.\nThe following model encodes this assumption; in par-\nticular this model specifies that income is three times\nas important as debt in determining a person\u2019s credit\nsolvency:\nsolvency=(income\u00d73)+(debt\u00d71)\n48 ChAPtER 2", "The drawback with using domain knowledge to set the\nparameters of a model is that experts often disagree. For\nexample, you may think that weighting income as three\ntimes as important as debt is not realistic; in that case the\nmodel can be adjusted by, for example, setting both in-\ncome and debt to have an equal weighting, which would be\nequivalent to assuming that income and debt are equally\nimportant in determining credit solvency. One way to\navoid arguments between experts is to use data to set the\nparameters. This is where machine learning helps. The\nlearning done by machine learning is finding the param-\neters (or weights) of a model using a dataset.\nLearning Model Parameters from Data\nLater in the book we will describe the standard algorithm\nused to learn the weights for a linear model, known as the\ngradient descent algorithm. However, we can give a brief\npreview of the algorithm here. We start with a dataset con-\ntaining a set of examples for which we have both the input\nvalues (income and debt) and the output value (credit sol-\nvency). Table 2.1 illustrates such a dataset from our credit\nsolvency scenario.2\nWe then begin the process of learning the weights by\nguessing initial values for each weight. It is very likely that\nthis initial, guessed, model will be a very bad model. This\nConCEPtuAL FoundAtIons 49", "The learning done by\nmachine learning is\nfinding the parameters\n(or weights) of a model\nusing a dataset.", "Table 2.1. A dataset of loan applications and known\ncredit solvency rating of the applicant\nID Annual income Current debt Credit solvency\n1 $150 - $100 100\n2 $250 - $300 - 50\n3 $450 - $250 400\n4 $200 - $350 - 300\nis not a problem, however, because we will use the dataset\nto iteratively update the weights so that the model gets\nbetter and better, in terms of how well it matches the data.\nFor the purpose of the example, we will use the model de-\nscribed above as our initial (guessed) model:\nsolvency=(income\u00d73)+(debt\u00d71)\nThe general process for improving the weights of the\nmodel is to select an example from the dataset and feed\nthe input values from the example into the model. This\nallows us to calculate an estimate of the output value for\nthe example. Once we have this estimated output, we can\ncalculate the error of the model on the example by sub-\ntracting the estimated output from the correct output for\nthe example listed in the dataset. Using the error of the\nmodel on the example, we can improve how well the model\nConCEPtuAL FoundAtIons 51", "fits the data by updating the weights in the model using\nthe following strategy, or learning rule:\n\u2022 If the error is 0, then we should not change the weights\nof the model.\n\u2022 If the error is positive, then the output of the model\nwas too low, so we should increase the output of the\nmodel for this example by increasing the weights for\nall the inputs that had positive values for the example\nand decreasing the weights for all the inputs that had\nnegative values for the example.\n\u2022 If the error is negative, then the output of the model\nwas too high, so we should decrease the output of the\nmodel for this example by decreasing the weights for\nall the inputs that had positive values for the example\nand increasing the weights for all the inputs that had\nnegative values for the example.\nTo illustrate the weight update process we will use ex-\nample 1 from table 2.1 (income = 150, debt = -1 00, and\nsolvency = 100) to test the accuracy of our guessed model\nand update the weights according to the resulting error.\nsolvency=(income\u00d73)+(debt\u00d71)\n=(150\u00d73)+(\u2212100\u00d71)\n=350\n52 ChAPtER 2", "When the input values for the example are passed into\nthe model, the credit solvency estimate returned by the\nmodel is 350. This is larger than the credit solvency listed\nfor this example in the dataset, which is 100. As a result,\nthe error of the model is negative (100 \u2013 350 = \u20132 50);\ntherefore, following the learning rule described above, we\nshould decrease the output of the model for this example\nby decreasing the weights for positive inputs and increas-\ning the weights for negative inputs. For this example, the\nincome input had a positive value and the debt input had\na negative value. If we decrease the weight for income by 1\nand increase the weight for debt by 1, we end up with the\nfollowing model:\nsolvency=(income\u00d72)+(debt\u00d72)\nWe can test if this weight update has improved the\nmodel by checking if the new model generates a better\nestimate for the example than the old model. The follow-\ning illustrates pushing the same example through the new\nmodel:\nsolvency=(income\u00d72)+(debt\u00d72)\n=(150\u00d72)+(\u2212100\u00d72)\n=100\nThis time the credit solvency estimate generated by the\nmodel matches the value in the dataset, showing that the\nConCEPtuAL FoundAtIons 53", "updated model fits the data more closely than the original\nmodel. In fact, this new model generates the correct out-\nput for all the examples in the dataset.\nIn this example, we only needed to update the weights\nonce in order to find a set of weights that made the be-\nhavior of the model consistent with all the examples in\nthe dataset. Typically, however, it takes many iterations\nof presenting examples and updating weights to get a\ngood model. Also, in this example, we have, for the sake\nof simplicity, assumed that the weights are updated by\neither adding or subtracting 1 from them. Generally, in\nmachine learning, the calculation of how much to update\neach weight by is more complicated than this. However,\nthese differences aside, the general process outlined here\nfor updating the weights (or parameters) of a model in or-\nder to fit the model to a dataset is the learning process at\nthe core of deep learning.\nCombining Models\nWe now understand how we can specify a linear model to\nestimate an applicant\u2019s credit solvency, and how we can\nmodify the parameters of the model in order to fit the\nmodel to a dataset. However, as a loan officer our job is\nnot simply to calculate an applicant\u2019s credit solvency; we\nhave to decide whether to grant the loan application or\nnot. In other words, we need a rule that will take a credit\n54 ChAPtER 2", "solvency score as input and return a decision on the loan\napplication. For example, we might use the decision rule\nthat a person with a credit solvency above 200 will be granted\na loan. This decision rule is also a model: it maps an input\nvariable, in this case credit solvency, to an output variable,\nloan decision.\nUsing this decision rule we can adjudicate on a loan\napplication by first using the model of credit solvency to\nconvert a loan applicant\u2019s profile (described in terms of the\nannual income and debt) into a credit solvency score, and\nthen passing the resulting credit solvency score through\nour decision rule model to generate the loan decision. We\ncan write this process out in a pseudomathematical short-\nhand as follows:\nloan decision\n=decision rule(solvency=(income\u00d72)+(debt\u00d72))\nUsing this notation, the entire decision process for ad-\njudicating the loan application for example 1 from\ntable 2.1 is:\nloan decision\n=decision rule(solvency=(income\u00d72)+(debt\u00d72))\n=decision rule(solvency=(150\u00d72)+(\u2212100\u00d72))\n=decision rule(solvency=100)\n=reject\nConCEPtuAL FoundAtIons 55", "We are now in a position where we can use a model\n(composed of two simpler models, a decision rule and a\nweighted sum) to describe how a loan decision is made.\nWhat is more, if we use data from previous loan applica-\ntions to set the parameters (i.e., the weights) of the model,\nour model will correspond to how we have processed pre-\nvious loan applications. This is useful because we can use\nthis model to process new applications in a way that is\nconsistent with previous decisions. If a new loan applica-\ntion is submitted, we simply use our model to process the\napplication and generate a decision. It is this ability to\napply a mathematical model to new examples that makes\nmathematical modeling so useful.\nWhen we use the output of one model as the input\nto another model, we are creating a third model by com-\nbining two models. This strategy of building a complex\nmodel by combining smaller simpler models is at the core\nof deep learning networks. As we will see, a neural net-\nwork is composed of a large number of small units called\nneurons. Each of these neurons is a simple model in its\nown right that maps from a set of inputs to an output.\nThe overall model implemented by the network is cre-\nated by feeding the outputs from one group of neurons as\ninputs into a second group of neurons and then feeding\nthe outputs of the second group of neurons as inputs to\na third group of neurons, as so on, until the final output\nof the model is generated. The core idea is that feeding\n56 ChAPtER 2", "the outputs of some neuron as inputs to other neurons\nenables these subsequent neurons to learn to solve a dif-\nferent part of the overall problem the network is trying to\nsolve by building on the partial solutions implemented by\nthe earlier neurons\u2014 in a similar way to the way the deci-\nsion rule generates the final adjudication for a loan appli-\ncation by building on the calculation of the credit solvency\nmodel. We will return to this topic of model composition\nin subsequent chapters.\nInput Spaces, Weight Spaces, and Activation Spaces\nAlthough mathematical models can be written out as\nequations, it is often useful to understand the geomet-\nric meaning of a model. For example, the plots in figure\n2.1 helped us understand how changes in the parameters\nof a linear model changed the relationship between the\nvariables that the model defined. There are a number of\ngeometric spaces that it is useful to distinguish between,\nand understand, when we are discussing neural networks.\nThese are the input space, the weight space, and the activa-\ntion space of a neuron. We can use the decision model for\nloan applications that we defined in the previous section\nto explain these three different types of spaces.\nWe will begin by describing the concept of an input\nspace. Our loan decision model took two inputs: the\nConCEPtuAL FoundAtIons 57", "annual income and current debt of the applicant. Table\n2.1 listed these input values for four example loan applica-\ntions. We can plot the input space of this model by treating\neach of the input variables as the axis of a coordinate sys-\ntem. This coordinate space is referred to as the input space\nbecause each point in this space defines a possible com-\nbination of input values to the model. For example, the\nplot at the top-l eft of figure 2.2 shows the position of each\nof the four example loan applications within the models\ninput space.\nThe weight space for a model describes the universe of\npossible weight combinations that a model might use. We\ncan plot the weight space for a model by defining a coor-\ndinate system with one axis per weight in the model. The\nloan decision model has only two weights, one weight for\nthe annual income input, and one weight for the current\ndebt input. Consequently, the weight space for this model\nhas two dimensions. The plot at the top-r ight of figure\n2.2 illustrates a portion of the weight space for this model.\nThe location of the weight combination used by the model\n2,2 is highlighted in this figure. Each point within this co-\nordinate system describes a possible set of weights for the\nmodel, and therefore corresponds to a different weighted\nsum function within the model. Consequently, moving\nfrom one location to another within this weight space is\nequivalent to changing the model because it changes the\nmapping from inputs to output that the model defines.\n58 ChAPtER 2", "Figure 2.2 There are four different coordinate spaces related to the\nprocessing of the loan decision model: top-l eft plots the input space; top-r ight\nplots the weight space; bottom-l eft plots the activation (or decision) space;\nand bottom-r ight plots the input space with the decision boundary plotted.\nConCEPtuAL FoundAtIons 59", "A linear model maps a set of input values to a point\nin a new space by applying a weighted sum calculation to\nthe inputs: multiply each input by a weight, and sum the\nresults of the multiplication. In our loan decision model it\nis in this space that we apply our decision rule. Thus, we\ncould call this space the decision space, but, for reasons\nthat will become clear when we describe the structure of\na neuron in the next chapter, we call this space the activa-\ntion space. The axes of a model\u2019s activation space corre-\nspond to the weighted inputs to the model. Consequently,\neach point in the activation space defines a set of weighted\ninputs. Applying a decision rule, such as our rule that a\nperson with a credit solvency above 200 will be granted a loan,\nto each point in this activation space, and recording the\nresult of the decision for each point, enables us to plot the\ndecision boundary of the model in this space. The decision\nboundary divides those points in the activation space that\nexceed the threshold, from those points in the space below\nthe threshold. The plot in the bottom-l eft of figure 2.2 il-\nlustrates the activation space for our loan decision model.\nThe positions of the four example loan applications listed\nin table 2.1 when they are projected into this activation\nspace are shown. The diagonal black line in this figure\nshows the decision boundary. Using this threshold, loan\napplication number three is granted and the other loan\napplications are rejected. We can, if we wish, project the\ndecision boundary back into the original input space by\n60 ChAPtER 2", "recording for each location in the input space which side of\nthe decision boundary in the activation space it is mapped\nto by the weighted sum function. The plot at the bottom-\nright of figure 2.2 shows the decision boundary in the\noriginal input space (note the change in the values on the\naxes) and was generated using this process. We will return\nto the concepts of weight spaces and decision boundar-\nies in next chapter when we describe how adjusting the\nparameters of a neuron changes the set of input combina-\ntions that cause the neuron to output a high activation.\nSummary\nThe main idea presented in this chapter is that a linear\nmathematical model, be it expressed as an equation or\nplotted as a line, describes a relationship between a set of\ninputs and an output. Be aware that not all mathematical\nmodels are linear models, and we will come across nonlin-\near models in this book. However, the fundamental cal-\nculation of a weighted sum of inputs does define a linear\nmodel. Another big idea introduced in this chapter is that\na linear model (a weighted sum) has a set of parameters,\nthat is, the weights used in the weighted sum. By chang-\ning these parameters we can change the relationship the\nmodel describes between the inputs and the output. If we\nwish we could set these weights by hand using our domain\nConCEPtuAL FoundAtIons 61", "expertise; however, we can also use machine learning to\nset the weights of the model so that the behavior of the\nmodel fits the patterns found in a dataset. The last big\nidea introduced in this chapter was that we can build com-\nplex models by combining simpler models. This is done by\nusing the output from one (or more) models as input(s)\nto another model. We used this technique to define our\ncomposite model to make loan decisions. As we will see in\nthe next chapter, the structure of a neuron in a neural net-\nwork is very similar to the structure of this loan decision\nmodel. Just like this model, a neuron calculates a weighted\nsum of its inputs and then feeds the result of this calcula-\ntion into a second model that decides whether the neuron\nactivates or not.\nThe focus of this chapter has been to introduce some\nfoundational concepts before we introduce the terminol-\nogy of machine learning and deep learning. To give a quick\noverview of how the concepts introduced in this chapter\nmap over to machine learning terminology, our loan deci-\nsion model is equivalent to a two- input neuron that uses\na threshold activation function. The two financial indica-\ntors (annual income and current debt) are analogous to\nthe inputs the neuron receives. The terms input vector or\nfeature vector are sometimes used to refer to the set of in-\ndicators describing a single example; in this context an ex-\nample is a single loan applicant, described in terms of two\nfeatures: annual income and current debt. Also, just like\n62 ChAPtER 2", "the loan decision model, a neuron associates a weight with\neach input. And, again, just like the loan decision model, a\nneuron multiplies each input by its associated weight and\nsums the results of these multiplications in order to calcu-\nlate an overall score for the inputs. Finally, similar to the\nway we applied a threshold to the credit solvency score to\nconvert it into a decision of whether to grant or reject the\nloan application, a neuron applies a function (known as\nan activation function) to convert the overall score of the\ninputs. In the earliest types of neurons, these activation\nfunctions were actually threshold functions that worked\nin exactly the same way as the score threshold used in this\ncredit scoring example. In more recent neural networks,\ndifferent types of activation functions (for example, the\nlogistic, tanh, or ReLU functions) are used. We will intro-\nduce these activation functions in the next chapter.\nConCEPtuAL FoundAtIons 63", "", "3\nNEURAL NETWORKS:\nTHE BUILDING BLOCKS\nOF DEEP LEARNING\nThe term deep learning describes a family of neural network\nmodels that have multiple layers of simple information\nprocessing programs, known as neurons, in the network.\nThe focus of this chapter is to provide a clear and com-\nprehensive introduction to how these neurons work and\nare interconnected in artificial neural networks. In later\nchapters, we will explain how neural networks are trained\nusing data.\nA neural network is a computational model that is in-\nspired by the structure of the human brain. The human\nbrain is composed of a massive number of nerve cells,\ncalled neurons. In fact, some estimates put the number\nof neurons in the human brain at one hundred billion\n(Herculano- Houzel 2009). Neurons have a simple three-\npart structure consisting of: a cell body, a set of fibers\ncalled dendrites, and a single long fiber called an axon.", "Figure 3.1 illustrates the structure of a neuron and how\nit connects to other neurons in the brain. The dendrites\nand the axon stem from the cell body, and the dendrites of\none neuron are connected to the axons of other neurons.\nThe dendrites act as input channels to the neuron and re-\nceive signals sent from other neurons along their axons.\nThe axon acts as the output channel of a neuron, and so\nother neurons, whose dendrites are connected to the axon,\nreceive the signals sent along the axon as inputs.\nNeurons work in a very simple manner. If the incom-\ning stimuli are strong enough, the neuron transmits an\nelectrical pulse, called an action potential, along its axon\nto the other neurons that are connected to it. So, a neuron\nacts as an all-o r- none switch, that takes in a set of inputs\nand either outputs an action potential or no output.\nThis explanation of the human brain is a significant\nsimplification of the biological reality, but it does capture\nFigure 3.1 The structure of a neuron in the brain.\n66 ChAPtER 3", "the main points necessary to understand the analogy\nbetween the structure of the human brain and compu-\ntational models called neural networks. These points of\nanalogy are: (1) the brain is composed of a large number\nof interconnected and simple units called neurons; (2) the\nfunctioning of the brain can be understood as processing\ninformation, encoded as high or low electrical signals, or\nactivation potentials, that spread across the network of\nneurons; and (3) each neuron receives a set of stimuli from\nits neighbors and maps these inputs to either a high- or\nlow- value output. All computational models of neural net-\nworks have these characteristics.\nArtificial Neural Networks\nAn artificial neural network consists of a network of\nsimple information processing units, called neurons. The\npower of neural networks to model complex relationships\nis not the result of complex mathematical models, but\nrather emerges from the interactions between a large set\nof simple neurons.\nFigure 3.2 illustrates the structure of a neural net-\nwork. It is standard to think of the neurons in a neural net-\nwork as organized into layers. The depicted network has\nfive layers: one input layer, three hidden layers, and one\noutput layer. A hidden layer is just a layer that is neither\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 67", "the input nor the output layer. Deep learning networks\nare neural networks that have many hidden layers of neu-\nrons. The minimum number of hidden layers necessary to\nbe considered deep is two. However, most deep learning\nnetworks have many more than two hidden layers. The\nimportant point is that the depth of a network is mea-\nsured in terms of the number of hidden layers, plus the\noutput layer.\nIn figure 3.2, the squares in the input layer represent\nlocations in memory that are used to present inputs to\nthe network. These locations can be thought of as sensing\nneurons. There is no processing of information in these\nsensing neurons; the output of each of these neurons is\nsimply the value of the data stored at the memory location.\nInput Hidden Hidden Hidden Output\nlayer layer1 layer2 layer3 layer4\nFigure 3.2 Topological illustration of a simple neural network.\n68 ChAPtER 3", "Deep learning networks\nare neural networks that\nhave many hidden layers\nof neurons.", "The circles in the figure represent the information proc-\nessing neurons in the network. Each of these neurons\ntakes a set of numeric values as input and maps them to\na single output value. Each input to a processing neuron\nis either the output of a sensing neuron or the output of\nanother processing neuron.\nThe arrows in figure 3.2 illustrate how information\nflows through the network from the output of one neu-\nron to the input of another neuron. Each connection in\na network connects two neurons and each connection is\ndirected, which means that information carried along a\nconnection only flows in one direction. Each of the con-\nnections in a network has a weight associated with it. A\nconnection weight is simply a number, but these weights\nare very important. The weight of a connection affects\nhow a neuron processes the information it receives along\nthe connection, and, in fact, training an artificial neural\nnetwork, essentially, involves searching for the best (or\noptimal) set of weights.\nHow an Artificial Neuron Processes Information\nThe processing of information within a neuron, that is,\nthe mapping from inputs to an output, is very similar\nto the loan decision model that we developed in chapter\n2. Recall that the loan decision model first calculated a\n70 ChAPtER 3", "weighted sum over the input features (income and debt).\nThe weights used in the weighted sum were adjusted using\na dataset so that the results of the weighted sum calcula-\ntion, given an loan applicant\u2019s income and debt as inputs,\nwas an accurate estimate of the applicant\u2019s credit solvency\nscore. The second stage of processing in the loan decision\nmodel involved passing the result of the weighted sum\ncalculation (the estimated credit solvency score) through\na decision rule. This decision rule was a function that\nmapped a credit solvency score to a decision on whether a\nloan application was granted or rejected.\nA neuron also implements a two- stage process to map\ninputs to an output. The first stage of processing involves\nthe calculation of a weighted sum of the inputs to the neu-\nron. Then the result of the weighted sum calculation is\npassed through a second function that maps the results of\nthe weighted sum score to the neuron\u2019s final output value.\nWhen we are designing a neuron, we can used many differ-\nent types of functions for this second stage or processing;\nit may be as simple as the decision rule we used for our\nloan decision model, or it may be more complex. Typically\nthe output value of a neuron is known as its activation\nvalue, so this second function, which maps from the result\nof the weighted sum to the activation value of the neuron,\nis known as an activation function.\nFigure 3.3 illustrates how these stages of processing\nare reflected in the structure of an artificial neuron. In\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 71", "x\n1 w\n1\nx 2 w 2\nx 3 w 3 \u03a3\u03d5 Output\nw 4\nx\n4\n.\n. wn\n.\nx\nn\nFigure 3.3 The structure of an artificial neuron.\nfigure 3.3, the \u03a3 symbol represents the calculation of the\nweighted sum, and the \u03c6 symbol represents the activation\nfunction processing the weighted sum and generating the\noutput from the neuron.\nThe neuron in figure 3.3 receives n inputs [x ,\u2026,x ]\n1 n\non n different input connections, and each connection has\nan associated weight [w ,\u2026,w ]. The weighted sum cal-\n1 n\nculation involves the multiplication of inputs by weights\nand the summation of the resulting values. Mathemati-\ncally this calculation is written as:\nz=(x \u00d7w )+(x \u00d7w )+\u2026+(x \u00d7w )\n1 1 2 2 n n\nThis calculation can also be written in a more compact\nmathematical form as:\nz=\u03a3n x \u00d7w\ni=1 i i\n72 ChAPtER 3", "For example, assuming a neuron received the inputs\n[x =3,x =9] and had the following weights [w =\u22123,\n1 2 1\nw =1], the weighted sum calculation would be:\n2\nz=(3\u00d7\u22123)+(9\u00d71)\n=0\nThe second stage of processing within a neuron is to\npass the result of the weighted sum, the z value, through\nan activation function. Figure 3.4 plots the shape of a num-\nber of possible activation functions, as the input to each\nfunction, z, ranges across an interval, either [- 1, ..., +1] or\n[- 10, ..., +10] depending on which interval best illustrates\nthe shape of the function. Figure 3.4 (top) plots a thresh-\nold activation function. The decision rule we used in the\nloan decision model was an example of a threshold func-\ntion; the threshold used in that decision rule was whether\nthe credit solvency score was above 200. Threshold acti-\nvations were common in early neural network research.\nFigure 3.4 (middle) plots the logistic and tanh activation\nfunctions. The units employing these activation functions\nwere popular in multilayer networks until quite recently.\nFigure 3.4 (bottom) plots the rectifier (or hinge, or posi-\ntive linear) activation function. This activation function is\nvery popular in modern deep learning networks; in 2011\nthe rectifier activation function was shown to enable bet-\nter training in deep networks (Glorot et al. 2011). In fact,\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 73", ".noitcnuf\nraenil\ndeifitcer\n:mottob\n;snoitcnuf\nhnat\ndna\ncitsigol\n:elddim\n;noitcnuf\ndlohserht\n:poT\n4.3\nerugiF", "as will be discussed in chapter 4, during the review of the\nhistory of deep learning, one of the trends in neural net-\nwork research has been a shift from threshold activation\nto logistic and tanh activations, and then onto rectifier\nactivation functions.\nReturning to the example, the result of the weighted\nsummation step was z=0. Figure 3.4 (middle plot, solid\nline) plots the logistic function. Assuming that the neuron\nis using a logistic activation function, this plot shows how\nthe result of the summation will be mapped to an output\nactivation: logistic(0)=0.5. The calculation of the output\nactivation of this neuron can be summarized as:\n( n )\nOutput=activation_function z= \u03a3x \u00d7w\ni i\ni=1\n=logistic(z=(3\u00d7\u22123)+(9\u00d71))\n=logistic(z=0)\n=0.5\nNotice that the processing of information in this neuron\nis nearly identical to the processing of information in the\nloan decision model we developed in the last chapter. The\nmajor difference is that we have replaced the decision\nthreshold rule that mapped the weighted sum score to an\naccepted or rejected output with a logistic function that\nmaps the weighted sum score to a value between 0 and 1.\nDepending on the location of this neuron in the network,\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 75", "the output activation of the neuron, in this instance\ny=0.5, will either be passed as input to one or more neu-\nrons in the next layer in the network, or will be part of\nthe overall output of the network. If a neuron is at the\noutput layer, the interpretation of what its output value\nmeans would be dependent on the task that the neuron\nis designed to model. If a neuron is in one of the hidden\nlayers of the network, then it may not be possible to put\na meaningful interpretation on the output of the neuron\napart from the general interpretation that it represents\nsome sort of derived feature (similar to the BMI feature we\ndiscussed in chapter 1) that the network has found useful\nin generating its outputs. We will return to the challenge\nof interpreting the meaning of activations within a neural\nnetwork in chapter 7.\nThe key point to remember from this section is that\na neuron, the fundamental building block of neural net-\nworks and deep learning, is defined by a simple two- step\nsequence of operations: calculating a weighted sum and\nthen passing the result through an activation function.\nFigure 3.4 illustrates that neither the tanh nor the\nlogistic function is a linear function. In fact, the plots of\nboth of these functions have a distinctive s- shaped (rather\nthan linear) profile. Not all activation functions have an\ns- shape (for example, the threshold and rectifier are not\ns- shaped), but all activation functions do apply a nonlin-\near mapping to the output of the weighted sum. In fact,\n76 ChAPtER 3", "it is the introduction of the nonlinear mapping into the\nprocessing of a neuron that is the reason why activation\nfunctions are used.\nWhy Is an Activation Function Necessary?\nTo understand why a nonlinear mapping is needed in a\nneuron, it is first necessary to understand that, essentially,\nall a neural network does is define a mapping from inputs\nto outputs, be it from a game position in Go to an evalu-\nation of that position, or from an X-r ay to a diagnosis of\na patient. Neurons are the basic building blocks of neural\nnetworks, and therefore they are the basic building blocks\nof the mapping a network defines. The overall mapping\nfrom inputs to outputs that a network defines is com-\nposed of the mappings from inputs to outputs that each of\nthe neurons within the network implement. The implica-\ntion of this is that if all the neurons within a network were\nrestricted to linear mappings (i.e., weighted sum calcula-\ntions), the overall network would be restricted to a linear\nmapping from inputs to outputs. However, many of the re-\nlationships in the world that we might want to model are\nnonlinear, and if we attempt to model these relationships\nusing a linear model, then the model will be very inaccu-\nrate. Attempting to model a nonlinear relationship with\na linear model would be an example of the underfitting\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 77", "problem we discussed in chapter 1: underfitting occurs\nwhen the model used to encode the patterns in a dataset\nis too simple and as a result it is not accurate.\nA linear relationship exists between two things when\nan increase in one always results in an increase or decrease\nin the other at a constant rate. For example, if an employee\nis on a fixed hourly rate, which does not vary at weekends\nor if they do overtime, then there is a linear relationship\nbetween the number of hours they work and their pay. A\nplot of their hours worked versus their pay will result in\na straight line; the steeper the line the higher their fixed\nhourly rate of pay. However, if we make the payment sys-\ntem for our hypothetical employee just slightly more com-\nplex, by, for example, increasing their hourly rate of pay\nwhen they do overtime or work weekends, then the rela-\ntionship between the number of hours they work and their\npay is no longer linear. Neural networks, and in particular\ndeep learning networks, are typically used to model rela-\ntionships that are much more complex than this employ-\nee\u2019s pay. Modeling these relationships accurately requires\nthat a network be able to learn and represent complex\nnonlinear mappings. So, in order to enable a neural net-\nwork to implement such nonlinear mappings, a nonlinear\nstep (the activation function) must be included within the\nprocessing of the neurons in the network.\nIn principle, using any nonlinear function as an activa-\ntion function enables a neural network to learn a nonlinear\n78 ChAPtER 3", "mapping from inputs to outputs. However, as we shall see\nlater, most of the activation functions plotted in figure 3.4\nhave nice mathematical properties that are helpful when\ntraining a neural network, and this is why they are so pop-\nular in neural network research.\nThe fact that the introduction of a nonlinearity into\nthe processing of the neurons enables the network to\nlearn a nonlinear mapping between input(s) and output\nis another illustration of the fact that the overall behav-\nior of the network emerges from the interactions of the\nprocessing carried out by individual neurons within the\nnetwork. Neural networks solve problems using a divide-\nand- conquer strategy: each of the neurons in a network\nsolves one component of the larger problem, and the\noverall problem is solved by combining these component\nsolutions. An important aspect of the power of neural\nnetworks is that during training, as the weights on the\nconnections within the network are set, the network is\nin effect learning a decomposition of the larger problem,\nand the individual neurons are learning how to solve and\ncombine solutions to the components within this problem\ndecomposition.\nWithin a neural network, some neurons may use dif-\nferent activation functions from other neurons in the net-\nwork. Generally, however, all the neurons within a given\nlayer of a network will be of the same type (i.e., they will\nall use the same activation function). Also, sometimes\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 79", "neurons are referred to as units, with a distinction made\nbetween units based on the activation function the units\nuse: neurons that use a threshold activation function are\nknown as threshold units, units that use a logistic acti-\nvation function are known as logistic units, and neurons\nthat use the rectifier activation function are known as\nrectified linear units, or ReLUs. For example, a network\nmay have a layer of ReLUs connected to a layer of logistic\nunits. The decision regarding which activation functions\nto use in the neurons in a network is made by the data\nscientist who is designing the network. To make this deci-\nsion, a data scientist may run a number of experiments\nto test which activation functions give the best perfor-\nmance on a dataset. However, frequently data scientists\ndefault to using whichever activation function is popular\nat a given point. For example, currently ReLUs are the\nmost popular type of unit in neural networks, but this\nmay change as new activation functions are developed and\ntested. As we will discuss at the end of this chapter, the\nelements of a neural network that are set manually by the\ndata scientist prior to the training process are known as\nhyperparameters.\nThe term hyperparameter is used to describe the\nmanually fixed parts of the model in order to distinguish\nthem from the parameters of the model, which are the\nparts of the model that are set automatically, by the ma-\nchine learning algorithm, during the training process. The\n80 ChAPtER 3", "Neural networks solve\nproblems using a divide-\nand- conquer strategy:\neach of the neurons in\na network solves one\ncomponent of the larger\nproblem, and the overall\nproblem is solved by\ncombining these\ncomponent solutions.", "parameters of a neural network are the weights used in\nthe weighted sum calculations of the neurons in the net-\nwork. As we touched on in chapters 1 and 2, the standard\ntraining process for setting the parameters of a neural\nnetwork is to begin by initializing the parameters (the\nnetwork\u2019s weights) to random values, and during train-\ning to use the performance of the network on the dataset\nto slowly adjust these weights so as to improve the ac-\ncuracy of the model on the data. Chapter 6 describes the\ntwo algorithms that are most commonly used to train a\nneural network: the gradient descent algorithm and the\nbackpropagation algorithm. What we will focus on next\nis understanding how changing the parameters of a neu-\nron affects how the neuron responds to the inputs it\nreceives.\nHow Does Changing the Parameters of a Neuron Affect\nIts Behavior?\nThe parameters of a neuron are the weights the neuron\nuses in the weighted sum calculation. Although the\nweighted sum calculation in a neuron is the same weighted\nsum used in a linear model, in a neuron the relationship\nbetween the weights and the final output of neuron is\nmore complex because the result of the weighted sum\nis passed through an activation function in order to\n82 ChAPtER 3", "generate the final output. To understand how a neuron\nmakes a decision on a given input, we need to understand\nthe relationship between the neuron\u2019s weights, the input\nit receives, and the output it generates in response.\nThe relationship between a neuron\u2019s weights and the\noutput it generates for a given input is most easily under-\nstood in neurons that use a threshold activation function.\nA neuron using this type of activation function is equiva-\nlent to our loan decision model that used a decision rule\nto classify the credit solvency scores, generated by the\nweighted sum calculation, to reject or grant loan applica-\ntions. At the end of chapter 2, we introduced the concepts\nof an input space, a weight space, and an activation space\n(see figure 2.2). The input space for our two- input loan\ndecision model could be visualized as a two- dimensional\nspace, with one input (annual income) plotted along the x-\naxis, and the other input (current debt) on the y- axis. Each\npoint in this plot defined a potential combination of in-\nputs to the model, and the set of points in the input space\ndefines the set of possible inputs the model could process.\nThe weights used in the loan decision model can be un-\nderstood as dividing the input space into two regions: the\nfirst region contains all of the inputs that result in the loan\napplication being granted, and the other region contains\nall the inputs that result in the loan application being re-\njected. In that scenario, changing the weights used by the\ndecision model would change the set of loan applications\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 83", "that were accepted or rejected. Intuitively, this makes\nsense because it changes the weighting that we put on an\napplicant\u2019s income relative to their debt when we are de-\nciding on granting the loan or not.\nWe can generalize the above analysis of the loan deci-\nsion model to a neuron in a neural network. The equivalent\nneuron structure to the loan decision model is a two-i nput\nneuron with a threshold activation function. The input\nspace for such a neuron has a similar structure to the in-\nput space for a loan decision model. Figure 3.5 presents\nthree plots of the input space for a two-i nput neuron us-\ning a threshold function that outputs a high activation if\nthe weighted sum result is greater than zero, and a low\nactivation otherwise. The differences between each of the\nplots in this figure is that the neuron defines a different\ndecision boundary in each case. In each plot, the decision\nboundary is marked with a black line.\nEach of the plots in figure 3.5 was created by first fix-\ning the weights of the neuron and then for each point in\nthe input space recording whether the neuron returned a\nhigh or low activation when the coordinates of the point\nwere used as the inputs to the neuron. The input points for\nwhich the neuron returned a high activation are plotted in\ngray, and the other points are plotted in white. The only\ndifference between the neurons used to create these plots\nwas the weights used in calculating the weighted sum of\nthe inputs. The arrow in each plot illustrates the weight\n84 ChAPtER 3", ":mottob\n;]1=2w\n,2\n-=1w[\nrotcev\nthgiew\n:elddim\n;]1=2w\n,1=1w[\nrotcev\nthgiew\n:poT\n.noruen\ntupni\n-owt\na\nrof\nseiradnuob\n.]2\n-=2w\nnoisiceD ,1=1w[\nrotcev\n5.3\nerugiF thgiew", "vector used by the neuron to generate the plot. In this\ncontext, a vector describes the direction and distance of\na point from the origin.1 As we shall see, interpreting the\nset of weights used by a neuron as defining a vector (an\narrow from the origin to the coordinates of the weights)\nin the neuron\u2019s input space is useful in understanding how\nchanges in the weights change the decision boundary of\nthe neuron.\nThe weights used to create each plot change from one\nplot to the next. These changes are reflected in the direc-\ntion of the arrow (the weight vector) in each plot. Spe-\ncifically, changing the weights rotates the weight vector\naround the origin. Notice that the decision boundary in\neach plot is sensitive to the direction of the weight vector:\nin all the plots, the decision boundary is orthogonal (i.e.,\nat a right, or 90\u00b0, angle) to the weight vector. So, chang-\ning the weights not only rotates the weight vector, it also\nrotates the decision boundary of the neuron. This rotation\nchanges the set of inputs that the neuron outputs a high\nactivation in response to (the gray regions).\nTo understand why this decision boundary is always\northogonal to the weight vector, we have to shift our per-\nspective, for a moment, to linear algebra. Remember that\nevery point in the input space defines a potential combi-\nnation of input values to the neuron. Now, imagine each\nof these sets of input values as defining an arrow from the\norigin to the coordinates of the point in the input space.\n86 ChAPtER 3", "There is one arrow for each point in the input space. Each\nof these arrows is very similar to the weight vector, ex-\ncept that it points to the coordinates of the inputs rather\nthan to the coordinates of the weights. When we treat a\nset of inputs as a vector, the weighted sum calculation is\nthe same as multiplying two vectors, the input vector by\nthe weight vector. In linear algebra terminology, multi-\nplying two vectors is known as the dot product operation.\nFor the purposes of this discussion, all we need to know\nabout the dot product is that the result of this operation\nis dependent on the angle between the two vectors that\nare multiplied. If the angle between the two vectors is less\nthan a right angle, then the result will be positive; other-\nwise, it will be negative. So, multiplying the weight vec-\ntor by an input vector will return a positive value for all\nthe input vectors at an angle less than a right angle to the\nweight vector, and a negative value for all the other vec-\ntors. The activation function used by this neuron returns\na high activation when positive values are input and a low\nactivation when negative values are input. Consequently,\nthe decision boundary lies at a right angle to the weight\nvector because all the inputs at an angle less than a right\nangle to the weight vector will result in a positive input\nto the activation function and, therefore, trigger a high-\noutput activation from the neuron; conversely, all the\nother inputs will result in a low- output activation from\nthe neuron.\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 87", "Switching back to the plots in figure 3.5, although the\ndecision boundaries in each of the plots are at different\nangles, all the decision boundaries go through the point in\nspace that the weight vectors originate from (i.e., the ori-\ngin). This illustrates that changing the weights of a neuron\nrotates the neuron\u2019s decision boundary but does not trans-\nlate it. Translating the decision boundary means moving\nthe decision boundary up and down the weight vector, so\nthat the point where it meets the vector is not the origin.\nThe restriction that all decision boundaries must pass\nthrough the origin limits the distinctions that a neuron\ncan learn between input patterns. The standard way to\novercome this limitation is to extend the weighted sum\ncalculation so that it includes an extra element, known as\nthe bias term. This bias term is not the same as the induc-\ntive bias we discussed in chapter 1. It is more analogous\nto the intercept parameter in the equation of a line, which\nmoves the line up and down the y- axis. The purpose of this\nbias term is to move (or translate) the decision boundary\naway from the origin.\nThe bias term is simply an extra value that is included\nin the calculation of the weighted sum. It is introduced\ninto the neuron by adding the bias to the result of the\nweighted summation prior to passing it through the ac-\ntivation function. Here is the equation describing the\nprocessing stages in a neuron with the bias term repre-\nsented by the term b:\n88 ChAPtER 3", "\uf8eb \uf8f6\n\uf8ec \uf8eb n \uf8f6 \uf8f7\nOutput=activation_function \uf8ecz= \uf8ed\uf8ec\u2211x i\u00d7w i\uf8f8\uf8f7+ (cid:27)b\n\uf8f7\n\uf8ec (cid:31)i=(cid:29)(cid:29)1 (cid:30)(cid:29)(cid:28) bias\uf8f7\n\uf8ed \uf8f8\nweighted sum\nFigure 3.6 illustrates how the value of the bias term affects\nthe decision boundary of a neuron. When the bias term is\nnegative, the decision boundary is moved away from the\norigin in the direction that the weight vector points to (as\nin the top and middle plots in figure 3.6); when the bias\nterm is positive, the decision boundary is translated in the\nopposite direction (see the bottom plot of figure 3.6). In\nboth cases, the decision boundary remains orthogonal to\nthe weight vector. Also, the size of the bias term affects\nthe amount the decision boundary is moved from the ori-\ngin; the larger the value of the bias term, the more the de-\ncision boundary is moved (compare the top plot of figure\n3.6 with the middle and bottom plots).\nInstead of manually setting the value of the bias term,\nit is preferable to allow a neuron to learn the appropriate\nbias. The simplest way to do this is to treat the bias term as\na weight and allow the neuron to learn the bias term at the\nsame time that it is learning the rest of the weights for its\ninputs. All that is required to achieve this is to augment all\nthe input vectors the neuron receives with an extra input\nthat is always set to 1. By convention, this input is input\n0 (x =1), and, consequently, the bias term is specified by\n0\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 89", ",1=1w[\n:poT\nrotcev\n.yradnuob\nthgiew\nnoisiced\n:mottob\neht\n;2\nno -\not\nmret lauqe\nsaib\nsaib\neht\ndna\nfo\n]1=2w\ntceffe\neht ,2\n-=1w[\netartsulli\nrotcev\ntaht\nthgiew\nnoruen\n:elddim\ntupni\n;1\n-owt -\not\na lauqe\nrof\nstolp saib\nyradnuob dna\n]1=2w\n.2\not\nnoisiceD ,1=1w[ lauqe\nsaib\nrotcev\ndna\n6.3\nerugiF thgiew ]2\n-=2w", "weight 0 (w ).2 Figure 3.7 illustrates the structure of an\n0\nartificial neuron when the bias term has been integrated\nas w .\n0\nWhen the bias term has been integrated into the\nweights of a neuron, the equation specifying the map-\nping from input(s) to output activation of the neuron can\nbe simplified (at least from a notational perspective) as\nfollows:\n\uf8eb n \uf8f6\nOutput=activation_function \uf8ed\uf8ecz=\u2211x i\u00d7w i\uf8f8\uf8f7\ni=0\nNotice that in this equation the index i goes from 0 to\nn, so that it now includes the fixed input, x =1, and the\n0\nbias term, w ; in the earlier version of this equation, the\n0\nindex only went from 1 to n. This new format means that\nthe neuron is able to learn the bias term, simply by learn-\ning the appropriate weight w , using the same process\n0\nx 1 w x 0 =1\n1\nx 2 w 2 w 0(originallyb)\nx 3 w 3 \u03a3\u03d5 Output\nw 4\nx4\n.\n. wn\n.\nx\nn\nFigure 3.7 An artificial neuron with a bias term included as w.\n0\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 91", "that is used to learn the weights for the other inputs: at\nthe start of training, the bias term for each neuron in the\nnetwork will be initialized to a random value and then ad-\njusted, along with the weights of the network, in response\nto the performance of the network on the dataset.\nAccelerating Neural Network Training Using GPUs\nMerging the bias term is more than a notational conve-\nnience; it enables us to use specialized hardware to accel-\nerate the training of neural networks. The fact that a bias\nterm can be treated as the same as a weight means that the\ncalculation of the weighted sum of inputs (including the\naddition of the bias term) can be treated as the multipli-\ncation of two vectors. As we discussed earlier, during the\nexplanation of why the decision boundary was orthogonal\nto the weight vector, we can think of a set of inputs as a\nvector. Recognizing that much of the processing within a\nneural network involves vector and matrix multiplications\nopens up the possibility of using specialized hardware to\nspeed up these calculations. For example, graphics proc-\nessing units (GPUs) are hardware components that have\nspecifically been designed to do extremely fast matrix\nmultiplications.\nIn a standard feedforward network, all the neurons\nin one layer receive all the outputs (i.e., activations) from\n92 ChAPtER 3", "all the neurons in the preceding layer. This means that all\nthe neurons in a layer receive the same set of inputs. As\na result, we can calculate the weighted sum calculation\nfor all the neurons in a layer using only a single vector by\nmatrix multiplication. Doing this is much faster than cal-\nculating a separate weighted sum for each neuron in the\nlayer. To do this calculation of weighted sums for an entire\nlayer of neurons in a single multiplication, we put the out-\nputs from the neurons in the preceding layer into a vector\nand store all the weights of the connections between the\ntwo layers of neurons in a matrix. We then multiply the\nvector by the matrix, and the resulting vector contains\nthe weighted sums for all the neurons.\nFigure 3.8 illustrates how the weighted summation\ncalculations for all the neurons in a layer in a network can\nbe calculated using a single matrix multiplication opera-\ntion. This figure is composed of two separate graphics: the\ngraphic on the left illustrates the connections between\nneurons in two layers of a network, and the graphic on\nthe right illustrates the matrix operation to calculate the\nweighted sums for the neurons in the second layer of the\nnetwork. To help maintain a correspondence between\nthe two graphics, the connections into neuron E are high-\nlighted in the graphic on the left, and the calculation of the\nweighted sum in neuron E is highlighted in the graphic on\nthe right.\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 93", "Focusing on the graphic on the right, the 1\u00d73 vec-\ntor (1 row, 3 columns) on the bottom-l eft of this graphic,\nstores the activations for the neurons in layer 1 of the net-\nwork; note that these activations are the outputs from an\nactivation function \u03d5 (the particular activation function is\nnot specified\u2014 it could be a threshold function, a tanh, a\nlogistic function, or a rectified linear unit/ReLU function).\nThe 3\u00d74 matrix (three rows and four columns), in the top-\nright of the graphic, holds the weights for the connections\nbetween the two layers of neurons. In this matrix, each\ncolumn stores the weights for the connections coming into\none of the neurons in the second layer of the network. The\nfirst column stores the weights for neuron D, the second\ncolumn for neuron E, etc.3 Multiplying the 1\u00d73 vector of\nactivations from layer 1 by the 3\u00d74 weight matrix results\nin a 1\u00d74 vector corresponding to the weighted summa-\ntions for the four neurons in layer 2 of the network: z is\nD\nthe weighted sum of inputs for neuron D, z for neuron E,\nE\nand so on.\nTo generate the 1\u00d74 vector containing the weighted\nsummations for the neurons in layer 2, the activation\nvector is multiplied by each column in the matrix in turn.\nThis is done by multiplying the first (leftmost) element in\nthe vector by the first (topmost) element in the column,\nthen multiplying the second element in the vector by the\nelement in the second row in the column, and so on, un-\ntil each element in the vector has been multiplied by its\n94 ChAPtER 3", "corresponding column element. Once all the multiplica-\ntions between the vector and the column have been com-\npleted, the results are summed together and the stored in\nthe output vector. Figure 3.8 illustrates multiplication of\nthe activation vector by the second column in the weight\nmatrix (the column containing the weights for inputs to\nneuron E) and the storing of the summation of these mul-\ntiplications in the output vector as the value z .\nE\nWeight matrix for\nedges in layer 2\nActivations from layer 1 Weighted sums for layer 2\nFigure 3.8 A graphical illustration of the topological connections of a\nspecific neuron E in a network, and the corresponding vector by matrix\nmultiplication that calculates the weighted summation of inputs for the\nneuron E, and its siblings in the same layer.5\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 95", "Indeed, the calculation implemented by an entire neu-\nral network can be represented as a chain of matrix multi-\nplications, with an element- wise application of activation\nfunctions to the results of each multiplication. Figure 3.9\nillustrates how a neural network can be represented in\nboth graph form (on the left) and as a sequence of matrix\noperations (on the right). In the matrix representation,\nthe \u00d7 symbol represents standard matrix multiplication\n(described above) and the \u2192\u03d5\u2192 notation represents the\napplication of an activation function to each element in\nthe vector created by the preceding matrix multiplication.\nThe output of this element- wise application of the activa-\ntion function is a vector containing the activations for the\nneurons in a layer of the network. To help show the corre-\nspondence between the two representations, both figures\nshow the inputs to the network, I and I , the activations\n1 2\nfrom the three hidden units, A , A , and A , and the over-\n1 2 3\nall output of the network, y.\nHidden layer Output layer\nweight matrix weight matrix\nActivations Activations Output\ninput layer hidden layer\nFigure 3.9 A graph representation of a neural network (left), and the same\nnetwork represented as a sequence of matrix operations (right).6\n96 ChAPtER 3", "As a side note, the matrix representation provides a\ntransparent view of the depth of a network; the network\u2019s\ndepth is counted as the number of layers that have a weight\nmatrix associated with them (or equivalently, the depth of\na network is the number of weight matrices required by\nthe network). This is why the input layer is not counted\nwhen calculating the depth of a network: it does not have\na weight matrix associated with it.\nAs mentioned above, the fact that the majority of cal-\nculations in a neural network can be represented as a se-\nquence of matrix operations has important computational\nimplications for deep learning. A neural network may con-\ntain over a million neurons, and the current trend is for the\nsize of these networks to double every two to three years.4\nFurthermore, deep learning networks are trained by itera-\ntively running a network on examples sampled from very\nlarge datasets and then updating the network parameters\n(i.e., the weights) to improve performance. Consequently,\ntraining a deep learning network can require very large\nnumbers of network runs, with each network run requir-\ning millions of calculations. This is why computational\nspeedups, such as those that can be achieved by using\nGPUs to perform matrix multiplications, have been so im-\nportant for the development of deep learning.\nThe relationship between GPUs and deep learning\nis not one- way. The growth in demand for GPUs gener-\nated by deep learning has had a significant impact on\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 97", "GPU manufacturers. Deep learning has resulted in these\ncompanies refocusing their business. Traditionally, these\ncompanies would have focused on the computer games\nmarket, since the original motivation for developing GPU\nchips was to improve graphics rendering, and this had a\nnatural application to computer games. However, in re-\ncent years these companies have focused on positioning\nGPUs as hardware for deep learning and artificial intel-\nligence applications. Furthermore, GPU companies have\nalso invested to ensure that their products support the top\ndeep learning software frameworks.\nSummary\nThe primary theme in this chapter has been that deep\nlearning networks are composed of large numbers of\nsimple processing units that work together to learn and\nimplement complex mappings from large datasets. These\nsimple units, neurons, execute a two- stage process: first, a\nweighted summation over the inputs to the neuron is cal-\nculated, and second, the result of the weighted summation\nis passed through a nonlinear function, known as an acti-\nvation function. The fact that a weighted summation func-\ntion can be efficiently calculated across a layer of neurons\nusing a single matrix multiplication operation is impor-\ntant: it means that neural networks can be understood as a\n98 ChAPtER 3", "sequence of matrix operations; this has permitted the use\nof GPUs, hardware optimized to perform fast matrix mul-\ntiplication, to speed up the training of networks, which in\nturn has enabled the size of networks to grow.\nThe compositional nature of neural networks means\nthat it is possible to understand at a very fundamental\nlevel how a neural network operates. Providing a compre-\nhensive description of this level of processing has been the\nfocus of this chapter. However, the compositional nature\nof neural networks also raises a raft of questions in rela-\ntion to how a network should be composed to solve a given\ntask, for example:\n\u2022 Which activation functions should the neurons in a\nnetwork use?\n\u2022 How many layers should there be in a network?\n\u2022 How many neurons should there be in each layer?\n\u2022 How should the neurons be connected together?\nUnfortunately, many of these questions cannot be an-\nswered at a level of pure principle. In machine learning\nterminology, the types of concepts these questions are\nabout are known as hyperparameters, as distinct from\nmodel parameters. The parameters of a neural network\nare the weights on the edges, and these are set by training\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 99", "the network using large datasets. By contrast, hyperpa-\nrameters are the parameters of a model (in these cases,\nthe parameters of a neural network architecture) and/or\ntraining algorithm that cannot be directly estimated from\nthe data but instead must be specified by the person cre-\nating the model, either through the use of heuristic rules,\nintuition, or trial and error. Often, much of the effort that\ngoes into the creation of a deep learning network involves\nexperimental work to answer the questions in relation to\nhyperparameters, and this process is known as hyperpa-\nrameter tuning. The next chapter will review the history\nand evolution of deep learning, and the challenges posed\nby many of these questions are themes running through\nthe review. Subsequent chapters in the book will explore\nhow answering these questions in different ways can cre-\nate networks with very different characteristics, each\nsuited to different types of tasks. For example, recurrent\nneural networks are best suited to processing sequential/\ntime- series data, whereas convolutional neural networks\nwere originally developed to process images. Both of these\nnetwork types are, however, built using the same funda-\nmental processing unit, the artificial neuron; the differ-\nences in the behavior and abilities of these networks stems\nfrom how these neurons are arranged and composed.\n100 ChAPtER 3", "4\nA BRIEF HISTORY OF\nDEEP LEARNING\nThe history of deep learning can be described as three\nmajor periods of excitement and innovation, interspersed\nwith periods of disillusionment. Figure 4.1 shows a time-\nline of this history, which highlights these periods of ma-\njor research: on threshold logic units (early 1940s to the\nmid 1960s), connectionism (early 1980s to mid-1 990s),\nand deep learning (mid 2000s to the present). Figure 4.1\ndistinguishes some of the primary characteristics of the\nnetworks developed in each of these three periods. The\nchanges in these network characteristics highlight some\nof the major themes within the evolution of deep learning,\nincluding: the shift from binary to continuous values; the\nmove from threshold activation functions, to logistic and\ntanh activation, and then onto ReLU activation; and the\nprogressive deepening of the networks, from single layer,", "to multiple layer, and then onto deep networks. Finally,\nthe upper half of figure 4.1 presents some of the impor-\ntant conceptual breakthroughs, training algorithms, and\nmodel architectures that have contributed to the evolu-\ntion of deep learning.\nFigure 4.1 provides a map of the structure of this\nchapter, with the sequence of concepts introduced in the\nchapter generally following the chronology of this time-\nline. The two gray rectangles in figure 4.1 represent the\ndevelopment of two important deep learning network ar-\nchitectures: convolutional neural networks (CNNs), and\nrecurrent neural networks (RNNs). We will describe the\nevolution of these two network architectures in this chap-\nter, and chapter 5 will give a more detailed explanation of\nhow these networks work.\nEarly Research: Threshold Logic Units\nIn some of the literature on deep learning, the early neural\nnetwork research is categorized as being part of cybernet-\nics, a field of research that is concerned with developing\ncomputational models of control and learning in biologi-\ncal units. However, in figure 4.1, following the terminol-\nogy used in Nilsson (1965), this early work is categorized\nas research on threshold logic units because this term\ntransparently describes the main characteristics of the\n102 ChAPtER 4", ".gninraeL\npeeD\nfo\nyrotsiH\n1.4\nerugiF\nA BRIEF hIstoRy oF dEEP LEARnIng 103", "systems developed during this period. Most of the models\ndeveloped in the 1940s, \u201950s, and \u201960s processed Boolean\ninputs (true/false represented as +1/-1 or 1/0) and gener-\nated Boolean outputs. They also used threshold activation\nfunctions (introduced in chapter 3), and were restricted to\nsingle- layer networks; in other words, they were restricted\nto a single matrix of tunable weights. Frequently, the fo-\ncus of this early research was on understanding whether\ncomputational models based on artificial neurons had the\ncapacity to learn logical relations, such as conjunction or\ndisjunction.\nIn 1943, Walter McCulloch and Walter Pitts published\nan influential computational model of biological neurons\nin a paper entitled: \u201cA Logical Calculus of the Ideas Im-\nmanent in Nervous Activity\u201d (McCulloch and Pitts 1943).\nThe paper highlighted the all- or- none characteristic of\nneural activity in the brain and set out to mathematically\ndescribe neural activity in terms of a calculus of propo-\nsitional logic. In the McCulloch and Pitts model, all the\ninputs and the output to a neuron were either 0 or 1.\nFurthermore, each input was either excitatory (having a\nweight of +1) or inhibitory (having a weight of -1 ). A key\nconcept introduced in the McCulloch and Pitts model was\na summation of inputs followed by a threshold function\nbeing applied to the result of the summation. In the sum-\nmation, if an excitatory input was on, it added 1; if an in-\nhibitory input was on, it subtracted 1. If the result of the\n104 ChAPtER 4", "summation was above a preset threshold, then the output\nof the neuron was 1; otherwise, it output a 0. In the paper,\nMcCulloch and Pitts demonstrated how logical operations\n(such as conjunction, disjunction, and negation) could be\nrepresented using this simple model. The McCulloch and\nPitts model integrated the majority of the elements that\nare present in the artificial neurons introduced in chapter\n3. In this model, however, the neuron was fixed; in other\nwords the weights and threshold were set by han.\nIn 1949, Donald O. Hebb published a book entitled\nThe Organization of Behavior, in which he set out a neu-\nropsychological theory (integrating psychology and the\nphysiology of the brain) to explain general human be-\nhavior. The fundamental premise of the theory was that\nbehavior emerged through the actions and interactions\nof neurons. For neural network research, the most im-\nportant idea in this book was a postulate, now known as\nHebb\u2019s postulate, which explained the creation of lasting\nmemory in animals based on a process of changes to the\nconnections between neurons:\nWhen an axon of a cell A is near enough to excite\na cell B and repeatedly or persistently takes part\nin firing it, some growth process or metabolic\nchange takes place in one or both cells such that A\u2019s\nefficiency, as one of the cells firing B, is increased.\n(Hebb 1949, p. 62)\nA BRIEF hIstoRy oF dEEP LEARnIng 105", "This postulate was important because it asserted that in-\nformation was stored in the connections between neurons\n(i.e., in the weights of a network), and furthermore that\nlearning occurred by changing these connections based\non repeated patterns of activation (i.e., learning can take\nplace within a network by changing the weights of the\nnetwork).\nRosenblatt\u2019s Perceptron Training Rule\nIn the years following Hebb\u2019s publication, a number of re-\nsearchers proposed computational models of neuron activ-\nity that integrated the Boolean threshold activation units\nof McCulloch and Pitts, with a learning mechanism based\non adjusting the weights applied to the inputs. The best\nknown of these models was Frank Rosenblatt\u2019s perceptron\nmodel (Rosenblatt 1958). Conceptually, the perceptron\nmodel can be understood as a neural network consisting\nof a single artificial neuron that uses a threshold activa-\ntion unit. Importantly, a perceptron network only has a\nsingle layer of weights. The first implementation of a per-\nceptron was a software implementation on an IBM 704\nsystem (and this was probably the first implementation\nof any neural network). However, Rosenblatt always in-\ntended the perceptron to be a physical machine and it was\nlater implemented in custom-b uilt hardware known as the\n\u201cMark 1 perceptron.\u201d The Mark 1 perceptron received input\nfrom a camera that generated a 400- pixel image that was\n106 ChAPtER 4", "passed into the machine via an array of 400 photocells\nthat were in turn connected to the neurons. The weights\non connections to the neurons were implemented using\nadjustable electrical resistors known as potentiometers,\nand weight adjustments were implemented by using elec-\ntric motors to adjust the potentiometers.\nRosenblatt proposed an error- correcting training pro-\ncedure for updating the weights of a perceptron so that it\ncould learn to distinguish between two classes of input:\ninputs for which the perceptron should produce the out-\nput y=+1, and inputs for which the perceptron should\nproduce the output y=\u22121 (Rosenblatt 1960). The train-\ning procedure assumes a set of Boolean encoded input pat-\nterns, each with an associated target output. At the start\nof training, the weights in the perceptron are initialized\nto random values. Training then proceeds by iterating\nthrough the training examples, and after each example\nhas been presented to the network, the weights of the net-\nwork are updated based on the error between the output\ngenerated by the perceptron and the target output speci-\nfied in the data. The training examples can be presented to\nthe network in any order and examples may be presented\nmultiple times before training is completed. A complete\ntraining pass through the set of examples is known as an\niteration, and training terminates when the perceptron\ncorrectly classifies all the examples in an iteration.\nA BRIEF hIstoRy oF dEEP LEARnIng 107", "Rosenblatt defined a learning rule (known as the\nperceptron training rule) to update each weight in a per-\nceptron after a training example has been processed. The\nstrategy the rule used to update the weights is the same as\nthe three- condition strategy we introduced in chapter 2 to\nadjust the weights in the loan decision model:\n1. If the output of the model for an example matches the\noutput specified for that example in the dataset, then\ndon\u2019t update the weights.\n2. If the output of the model is too low for the current\nexample, then increase the output of the model by\nincreasing the weights for the inputs that had positive\nvalue for the example and decreasing the weights for the\ninputs that had a negative value for the example.\n3. If the output of the model is too high for the current\nexample, then reduce the output of the model by\ndecreasing the weights for the inputs that had a positive\nvalue and increasing the weights for the inputs that had a\nnegative value for the example.\nWritten out in an equation, Rosenblatt\u2019s learning rule\nupdates a weight i (w ) as:\ni\nwt+1 = wt +(\u03b7\u00d7(yt \u2212y\u02c6t)\u00d7xt)\ni i i\n108 ChAPtER 4", "In this rule, wt+1 is the value of weight i after the net-\ni\nwork weights have been updated in response to the proc-\nessing of example t, wt is the value of weight i used during\ni\nthe processing of example t, \u03b7 is a preset positive constant\n(known as the learning rate, discussed below), yt is the ex-\npected output for example t as specified in the training\ndataset, y\u02c6t is the output generated by the perceptron for\nexample t, and xt is the component of input t that was\ni\nweighted by wt during the processing of the example.\ni\nAlthough it may look complex, the perceptron train-\ning rule is in fact just a mathematical specification of the\nthree- condition weight update strategy described above.\nThe primary part of the equation to understand is the\ncalculation of the difference between the expected output\nand what the perceptron actually predicted: yt \u2212y\u02c6t. The\noutcome of this subtraction tells us which of the three\nupdate conditions we are in. In understanding how this\nsubtraction works, it is important to remember that for\na perceptron model the desired output is always either\ny=+1 or y=\u22121. The first condition is when yt \u2212y\u02c6t =0;\nthen the output of the perceptron is correct and the\nweights are not changed.\nThe second weight update condition is when the out-\nput of the perceptron is too large. This condition can only\nbe occur when the correct output for example t is yt =\u22121\nand so this condition is triggered when yt \u2212y\u02c6t <0. In\nthis case, if the perceptron output for the example t is\nA BRIEF hIstoRy oF dEEP LEARnIng 109", "y\u02c6t =+1, then the error term is negative (yt \u2212y\u02c6t =\u22122) and\nthe weight w is updated by +(\u03b7\u00d7\u22122\u00d7xt). Assuming, for\ni i\nthe purpose of this explanation, that \u03b7 is set to 0.5, then\nthis weight update simplifies to \u2212xt. In other words, when\ni\nthe perceptron\u2019s output is too large, the weight update\nrule subtracts the input values from the weights. This will\ndecrease the weights on inputs with positive values for the\nexample, and increase the weights on inputs with negative\nvalues for the example (subtracting a negative number is\nthe same as adding a positive number).\nThe third weight update condition is when the out-\nput of the perceptron is too small. This weight update\ncondition is the exact opposite of the second. It can only\noccur when yt =+1 and so is triggered when yt \u2212y\u02c6t >0.\nIn this case (yt \u2212y\u02c6t =2), and the weight is updated by\n+(\u03b7\u00d72\u00d7xt). Again assuming that \u03b7 is set to 0.5, then\ni\nthis update simplifies to +xt, which highlights that when\ni\nthe error of the perceptron is positive, the rule updates\nthe weight by adding the input to the weight. This has the\neffect of decreasing the weights on inputs with negative\nvalues for the example and increasing the weight on in-\nputs with positive values for the example.\nAt a number of points in the preceding paragraphs\nwe have referred to learning rate, \u03b7. The purpose of the\nlearning rate, \u03b7, is to control the size of the adjustments\nthat are applied to a weight. The learning rate is an ex-\nample of a hyperparameter that is preset before the model\nis trained. There is a tradeoff in setting the learning rate:\n110 ChAPtER 4", "\u2022 If the learning rate is too small, it may take a very\nlong time for the training process to converge on an\nappropriate set of weights.\n\u2022 If the learning rate is too large, the network\u2019s weights\nmay jump around the weight space too much and the\ntraining may not converge at all.\nOne strategy for setting the learning rate is to set it to\na relatively small positive value (e.g., 0.01), and another\nstrategy is to initialize it to a larger value (e.g., 1.0)\nbut to systematically reduce it as the training progresses\n1\n(e.g., \u03b7t+1 =\u03b71\u00d7 ).\nt\nTo make this discussion regarding the learning rate\nmore concrete, imagine you are trying to solve a puzzle\nthat requires you to get a small ball to roll into a hole. You\nare able to control the direction and speed of the ball by\ntilting the surface that the ball is rolling on. If you tilt the\nsurface too steeply, the ball will move very fast and is likely\nto go past the hole, requiring you to adjust the surface\nagain, and if you overadjust you may end up repeatedly\ntilting the surface. On the other hand, if you only tilt the\nsurface a tiny bit, the ball may not start to move at all, or it\nmay move very slowly taking a long time to reach the hole.\nNow, in many ways the challenge of getting the ball to roll\ninto the hole is similar to the problem of finding the best\nset of weights for a network. Think of each point on the\nA BRIEF hIstoRy oF dEEP LEARnIng 111", "surface the ball is rolling across as a possible set of network\nweights. The ball\u2019s position at each point in time specifies\nthe current set of weights of the network. The position\nof the hole specifies the optimal set of network weights for\nthe task we are training the network to complete. In this\ncontext, guiding the network to the optimal set of weights\nis analogous to guiding the ball to the hole. The learning\nrate allows us to control how quickly we move across the\nsurface as we search for the optimal set of weights. If we set\nthe learning rate to a high value, we move quickly across\nthe surface: we allow large updates to the weights at each\niteration, so there are big differences between the network\nweights in one iteration and the next. Or, using our rolling\nball analogy, the ball is moving very quickly, and just like\nin the puzzle when the ball is rolling too fast and passes\nthe hole, our search process may be moving so fast that it\nmisses the optimal set of weights. Conversely, if we set the\nlearning rate to a low value, we move very slowly across\nthe surface: we only allow small updates to the weights at\neach iteration; or, in other words, we only allow the ball\nto move very slowly. With a low learning rate, we are less\nlikely to miss the optimal set of weights, but it may take\nan inordinate amount of time to get to them. The strategy\nof starting with a high learning rate and then systemati-\ncally reducing it is equivalent to steeply tilting the puzzle\nsurface to get the ball moving and then reducing the tilt to\ncontrol the ball as it approaches the hole.\n112 ChAPtER 4", "Rosenblatt proved that if a set of weights exists that\nenables the perceptron to properly classify all of the train-\ning examples correctly, the perceptron training algorithm\nwill eventually converge on this set of weights. This find-\ning is known as the perceptron convergence theorem\n(Rosenblatt 1962). The difficulty with training a percep-\ntron, however, is that it may require a substantial number\nof iterations through the data before the algorithm con-\nverges. Furthermore, for many problems it is unknown\nwhether an appropriate set of weights exists in advance;\nconsequently, if training has been going on for a long time,\nit is not possible to know whether the training process is\nsimply taking a long time to converge on the weights and\nterminate, or whether it will never terminate.\nThe Least Mean Squares Algorithm\nAround the same time that Rosenblatt was developing the\nperceptron, Bernard Widrow and Marcian Hoff were devel-\noping a very similar model called the ADALINE (short for\nadaptive linear neuron), along with a learning rule called\nthe LMS (least mean square) algorithm (Widrow and Hoff\n1960). An ADALINE network consists of a single neuron\nthat is very similar to a perceptron; the only difference is\nthat an ADALINE network does not use a threshold func-\ntion. In fact, the output of an ADALINE network is the just\nthe weighted sum of the inputs. This is why it is known\nas a linear neuron: a weighted sum is a linear function (it\nA BRIEF hIstoRy oF dEEP LEARnIng 113", "defines a line), and so an ADALINE network implements\na linear mapping from inputs to output. The LMS rule is\nnearly identical to the perceptron learning rule, except\nthat the output of the perceptron for a given example y\u02c6t\nis replaced by the weighted sum of the inputs:\n\uf8eb \uf8eb \uf8eb n \uf8f6\uf8f6 \uf8f6\nwt i+1 =wt i + \uf8ed\uf8ec\u03b7\u00d7 \uf8ed\uf8ecyt\u2212 \uf8ed\uf8ec\u2211wt i \u00d7xt i\uf8f8\uf8f7 \uf8f8\uf8f7\u00d7xt i\uf8f8\uf8f7\ni=0\nThe logic of the LMS update rule is the same as that\nof the perceptron training rule. If the output is too large,\nthen weights that were applied to a positive input caused\nthe output to be larger, and these weights should be de-\ncreased, and those that were applied to a negative input\nshould be increased, thereby reducing the output the next\ntime this input pattern is received. And, by the same logic,\nif the output is too small, then weights that were applied\nto a positive input are increased and those that were ap-\nplied to a negative input should be decreased.\nOne of the important aspects of Widrow and Hoff\u2019s\nwork was to show that LMS rule could be used to train\nnetwork to predict a number of any value, not just a +1\nor - 1. This learning rule was called the least mean square\nalgorithm because using the LMS rule to iteratively ad-\njust the weights in a neuron is equivalent to minimizing\nthe average squared error on the training set. Today, the\nLMS learning rule is sometimes called the Widrow- Hoff\n114 ChAPtER 4", "If the output of the\nmodel is too large, then\nweights associated with\npositive inputs should\nbe reduced, whereas if\nthe output is too small,\nthen these weights\nshould be increased.", "learning rule, after the inventors; however, it is more com-\nmonly called the delta rule because it uses the difference\n(or delta) between desired output and the actual output\nto calculate the weight adjustments. In other words, the\nLMS rule specifies that a weight should be adjusted in pro-\nportion to the difference between the output of an ADA-\nLINE network and the desired output: if the neuron makes\na large error, then the weights are adjusted by a large\namount, if the neuron makes a small error, then weights\nare adjusted by a small amount.\nToday, the perceptron is recognized as important mile-\nstone in the development of neural networks because it\nwas the first neural network to be implemented. However,\nmost modern algorithms for training neural networks are\nmore similar to the LMS algorithm. The LMS algorithm\nattempts to minimize the mean squared error of the net-\nwork. As will be discussed in chapter 6, technically this\niterative error reduction process involves a gradient de-\nscent down an error surface; and, today, nearly all neu-\nral networks are trained using some variant of gradient\ndescent.\nThe XOR Problem\nThe success of Rosenblatt, Widrow and Hoff, and others,\nin demonstrating that neural network models could au-\ntomatically learn to distinguish between different sets of\npatterns, generated a lot of excitement around artificial\n116 ChAPtER 4", "intelligence and neural network research. However, in\n1969, Marvin Minsky and Seymour Papert published a\nbook entitled Perceptrons, which, in the annals of neural\nnetwork research, is attributed with single- handedly de-\nstroying this early excitement and optimism (Minsky and\nPapert 1969). Admittedly, throughout the 1960s neural\nnetwork research had suffered from a lot of hype, and a\nlack of success in terms of fulfilling the correspondingly\nhigh expectations. However, Minsky and Papert\u2019s book\nset out a very negative view of the representational power\nof neural networks, and after its publication funding for\nneural network research dried up.\nMinsky and Papert\u2019s book primarily focused on single\nlayer perceptrons. Remember that a single layer percep-\ntron is the same as a single neuron that uses a threshold\nactivation function, and so a single layer perceptron is re-\nstricted to implementing a linear (straight- line) decision\nboundary.1 This means that a single layer perceptron can\nonly learn to distinguish between two classes of inputs if\nit is possible to draw a straight line in the input space that\nhas all of the examples of one class on one side of the line\nand all examples of the other class on the other side of the\nline. Minsky and Papert highlighted this restriction as a\nweakness of these models.\nTo understand Minsky and Papert\u2019s criticism of single\nlayer perceptrons, we must first understand the concept\nof a linearly separable function. We will use a comparison\nA BRIEF hIstoRy oF dEEP LEARnIng 117", "between the logical AND and OR functions with the logi-\ncal XOR function to explain the concept of a linearly sepa-\nrable function. The AND function takes two inputs, each\nof which can be either TRUE or FALSE, and returns TRUE\nif both inputs are TRUE. The plot on the left of figure 4.4\nshows the input space for the AND function and catego-\nrizes each of the four possible input combinations as ei-\nther resulting in an output value of TRUE (shown in the\nfigure by using a clear dot) or FALSE (shown in the figure\nby using black dots). This plot illustrates that is possible\nto draw a straight line between the inputs for which the\nAND function returns TRUE, (T,T), and the inputs for\nwhich the function returns FALSE, {(F,F), (F,T), (T,F)}.\nThe OR function is similar to the AND function, except\nthat it returns TRUE if either or both inputs are TRUE.\nThe middle plot in figure 4.4 shows that it is possible to\ndraw a line that separates the inputs that the OR function\nclassifies as TRUE, {(F,T), (T,F), (T,T)}, from those it clas-\nsifies as FALSE, (F,F). It is because we can draw a single\nstraight line in the input space of these functions that\ndivides the inputs belonging to one category of output\nfrom the inputs belonging to the other output category\nthat the AND and OR functions are linearly separable\nfunctions.\nThe XOR function is also similar in structure to the\nAND and OR functions; however, it only returns TRUE\nif one (but not both) of its inputs are TRUE. The plot on\n118 ChAPtER 4", "Figure 4.2 Illustrations of the linearly separable function. In each figure,\nblack dots represent inputs for which the function returns FALSE, circles\nrepresent inputs for which the function returns TRUE. (T stands for true and\nF stands for false.)\nthe right of figure 4.2 shows the input space for the XOR\nfunction and categorizes each of the four possible input\ncombinations as returning either TRUE (shown in the fig-\nure by using a clear dot) or FALSE (shown in the figure by\nusing black dots). Looking at this plot you will see that it is\nnot possible to draw a straight line between the inputs the\nXOR function classifies as TRUE and those that it classi-\nfies as FALSE. It is because we cannot use a single straight\nline to separate the inputs belonging to different catego-\nries of outputs for the XOR function that this function is\nsaid to be a nonlinearly separable function. The fact that\nthe XOR function is nonlinearly separable does not make\nthe function unique, or even rare\u2014 there are many func-\ntions that are nonlinearly separable.\nThe key criticism that Minsky and Papert made of sin-\ngle layer perceptrons was that these single layer models\nA BRIEF hIstoRy oF dEEP LEARnIng 119", "were unable to learn nonlinearly separable functions, such\nas the XOR function. The reason for this limitation is that\nthe decision boundary of a perceptron is linear and so a\nsingle layer perceptron cannot learn to distinguish be-\ntween the inputs that belong to one output category of a\nnonlinearly separable function from those that belong to\nthe other category.\nIt was known at the time of Minsky and Papert\u2019s\npublication that it was possible to construct neural net-\nworks that defined a nonlinear decision boundary, and\nthus learn nonlinearly separable functions (such as the\nXOR function). The key to creating networks with more\ncomplex (nonlinear) decision boundaries was to extend\nthe network to have multiple layers of neurons. For ex-\nample, figure 4.3 shows a two-l ayer network that imple-\nments the XOR function. In this network, the logical\nTRUE and FALSE values are mapped to numeric values:\nFALSE values are represented by 0, and TRUE values are\nrepresented by 1. In this network, units activate (out-\nput +1) if the weighted sum of inputs is \u22651; otherwise,\nthey output 0. Notice that the units in the hidden layer\nimplement the logical AND and OR functions. These can\nbe understood as intermediate steps to solving the XOR\nchallenge. The unit in the output layer implements the\nXOR by composing the outputs of these hidden layers. In\nother words, the unit in the output layer returns TRUE\nonly when the AND node is off (output=0) and the OR\n120 ChAPtER 4", "Figure 4.3 A network that implements the XOR function. All processing\nunits use a threshold activation function with a threshold of \u22651.\nnode is on (output=1). However, it wasn\u2019t clear at the time\nhow to train networks with multiple layers. Also, at the\nend of their book, Minsky and Papert argued that \u201cin their\njudgment\u201d the research on extending neural networks\nto multiple layers was \u201csterile\u201d (Minsky and Papert 1969,\nsec. 13.2 page 23).\nIn a somewhat ironic historical twist, contempo-\nraneous with Minsky and Papert\u2019s publication, Alexey\nIvakhnenko, a Ukrainian researcher, proposed the group\nmethod for data handling (GMDH), and in 1971 published\nA BRIEF hIstoRy oF dEEP LEARnIng 121", "a paper that described how it could be used to learn a neu-\nral network with eight layers (Ivakhnenko 1971). Today\nIvakhnenko\u2019s 1971 GMDH network is credited with be-\ning the first published example of a deep network trained\nfrom data (Schmidhuber 2015). However, for many years,\nIvaknenko\u2019s accomplishment was largely overlooked by the\nwider neural network community. As a consequence, very\nlittle of the current work in deep learning uses the GMDH\nmethod for training: in the intervening years other train-\ning algorithms, such as backpropagation (described below),\nbecame standardized in the community. At the same time\nof Ivakhnenko\u2019s overlooked accomplishment, Minsky\nand Papert\u2019s critique was proving persuasive and it her-\nalded the end of the first period of significant research on\nneural networks.\nThis first period of neural network research, did, how-\never, leave a legacy that shaped the development of the\nfield up to the present day. The basic internal structure\nof an artificial neuron was defined: a weighted sum of in-\nputs fed through an activation function. The concept of\nstoring information within the weights of a network was\ndeveloped. Furthermore, learning algorithms based on\niteratively adapting weights were proposed, along with\npractical learning rules, such as the LMS rule. In particu-\nlar, the LMS approach, of adjusting the weights of neu-\nrons in proportion to the difference between the output\nof the neuron and the desired output, is present in most\n122 ChAPtER 4", "modern training algorithms. Finally, there was recogni-\ntion of the limitations of single layer networks, and an\nunderstanding that one way to address these limitations\nwas to extend the networks to include multiple layers of\nneurons. At this time, however, it was unclear how to train\nnetworks with multiple layers. Updating a weight requires\nan understanding of how the weight affects the error of\nthe network. For example, in the LMS rule if the output of\nthe neuron was too large, then weights that were applied\nto positive inputs caused the output to increase. There-\nfore, decreasing the size of these weight would reduce the\noutput and thereby reduce the error. But, in the late 1960s,\nthe question of how to model the relationship between the\nweights of the inputs to neurons in the hidden layers of\na network and the overall error of the network was still\nunanswered; and, without this estimation of the contri-\nbution of the weight to the error, it was not possible to\nadjust the weights in the hidden layers of a network. The\nproblem of attributing (or assigning) an amount of error\nto the components in a network is sometimes referred to\nas the credit assignment problem, or as the blame assign-\nment problem.\nConnectionism: Multilayer Perceptrons\nIn the 1980s, people began to reevaluate the criticisms of\nthe late 1960s as being overly severe. Two developments,\nA BRIEF hIstoRy oF dEEP LEARnIng 123", "in particular, reinvigorated the field: (1) Hopfield net-\nworks; and (2) the backpropagation algorithm.\nIn 1982, John Hopfield published a paper where he\ndescribed a network that could function as an associative\nmemory (Hopfield 1982). During training, an associative\nmemory learns a set of input patterns. Once the associate\nmemory network has been trained, then, if a corrupted\nversion of one of the input patterns is presented to the\nnetwork, the network is able to regenerate the complete\ncorrect pattern. Associative memories are useful for a\nnumber of tasks, including pattern completion and error\ncorrection. Table 4.12 illustrates the tasks of pattern com-\npletion and error correction using the example of an asso-\nciative memory that has been trained to store information\non people\u2019s birthdays. In a Hopfield network, the memo-\nries, or input patterns, are encoded in binary strings; and,\nTable 4.1. Illustration of the uses of an association\nmemory for pattern completion and error correction\nTraining patterns Pattern completion\nJohn**12May Liz***????? \u2192 Liz***25Feb\nKerry*03Jan ???***10Mar \u2192 Des***10Mar\nLiz***25Feb Error correction\nDes***10Mar Kerry*01Apr \u2192 Kerry*03Jan\nJosef*13Dec Jxsuf*13Dec \u2192 Josef*13Dec\n124 ChAPtER 4", "assuming binary patterns are relatively distinct from each\nother, a Hopfield network can store up to 0.138N of these\nstrings, where N is the number of neurons in the network.\nSo to store 10 distinct patterns requires a Hopfield net-\nwork with 73 neurons, and to store 14 distinct patterns\nrequires 100 neurons.\nBackpropagation and Vanishing Gradients\nIn 1986, a group of researchers known as the parallel\ndistributed processing (PDP) research group published a\ntwo- book overview of neural network research (Rumel-\nhart et al. 1986b, 1986c). These books proved to be in-\ncredibly popular, and chapter 8 in volume one described\nthe backpropagation algorithm (Rumelhart et al. 1986a).\nThe backpropagation algorithm has been invented a num-\nber of times,3 but it was this chapter by Rumelhart, Hin-\nton, and Williams, published by PDP, that popularized\nits use. The backpropagation algorithm is a solution to\nthe credit assignment problem and so it can be used to\ntrain a neural network that has hidden layers of neurons.\nThe backpropagation algorithm is possibly the most im-\nportant algorithm in deep learning. However, a clear and\ncomplete explanation of the backpropagation algorithm\nrequires first explaining the concept of an error gradient,\nand then the gradient descent algorithm. Consequently,\nthe in- depth explanation of backpropagation is post-\nponed until chapter 6, which begins with an explanation\nA BRIEF hIstoRy oF dEEP LEARnIng 125", "of these necessary concepts. The general structure of the\nalgorithm, however, can be described relatively quickly.\nThe backpropagation algorithm starts by assigning ran-\ndom weights to each of the connections in the network.\nThe algorithm then iteratively updates the weights in the\nnetwork by showing training instances to the network and\nupdating the network weights until the network is work-\ning as expected. The core algorithm works in a two- stage\nprocess. In the first stage (known as the forward pass), an\ninput is presented to the network and the neuron activa-\ntions are allowed to flow forward through the network un-\ntil an output is generated. The second stage (known as the\nbackward pass) begins at the output layer and works back-\nward through the network until the input layer is reached.\nThis backward pass begins by calculating an error for each\nneuron in the output layer. This error is then used to up-\ndate the weights of these output neurons. Then the error\nof each output neuron is shared back (backpropagated) to\nthe hidden neurons that connect to it, in proportion to\nthe weights on the connections between the output neu-\nron and the hidden neuron. Once this sharing (or blame\nassignment) has been completed for a hidden neuron, the\ntotal blame attributable to that hidden neuron is summed\nand this total is used to update the weights on that neuron.\nThe backpropagation (or sharing back) of blame is then\nrepeated for the neurons that have not yet had blame at-\ntributed to them. This process of blame assignment and\n126 ChAPtER 4", "weight updates continues back through the network until\nall the weights have been updated.\nA key innovation that enabled the backpropagation al-\ngorithm to work was a change in the activation functions\nused in the neurons. The networks that were developed\nin the early years of neural network research used thresh-\nold activation functions. The backpropagation algorithm\ndoes not work with threshold activation functions be-\ncause backpropagation requires that the activation func-\ntions used by the neurons in the network be differentiable.\nThreshold activation functions are not differentiable be-\ncause there is a discontinuity in the output of the function\nat the threshold. In other words, the slope of a threshold\nfunction at the threshold is infinite and therefore it is not\npossible to calculate the gradient of the function at that\npoint. This led to the use of differentiable activation func-\ntions in multilayer neural networks, such as the logistic\nand tanh functions.\nThere is, however, an inherent limitation with using\nthe backpropagation algorithm to train deep networks.\nIn the 1980s, researchers found that backpropagation\nworked well with relatively shallow networks (one or two\nlayers of hidden units), but that as the networks got deeper,\nthe networks either took an inordinate amount of time to\ntrain, or else they entirely failed to converge on a good set\nof weights. In 1991, Sepp Hochreiter (working with J\u00fcrgen\nSchmidhuber) identified the cause of this problem in his\nA BRIEF hIstoRy oF dEEP LEARnIng 127", "diploma thesis (Hochreiter 1991). The problem is caused\nby the way the algorithm backpropagates errors. Fun-\ndamentally, the backpropagation algorithm is an imple-\nmentation of the chain rule from calculus. The chain rule\ninvolves the multiplication of terms, and backpropagat-\ning an error from one neuron back to another can involve\nmultiplying the error by a number terms with values less\nthan 1. These multiplications by values less than 1 happen\nrepeatedly as the error signal gets passed back through the\nnetwork. This results in the error signal becoming smaller\nand smaller as it is backpropagated through the network.\nIndeed, the error signal often diminishes exponentially\nwith respect to the distance from the output layer. The\neffect of this diminishing error is that the weights in the\nearly layers of a deep network are often adjusted by only\na tiny (or zero) amount during each training iteration. In\nother words, the early layers either train very, very slowly\nor do not move away from their random starting positions\nat all. However, the early layers in a neural network are\nvitally important to the success of the network, because\nit is the neurons in these layers that learn to detect the\nfeatures in the input that the later layers of the network\nuse as the fundamental building blocks of the representa-\ntions that ultimately determine the output of the network.\nFor technical reasons, which will be explained in chapter\n6, the error signal that is backpropagated through the net-\nwork is in fact the gradient of the error of the network,\n128 ChAPtER 4", "and, as a result, this problem of the error signal rapidly di-\nminishing to near zero is known as the vanishing gradient\nproblem.\nConnectionism and Local versus Distributed\nRepresentations\nDespite the vanishing gradient problem, the backpropa-\ngation algorithm opened up the possibility of training\nmore complex (deeper) neural network architectures.\nThis aligned with the principle of connectionism. Connec-\ntionism is the idea that intelligent behavior can emerge\nfrom the interactions between large numbers of simple\nprocessing units. Another aspect of connectionism was\nthe idea of a distributed representation. A distinction can\nbe made in the representations used by neural networks\nbetween localist and distributed representations. In a lo-\ncalist representation there is a one- to- one correspondence\nbetween concepts and neurons, whereas in a distributed\nrepresentation each concept is represented by a pattern\nof activations across a set of neurons. Consequently, in a\ndistributed representation each concept is represented by\nthe activation of multiple neurons and the activation of\neach neuron contributes to the representation of multiple\nconcepts.\nTo illustrate the distinction between localist and dis-\ntributed representations, consider a scenario where (for\nsome unspecified reason) a set of neuron activations is\nA BRIEF hIstoRy oF dEEP LEARnIng 129", "In a distributed\nrepresentation each\nconcept is represented\nby the activation of\nmultiple neurons and\nthe activation of each\nneuron contributes to\nthe representation of\nmultiple concepts.", "being used to represent the absence or presence of dif-\nferent foods. Furthermore, each food has two properties,\nthe country of origin of the recipe and its taste. The pos-\nsible countries of origin are: Italy, Mexico, or France; and,\nthe set of possible tastes are: Sweet, Sour, or Bitter. So, in\ntotal there are nine possible types of food: Italian+Sweet,\nItalian+Sour, Italian+Bitter, Mexican+Sweet, etc. Using a\nlocalist representation would require nine neurons, one\nneuron per food type. There are, however, a number of\nways to define a distributed representation of this do-\nmain. One approach is to assign a binary number to each\ncombination. This representation would require only four\nneurons, with the activation pattern 0000 representing\nItalian+Sweet, 0001 representing Italian+Sour, 0010 rep-\nresenting Italian+Bitter, and so on up to 1000 represent-\ning French+Bitter. This is a very compact representation.\nHowever, notice that in this representation the activation\nof each neuron in isolation has no independently mean-\ningful interpretation: the rightmost neuron would be ac-\ntive (***1) for Italian+Sour, Mexican+Sweet, Mexican+Bitter,\nand France+Sour, and without knowledge of the activa-\ntion of the other neurons, it is not possible know what\ncountry or taste is being represented. However, in a\ndeep network the lack of semantic interpretability of the\nactivations of hidden units is not a problem, so long as\nthe neurons in the output layer of the network are able\nto combine these representations in such a way so as to\nA BRIEF hIstoRy oF dEEP LEARnIng 131", "generate the correct output. Another, more transparent,\ndistributed representation of this food domain is to use\nthree neurons to represent the countries and three neu-\nrons to represent the tastes. In this representation, the\nactivation pattern 100100 could represent Italian+Sweet,\n001100 could represent French+Sweet, and 001001 could\nrepresent French+Bitter. In this representation, the acti-\nvation of each neuron can be independently interpreted;\nhowever the distribution of activations across the set of\nneurons is required in order to retrieve the full description\nof the food (country+taste). Notice, however, that both of\nthese distributed representations are more compact than\nthe localist representation. This compactness can signifi-\ncantly reduce the number of weights required in a network,\nand this in turn can result in faster training times for the\nnetwork.\nThe concept of a distributed representation is very\nimportant within deep learning. Indeed, there is a good\nargument that deep learning might be more appropriately\nnamed representation learning\u2014 the argument being that\nthe neurons in the hidden layers of a network are learning\ndistributed representations of the input that are useful in-\ntermediate representations in the mapping from inputs to\noutputs that the network is attempting to learn. The task\nof the output layer of a network is then to learn how to\ncombine these intermediate representations so as to gen-\nerate the desired outputs. Consider again the network in\n132 ChAPtER 4", "figure 4.3 that implements the XOR function. The hidden\nunits in this network learn an intermediate representa-\ntion of the input, which can be understood as composed\nof the AND and OR functions; the output layer then com-\nbines this intermediate representation to generate the\nrequired output. In a deep network with multiple hidden\nlayers, each subsequent hidden layer can be interpreted as\nlearning a representation that is an abstraction over the\noutputs of the preceding layer. It is this sequential abstrac-\ntion, through learning intermediate representations, that\nenables deep networks to learn such complex mappings\nfrom inputs to outputs.\nNetwork Architectures: Convolutional and Recurrent\nNeural Networks\nThere are a considerable number of ways in which a set\nof neurons can be connected together. The network ex-\namples presented so far in the book have been connected\ntogether in a relatively uncomplicated manner: neurons\nare organized into layers and each neuron in a layer is di-\nrectly connected to all of the neurons in the next layer of\nthe network. These networks are known as feedforward\nnetworks because there are no loops within the network\nconnections: all the connections point forward from the\ninput toward the output. Furthermore, all of our net-\nwork examples thus far would be considered to be fully\nconnected, because each neuron is connected to all the\nA BRIEF hIstoRy oF dEEP LEARnIng 133", "neurons in the next layer. It is possible, and often use-\nful, to design and train networks that are not feedforward\nand/or that are not fully connected. When done correctly,\ntailoring network architectures can be understood as em-\nbedding into the network architecture information about\nthe properties of the problem that the network is trying\nto learn to model.\nA very successful example of incorporating domain\nknowledge into a network by tailoring the networks ar-\nchitecture is the design of convolutional neural networks\n(CNNs) for object recognition in images. In the 1960s,\nHubel and Wiesel carried out a series of experiments on\nthe visual cortex of cats (Hubel and Wiesel 1962, 1965).\nThese experiments used electrodes inserted into the\nbrains of sedated cats to study the response of the brain\ncells as the cats were presented with different visual stim-\nuli. Examples of the stimuli used included bright spots or\nlines of light appearing at a location in the visual field, or\nmoving across a region of the visual field. The experiments\nfound that different cells responded to different stimuli at\ndifferent locations in the visual field: in effect a single cell\nin the visual cortex would be wired to respond to a par-\nticular type of visual stimulus occurring within a particu-\nlar region of the visual field. The region of the visual field\nthat a cell responded to was known as the receptive field\nof the cell. Another outcome of these experiments was the\ndifferentiation between two types of cells: \u201csimple\u201d and\n134 ChAPtER 4", "\u201ccomplex.\u201d For simple cells, the location of the stimulus is\ncritical with a slight displacement of the stimulus resulting\nin a significant reduction in the cell\u2019s response. Complex\ncells, however, respond to their target stimuli regardless\nof where in the field of vision the stimulus occurs. Hubel\nand Wiesel (1965) proposed that complex cells behaved as\nif they received projections from a large number of simple\ncells all of which respond to the same visual stimuli but\ndiffering in the position of their receptive fields. This hi-\nerarchy of simple cells feeding into complex cells results\nin funneling of stimuli from large areas of the visual field,\nthrough a set of simple cells, into a single complex cell. Fig-\nure 4.4 illustrates this funneling effect. This figure shows\na layer of simple cells each monitoring a receptive field at\na different location in the visual field. The receptive field\nof the complex cell covers the layer of simple cells, and\nVisual\nfield Layerof\nsimplecells\nComplex\ncell\nFigure 4.4 The funneling effect of receptive fields created by the hierarchy\nof simple and complex cells.\nA BRIEF hIstoRy oF dEEP LEARnIng 135", "this complex cell activates if any of the simple cells in its\nreceptive field activates. In this way the complex cell can\nrespond to a visual stimulus if it occurs at any location\nin the visual field.\nIn the late 1970s and early 1980s, Kunihiko Fuku-\nshima was inspired by Hubel and Wiesel\u2019s analysis of the\nvisual cortex and developed a neural network architecture\nfor visual pattern recognition that was called the neocog-\nnitron (Fukushima 1980). The design of the neocognitron\nwas based on the observation that an image recogni-\ntion network should be able to recognize if a visual fea-\nture is present in an image irrespective of location in the\nimage\u2014 or, to put it slightly more technically, the network\nshould be able to do spatially invariant visual feature de-\ntection. For example, a face recognition network should\nbe able to recognize the shape of an eye no matter where\nin the image it occurs, similar to the way a complex cell\nin Hubel and Wiesel\u2019s hierarchical model could detect the\npresence of a visual feature irrespective of where in the\nvisual field it occurred.\nFukushima realized that the functioning of the simple\ncells in the Hubel and Wiesel hierarchy could be replicated\nin a neural network using a layer of neurons that all use\nthe same set of weights, but with each neuron receiving in-\nputs from fixed small regions (receptive fields) at different\nlocations in the input field. To understand the relationship\nbetween neurons sharing weights and spatially invariant\n136 ChAPtER 4", "visual feature detection, imagine a neuron that receives a\nset of pixel values, sampled from a region of an image, as\nits inputs. The weights that this neuron applies to these\npixel values define a visual feature detection function that\nreturns true (high activation) if a particular visual feature\n(pattern) occurs in the input pixels, and false otherwise.\nConsequently, if a set of neurons all use the same weights,\nthey will all implement the same visual feature detector. If\nthe receptive fields of these neurons are then organized\nso that together they cover the entire image, then if the\nvisual feature occurs anywhere in the image at least one of\nthe neurons in the group will identify it and activate.\nFukushima also recognized that the Hubel and Wiesel\nfunneling effect (into complex cells) could be obtained by\nneurons in later layers also receiving as input the outputs\nfrom a fixed set of neurons in a small region of the preced-\ning layer. In this way, the neurons in the last layer of the\nnetwork each receive inputs from across the entire input\nfield allowing the network to identify the presence of a\nvisual feature anywhere in the visual input.\nSome of the weights in neocognitron were set by hand,\nand others were set using an unsupervised training pro-\ncess. In this training process, each time an example is pre-\nsented to the network a single layer of neurons that share\nthe same weights is selected from the layers that yielded\nlarge outputs in response to the input. The weights of the\nneurons in the selected layer are updated so as to reinforce\nA BRIEF hIstoRy oF dEEP LEARnIng 137", "their response to that input pattern and the weights of\nneurons not in the layer are not updated. In 1989 Yann\nLeCun developed the convolutional neural network (CNN)\narchitecture specifically for the task of image processing\n(LeCun 1989). The CNN architecture shared many of\nthe design features found in the neocognitron; however,\nLeCun showed how these types of networks could be\ntrained using backpropagation. CNNs have proved to be\nincredibly successful in image processing and other tasks.\nA particularly famous CNN is the AlexNet network, which\nwon the ImageNet Large- Scale Visual Recognition Chal-\nlenge (ILSVRC) in 2012 (Krizhevsky et al. 2012). The goal\nof the ILSVRC competition is to identify objects in pho-\ntographs. The success of AlexNet at the ILSVRC competi-\ntion generated a lot of excitement about CNNs, and since\nAlexNet a number of other CNN architectures have won\nthe competition. CNNs are one of the most popular types\nof deep neural networks, and chapter 5 will provide a more\ndetailed explanation of them.\nRecurrent neural networks (RNNs) are another ex-\nample of a neural network architecture that has been tai-\nlored to the specific characteristics of a domain. RNNs are\ndesigned to process sequential data, such as language. An\nRNN network processes a sequence of data (such as a sen-\ntence) one input at a time. An RNN has only a single hid-\nden layer. However, the output from each of these hidden\nneurons is not only fed forward to the output neurons, it\n138 ChAPtER 4", "is also temporarily stored in a buffer and then fed back\ninto all of the hidden neurons at the next input. Conse-\nquently, each time the network processes an input, each\nneuron in the hidden layer receives both the current input\nand the output the hidden layer generated in response to\nthe previous input. In order to understand this explana-\ntion, it may at this point be helpful to briefly skip forward\nto figure 5.2 to see an illustration of the structure of an\nRNN and the flow of information through the network.\nThis recurrent loop, of activations from the output of the\nhidden layer for one input being fed back into the hidden\nlayer alongside the next input, gives an RNN a memory\nthat enables it to process each input in the context of the\nprevious inputs it has processed.4 RNNs are considered\ndeep networks because this evolving memory can be con-\nsidered as deep as the sequence is long.\nAn early well-k nown RNN is the Elman network. In\n1990, Jeffrey Locke Elman published a paper that de-\nscribed an RNN that had been trained to predict the end-\nings of simple two- and three- word utterances (Elman\n1990). The model was trained on a synthesized dataset\nof simple sentences generated using an artificial gram-\nmar. The grammar was built using a lexicon of twenty-\nthree words, with each word assigned to a single lexical\ncategory (e.g., man=NOUN- HUM, woman=NOUN- HUM,\neat=VERB- EAT, cookie=NOUN- FOOD, etc.). Using this\nlexicon, the grammar defined fifteen sentence generation\nA BRIEF hIstoRy oF dEEP LEARnIng 139", "templates (e.g., NOUN- HUM+VERB- EAT+NOUN- FOOD\nwhich would generate sentences such as man eat cookie).\nOnce trained, the model was able to generate reason-\nable continuations for sentences, such as woman+eat+? =\ncookie. Furthermore, once the network was started, it was\nable to generate longer strings consisting of multiple sen-\ntences, using the context it generated itself as the input\nfor the next word, as illustrated by this three- sentence\nexample:\ngirl eat bread dog move mouse mouse move book\nAlthough this sentence generation task was applied\nto a very simple domain, the ability of the RNN to gener-\nate plausible sentences was taken as evidence that neural\nnetworks could model linguistic productivity without re-\nquiring explicit grammatical rules. Consequently, Elman\u2019s\nwork had a huge impact on psycholinguistics and psychol-\nogy. The following quote, from Churchland 1996, illus-\ntrates the importance that some researchers attributed to\nElman\u2019s work:\nThe productivity of this network is of course a feeble\nsubset of the vast capacity that any normal English\nspeaker commands. But productivity is productivity,\nand evidently a recurrent network can possess\nit. Elman\u2019s striking demonstration hardly settles\n140 ChAPtER 4", "the issue between the rule- centered approach to\ngrammar and the network approach. That will be\nsome time in working itself out. But the conflict is\nnow an even one. I\u2019ve made no secret where my own\nbets will be placed. (Churchland 1996, p. 143)5\nAlthough RNNs work well with sequential data, the\nvanishing gradient problem is particularly severe in these\nnetworks. In 1997, Sepp Hochreiter and J\u00fcrgen Schmid-\nhuber, the researchers who in 1991 had presented an ex-\nplanation of the vanishing gradient problem, proposed\nthe long short- term memory (LSTM) units as a solution\nto this problem in RNNs (Hochreiter and Schmidhuber\n1997). The name of these units draws on a distinction be-\ntween how a neural network encodes long- term memory\n(understood as concepts that are learned over a period of\ntime) through training and short- term memory (under-\nstood as the response of the system to immediate stim-\nuli). In a neural network, long- term memory is encoded\nthrough adjusting the weights of the network and once\ntrained these weights do not change. Short- term memory\nis encoded in a network through the activations that flow\nthrough the network and these activation values decay\nquickly. LSTM units are designed to enable the short- term\nmemory (the activations) in the network to be propagated\nover long periods of time (or sequences of inputs). The\ninternal structure of an LSTM is relatively complex, and\nA BRIEF hIstoRy oF dEEP LEARnIng 141", "we will describe it in chapter 5. The fact that LSTM can\npropagate activations over long periods enables them to\nprocess sequences that include long- distance dependen-\ncies (interactions between elements in a sequence that\nare separated by two or more positions). For example,\nthe dependency between the subject and the verb in an\nEnglish sentence: The dog/dogs in that house is/are aggres-\nsive. This has made LSTM networks suitable for language\nprocessing, and for a number of years they have been the\ndefault neural network architecture for many natural\nlanguage processing models, including machine transla-\ntion. For example, the sequence- to- sequence (seq2seq)\nmachine translation architecture introduced in 2014 con-\nnects two LSTM networks in sequence (Sutskever et al.\n2014). The first LSTM network, the encoder, processes\nthe input sequence one input at a time, and generates a\ndistributed representation of that input. The first LSTM\nnetwork is called an encoder because it encodes the se-\nquence of words into a distributed representation. The\nsecond LSTM network, the decoder, is initialized with the\ndistributed representation of the input and is trained to\ngenerate the output sequence one element at a time us-\ning a feedback loop that feeds the most recent output ele-\nment generated by the network back in as the input for\nthe next time step. Today, this seq2seq architecture is the\nbasis for most modern machine translation systems, and\nis explained in more detail in chapter 5.\n142 ChAPtER 4", "By the late 1990s, most of the conceptual require-\nments for deep learning were in place, including both the\nalgorithms to train networks with multiple layers, and\nthe network architectures that are still very popular today\n(CNNs and RNNs). However, the problem of the vanishing\ngradients still stifled the creation of deep networks. Also,\nfrom a commercial perspective, the 1990s (similar to the\n1960s) experienced a wave of hype based on neural net-\nworks and unrealized promises. At the same time, a num-\nber of breakthroughs in other forms of machine learning\nmodels, such as the development of support vector ma-\nchines (SVMs), redirected the focus of the machine learn-\ning research community away from neural networks: at\nthe time SVMs were achieving similar accuracy to neural\nnetwork models but were easier to train. Together these\nfactors led to a decline in neural network research that\nlasted up until the emergence of deep learning.\nThe Era of Deep Learning\nThe first recorded use of the term deep learning is credited\nto Rina Dechter (1986), although in Dechter\u2019s paper the\nterm was not used in relation to neural networks; and the\nfirst use of the term in relation to neural networks is cred-\nited to Aizenberg et al. (2000).6 In the mid- 2000s, inter-\nest in neural networks started to grow, and it was around\nA BRIEF hIstoRy oF dEEP LEARnIng 143", "this time that the term deep learning came to prominence\nto describe deep neural networks. The term deep learn-\ning is used to emphasize the fact that the networks being\ntrained are much deeper than previous networks.\nOne of the early successes of this new era of neural\nnetwork research was when Geoffrey Hinton and his col-\nleagues demonstrated that it was possible to train a deep\nneural network using a process known as greedy layer-\nwise pretraining. Greedy layer- wise pretraining begins by\ntraining a single layer of neurons that receives input di-\nrectly from the raw input. There are a number of different\nways that this single layer of neurons can be trained, but\none popular way is to use an autoencoder. An autoencoder\nis a neural network with three layers: an input layer, a hid-\nden (encoding) layer, and an output (decoding) layer. The\nnetwork is trained to reconstruct the inputs it receives in\nthe output layer; in other words, the network is trained\nto output the exact same values that it received as input.\nA very important feature in these networks is that they\nare designed so that it is not possible for the network to\nsimply copy the inputs to the outputs. For example, an\nautoencoder may have fewer neurons in the hidden layer\nthan in the input and output layer. Because the autoen-\ncoder is trying to reconstruct the input at the output layer,\nthe fact that the information from the input must pass\nthrough this bottleneck in the hidden layer forces the au-\ntoencoder to learn an encoding of the input data in the\n144 ChAPtER 4", "hidden layer that captures only the most important fea-\ntures in the input, and disregards redundant or superflu-\nous information.7\nLayer- Wise Pretraining Using Autoencoders\nIn layer- wise pretraining, the initial autoencoder learns an\nencoding for the raw inputs to the network. Once this en-\ncoding has been learned, the units in the hidden encoding\nlayer are fixed, and the output (decoding) layer is thrown\naway. Then a second autoencoder is trained\u2014 but this\nautoencoder is trained to reconstruct the representation\nof the data generated by passing it through the encoding\nlayer of the initial autoencoder. In effect, this second au-\ntoencoder is stacked on top of the encoding layer of the\nfirst autoencoder. This stacking of encoding layers is con-\nsidered to be a greedy process because each encoding layer\nis optimized independently of the later layers; in other\nwords, each autoencoder focuses on finding the best solu-\ntion for its immediate task (learning a useful encoding for\nthe data it must reconstruct) rather than trying to find a\nsolution to the overall problem for the network.\nOnce a sufficient number8 of encoding layers have\nbeen trained, a tuning phase can be applied. In the tuning\nphase, a final network layer is trained to predict the tar-\nget output for the network. Unlike the pretraining of the\nearlier layers of the network, the target output for the fi-\nnal layer is different from the input vector and is specified\nA BRIEF hIstoRy oF dEEP LEARnIng 145", "in the training dataset. The simplest tuning is where the\npretrained layers are kept frozen (i.e., the weights in the\npretrained layers don\u2019t change during the tuning); how-\never, it is also feasible to train the entire network during\nthe tuning phase. If the entire network is trained during\ntuning, then the layer- wise pretraining is best understood\nas finding useful initial weights for the earlier layers in the\nnetwork. Also, it is not necessary that the final prediction\nmodel that is trained during tuning be a neural network.\nIt is quite possible to take the representations of the data\ngenerated by the layer- wise pretraining and use it as the\ninput representation for a completely different type of\nmachine learning algorithm, for example, a support vector\nmachine or a nearest neighbor algorithm. This scenario is\na very transparent example of how neural networks learn\nuseful representations of data prior to the final prediction\ntask being learned. Strictly speaking, the term pretraining\ndescribes only the layer- wise training of the autoencoders;\nhowever, the term is often used to refer to both the layer-\nwise training stage and the tuning stage of the model.\nFigure 4.5 shows the stages in layer- wise pretraining.\nThe figure on the left illustrates the training of the initial\nautoencoder where an encoding layer (the black circles) of\nthree units is attempting to learn a useful representation\nfor the task of reconstructing an input vector of length 4.\nThe figure in the middle of figure 4.5 shows the training of\na second autoencoder stacked on top of the encoding layer\n146 ChAPtER 4", "Targetoutput\nB1 B2 B3 D4\nA1 A2 A3 A4 C3 C5 C1 C2\nB2 B4 B6 B1 B2 B3 B1 B2 B3\nA1 A2 A3 A4 A1 A2 A3 A4 A1 A2 A3 A4\nPretraininglayerB PretraininglayerC Tuning\nFigure 4.5 The pretraining and tuning stages in greedy layer- wise\npretraining. Black circles represent the neurons whose training is the primary\nobjective at each training stage. The gray background marks the components\nin the network that are frozen during each training stage.\nof the first autoencoder. In this autoencoder, a hidden\nlayer of two units is attempting to learn an encoding for an\ninput vector of length 3 (which in turn is an encoding of a\nvector of length 4). The grey background in each figure de-\nmarcates the components in the network that are frozen\nduring this training stage. The figure on the right shows\nthe tuning phase where a final output layer is trained to\npredict the target feature for the model. For this example,\nin the tuning phase the pretrained layers in the network\nhave been frozen.\nLayer- wise pretraining was important in the evolu-\ntion of deep learning because it was the first approach\nto training deep networks that was widely adopted.9\nHowever, today most deep learning networks are trained\nA BRIEF hIstoRy oF dEEP LEARnIng 147", "without using layer- wise pretraining. In the mid- 2000s,\nresearchers began to appreciate that the vanishing gra-\ndient problem was not a strict theoretical limit, but was\ninstead a practical obstacle that could be overcome. The\nvanishing gradient problem does not cause the error gra-\ndients to disappear entirely; there are still gradients being\nbackpropagated through the early layers of the network, it\nis just that they are very small. Today, there are a number\nof factors that have been identified as important in suc-\ncessfully training a deep network.\nWeight Initialization and ReLU Activation Functions\nOne factor that is important in successfully training a\ndeep network is how the network weights are initialized.\nThe principles controlling how weight initialization af-\nfects the training of a network are still not clear. There\nare, however, weight initialization procedures that have\nbeen empirically shown to help with training a deep net-\nwork. Glorot initialization10 is a frequently used weight\ninitialization procedure for deep networks. It is based on\na number of assumptions but has empirical success to sup-\nport its use. To get an intuitive understanding of Glorot\ninitialization, consider the fact that there is typically a re-\nlationship between the magnitude of values in a set and\nthe variance of the set: generally the larger the values in\na set, the larger the variance of the set. So, if the variance\ncalculated on a set of gradients propagated through a layer\n148 ChAPtER 4", "In the mid- 2000s,\nresearchers began to\nappreciate that the\nvanishing gradient\nproblem was not a strict\ntheoretical limit, but\nwas instead a practical\nobstacle that could be\novercome.", "at one point in the network is similar to the variance for\nthe set of gradients propagated through another layer in\na network, it is likely that the magnitude of the gradients\npropagated through both of these layers will also be simi-\nlar. Furthermore, the variance of gradients in a layer can\nbe related to the variance of the weights in the layer, so a\npotential strategy to maintain gradients flowing through\na network is to ensure similar variances across each of the\nlayer in a network. Glorot initialization is designed to ini-\ntialize the weight in a network in such a way that all of the\nlayers in a network will have a similar variance in terms\nof both forward pass activations and the gradients propa-\ngated during the backward pass in backpropagation. Glo-\nrot initialization defines a heuristic rule to meet this goal\nthat involves sampling the weights for a network using the\nfollowing uniform distribution (where w is the weight on a\nconnection between layer j and j+i that is being initialized,\nU[- a,a] is the uniform distribution over the interval (- a,a),\nn is the number of neurons in layer j, and the notation w\nj\n~ U indicates that the value of w is sampled from distribu-\ntion U)11:\n\uf8ee 6 6 \uf8f9\nw\u223cU\uf8ef\u2212 , \uf8fa\n\uf8f0 n j+n j+1 n j+n j+1 \uf8fb\nAnother factor that contributes to the success or\nfailure of training a deep network is the selection of the\n150 ChAPtER 4", "activation function used in the neurons. Backpropagating\nan error gradient through a neuron involves multiplying\nthe gradient by the value of the derivative of the activation\nfunction at the activation value of the neuron recorded\nduring the forward pass. The derivatives of the logistic\nand tanh activation functions have a number of properties\nthat can exacerbate the vanishing gradient problem if they\nare used in this multiplication step. Figure 4.6 presents a\nplot of the logistic function and the derivative of the logis-\ntic function. The maximum value of the derivative is 0.25.\nConsequently, after an error gradient has been multiplied\n0.1\nLogistic(z)\n\u2202 logistic(z)\n\u2202 z 8.0\n)z(noitavitcA 6.0\n4.0\nmax = 0.25\n2.0\nsaturated = 0 saturated = 0 0.0\n\u221210 \u22125 0 5 10\nz\nFigure 4.6 Plots of the logistic function and the derivative of the logistic\nfunction.\nA BRIEF hIstoRy oF dEEP LEARnIng 151", "by the value of the derivative of the logistic function at\nthe appropriate activation for the neuron, the maximum\nvalue the gradient will have is a quarter of the gradient\nprior to the multiplication. Another problem with using\nthe logistic function is that there are large portions of the\ndomain of the function where the function is saturated\n(returning values that very close to 0 or 1), and the rate\nof change of the function in these regions is near zero;\nthus, the derivative of the function is near 0. This is an\nundesirable property when backpropagating error gradi-\nents because the error gradients will be forced to zero (or\nclose to zero) when backpropagated through any neuron\nwhose activation is within one of these saturated regions.\nIn 2011 it was shown that switching to a rectified linear\nactivation function, g(x)=max(0,x), improved training\nfor deep feedforward neural networks (Glorot et al. 2011).\nNeurons that use a rectified linear activation function are\nknown as rectified linear units (ReLUs). One advantage\nof ReLUs is that the activation function is linear for the\npositive portion of its domain with a derivative equal to 1.\nThis means that gradients can flow easily through ReLUs\nthat have positive activation. However, the drawback of\nReLUs is that the gradient of the function for the nega-\ntive part of its domain is zero, so ReLUs do not train in\nthis portion of the domain. Although undesirable, this\nis not necessarily a fatal flaw for learning because when\nbackpropagating through a layer of ReLUs the gradients\n152 ChAPtER 4", "can still flow through the ReLUs in the layers that have\npositive activation. Furthermore, there are a number of\nvariants of the basic ReLU that introduce a gradient on\nthe negative side of the domain, a commonly used variant\nbeing the leaky ReLU (Maas et al. 2013). Today, ReLUs (or\nvariants of ReLUs) are the most frequently used neurons\nin deep learning research.\nThe Virtuous Cycle: Better Algorithms, Faster Hardware,\nBigger Data\nAlthough improved weight initialization methods and\nnew activation functions have both contributed to the\ngrowth of deep learning, in recent years the two most\nimportant factors driving deep learning have been the\nspeedup in computer power and the massive increase in\ndataset sizes. From a computational perspective, a major\nbreakthrough for deep learning occurred in the late 2000s\nwith the adoption of graphical processing units (GPUs)\nby the deep learning community to speed up training. A\nneural network can be understood as a sequence of matrix\nmultiplications that are interspersed with the application\nof nonlinear activation functions, and GPUs are optimized\nfor very fast matrix multiplication. Consequently, GPUs\nare ideal hardware to speed up neural network train-\ning, and their use has made a significant contribution to\nthe development of the field. In 2004, Oh and Jung re-\nported a twentyfold performance increase using a GPU\nA BRIEF hIstoRy oF dEEP LEARnIng 153", "implementation of a neural network (Oh and Jung 2004),\nand the following year two further papers were published\nthat demonstrated the potential of GPUs to speed up the\ntraining of neural networks: Steinkraus et al. (2005) used\nGPUs to train a two- layer neural network, and Chella-\npilla et al. (2006) used GPUs to train a CNN. However, at\nthat time there were significant programming challenges\nto using GPUs for training networks (the training algo-\nrithm had to be implemented as a sequence of graphics\noperations), and so the initial adoption of GPUs by neural\nnetwork researchers was relatively slow. These program-\nming challenges were significantly reduced in 2007 when\nNVIDIA (a GPU manufacturer) released a C- like program-\nming interface for GPUs called CUDA (compute unified\ndevice architecture).12 CUDA was specifically designed to\nfacilitate the use of GPUs for general computing tasks. In\nthe years following the release of CUDA, the use of GPUs\nto speed up neural network training became standard.\nHowever, even with these more powerful computer\nprocessors, deep learning would not have been possible\nunless massive datasets had also become available. The de-\nvelopment of the internet and social media platforms, the\nproliferation of smartphones and \u201cinternet of things\u201d sen-\nsors, has meant that the amount of data being captured\nhas grown at an incredible rate over the last ten years.\nThis has made it much easier for organizations to gather\nlarge datasets. This growth in data has been incredibly\n154 ChAPtER 4", "important to deep learning because neural network mod-\nels scale well with larger data (and in fact they can struggle\nwith smaller datasets). It has also prompted organizations\nto consider how this data can be used to drive the develop-\nment of new applications and innovations. This in turn\nhas driven a need for new (more complex) computational\nmodels in order to deliver these new applications. And, the\ncombination of large data and more complex algorithms\nrequires faster hardware in order to make the necessary\ncomputational workload tractable. Figure 4.7 illustrates\nthe virtuous cycle between big data, algorithmic break-\nthroughs (e.g., better weight initialization, ReLUs, etc.),\nBig\ndata\nFaster\nhardware\nBetter\nalgorithms\nFigure 4.7 The virtuous cycle driving deep learning. Figure inspired by\nfigure 1.2 in Reagen et al. 2017.\nA BRIEF hIstoRy oF dEEP LEARnIng 155", "and improved hardware that is driving the deep learning\nrevolution.\nSummary\nThe history of deep learning reveals a number of under-\nlying themes. There has been a shift from simple binary\ninputs to more complex continuous valued input. This\ntrend toward more complex inputs is set to continue\nbecause deep learning models are most useful in high-\ndimensional domains, such as image processing and lan-\nguage. Images often have thousands of pixels in them,\nand language processing requires the ability represents\nand process hundreds of thousands of different words.\nThis is why some of the best- known applications of deep\nlearning are in these domains, for example, Facebook\u2019s\nface- recognition software, and Google\u2019s neural machine\ntranslation system. However, there are a growing number\nof new domains where large and complex digital datasets\nare being gathered. One area where deep learning has the\npotential to make a significant impact within the coming\nyears is healthcare, and another complex domain is the\nsensor rich field of self- driving cars.\nSomewhat surprisingly, at the core of these powerful\nmodels are simple information processing units: neurons.\nThe connectionist idea that useful complex behavior can\n156 ChAPtER 4", "emerge from the interactions between large numbers of\nsimple processing units is still valid today. This emergent\nbehavior arises through the sequences of layers in a net-\nwork learning a hierarchical abstraction of increasingly\ncomplex features. This hierarchical abstraction is achieved\nby each neuron learning a simple transformation of the\ninput it receives. The network as a whole then composes\nthese sequences of smaller transformations in order to\napply a complex (highly) nonlinear mapping to the input.\nThe output from the model is then generated by the final\noutput layers of neuron, based the learned representa-\ntion generated through the hierarchical abstraction. This\nis why depth is such an important factor in neural net-\nworks: the deeper the network, the more powerful the\nmodel becomes in terms of its ability to learn complex\nnonlinear mappings. In many domains, the relationship\nbetween input data and desired outputs involves just such\ncomplex nonlinear mappings, and it is in these domains\nthat deep learning models outdo other machine learning\napproaches.\nAn important design choice in creating a neural net-\nwork is deciding which activation function to use within\nthe neurons in a network. The activation function within\neach neuron in a network is how nonlinearity is intro-\nduced into the network, and as a result it is a necessary\ncomponent if the network is to learn a nonlinear mapping\nfrom inputs to output. As networks have evolved, so too\nA BRIEF hIstoRy oF dEEP LEARnIng 157", "have the activation functions used in them. New activa-\ntion functions have emerged throughout the history of\ndeep learning, often driven by the need for functions with\nbetter properties for error- gradient propagation: a major\nfactor in the shift from threshold to logistic and tanh acti-\nvation functions was the need for differentiable functions\nin order to apply backpropagation; the more recent shift\nto ReLUs was, similarly, driven by the need to improve the\nflow of error gradients through the network. Research on\nactivations functions is ongoing, and new functions will\nbe developed and adopted in the coming years.\nAnother important design choice in creating a neural\nnetwork is to decide on the structure of the network: for\nexample, how should the neurons in the network be con-\nnected together? In the next chapter, we will discuss two\nvery different answers to this question: convolution neu-\nral networks and recurrent neural networks.\n158 ChAPtER 4", "5\nCONVOLUTIONAL AND RECURRENT\nNEURAL NETWORKS\nTailoring the structure of a network to the specific char-\nacteristics of the data from a task domain can reduce the\ntraining time of the network, and improves the accuracy\nof the network. Tailoring can be done in a number of ways,\nsuch as: constraining the connections between neurons\nin adjacent layers to subsets (rather than having fully\nconnected layers); forcing neurons to share weights; or\nintroducing backward connections into the network. Tai-\nloring in these ways can be understood as building domain\nknowledge into the network. Another, related, perspec-\ntive is it helps the network to learn by constraining the\nset of possible functions that it can learn, and by so do-\ning guides the network to find a useful solution. It is not\nalways clear how to fit a network structure to a domain,\nbut for some domains where the data has a very regular\nstructure (e.g., sequential data such as text, or gridlike", "data such as images) there are well-k nown network ar-\nchitectures that have proved successful. This chapter will\nintroduce two of the most popular deep learning architec-\ntures: convolutional neural networks and recurrent neural\nnetworks.\nConvolutional Neural Networks\nConvolution neural networks (CNNs) were designed for\nimage recognition tasks and were originally applied to the\nchallenge of handwritten digit recognition (Fukushima\n1980; LeCun 1989). The basic design goal of CNNs was to\ncreate a network where the neurons in the early layer of\nthe network would extract local visual features, and neu-\nrons in later layers would combine these features to form\nhigher- order features. A local visual feature is a feature\nwhose extent is limited to a small patch, a set of neighbor-\ning pixels, in an image. For example, when applied to the\ntask of face recognition, the neurons in the early layers of a\nCNN learn to activate in response to simple local features\n(such as lines at a particular angle, or segments of curves),\nneurons deeper in the network combine these low-l evel\nfeatures into features that represent body parts (such as\neyes or noises), and the neurons in the final layers of the\nnetwork combine body part activations in order to be able\nto identify whole faces in an image.\n160 ChAPtER 5", "Using this approach, the fundamental task in image\nrecognition is learning the feature detection functions\nthat can robustly identify the presence, or absence, of local\nvisual features in an image. The process of learning func-\ntions is at the core of neural networks, and is achieved by\nlearning the appropriate set of weights for the connec-\ntions in the network. CNNs learn the feature detection\nfunctions for local visual features in this way. However, a\nrelated challenge is designing the architecture of the net-\nwork so that the network will identify the presence of a\nlocal visual feature in an image irrespective of where in\nthe image it occurs. In other words, the feature detection\nfunctions must be able to work in a translation invariant\nmanner. For example, a face recognition system should be\nable to recognize the shape of an eye in an image whether\nthe eye is in the center of the image or in the top-r ight\ncorner of the image. This need for translation invariance\nhas been a primary design principle of CNNs for image\nprocessing, as Yann LeCun stated in 1989:\nIt seems useful to have a set of feature detectors that\ncan detect a particular instance of a feature anywhere\non the input plane. Since the precise location of a\nfeature is not relevant to the classification, we can\nafford to lose some position information in the\nprocess. (LeCun 1989, p. 14)\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 161", "CNNs achieve this translation invariance of local vi-\nsual feature detection by using weight sharing between\nneurons. In an image recognition setting, the function\nimplemented by a neuron can be understood as a visual\nfeature detector. For example, neurons in the first hidden\nlayer of the network will receive a set of pixel values as\ninput and output a high activation if a particular pattern\n(local visual feature) is present in this set of pixels. The\nfact that the function implemented by a neuron is defined\nby the weights the neuron uses means that if two neurons\nuse the same set of weights then they both implement the\nsame function (feature detector). In chapter 4, we intro-\nduced the concept of a receptive field to describe the area\nthat a neuron receives its input from. If two neurons share\nthe same weights but have different receptive fields (i.e.,\neach neuron inspects different areas of the input), then\ntogether the neurons act as a feature detector that acti-\nvates if the feature occurs in either of the receptive fields.\nConsequently, it is possible to design a network with\ntranslation invariant feature detection by creating a set of\nneurons that share the same weights and that are orga-\nnized so that: (1) each neuron inspects a different portion\nof the image; and (2) together the receptive fields of the\nneurons cover the entire image.\nThe scenario of searching an image in a dark room with\na flashlight that has a narrow beam is sometimes used to\nexplain how a CNN searches an image for local features.\n162 ChAPtER 5", "At each moment you can point the flashlight at a region of\nthe image and inspect that local region. In this flashlight\nmetaphor, the area of the image illuminated by the flash-\nlight at any moment is equivalent to the receptive field of a\nsingle neuron, and so pointing the flashlight at a location\nis equivalent to applying the feature detection function to\nthat local region. If, however, you want to be sure you in-\nspect the whole image, then you might decide to be more\nsystematic in how you direct the flashlight. For example,\nyou might begin by pointing the flashlight at the top- left\ncorner of the image and inspecting that region. You then\nmove the flashlight to the right, across the image, inspect-\ning each new location as it becomes visible, until you reach\nthe right side of the image. You then point the flashlight\nback to the left of the image, but just below where you\nbegan, and move across the image again. You repeat this\nprocess until you reach the bottom-r ight corner of the im-\nage. The process of sequentially searching across an im-\nage and at each location in the search applying the same\nfunction to the local (illuminated) region is the essence of\nconvolving a function across an image. Within a CNN, this\nsequential search across an image is implemented using\na set of neurons that share weights and whose union of\nreceptive fields covers the entire image.\nFigure 5.1 illustrates the different stages of processing\nthat are often found in a CNN. The 6\u00d76 matrix on the left\nof the figure represents the image that is the input to the\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 163", "CNN. The 4\u00d74 matrix immediately to the right of the in-\nput represents a layer of neurons that together search the\nentire image for the presence of a particular local feature.\nEach neuron in this layer is connected to a different 3\u00d73\nreceptive field (area) in the image, and they all apply the\nsame weight matrix to their inputs:\n\uf8eew w w \uf8f9\n0 1 2\n\uf8ef \uf8fa\nw w w\n\uf8ef 3 4 5\uf8fa\n\uf8f0\uf8efw w w 8\uf8fb\uf8fa\n6 7\nThe receptive field of the neuron [0,0] (top- left) in\nthis layer is marked with the gray square covering the 3\u00d73\narea in the top- left of the input image. The dotted arrows\nemerging from each of the locations in this gray area rep-\nresent the inputs to neuron [0,0]. The receptive field of\nthe neighboring neuron [0,1] is indicated by 3\u00d73 square,\noutlined in bold in the input image. Notice that the recep-\ntive fields of these two neurons overlap. The amount of\noverlap of receptive fields is controlled by a hyperparam-\neter called the stride length. In this instance, the stride\nlength is one, meaning that for each position moved in\nthe layer the receptive field of the neuron is translated by\nthe same amount on the input. If the stride length hyper-\nparameter is increased, the amount of overlap between\nreceptive fields is decreased.\n164 ChAPtER 5", "The receptive fields of both of these neurons ([0,0] and\n[0,1]) are matrices of pixel values and the weights used by\nthese neurons are also matrices. In computer vision, the\nmatrix of weights applied to an input is known as the ker-\nnel (or convolution mask); the operation of sequentially\npassing a kernel across an image and within each local\nregion, weighting each input and adding the result to its\nlocal neighbors, is known as a convolution. Notice that a\nconvolution operation does not include a nonlinear activa-\ntion function (this is applied at a later stage in processing).\nThe kernel defines the feature detection function that all\nthe neurons in the convolution implement. Convolving\na kernel across an image is equivalent to passing a local\nvisual feature detector across the image and recording all\nthe locations in the image where the visual feature was\npresent. The output from this process is a map of all the\nlocations in the image where the relevant visual feature oc-\ncurred. For this reason, the output of a convolution process\nis sometimes known as a feature map. As noted above, the\nconvolution operation does not include a nonlinear activa-\ntion function (it only involves a weighted summation of\nthe inputs). Consequently, it is standard to apply a nonlin-\nearity operation to a feature map. Frequently, this is done\nby applying a rectified linear function to each position in a\nfeature map; the rectified linear activation function is de-\nfined as: rectifier(z)=max(0,z). Passing a rectified linear\nactivation function over a feature map simply changes all\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 165", "negative values to 0. In figure 5.1, the process of updat-\ning a feature map by applying a rectified linear activation\nfunction to each of its elements is represented by the layer\nlabeled Nonlinearity.\nThe quote from Yann LeCun, at the start of this sec-\ntion, mentions that the precise location of a feature in an\nimage may not be relevant to an image processing task.\nWith this in mind, CNNs often discard location informa-\ntion in favor of generalizing the network\u2019s ability to do\nimage classification. Typically, this is achieved by down-\nsampling the updated feature map using a pooling layer.\nIn some ways pooling is similar to the convolution opera-\ntion described above, in so far as pooling involves repeat-\nedly applying the same function across an input space. For\npooling, the input space is frequently a feature map whose\nelements have been updated using a rectified linear func-\ntion. Furthermore, each pooling operation has a receptive\nfield on the input space\u2014a lthough, for pooling, the recep-\ntive fields sometimes do not overlap. There are a number\nof different pooling functions used; the most common is\ncalled max pooling, which returns the maximum value of\nany of its inputs. Calculating the average value of the in-\nputs is also used as a pooling function.\nThe operation sequence of applying a convolution,\nfollowed by a nonlinearity, to the feature map, and then\ndown- sampling using pooling, is relatively standard across\nmost CNNs. Often these three operations are together\n166 ChAPtER 5", "Convolving a kernel\nacross an image is\nequivalent to passing\na local visual feature\ndetector across the\nimage and recording all\nthe locations in the\nimage where the visual\nfeature was present.", "considered to define a convolutional layer in a network,\nand this is how they are presented in figure 5.1.\nThe fact that a convolution searches an entire image\nmeans that if the visual feature (pixel pattern) that the\nfunction (defined by shared kernel) detects occurs any-\nwhere in the image, its presence will be recorded in the\nfeature map (and if pooling is used, also in the subsequent\noutput from the pooling layer). In this way, a CNN sup-\nports translation invariant visual feature detection. How-\never, this has the limitation that the convolution can only\nidentify a single type of feature. CNNs generalize beyond\none feature by training multiple convolutional layers\nin parallel (or filters), with each filter learning a single\nConvolution:\nlayerofneurons\nInput Layerof\nwithsharedweights\nimage nonlinearity\nfunctions\nFeature\nmap\nPooling\nlayer Dense\nlayer\nConvolutionallayer\nFigure 5.1 Illustrations of the different stages of processing in a\nconvolutional layer. Note in this figure the Image and Feature Map are data\nstructures; the other stages represent operations on data.\n168 ChAPtER 5", "kernel matrix (feature detection function). Note the con-\nvolution layer in figure 5.1 illustrates a single filter. The\noutputs of multiple filters can be integrated in a variety\nof ways. One way to integrate information from differ-\nent filters is to take the feature maps generated by the\nseparate filters and combine them into a single multifil-\nter feature map. A subsequent convolutional layer then\ntakes this multifilter feature map as input. Another other\nway to integrate information from different filter is to\nuse a densely connected layer of neurons. The final layer\nin figure 5.1 illustrates a dense layer. This dense layer\noperates in exactly the same way as a standard layer in\na fully connected feedforward network. Each neuron in\nthe dense layer is connected to all of the elements out-\nput by each of the filters, and each neuron learns a set\nof weights unique to itself that it applies to the inputs.\nThis means that each neuron in a dense layer can learn\na different way to integrate information from across the\ndifferent filters.\nThe AlexNet CNN, which won the ImageNet Large-\nScale Visual Recognition Challenge (ILSVRC) in 2012,\nhad five convolutional layers, followed by three dense lay-\ners. The first convolutional layer had ninety- six different\nkernels (or filters) and included a ReLU nonlinearity and\npooling. The second convolution layer had 256 kernels and\nalso included ReLU nonlinearity and pooling. The third,\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 169", "fourth, and fifth convolutional layers did not include a\nnonlinearity step or pooling, and had 384, 384, and 256\nkernels, respectively. Following the fifth convolutional\nlayer, the network had three dense layers with 4096 neu-\nrons each. In total, AlexNet had sixty million weights and\n650,000 neurons. Although sixty million weights is a large\nnumber, the fact that many of the neurons shared weights\nactually reduced the number of weights in the network.\nThis reduction in the number of required weights is one\nof the advantages of CNN networks. In 2015, Microsoft\nResearch developed a CNN network called ResNet, which\nwon the ILSVRC 2015 challenge (He et al. 2016). The\nResNet architecture extended the standard CNN architec-\nture using skip- connections. A skip- connection takes the\noutput from one layer in the network and feeds it directly\ninto a layer that may be much deeper in the network. Us-\ning skip- connections it is possible to train very deep net-\nworks. In fact, the ResNet model developed by Microsoft\nResearch had a depth of 152 layers.\nRecurrent Neural Networks\nRecurrent neural networks (RNNs) are tailored to the\nprocessing of sequential data. An RNN processes a se-\nquence of data by processing each element in the sequence\none at time. An RNN network only has a single hidden\n170 ChAPtER 5", "layer, but it also has a memory buffer that stores the out-\nput of this hidden layer for one input and feeds it back\ninto the hidden layer along with the next input from the\nsequence. This recurrent flow of information means that\nthe network processes each input within the context gen-\nerated by processing the previous input, which in turn was\nprocessed in the context of the input preceding it. In this\nway, the information that flows through the recurrent\nloop encodes contextual information from (potentially)\nall of the preceding inputs in the sequence. This allows\nthe network to maintain a memory of what it has seen\npreviously in the sequence to help it decide what to do\nwith the current input. The depth of an RNN arises from\nthe fact that the memory vector is propagated forward\nand evolved through each input in the sequence; as a re-\nsult an RNN network is considered as deep as a sequence\nis long.\nFigure 5.2 illustrates the architecture of an RNN and\nshows how information flows through the network as\nit processes a sequence. At each time step, the network\nin this figure receives a vector containing two elements\nas input. The schematic on the left of figure 5.2 (time\nstep=1.0) shows the flow of information in the network\nwhen it receives the first input in the sequence. This input\nvector is fed forward into the three neurons in the hid-\nden layer of the network. At the same time these neurons\nalso receive whatever information is stored in the memory\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 171", "The depth of an RNN\narises from the fact that\nthe memory vector is\npropagated forward and\nevolved through each\ninput in the sequence;\nas a result an RNN\nnetwork is considered\nas deep as a sequence\nis long.", "buffer. Because this is the initial input, the memory buf-\nfer will only contain default initialization values. Each of\nthe neurons in the hidden layer will process the input and\ngenerate an activation. The schematic in the middle of fig-\nure 5.2 (time step=1.5) shows how this activation flows\non through the network: the activation of each neuron is\npassed to the output layer where it is processed to gener-\nate the output of the network, and it is also stored in the\nmemory buffer (overwriting whatever information was\nstored there). The elements of the memory buffer simply\nstore the information written to them; they do not trans-\nform it in any way. As a result, there are no weights on\nthe edges going from the hidden units to the buffer. There\nare, however, weights on all the other edges in the net-\nwork, including those from the memory buffer units to\nthe neurons in the hidden layer. At time step 2, the net-\nwork receives the next input from the sequence, and this\nis passed to the hidden layer neurons along with the infor-\nmation stored in the buffer. This time the buffer contains\nthe activations that were generated by the hidden neurons\nin response to the first input.\nFigure 5.3 shows an RNN that has been unrolled\nthrough time as it processes a sequence of inputs\n[X ,X ,\u2026,X ]. Each box in this figure represents a layer\n1 2 t\nof neurons. The box labeled h represents the state of\n0\nthe memory buffer when the network is initialized; the\nboxes labeled [h ,\u2026,h ] represent the hidden layer of the\n1 t\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 173", "evitca\n.emit\neht\ntaht\nera\ndlob ta\nevitca\nni\nsworra ton\nera\nehT\ntaht\n.stupni\nsnoitcennoc\nfo\necneuqes\nwohs\na sworra\nsessecorp\ndehsad\nti\nsa\neht\nNNR\n;tniop\nna\nni emit\nnoitamrofni\nhcae\nta\nwolf\nfo\nwolf noitamrofni\nehT\n2.5\nfo\nerugiF shtap\n174 ChAPtER 5", "Output: Y Y Y Y\n1 2 3 t\nh h h h h\n0 1 2 3 \u00b7\u00b7\u00b7 t\nInput: X1 X2 X3 Xt\nFigure 5.3 An RNN network unrolled through time as it processes a\nsequence of inputs [X ,X ,\u2026,X ].\n1 2 t\nnetwork at each time step; and the boxes labeled [Y ,\u2026,Y ]\n1 t\nrepresent the output layer of the network at each time\nstep. Each of the arrows in the figure represents a set of\nconnections between one layer and another layer. For ex-\nample, the vertical arrow from X to h represents the con-\n1 1\nnections between the input layer and the hidden layer at\ntime step 1. Similarly, the horizontal arrows connecting\nthe hidden layers represent the storing of the activations\nfrom a hidden state at one time step in the memory buffer\n(not shown) and the propagation of these activations to\nthe hidden layer at the next time step through the connec-\ntions from the memory buffer to the hidden state. At each\ntime step, an input from the sequence is presented to the\nnetwork and is fed forward to the hidden layer. The hid-\nden layer generates a vector of activations that is passed\nto the output layer and is also propagated forward to the\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 175", "next time step along the horizontal arrows connecting the\nhidden states.\nAlthough RNNs can process a sequence of inputs,\nthey struggle with the problem of vanishing gradients.\nThis is because training an RNN to process a sequence of\ninputs requires the error to be backpropagated through\nthe entire length of the sequence. For example, for the\nnetwork in figure 5.3, the error calculated on the output\nY must be backpropagated through the entire network\nt\nso that it can be used to update the weights on the con-\nnections from h and X to h . This entails backpropagat-\n0 1 1\ning the error through all the hidden layers, which in turn\ninvolves repeatedly multiplying the error by the weights\non the connections feeding activations from one hidden\nlayer forward to the next hidden layer. A particular prob-\nlem with this process is that it is the same set of weights\nthat are used on all the connections between the hidden\nlayers: each horizontal arrow represents the same set of\nconnections between the memory buffer and the hidden\nlayer, and the weights on these connections are stationary\nthrough time (i.e., they don\u2019t change from one time step\nto the next during the processing of a given sequence of\ninputs). Consequently, backpropogating an error through\nk time steps involves (among other multiplications) mul-\ntiplying the error gradient by the same set of weights k\ntimes. This is equivalent to multiplying each error gradi-\nent by a weight raised to the power of k. If this weight is\n176 ChAPtER 5", "less than 1, then when it is raised to a power, it diminishes\nat an exponential rate, and consequently, the error gra-\ndient also tends to diminish at an exponential rate with\nrespect to the length of the sequence\u2014 and vanish.\nLong short- term memory networks (LSTMs) are de-\nsigned to reduce the effect of vanishing gradients by re-\nmoving the repeated multiplication by the same weight\nvector during backpropagation in an RNN. At the core of\nan LSTM1 unit is a component called the cell. The cell is\nwhere the activation (the short- term memory) is stored\nand propagated forward. In fact, the cell often maintains\na vector of activations. The propagation of the activations\nwithin the cell through time is controlled by three compo-\nnents called gates: the forget gate, the input gate, and the\noutput gate. The forget gate is responsible for determining\nwhich activations in the cell should be forgotten at each\ntime step, the input gate controls how the activations in\nthe cell should be updated in response to the new input,\nand the output gate controls what activations should be\nused to generate the output in response to the current\ninput. Each of the gates consists of layers of standard neu-\nrons, with one neuron in the layer per activation in the\ncell state.\nFigure 5.4 illustrates the internal structure of an\nLSTM cell. Each of the arrows in this image represents a\nvector of activations. The cell runs along the top of the\nfigure from left (c ) to right (c ). Activations in the cell\nt\u22121 t\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 177", "can take values in the range -1 to +1. Stepping through the\nprocessing for a single input, the input vector x is first\nt\nconcatenated with the hidden state vector that has been\npropagated forward from the preceding time step h .\nt\u22121\nWorking from left to right through the processing of the\ngates, the forget gate takes the concatenation of the input\nand the hidden state and passes this vector through a layer\nof neurons that use a sigmoid (also known as logistic)2 ac-\ntivation function. As a result of the neurons in the forget\nlayer using sigmoid activation functions the output of this\nforget layer is a vector of values in the range 0 to 1. The cell\nstate is then multiplied by this forget vector. The result\nof this multiplication is that activations in the cell state\nthat are multiplied by components in the forget vector\nwith values near 0 are forgotten, and activations that are\nmultiplied by forget vector components with values near 1\nare remembered. In effect, multiplying the cell state by the\noutput of a sigmoid layer acts as a filter on the cell state.\nNext, the input gate decides what information should\nbe added to the cell state. The processing in this step is\ndone by the components in the middle block of figure 5.4,\nmarked Input. This processing is broken down into two\nsubparts. First, the gate decides which elements in the\ncell state should be updated, and second it decides what\ninformation should be included in the update. The deci-\nsion regarding which elements in the cell state should be\nupdated is implemented using a similar filter mechanism\n178 ChAPtER 5", "Forget Input Output\nc t 1 + c t\n\u2212 \u00d7\nT\n\u00d7 \u00d7\n\u03c3 \u03c3 T \u03c3\nh t 1 h t\n\u2212\nxt output\nFigure 5.4 Schematic of the internal structure of an LSTM unit: \u03c3\nrepresents a layer of neurons with sigmoid activations, T represents a layer\nof neurons with tanh activations, \u00d7 represents vector multiplication, and +\nrepresents vector addition. The figure is inspired by an image by Christopher\nOlah available at: http://colah.github.io/posts/2015-08-Understanding\n-LSTMs/.\nto the forget gate: the concatenated input x plus hidden\nt\nstate h is passed through a layer of sigmoid units to\nt\u22121\ngenerate a vector of elements, the same width as the cell,\nwhere each element in the vector is in the range 0 to 1;\nvalues near 0 indicate that the corresponding cell element\nwill not be updated, and values near 1 indicate that the\ncorresponding cell element will be updated. At the same\ntime that the filter vector is generated, the concatenated\ninput and hidden state are also passed through a layer\nof tanh units (i.e., neurons that use the tanh activation\nfunction). Again, there is one tanh unit for each activation\nin the LSTM cell. This vector represents the information\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 179", "that may be added to the cell state. Tanh units are used\nto generate this update vector because tanh units out-\nput values in the range - 1 to +1, and consequently the\nvalue of the activations in the cell elements can be both\nincreased and decreased by an update.3 Once these two\nvectors have been generated, the final update vector is\ncalculated by multiplying the vector output from the\ntanh layer by the filter vector generated from the sigmoid\nlayer. The resulting vector is then added to the cell using\nvector addition.\nThe final stage of processing in an LSTM is to decide\nwhich elements of the cell should be output in response to\nthe current input. This processing is done by the compo-\nnents in the block marked Output (on the right of figure\n5.4). A candidate output vector is generated by passing\nthe cell through a tanh layer. At the same time, the con-\ncatenated input and propagated hidden state vector are\npassed through a layer of sigmoid units to create another\nfilter vector. The actual output vector is then calculated by\nmultiplying the candidate output vector by this filter vec-\ntor. The resulting vector is then passed to the output layer,\nand is also propagated forward to the next time step as the\nnew hidden state h .\nt\nThe fact that an LSTM unit contains multiple layers\nof neurons means that an LSTM is a network in itself.\nHowever, an RNN can be constructed by treating an LSTM\nas the hidden layer in the RNN. In this configuration, an\n180 ChAPtER 5", "LSTM unit receives an input at each time step and gener-\nates an output for each input. RNNs that use LSTM units\nare often known as LSTM networks.\nLSTM networks are ideally suited for natural language\nprocessing (NLP). A key challenge in using a neural net-\nwork to do natural language processing is that the words\nin language must be converted into vectors of numbers.\nThe word2vec models, created by Tomas Mikolov and col-\nleagues at Google research, are one of the most popular\nways of doing this conversion (Mikolov et al. 2013). The\nword2vec models are based on the idea that words that\nappear in similar contexts have similar meanings. The\ndefinition of context here is surrounding words. So for ex-\nample, the words London and Paris are semantically simi-\nlar because each of them often co- occur with words that\nthe other word also co- occurs with, such as: capital, city,\nEurope, holiday, airport, and so on. The word2vec models\nare neural networks that implement this idea of seman-\ntic similarity by initially assigning random vectors to each\nword and then using co- occurrences within a corpus to it-\neratively update these vectors so that semantically similar\nwords end up with similar vectors. These vectors (known\nas word embeddings) are then used to represent a word\nwhen it is being input to a neural network.\nOne of the areas of NLP where deep learning has\nhad a major impact is in machine translation. Figure\n5.5 presents a high- level schematic of the seq2seq (or\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 181", "Life is beautiful <eos>\nEncoder\nh1 h2 h3 h4 C d1 d2 d3\nDecoder\nbelle est vie La <eos>\nFigure 5.5 Schematic of the seq2seq (or encoder-d ecoder) architecture.\nencoder- decoder) architecture for neural machine transla-\ntion (Sutskever et al. 2014). This architecture is composed\nof two LSTM networks that have been joined together.\nThe first LSTM network processes the input sentence in\na word- by- word fashion. In this example, the source lan-\nguage is French. The words are entered into the system in\nreverse order as it has been found that this leads to better\ntranslations. The symbol eos is a special end of sentence\nsymbol. As each word is entered, the encoder updates the\nhidden state and propagates it forward to the next time\nstep. The hidden state generated by the encoder in re-\nsponse to the eos symbol is taken to be a vector represen-\ntation of the input sentence. This vector is passed as the\ninitial input to the decoder LSTM. The decoder is trained\nto output the translation sentence word by word, and af-\nter each word has been generated, this word is fed back\ninto the system as the input for the next time step. In a\n182 ChAPtER 5", "way, the decoder is hallucinating the translation because\nit uses its own output to drive its own generation pro-\ncess. This process continues until the decoder outputs an\neos symbol.\nThe idea of using a vector of numbers to represent the\n(interlingual) meaning of a sentence is very powerful, and\nthis concept has been extended to the idea of using vectors\nto represent intermodal/multimodal representations. For\nexample, an exciting development in recent years has been\nthe development of automatic image captioning systems.\nThese systems can take an image as input and generate a\nnatural language description of the image. The basic struc-\nture of these systems is very similar to the neural machine\ntranslation architecture shown in figure 5.5. The main\ndifference is that the encoder LSTM network is replaced\nby a CNN architecture that processes the input image and\ngenerates a vector representation that is then propagated\nto the decoder LSTM (Xu et al. 2015). This is another ex-\nample of the power of deep learning arising from its ability\nto learn complex representations of information. In this\ninstance, the system learns intermodal representations\nthat enable information to flow from what is in an im-\nage to language. Combining CNN and RNN architectures\nis becoming more and more popular because it offers the\npotential to integrate the advantages of both systems and\nenables deep learning architectures to handle very com-\nplex data.\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 183", "Irrespective of the network architecture we use, we\nneed to find the correct weights for the network if we\nwish to create an accurate model. The weights of a neu-\nron determine the transformation the neuron applies to\nits inputs. So, it is the weights of the network that define\nthe fundamental building blocks of the representation the\nnetwork learns. Today the standard method for finding\nthese weights is an algorithm that came to prominence in\nthe 1980s: backpropagation. The next chapter will present\na comprehensive introduction to this algorithm.\n184 ChAPtER 5", "6\nLEARNING FUNCTIONS\nA neural network model, no matter how deep or complex,\nimplements a function, a mapping from inputs to outputs.\nThe function implemented by a network is determined\nby the weights the network uses. So, training a network\n(learning the function the network should implement) on\ndata involves searching for the set of weights that best\nenable the network to model the patterns in the data. The\nmost commonly used algorithm for learning patterns\nfrom data is the gradient descent algorithm. The gradi-\nent descent algorithm is very like the perceptron learn-\ning rule and the LMS algorithm described in chapter 4:\nit defines a rule to update the weights used in a function\nbased on the error of the function. By itself the gradient\ndescent algorithm can be used to train a single output neu-\nron. However, it cannot be used to train a deep network\nwith multiple hidden layers. This limitation is because of", "the credit assignment problem: how should the blame for\nthe overall error of a network be shared out among the\ndifferent neurons (including the hidden neurons) in the\nnetwork? Consequently, training a deep neural network\ninvolves using both the gradient descent algorithm and\nthe backpropagation algorithm in tandem.\nThe process used to train a deep neural network can\nbe characterized as: randomly initializing the weight of a\nnetwork, and then iteratively updating the weights of the\nnetwork, in response to the errors the network makes on a\ndataset, until the network is working as expected. Within\nthis training framework, the backpropagation algorithm\nsolves the credit (or blame) assignment problem, and the\ngradient descent algorithm defines the learning rule that\nactually updates the weights in the network.\nThis chapter is the most mathematical chapter in the\nbook. However, at a high level, all you need to know about\nthe backpropagation algorithm and the gradient descent\nalgorithm is that they can be used to train deep networks.\nSo, if you don\u2019t have the time to work through the details\nin this chapter, feel free to skim through it. If, however,\nyou wish to get a deeper understanding of these two al-\ngorithms, then I encourage you to engage with the mate-\nrial. These algorithms are at the core of deep learning and\nunderstanding how they work is, possibly, the most direct\nway of understanding its potentials and limitations. I have\nattempted to present the material in this chapter in an\n186 ChAPtER 6", "accessible way, so if you are looking for a relatively gentle\nbut still comprehensive introduction to these algorithms,\nthen I believe that this will provide it for you. The chapter\nbegins by explaining the gradient descent algorithm, and\nthen explains how gradient descent can be used in con-\njunction with the backpropagation algorithm to train a\nneural network.\nGradient Descent\nA very simple type of function is a linear mapping from a\nsingle input to a single output. Table 6.1 presents a data-\nset with a single input feature and a single output. Figure\n6.1 presents a scatterplot of this data along with a plot of\nthe line that best fits this data. This line can be used as a\nfunction to map from an input value to a prediction of the\nTable 6.1. A sample dataset with one input feature, x,\nand an output (target) feature, y\nX Y\n0.72 0.54\n0.45 0.56\n0.23 0.38\n0.76 0.57\n0.14 0.17\nLEARnIng FunC tIons 187", "0.1\n8.0\ny = 0.6746\n6.0\ny\n4.0\nx = 0.9\n2.0\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nx\nFigure 6.1 Scatterplot of data with \u201cbest fit\u201d line and the errors of the line\non each example plotted as vertical dashed line segments. The figure also\nshows the mapping defined by the line for input x=0.9 to output y=0.6746.\noutput value. For example, if x = 0.9, then the response\nreturned by this linear function is y = 0.6746. The error (or\nloss) of using this line as a model for the data is shown by\nthe dashed lines from the line to each datum.\nIn chapter 2, we described how a linear function can\nbe represented using the equation of a line:\ny=mx+c\nwhere m is the slope of the line, and c is the y-i ntercept,\nwhich specifies where the line crosses the y-a xis. For the\n188 ChAPtER 6", "line in figure 6.1, c=0.203 and m=0.524; this is why the\nfunction returns the value y=0.6746 when x=0.9, as in\nthe following:\n0.6746=(0.524\u00d70.9)+0.203\nThe slope m and the y- intercept c are the parameters of\nthis model, and these parameters can be varied to fit the\nmodel to the data.\nThe equation of a line has a close relationship with the\nweighted sum operation used in a neuron. This becomes\napparent if we rewrite the equation of a line with model\nparameters rewritten as weights (c\u2192w ,m\u2192w ):\n0 1\ny=(w \u00d71)+(w \u00d7x)\n0 1\nDifferent lines (different linear models for the data) can\nbe created by varying either of these weights (or model pa-\nrameters). Figure 6.2 illustrates how a line changes as the\nintercept and slope of the line varies: the dashed line illus-\ntrates what happens if the y-i ntercept is increased, and the\ndotted line shows what happens if the slope is decreased.\nChanging the y-i ntercept w vertically translates the line,\n0\nwhereas modifying the slope w rotates the line around\n1\nthe point (x=0,y=intercept).\nEach of these new lines defines a different func-\ntion, mapping from x to y, and each function will have\nLEARnIng FunC tIons 189", "0.1\n8.0\n6.0\ny\n4.0\n2.0\n[w0= 0.203, w1= 0.524]\n[w0= 0.400, w1= 0.524]\n[w0= 0.203, w1= 0.300] 0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nx\nFigure 6.2 Plot illustrating how a line changes as the intercept (w ) and\n0\nslope (w ) are varied.\n1\na different error with respect to how well it matches the\ndata. Looking at figure 6.2, we can see that the full line,\n[w =0.203,w =0.524], fits the data better than the\n0 1\nother two lines because on average it passes closer to the\ndata points. In other words, on average the error for this\nline for each data point is less than those of the other two\nlines. The total error of a model on a dataset can be mea-\nsured by summing together the error the model makes on\neach example in the dataset. The standard way to calculate\nthis total error is to use an equation known as the sum of\nsquared errors (SSE):\n190 ChAPtER 6", "1 n\nSSE= \u2211(y \u2212y\u02c6 )2\nj j\n2\nj=1\nThis equation tells us how to add together the errors of a\nmodel on a dataset containing n examples. This equation\ncalculates for each of the n examples in the dataset the er-\nror of the model by subtracting the prediction of the target\nvalue returned by the model from the correct target value\nfor that example, as specified in the dataset. In this equa-\ntion y is the correct output value for target feature listed\nj\nin the dataset for example j, and y\u02c6 is the estimate of the\nj\ntarget value returned by the model for the same example.\nEach of these errors is then squared and these squared er-\nrors are then summed. Squaring the errors ensures that\nthey are all positive, and therefore in the summation the\nerrors for examples where the function underestimated\nthe target do not cancel out the errors on examples where\nit overestimated the target. The multiplication of the sum-\nmation of the errors by 1/2, although not important for\nthe current discussion, will become useful later. The lower\nthe SSE of a function, the better the function models the\ndata. Consequently, the sum of squared errors can be used\nas a fitness function to evaluate how well a candidate\nfunction (in this situation a model instantiating a line)\nmatches the data.\nLEARnIng FunC tIons 191", "Figure 6.3 shows how the error of a linear model var-\nies as the parameters of the model change. These plots\nshow the SSE of a linear model on the example single-\ninput\u2013 single- output dataset listed in table 6.1. For each\nparameter there is a single best setting and as the param-\neter moves away from this setting (in either direction)\nthe error of the model increases. A consequence of this\nis that the error profile of the model as each parameter\nvaries is convex (bowl- shaped). This convex shape is par-\nticularly apparent in the top and middle plots in figure 6.3,\nwhich show that the SSE of the model is minimized when\nw =0.203 (lowest point of the curve in the top plot),\n0\nand when w =0.524 (lowest point of the curve in the\n1\nmiddle plot).\nIf we plot the error of the model as both parame-\nters are varied, we generate a three-d imensional convex\nbowl- shaped surface, known as an error surface. The\nbowl- shaped mesh in the plot at the bottom of figure 6.3\nillustrates this error surface. This error surface was cre-\nated by first defining a weight space. This weight space is\nrepresented by the flat grid at the bottom of the plot. Each\ncoordinate in this weight space defines a different line be-\ncause each coordinate specifies an intercept (a w value)\n0\nand slope (a w value). Consequently, moving across this\n1\nplanar weight space is equivalent to moving between dif-\nferent models. The second step in constructing the error\nsurface is to associate an elevation with each line (i.e.,\n192 ChAPtER 6", "0.1\n8.0\n6.0\nESS\n4.0\n2.0\n0.0\n0.2 0.0 0.2 0.4 0.6\nw0 (y intercept)\n0.1\n8.0\n6.0\nESS\n4.0\n2.0\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nw1 (slope)\nESS\nw\n0\nw\n1\nFigure 6.3 Plots of the changes in the error (SSE) of a linear model as the\nparameters of the model change. Top: the SSE profile of a linear model with a\nfixed slope w =0.524 when w ranges across the interval 0.3 to 1. Middle:\n1 0\nthe SSE profile of a linear model with a y-i ntercept fixed at w =0.203 when\n0\nw ranges across the interval 0 to 1. Bottom: the error surface of the linear\n1\nmodel when both w and w are varied.\n0 1", "coordinate) in the weight space. The elevation associated\nwith each weight space coordinate is the SSE of the model\ndefined by that coordinate; or, put more directly, the\nheight of the error surface above the weight space plane\nis the SSE of the corresponding linear model when it is\nused as a model for the dataset. The weight space coordi-\nnates that correspond with the lowest point of the error\nsurface define the linear model that has the lowest SSE on\nthe dataset (i.e., the linear model that best fits the data).\nThe shape of the error surface in the plot on the right\nof figure 6.3 indicates that there is only a single best linear\nmodel for this dataset because there is a single point at\nthe bottom of the bowl that has a lower elevation (lower\nerror) than any other points on the surface. Moving away\nfrom this best model (by varying the weights of the model)\nnecessarily involves moving to a model with a higher SSE.\nSuch a move is equivalent to moving to a new coordinate\nin the weight space, which has a higher elevation associ-\nated with it on the error surface. A convex or bowl- shaped\nerror surface is incredibly useful for learning a linear func-\ntion to model a dataset because it means that the learning\nprocess can be framed as a search for the lowest point on\nthe error surface. The standard algorithm used to find this\nlowest point is known as gradient descent.\nThe gradient descent algorithm begins by creat-\ning an initial model using a randomly selected a set of\nweights. Next the SSE of this randomly initialized model\n194 ChAPtER 6", "A convex or bowl-\nshaped error surface\nis incredibly useful\nfor learning a linear\nfunction to model a\ndataset because it\nmeans that the learning\nprocess can be framed\nas a search for the\nlowest point on the\nerror surface.", "is calculated. Taken together, the guessed set of weights\nand the SSE of the corresponding model define the ini-\ntial starting point on the error surface for the search. It\nis very likely that the randomly initialized model will be\na bad model, so it is very likely that the search will begin\nat a location that has a high elevation on the error surface.\nThis bad start, however, is not a problem, because once\nthe search process is positioned on the error surface, the\nprocess can find a better set of weights by simply following\nthe gradient of the error surface downhill until it reaches\nthe bottom of the error surface (the location where mov-\ning in any direction results in an increase in SSE). This is\nwhy the algorithm is known as gradient descent: the gradi-\nent that the algorithm descends is the gradient of the error\nsurface of the model with respect to the data.\nAn important point is that the search does not pro-\ngress from the starting location to the valley floor in one\nweight update. Instead, it moves toward the bottom of the\nerror surface in an iterative manner, and during each itera-\ntion the current set of weights are updated so as to move\nto a nearby location in the weight space that has a lower\nSSE. Reaching the bottom of the error surface can take\na large number of iterations. An intuitive way of under-\nstanding the process is to imagine a hiker who is caught\non the side of a hill when a thick fog descends. Their car\nis parked at the bottom of the valley; however, due to the\nfog they can only see a few feet in any direction. Assuming\n196 ChAPtER 6", "that the valley has a nice convex shape to it, they can still\nfind their way to their car, despite the fog, by repeatedly\ntaking small steps that move down the hill following the\nlocal gradient at the position they are currently located.\nA single run of a gradient descent search is illustrated in\nthe bottom plot of figure 6.3. The black curve plotted on\nthe error surface illustrates the path the search followed\ndown the surface, and the black line on the weight space\nplots the corresponding weight updates that occurred dur-\ning the journey down the error surface. Technically, the\ngradient descent algorithm is known as an optimization\nalgorithm because the goal of the algorithm is to find the\noptimal set of weights.\nThe most important component of the gradient de-\nscent algorithm is the rule that defines how the weights are\nupdated during each iteration of the algorithm. In order to\nunderstand how this rule is defined it is first necessary to\nunderstand that the error surface is made up of multiple\nerror gradients. For our simple example, the error surface\nis created by combining two error curves. One error curve\nis defined by the changes in the SSE as w changes, shown\n0\nin the top plot of figure 6.3. The other error curve is de-\nfined by the changes in the SSE as w changes, shown in\n1\nthe plot in the middle of figure 6.3. Notice that the gradi-\nent of each of these curves can vary along the curve, for\nexample, the w error curve has a steep gradient on the ex-\n0\ntreme left and right of the plot, but the gradient becomes\nLEARnIng FunC tIons 197", "somewhat shallower in the middle of the curve. Also, the\ngradients of two different curves can vary dramatically; in\nthis particular example the w error curve generally has a\n0\nmuch steeper gradient than the w error curve.\n1\nThe fact that the error surface is composed of mul-\ntiple curves, each with a different gradient, is important\nbecause the gradient descent algorithm moves down the\ncombined error surface by independently updating each\nweight so as to move down the error curve associated\nwith that weight. In other words, during a single itera-\ntion of the gradient descent algorithm, w is updated to\n0\nmove down the w error curve and w is updated the move\n0 1\ndown the w error curve. Furthermore, the amount each\n1\nweight is updated in an iteration is proportional to the\nsteepness of the gradient of the weight\u2019s error curve, and\nthis gradient will vary from one iteration to the next as\nthe process moves down the error curve. For example, w\n0\nwill be updated by relatively large amounts in iterations\nwhere the search process is located high up on either side\nof the w error curve, but by smaller amounts in iterations\n0\nwhere the search process is nearer to the bottom of the w\n0\nerror curve.\nThe error curve associated with each weight is defined\nby how the SSE changes with respect to the change in the\nvalue of the weight. Calculus, and in particular differentia-\ntion, is the field of mathematics that deals with rates of\nchange. For example, taking the derivative of a function,\n198 ChAPtER 6", "y=f(x), calculates the rate of change of y (the output)\nfor each unit change in x (the input). Furthermore, if a\nfunction takes multiple inputs [y=f(x ,\u2026,x )] then it\n1 n\nis possible to calculate the rate of change of the output,\ny, with respect to changes in each of these inputs, x , by\ni\ntaking the partial derivative of the function of with re-\nspect to each input. The partial derivative of a function\nwith respect to a particular input is calculated by first as-\nsuming that all the other inputs are held constant (and so\ntheir rate of change is 0 and they disappear from the cal-\nculation) and then taking the derivative of what remains.\nFinally, the rate of change of a function for a given input is\nalso known as the gradient of the function at the location\non the curve (defined by the function) that is specified by\nthe input. Consequently, the partial derivative of the SSE\nwith respect to a weight specifies how the output of the\nSSE changes as that weight changes, and so it specifies\nthe gradient of the error curve of the weight. This is ex-\nactly what is needed to define the gradient descent weight\nupdate rule: the partial derivative of the SSE with respect\nto a weight specifies how to calculate the gradient of the\nweight\u2019s error curve, and in turn this gradient specifies\nhow the weight should be updated to reduce the error (the\noutput of the SSE).\nThe partial derivative of a function with respect to a\nparticular variable is the derivative of the function when\nLEARnIng FunC tIons 199", "all the other variables are held constant. As a result there\nis a different partial derivative of a function with respect\nto each variable, because a different set of terms are con-\nsidered constant in the calculation of each of the partial\nderivatives. Therefore, there is a different partial deriva-\ntive of the SSE for each weight, although they all have a\nsimilar form. This is why each of the weights is updated in-\ndependently in the gradient descent algorithm: the weight\nupdate rule is dependent on the partial derivative of the\nSSE for each weight, and because there is a different par-\ntial derivative for each weight, there is a separate weight\nupdate rule for each weight. Again, although the partial\nderivative for each weight is distinct, all of these deriva-\ntives have the same form, and so the weight update rule\nfor each weight will also have the same form. This simpli-\nfies the definition of the gradient descent algorithm. An-\nother simplifying factor is that the SSE is defined relative\nto a dataset with n examples. The relevance of this is that\nthe only variables in the SSE are the weights; the target\noutput y and the inputs x are all specified by the dataset\nfor each example, and so can be considered constants. As\na result, when calculating the partial derivative of the SSE\nwith respect to a weight, many of the terms in the equa-\ntion that do not include the weight can be deleted because\nthey are considered constants.\nThe relationship between the output of the SSE and\neach weight becomes more explicit if the SSE definition\n200 ChAPtER 6", "is rewritten so that the term y\u02c6 , denoting the output pre-\nj\ndicted by the model, is replaced by the structure of the\nmodel generating the prediction. For the model with a\nsingle input x and a dummy input, x =1,this rewritten\n1 0\nversion of the SSE is:\n1 n\nSSE= \u2211(y \u2212(w \u00d7x +w \u00d7x ))2\nj 0 j,0 1 j,1\n2\nj=1\nThis equation uses a double subscript on the inputs, the\nfirst subscript j identifies the example (or row in the\ndataset) and the second subscript specifies the feature (or\ncolumn in the dataset) of the input. For example, x rep-\nj,1\nresents feature 1 from example j. This definition of the\nSSE can be generalized to a model with m inputs:\n1 n \uf8eb \uf8eb m \uf8f6\uf8f62\nSSE= 2\u2211 \uf8ed\uf8ecy j\u2212 \uf8ed\uf8ec\u2211w i\u00d7x j,i\uf8f8\uf8f7 \uf8f8\uf8f7\nj=1 i=0\nCalculating the partial derivative of the SSE with re-\nspect to a specific weight involves the application of the\nchain rule from calculus and a number of standard dif-\nferentiation rules. The result of this derivation is the fol-\nlowing equation (for simplicity of presentation we switch\nback to the notation y\u02c6 to represent the output from the\ni\nmodel):\nLEARnIng FunC tIons 201", "\uf8eb \uf8f6\n\uf8ec \uf8f7\n\uf8ec \uf8f7\n\u2202SSE n \uf8ec \uf8f7\n=\u2211\uf8ec(y \u2212y\u02c6 )\u00d7 \u2212x \uf8f7\nj=1\uf8ec(cid:31)(cid:29)j(cid:30)(cid:29)(cid:28)j (cid:27)j,ii\n\u2202 w i \uf8f7\nerrorofthe rateofchangeof\n\uf8ec\uf8ec \uf8f7\noutputofthe weightedsum\n\uf8ec \uf8f7\n\uf8ecweightedsum withrespectto \uf8f7\n\uf8ed changeinwi \uf8f8\nThis partial derivative specifies how to calculate the\nerror gradient for weight w for the dataset where x is the\ni j,i\ninput associated with w for each example in the dataset.\ni\nThis calculation involves multiplying two terms, the error\nof the output and the rate of change of the output (i.e., the\nweighted sum) with respect to changes in the weight. One\nway of understanding this calculation is that if changing\nthe weight changes the output of the weighted sum by a\nlarge amount, then the gradient of the error with respect\nto the weight is large (steep) because changing the weight\nwill result in big changes in the error. However, this gradi-\nent is the uphill gradient, and we wish to move the weights\nso as to move down the error curve. So in the gradient\ndescent weight update rule (shown below) the \u201c\u2013 \u201d sign in\nfront of the input x is dropped. Using t to represent the\nj,i\niteration of the algorithm (an iteration involves a single\npass through the n examples in the dataset), the gradient\ndescent weight update rule is defined as:\n202 ChAPtER 6", "\uf8eb \uf8f6\n\uf8ec n \uf8f7\nwt+1 =wt +\uf8ec\u03b7\u00d7\u2211((yt \u2212y\u02c6t)\u00d7xt )\uf8f7\ni i j j j,i\n\uf8ec \uf8f7\n(cid:31)j=1(cid:29)(cid:29)(cid:29)(cid:29)(cid:30)(cid:29)(cid:29)(cid:29)(cid:28)\uf8f7\n\uf8ec\n\uf8ed error gradient for wi \uf8f8\nThere are a number of notable factors about this\nweight update rule. First, the rule specifies how the weight\nw should be updated after iteration t through the dataset.\ni\nThis update is proportional to the gradient of the error\ncurve for the weight for that iteration (i.e., the summa-\ntion term, which actually defines the partial derivative\nof the SSE for that weight). Second, the weight update\nrule can be used to update the weights for functions with\nmultiple inputs. This means that the gradient descent al-\ngorithm can be used to descend error surfaces with more\nthan two weight coordinates. It is not possible to visual-\nize these error surfaces because they will have more than\nthree dimensions, but the basic principles of descending\nan error surface using the error gradient generalizes to\nlearning functions with multiple inputs. Third, although\nthe weight update rule has a similar structure for each\nweight, the rule does define a different update for each\nweight during each iteration because the update is de-\npendent on the inputs in the dataset examples to which\nthe weight is applied. Fourth, the summation in the rule\nLEARnIng FunC tIons 203", "indicates that, in each iteration of the gradient descent al-\ngorithm, the current model should be applied to all n of\nthe examples in the dataset. This is one of the reasons why\ntraining a deep learning network is such a computation-\nally expensive task. Typically for very large datasets, the\ndataset is split up into batches of examples sampled from\nthe dataset, and each iteration of training is based on a\nbatch, rather than the entire dataset. Fifth, apart from the\nmodifications necessary to include the summation, this\nrule is identical to the LMS (also known as the Widrow-\nHoff or delta) learning rule introduced in chapter 4, and\nthe rule implements the same logic: if the output of the\nmodel is too large, then weights associated with positive\ninputs should be reduced; if the output is too small, then\nthese weights should be increased. Moreover, the purpose\nand function of the learning rate hyperparameter (\u03b7) is\nthe same as in the LMS rule: scale the weight adjustments\nto ensure that the adjustments aren\u2019t so large that the\nalgorithm misses (or steps over) the best set of weights.\nUsing this weight update rule, the gradient descent algo-\nrithm can be summarized as follows:\n1. Construct a model using an initial set of weights.\n2. Repeat until the model performance is good enough.\na. Apply the current model to the examples in the\ndataset.\n204 ChAPtER 6", "b. Adjust each weight using the weight update\nrule.\n3. Return the final model.\nOne consequence of the independent updating of\nweights, and the fact that weight updates are proportional\nto the local gradient on the associated error curve, is that\nthe path the gradient descent algorithm follows to the\nlowest point on the error surface may not be a straight\nline. This is because the gradient of each of the component\nerror curves may not be equal at each location on the error\nsurface (the gradient for one of the weights may be steeper\nthan the gradient for the other weight). As a result, one\nweight may be updated by a larger amount than another\nweight during a given iteration, and thus the descent to\nthe valley floor may not follow a direct route. Figure 6.4\nillustrates this phenomenon. Figure 6.4 presents a set of\ntop- down views of a portion of a contour plot of an error\nsurface. This error surface is a valley that is quite long and\nnarrow with steeper sides and gentler sloping ends; the\nsteepness is reflected by the closeness of the contours. As\na result, the search initially moves across the valley before\nturning toward the center of the valley. The plot on the\nleft illustrates the first iteration of the gradient descent\nalgorithm. The initial starting point is the location where\nthe three arrows, in this plot, meet. The lengths of the\nLEARnIng FunC tIons 205", "Figure 6.4 Top-d own views of a portion of a contour plot of an error surface,\nillustrating the gradient descent path across the error surface. Each of the\nthick arrows illustrates the overall movement of the weight vector for a\nsingle iteration of the gradient descent algorithm. The length of dotted and\ndashed arrows represent the local gradient of the w and w error curves,\n0 1\nrespectively, for that iteration. The plot on the right shows the overall path\ntaken to the global minimum of the error surface.\ndotted and dashed arrows represent the local gradients\nof the w and w error curves, respectively. The dashed\n0 1\narrow is longer than the dotted arrow reflecting the fact\nthat the local gradient of the w error curve is steeper than\n0\nthat of the w error curve. In each iteration, each of the\n1\nweights is updated in proportion to the gradient of their\nerror curve; so in the first iteration, the update for w is\n0\nlarger than for w and therefore the overall movement is\n1\ngreater across the valley than along the valley. The thick\nblack arrow illustrates the overall movement in the un-\nderlying weight space, resulting from the weight updates\nin this first iteration. Similarly, the middle plot illustrates\nthe error gradients and overall weight update for the next\niteration of gradient descent. The plot on the right shows\nthe complete path of descent taken by the search process\nfrom initial location to the global minimum (the lowest\npoint on the error surface).\n206 ChAPtER 6", "It is relatively straightforward to map the weight up-\ndate rule over to training a single neuron. In this mapping,\nthe weight w is the bias term for a neuron, and the other\n0\nweights are associated with the other inputs to the neu-\nron. The derivation of the partial derivative of the SSE\nis dependent on the structure of the function that gen-\nerates y\u02c6. The more complex this function is, the more\ncomplex the partial derivative becomes. The fact that the\nfunction a neuron defines includes both a weighted sum-\nmation and an activation function means that the partial\nderivative of the SSE with respect to a weight in a neuron\nis more complex than the partial derivative given above.\nThe inclusion of the activation function within the neuron\nresults in an extra term in the partial derivative of the SSE.\nThis extra term is the derivative of the activation function\nwith respect to the output from the weighted summation\nfunction. The derivative of the activation function is with\nrespect to the output of the weighted summation function\nbecause this is the input that the activation function re-\nceives. The activation function does not receive the weight\ndirectly. Instead, the changes in the weight only affect\nthe output of the activation function indirectly through\nthe effect that these weight changes have on the output\nof the weighted summation. The main reason why the\nlogistic function was such a popular activation func-\ntion in neural networks for so long was that it has a very\nstraightforward derivative with respect to its inputs. The\nLEARnIng FunC tIons 207", "gradient descent weight update rule for a neuron using the\nlogistic function is as follows:\n\uf8eb \uf8f6\n\uf8ec \uf8f7\n\uf8eb \uf8f6\n\uf8ec \uf8ec \uf8f7\uf8f7\n\uf8ec \uf8ec \uf8f7\uf8f7\n\uf8ec \uf8ec \uf8f7\uf8f7\nn\nwt+1 =wt +\uf8ec \u03b7\u00d7\u2211\uf8ec(yt \u2212y\u02c6t)\u00d7(y\u02c6t\u00d7(1\u2212y\u02c6t))\u00d7xtt \uf8f7\uf8f7\ni i \uf8ec \uf8ec j j (cid:31)j(cid:29)(cid:29)(cid:30)(cid:29)(cid:29)j(cid:28) j,i \uf8f7\uf8f7\nj=1\n\uf8ec \uf8ec derivativeofthe \uf8f7\uf8f7\n\uf8ec \uf8ec logissticfunction \uf8f7\uf8f7\n\uf8ec \uf8ec withrespecttothe \uf8f7\uf8f7\n(cid:31)(cid:29)\uf8ed (cid:29)(cid:29)(cid:29)(cid:29)(cid:29)w(cid:30)eigh(cid:29)ted(cid:29)(cid:29)sum(cid:29)mat(cid:29)ion(cid:29)(cid:29)(cid:28)\uf8f8\n\uf8ec \uf8f7\n\uf8ed error gradient for wi \uf8f8\nThe fact that the weight update rule includes the derivative\nof the activation function means that the weight update\nrule will change if the activation function of the neuron is\nchanged. However, this change will simply involve updat-\ning the derivative of the activation function; the overall\nstructure of the rule will remain the same.\nThis extended weight update rule means that the gra-\ndient descent algorithm can be used to train a single neu-\nron. It cannot, however, be used to train neural networks\nwith multiple layers of neurons because the definition of\nthe error gradient for a weight depends on the error of\nthe output of the function, the term y \u2212y\u02c6 . Although it\nj j\nis possible to calculate the error of the output of a neuron\nin the output layer of the network by directly comparing\n208 ChAPtER 6", "the output with the expected output, it is not possible to\ncalculate this error term directly for the neurons in the\nhidden layer of the network, and as a result it is not pos-\nsible to calculate the error gradients for each weight. The\nbackpropagation algorithm is a solution to the problem of\ncalculating error gradients for the weights in the hidden\nlayers of the network.\nTraining a Neural Network Using Backpropagation\nThe term backpropagation has two different meanings.\nThe primary meaning is that it is an algorithm that can be\nused to calculate, for each neuron in a network, the sen-\nsitivity (gradient/rate-o f- change) of the error of the net-\nwork to changes in the weights. Once the error gradient\nfor a weight has been calculated, the weight can then be\nadjusted to reduce the overall error of the network using a\nweight update rule similar to the gradient descent weight\nupdate rule. In this sense, the backpropagation algorithm\nis a solution to the credit assignment problem, introduced\nin chapter 4. The second meaning of backpropagation is\nthat it is a complete algorithm for training a neural net-\nwork. This second meaning encompasses the first sense,\nbut also includes a learning rule that defines how the er-\nror gradients of the weights should be used to update the\nweights within the network. Consequently, the algorithm\nLEARnIng FunC tIons 209", "described by this second meaning involves a two- step pro-\ncess: solve the credit assignment problem, and then use\nthe error gradients of the weights, calculated during credit\nassignment, to update the weights in the network. It is\nuseful to distinguish between these two meanings of back-\npropagation because there are a number of different learn-\ning rules that can be used to update the weights, once the\ncredit assignment problem has been resolved. The learn-\ning rule that is most commonly used with backpropaga-\ntion is the gradient descent algorithm introduced earlier.\nThe description of the backpropagation algorithm given\nhere focuses on the first meaning of backpropagation, that\nof the algorithm being a solution to the credit assignment\nproblem.\nBackpropagation: The Two- Stage Algorithm\nThe backpropagation algorithm begins by initializing all\nthe weights of the network using random values. Note\nthat even a randomly initialized network can still generate\nan output when an input is presented to the network, al-\nthough it is likely to be an output with a large error. Once\nthe network weights have been initialized, the network\ncan be trained by iteratively updating the weights so as\nto reduce the error of the network, where the error of the\nnetwork is calculated in terms of the difference between\nthe output generated by the network in response to an\ninput pattern, and the expected output for that input, as\n210 ChAPtER 6", "defined in the training dataset. A crucial step in this itera-\ntive weight adjustment process involves solving the credit\nassignment problem, or, in other words, calculating the\nerror gradients for each weight in the network. The back-\npropagation algorithm solves this problem using a two-\nstage process. In first stage, known as the forward pass,\nan input pattern is presented to the network, and the re-\nsulting neuron activations flow forward through the net-\nwork until an output is generated. Figure 6.5 illustrates\nthe forward pass of the backpropagation algorithm. In\nthis figure, the weighted summation of inputs calculated\nat each neuron (e.g., z represents the weighted summa-\n1\ntion of inputs calculated for neuron 1) and the outputs (or\nactivations, e.g., a represents the activation for neuron 1)\n1\nof each neuron is shown. The reason for listing the z and\ni\na values for each neuron in this figure is to highlight the\ni\nfact that during the forward pass both of these values, for\neach neuron, are stored in memory. The reason they are\nstored in memory is that they are used in the backward\npass of the algorithm. The z value for a neuron is used to\ni\ncalculate the update to the weights on input connections\nto the neuron. The a value for a neuron is used to calculate\ni\nthe update to the weights on the output connections from\na neuron. The specifics of how these values are used in the\nbackward pass will be described below.\nThe second stage, known as the backward pass, be-\ngins by calculating an error gradient for each neuron in\nLEARnIng FunC tIons 211", "the output layer. These error gradients represent the sen-\nsitivity of the network error to changes in the weighted\nsummation calculation of the neuron, and they are often\ndenoted by the shorthand notation \u03b4 (pronounced delta)\nwith a subscript indicating the neuron. For example, \u03b4 is\nk\nthe gradient of the network error with respect to small\nchanges in the weighted summation calculation of the\nneuron k. It is important to recognize that there are two\ndifferent error gradients calculated in the backpropaga-\ntion algorithm:\n1. The first is the \u03b4 value for each neuron. The \u03b4 for each\nneuron is the rate of change of the error of the network\nwith respect to changes in the weighted summation\ncalculation of the neuron. There is one \u03b4 for each\nneuron. It is these \u03b4 error gradients that the algorithm\nbackpropagates.\n2. The second is the error gradient of the network with\nrespect to changes in the weights of the network. There\nis one of these error gradients for each weight in the\nnetwork. These are the error gradients that are used\nto update the weights in the network. However, it is\nnecessary to first calculate the \u03b4 term for each neuron\n(using backpropagation) in order to calculate the error\ngradients for the weights.\n212 ChAPtER 6", "Note there is only a single \u03b4 per neuron, but there may be\nmany weights associated with that neuron, so the \u03b4 term\nfor a neuron may be used in the calculation of multiple\nweight error gradients.\nOnce the \u03b4s for the output neurons have been calcu-\nlated, the \u03b4s for the neurons in the last hidden layer are\nthen calculated. This is done by assigning a portion of the\n\u03b4 from each output neuron to each hidden neuron that is\ndirectly connected to it. This assignment of blame, from\noutput neuron to hidden neuron, is dependent on the\nweight of the connection between the neurons, and the\nactivation of the hidden neuron during the forward pass\n(this is why the activations are recorded in memory dur-\ning the forward pass). Once the blame assignment, from\nthe output layer, has been completed, the \u03b4 for each neu-\nron in the last hidden layer is calculated by summing the\nportions of the \u03b4s assigned to the neuron from all of the\noutput neurons it connects to. The same process of blame\nassignment and summing is then repeated to propagate\nthe error gradient back from the last layer of hidden neu-\nrons to the neurons in the second last layer, and so on,\nback to the input layer. It is this backward propagation of\n\u03b4s through the network that gives the algorithm its name.\nAt the end of this backward pass there is a \u03b4 calculated\nfor each neuron in the network (i.e., the credit assignment\nproblem has been solved) and these \u03b4s can then be used\nto update the weights in the network (using, for example,\nLEARnIng FunC tIons 213", "z1 1 a1\nz5 5 a5 z8 8 a8\nz2 2 a2 z11 11 a11\nz6 6 a6 z9 9 a9\nz3 3 a3 z12 12 a12\nz7 7 a7 z10 10 a10\nz4 4 a4\nForward pass: activations flow from inputs to outputs\nFigure 6.5 The forward pass of the backpropagation algorithm.\nthe gradient descent algorithm introduced earlier). Figure\n6.6 illustrates the backward pass of the backpropagation\nalgorithm. In this figure, the \u03b4s get smaller and smaller as\nthe backpropagation process gets further from the output\nlayer. This reflects the vanishing gradient problem dis-\ncussed in chapter 4 that slows down the learning rate of\nthe early layers of the network.\nIn summary, the main steps within each iteration of\nthe backpropagation algorithm are as follows:\n1. Present an input to the network and allow the neuron\nactivations to flow forward through the network until an\noutput is generated. Record both the weighted sum and\nthe activation of each neuron.\n214 ChAPtER 6", "\u03b41 1\n\u03b45 5 \u03b4 8 8\n\u03b4\n\u03b42 2 11 11\n\u03b46 6 \u03b4 9 9\n\u03b4\n\u03b43 3 12 12\n\u03b47 7 \u03b4 10 10\n4\n\u03b44\nBackward pass: error gradients (\u03b4s) flow from outputs to inputs\nFigure 6.6 The backward pass of the backpropagation algorithm.\n2. Calculate a \u03b4 (delta) error gradient for each neuron in\nthe output layer.\n3. Backpropagate the \u03b4 error gradients to obtain a \u03b4\n(delta) error gradient for each neuron in the network.\n4. Use the \u03b4 error gradients and a weight update\nalgorithm, such as gradient descent, to calculate the error\ngradients for the weights and use these to update the\nweights in the network.\nThe algorithm continues iterating through these steps\nuntil the error of the network is reduced (or converged) to\nan acceptable level.\nLEARnIng FunC tIons 215", "Backpropagation: Backpropagating the \u03b4 s\nA \u03b4 term of a neuron describes the error gradient for the\nnetwork with respect to changes in the weighted summa-\ntion of inputs calculated by the neuron. To help make this\nmore concrete, figure 6.7 (top) breaks open the processing\nstages within a neuron k and uses the term z to denote\nk\nthe result of the weighted summation within the neuron.\nThe neuron in this figure receives inputs (or activations)\nfrom three other neurons (h,i,j), and z is the weighted\nk\nsum of these activations. The output of the neuron, a , is\nk\nthen calculated by passing z through a nonlinear activa-\nk\ntion function, \u03d5, such as the logistic function. Using this\nnotation a \u03b4 for a neuron k is the rate of change of the\nerror of the network with respect to small changes in the\nvalue of z . Mathematically, this term is the partial deriva-\nk\ntive of the networks error with respect to z :\nk\n\u2202Error\n\u03b4 =\nk\n\u2202z\nk\nNo matter where in a network a neuron is located\n(output layer or hidden layer), the \u03b4 for the neuron is cal-\nculated as the product of two terms:\n1. the rate of change of the network error in response to\nchanges in the neuron\u2019s activation (output): \u2202E/\u2202a ;\nk\n216 ChAPtER 6", "Figure 6.7 Top: the forward propagation of activations through the\nweighted sum and activation function of a neuron. Middle: The calculation of\nthe \u03b4 term for an output neuron (t is the expected activation for the neuron\nk\nand a is the actual activation). Bottom: The calculation of the \u03b4 term for a\nk\nhidden neuron. This figure is loosely inspired by figure 5.2 and figure 5.3 in\nReed and Marks II 1999.\nLEARnIng FunC tIons 217", "2. the rate of change of the activation of the neuron with\nrespect to changes in the weighted sum of inputs to the\nneuron: \u2202a /\u2202z .\nk k\n\u2202E \u2202a\n\u03b4 = \u00d7 k\nk\n\u2202a \u2202z\nk k\nFigure 6.7 (middle) illustrates how this product is cal-\nculated for neurons in the output layer of a network. The\nfirst step is to calculate the rate of change of the error of\nthe network with respect to the output of the neuron, the\nterm \u2202E/\u2202a . Intuitively, the larger the difference between\nk\nthe activation of a neuron, a , and the expected activation,\nk\nt , the faster the error can be changed by changing the\nk\nactivation of the neuron. So the rate of change of the error\nof the network with respect to changes in the activation of\nan output neuron k can be calculated by subtracting the\nneuron\u2019s activation (a ) from the expected activation (t ):\nk k\n\u2202E\n=t \u2212a\nk k\n\u2202a\nk\nThis term connects the error of the network to the out-\nput of the neuron. The neuron\u2019s \u03b4, however, is the rate\nof change of the error with respect to the input to the\nactivation function (z ), not the output of that function\nk\n(a ). Consequently, in order to calculate the \u03b4 for the\nk\n218 ChAPtER 6", "neuron, the \u2202E/\u2202a value must be propagated back\nk\nthrough the activation function to connect it to the in-\nput to the activation function. This is done by multiplying\n\u2202E/\u2202a by the rate of change of the activation function\nk\nwith respect to the input value to the function, z . In fig-\nk\nure 6.7, the rate of change of the activation function with\nrespect to its input is denoted by the term: \u2202a /\u2202z . This\nk k\nterm is calculated by plugging the value z (stored from\nk\nthe forward pass through the network) into the equation\nof the derivative of the activation function with respect\nto z . For example, the derivative of the logistic function\nk\nwith respect to its input is:\n\u2202logistic(z)\n=logistic(z)\u00d7(1\u2212logistic(z))\n\u2202z\nFigure 6.81 plots this function and shows that plugging a\nz value into this equation will result in a value between\nk\n0 and 0.25. For example, figure 6.8 shows that if z =0\nk\nthen \u2202a /\u2202z =0.25. This is why the weighted summa-\nk k\ntion value for each neuron (z ) is stored during the for-\nk\nward pass of the algorithm.\nThe fact that the calculation of a neuron\u2019s \u03b4 involves\na product that includes the derivative of the neuron\u2019s ac-\ntivation function makes it necessary to be able to take the\nderivative of the neuron\u2019s activation function. It is not\npossible to take the derivative of a threshold activation\nLEARnIng FunC tIons 219", "0.1\nLogistic(z)\n\u2202 logistic(z)\n\u2202 z 8.0\n)z(noitavitcA 6.0\n4.0\nmax = 0.25\n2.0\nsaturated = 0 saturated = 0 0.0\n\u221210 \u22125 0 5 10\nz\nFigure 6.8 Plots of the logistic function and the derivative of the logistic\nfunction.\nfunction because there is a discontinuity in the function\nat the threshold. As a result, the backpropagation algo-\nrithm does not work for networks composed of neurons\nthat use threshold activation functions. This is one of the\nreasons why neural networks moved away from threshold\nactivation and started to use the logistic and tanh activa-\ntion functions. The logistic and tanh functions both have\nvery simple derivatives and this made them particularly\nsuitable to backpropagation.\nFigure 6.7 (bottom) illustrates how the \u03b4 for a neu-\nron in a hidden layer is calculated. This involves the same\n220 ChAPtER 6", "product of terms as was used for neurons in the output\nlayer. The difference is that the calculation of the \u2202E/\u2202a\nk\nis more complex for hidden units. For hidden neurons, it is\nnot possible to directly connect the output of the neuron\nwith the error of a network. The output of a hidden neu-\nron only indirectly affects the overall error of the network\nthrough the variations that it causes in the downstream\nneurons that receive the output as input, and the magni-\ntude of these variations is dependent on the weight each\nof these downstream neurons applies to the output. Fur-\nthermore, this indirect effect on the network error is in\nturn dependent on the sensitivity of the network error to\nthese later neurons, that is, their \u03b4 values. Consequently,\nthe sensitivity of the network error to the output of a hid-\nden neuron can be calculated as a weighted sum of the \u03b4\nvalues of the neurons immediately downstream of the\nneuron:\n\u2202E N\n=\u2211w \u00d7\u03b4\nk,i i\n\u2202a\nk i=1\nAs a result, the error terms (the \u03b4 values) for all the down-\nstream neurons to which a neuron\u2019s output is passed in\nthe forward pass must be calculated before the \u2202E/\u2202a for\nk\nneuron k can be calculated. This, however, is not a prob-\nlem because in the backward pass the algorithm is working\nbackward through the network and will have calculated\nLEARnIng FunC tIons 221", "the \u03b4 terms for the downstream neurons before it reaches\nneuron k.\nFor hidden neurons, the other term in the \u03b4 prod-\nuct, \u2202a /\u2202z , is calculated in the same way as it is calcu-\nk k\nlated for output neurons: the z value for the neuron (the\nk\nweighted summation of inputs, stored during the forward\npass through the network) is plugged into the derivative\nof the neuron\u2019s activation function with respect to z .\nk\nBackpropagation: Updating the Weights\nThe fundamental principle of the backpropagation algo-\nrithm in adjusting the weights in a network is that each\nweight in a network should be updated in proportion to the\nsensitivity of the overall error of the network to changes\nin that weight. The intuition is that if the overall error of\nthe network is not affected by a change in a weight, then\nthe error of the network is independent of that weight,\nand, therefore, the weight did not contribute to the error.\nThe sensitivity of the network error to a change in an in-\ndividual weight is measured in terms of the rate of change\nof the network error in response to changes in that weight.\nThe overall error of a network is a function with mul-\ntiple inputs: both the inputs to the network and all the\nweights in the network. So, the rate of change of the er-\nror of a network in response to changes in a given net-\nwork weight is calculated by taking the partial derivative\nof the network error with respect to that weight. In the\n222 ChAPtER 6", "The fundamental\nprinciple of the\nbackpropagation\nalgorithm in adjusting\nthe weights in a network\nis that each weight in\na network should be\nupdated in proportion\nto the sensitivity of the\noverall error of the\nnetwork to changes\nin that weight.", "backpropagation algorithm, the partial derivative of the\nnetwork error for a given weight is calculated using the\nchain rule. Using the chain rule, the partial derivative of\nthe network error with respect a weight w on the con-\nj,k\nnection between a neuron j and a neuron k is calculated\nas the product of two terms:\n1. the first term describes the rate of change of the\nweighted sum of inputs in neuron k with respect to\nchanges in the weight \u2202z /\u2202w ;\nk j,k\n2. and the second term describes the rate of change of\nthe network error in response to changes in the weighted\nsum of inputs calculated by the neuron k. (This second\nterm is the \u03b4 for neuron k.)\nk\nFigure 6.9 shows how the product of these two terms\nconnects a weight to the output error of the network.\nThe figure shows the processing of the last two neurons\n(k and l) in a network with a single path of activation.\nNeuron k receives a single input a and the output from\nj\nneuron k is the sole input to neuron l. The output of neu-\nron l is the output of the network. There are two weights\nin this portion of the network, w and w .\nj,k k,l\nThe calculations shown in figure 6.9 appear compli-\ncated because they contain a number of different compo-\nnents. However, as we will see, by stepping through these\ncalculations, each of the individual elements is actually\n224 ChAPtER 6", "Neuron k Neuron l\na j w j,k (cid:31) a j \u00d7w j,k z k \u03d5(z k) w k,l (cid:31) a k \u00d7w k,l z l \u03d5(z l) Output\n\u2202zl \u03b4\n\u2202wk,l \u00d7 l\n\u2202Error\n\u2202wk,l\n\u2202zk \u03b4\n\u2202wj,k \u00d7 k\n\u2202Error\n\u2202wj,k\nFigure 6.9 An illustration of how the product of derivatives connects\nweights in the network to the error of the network.\neasy to calculate; it\u2019s just keeping track of all the different\nelements that poses a difficulty.\nFocusing on w , this weight is applied to an input of\nk,l\nthe output neuron of the network. There are two stages\nof processing between this weight and the network out-\nput (and error): the first is the weighted sum calculated\nin neuron l; the second is the nonlinear function applied\nto this weighted sum by the activation function of neuron\nl. Working backward from the output, the \u03b4 term is calcu-\nl\nlated using the calculation shown in the middle figure of\nfigure 6.7: the difference between the target activation for\nthe neuron and the actual activation is calculated and is\nmultiplied by the partial derivative of the neuron\u2019s activa-\ntion function with respect to its input (the weighted sum\nz ), \u2202a /\u2202z . Assuming that the activation function used\nk l l\nLEARnIng FunC tIons 225", "by neuron l is the logistic function, the term \u2202a /\u2202z is\nl l\ncalculated by plugging in the value z (stored during the\nl\nforward pass of the algorithm) into the derivation of the\nlogistic function:\n\u2202a \u2202logistic(z )\nl = l =logistic(z )\u00d7(1\u2212logistic(z ))\nl l\n\u2202z \u2202z\nl l\nSo the calculation of \u03b4 under the assumption that neuron\nl\nl uses a logistic function is:\n\u03b4 =logistic(z )\u00d7(1\u2212logistic(z ))\u00d7(t \u2212a )\nl l l l l\nThe \u03b4 term connects the error of the network to the\nl\ninput to the activation function (the weighted sum z ).\nl\nHowever, we wish to connect the error of the network back\nto the weight w . This is done by multiplying the \u03b4 term\nk,l l\nby the partial derivative of the weighted summation func-\ntion with respect to weight w : \u2202z /\u2202w . This partial\nk,l l k,l\nderivative describes how the output of the weighted sum\nfunction z changes as the weight w changes. The fact\nl k,l\nthat the weighted summation function is a linear function\nof weights and activations means that in the partial de-\nrivative with respect to a particular weight all the terms in\nthe function that do not involve the specific weight go to\nzero (are considered constants) and the partial derivative\n226 ChAPtER 6", "simplifies to just the input associated with that weight, in\nthis instance input a .\nk\n\u2202z\nl =a\nk\n\u2202w\nk,l\nThis is why the activations for each neuron in the network\nare stored in the forward pass. Taken together these two\nterms, \u2202z /\u2202w and \u03b4 , connect the weight w to the\nl k,l l k,l\nnetwork error by first connecting the weight to z , and\nl\nthen connecting z to the activation of the neuron, and\nl\nthereby to the network error. So, the error gradient of\nthe network with respect to changes in weight w is\nk,l\ncalculated as:\n\u2202Error \u2202z\n= l \u00d7\u03b4 =a \u00d7\u03b4\nl k l\n\u2202w \u2202w\nk,l k,l\nThe other weight in the figure 6.9 network, w , is\nk,l\ndeeper in the network, and, consequently, there are more\nprocessing steps between it and the network output (and\nerror). The \u03b4 term for neuron k is calculated, through\nbackpropagation (as shown at the bottom of figure 6.7),\nusing the following product of terms:\n\u2202a\n\u03b4 = k \u00d7(w \u00d7\u03b4 )\nk k,l l\n\u2202z\nk\nLEARnIng FunC tIons 227", "Assuming the activation function used by neuron k is the\nlogistic function, then the term \u2202a /\u2202z is calculated in\nk k\na similar way to \u2202a /\u2202z : the value z is plugged into the\nl l k\nequation for the derivative of the logistic function. So,\nwritten out in long form the calculation of \u03b4 is:\nk\n\u03b4 =logistic(z )\u00d7(1\u2212logistic(z ))\u00d7(w \u00d7\u03b4 )\nk k k k,l l\nHowever, in order to connect the weight w with the error\nj,k\nof the network, the term \u03b4 must be multiplied by the par-\nk\ntial derivative of the weighted summation function with\nrespect to the weight: \u2202z /\u2202w . As described above, the\nk j,k\npartial derivative of a weighted sum function with respect\nto a weight reduces to the input associated with the weight\nw (i.e., a); and the gradient of the networks error with\nj,k j\nrespect to the hidden weight w is calculated by multi-\nj,k\nplying a by \u03b4 . Consequently, the product of the terms\nj k\n(\u2202z /\u2202w and \u03b4 ) forms a chain connecting the weight\nk j,k k\nw to the network error. For completeness, the product\nj,k\nof terms for w , assuming logistic activation functions in\nj,k\nthe neurons, is:\n\u2202Error \u2202z\n= k \u00d7\u03b4 =a \u00d7\u03b4\nk j k\n\u2202w \u2202w\nj,k j,k\nAlthough this discussion has been framed in the con-\ntext of a very simple network with only a single path of\n228 ChAPtER 6", "connections, it generalizes to more complex networks be-\ncause the calculation of the \u03b4 terms for hidden units already\nconsiders the multiple connections emanating from a neu-\nron. Once the gradient of the network error with respect\nto a weight has been calculated (\u2202Error/w =\u03b4 \u00d7a ),\nj,k k j\nthe weight can be adjusted so as to reduce the weight of\nthe network using the gradient descent weight update\nrule. Here is the weight update rule, specified using the\nnotation from backpropagation, for the weight on the con-\nnection between neuron j and neuron k during iteration\nt of the algorithm:\nwt+1 =wt +(\u03b7\u00d7\u03b4 \u00d7a )\nj,k j,k k j\nFinally, an important caveat on training neural net-\nworks with backpropagation and gradient descent is that\nthe error surface of a neural network is much more com-\nplex than that of a linear models. Figure 6.3 illustrated the\nerror surface of a linear model as a smooth convex bowl\nwith a single global minimum (a single best set of weights).\nHowever, the error surface of a neural network is more like\na mountain range with multiple valleys and peaks. This is\nbecause each of the neurons in a network includes a non-\nlinear function in its mapping of inputs to outputs, and so\nthe function implemented by the network is a nonlinear\nfunction. Including a nonlinearity within the neurons of\na network increases the expressive power of the network\nLEARnIng FunC tIons 229", "in terms of its ability to learn more complex functions.\nHowever, the price paid for this is that the error surface\nbecomes more complex and the gradient descent algo-\nrithm is no longer guaranteed to find the set of weights\nthat define the global minimum on the error surface; in-\nstead it may get stuck within a minima (local minimum).\nFortunately, however, backpropagation and gradient de-\nscent can still often find sets of weights that define useful\nmodels, although searching for useful models may require\nrunning the training process multiple times to explore dif-\nferent parts of the error surface landscape.\n230 ChAPtER 6", "7\nTHE FUTURE OF DEEP LEARNING\nOn March 27, 2019, Yoshua Bengio, Geoffrey Hinton, and\nYann LeCun jointly received the ACM A.M. Turing award.\nThe award recognized the contributions they have made\nto deep learning becoming the key technology driving the\nmodern artificial intelligence revolution. Often described\nas the \u201cNobel Prize for Computing,\u201d the ACM A.M Tur-\ning award carries a $1 million prize. Sometimes working\ntogether, and at other times working independently or in\ncollaboration with others, these three researchers have,\nover a number of decades of work, made numerous contri-\nbutions to deep learning, ranging from the popularization\nof backpropagation in the 1980s, to the development of\nconvolutional neural networks, word embeddings, atten-\ntion mechanisms in networks, and generative adversarial\nnetworks (to list just some examples). The announcement\nof the award noted the astonishing recent breakthroughs", "that deep learning has led to in computer vision, robot-\nics, speech recognition, and natural language processing,\nas well as the profound impact that these technologies\nare having on society, with billions of people now using\ndeep learning based artificial intelligence on a daily basis\nthrough smart phones applications. The announcement\nalso highlighted how deep learning has provided scien-\ntists with powerful new tools that are resulting in scien-\ntific breakthroughs in areas as diverse as medicine and\nastronomy. The awarding of this prize to these research-\ners reflects the importance of deep learning to modern\nscience and society. The transformative effects of deep\nlearning on technology is set to increase over the com-\ning decades with the development and adoption of deep\nlearning continuing to be driven by the virtuous cycle of\never larger datasets, the development of new algorithms,\nand improved hardware. These trends are not stopping,\nand how the deep learning community responds to them\nwill drive growth and innovations within the field over the\ncoming years.\nBig Data Driving Algorithmic Innovations\nChapter 1 introduced the different types of machine learn-\ning: supervised, unsupervised, and reinforcement learn-\ning. Most of this book has focused on supervised learning,\n232 ChAPtER 7", "primarily because it is the most popular form of machine\nlearning. However, a difficulty with supervised learning\nis that it can cost a lot of money and time to annotate\nthe dataset with the necessary target labels. As datasets\ncontinue to grow, the data annotation cost is becoming\na barrier to the development of new applications. The\nImageNet dataset1 provides a useful example of the scale\nof the annotation task involved in deep learning projects.\nThis data was released in 2010, and is the basis for the Ima-\ngeNet Large- Scale Visual Recognition Challenge (ILSVRC).\nThis is the challenge that the AlexNet CNN won in 2012\nand the ResNet system won in 2015. As was discussed in\nchapter 4, AlexNet winning the 2012 ILSVRC challenge\ngenerated a lot of excitement about deep learning mod-\nels. However, the AlexNet win would not have been pos-\nsible without the creation of the ImageNet dataset. This\ndataset contains more than fourteen million images that\nhave been manually annotated to indicate which objects\nare present in each image; and more than one million of\nthe images have actually been annotated with the bound-\ning boxes of the objects in the image. Annotating data at\nthis scale required a significant research effort and budget,\nand was achieved using crowdsourcing platforms. It is not\nfeasible to create annotated datasets of this size for every\napplication.\nOne response to this annotation challenge has\nbeen a growing interest in unsupervised learning. The\nthE FutuRE oF dEEP LEARnIng 233", "As datasets continue\nto grow, the data\nannotation cost is\nbecoming a barrier to\nthe development of new\napplications.", "autoencoder models used in Hinton\u2019s pretraining (see\nchapter 4) are one neural network approach to unsuper-\nvised learning, and in recent years different types of au-\ntoencoders have been proposed. Another approach to this\nproblem is to train generative models. Generative models\nattempt to learn the distribution of the data (or, to model\nthe process that generated the data). Similar to autoen-\ncoders, generative models are often used to learn a useful\nrepresentation of the data prior to training a supervised\nmodel. Generative adversarial networks (GANs) are an ap-\nproach to training generative models that has received a\nlot of attention in recent years (Goodfellow et al. 2014). A\nGAN consists of two neural networks, a generative model\nand a discriminative model, and a sample of real data. The\nmodels are trained in an adversarial manner. The task of\nthe discriminative model is to learn to discriminate be-\ntween real data sampled from the dataset, and fake data\nthat has been synthesized by the generator. The task of\nthe generator is to learn to synthesize fake data that can\nfool the discriminative model. Generative models trained\nusing a GAN can learn to synthesize fake images that\nmimic an artistic style (Elgammal et al. 2017), and also to\nsynthesize medical images along with lesion annotations\n(Frid- Adar et al. 2018). Learning to synthesize medical\nimages, along with the segmentation of the lesions in\nthe synthesized image, opens the possibility of automati-\ncally generating massive labeled datasets that can be used\nthE FutuRE oF dEEP LEARnIng 235", "for supervised learning. A more worrying application of\nGANs is the use of these networks to generate deep fakes:\na deep fake is a fake video of a person doing something\nthey never did that is created by swapping their face into a\nvideo of someone else. Deep fakes are very hard to detect,\nand have been used maliciously on a number of occasions\nto embarrass public figures, or to spread fake news stories.\nAnother solution to the data labeling bottleneck is that\nrather than training a new model from scratch for each\nnew application, we rather repurpose models that have\nbeen trained on a similar task. Transfer learning is the ma-\nchine learning challenge of using information (or repre-\nsentations) learned on one task to aid learning on another\ntask. For transfer learning to work, the two tasks should\nbe from related domains. Image processing is an example\nof a domain where transfer learning is often used to speed\nup the training of models across different tasks. Transfer\nlearning is appropriate for image processing tasks because\nlow- level visual features, such as edges, are relatively stable\nand useful across nearly all visual categories. Furthermore,\nthe fact that CNN models learn a hierarchy of visual fea-\nture, with the early layers in CNN learning functions that\ndetect these low- level visual features in the input, makes it\npossible to repurpose the early layers of pretrained CNNs\nacross multiple image processing projects. For example,\nimagine a scenario where a project requires an image clas-\nsification model that can identify objects from specialized\n236 ChAPtER 7", "categories for which there are no samples in general image\ndatasets, such as ImageNet. Rather than training a new\nCNN model from scratch, it is now relatively standard to\nfirst download a state- of- the- art model (such as the Mi-\ncrosoft ResNet model) that has been trained on ImageNet,\nthen replace the later layers of the model with a new set\nof layers, and finally to train this new hybrid-m odel on\na relatively small dataset that has been labeled with the\nappropriate categories for the project. The later layers of\nthe state- of- the- art (general) model are replaced because\nthese layers contain the functions that combine the low-\nlevel features into the task specific categories the model\nwas originally trained to identify. The fact that the early\nlayers of the model have already been trained to identify\nthe low- level visual features speeds up the training and re-\nduces the amount of data needed to train the new project\nspecific model.\nThe increased interest in unsupervised learning, gen-\nerative models, and transfer learning can all be understood\nas a response to the challenge of annotating increasingly\nlarge datasets.\nThe Emergence of New Models\nThe rate of emergence of new deep learning models is ac-\ncelerating every year. A recent example is capsule networks\nthE FutuRE oF dEEP LEARnIng 237", "(Hinton et al. 2018; Sabour et al. 2017). Capsule networks\nare designed to address some of the limitations of CNNs.\nOne problem with CNNs, sometimes known as the Picasso\nproblem, is the fact that a CNN ignores the precise spatial\nrelationships between high-l evel components within an\nobject\u2019s structure. What this means in practice is that a\nCNN that has been trained to identify faces may learn to\nidentify the shapes of eyes, the nose, and the mouth, but\nwill not learn the required spatial relationships between\nthese parts. Consequently, the network can be fooled by\nan image that contains these body parts, even if they are\nnot in the correct relative position to each other. This\nproblem arises because of the pooling layers in CNNs that\ndiscard positional information.\nAt the core of capsule networks is the intuition that\nthe human brain learns to identify object types in a view-\npoint invariant manner. Essentially, for each object type\nthere is an object class that has a number of instantiation\nparameters. The object class encodes information such as\nthe relative relationship of different object parts to each\nother. The instantiation parameters control how the ab-\nstract description of an object type can be mapped to the\nspecific instance of the object that is currently in view (for\nexample, its pose, scale, etc.).\nA capsule is a set of neurons that learns to identify\nwhether a specific type of object or object part is present\nat a particular location in an image. A capsule outputs an\n238 ChAPtER 7", "activity vector that represents the instantiation parame-\nters of the object instance, if one is present at the relevant\nlocation. Capsules are embedded within convolutional\nlayers. However, capsule networks replace the pooling\nprocess, which often defines the interface between convo-\nlutional layers, with a process called dynamic routing. The\nidea behind dynamic routing is that each capsule in one\nlayer in the network learns to predict which capsule in the\nnext layer is the most relevant capsule for it to forward its\noutput vector to.\nAt the time or writing, capsule networks have the state-\nof- the- art performance on the MNIST handwritten digit\nrecognition dataset that the original CNNs were trained\non. However, by today\u2019s standards, this is a relatively small\ndataset, and capsule networks have not been scaled to\nlarger datasets. This is partly because the dynamic rout-\ning process slows down the training of capsule networks.\nHowever, if capsule networks are successfully scaled, then\nthey may introduce an important new form of model that\nextends the ability of neural networks to analyze images\nin a manner much closer to the way humans do.\nAnother recent model that has garnered a lot of in-\nterest is the transformer model (Vaswani et al. 2017).\nThe transformer model is an example of a growing trend\nin deep learning where models are designed to have so-\nphisticated internal attention mechanisms that enable a\nmodel to dynamically select subsets of the input to focus\nthE FutuRE oF dEEP LEARnIng 239", "on when generating an output. The transformer model\nhas achieved state- of- the- art performance on machine\ntranslation for some language pairs, and in the future this\narchitecture may replace the encoder-d ecoder architecture\ndescribed in chapter 5. The BERT (Bidirectional Encoder\nRepresentations from Transformers) model has built on\nthe Transformer architecture (Devlin et al. 2018). The\nBERT development is particularly interesting because at\nits core is the idea of transfer learning (as discussed above\nin relation to the data annotation bottleneck). The basic\napproach to creating a natural language processing model\nwith BERT is to pretrain a model for a given language us-\ning a large unlabeled dataset (the fact that the dataset is\nunlabeled means that it is relatively cheap to create). This\npretrained model can then be used as the basis to create a\nmodels for specific tasks for the language (such as senti-\nment classification or question answering) by fine- tuning\nthe pretrained model using supervised learning and a\nrelatively small annotated dataset. The success of BERT\nhas shown this approach to be tractable and effective in\ndeveloping state- of- the- art natural language processing\nsystems.\nNew Forms of Hardware\nToday\u2019s deep learning is powered by graphics processing\nunits (GPUs): specialized hardware that is optimized to\n240 ChAPtER 7", "do fast matrix multiplications. The adoption, in the late\n2000s, of commodity GPUs to speed up neural network\ntraining was a key factor in many of the breakthroughs\nthat built momentum behind deep learning. In the last\nten years, hardware manufacturers have recognized the\nimportance of the deep learning market and have devel-\noped and released hardware specifically designed for deep\nlearning, and which supports deep learning libraries, such\nas TensorFlow and PyTorch. As datasets and networks\ncontinue to grow in size, the demand for faster hardware\ncontinues. At the same time, however, there is a grow-\ning recognition of the energy costs associated with deep\nlearning, and people are beginning to look for hardware\nsolutions that have a reduced energy footprint.\nNeuromorphic computing emerged in the late 1980s\nfrom the work of Carver Mead.2 A neuromorphic chip is\ncomposed of a very- large- scale integrated (VLSI) circuit,\nconnecting potentially millions of low-p ower units known\nas spiking neurons. Compared with the artificial neurons\nused in standard deep learning systems, the design of a\nspiking neuron is closer to the behavior of biological neu-\nrons. In particular, a spiking neuron does not fire in re-\nsponse to the set of input activations propagated to it at a\nparticular time point. Instead, a spiking neuron maintains\nan internal state (or activation potential) that changes\nthrough time as it receives activation pulses. The activa-\ntion potential increases when new activations are received,\nthE FutuRE oF dEEP LEARnIng 241", "and decays through time in the absence of incoming ac-\ntivations. The neuron fires when its activation potential\nsurpasses a specific threshold. Due to the temporal decay\nof the neuron\u2019s activation potential, a spiking neuron only\nfires if it receives the requisite number of input activations\nwithin a time window (a spiking pattern). One advantage\nof this temporal based processing is that spiking neurons\ndo not fire on every propagation cycle, and this reduces\nthe amount of energy the network consumes.\nIn comparison with traditional CPU design, neuro-\nmorphic chips have a number of distinctive characteristics,\nincluding:\n1. Basic building blocks: traditional CPUs are built using\ntransistor based logic gates (e.g., AND, OR, NAND gates),\nwhereas neuromorphic chips are built using spiking\nneurons.\n2. Neuromorphic chips have an analog aspect to them:\nin a traditional digital computer, information is sent in\nhigh- low electrical bursts in sync with a central clock; in\na neuromorphic chip, information is sent as patterns of\nhigh- low signals that vary through time.\n3. Architecture: the architecture of traditional CPUs\nis based on the von Neumann architecture, which is\nintrinsically centralized with all the information passing\nthrough the CPU. A neuromorphic chip is designed to\n242 ChAPtER 7", "allow massive parallelism of information flow between\nthe spiking neurons. Spiking neurons communicate\ndirectly with each other rather than via a central\ninformation processing hub.\n4. Information representation is distributed through\ntime: the information signals propagated through a\nneuromorphic chip use a distributed representation,\nsimilar to the distributed representations discussed in\nchapter 4, with the distinction that in a neuromorphic\nchip these representations are also distributed through\ntime. Distributed representations are more robust to\ninformation loss than local representations, and this is\na useful property when passing information between\nhundreds of thousands, or millions, of components,\nsome of which are likely to fail.\nCurrently there are a number of major research proj-\nects focused on neuromorphic computing. For example,\nin 2013 the European Commission allocated one billion\neuros in funding to the ten- year Human Brain Project.3\nThis project directly employs more than five hundred sci-\nentists, and involves research from more than a hundred\nresearch centers across Europe. One of the projects key ob-\njectives is the development of neuromorphic computing\nplatforms capable of running a simulation of a complete\nhuman brain. A number of commercial neuromorphic\nthE FutuRE oF dEEP LEARnIng 243", "chips have also been developed. In 2014, IBM launched\nthe TrueNorth chip, which contained just over a million\nneurons that are connected together by over 286 million\nsynapses. This chip uses approximately 1/10,000th the\npower of a conventional microprocessor. In 2018, Intel\nLabs announced the Loihi (pronounced low- ee- hee) neu-\nromorphic chip. The Loihi chip has 131,072 neurons con-\nnected together by 130,000,000 synapses. Neuromorphic\ncomputing has the potential to revolutionize deep learn-\ning; however, it still faces a number of challenges, not least\nof which is the challenge of developing the algorithms and\nsoftware patterns for programming this scale of massively\nparallel hardware.\nFinally, on a slightly longer time horizon, quantum\ncomputing is another stream of hardware research that\nhas the potential to revolutionize deep learning. Quantum\ncomputing chips are already in existence; for example, In-\ntel has created a 49-q ubit quantum test chip, code named\nTangle Lake. A qubit is the quantum equivalent of a binary\ndigit (bit) in traditional computing. A qubit can store more\nthan one bit of information; however, it is estimated that\nit will require a system with one million or more qubits\nbefore quantum computing will be useful for commercial\npurposes. The current time estimate for scaling quantum\nchips to this level is around seven years.\n244 ChAPtER 7", "The Challenge of Interpretability\nMachine learning, and deep learning, are fundamentally\nabout making data- driven decisions. Although deep learn-\ning provides a powerful set of algorithms and techniques\nto train models that can compete (and in some cases out-\nperform) humans on a range of decision- making tasks,\nthere are many situations where a decision by itself is not\nsufficient. Frequently, it is necessary to provide not only\na decision but also the reasoning behind a decision. This\nis particularly true when the decision affects a person, be\nit a medical diagnosis or a credit assessment. This concern\nis reflected in privacy and ethics regulations in relation to\nthe use of personal data and algorithmic decision-m aking\npertaining to individuals. For example, Recital 714 of the\nGeneral Data Protection Regulations (GDPR) states that\nindividuals, affected by a decision made by an automated\ndecision- making process, have the right to an explanation\nwith regards to how the decision was reached.\nDifferent machine learning models provide different\nlevels of interpretability with regard to how they reach a\nspecific decision. Deep learning models, however, are pos-\nsibly the least interpretable. At one level of description,\na deep learning model is quite simple: it is composed of\nsimple processing units (neurons) that are connected to-\ngether into a network. However, the scale of the networks\n(in terms of the number of neurons and the connections\nthE FutuRE oF dEEP LEARnIng 245", "between them), the distributed nature of the represen-\ntations, and the successive transformations of the input\ndata as the information flows deeper into the network,\nmakes it incredibly difficult to interpret, understand, and\ntherefore explain, how the network is using an input to\nmake a decision.\nThe legal status of the right to explanation within\nGDPR is currently vague, and the specific implications\nof it for machine learning and deep learning will need to\nbe worked out in the courts. This example does, however,\nhighlight the societal need for a better understanding of\nhow deep learning models use data. The ability to inter-\npret and understand the inner workings of a deep learn-\ning model is also important from a technical perspective.\nFor example, understanding how a model uses data can\nreveal if a model has an unwanted bias in how it makes its\ndecisions, and also reveal the corner cases that the model\nwill fail on. The deep learning and the broader artificial\nintelligence research communities are already responding\nto this challenge. Currently, there are a number of proj-\nects and conferences focused on topics such as explainable\nartificial intelligence, and human interpretability in ma-\nchine learning.\nChis Olah and his colleagues summarize the main\ntechniques currently used to examine the inner workings\nof deep learning models as: feature visualization, attribu-\ntion, and dimensionality reduction (Olah et al. 2018). One\n246 ChAPtER 7", "way to understand how a network processes information is\nto understand what inputs trigger particular behaviors in\na network, such as a neuron firing. Understanding the spe-\ncific inputs that trigger the activation of a neuron enables\nus to understand what the neuron has learned to detect in\nthe input. The goal of feature visualization is to generate\nand visualize inputs that cause a specific activity within a\nnetwork. It turns out that optimization techniques, such\na backpropogation, can be used to generate these inputs.\nThe process starts with a random generated input and the\ninput is then iteratively updated until the target behavior\nis triggered. Once the required necessary input has been\nisolated, it can then be visualized in order to provide a bet-\nter understanding of what the network is detecting in the\ninput when it responds in a particular way. Attribution fo-\ncuses on explaining the relationship between neurons, for\nexample, how the output of a neuron in one layer of the\nnetwork contributes to the overall output of the network.\nThis can be done by generating a saliency (or heat- map)\nfor the neurons in a network that captures how much\nweight the network puts on the output of a neuron when\nmaking a particular decision. Finally, much of the activity\nwithin a deep learning network is based on the processing\nof high- dimensional vectors. Visualizing data enables us\nto use our powerful visual cortex to interpret the data and\nthe relationships within the data. However, it is very dif-\nficult to visualize data that has a dimensionality greater\nthE FutuRE oF dEEP LEARnIng 247", "than three. Consequently, visualization techniques that\nare able to systematically reduce the dimensionality of\nhigh- dimensional data and visualize the results are incred-\nibly useful tools for interpreting the flow of information\nwithin a deep network. t- SNE5 is a well-k nown technique\nthat visualizes high- dimensional data by projecting each\ndatapoint into a two- or three- dimensional map (van der\nMaaten and Hinton 2008). Research on interpreting deep\nlearning networks is still in its infancy, but in the com-\ning years, for both societal and technical reasons, this re-\nsearch is likely to become a more central concern to the\nbroader deep learning community.\nFinal Thoughts\nDeep learning is ideally suited for applications involving\nlarge datasets of high-d imensional data. Consequently,\ndeep learning is likely to make a significant contribution\nto some of the major scientific challenges of our age. In\nthe last two decades, breakthroughs in biological se-\nquencing technology have made it possible to generate\nhigh- precision DNA sequences. This genetic data has the\npotential to be the foundation for the next generation of\npersonalized precision medicine. At the same time, inter-\nnational research projects, such as the Large Hadron Col-\nlider and Earth orbit telescopes, generate huge amounts\n248 ChAPtER 7", "One way to understand\nhow a network\nprocesses information\nis to understand what\ninputs trigger particular\nbehaviors in a network,\nsuch as a neuron firing.", "of data on a daily basis. Analyzing this data can help us to\nunderstand the physics of our universe at the smallest and\nthe biggest scales. In response to this flood of data, scien-\ntists are, in ever increasing numbers, turning to machine\nlearning and deep learning to enable them to analyze\nthis data.\nAt a more mundane level, however, deep learning al-\nready directly affects our lives. It is likely, that for the last\nfew years, you have unknowingly been using deep learning\nmodels on a daily basis. A deep learning model is prob-\nably being invoked every time you use an internet search\nengine, a machine translation system, a face recognition\nsystem on your camera or social media website, or use a\nspeech interface to a smart device. What is potentially\nmore worrying is that the trail of data and metadata that\nyou leave as you move through the online world is also\nbeing processed and analzsed using deep learning models.\nThis is why it is so important to understand what deep\nlearning is, how it works, what is it capable of, and its cur-\nrent limitations.\n250 ChAPtER 7", "GLOSSARY\nActivation Function\nA function that takes as input the result of the weighted sum of the inputs to\na neuron and applies a nonlinear mapping to this weighted sum. Including an\nactivation function within the neurons of a network enables the network to\nlearn a nonlinear mapping. Examples of commonly used activation functions\ninclude: logistic, tanh, and ReLU.\nArtificial Intelligence\nThe field of research that is focused on developing computational systems\nthat can perform tasks and activities normally considered to require human\nintelligence.\nBackpropagation\nBackpropagation is an algorithm used to train a neural network with hidden\nlayers of neurons. During training, the weights in a network are iteratively\nupdated to reduce the error of the network. In order to update the weights\non the links coming into a specific neuron in a network, it is necessary to first\ncalculate an estimate of the contribution of the output of that neuron to the\noverall error of the network. The backpropagation algorithm is a solution to\ncalculating these estimates for each neuron in the network. Once these errors\nestimates have been calculated for each neuron, the weights of the neurons\ncan be updated using an optimization algorithm such as gradient descent.\nBackpropagation works in two phases: a forward pass and a backward pass. In\nthe forward pass, an example is presented to the network and the overall error\nof the network is calculated at the output layer of the network by comparing\nthe output of the network with the expected output for the example specified\nin the dataset. In the backward pass, the error of the network is shared back\nthrough the network with each neuron receiving a portion of blame for the\nerror in proportion to the sensitivity of the error to changes in the output\nof that neuron. The process of sharing back the errors through the network\nis known as backpropagating the errors and this is where the algorithm gets\nits name.", "Convolutional Neural Network\nA convolutional neural network is a network that has at least one convolu-\ntional layer in it. A convolution layer is composed of a set of neurons that share\nthe same set of weights and whose combined receptive fields cover an entire\ninput. The union of the outputs of such a set of neurons is known as a fea-\nture map. In many convolutional neural networks, features maps are passed\nthrough a ReLU activation layer and then a pooling layer.\nDataset\nA collection of instances with each instances described in terms of a set of\nfeatures. In its most basic form, a dataset is organized in an n \u00d7 m matrix,\nwhere n is the number of instances (rows) and m is the number of features\n(columns).\nDeep Learning\nDeep learning is the subfield of machine learning that designs and evaluates\ntraining algorithms and architectures for modern neural network models. A\ndeep neural network is a network that has multiple (e.g., >2) layers of hidden\nunits (or neurons).\nFeedforward Network\nA feedforward network is a neural network where all the connections in the\nnetwork point forward to the neurons in subsequent layer. In other words,\nthere are no links backward from the output of a neuron to the input of a\nneuron in an earlier layer.\nFunction\nA function is a deterministic mapping from a set of input values to one or more\noutput values. In the context of machine learning, the term function is often\nused interchangeably with the term model.\nGradient Descent\nGradient descent is an optimization algorithm for finding a function with the\nminimum error with respect to modeling the patterns in a dataset. In the\ncontext of training a neural network, gradient descent is used to find the set\nof weights for a neuron that minimizes the error of the output of the neuron.\nThe gradient the algorithm descends is the error gradient of the neuron as its\nweights are updated. The algorithm is frequently used in conjunction with\nbackpropagation to train neural networks with hidden layers of neurons.\n252 gLossARy", "GPU (Graphical Processing Unit)\nSpecialized hardware that is optimized for fast matrix multiplication. Origi-\nnally designed to increase the speed in graphics rendering but also found to\nspeed up the training of neural networks.\nLSTM (Long Short- Term Memory)\nA network designed to address the problem of vanishing gradients in recurrent\nneural networks. The network is composed of a cell block where activations\nflow through from one time- step to the next and a set of gates on the cell\nblock that control the flow of these activations. The gates are implemented\nusing layers of sigmoid and tanh activation functions. The standard LSTM\narchitecture has three such gates: the forget gate, the update gate, and the\noutput gate.\nMachine Learning (ML)\nThe field of computer science research that focuses on developing and evalu-\nating algorithms that enable computers to learn from experience. Generally\nthe concept of experience is represented as a dataset of historic events, and\nlearning involves identifying and extracting useful patterns from a data-\nset. A machine learning algorithm takes a dataset as input and returns a\nmodel that encodes the patterns the algorithm extracted (or learned) from\nthe data.\nMachine Learning Algorithm\nA process that analyzes a dataset and returns as model (i.e., an instan-\ntiation of a function as a computer program) that matches the patterns in\nthe data.\nModel\nIn machine learning, a model is a computer program that encodes the patterns\nthe machine learning algorithm has extracted from a dataset. There are many\ndifferent types of machine learning models; however, deep learning is focused\non creating neural network models with multiple layers of hidden neurons. A\nmodel is created (or trained) by running a machine learning algorithm on a\ndataset. Once the model has been trained, it can then be used to analyze new\ninstances; the term inference is sometimes used to describe the process of ana-\nlyzing a new instance using a trained model. In the context of machine learn-\ning, the terms model and function are often used interchangeably: a model is\nan instantiation of a function as a computer program.\ngLossARy 253", "Neuromorphic Computing\nNeuromorphic chips are composed of very large sets of spiking neurons archi-\ntecture that are connected in a massively parallel manner.\nNeural Network\nA machine learning model that is implemented as a network of simple infor-\nmation processing units called neurons. It is possible to create a variety of\ndifferent types of neural networks by modifying the connections between the\nneurons in the network. Examples of popular types of neural networks in-\nclude: feedforward, convolutional, and recurrent networks.\nNeuron\nIn the context of deep learning (as opposed to brain science), a neuron is a\nsimple information processing algorithm that takes a number of numeric val-\nues as input and maps these values to a high- or low- output activation. This\nmapping is typically implemented by first multiplying each input value by a\nweight, then summing the results of these multiplications, and finally passing\nthe results of the weighted summation through an activation function.\nOverfitting\nOverfitting a dataset occurs if the model returned by a machine learning algo-\nrithm is so complex that it is able to model small variations in the data caused\nby the noise in the data sample.\nRecurrent Neural Network\nA recurrent neural network has a single layer of hidden neurons, the output\nof which is fed back into this layer with the next input. This feedback (or re-\ncurrence) within the network gives the network a memory that enables it to\nprocess each input within the context of what it has previously processed.\nRecurrent neural networks are ideally suited to processing sequential or time-\nseries data.\nReinforcement Learning\nThe goal of reinforcement learning is to enable an agent to learn a policy on\nhow it should act in a given environment. A policy is a function that maps\nfrom an agent\u2019s current observations of its environment and its own internal\nstate to an action. Typically used for online control tasks such as robot control\nand game playing.\n254 gLossARy", "ReLU Unit\nA ReLU unit is a neuron that uses a rectified linear function as its activation\nfunction.\nSupervised Learning\nA form of machine learning where the goal is to learn a function that maps\nfrom a set of input attributes for an instance to an accurate estimate of the\nmissing value for the target attribute of the same instance.\nTarget Attribute\nIn supervised machine learning, a target attribute is the attribute that the\nmodel is trained to estimate the value of.\nUnderfitting\nUnderfitting a dataset occurs if the model returned by a machine learning\nalgorithm is too simplistic to capture the real complexity of the relationship\nbetween the inputs and outputs in a domain.\nUnsupervised Learning\nA form of machine learning where the goal is to identify regularities, such as\nclusters of similar instances, in the data. Unlike supervised learning, there is\nno target attribute in an unsupervised learning task.\nVanishing Gradient\nThe vanishing gradient problem describes the fact that as more layers are\nadded to a network it takes longer to train the network. This problem is caused\nby the fact that when a neural network is trained using backpropagation and\ngradient descent, the updating of the weights on links coming into a neuron\nin the network is dependent on the gradient (or sensitivity) of the network\nerror with respect to the output of the neuron. Using backpropagation, the\nprocess of sharing back the error gradients through a neuron involves a se-\nquence of multiplications, often by values less than one. As a result, as the\nerror gradient is passed back through the network, the error gradient tends\nto get smaller and smaller (i.e., vanish). As a direct consequence of this, the\nupdates to weights in the early layers of the network are very small and the\nneurons in these layers take a long time to train.\ngLossARy 255", "", "NOTES\nChapter 1\n1. https://deepmind.com/research/alphago/.\n2. The Elo rating system is a method for calculating the skill level of\nplayers in zero- sum games, such as Chess. It is named after its inventor,\nArpad Elo.\n3. Noise in data refers to corrupt or incorrect data. Noise in data can been\ncaused by broken sensors, or mistakes in data entry, and so on.\n4. By domain we mean the problem or task that we are trying to solve using\nmachine learning. For example, it could be spam filtering, house prices predic-\ntion, or automatically classifying X- rays.\n5. There are some scenarios where more complex dataset representations are\nrequired. For example, for time- series data, a dataset may require a three-\ndimensional representation, composed of a series of two- dimensional matri-\nces, each describing the state of the system at a point in time, linked together\nthrough time. The term tensor generalizes the concept of a matrix to higher\ndimensions.\nChapter 2\n1. It turns out that the relationship between annual income and happiness\nis linear up to a point, but that once your annual income goes beyond this\npoint more money won\u2019t make you happier. A study by Kahneman and Deaton\n(2010) found that in the US the general cutoff, after which increases in income\nno longer increase emotional well- being, was around $75,000.\n2. This is the same dataset that appears in table 1.1 in chapter 1; it is repeated\nhere for convenience.\nChapter 3\n1. The origin is the location in a coordinate system where the axes cross. In a\ntwo- dimensional coordinate system, it is where the x-a xis and y- axis cross\u2014 in\nother words, it is the location at coordinates x=0, y=0.\n2. In chapter 2, we used the same approach to merge the intercept parameter\nof the linear model into the weights of the model.\n3. To highlight this column organization the weights have been indexed\ncolumn- row, rather than row- column.", "4. For further discussion on the size and growth of networks, see page 23 of\nGoodfellow et al. 2016.\nChapter 4\n1. Figures 3.6 and 3.7 show the linear (straight line) decision boundary of\nneuron that uses a threshold activation function.\n2. This illustration of the use of associative memory for pattern completion\nand error correction is inspired from an example in chapter 42 of MacKay\n2003.\n3. For example, Paul Werbos\u2019s 1974 PhD thesis is credited with being the first\npublication to describe the use of backpropagation of errors in the training of\nartificial neural networks (Werbos 1974).\n4. The Hopfield network architecture, introduced at the start of this sec-\ntion, also included recurrent connections (feedback loops between neurons).\nHowever, the design of the Hopfield architecture is such that a Hopfield net-\nwork cannot process sequences. Consequently, it is not considered a full RNN\narchitecture.\n5. I originally came across this Churchland quote in Marcus 2003 (p. 25).\n6. Critique of paper \u201cDeep Learning Conspiracy\u201d (Nature 521, p. 436), cri-\ntique posted by J\u00fcrgen Schmidhuber, June 2015, available at: http://people\n.idsia.ch/~juergen/deep-learning-conspiracy.html.\n7. There are a number of other ways that autoencoders can be constrained to\npreclude the possibility that the network will learn an uninformative identity\nmapping from inputs to outputs; for example, noise can be injected into the in-\nput patterns and the network can be trained to reconstruct the un- noisy data.\nAlternatively, the units in the hidden (or encoding) layer can be restricted\nto have binary values. Indeed, Hinton and his colleagues originally used net-\nworks called Restricted Boltzman Machines (RBMs) in their initial pretraining\nwork, which used binary units in the encoding layer.\n8. The number of layers trained during pretraining is a hyperparameter\nthat is set based on the intuition of the data scientist and trial- and- error\nexperimentation.\n9. As early as 1971, Alexey Ivakhnenko\u2019s GMDH method had been shown to\nbe able to train a deep network (up to eight layers), but this method had been\nlargely overlooked by the research community.\n10. Glorot initialization is also known as Xavier initialization. Both of these\nnames are references to one of the authors (Xavier Glorot) of the first paper\nthat introduced this initialization procedure: Xavier Glorot and Yoshua Bengio,\n\u201cUnderstanding the Difficulty of Training Deep Feedforward Neural Networks,\u201d\n258 notEs", "in Proceedings of the 13th International Conference on Artificial Intelligence and\nStatistics (AISTATS), 2010, pp. 249\u2013 256.\n11. Glorot initialization can also be defined as sampling the weights from\na Gaussian distribution with a mean of 0 and standard deviation set to the\nsquare root of 2 divided by n + n . However, both of these definitions of\nj j+1\nGlorot initialization have the same goal of ensuring a similar variance in acti-\nvations and gradients across the layers in a network.\n12. https://developer.nvidia.com/cuda-zone.\nChapter 5\n1. The explanation of LSTM units presented here is inspired by an excellent\nblog post by Christopher Olah, which explains LSTMs clearly and in detail; post\navailable at: http://colah.github.io/posts/2015\u201308-Understanding-LSTMs/.\n2. A sigmoid function is in fact a special case of the logistic function, and for\nthe purposes of this discussion the distinction is not relevant.\n3. If, for example, sigmoid units with an output range of 0 to 1 were used\nthen activations could only be either maintained or increased at each update\nand eventually the cell state would become saturated with maximum values.\nChapter 6\n1. This figure also appears in chapter 4 but it is repeated here for convenience.\nChapter 7\n1. http://www.image-net.org.\n2. https://en.wikipedia.org/wiki/Carver_Mead.\n3. https://www.humanbrainproject.eu/en/.\n4. Recitals are a non- legally binding section of a regulation that seeks to clar-\nify the meaning of the legal text.\n5. Laurens van der Maaten and Geoffrey Hinton, \u201cVisualizing Data using\nt- SNE,\u201d Journal of Machine Learning Research 9 (2008): 2579\u2013 2605.\nnotEs 259", "", "REFERENCES\nAizenberg, I. N., N. N. Aizenberg, and J. Vandewalles. 2000. Multi-V alued and\nUniversal Binary Neurons: Theory, Learning and Applications. Springer.\nChellapilla, K., S. Puri, and Patrice Simard. 2006. \u201cHigh Performance Convo-\nlutional Neural Networks for Document Processing.\u201d In Tenth International\nWorkshop on Frontiers in Handwriting Recognition.\nChurchland, P. M. 1996. The Engine of Reason, the Seat of the Soul: A Philosophi-\ncal Journey into the Brain. MIT Press.\nDechter, R. 1986. \u201cLearning While Searching in Constraint- Satisfaction-\nProblems.\u201d In Proceedings of the Fifth National Conference on Artificial\nIntelligence (AAAI- 86), pp. 178\u2013 183.\nDevlin, J., M. W. Chang, K. Lee, and K. Toutanova. 2018. \u201cBert: Pre- training\nof deep bidirectional transformers for language understanding.\u201d arXiv pre-\nprint arXiv:1810.04805.\nElgammal, A., B. Liu, M. Elhoseiny, and M. Mazzone. 2017. \u201cCAN: Creative\nAdversarial Networks, Generating \u2018Art\u2019 by Learning about Styles and Deviat-\ning from Style Norms.\u201d arXiv:1706.07068.\nElman, J. L. 1990. \u201cFinding Structure in Time.\u201d Cogn. Sci. 14: 179\u2013 211.\nFrid- Adar, M., I. Diamant, E. Klang, M. Amitai, J. Goldberger, and H. Greens-\npan. 2018. \u201cGAN- based Synthetic Medical Image Augmentation for Increased\nCNN Performance in Liver Lesion Classification.\u201d arXiv:1803.01229.\nFukushima, K. 1980. \u201cNeocognitron: A self- organizing neural network model\nfor a mechanism of pattern recognition unaffected by shift in position.\u201d Biol.\nCybern. 36: 193\u2013 202.\nGlorot, X., and Y. Bengio. 2010. \u201cUnderstanding the Difficulty of Train-\ning Deep Feedforward Neural Networks.\u201d In Proceedings of the Thirteenth\nInternational Conference on Artificial Intelligence and Statistics (AISTATS),\npp. 249\u20132 56.\nGlorot, X., A. Bordes, and Y. Bengio. 2011. \u201cDeep Sparse Rectifier Neural Net-\nworks.\u201d In Proceedings of the Fourteenth International Conference on Artificial\nIntelligence and Statistics (AISTATS), pp. 315\u2013 323.", "Goodfellow, I., Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.\nGoodfellow, I., J. Pouget- Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A.\nCourville, and J. Bengio. 2014. \u201cGenerative Adversarial Nets.\u201d In Advances in\nNeural Information Processing Systems 27: 2672\u2013 2680.\nHe, K., X. Zhang, S. Ren, and J. Sun. 2016. \u201cDeep Residual Learning for Image\nRecognition.\u201d In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE, pp. 770\u2013 778. https://doi.org/10.1109/CVPR.2016.90.\nHebb, D. O. 1949. The Organization of Behavior: A Neuropsychological Theory.\nJohn Wiley & Sons.\nHerculano- Houzel, S. 2009. \u201cThe Human Brain in Numbers: A Linearly\nScaled- up Primate Brain.\u201d Front. Hum. Neurosci. 3. https://doi.org/10.3389/\nneuro.09.031.2009.\nHinton, G. E., S. Sabour, and N. Frosst. 2018. \u201cMatrix Capsules with EM\nRouting.\u201d In Proceedings of the 7th International Conference on Learning Rep-\nresentations (ICLR).\nHochreiter, S. 1991. Untersuchungen zu dynamischen neuronalen Netzen\n(Diploma). Technische Universit\u00e4t M\u00fcnchen.\nHochreiter, S., Schmidhuber, J. 1997. \u201cLong Short- Term Memory.\u201d Neural\nComput. 9: 1735\u2013 1780.\nHopfield, J. J. 1982. \u201cNeural Networks and Physical Systems with Emergent\nCollective Computational Abilities.\u201d Proc. Natl. Acad. Sci. 79: 2554\u2013 2558.\nhttps://doi.org/10.1073/pnas.79.8.2554.\nHubel, D. H., and T. N. Wiesel. 1962. \u201cReceptive Fields, Binocular Interaction\nand Functional Architecture in the Cat\u2019s Visual Cortex.\u201d J. Physiol. Lond. 160:\n106\u20131 54.\nHubel, D. H., and T. N. Wiesel. 1965. \u201cReceptive Fields and Function Architec-\nture in Two Nonstriate Visual Areas (18 and 19) of the Cat.\u201d J. Neurophysiol.\n28: 229\u20132 89.\nIvakhnenko, A. G. 1971. \u201cPolynomial Theory of Complex Systems.\u201d IEEE\nTrans. Syst. Man Cybern. 4: 364\u2013 378.\nKelleher, J. D., and B. Tierney. 2018. Data Science. MIT Press.\nKrizhevsky, A., I. Sutskever, and G. E. Hinton. 2012. \u201cImagenet Classification\nwith Deep Convolutional Neural Networks.\u201d In Advances in Neural Information\nProcessing Systems, pp. 1097\u2013 1105.\n262 REFEREnCEs", "LeCun, Y. 1989. Generalization and Network Design Strategies (Technical\nReport No. CRG- TR- 89- 4). University of Toronto Connectionist Research\nGroup.\nMaas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. \u201cRectifier Nonlinearities Im-\nprove Neural Network Acoustic Models.\u201d In Proceedings of the Thirteenth Inter-\nnational Conference on Machine Learning (ICML) Workshop on Deep Learning\nfor Audio, Speech and Language Processing, p. 3.\nMacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.\nCambridge University Press.\nMarcus, G.F. 2003. The Algebraic Mind: Integrating Connectionism and Cognitive\nScience. MIT Press.\nMcCulloch, W. S., and W. Pitts. 1943. \u201cA Logical Calculus of the Ideas Imma-\nnent in Nervous Activity.\u201d Bull. Math. Biophys. 5: 115\u2013 133.\nMikolov, T., K. Chen, G. Corrado, and J. Dean. 2013. \u201cEfficient Estimation of\nWord Representations in Vector Space.\u201d arXiv:1301.3781.\nMinsky, M., and S. Papert. 1969. Perceptrons. MIT Press.\nMnih, V., K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,\nand M. Riedmiller. 2013. \u201cPlaying Atari with Deep Reinforcement Learning.\u201d\nArXiv13125602 Cs.\nNilsson, N. J. 1965. Learning Machines: Foundations of Trainable Pattern-\nClassifying Systems, Series in Systems Science. McGraw-H ill.\nOh, K.- S., and K. Jung. 2004. \u201cGPU Implementation of Neural Networks.\u201d\nPattern Recognit. 36: 1311\u2013 1314.\nOlah, C., A. Satyanarayan, I. Johnson, S. Carter, S. Ludwig, K. Ye, and A.\nMordvintsev. 2018. \u201cThe Building Blocks of Interpretability.\u201d Distill. https://\ndoi.org/10.23915/distill.00010.\nReagen, B., R. Adolf, P. Whatmough, G.- Y. Wei, and D. Brooks. 2017. \u201cDeep\nLearning for Computer Architects.\u201d Synth. Lect. Comput. Archit. 12: 1\u2013 123.\nhttps://doi.org/10.2200/S00783ED1V01Y201706CAC041.\nReed, R. D., and R. J. Marks II. 1999. Neural Smithing: Supervised Learning in\nFeedforward Artificial Neural Networks. MIT Press.\nRosenblatt, F. 1960. On the Convergence of Reinforcement Procedures in\nSimple Perceptrons (Project PARA). (Report No. VG- 1196- G- 4). Cornell Aero-\nnautical Laboratory, Inc., Buffalo, NY.\nREFEREnCEs 263", "Rosenblatt, F. 1962. Principles of Neurodynamics: Perceptrons and the Theory of\nBrain Mechanisms. Spartan Books.\nRosenblatt, Frank, 1958. \u201cThe Perceptron: A Probabilistic Model for Infor-\nmation Storage and Organization in the Brain.\u201d Psychol. Rev. 65: 386\u2013 408.\nhttps://doi.org/10.1037/h0042519.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986a. \u201cLearning Internal\nRepresentations by Error Propagation.\u201d In D. E. Rumelhart, J. L. McClelland,\nand PDP Research Group, eds. Parallel Distributed Processing: Explorations in\nthe Microstructure of Cognition, Vol. 1. MIT Press, pp. 318\u2013 362.\nRumelhart, D.E., J. L. McClelland, PDP Research Group, eds. 1986b. Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, Vol. 1:\nFoundations. MIT Press.\nRumelhart, D.E., J. L. McClelland, PDP Research Group, eds. 1986c. Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, Vol. 2:\nPsychological and Biological Models. MIT Press.\nSabour, S., N. Frosst, and G. E. Hinton. 2017. \u201cDynamic Routing Between\nCapsules.\u201d In Proceedings of the 31st Conference on Neural Information Proc-\nessing (NIPS). pp. 3856\u2013 3866.\nSchmidhuber, J. 2015. \u201cDeep Learning in Neural Networks: An Overview.\u201d\nNeural Netw. 61: 85\u2013 117.\nSteinkraus, D., Patrice Simard, and I. Buck. 2005. \u201cUsing GPUs for Machine\nLearning Algorithms.\u201d In Eighth International Conference on Document Analy-\nsis and Recognition (ICDAR\u201905). IEEE. https://doi.org/10.1109/ICDAR.2005\n.251.\nSutskever, I., O. Vinyals, and Q. V. Le. 2014. \u201cSequence to Sequence Learning\nwith Neural Networks.\u201d In Advances in Neural Information Processing Systems\n(NIPS), pp. 3104\u2013 3112.\nTaigman, Y., M. Yang, M. Ranzato, and L. Wolf. 2014. \u201cDeepFace: Closing\nthe Gap to Human- Level Performance in Face Verification.\u201d Presented at the\nProceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 1701\u2013 1708.\nvan der Maaten, L., and G. E. Hinton. 2008. \u201cVisualizing Data Using t- SNE.\u201d J.\nMach. Learn. Res. 9, 2579\u2013 2605.\n264 REFEREnCEs", "Vaswani, A., N. Shazer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kai-\nser, and I. Polosukhin. 2017. \u201cAttention Is All You Need.\u201d In Proceedings of\nthe 31st Conference on Neural Information Processing (NIPS), pp. 5998\u2013 6008.\nWerbos, P. 1974. \u201cBeyond Regression: New Tools for Prediction and Analysis\nin the Behavioral Sciences.\u201d PhD diss., Harvard University.\nWidrow, B., and M.E. Hoff. 1960. Adaptive Switching Circuits (Technical\nReport No. 1553- 1). Stanford Electronics Laboratories, Stanford University,\nStanford, California.\nXu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\nBengio, Y. 2015. \u201cShow, Attend and Tell: Neural Image Caption Generation\nwith Visual Attention.\u201d In Proceedings of the 32nd International Conference on\nMachine Learning, Proceedings of Machine Learning Research. PMLR, pp. 2048\u2013\n2057.\nREFEREnCEs 265", "", "FURTHER READINGS\nBooks on Deep Learning and Neural Networks\nCharniak, Eugene. 2018. Introduction to Deep Learning. MIT Press.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning.\nMIT Press.\nHagan, Martin T., Howard B. Demuth, Mark Hudson Beale, and Orlando De\nJes\u00fas. 2014. Neural Network Design. 2nd ed.\nReagen, Brandon, Robert Adolf, Paul Whatmough, Gu- Yeon Wei, and David\nBrooks. 2017. \u201cDeep Learning for Computer Architects.\u201d Synthesis Lectures on\nComputer Architecture 12 (4): 1\u2013 123.\nSejnowski, Terrence J. 2018. The Deep Learning Revolution. MIT Press.\nOnline Resources\nNielsen, Michael A. 2015. Neural Networks and Deep Learning. Determination\nPress. Available at: http://neuralnetworksanddeeplearning.com.\nDistill (an open access journal with many articles on deep learning and\nmachine learning). Available at: https://distill.pub.\nOverview Journal Articles\nLeCun, Yann, Yoshua Bengio, and Geoffrey E. Hintron. 2015. \u201cDeep Learn-\ning.\u201d Nature 521: 436\u2013 444.\nSchmidhuber, J\u00fcrgen. 2015. \u201cDeep Learning in Neural Networks: An Over-\nview.\u201d Neural Networks 61: 85\u2013 117.", "", "INDEX\nActivation functions. See also specific Attribution, 247\nfunctions Autoencoders, 144, 145\u2013 148\nbackpropagation and, 127 Axon, 65\u2013 66\ncommon characteristics, 79\ndefined, 251 Backpropagation\nderivative plot, 220 in deep learning history, 102,\nelement- wise application, 96 125\u2013 129\nhistory of, 158 defined, 251\nnecessity of, 77\u2013 80, 82 error gradients, 150\u2013 151\nneural network, 62 learning rule, 210\nneuron, 70\u2013 79, 127, 150\u2013 151 meanings of, 209\u2013 210\n\u03c6 notation, 96 ReLUs, 152\nshape of, 76, 79 RNNs, 175\u2013 176\nweight adjustments, 207 training neural networks, 138,\nActivation space, 59\u2013 61 209\u2013 210\nAdaptive linear neuron (ADALINE) the \u03b4 s, 216\u2013 222\nnetwork, 116\u2013 117 Backpropagation algorithm\nAizenberg, I. N., 143 background, 125\u2013 126\nAlexNet, 102, 138, 169\u2013 170, 233 backward pass, 126, 211\u2013 213, 215\nAlgorithms, 7\u2013 8. See also blame assignment, 126, 213\u2013 214\nBackpropagation algorithm; credit assignment problem,\nGradient descent algorithm; solving the, 125, 186, 209\u2013 210\nLeast mean squares (LSM) described, 209\u2013 210\nalgorithm; Machine learning error gradients, 211\u2013 213\n(ML) algorithm error propagation, 128\u2013 129\nAlphaGo (Deep Mind), 2, 4 forward pass, 126, 211, 214\nAND function, 119\u2013 119, 133 iteration steps, 213\u2013 214\nArtificial intelligence threshold activation function\nbackground, 4, 6\u2013 8 in, 127\nchallenges, 246 training function, 127, 186\ndefined, 251 two- stage, 126, 210\u2013 215\nmachine learning and, 4, 6 weight adjustments, 126\u2013 127,\nrelationships, 6, 10 222\u2013 230\nAssumptions, encoded, 18, 21 Backward pass, 126, 211\u2013 213, 215", "Baidu, 1 Compute unified device architecture\nBengio, Yoshua, 231 (CUDA), 154\nBias Connectionism, 124, 129\u20131 41,\nin artificial neurons, 88 156\u2013 157\ninductive, 17\u2013 22 Connection weights, 70\npermissive, 20 Consumer devices, 37\npreference, 19\u2013 20 Convolutional layer, 168\u2013 170\nrestriction, 19 Convolutional neural network\nBias term, 88\u2013 92 (CNN)\nBidirectional Encoder architecture, 182\nRepresentations from convolution operation, 165\nTransformers (BERT) model, in deep learning history, 133\u2013 143\n240 defined, 252\nBig data design goal, 160\ndeep learning\u2019s impact on, 35 feature maps, 165\u2013 166, 168\ndriving algorithmic innovation, functions, 160\n232\u2013 237 kernels, 165, 168\u2013 169\nemergence of, 23 limitations, 237\u2013 238\nBiological neurons, 241 output, 165\nBlame assignment, 123, 126, pooling function, 166, 168\u2013 170,\n213\u2013 214 238\u2013 239\nBMI example, 32 processing stages, 163\u2013 164, 168\nBox, George, 40 receptive field, 162\u2013 166\nBrain, human, 65\u20136 7, 238 training, 153\ntranslation invariance, 161\u2013 163\nCandidate functions, 25\u2013 26, 28 visual feature detection, 160\u2013 163,\nCapsule networks, 237\u2013 239 168\u2013 170\nCars, self- driving, 1 Convolutional neural network\nCells, LSTM, 177\u2013 178, 180 (CNN) models\nChain rule, 128 pooling function, 238\u2013 239\nChellapilla, K., 154 transfer learning, 236\u2013 237\nChess, 2\u2013 4 visual feature detection, 236,\nChurchland, P. M., 140 238\nCivil liberties, 37, 245 Convolution mask, 165\nComplex cells, 135\u2013 136 Credit assignment problem, defined,\nComplex models, 62 123\nComputer games. See Game playing Credit assignment problem, solving\nComputer power, growth of, the. See also Loan decision\n153\u2013 155 model\n270 IndEX", "algorithms in, 7 growth, 241, 248\nbackpropagation algorithm, 125, high- dimensional, 35\n186, 209\u2013 210 historic, creating, 30\ndataset example, 6, 7, 27, 49, 51, large, 22\u2013 23, 35\n53 in machine learning, 6\u2013 7\nfunctions in the, 8, 10 modeling, 194\u2013 196\nmodeling, 27, 46, 48\u2013 55, 73 parameters, modifying to fit the\nweight adjustments, 51, 71, model, 49\u2013 54\n210\u2013 211 simplest form, 6\u2013 7, 24, 26\nCybernetics, 102 single input- output, 187\u2013 188\nDataset sizes, increases in, 153\u2013 155,\nData 233\u2013 235\nanalyzing for customer Dechter, Rina, 143\nsegmentation, 28 Decision boundaries, two- input\nclustering, 28\u2013 29 neurons, 84\u2013 91\nextracting accurate functions Decision- making\nfrom, 14 automated, GDPR rights,\nlearning patterns from, algorithm 245\u2013 246\nfor, 185 data- driven, 1, 3, 4\u2013 5\nneural network training on, 122, intuitive, 4, 22\n185 Decision space, 59\u2013 60. See also\nnoise in, 16, 20 Activation space\noverfitting/underfitting, 20, 22 Decoder, 142, 182\u2013 183\npersonal, protections for, 37, DeepBlue, 3\n245\u2013 246 DeepFace, 23\nunderfitting, 77\u2013 78 Deep fakes, 235\u2013 236\nData annotation costs, 233 Deep learning\nData-d riven decisions, enabling, 3 benefits, 248\u2013 250\nData labeling bottleneck, 144, 236 data- driven decision making, 1,\nDataset analysis algorithms, 7\u2013 8 3, 4\u2013 5\nDataset design, 25, 32, 34\u2013 35 defined, 252\nDatasets development drivers, 232\nannotated, 233\u2013 235 emergence of, 23\ncredit assignment problem, era of, 143\u2013 144\nsolving the, 6, 7, 27, 49, 51, 53 examples, 1\u2013 2\ndefined, 252 power of, 183\nerror of a model on, 190\u2013 191 relationships, 6, 10\nfeature selection, tradeoffs in, success, factors in, 32\u2013 35\n24\u2013 25 summary overview, 36\u2013 37\nIndEX 271", "Deep learning (cont.) perceptrons, multilayer, 124\nterm use, 143 perceptron training model, 103,\nusefulness, 4, 248 105\u2013 113, 116\nusers of, 1\u2013 2 periods in, 101\nDeep learning, future of ReLU activation functions, 148,\nbig data driving algorithmic 150\u2013 152\ninnovation, 232\u2013 237 RNNs, 103, 133\u2013 143, 173\ninterpretability, challenge of, seq2seq, 103, 142\n244\u2013 248 summary overview, 155\u2013 158\nnew hardware, 240\u2013 244 themes within, 101\u2013 102\nnew models, emergence of, threshold logic units, 103,\n237\u2013 240 104\u2013 105\nsummary overview, 248, 250 timeline, 103\nDeep learning, history of vanishing gradients, 103,\nbackpropagation, 103, 125\u2013 129 125\u2013 129\nCNNs, 102 virtuous cycle, 153\u2013 155\ncomputer power, growth of, weight initialization, 148,\n153\u2013 155 150\u2013 152\nconnectionism, 124, 129\u2013 133 XOR problem, 103, 116\u2013 123\ndataset sizes, increases in, Deep learning architectures. See\n153\u2013 155 Capsule network; Convolutional\ndeep learning era, 103, 143\u2013 144 neural network (CNN);\nElman network, 103, 139\u2013 140 Generative adversarial network\nGlorot initialization, 103, 148, (GAN); Long short- term\n150 memory (LSTM) network;\nGPUs, 103, 153 Recurrent neural network\nHebb\u2019s postulate, 103, 104\u2013 105 (RNN); Transformer model\nlayer- wise pretraining, 103 Deep learning- GPU relation, 97\nlayer- wise pretraining using Deep learning models\nautoencoders, 145\u2013 148 feature learning function, 36\u2013 37\nLMS algorithm, 103, 123 new, emergence of, 237\u2013 240\nlocal vs. distributed training, 31\nrepresentations, 129\u2013 133 usefulness, 156\nLSTM algorithm, 103, 113\u2013 116 Deep learning networks\nMcCulloch & Pitts model, 103, components, 68\n104 defined, 39, 68\nneocognitron, 103 neuron hidden layers in,\nnetwork architectures, 133\u2013 143, 67\u2013 68summary overview,\n173 98\u2013 100\n272 IndEX", "training, 97, 127\u2013 129, 147, 150, Feature map, 165\u2013 166, 168\n170 Feature selection, 32\nDeepMind, 2, 31 Feature vector, 622\nDelta rule, 114, 204 Feature visualization, 246\u2013 248\n\u03b4 s, backpropagating the, 216\u2013 222 Feedforward network\nDendrite, 65\u2013 66 defined, 252\nDense layer, 169\u2013 170 dense layer, 168\u2013 169\nDimensionality reduction, 247 fully connected, 133\u2013 134, 169\nDiscriminative models, 235 neuron inputs and outputs, 92\ndistributed representation, standard, 92, 169\n129\u2013 132, 142, 243 training, 134, 151\nDivide-a nd conquer strategy, 10, Filter vector, 179\n79\u2013 82 Fitness functions, 26\u2013 27\nDNA sequencing, 248 Forget gate, 177\u2013 178\nDot product operation, 87\u2013 88 Forward pass, 126, 211, 214\nFukushima, Kunihiko, 136\u2013 137\nEarth orbit telescopes, 248 Fully connected networks, 133\u2013 134\nElman, Jeffrey Locke, 139\u2013 140 Functions. See also specific functions\nElman network, 103, 139\u2013 141 defined, 4, 14, 252\nElo rating, 3 encoded, 12\nEncoder, 142, 182\u2013 183 equation of a line to define a,\nEncoder- decoder architecture, 18\n182\u2013 183, 244 examples, 15, 21\nError, calculating, 190\u2013 191 if- then- else rules, 19\nError curves, 197\u2013 198 in machine learning, 7\u2013 8\nError gradients, 211\u2013 211 mathematical model vs., 40\nError signals, 128\u2013 129 models vs., 13\nError surface, 192, 193, 194\u20131 96, nonlinear as activation function,\n198 77\nEthics regulation, 245 partial derivatives, 199\u2013 200\nrate of change, 199\nFacebook, 1, 23, 156 representing, 8\nFace recognition simpler, 19\nCNNs for, 160\u2013 163, 168\u2013 169, 238 template structure defining,\nspatially invariant, 136 18\u2013 19\ntransfer learning for, 236\nFace recognition function, 15 Game playing, 2\u2013 4, 29, 31\nFace- recognition software, 23, 35, Gates, LSTM networks, 177\u2013 178\n156 Gene prediction function, 15\nIndEX 273", "General Data Protection Regulations Happiness- income example, 41\u2013 43\n(GDPR), 245\u2013 246 Hardware energy costs, 241\nGenerative adversarial networks Healthcare sector, 1\n(GANs), 235 Hebb, Donald O., 104\u2013 105\nGenerative models, 235 Hebb\u2019s postulate, 103, 104\u2013 105\nGeometric spaces, 59\u2013 63 Hiker example of gradient descent,\nGlorot, X., 148 196\u2013 197\nGlorot initialization, 103, 148, 150 Hinge activation function, 73\nGo, 2\u2013 4 Hinton, Geoffrey E., 125, 144, 231,\nGoogle, 1, 30, 156 235\nGradient descent, 260 Hochreiter, Sepp, 128, 141\nGradient descent algorithm Hoff, Marcian, 113\u2013 114, 116\ncomponents, 197 Hopfield, John, 124\ndefined, 252 Hopfield network, 103, 124\u2013 125\ndescending error surfaces, 203, Hubel, D. H., 134\u2013 137\n205\u2013 206 Human Brain Project, 243\nerror curves, 197\u2013 198, 205\u2013 206 Hyperparameter, 80, 100\ngoal of, 197\nhiker example, 196\u2013 197 IBM, 244\ninitial model, creating, 194, 196 If- then- else rules, 19\nsimplifying factors, 200 ill- posed problem, 16\u2013 17\nsummary, 204\u2013 205 Image captioning systems,\ntraining function, 185\u2013 186, 208 automatic, 182\nweight updates, 51, 53\u2013 56, Image map, 170\n197\u2013 208 ImageNet, 233, 236\u2013 237\nGraphical processing unit (GPU) ImageNet Large- Scale Visual\naccelerating training, 92\u2013 98 recognition Challenge (ILSVRC),\nadoption of, 240\u2013 241 138, 169\u2013 170, 233\nin deep learning history, 103, Image processing, 134\u2013 138,\n153\u2013 154 236\u2013 237. See also Face\ndefined, 253 recognition\nmanufacturing, 98 Image recognition, 136\nGreedy layer- wise pretraining, 144, Income- happiness relation, 41\u2013 43\n147 Inductive bias, 17\u2013 22\nGroup method for data handling Inference, 12, 14\u2013 15, 20, 29\n(GMDH) network, 103, 122 Information flows\ninterpreting, 247\nHandwritten digit recognition, 160, neural networks, 68, 70\n239 RNNs, 139, 171, 173\n274 IndEX", "Information processing intercept- slope changing a,\nneurons, artificial, 70\u2013 77 189\u2013 190\nunderstanding, 246\u2013 247 mapping function, 187\u2013 189\nInput gate, 177\u2013 178 Linear activation function. See\nInput- output mapping, 10\u2013 11 also Rectified linear activation\nInput space function\nloan decision model, 57\u2013 58, in deep learning history, 73\n83\u2013 84 equation of a line representing,\ntwo- input neurons, 84, 85, 86 188\u2013 189\nInput vector, 62 Linearly separable functions,\nIntel Labs, 244 117\u2013 119\nIntercept, 43, 46, 188\u2013 189 Linear models\nInterpretability, challenge of, combining, 54\u2013 57, 62\n244\u2013 248 credit solvency example, 44\u2013 48,\nIntuition, 4, 22 49, 54\u2013 60, 62\u2013 63\nIvakhenko, Alexey, 122 error variation, 192, 193, 194\nincome- happiness relation, 41\u2013 43\nJung, K., 153 learning weights in, 49\u2013 54\nmodeling nonlinear relations,\nKasparov, Gary, 3 77\u2013 78\nKe Jie, 2 with multiple inputs, 44\u2013 46\nKernels, 165, 168\u2013 169 parameter setting, 46, 48\u2013 49,\n61\u2013 62\nLanguage processing, 142, 240 summary overview, 61\u2013 63\nLarge Hadron Collider, 248 templates, 41\u2013 44\nLayer- wise pretraining, 103, Loan decision model. See also Credit\n144\u2013 148 assignment problem\nLearning. See specific types of coordinate spaces, 59\nLearning rate (\u019e), 110\u2013 112, dataset example, 6, 7\n204 input space, 57\u2013 58, 83\u2013 84\nLeast mean squares (LSM) two- input, 83\u2013 84\nalgorithm, 103, 113\u2013 116, 123, weights, adjusting, 84, 107\u2013 108\n185, 204 Localist representation, 129,\nLeast mean squares (LSM) rule, 131\u2013 132, 243\n123 \u201cLogical Calculus of the Ideas\nLeCun, Yann, 138, 161, 166, 231 Immanent in Nervous Activity,\nLine A\u201d (McCulloch & Pitts), 104\nbest fit, 187 Logistic activation function, 152\nequation of a, 18, 41\u2013 43, 188\u2013 190 Logistic units, 75, 80, 235\nIndEX 275", "Loihi chip, 247 candidate functions, 23, 25\u2013 26,\nLong short- term memory (LSTM), 28\n103, 141\u2013 142, 253 data, 23\u2013 25\nLong short- term memory (LSTM) fitness functions, 26\u2013 27\nnetwork cells, 177\u2013 178, 180 fitness measures, 24\nLong short- term memory (LSTM) Machine translation, 15, 35, 142,\nnetworks, 177\u2013 178, 181\u2013 183 181\u2013 182\nMapping\nMacHack-6 (MIT), 3 deterministic, 7\nMachine learning (ML) nonlinear, 76, 78, 79\nartificial intelligence and, 4, 6 Mathematical model, 40\nbenefits, 248\u2013 250 Matrix multiplication, 72\ndefined, 253 max pooling, 166\ndifficulty factors in, 16\u2013 17 McCulloch, Walter, 103\u2013 104\nfeature selection and design, 32, Mead, Carver, 241\n34 Medical images, synthesizing, 235\nfunctions, 10\u2013 11 Memory. See also Long short- term\ngoal of, 8 memory (LSTM)\nreinforcement, 29\u2013 31 associative, 148\u2013 125\nrelationships, 6, 10 forward pass stored in, 211\nin situ, 30 RNN, 139, 170\u2013 177\nsummary overview, 36\u2013 37 Microsoft, 1\nsupervised, 27\u2013 31 Microsoft Research, 170\ntraining model, 12\u2013 14 Mikolov, Tomas, 181\nunderstanding, 6\u2013 9, 10\u2013 11 Minsky, Marvin, 116\u2013 120, 122\nMachine learning (ML) algorithm MIT, 3\nassumptions, 18, 21 MNIST handwritten digit\nbias in, 17\u2013 22 recognition dataset, 239\ndefined, 10, 253 Mobile phones, 1\nill- posed problems, solving, 17 Model parameters, 48\u2013 54, 9\nsources of information to select Models\nthe best function, 17\u2013 18 complex, 56, 62\nsuccess criterion, 21\u2013 22 defined, 12, 253\ntemplate structure defining, equation of a line defining, 41\u2013 44\n18\u2013 19 fixed, 14\nMachine learning (ML) models, functions vs., 13\n28\u2013 30, 143 geometric spaces, 57\u2013 61\nMachine learning (ML) success real- world correspondence, 40\u2013 41\nfactors templates, 40\u2013 43\n276 IndEX", "training, 12\u2013 14 tailoring, 152\nusefulness, 40 weighted sum calculations, 80\u2013 82\nvariables in, 40\u2013 41 Neural network model\nbias in, 22\nNatural language processing (NLP), data, overfitting vs. underfitting,\n181\u2013 182 22\nNeocognitron, 103, 136 datasets, suitability to large,\nNetwork architectures 22\u2013 23\nconvolutional neural, 133\u2013 143 function, 13, 185\nin deep learning history, 133\u2013 143 training, 23, 80, 185\nencoder- decoder, 240 Neural network training\nrecurrent neural, 133\u2013 143 accelerating using GPUs, 92\u2013 98\nNetwork error, 210\u2013 213, 222\u2013 225 backpropagation for, 209\u2013 210\nNeural machine translation, 156 on data, 122, 193\nNeural network deep neural networks, 127\u2013 128,\nactivation function, 62 185\u2013 186\nartificial, 67\u2013 68, 70 hardware to speed up, 153\u2013 154\ncompositional nature, 99 with multiple layers, 120, 208\nconnection weights, 70 Neural network training model, 23,\ndefined, 65, 262 82, 185\ndepth, 97 Neuromorphic computing, 241\u2013 244,\ndesigning, 157\u2013 158 254\nfunctions, 78\u2013 79 Neurons\ngeometric spaces, 57\u2013 61 activation function, 61\u2013 62, 71\u2013 77,\ngraphic representation, 95, 96 127\nhuman brain, analogy to the, 67 artificial, 70\u2013 77, 91\ninformation flows, 68, 70 changing parameters effect on\nlearning functions, 10 behavior, 82\u2013 91\nlearning nonlinear mapping, 79 defined, 76, 254\nmatrix representation, 95, 96, 98 feedforward network, 92\nmodeling relationships, 78\u2013 79 function, 8, 56\nneurons in complex models, hidden layers, 69\n56\u2013 57 human brain, 65\u2013 67\nparameters, 82\u2013 83, 99\u2013 100 information processing, 70\npower of, 67, 79 input- output mapping, 8, 70\u2013 71\nschematic, 10 parameters, 82\nsimple, topological illustration, 68 receptive fields, 134\u2013 137, 162\nsize, growth in, 97\u2013 98 sensing, 68, 70\nstructure, 8\u2013 9, 67\u2013 68 sequence of operations, 71\u2013 77\nIndEX 277", "Neurons (cont.) Perceptron training model, 105\u2013 113,\nstructure, 65\u2013 66 116\nthreshold functions, 62 Permissive bias, 20\nweight- output relation, 82 Personal data protections, 245\nNeurons, two- input \u03c6 symbol, 72, 94\ndecision boundaries, 84, 85, 90, Picasso problem, 237\u2013 238\n91 Pitts, Walter, 103\u2013 105\ninput space, 84, 85, 86 Planar models, 44\nloan decision model equivalence, Pooling function, 166, 168\u2013 172,\n84 238\u2013 239\nNilsson, N. J., 102 Positive linear activation function,\nNoise in data, 20 73\nNonlinear activation function, 165 Preference bias, 19\u2013 20\nNonlinear models, 77\u2013 78 Pretraining, term use, 146\nNVIDIA, 154 Privacy rights, 37, 245\nProblems, ill- posed, 16\u2013 17\nOh, K.- S., 153 Problem solving, neural networks,\nOlah, Chis, 246 79\nOptimization algorithm, 197 PyTorch, 241\nOR function, 117\u2013 118, 133\nOrganization of Behavior, The (Hebb), Quantum computing, 244\n104 Qubit, 244\nOutput gate, 177\u2013 178 Quetelet, Adolphe, 33\nOutput vector, 180\u2013 181\nOverfitting, 20, 22, 254 Reasoning, inductive, 17\nReceptive field, 134\u2013 137, 162\u2013 168\nParallel Distributed Processing Recital 69, 245\n(PDP), 125 120\u2013 124, 126 Rectified linear activation function,\nPapert, Seymour, 117, 119\u2013 122 73, 74, 165\u2013 166\nPerceptron Rectified linear units (ReLUs), 80,\nin deep learning history, 103 255\nmultilayer, 124 Rectifier activation function, 79, 80\nsingle layer, limitations of, 117, Recurrent neural network (RNN)\n119, 122\u2013 123 constructing a, 180\nPerceptron convergence theorem, in deep learning history, 133\u2013 143\n112 defined, 254\nPerceptron learning rule, 185 depth, 171\u2013 172\nPerceptrons (Minsky & Papert), functions, 170\n116\u2013 117 hidden layers, 170\u2013 177\n278 IndEX", "information flows, 171, 173 Support vector machines (SVMs),\nlayer connections, 175\u2013 176 143\nmemory buffer, 170\u2013 177\nstructure, 173 Tangle Lake chip, 244\nunrolled through time, 174 Tanh activation function, 73, 74, 76,\nvanishing gradient problem in, 79, 127, 150\u2013 151\n141, 175 Tanh layer, 180\nReinforcement learning, 29\u2013 31, Tan units, 179\u2013 180\n254 Target attributes, 27\u2013 28, 255\nRepresentation learning, 132 Templates, 18\u2013 19\nRepresentations, localist vs. TensorFlow, 241\ndistributed, 129\u2013 133 Threshold activation function,\nResNet, 170, 233, 237 73\u2013 75, 78\u2013 80, 83, 127\nRestriction bias, 19 Threshold logic units, 103\u2013 105\nRobot control, 30 Training model, 12\u2013 14, 31, 82\nRosenblatt, Frank, 106\u2013 113, 116 Transfer learning, 236\u2013 237\nRumelhart, D. E., 125 Transformer model, 239\u2013 240\nTrueNorth chip (IBM), 243\u2013 244\nSaliency, 247 T- SNE, 248\nSchmidhuber, J\u00fcrgen, 127, 141 Tuning phase, 145\u2013 147\nSedol, Lee, 2 Two- input neurons. See Neurons,\nSentence generation, 139\u2013 140, two- input\n181\u2013 182 Two- stage backpropagation\nseq2seq, 103, 181 algorithm, 126, 210\u2013 215\nseq2seq architecture, 142\nSequential data, 170 Underfitting, 22, 77\u2013 78, 255\nSimple cells, 135\u2013 136 Units, 79. See also Neurons\nSimplicity, 19 Unsupervised learning, 28\u2013 30, 233,\nSkip- connections, 170 237, 255\nSlope parameter, 43, 188\u2013 189 Update vector, 179\nSpam filtering, 15, 21\nSpeech recognition, 1, 15 Vanishing gradient problem\nSpiking neurons, 241\u2013 242 in deep learning history, 103,\nSteinkraus, D., 154 125\u2013 129, 143\n\u2211 symbol, 45, 72 defined, 129, 255\nSum of squared errors (SSE), Elman network, 139\n190\u2013 192, 193, 194\u2013 203 LSTM networks, 177\nSupervised learning, 27\u2013 30, overcoming the, 147\u2013 148\n232\u2013 233, 255 in RNNs, 141, 176\nIndEX 279", "Variables in models, 40\u2013 41 Widrow- Hoff learning rule, 114, 204\nVectors, 62, 86\u2013 88 Wiesel, T. N., 134\u2013 137\nVery- large- scale integrated (VLSI) Williams, R. J., 125\ncircuit, 241 word2vec models, 180\u2013 181\nVirtuous cycle, 153\u2013 155\nVisual cortex experiments, 134\u2013 136 XOR function, 103, 119, 133\nVisual feature detection XOR problem, 103, 116\u2013 123\nCNNs for, 160\u2013 163, 168\u2013 169, 238\nspatially invariant, 136\ntransfer learning for, 236\nVisual feature detection function,\n15, 236, 238\nVisual feature detection software,\n15, 23, 35, 156\nVisualization techniques, 246\u2013 248\nWeight adjustment\nactivation functions and, 207\nbackpropagation algorithm,\n126\u2013 127, 222\u2013 230\ncredit assignment problem, 123,\n210\u2013 211\nWeight initialization, 148, 153, 155\nWeighted sum, 46, 47, 48, 61\u2013 64, 71\nWeighted sum calculations\nbias term in, 88\nneural networks, 80\u2013 82\nin a neuron, 82\nneuron layer, 92\u2013 97\nWeighted summation function, 98\nWeights\nerror gradients, adjusting, 209,\n211\nupdating, 53\u2013 54\nWeight space, 58\u2013 60, 192, 193, 194\nWeight update rule, 197\u2013 208\nWeight update strategy, 108\u2013 112\nWeight vector, 86\u2013 89\nWidrow, Bernard, 113\u2013 114, 116\n280 IndEX", "", "", "The MIT Press Essential Knowledge Series\nAuctions, Timothy P. Hubbard and Harry J. Paarsch\nThe Book, Amaranth Borsuk\nCarbon Capture, Howard J. Herzog\nCloud Computing, Nayan B. Ruparelia\nComputational Thinking, Peter J. Denning and Matti Tedre\nComputing: A Concise History, Paul E. Ceruzzi\nThe Conscious Mind, Zoltan E. Torey\nCrowdsourcing, Daren C. Brabham\nData Science, John D. Kelleher and Brendan Tierney\nDeep Learning, John D. Kelleher\nExtremism, J. M. Berger\nFood, Fabio Parasecoli\nFree Will, Mark Balaguer\nThe Future, Nick Montfort\nGPS, Paul E. Ceruzzi\nHaptics, Lynette A. Jones\nInformation and Society, Michael Buckland\nInformation and the Modern Corporation, James W. Cortada\nIntellectual Property Strategy, John Palfrey\nThe Internet of Things, Samuel Greengard\nMachine Learning: The New AI, Ethem Alpaydin\nMachine Translation, Thierry Poibeau\nMemes in Digital Culture, Limor Shifman\nMetadata, Jeffrey Pomerantz\nThe Mind\u2013 Body Problem, Jonathan Westphal\nMOOCs, Jonathan Haber\nNeuroplasticity, Moheb Costandi\nNihilism, Nolen Gertz\nOpen Access, Peter Suber\nParadox, Margaret Cuonzo\nPhoto Authentication, Hany Farid\nPost-T ruth, Lee McIntyre\nRobots, John Jordan\nSchool Choice, David R. Garcia\nSelf- Tracking, Gina Neff and Dawn Nafus\nSexual Consent, Milena Popova\nSpaceflight, Michael J. Neufeld\nSustainability, Kent E. Portney\nSynesthesia, Richard E. Cytowic\nThe Technological Singularity, Murray Shanahan\n3D Printing, John Jordan\nUnderstanding Beliefs, Nils J. Nilsson\nWaves, Frederic Raichlen", "John d. kELLEhER is a Professor of Computer Science and the Academic\nLeader of the Information, Communication and Entertainment (ICE) research\ninstitute at the Technological University Dublin (TU Dublin). He has over\ntwenty years\u2019 experience in research and teaching in the fields of artificial\nintelligence, natural language processing, and machine learning. He has pub-\nlished more than a hundred academic articles in these fields, and two MIT\nPress books: Data Science (2018) and Fundamentals of Machine Learning for Pre-\ndictive Data Analytics (2015). His research is supported by the ADAPT Research\nCentre (https://www.adaptcentre.ie), which is funded by Science Foundation\nIreland (Grant 13/RC/2106) and is co- funded by the European Regional De-\nvelopment fund, and by PRECISE4Q project (https://precise4q.eu), which is\nfunded through the European Union\u2019s Horizon 2020 research and innovation\nprogram under grant agreement No. 777107."]