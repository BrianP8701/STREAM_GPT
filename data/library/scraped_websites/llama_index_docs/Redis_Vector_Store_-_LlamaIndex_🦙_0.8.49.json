{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/RedisIndexDemo.html",
        "title": "Redis Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Redis Vector Store[\uf0c1](#redis-vector-store \"Permalink to this heading\")\n\nIn this notebook we are going to show a quick demo of using the RedisVectorStore.\n\nimport os\nimport sys\nimport logging\nimport textwrap\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\\# stop huggingface warnings\nos.environ\\[\"TOKENIZERS\\_PARALLELISM\"\\] \\= \"false\"\n\n\\# Uncomment to see debug logs\n\\# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, Document\nfrom llama\\_index.vector\\_stores import RedisVectorStore\nfrom IPython.display import Markdown, display\n\n## Start Redis[\uf0c1](#start-redis \"Permalink to this heading\")\n\nThe easiest way to start Redis as a vector database is using the [redis-stack](https://hub.docker.com/r/redis/redis-stack) docker image.\n\nTo follow every step of this tutorial, launch the image as follows:\n\ndocker run \\--name redis-vecdb \\-d \\-p 6379:6379 \\-p 8001:8001 redis/redis-stack:latest\n\nThis will also launch the RedisInsight UI on port 8001 which you can view at http://localhost:8001.\n\n## Setup OpenAI[\uf0c1](#setup-openai \"Permalink to this heading\")\n\nLets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt.\n\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-<your key here>\"\n\n## Read in a dataset[\uf0c1](#read-in-a-dataset \"Permalink to this heading\")\n\nHere we will use a set of Paul Graham essays to provide the text to turn into embeddings, store in a `RedisVectorStore` and query to find context for our LLM QnA loop.\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\nprint(\n    \"Document ID:\",\n    documents\\[0\\].doc\\_id,\n    \"Document Hash:\",\n    documents\\[0\\].doc\\_hash,\n)\n\nDocument ID: faa23c94-ac9e-4763-92ba-e0f87bf38195 Document Hash: 77ae91ab542f3abb308c4d7c77c9bc4c9ad0ccd63144802b7cbe7e1bb3a4094e\n\nYou can process your files individually using SimpleDirectoryReader:\n\nloader \\= SimpleDirectoryReader(\"../data/paul\\_graham\")\ndocuments \\= loader.load\\_data()\nfor file in loader.input\\_files:\n    print(file)\n    \\# Here is where you would do any preprocessing\n\n## Initialize the Redis Vector Store[\uf0c1](#initialize-the-redis-vector-store \"Permalink to this heading\")\n\nNow we have our documents read in, we can initialize the Redis Vector Store. This will allow us to store our vectors in Redis and create an index.\n\nBelow you can see the docstring for `RedisVectorStore`.\n\nprint(RedisVectorStore.\\_\\_init\\_\\_.\\_\\_doc\\_\\_)\n\nInitialize RedisVectorStore.\n\n        For index arguments that can be passed to RediSearch, see\n        https://redis.io/docs/stack/search/reference/vectors/\n\n        The index arguments will depend on the index type chosen. There\n        are two available index types\n            - FLAT: a flat index that uses brute force search\n            - HNSW: a hierarchical navigable small world graph index\n\n        Args:\n            index\\_name (str): Name of the index.\n            index\\_prefix (str): Prefix for the index. Defaults to \"llama\\_index\".\n                The actual prefix used by Redis will be\n                \"{index\\_prefix}{prefix\\_ending}\".\n            prefix\\_ending (str): Prefix ending for the index. Be careful when\n                changing this: https://github.com/jerryjliu/llama\\_index/pull/6665.\n                Defaults to \"/vector\".\n            index\\_args (Dict\\[str, Any\\]): Arguments for the index. Defaults to None.\n            metadata\\_fields (List\\[str\\]): List of metadata fields to store in the index\n                (only supports TAG fields).\n            redis\\_url (str): URL for the redis instance.\n                Defaults to \"redis://localhost:6379\".\n            overwrite (bool): Whether to overwrite the index if it already exists.\n                Defaults to False.\n            kwargs (Any): Additional arguments to pass to the redis client.\n\n        Raises:\n            ValueError: If redis-py is not installed\n            ValueError: If RediSearch is not installed\n\n        Examples:\n            >>> from llama\\_index.vector\\_stores.redis import RedisVectorStore\n            >>> # Create a RedisVectorStore\n            >>> vector\\_store = RedisVectorStore(\n            >>>     index\\_name=\"my\\_index\",\n            >>>     index\\_prefix=\"llama\\_index\",\n            >>>     index\\_args={\"algorithm\": \"HNSW\", \"m\": 16, \"ef\\_construction\": 200,\n                \"distance\\_metric\": \"cosine\"},\n            >>>     redis\\_url=\"redis://localhost:6379/\",\n            >>>     overwrite=True)\n\n        \n\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\nvector\\_store \\= RedisVectorStore(\n    index\\_name\\=\"pg\\_essays\",\n    index\\_prefix\\=\"llama\",\n    redis\\_url\\=\"redis://localhost:6379\",  \\# Default\n    overwrite\\=True,\n)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\nWith logging on, it prints out the following:\n\nINFO:llama\\_index.vector\\_stores.redis:Creating index pg\\_essays\nCreating index pg\\_essays\nINFO:llama\\_index.vector\\_stores.redis:Added 15 documents to index pg\\_essays\nAdded 15 documents to index pg\\_essays\nINFO:llama\\_index.vector\\_stores.redis:Saving index to disk in background\n\nNow you can browse these index in redis-cli and read/write it as Redis hash. It looks like this:\n\n$ redis-cli\n127.0.0.1:6379> keys \\*\n 1) \"llama/vector\\_0f125320-f5cf-40c2-8462-aefc7dbff490\"\n 2) \"llama/vector\\_bd667698-4311-4a67-bb8b-0397b03ec794\"\n127.0.0.1:6379> HGETALL \"llama/vector\\_bd667698-4311-4a67-bb8b-0397b03ec794\"\n...\n\n## Handle duplicated index[\uf0c1](#handle-duplicated-index \"Permalink to this heading\")\n\nRegardless of whether overwrite=True is used in RedisVectorStore(), the process of generating the index and storing data in Redis still takes time. Currently, it is necessary to implement your own logic to manage duplicate indexes. One possible approach is to set a flag in Redis to indicate the readiness of the index. If the flag is set, you can bypass the index generation step and directly load the index from Redis.\n\nimport redis\nr \\= redis.Redis()\nindex\\_name \\= \"pg\\_essays\"\nr.set(f\"added:{index\\_name}\", \"true\")\n\n\\# Later in code\nif r.get(f\"added:{index\\_name}\"):\n    \\# Skip to deploy your index, restore it. Please see \"Restore index from Redis\" section below. \n\n## Query the data[\uf0c1](#query-the-data \"Permalink to this heading\")\n\nNow that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for ChatGPT. The default setting for as\\_query\\_engine() utilizes OpenAI embeddings and ChatGPT as the language model. Therefore, an OpenAI key is required unless you opt for a customized or local language model.\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n\n The author learned that it is possible to publish essays online, and that working on things that\nare not prestigious can be a sign that one is on the right track. They also learned that impure\nmotives can lead ambitious people astray, and that it is possible to make connections with people\nthrough cleverly planned events. Finally, the author learned that they could find love through a\nchance meeting at a party.\n\nresponse \\= query\\_engine.query(\"What was a hard moment for the author?\")\nprint(textwrap.fill(str(response), 100))\n\n A hard moment for the author was when he realized that he had been working on things that weren't\nprestigious. He had been drawn to these types of work despite their lack of prestige, and he was\nworried that his ambition was leading him astray. He was also concerned that people would give him a\n\"glassy eye\" when he explained what he was writing.\n\n## Saving and Loading[\uf0c1](#saving-and-loading \"Permalink to this heading\")\n\nRedis allows the user to perform backups in the background or synchronously. With Llamaindex, the `RedisVectorStore.persist()` function can be used to trigger such a backup.\n\n!docker exec \\-it redis-vecdb ls /data\n\n\\# RedisVectorStore's persist method doesn't use the persist\\_path argument\nvector\\_store.persist(persist\\_path\\=\"\")\n\n!docker exec \\-it redis-vecdb ls /data\n\ndump.rdb  redis  redisinsight\n\n## Restore index from Redis[\uf0c1](#restore-index-from-redis \"Permalink to this heading\")\n\nvector\\_store \\= RedisVectorStore(\n    index\\_name\\=\"pg\\_essays\",\n    index\\_prefix\\=\"llama\",\n    redis\\_url\\=\"redis://localhost:6379\",\n    overwrite\\=True,\n)\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store\\=vector\\_store)\n\nNow you can reuse your index as discussed above.\n\npgQuery \\= index.as\\_query\\_engine()\npgQuery.query(\"What is the meaning of life?\")\n\\# or\npgRetriever \\= index.as\\_retriever()\npgRetriever.retrieve(\"What is the meaning of life?\")\n\nLearn more about [query\\_engine](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html) and retriveve.\n\n## Deleting documents or index completely[\uf0c1](#deleting-documents-or-index-completely \"Permalink to this heading\")\n\nSometimes it may be useful to delete documents or the entire index. This can be done using the `delete` and `delete_index` methods.\n\ndocument\\_id \\= documents\\[0\\].doc\\_id\ndocument\\_id\n\n'faa23c94-ac9e-4763-92ba-e0f87bf38195'\n\nredis\\_client \\= vector\\_store.client\nprint(\"Number of documents\", len(redis\\_client.keys()))\n\nvector\\_store.delete(document\\_id)\n\nprint(\"Number of documents\", len(redis\\_client.keys()))\n\n\\# now lets delete the index entirely (happens in the background, may take a second)\n\\# this will delete all the documents and the index\nvector\\_store.delete\\_index()\n\nprint(\"Number of documents\", len(redis\\_client.keys()))"
}