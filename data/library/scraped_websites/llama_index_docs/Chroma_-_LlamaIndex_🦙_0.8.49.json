{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html",
        "title": "Chroma - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Chroma[\uf0c1](#chroma \"Permalink to this heading\")\n\n> [Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\n\n [![Discord](https://img.shields.io/discord/1073293645303795742)](https://discord.gg/MMeYNTmh3x)\u00a0\u00a0 [![License](https://img.shields.io/static/v1?label=license&message=Apache%202.0&color=white) ](https://github.com/chroma-core/chroma/blob/master/LICENSE)\u00a0\u00a0 ![Integration Tests](https://github.com/chroma-core/chroma/actions/workflows/chroma-integration-test.yml/badge.svg?branch=main)\n\n*   [Website](https://www.trychroma.com/)\n    \n*   [Documentation](https://docs.trychroma.com/)\n    \n*   [Twitter](https://twitter.com/trychroma)\n    \n*   [Discord](https://discord.gg/MMeYNTmh3x)\n    \n\nChroma is fully-typed, fully-tested and fully-documented.\n\nInstall Chroma with:\n\nChroma runs in various modes. See below for examples of each integrated with LangChain.\n\n*   `in-memory` - in a python script or jupyter notebook\n    \n*   `in-memory with persistance` - in a script or notebook and save/load to disk\n    \n*   `in a docker container` - as a server running your local machine or in the cloud\n    \n\nLike any other database, you can:\n\n*   `.add`\n    \n*   `.get`\n    \n*   `.update`\n    \n*   `.upsert`\n    \n*   `.delete`\n    \n*   `.peek`\n    \n*   and `.query` runs the similarity search.\n    \n\nView full docs at [docs](https://docs.trychroma.com/reference/Collection).\n\n## Basic Example[\uf0c1](#basic-example \"Permalink to this heading\")\n\nIn this basic example, we take the a Paul Graham essay, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.\n\n### Creating a Chroma Index[\uf0c1](#creating-a-chroma-index \"Permalink to this heading\")\n\n\\# !pip install llama-index chromadb --quiet\n\\# !pip install chromadb\n\\# !pip install sentence-transformers\n\\# !pip install pydantic==1.10.11\n\n\\# import\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama\\_index.vector\\_stores import ChromaVectorStore\nfrom llama\\_index.storage.storage\\_context import StorageContext\nfrom llama\\_index.embeddings import HuggingFaceEmbedding\nfrom IPython.display import Markdown, display\nimport chromadb\n\n\\# set up OpenAI\nimport os\nimport getpass\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= getpass.getpass(\"OpenAI API Key:\")\nimport openai\n\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n\\# create client and a new collection\nchroma\\_client \\= chromadb.EphemeralClient()\nchroma\\_collection \\= chroma\\_client.create\\_collection(\"quickstart\")\n\n\\# define embedding function\nembed\\_model \\= HuggingFaceEmbedding(model\\_name\\=\"BAAI/bge-base-en-v1.5\")\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\n    \"../../../examples/paul\\_graham\\_essay/data\"\n).load\\_data()\n\n\\# set up ChromaVectorStore and load in data\nvector\\_store \\= ChromaVectorStore(chroma\\_collection\\=chroma\\_collection)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context, service\\_context\\=service\\_context\n)\n\n\\# Query Data\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n/Users/loganmarkewich/llama\\_index/llama-index/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n/Users/loganmarkewich/llama\\_index/llama-index/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n\n'NoneType' object has no attribute 'cadam32bit\\_grad\\_fp32'\n\n**The author worked on writing and programming growing up. They wrote short stories and tried writing programs on an IBM 1401 computer. Later, they got a microcomputer and started programming more extensively.**\n\n## Basic Example (including saving to disk)[\uf0c1](#basic-example-including-saving-to-disk \"Permalink to this heading\")\n\nExtending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.\n\n`Caution`: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other\u2019s work. As a best practice, only have one client per path running at any given time.\n\n\\# save to disk\n\ndb \\= chromadb.PersistentClient(path\\=\"./chroma\\_db\")\nchroma\\_collection \\= db.get\\_or\\_create\\_collection(\"quickstart\")\nvector\\_store \\= ChromaVectorStore(chroma\\_collection\\=chroma\\_collection)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context, service\\_context\\=service\\_context\n)\n\n\\# load from disk\ndb2 \\= chromadb.PersistentClient(path\\=\"./chroma\\_db\")\nchroma\\_collection \\= db2.get\\_or\\_create\\_collection(\"quickstart\")\nvector\\_store \\= ChromaVectorStore(chroma\\_collection\\=chroma\\_collection)\nindex \\= VectorStoreIndex.from\\_vector\\_store(\n    vector\\_store,\n    service\\_context\\=service\\_context,\n)\n\n\\# Query Data from the persisted index\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author worked on writing and programming growing up. They wrote short stories and tried writing programs on an IBM 1401 computer. Later, they got a microcomputer and started programming games and a word processor.**\n\n## Basic Example (using the Docker Container)[\uf0c1](#basic-example-using-the-docker-container \"Permalink to this heading\")\n\nYou can also run the Chroma Server in a Docker container separately, create a Client to connect to it, and then pass that to LlamaIndex.\n\nHere is how to clone, build, and run the Docker Image:\n\ngit clone git@github.com:chroma\\-core/chroma.git\ndocker\\-compose up \\-d \\--build\n\n\\# create the chroma client and add our data\nimport chromadb\n\nremote\\_db \\= chromadb.HttpClient()\nchroma\\_collection \\= remote\\_db.get\\_or\\_create\\_collection(\"quickstart\")\nvector\\_store \\= ChromaVectorStore(chroma\\_collection\\=chroma\\_collection)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context, service\\_context\\=service\\_context\n)\n\n\\# Query Data from the Chroma Docker index\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**Growing up, the author wrote short stories, programmed on an IBM 1401, and wrote programs on a TRS-80 microcomputer. He also took painting classes at Harvard and worked as a de facto studio assistant for a painter. He also tried to start a company to put art galleries online, and wrote software to build online stores.**\n\n## Update and Delete[\uf0c1](#update-and-delete \"Permalink to this heading\")\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nChroma has users provide `ids` to simplify the bookkeeping here. `ids` can be the name of the file, or a combined has like `filename_paragraphNumber`, etc.\n\nHere is a basic example showing how to do various operations:\n\ndoc\\_to\\_update \\= chroma\\_collection.get(limit\\=1)\ndoc\\_to\\_update\\[\"metadatas\"\\]\\[0\\] \\= {\n    \\*\\*doc\\_to\\_update\\[\"metadatas\"\\]\\[0\\],\n    \\*\\*{\"author\": \"Paul Graham\"},\n}\nchroma\\_collection.update(\n    ids\\=\\[doc\\_to\\_update\\[\"ids\"\\]\\[0\\]\\], metadatas\\=\\[doc\\_to\\_update\\[\"metadatas\"\\]\\[0\\]\\]\n)\nupdated\\_doc \\= chroma\\_collection.get(limit\\=1)\nprint(updated\\_doc\\[\"metadatas\"\\]\\[0\\])\n\n\\# delete the last document\nprint(\"count before\", chroma\\_collection.count())\nchroma\\_collection.delete(ids\\=\\[doc\\_to\\_update\\[\"ids\"\\]\\[0\\]\\])\nprint(\"count after\", chroma\\_collection.count())\n\n{'\\_node\\_content': '{\"id\\_\": \"be08c8bc-f43e-4a71-ba64-e525921a8319\", \"embedding\": null, \"metadata\": {}, \"excluded\\_embed\\_metadata\\_keys\": \\[\\], \"excluded\\_llm\\_metadata\\_keys\": \\[\\], \"relationships\": {\"1\": {\"node\\_id\": \"2cbecdbb-0840-48b2-8151-00119da0995b\", \"node\\_type\": null, \"metadata\": {}, \"hash\": \"4c702b4df575421e1d1af4b1fd50511b226e0c9863dbfffeccb8b689b8448f35\"}, \"3\": {\"node\\_id\": \"6a75604a-fa76-4193-8f52-c72a7b18b154\", \"node\\_type\": null, \"metadata\": {}, \"hash\": \"d6c408ee1fbca650fb669214e6f32ffe363b658201d31c204e85a72edb71772f\"}}, \"hash\": \"b4d0b960aa09e693f9dc0d50ef46a3d0bf5a8fb3ac9f3e4bcf438e326d17e0d8\", \"text\": \"\", \"start\\_char\\_idx\": 0, \"end\\_char\\_idx\": 4050, \"text\\_template\": \"{metadata\\_str}\\\\\\\\n\\\\\\\\n{content}\", \"metadata\\_template\": \"{key}: {value}\", \"metadata\\_seperator\": \"\\\\\\\\n\"}', 'author': 'Paul Graham', 'doc\\_id': '2cbecdbb-0840-48b2-8151-00119da0995b', 'document\\_id': '2cbecdbb-0840-48b2-8151-00119da0995b', 'ref\\_doc\\_id': '2cbecdbb-0840-48b2-8151-00119da0995b'}\ncount before 20\ncount after 19"
}