{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/OptimizerDemo.html",
        "title": "Sentence Embedding Optimizer - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Sentence Embedding Optimizer[\uf0c1](#sentence-embedding-optimizer \"Permalink to this heading\")\n\n\\# My OpenAI Key\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"INSERT OPENAI KEY\"\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nfrom llama\\_index import download\\_loader\n\nWikipediaReader \\= download\\_loader(\"WikipediaReader\")\n\nloader \\= WikipediaReader()\ndocuments \\= loader.load\\_data(pages\\=\\[\"Berlin\"\\])\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n<class 'llama\\_index.readers.schema.base.Document'>\n\nINFO:root:> \\[build\\_index\\_from\\_documents\\] Total LLM token usage: 0 tokens\nINFO:root:> \\[build\\_index\\_from\\_documents\\] Total embedding token usage: 18390 tokens\n\nCompare query with and without optimization for LLM token usage, Embedding Model usage on query, Embedding model usage for optimizer, and total time.\n\nimport time\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\nprint(\"Without optimization\")\nstart\\_time \\= time.time()\nquery\\_engine \\= index.as\\_query\\_engine()\nres \\= query\\_engine.query(\"What is the population of Berlin?\")\nend\\_time \\= time.time()\nprint(\"Total time elapsed: {}\".format(end\\_time \\- start\\_time))\nprint(\"Answer: {}\".format(res))\n\nprint(\"With optimization\")\nstart\\_time \\= time.time()\nquery\\_engine \\= index.as\\_query\\_engine(\n    node\\_postprocessors\\=\\[SentenceEmbeddingOptimizer(percentile\\_cutoff\\=0.5)\\]\n)\nres \\= query\\_engine.query(\"What is the population of Berlin?\")\nend\\_time \\= time.time()\nprint(\"Total time elapsed: {}\".format(end\\_time \\- start\\_time))\nprint(\"Answer: {}\".format(res))\n\nprint(\"Alternate optimization cutoff\")\nstart\\_time \\= time.time()\nquery\\_engine \\= index.as\\_query\\_engine(\n    node\\_postprocessors\\=\\[SentenceEmbeddingOptimizer(threshold\\_cutoff\\=0.7)\\]\n)\nres \\= query\\_engine.query(\"What is the population of Berlin?\")\nend\\_time \\= time.time()\nprint(\"Total time elapsed: {}\".format(end\\_time \\- start\\_time))\nprint(\"Answer: {}\".format(res))\n\nINFO:root:> \\[query\\] Total LLM token usage: 3545 tokens\nINFO:root:> \\[query\\] Total embedding token usage: 7 tokens\n\nTotal time elapsed: 2.8928110599517822\nAnswer: \nThe population of Berlin in 1949 was approximately 2.2 million inhabitants. After the fall of the Berlin Wall in 1989, the population of Berlin increased to approximately 3.7 million inhabitants.\n\nWith optimization\n\nINFO:root:> \\[optimize\\] Total embedding token usage: 7 tokens\nINFO:root:> \\[query\\] Total LLM token usage: 1779 tokens\nINFO:root:> \\[query\\] Total embedding token usage: 7 tokens\n\nTotal time elapsed: 2.346346139907837\nAnswer: \nThe population of Berlin is around 4.5 million.\nAlternate optimization cutoff\n\nINFO:root:> \\[optimize\\] Total embedding token usage: 7 tokens\nINFO:root:> \\[query\\] Total LLM token usage: 3215 tokens\nINFO:root:> \\[query\\] Total embedding token usage: 7 tokens\n\nTotal time elapsed: 2.101111888885498\nAnswer: \nThe population of Berlin is around 4.5 million."
}