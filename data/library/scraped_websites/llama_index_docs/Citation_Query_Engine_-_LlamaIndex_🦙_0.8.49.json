{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/query/query_engines/citation_query_engine.html",
        "title": "Citation Query Engine - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Citation Query Engine[\uf0c1](#module-llama_index.query_engine.citation_query_engine \"Permalink to this heading\")\n\n_class_ llama\\_index.query\\_engine.citation\\_query\\_engine.CitationQueryEngine(_retriever: [BaseRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html#llama_index.indices.base_retriever.BaseRetriever \"llama_index.indices.base_retriever.BaseRetriever\")_, _response\\_synthesizer: Optional\\[[BaseSynthesizer](https://docs.llamaindex.ai/en/stable/api_reference/query/response_synthesizer.html#llama_index.response_synthesizers.BaseSynthesizer \"llama_index.response_synthesizers.base.BaseSynthesizer\")\\] \\= None_, _citation\\_chunk\\_size: int \\= 512_, _citation\\_chunk\\_overlap: int \\= 20_, _text\\_splitter: Optional\\[TextSplitter\\] \\= None_, _node\\_postprocessors: Optional\\[List\\[BaseNodePostprocessor\\]\\] \\= None_, _callback\\_manager: Optional\\[[CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")\\] \\= None_)[\uf0c1](#llama_index.query_engine.citation_query_engine.CitationQueryEngine \"Permalink to this definition\")\n\nCitation query engine.\n\nParameters\n\n*   **retriever** ([_BaseRetriever_](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html#llama_index.indices.base_retriever.BaseRetriever \"llama_index.indices.base_retriever.BaseRetriever\")) \u2013 A retriever object.\n    \n*   **response\\_synthesizer** (_Optional__\\[_[_BaseSynthesizer_](https://docs.llamaindex.ai/en/stable/api_reference/query/response_synthesizer.html#llama_index.response_synthesizers.BaseSynthesizer \"llama_index.response_synthesizers.BaseSynthesizer\")_\\]_) \u2013 A BaseSynthesizer object.\n    \n*   **citation\\_chunk\\_size** (_int_) \u2013 Size of citation chunks, default=512. Useful for controlling granularity of sources.\n    \n*   **citation\\_chunk\\_overlap** (_int_) \u2013 Overlap of citation nodes, default=20.\n    \n*   **text\\_splitter** (_Optional__\\[__TextSplitterType__\\]_) \u2013 A text splitter for creating citation source nodes. Default is a SentenceSplitter.\n    \n*   **callback\\_manager** (_Optional__\\[_[_CallbackManager_](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.CallbackManager\")_\\]_) \u2013 A callback manager.\n    \n\n_classmethod_ from\\_args(_index: ~llama\\_index.indices.base.BaseIndex, response\\_synthesizer: ~typing.Optional\\[~llama\\_index.response\\_synthesizers.base.BaseSynthesizer\\] \\= None, citation\\_chunk\\_size: int \\= 512, citation\\_chunk\\_overlap: int \\= 20, text\\_splitter: ~typing.Optional\\[~llama\\_index.text\\_splitter.types.TextSplitter\\] \\= None, citation\\_qa\\_template: ~llama\\_index.prompts.base.BasePromptTemplate \\= PromptTemplate(metadata={'prompt\\_type': <PromptType.CUSTOM: 'custom'>}, template\\_vars=\\['context\\_str', 'query\\_str'\\], kwargs={}, output\\_parser=None, template=\"Please provide an answer based solely on the provided sources. When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Every answer should include at least one source citation. Only cite a source when you are explicitly referencing it. If none of the sources are helpful, you should indicate that. For example:\\\\nSource 1:\\\\nThe sky is red in the evening and blue in the morning.\\\\nSource 2:\\\\nWater is wet when the sky is red.\\\\nQuery: When is water wet?\\\\nAnswer: Water will be wet when the sky is red \\[2\\], which occurs in the evening \\[1\\].\\\\nNow it's your turn. Below are several numbered sources of information:\\\\n------\\\\n{context\\_str}\\\\n------\\\\nQuery: {query\\_str}\\\\nAnswer: \"), citation\\_refine\\_template: ~llama\\_index.prompts.base.BasePromptTemplate \\= PromptTemplate(metadata={'prompt\\_type': <PromptType.CUSTOM: 'custom'>}, template\\_vars=\\['existing\\_answer', 'context\\_msg', 'query\\_str'\\], kwargs={}, output\\_parser=None, template=\"Please provide an answer based solely on the provided sources. When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Every answer should include at least one source citation. Only cite a source when you are explicitly referencing it. If none of the sources are helpful, you should indicate that. For example:\\\\nSource 1:\\\\nThe sky is red in the evening and blue in the morning.\\\\nSource 2:\\\\nWater is wet when the sky is red.\\\\nQuery: When is water wet?\\\\nAnswer: Water will be wet when the sky is red \\[2\\], which occurs in the evening \\[1\\].\\\\nNow it's your turn. We have provided an existing answer: {existing\\_answer}Below are several numbered sources of information. Use them to refine the existing answer. If the provided sources are not helpful, you will repeat the existing answer.\\\\nBegin refining!\\\\n------\\\\n{context\\_msg}\\\\n------\\\\nQuery: {query\\_str}\\\\nAnswer: \"), retriever: ~typing.Optional\\[~llama\\_index.indices.base\\_retriever.BaseRetriever\\] \\= None, node\\_postprocessors: ~typing.Optional\\[~typing.List\\[~llama\\_index.indices.postprocessor.types.BaseNodePostprocessor\\]\\] \\= None, response\\_mode: ~llama\\_index.response\\_synthesizers.type.ResponseMode \\= ResponseMode.COMPACT, use\\_async: bool \\= False, streaming: bool \\= False, \\*\\*kwargs: ~typing.Any_) \u2192 [CitationQueryEngine](#llama_index.query_engine.citation_query_engine.CitationQueryEngine \"llama_index.query_engine.citation_query_engine.CitationQueryEngine\")[\uf0c1](#llama_index.query_engine.citation_query_engine.CitationQueryEngine.from_args \"Permalink to this definition\")\n\nInitialize a CitationQueryEngine object.\u201d.\n\nParameters\n\n*   **index** \u2013 (BastGPTIndex): index to use for querying\n    \n*   **citation\\_chunk\\_size** (_int_) \u2013 Size of citation chunks, default=512. Useful for controlling granularity of sources.\n    \n*   **citation\\_chunk\\_overlap** (_int_) \u2013 Overlap of citation nodes, default=20.\n    \n*   **text\\_splitter** (_Optional__\\[__TextSplitter__\\]_) \u2013 A text splitter for creating citation source nodes. Default is a SentenceSplitter.\n    \n*   **citation\\_qa\\_template** ([_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")) \u2013 Template for initial citation QA\n    \n*   **citation\\_refine\\_template** ([_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")) \u2013 Template for citation refinement.\n    \n*   **retriever** ([_BaseRetriever_](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html#llama_index.indices.base_retriever.BaseRetriever \"llama_index.indices.base_retriever.BaseRetriever\")) \u2013 A retriever object.\n    \n*   **service\\_context** (_Optional__\\[_[_ServiceContext_](https://docs.llamaindex.ai/en/stable/api_reference/service_context.html#llama_index.indices.service_context.ServiceContext \"llama_index.indices.service_context.ServiceContext\")_\\]_) \u2013 A ServiceContext object.\n    \n*   **node\\_postprocessors** (_Optional__\\[__List__\\[__BaseNodePostprocessor__\\]__\\]_) \u2013 A list of node postprocessors.\n    \n*   **verbose** (_bool_) \u2013 Whether to print out debug info.\n    \n*   **response\\_mode** ([_ResponseMode_](https://docs.llamaindex.ai/en/stable/api_reference/query/response_synthesizer.html#llama_index.response_synthesizers.ResponseMode \"llama_index.response_synthesizers.ResponseMode\")) \u2013 A ResponseMode object.\n    \n*   **use\\_async** (_bool_) \u2013 Whether to use async.\n    \n*   **streaming** (_bool_) \u2013 Whether to use streaming.\n    \n*   **optimizer** (_Optional__\\[__BaseTokenUsageOptimizer__\\]_) \u2013 A BaseTokenUsageOptimizer object.\n    \n\n_property_ retriever_: [BaseRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html#llama_index.indices.base_retriever.BaseRetriever \"llama_index.indices.base_retriever.BaseRetriever\")_[\uf0c1](#llama_index.query_engine.citation_query_engine.CitationQueryEngine.retriever \"Permalink to this definition\")\n\nGet the retriever object."
}