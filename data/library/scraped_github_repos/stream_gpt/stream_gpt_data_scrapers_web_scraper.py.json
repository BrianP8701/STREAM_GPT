{
    "metadata": {
        "type": "repo",
        "path": "stream_gpt/data_scrapers/web_scraper.py"
    },
    "text": "from apify_client import ApifyClient\nfrom stream_gpt.types.document import Document\nfrom stream_gpt.interfaces.scraper_interface import Scraper\nfrom typing import List\n\nclass WebScraper(Scraper):\n    '''\n    Use Apify's Website Content Crawler to scrape content from a given URL.\n    \n    Parameters:\n    - apify_key (str): The API key used to authenticate with Apify. Get one at: https://console.apify.com/\n    \n    Note:\n    - This function requires the APIFY_KEY to be set and valid.\n    - For more details on Apify's Website Content Crawler, visit: https://apify.com/apify/website-content-crawler\n    '''\n    def __init__(self, apify_key):\n        self.apify_key = apify_key\n        \n    def scrape(self, url, includeUrlGlobs=[], excludeUrlGlobs=[]) -> List[Document]:\n        \"\"\"\n        This function leverages Apify's Website Content Crawler to extract content from websites. \n        It allows users to specify URLs to include or exclude from crawling and uses predefined \n        selectors to interact with and modify content during the scraping process.\n\n        Parameters:\n        - url (str): The starting URL for the web crawling process.\n        - includeUrlGlobs (list, optional): List of URL glob patterns to be exclusively included in the crawl.\n        - excludeUrlGlobs (list, optional): List of URL glob patterns to be excluded from the crawl.\n\n        Returns:\n        - list: A list of dataset items collected during the crawl.\n\n        Example:\n        >>> result = apify_website_content_crawler(\"https://example.com\", [\"https://example.com/blog/*\"], [\"https://example.com/private/*\"])\n        >>> print(result)\n        \"\"\"\n        # Initialize the ApifyClient with your API token\n        client = ApifyClient(self.apify_key)\n\n        # Prepare the Actor input\n        run_input = {\n            \"startUrls\": [{ \"url\": url}],\n            \"includeUrlGlobs\": includeUrlGlobs,\n            \"excludeUrlGlobs\": excludeUrlGlobs,\n            \"initialCookies\": [],\n            \"proxyConfiguration\": { \"useApifyProxy\": True },\n            \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n        [role=\\\"alert\\\"],\n        [role=\\\"banner\\\"],\n        [role=\\\"dialog\\\"],\n        [role=\\\"alertdialog\\\"],\n        [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n        [aria-modal=\\\"true\\\"]\"\"\",\n            \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n        }\n\n        # Run the Actor and wait for it to finish\n        run = client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\n        # Fetch and print Actor results from the run's dataset (if there are any)\n        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n            print(item)\n\n        dataset_items = client.dataset(run['defaultDatasetId']).list_items().items\n        return self.convert_scraped_data_to_documents(dataset_items)\n    \n    @staticmethod\n    def convert_scraped_data_to_documents(dataset_items):\n        documents = []\n        for item in dataset_items:\n            documents.append(Document(metadata={\n                'type': 'website', \n                'url': item['url'], \n                'title': item['metadata']['title'], \n                'description': item['metadata']['description']\n                }, text=item['markdown']))\n        return documents\n"
}