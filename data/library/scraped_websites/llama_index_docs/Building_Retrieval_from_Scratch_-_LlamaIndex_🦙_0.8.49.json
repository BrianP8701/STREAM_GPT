{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/retrieval.html",
        "title": "Building Retrieval from Scratch - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Building Retrieval from Scratch[\uf0c1](#building-retrieval-from-scratch \"Permalink to this heading\")\n\nIn this tutorial, we show you how to build a standard retriever against a vector database, that will fetch nodes via top-k similarity.\n\nWe use Pinecone as the vector database. We load in nodes using our high-level ingestion abstractions (to see how to build this from scratch, see our previous tutorial!).\n\nWe will show how to do the following:\n\n1.  How to generate a query embedding\n    \n2.  How to query the vector database using different search modes (dense, sparse, hybrid)\n    \n3.  How to parse results into a set of Nodes\n    \n4.  How to put this in a custom retriever\n    \n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can start loading data into Pinecone.\n\n### Build Pinecone Index[\uf0c1](#build-pinecone-index \"Permalink to this heading\")\n\nimport pinecone\nimport os\n\napi\\_key \\= os.environ\\[\"PINECONE\\_API\\_KEY\"\\]\npinecone.init(api\\_key\\=api\\_key, environment\\=\"us-west1-gcp\")\n\n\\# dimensions are for text-embedding-ada-002\npinecone.create\\_index(\n    \"quickstart\", dimension\\=1536, metric\\=\"euclidean\", pod\\_type\\=\"p1\"\n)\n\npinecone\\_index \\= pinecone.Index(\"quickstart\")\n\n\\# \\[Optional\\] drop contents in index\npinecone\\_index.delete(deleteAll\\=True)\n\n### Create PineconeVectorStore[\uf0c1](#create-pineconevectorstore \"Permalink to this heading\")\n\nSimple wrapper abstraction to use in LlamaIndex. Wrap in StorageContext so we can easily load in Nodes.\n\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\n\nvector\\_store \\= PineconeVectorStore(pinecone\\_index\\=pinecone\\_index)\n\n### Load Documents[\uf0c1](#load-documents \"Permalink to this heading\")\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocuments \\= loader.load(file\\_path\\=\"./data/llama2.pdf\")\n\n### Load into Vector Store[\uf0c1](#load-into-vector-store \"Permalink to this heading\")\n\nLoad in documents into the PineconeVectorStore.\n\n**NOTE**: We use high-level ingestion abstractions here, with `VectorStoreIndex.from_documents.` We\u2019ll refrain from using `VectorStoreIndex` for the rest of this tutorial.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.storage import StorageContext\n\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=1024)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context, storage\\_context\\=storage\\_context\n)\n\n## Define Vector Retriever[\uf0c1](#define-vector-retriever \"Permalink to this heading\")\n\nNow we\u2019re ready to define our retriever against this vector store to retrieve a set of nodes.\n\nWe\u2019ll show the processes step by step and then wrap it into a function.\n\nquery\\_str \\= \"Can you tell me about the key concepts for safety finetuning\"\n\n### 1\\. Generate a Query Embedding[\uf0c1](#generate-a-query-embedding \"Permalink to this heading\")\n\nfrom llama\\_index.embeddings import OpenAIEmbedding\n\nembed\\_model \\= OpenAIEmbedding()\n\nquery\\_embedding \\= embed\\_model.get\\_query\\_embedding(query\\_str)\n\n### 2\\. Query the Vector Database[\uf0c1](#query-the-vector-database \"Permalink to this heading\")\n\nWe show how to query the vector database with different modes: default, sparse, and hybrid.\n\nWe first construct a `VectorStoreQuery` and then query the vector db.\n\n\\# construct vector store query\nfrom llama\\_index.vector\\_stores import VectorStoreQuery\n\nquery\\_mode \\= \"default\"\n\\# query\\_mode = \"sparse\"\n\\# query\\_mode = \"hybrid\"\n\nvector\\_store\\_query \\= VectorStoreQuery(\n    query\\_embedding\\=query\\_embedding, similarity\\_top\\_k\\=2, mode\\=query\\_mode\n)\n\n\\# returns a VectorStoreQueryResult\nquery\\_result \\= vector\\_store.query(vector\\_store\\_query)\nquery\\_result\n\n### 3\\. Parse Result into a set of Nodes[\uf0c1](#parse-result-into-a-set-of-nodes \"Permalink to this heading\")\n\nThe `VectorStoreQueryResult` returns the set of nodes and similarities. We construct a `NodeWithScore` object with this.\n\nfrom llama\\_index.schema import NodeWithScore\nfrom typing import Optional\n\nnodes\\_with\\_scores \\= \\[\\]\nfor index, node in enumerate(query\\_result.nodes):\n    score: Optional\\[float\\] \\= None\n    if query\\_result.similarities is not None:\n        score \\= query\\_result.similarities\\[index\\]\n    nodes\\_with\\_scores.append(NodeWithScore(node\\=node, score\\=score))\n\nfrom llama\\_index.response.notebook\\_utils import display\\_source\\_node\n\nfor node in nodes\\_with\\_scores:\n    display\\_source\\_node(node, source\\_length\\=1000)\n\n### 4\\. Put this into a Retriever[\uf0c1](#put-this-into-a-retriever \"Permalink to this heading\")\n\nLet\u2019s put this into a Retriever subclass that can plug into the rest of LlamaIndex workflows!\n\nfrom llama\\_index import QueryBundle\nfrom llama\\_index.retrievers import BaseRetriever\nfrom typing import Any, List\n\nclass PineconeRetriever(BaseRetriever):\n    \"\"\"Retriever over a pinecone vector store.\"\"\"\n\n    def \\_\\_init\\_\\_(\n        self,\n        vector\\_store: PineconeVectorStore,\n        embed\\_model: Any,\n        query\\_mode: str \\= \"default\",\n        similarity\\_top\\_k: int \\= 2,\n    ) \\-> None:\n        \"\"\"Init params.\"\"\"\n        self.\\_vector\\_store \\= vector\\_store\n        self.\\_embed\\_model \\= embed\\_model\n        self.\\_query\\_mode \\= query\\_mode\n        self.\\_similarity\\_top\\_k \\= similarity\\_top\\_k\n\n    def \\_retrieve(self, query\\_bundle: QueryBundle) \\-> List\\[NodeWithScore\\]:\n        \"\"\"Retrieve.\"\"\"\n        query\\_embedding \\= embed\\_model.get\\_query\\_embedding(query\\_str)\n        vector\\_store\\_query \\= VectorStoreQuery(\n            query\\_embedding\\=query\\_embedding,\n            similarity\\_top\\_k\\=self.\\_similarity\\_top\\_k,\n            mode\\=self.\\_query\\_mode,\n        )\n        query\\_result \\= vector\\_store.query(vector\\_store\\_query)\n\n        nodes\\_with\\_scores \\= \\[\\]\n        for index, node in enumerate(query\\_result.nodes):\n            score: Optional\\[float\\] \\= None\n            if query\\_result.similarities is not None:\n                score \\= query\\_result.similarities\\[index\\]\n            nodes\\_with\\_scores.append(NodeWithScore(node\\=node, score\\=score))\n\n        return nodes\\_with\\_scores\n\nretriever \\= PineconeRetriever(\n    vector\\_store, embed\\_model, query\\_mode\\=\"default\", similarity\\_top\\_k\\=2\n)\n\nretrieved\\_nodes \\= retriever.retrieve(query\\_str)\nfor node in retrieved\\_nodes:\n    display\\_source\\_node(node, source\\_length\\=1000)\n\n## Plug this into our RetrieverQueryEngine to synthesize a response[\uf0c1](#plug-this-into-our-retrieverqueryengine-to-synthesize-a-response \"Permalink to this heading\")\n\n**NOTE**: We\u2019ll cover more on how to build response synthesis from scratch in future tutorials!\n\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever)\n\nresponse \\= query\\_engine.query(query\\_str)\n\nThe key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to train the model to align with safety guidelines. Safety RLHF integrates safety into the RLHF pipeline by training a safety-specific reward model and gathering challenging adversarial prompts for fine-tuning. Safety context distillation refines the RLHF pipeline by generating safer model responses using a safety preprompt and fine-tuning the model on these responses without the preprompt. These concepts are used to mitigate safety risks and improve the safety of the model's responses."
}