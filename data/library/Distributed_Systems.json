["Distributed Systems\nThird edition\nPreliminary version 3.01pre (2017)\nMaarten van Steen\nAndrew S. Tanenbaum\nDistributed Systems\nThird edition\nPreliminary version 3.01pre (2017)\nMaarten van Steen\nAndrew S. Tanenbaum", "", "Copyright \u00a9 2017 Maarten van Steen and Andrew S. Tanenbaum\nPublished by Maarten van Steen\nThis book was previously published by: Pearson Education, Inc.\nISBN: 978-15-430573-8-6 (printed version)\nISBN: 978-90-815406-2-9 (digital version)\nEdition: 3. Version: 01 (February 2017)\nAll rights to text and illustrations are reserved by Maarten van Steen and Andrew S. Tanenbaum. This work may\nnot be copied, reproduced, or translated in whole or part without written permission of the publisher, except for\nbrief excerpts in reviews or scholarly analysis. Use with any form of information storage and retrieval, electronic\nadaptation or whatever, computer software, or by similar or dissimilar methods now known or developed in the\nfuture is strictly forbidden without written permission of the publisher.\nCopyright \u00a9 2017 Maarten van Steen and Andrew S. Tanenbaum\nPublished by Maarten van Steen\nThis book was previously published by: Pearson Education, Inc.\nISBN: 978-15-430573-8-6 (printed version)\nISBN: 978-90-815406-2-9 (digital version)\nEdition: 3. Version: 01 (February 2017)\nAll rights to text and illustrations are reserved by Maarten van Steen and Andrew S. Tanenbaum. This work may\nnot be copied, reproduced, or translated in whole or part without written permission of the publisher, except for\nbrief excerpts in reviews or scholarly analysis. Use with any form of information storage and retrieval, electronic\nadaptation or whatever, computer software, or by similar or dissimilar methods now known or developed in the\nfuture is strictly forbidden without written permission of the publisher.", "", "To Mari\u00eblle, Max, and Elke\n\u2013 MvS\nTo Suzanne, Barbara, Marvin, Aron, Nathan, Olivia, and Mirte\n\u2013 AST\nTo Mari\u00eblle, Max, and Elke\n\u2013 MvS\nTo Suzanne, Barbara, Marvin, Aron, Nathan, Olivia, and Mirte\n\u2013 AST", "", "Contents\nPreface xi\n1 Introduction 1\n1.1 What is a distributed system? . . . . . . . . . . . . . . . . . . . . 2\nCharacteristic 1: Collection of autonomous computing elements 2\nCharacteristic 2: Single coherent system . . . . . . . . . . . . . . 4\nMiddleware and distributed systems . . . . . . . . . . . . . . . . 5\n1.2 Design goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nSupporting resource sharing . . . . . . . . . . . . . . . . . . . . . 7\nMaking distribution transparent . . . . . . . . . . . . . . . . . . 8\nBeing open . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nBeing scalable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nPitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.3 Types of distributed systems . . . . . . . . . . . . . . . . . . . . 24\nHigh performance distributed computing . . . . . . . . . . . . . 25\nDistributed information systems . . . . . . . . . . . . . . . . . . 34\nPervasive systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n1.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2 Architectures 55\n2.1 Architectural styles . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nLayered architectures . . . . . . . . . . . . . . . . . . . . . . . . . 57\nObject-based and service-oriented architectures . . . . . . . . . 62\nResource-based architectures . . . . . . . . . . . . . . . . . . . . 64\nPublish-subscribe architectures . . . . . . . . . . . . . . . . . . . 66\n2.2 Middleware organization . . . . . . . . . . . . . . . . . . . . . . 71\nWrappers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\nInterceptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\nModi\ufb01able middleware . . . . . . . . . . . . . . . . . . . . . . . . 75\n2.3 System architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 76\nv\nContents\nPreface xi\n1 Introduction 1\n1.1 What is a distributed system? . . . . . . . . . . . . . . . . . . . . 2\nCharacteristic 1: Collection of autonomous computing elements 2\nCharacteristic 2: Single coherent system . . . . . . . . . . . . . . 4\nMiddleware and distributed systems . . . . . . . . . . . . . . . . 5\n1.2 Design goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nSupporting resource sharing . . . . . . . . . . . . . . . . . . . . . 7\nMaking distribution transparent . . . . . . . . . . . . . . . . . . 8\nBeing open . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nBeing scalable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nPitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.3 Types of distributed systems . . . . . . . . . . . . . . . . . . . . 24\nHigh performance distributed computing . . . . . . . . . . . . . 25\nDistributed information systems . . . . . . . . . . . . . . . . . . 34\nPervasive systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n1.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2 Architectures 55\n2.1 Architectural styles . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nLayered architectures . . . . . . . . . . . . . . . . . . . . . . . . . 57\nObject-based and service-oriented architectures . . . . . . . . . 62\nResource-based architectures . . . . . . . . . . . . . . . . . . . . 64\nPublish-subscribe architectures . . . . . . . . . . . . . . . . . . . 66\n2.2 Middleware organization . . . . . . . . . . . . . . . . . . . . . . 71\nWrappers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\nInterceptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\nModi\ufb01able middleware . . . . . . . . . . . . . . . . . . . . . . . . 75\n2.3 System architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 76\nv", "vi CONTENTS\nCentralized organizations . . . . . . . . . . . . . . . . . . . . . . 76\nDecentralized organizations: peer-to-peer systems . . . . . . . . 80\nHybrid Architectures . . . . . . . . . . . . . . . . . . . . . . . . . 90\n2.4 Example architectures . . . . . . . . . . . . . . . . . . . . . . . . 94\nThe Network File System . . . . . . . . . . . . . . . . . . . . . . 94\nThe Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n3 Processes 103\n3.1 Threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\nIntroduction to threads . . . . . . . . . . . . . . . . . . . . . . . . 104\nThreads in distributed systems . . . . . . . . . . . . . . . . . . . 111\n3.2 Virtualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nPrinciple of virtualization . . . . . . . . . . . . . . . . . . . . . . 116\nApplication of virtual machines to distributed systems . . . . . 122\n3.3 Clients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\nNetworked user interfaces . . . . . . . . . . . . . . . . . . . . . . 124\nClient-side software for distribution transparency . . . . . . . . 127\n3.4 Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nGeneral design issues . . . . . . . . . . . . . . . . . . . . . . . . . 129\nObject servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\nExample: The Apache Web server . . . . . . . . . . . . . . . . . 139\nServer clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n3.5 Code migration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nReasons for migrating code . . . . . . . . . . . . . . . . . . . . . 152\nMigration in heterogeneous systems . . . . . . . . . . . . . . . . 158\n3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n4 Communication 163\n4.1 Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\nLayered Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\nTypes of Communication . . . . . . . . . . . . . . . . . . . . . . 172\n4.2 Remote procedure call . . . . . . . . . . . . . . . . . . . . . . . . 173\nBasic RPC operation . . . . . . . . . . . . . . . . . . . . . . . . . 174\nParameter passing . . . . . . . . . . . . . . . . . . . . . . . . . . 178\nRPC-based application support . . . . . . . . . . . . . . . . . . . 182\nVariations on RPC . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\nExample: DCE RPC . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n4.3 Message-oriented communication . . . . . . . . . . . . . . . . . 193\nSimple transient messaging with sockets . . . . . . . . . . . . . 193\nAdvanced transient messaging . . . . . . . . . . . . . . . . . . . 198\nMessage-oriented persistent communication . . . . . . . . . . . 206\nExample: IBM\u2019s WebSphere message-queuing system . . . . . . 212\nExample: Advanced Message Queuing Protocol (AMQP) . . . . 218\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\nvi CONTENTS\nCentralized organizations . . . . . . . . . . . . . . . . . . . . . . 76\nDecentralized organizations: peer-to-peer systems . . . . . . . . 80\nHybrid Architectures . . . . . . . . . . . . . . . . . . . . . . . . . 90\n2.4 Example architectures . . . . . . . . . . . . . . . . . . . . . . . . 94\nThe Network File System . . . . . . . . . . . . . . . . . . . . . . 94\nThe Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n3 Processes 103\n3.1 Threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\nIntroduction to threads . . . . . . . . . . . . . . . . . . . . . . . . 104\nThreads in distributed systems . . . . . . . . . . . . . . . . . . . 111\n3.2 Virtualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nPrinciple of virtualization . . . . . . . . . . . . . . . . . . . . . . 116\nApplication of virtual machines to distributed systems . . . . . 122\n3.3 Clients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\nNetworked user interfaces . . . . . . . . . . . . . . . . . . . . . . 124\nClient-side software for distribution transparency . . . . . . . . 127\n3.4 Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nGeneral design issues . . . . . . . . . . . . . . . . . . . . . . . . . 129\nObject servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\nExample: The Apache Web server . . . . . . . . . . . . . . . . . 139\nServer clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n3.5 Code migration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nReasons for migrating code . . . . . . . . . . . . . . . . . . . . . 152\nMigration in heterogeneous systems . . . . . . . . . . . . . . . . 158\n3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n4 Communication 163\n4.1 Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\nLayered Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\nTypes of Communication . . . . . . . . . . . . . . . . . . . . . . 172\n4.2 Remote procedure call . . . . . . . . . . . . . . . . . . . . . . . . 173\nBasic RPC operation . . . . . . . . . . . . . . . . . . . . . . . . . 174\nParameter passing . . . . . . . . . . . . . . . . . . . . . . . . . . 178\nRPC-based application support . . . . . . . . . . . . . . . . . . . 182\nVariations on RPC . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\nExample: DCE RPC . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n4.3 Message-oriented communication . . . . . . . . . . . . . . . . . 193\nSimple transient messaging with sockets . . . . . . . . . . . . . 193\nAdvanced transient messaging . . . . . . . . . . . . . . . . . . . 198\nMessage-oriented persistent communication . . . . . . . . . . . 206\nExample: IBM\u2019s WebSphere message-queuing system . . . . . . 212\nExample: Advanced Message Queuing Protocol (AMQP) . . . . 218\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "CONTENTS vii\n4.4 Multicast communication . . . . . . . . . . . . . . . . . . . . . . 221\nApplication-level tree-based multicasting . . . . . . . . . . . . . 221\nFlooding-based multicasting . . . . . . . . . . . . . . . . . . . . . 225\nGossip-based data dissemination . . . . . . . . . . . . . . . . . . 229\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n5 Naming 237\n5.1 Names, identi\ufb01ers, and addresses . . . . . . . . . . . . . . . . . 238\n5.2 Flat naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nSimple solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nHome-based approaches . . . . . . . . . . . . . . . . . . . . . . . 245\nDistributed hash tables . . . . . . . . . . . . . . . . . . . . . . . . 246\nHierarchical approaches . . . . . . . . . . . . . . . . . . . . . . . 251\n5.3 Structured naming . . . . . . . . . . . . . . . . . . . . . . . . . . 256\nName spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\nName resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\nThe implementation of a name space . . . . . . . . . . . . . . . 264\nExample: The Domain Name System . . . . . . . . . . . . . . . 271\nExample: The Network File System . . . . . . . . . . . . . . . . 278\n5.4 Attribute-based naming . . . . . . . . . . . . . . . . . . . . . . . 283\nDirectory services . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nHierarchical implementations: LDAP . . . . . . . . . . . . . . . 285\nDecentralized implementations . . . . . . . . . . . . . . . . . . . 288\n5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n6 Coordination 297\n6.1 Clock synchronization . . . . . . . . . . . . . . . . . . . . . . . . 298\nPhysical clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\nClock synchronization algorithms . . . . . . . . . . . . . . . . . 302\n6.2 Logical clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\nLamport\u2019s logical clocks . . . . . . . . . . . . . . . . . . . . . . . 310\nVector clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n6.3 Mutual exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\nA centralized algorithm . . . . . . . . . . . . . . . . . . . . . . . 322\nA distributed algorithm . . . . . . . . . . . . . . . . . . . . . . . 323\nA token-ring algorithm . . . . . . . . . . . . . . . . . . . . . . . . 325\nA decentralized algorithm . . . . . . . . . . . . . . . . . . . . . . 326\n6.4 Election algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 329\nThe bully algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 330\nA ring algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\nElections in wireless environments . . . . . . . . . . . . . . . . . 333\nElections in large-scale systems . . . . . . . . . . . . . . . . . . . 335\n6.5 Location systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nCONTENTS vii\n4.4 Multicast communication . . . . . . . . . . . . . . . . . . . . . . 221\nApplication-level tree-based multicasting . . . . . . . . . . . . . 221\nFlooding-based multicasting . . . . . . . . . . . . . . . . . . . . . 225\nGossip-based data dissemination . . . . . . . . . . . . . . . . . . 229\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n5 Naming 237\n5.1 Names, identi\ufb01ers, and addresses . . . . . . . . . . . . . . . . . 238\n5.2 Flat naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nSimple solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nHome-based approaches . . . . . . . . . . . . . . . . . . . . . . . 245\nDistributed hash tables . . . . . . . . . . . . . . . . . . . . . . . . 246\nHierarchical approaches . . . . . . . . . . . . . . . . . . . . . . . 251\n5.3 Structured naming . . . . . . . . . . . . . . . . . . . . . . . . . . 256\nName spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\nName resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\nThe implementation of a name space . . . . . . . . . . . . . . . 264\nExample: The Domain Name System . . . . . . . . . . . . . . . 271\nExample: The Network File System . . . . . . . . . . . . . . . . 278\n5.4 Attribute-based naming . . . . . . . . . . . . . . . . . . . . . . . 283\nDirectory services . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nHierarchical implementations: LDAP . . . . . . . . . . . . . . . 285\nDecentralized implementations . . . . . . . . . . . . . . . . . . . 288\n5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n6 Coordination 297\n6.1 Clock synchronization . . . . . . . . . . . . . . . . . . . . . . . . 298\nPhysical clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\nClock synchronization algorithms . . . . . . . . . . . . . . . . . 302\n6.2 Logical clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\nLamport\u2019s logical clocks . . . . . . . . . . . . . . . . . . . . . . . 310\nVector clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n6.3 Mutual exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\nA centralized algorithm . . . . . . . . . . . . . . . . . . . . . . . 322\nA distributed algorithm . . . . . . . . . . . . . . . . . . . . . . . 323\nA token-ring algorithm . . . . . . . . . . . . . . . . . . . . . . . . 325\nA decentralized algorithm . . . . . . . . . . . . . . . . . . . . . . 326\n6.4 Election algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 329\nThe bully algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 330\nA ring algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\nElections in wireless environments . . . . . . . . . . . . . . . . . 333\nElections in large-scale systems . . . . . . . . . . . . . . . . . . . 335\n6.5 Location systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "viii CONTENTS\nGPS: Global Positioning System . . . . . . . . . . . . . . . . . . . 337\nWhen GPS is not an option . . . . . . . . . . . . . . . . . . . . . 339\nLogical positioning of nodes . . . . . . . . . . . . . . . . . . . . . 339\n6.6 Distributed event matching . . . . . . . . . . . . . . . . . . . . . 343\nCentralized implementations . . . . . . . . . . . . . . . . . . . . 343\n6.7 Gossip-based coordination . . . . . . . . . . . . . . . . . . . . . . 349\nAggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\nA peer-sampling service . . . . . . . . . . . . . . . . . . . . . . . 350\nGossip-based overlay construction . . . . . . . . . . . . . . . . . 352\n6.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\n7 Consistency and replication 355\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\nReasons for replication . . . . . . . . . . . . . . . . . . . . . . . . 356\nReplication as scaling technique . . . . . . . . . . . . . . . . . . 357\n7.2 Data-centric consistency models . . . . . . . . . . . . . . . . . . 358\nContinuous consistency . . . . . . . . . . . . . . . . . . . . . . . 359\nConsistent ordering of operations . . . . . . . . . . . . . . . . . 364\nEventual consistency . . . . . . . . . . . . . . . . . . . . . . . . . 373\n7.3 Client-centric consistency models . . . . . . . . . . . . . . . . . . 375\nMonotonic reads . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\nMonotonic writes . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\nRead your writes . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\nWrites follow reads . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n7.4 Replica management . . . . . . . . . . . . . . . . . . . . . . . . . 383\nFinding the best server location . . . . . . . . . . . . . . . . . . . 383\nContent replication and placement . . . . . . . . . . . . . . . . . 385\nContent distribution . . . . . . . . . . . . . . . . . . . . . . . . . 388\nManaging replicated objects . . . . . . . . . . . . . . . . . . . . . 393\n7.5 Consistency protocols . . . . . . . . . . . . . . . . . . . . . . . . 396\nContinuous consistency . . . . . . . . . . . . . . . . . . . . . . . 396\nPrimary-based protocols . . . . . . . . . . . . . . . . . . . . . . . 398\nReplicated-write protocols . . . . . . . . . . . . . . . . . . . . . . 401\nCache-coherence protocols . . . . . . . . . . . . . . . . . . . . . . 403\nImplementing client-centric consistency . . . . . . . . . . . . . . 407\n7.6 Example: Caching and replication in the Web . . . . . . . . . . 409\n7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n8 Fault tolerance 423\n8.1 Introduction to fault tolerance . . . . . . . . . . . . . . . . . . . . 424\nBasic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nFailure models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\nFailure masking by redundancy . . . . . . . . . . . . . . . . . . 431\n8.2 Process resilience . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\nviii CONTENTS\nGPS: Global Positioning System . . . . . . . . . . . . . . . . . . . 337\nWhen GPS is not an option . . . . . . . . . . . . . . . . . . . . . 339\nLogical positioning of nodes . . . . . . . . . . . . . . . . . . . . . 339\n6.6 Distributed event matching . . . . . . . . . . . . . . . . . . . . . 343\nCentralized implementations . . . . . . . . . . . . . . . . . . . . 343\n6.7 Gossip-based coordination . . . . . . . . . . . . . . . . . . . . . . 349\nAggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\nA peer-sampling service . . . . . . . . . . . . . . . . . . . . . . . 350\nGossip-based overlay construction . . . . . . . . . . . . . . . . . 352\n6.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\n7 Consistency and replication 355\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\nReasons for replication . . . . . . . . . . . . . . . . . . . . . . . . 356\nReplication as scaling technique . . . . . . . . . . . . . . . . . . 357\n7.2 Data-centric consistency models . . . . . . . . . . . . . . . . . . 358\nContinuous consistency . . . . . . . . . . . . . . . . . . . . . . . 359\nConsistent ordering of operations . . . . . . . . . . . . . . . . . 364\nEventual consistency . . . . . . . . . . . . . . . . . . . . . . . . . 373\n7.3 Client-centric consistency models . . . . . . . . . . . . . . . . . . 375\nMonotonic reads . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\nMonotonic writes . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\nRead your writes . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\nWrites follow reads . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n7.4 Replica management . . . . . . . . . . . . . . . . . . . . . . . . . 383\nFinding the best server location . . . . . . . . . . . . . . . . . . . 383\nContent replication and placement . . . . . . . . . . . . . . . . . 385\nContent distribution . . . . . . . . . . . . . . . . . . . . . . . . . 388\nManaging replicated objects . . . . . . . . . . . . . . . . . . . . . 393\n7.5 Consistency protocols . . . . . . . . . . . . . . . . . . . . . . . . 396\nContinuous consistency . . . . . . . . . . . . . . . . . . . . . . . 396\nPrimary-based protocols . . . . . . . . . . . . . . . . . . . . . . . 398\nReplicated-write protocols . . . . . . . . . . . . . . . . . . . . . . 401\nCache-coherence protocols . . . . . . . . . . . . . . . . . . . . . . 403\nImplementing client-centric consistency . . . . . . . . . . . . . . 407\n7.6 Example: Caching and replication in the Web . . . . . . . . . . 409\n7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n8 Fault tolerance 423\n8.1 Introduction to fault tolerance . . . . . . . . . . . . . . . . . . . . 424\nBasic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nFailure models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\nFailure masking by redundancy . . . . . . . . . . . . . . . . . . 431\n8.2 Process resilience . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "CONTENTS ix\nResilience by process groups . . . . . . . . . . . . . . . . . . . . 433\nFailure masking and replication . . . . . . . . . . . . . . . . . . 435\nConsensus in faulty systems with crash failures . . . . . . . . . 436\nExample: Paxos . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\nConsensus in faulty systems with arbitrary failures . . . . . . . 449\nSome limitations on realizing fault tolerance . . . . . . . . . . . 459\nFailure detection . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n8.3 Reliable client-server communication . . . . . . . . . . . . . . . 464\nPoint-to-point communication . . . . . . . . . . . . . . . . . . . . 464\nRPC semantics in the presence of failures . . . . . . . . . . . . . 464\n8.4 Reliable group communication . . . . . . . . . . . . . . . . . . . 470\nAtomic multicast . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\n8.5 Distributed commit . . . . . . . . . . . . . . . . . . . . . . . . . . 483\n8.6 Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nCheckpointing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493\nMessage logging . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\nRecovery-oriented computing . . . . . . . . . . . . . . . . . . . . 498\n8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499\n9 Security 501\n9.1 Introduction to security . . . . . . . . . . . . . . . . . . . . . . . 502\nSecurity threats, policies, and mechanisms . . . . . . . . . . . . 502\nDesign issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504\nCryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n9.2 Secure channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\nAuthentication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513\nMessage integrity and con\ufb01dentiality . . . . . . . . . . . . . . . 520\nSecure group communication . . . . . . . . . . . . . . . . . . . . 523\nExample: Kerberos . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n9.3 Access control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529\nGeneral issues in access control . . . . . . . . . . . . . . . . . . . 529\nFirewalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533\nSecure mobile code . . . . . . . . . . . . . . . . . . . . . . . . . . 535\nDenial of service . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\n9.4 Secure naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540\n9.5 Security management . . . . . . . . . . . . . . . . . . . . . . . . 541\nKey management . . . . . . . . . . . . . . . . . . . . . . . . . . . 542\nSecure group management . . . . . . . . . . . . . . . . . . . . . 545\nAuthorization management . . . . . . . . . . . . . . . . . . . . . 547\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552\nBibliography 555\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nCONTENTS ix\nResilience by process groups . . . . . . . . . . . . . . . . . . . . 433\nFailure masking and replication . . . . . . . . . . . . . . . . . . 435\nConsensus in faulty systems with crash failures . . . . . . . . . 436\nExample: Paxos . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\nConsensus in faulty systems with arbitrary failures . . . . . . . 449\nSome limitations on realizing fault tolerance . . . . . . . . . . . 459\nFailure detection . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n8.3 Reliable client-server communication . . . . . . . . . . . . . . . 464\nPoint-to-point communication . . . . . . . . . . . . . . . . . . . . 464\nRPC semantics in the presence of failures . . . . . . . . . . . . . 464\n8.4 Reliable group communication . . . . . . . . . . . . . . . . . . . 470\nAtomic multicast . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\n8.5 Distributed commit . . . . . . . . . . . . . . . . . . . . . . . . . . 483\n8.6 Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nCheckpointing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493\nMessage logging . . . . . . . . . . . . . . . . . . . . . . . . . . . 496\nRecovery-oriented computing . . . . . . . . . . . . . . . . . . . . 498\n8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499\n9 Security 501\n9.1 Introduction to security . . . . . . . . . . . . . . . . . . . . . . . 502\nSecurity threats, policies, and mechanisms . . . . . . . . . . . . 502\nDesign issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504\nCryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n9.2 Secure channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\nAuthentication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513\nMessage integrity and con\ufb01dentiality . . . . . . . . . . . . . . . 520\nSecure group communication . . . . . . . . . . . . . . . . . . . . 523\nExample: Kerberos . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n9.3 Access control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529\nGeneral issues in access control . . . . . . . . . . . . . . . . . . . 529\nFirewalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533\nSecure mobile code . . . . . . . . . . . . . . . . . . . . . . . . . . 535\nDenial of service . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\n9.4 Secure naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540\n9.5 Security management . . . . . . . . . . . . . . . . . . . . . . . . 541\nKey management . . . . . . . . . . . . . . . . . . . . . . . . . . . 542\nSecure group management . . . . . . . . . . . . . . . . . . . . . 545\nAuthorization management . . . . . . . . . . . . . . . . . . . . . 547\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552\nBibliography 555\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "", "Preface\nThis is the third edition of \u201cDistributed Systems.\u201d In many ways, it is a\nhuge difference compared to the previous editions, the most important one\nperhaps being that we have fully integrated the \u201cprinciples\u201d and \u201cparadigms\u201d\nby including the latter at appropriate places in the chapters that discussed the\nprinciples of distributed systems.\nThe material has been thoroughly revised and extended, while at the same\ntime we were keen on limiting the total number of pages. The size of book\nhas been reduced by more than 10% compared to the second edition, which is\nmainly due to removing material on paradigms. To make it easier to study\nthe material by a wide range of readers, we have moved speci\ufb01c material to\nseparate boxed sections. These sections can be skipped on \ufb01rst reading.\nAnother major difference is the use of coded examples, all written in\nPython and supported by a simple communication system wrapped around\ntheRedis package. The examples in the book leave out many details for read-\nability, but the complete examples are available through the book\u2019s Website,\nhosted at www.distributed-systems.net . Next to code for running, testing,\nand extending algorithms, the site provides access to slides, all \ufb01gures, and\nexercises.\nThe new material has been classroom tested, for which we particularly\nthank Thilo Kielmann at VU University Amsterdam. His constructive and\ncritical observatiions have helped us improve matters considerably.\nOur publisher Pearson Education was kind enough to return the copy-\nrights, and we owe many thanks to Tracy Johnson for making this a smooth\ntransition. Having the copyrights back has made it possible for us to start\nwith something that we both feel comfortable with: running experiments. In\nthis case, we were looking for a means that would make the material easy\nto access, relatively inexpensive to obtain, and manageable when it came to\nupgrades. The book can now be (freely) downloaded, making it much easier\nto use hyperlinks where appropriate. At the same time, we are offering a\nprinted version through Amazon.com, available at minimal costs.\nThe book now being fully digital allows us to incorporate updates when\nxi\nPreface\nThis is the third edition of \u201cDistributed Systems.\u201d In many ways, it is a\nhuge difference compared to the previous editions, the most important one\nperhaps being that we have fully integrated the \u201cprinciples\u201d and \u201cparadigms\u201d\nby including the latter at appropriate places in the chapters that discussed the\nprinciples of distributed systems.\nThe material has been thoroughly revised and extended, while at the same\ntime we were keen on limiting the total number of pages. The size of book\nhas been reduced by more than 10% compared to the second edition, which is\nmainly due to removing material on paradigms. To make it easier to study\nthe material by a wide range of readers, we have moved speci\ufb01c material to\nseparate boxed sections. These sections can be skipped on \ufb01rst reading.\nAnother major difference is the use of coded examples, all written in\nPython and supported by a simple communication system wrapped around\ntheRedis package. The examples in the book leave out many details for read-\nability, but the complete examples are available through the book\u2019s Website,\nhosted at www.distributed-systems.net . Next to code for running, testing,\nand extending algorithms, the site provides access to slides, all \ufb01gures, and\nexercises.\nThe new material has been classroom tested, for which we particularly\nthank Thilo Kielmann at VU University Amsterdam. His constructive and\ncritical observatiions have helped us improve matters considerably.\nOur publisher Pearson Education was kind enough to return the copy-\nrights, and we owe many thanks to Tracy Johnson for making this a smooth\ntransition. Having the copyrights back has made it possible for us to start\nwith something that we both feel comfortable with: running experiments. In\nthis case, we were looking for a means that would make the material easy\nto access, relatively inexpensive to obtain, and manageable when it came to\nupgrades. The book can now be (freely) downloaded, making it much easier\nto use hyperlinks where appropriate. At the same time, we are offering a\nprinted version through Amazon.com, available at minimal costs.\nThe book now being fully digital allows us to incorporate updates when\nxi", "xii PREFACE\nneeded. We plan to run updates on a yearly basis, while keeping previous\nversions digitally available, as well as the printed versions for some \ufb01xed\nperiod. Running frequent updates is not always the right thing to do from the\nperspective of teaching, but yearly updates and maintaining previous versions\nseems a good compromise.\nMaarten van Steen\nAndrew S. Tanenbaum\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\nxii PREFACE\nneeded. We plan to run updates on a yearly basis, while keeping previous\nversions digitally available, as well as the printed versions for some \ufb01xed\nperiod. Running frequent updates is not always the right thing to do from the\nperspective of teaching, but yearly updates and maintaining previous versions\nseems a good compromise.\nMaarten van Steen\nAndrew S. Tanenbaum\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 1\nIntroduction\nThe pace at which computer systems change was, is, and continues to be\noverwhelming. From 1945, when the modern computer era began, until about\n1985, computers were large and expensive. Moreover, for lack of a way to\nconnect them, these computers operated independently from one another.\nStarting in the mid-1980s, however, two advances in technology began to\nchange that situation. The \ufb01rst was the development of powerful microproces-\nsors. Initially, these were 8-bit machines, but soon 16-, 32-, and 64-bit CPUs\nbecame common. With multicore CPUs, we now are refacing the challenge\nof adapting and developing programs to exploit parallelism. In any case, the\ncurrent generation of machines have the computing power of the mainframes\ndeployed 30 or 40 years ago, but for 1/1000th of the price or less.\nThe second development was the invention of high-speed computer net-\nworks. Local-area networks orLANs allow thousands of machines within a\nbuilding to be connected in such a way that small amounts of information\ncan be transferred in a few microseconds or so. Larger amounts of data\ncan be moved between machines at rates of billions of bits per second (bps).\nWide-area networks orWANs allow hundreds of millions of machines all\nover the earth to be connected at speeds varying from tens of thousands to\nhundreds of millions bps.\nParallel to the development of increasingly powerful and networked ma-\nchines, we have also been able to witness miniaturization of computer systems\nwith perhaps the smartphone as the most impressive outcome. Packed with\nsensors, lots of memory, and a powerful CPU, these devices are nothing less\nthan full-\ufb02edged computers. Of course, they also have networking capabilities.\nAlong the same lines, so-called plug computers are \ufb01nding their way to the\nA version of this chapter has been published as \u201cA Brief Introduction to Distributed Systems,\u201d\nComputing, vol. 98(10):967-1009, 2016.\n1\nChapter 1\nIntroduction\nThe pace at which computer systems change was, is, and continues to be\noverwhelming. From 1945, when the modern computer era began, until about\n1985, computers were large and expensive. Moreover, for lack of a way to\nconnect them, these computers operated independently from one another.\nStarting in the mid-1980s, however, two advances in technology began to\nchange that situation. The \ufb01rst was the development of powerful microproces-\nsors. Initially, these were 8-bit machines, but soon 16-, 32-, and 64-bit CPUs\nbecame common. With multicore CPUs, we now are refacing the challenge\nof adapting and developing programs to exploit parallelism. In any case, the\ncurrent generation of machines have the computing power of the mainframes\ndeployed 30 or 40 years ago, but for 1/1000th of the price or less.\nThe second development was the invention of high-speed computer net-\nworks. Local-area networks orLANs allow thousands of machines within a\nbuilding to be connected in such a way that small amounts of information\ncan be transferred in a few microseconds or so. Larger amounts of data\ncan be moved between machines at rates of billions of bits per second (bps).\nWide-area networks orWANs allow hundreds of millions of machines all\nover the earth to be connected at speeds varying from tens of thousands to\nhundreds of millions bps.\nParallel to the development of increasingly powerful and networked ma-\nchines, we have also been able to witness miniaturization of computer systems\nwith perhaps the smartphone as the most impressive outcome. Packed with\nsensors, lots of memory, and a powerful CPU, these devices are nothing less\nthan full-\ufb02edged computers. Of course, they also have networking capabilities.\nAlong the same lines, so-called plug computers are \ufb01nding their way to the\nA version of this chapter has been published as \u201cA Brief Introduction to Distributed Systems,\u201d\nComputing, vol. 98(10):967-1009, 2016.\n1", "2 CHAPTER 1. INTRODUCTION\nmarket. These small computers, often the size of a power adapter, can be\nplugged directly into an outlet and offer near-desktop performance.\nThe result of these technologies is that it is now not only feasible, but\neasy, to put together a computing system composed of a large numbers of\nnetworked computers, be they large or small. These computers are generally\ngeographically dispersed, for which reason they are usually said to form a\ndistributed system . The size of a distributed system may vary from a handful\nof devices, to millions of computers. The interconnection network may be\nwired, wireless, or a combination of both. Moreover, distributed systems are\noften highly dynamic, in the sense that computers can join and leave, with the\ntopology and performance of the underlying network almost continuously\nchanging.\nIn this chapter, we provide an initial exploration of distributed systems\nand their design goals, and follow that up by discussing some well-known\ntypes of systems.\n1.1 What is a distributed system?\nVarious de\ufb01nitions of distributed systems have been given in the literature,\nnone of them satisfactory, and none of them in agreement with any of the\nothers. For our purposes it is suf\ufb01cient to give a loose characterization:\nA distributed system is a collection of autonomous computing elements\nthat appears to its users as a single coherent system.\nThis de\ufb01nition refers to two characteristic features of distributed systems.\nThe \ufb01rst one is that a distributed system is a collection of computing elements\neach being able to behave independently of each other. A computing element,\nwhich we will generally refer to as a node , can be either a hardware device\nor a software process. A second feature is that users (be they people or\napplications) believe they are dealing with a single system. This means\nthat one way or another the autonomous nodes need to collaborate. How\nto establish this collaboration lies at the heart of developing distributed\nsystems. Note that we are not making any assumptions concerning the\ntype of nodes. In principle, even within a single system, they could range\nfrom high-performance mainframe computers to small devices in sensor\nnetworks. Likewise, we make no assumptions concerning the way that nodes\nare interconnected.\nCharacteristic 1: Collection of autonomous computing elements\nModern distributed systems can, and often will, consist of all kinds of nodes,\nranging from very big high-performance computers to small plug computers\nor even smaller devices. A fundamental principle is that nodes can act inde-\npendently from each other, although it should be obvious that if they ignore\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n2 CHAPTER 1. INTRODUCTION\nmarket. These small computers, often the size of a power adapter, can be\nplugged directly into an outlet and offer near-desktop performance.\nThe result of these technologies is that it is now not only feasible, but\neasy, to put together a computing system composed of a large numbers of\nnetworked computers, be they large or small. These computers are generally\ngeographically dispersed, for which reason they are usually said to form a\ndistributed system . The size of a distributed system may vary from a handful\nof devices, to millions of computers. The interconnection network may be\nwired, wireless, or a combination of both. Moreover, distributed systems are\noften highly dynamic, in the sense that computers can join and leave, with the\ntopology and performance of the underlying network almost continuously\nchanging.\nIn this chapter, we provide an initial exploration of distributed systems\nand their design goals, and follow that up by discussing some well-known\ntypes of systems.\n1.1 What is a distributed system?\nVarious de\ufb01nitions of distributed systems have been given in the literature,\nnone of them satisfactory, and none of them in agreement with any of the\nothers. For our purposes it is suf\ufb01cient to give a loose characterization:\nA distributed system is a collection of autonomous computing elements\nthat appears to its users as a single coherent system.\nThis de\ufb01nition refers to two characteristic features of distributed systems.\nThe \ufb01rst one is that a distributed system is a collection of computing elements\neach being able to behave independently of each other. A computing element,\nwhich we will generally refer to as a node , can be either a hardware device\nor a software process. A second feature is that users (be they people or\napplications) believe they are dealing with a single system. This means\nthat one way or another the autonomous nodes need to collaborate. How\nto establish this collaboration lies at the heart of developing distributed\nsystems. Note that we are not making any assumptions concerning the\ntype of nodes. In principle, even within a single system, they could range\nfrom high-performance mainframe computers to small devices in sensor\nnetworks. Likewise, we make no assumptions concerning the way that nodes\nare interconnected.\nCharacteristic 1: Collection of autonomous computing elements\nModern distributed systems can, and often will, consist of all kinds of nodes,\nranging from very big high-performance computers to small plug computers\nor even smaller devices. A fundamental principle is that nodes can act inde-\npendently from each other, although it should be obvious that if they ignore\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.1. WHAT IS A DISTRIBUTED SYSTEM? 3\neach other, then there is no use in putting them into the same distributed\nsystem. In practice, nodes are programmed to achieve common goals, which\nare realized by exchanging messages with each other. A node reacts to in-\ncoming messages, which are then processed and, in turn, leading to further\ncommunication through message passing.\nAn important observation is that, as a consequence of dealing with inde-\npendent nodes, each one will have its own notion of time. In other words, we\ncannot always assume that there is something like a global clock . This lack\nof a common reference of time leads to fundamental questions regarding the\nsynchronization and coordination within a distributed system, which we will\ncome to discuss extensively in Chapter 6. The fact that we are dealing with a\ncollection of nodes implies that we may also need to manage the membership\nand organization of that collection. In other words, we may need to register\nwhich nodes may or may not belong to the system, and also provide each\nmember with a list of nodes it can directly communicate with.\nManaging group membership can be exceedingly dif\ufb01cult, if only for\nreasons of admission control. To explain, we make a distinction between\nopen and closed groups. In an open group , any node is allowed to join the\ndistributed system, effectively meaning that it can send messages to any other\nnode in the system. In contrast, with a closed group , only the members of\nthat group can communicate with each other and a separate mechanism is\nneeded to let a node join or leave the group.\nIt is not dif\ufb01cult to see that admission control can be dif\ufb01cult. First, a\nmechanism is needed to authenticate a node, and as we shall see in Chap-\nter 9, if not properly designed, managing authentication can easily create\na scalability bottleneck. Second, each node must, in principle, check if it is\nindeed communicating with another group member and not, for example,\nwith an intruder aiming to create havoc. Finally, considering that a member\ncan easily communicate with nonmembers, if con\ufb01dentiality is an issue in the\ncommunication within the distributed system, we may be facing trust issues.\nConcerning the organization of the collection, practice shows that a dis-\ntributed system is often organized as an overlay network [Tarkoma, 2010]. In\nthis case, a node is typically a software process equipped with a list of other\nprocesses it can directly send messages to. It may also be the case that a neigh-\nbor needs to be \ufb01rst looked up. Message passing is then done through TCP/IP\nor UDP channels, but as we shall see in Chapter 4, higher-level facilities may\nbe available as well. There are roughly two types of overlay networks:\nStructured overlay: In this case, each node has a well-de\ufb01ned set of neighbors\nwith whom it can communicate. For example, the nodes are organized\nin a tree or logical ring.\nUnstructured overlay: In these overlays, each node has a number of refer-\nences to randomly selected other nodes.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.1. WHAT IS A DISTRIBUTED SYSTEM? 3\neach other, then there is no use in putting them into the same distributed\nsystem. In practice, nodes are programmed to achieve common goals, which\nare realized by exchanging messages with each other. A node reacts to in-\ncoming messages, which are then processed and, in turn, leading to further\ncommunication through message passing.\nAn important observation is that, as a consequence of dealing with inde-\npendent nodes, each one will have its own notion of time. In other words, we\ncannot always assume that there is something like a global clock . This lack\nof a common reference of time leads to fundamental questions regarding the\nsynchronization and coordination within a distributed system, which we will\ncome to discuss extensively in Chapter 6. The fact that we are dealing with a\ncollection of nodes implies that we may also need to manage the membership\nand organization of that collection. In other words, we may need to register\nwhich nodes may or may not belong to the system, and also provide each\nmember with a list of nodes it can directly communicate with.\nManaging group membership can be exceedingly dif\ufb01cult, if only for\nreasons of admission control. To explain, we make a distinction between\nopen and closed groups. In an open group , any node is allowed to join the\ndistributed system, effectively meaning that it can send messages to any other\nnode in the system. In contrast, with a closed group , only the members of\nthat group can communicate with each other and a separate mechanism is\nneeded to let a node join or leave the group.\nIt is not dif\ufb01cult to see that admission control can be dif\ufb01cult. First, a\nmechanism is needed to authenticate a node, and as we shall see in Chap-\nter 9, if not properly designed, managing authentication can easily create\na scalability bottleneck. Second, each node must, in principle, check if it is\nindeed communicating with another group member and not, for example,\nwith an intruder aiming to create havoc. Finally, considering that a member\ncan easily communicate with nonmembers, if con\ufb01dentiality is an issue in the\ncommunication within the distributed system, we may be facing trust issues.\nConcerning the organization of the collection, practice shows that a dis-\ntributed system is often organized as an overlay network [Tarkoma, 2010]. In\nthis case, a node is typically a software process equipped with a list of other\nprocesses it can directly send messages to. It may also be the case that a neigh-\nbor needs to be \ufb01rst looked up. Message passing is then done through TCP/IP\nor UDP channels, but as we shall see in Chapter 4, higher-level facilities may\nbe available as well. There are roughly two types of overlay networks:\nStructured overlay: In this case, each node has a well-de\ufb01ned set of neighbors\nwith whom it can communicate. For example, the nodes are organized\nin a tree or logical ring.\nUnstructured overlay: In these overlays, each node has a number of refer-\nences to randomly selected other nodes.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "4 CHAPTER 1. INTRODUCTION\nIn any case, an overlay network should, in principle, always be connected ,\nmeaning that between any two nodes there is always a communication path\nallowing those nodes to route messages from one to the other. A well-known\nclass of overlays is formed by peer-to-peer (P2P) networks . Examples of\noverlays will be discussed in detail in Chapter 2 and later chapters. It is\nimportant to realize that the organization of nodes requires special effort and\nthat it is sometimes one of the more intricate parts of distributed-systems\nmanagement.\nCharacteristic 2: Single coherent system\nAs mentioned, a distributed system should appear as a single coherent system.\nIn some cases, researchers have even gone so far as to say that there should be\na single-system view, meaning that end users should not even notice that they\nare dealing with the fact that processes, data, and control are dispersed across\na computer network. Achieving a single-system view is often asking too much,\nfor which reason, in our de\ufb01nition of a distributed system, we have opted for\nsomething weaker, namely that it appears to be coherent. Roughly speaking, a\ndistributed system is coherent if it behaves according to the expectations of its\nusers. More speci\ufb01cally, in a single coherent system the collection of nodes\nas a whole operates the same, no matter where, when, and how interaction\nbetween a user and the system takes place.\nOffering a single coherent view is often challenging enough. For example,\nit requires that an end user would not be able to tell exactly on which computer\na process is currently executing, or even perhaps that part of a task has\nbeen spawned off to another process executing somewhere else. Likewise,\nwhere data is stored should be of no concern, and neither should it matter\nthat the system may be replicating data to enhance performance. This so-\ncalled distribution transparency , which we will discuss more extensively in\nSection 1.2, is an important design goal of distributed systems. In a sense, it\nis akin to the approach taken in many Unix-like operating systems in which\nresources are accessed through a unifying \ufb01le-system interface, effectively\nhiding the differences between \ufb01les, storage devices, and main memory, but\nalso networks.\nHowever, striving for a single coherent system introduces an important\ntrade-off. As we cannot ignore the fact that a distributed system consists of\nmultiple, networked nodes, it is inevitable that at any time only a part of the\nsystem fails. This means that unexpected behavior in which, for example,\nsome applications may continue to execute successfully while others come\nto a grinding halt, is a reality that needs to be dealt with. Although partial\nfailures are inherent to any complex system, in distributed systems they are\nparticularly dif\ufb01cult to hide. It lead Turing-Award winner Leslie Lamport, to\ndescribe a distributed system as \u201c[ . . .] one in which the failure of a computer\nyou didn\u2019t even know existed can render your own computer unusable.\u201d\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n4 CHAPTER 1. INTRODUCTION\nIn any case, an overlay network should, in principle, always be connected ,\nmeaning that between any two nodes there is always a communication path\nallowing those nodes to route messages from one to the other. A well-known\nclass of overlays is formed by peer-to-peer (P2P) networks . Examples of\noverlays will be discussed in detail in Chapter 2 and later chapters. It is\nimportant to realize that the organization of nodes requires special effort and\nthat it is sometimes one of the more intricate parts of distributed-systems\nmanagement.\nCharacteristic 2: Single coherent system\nAs mentioned, a distributed system should appear as a single coherent system.\nIn some cases, researchers have even gone so far as to say that there should be\na single-system view, meaning that end users should not even notice that they\nare dealing with the fact that processes, data, and control are dispersed across\na computer network. Achieving a single-system view is often asking too much,\nfor which reason, in our de\ufb01nition of a distributed system, we have opted for\nsomething weaker, namely that it appears to be coherent. Roughly speaking, a\ndistributed system is coherent if it behaves according to the expectations of its\nusers. More speci\ufb01cally, in a single coherent system the collection of nodes\nas a whole operates the same, no matter where, when, and how interaction\nbetween a user and the system takes place.\nOffering a single coherent view is often challenging enough. For example,\nit requires that an end user would not be able to tell exactly on which computer\na process is currently executing, or even perhaps that part of a task has\nbeen spawned off to another process executing somewhere else. Likewise,\nwhere data is stored should be of no concern, and neither should it matter\nthat the system may be replicating data to enhance performance. This so-\ncalled distribution transparency , which we will discuss more extensively in\nSection 1.2, is an important design goal of distributed systems. In a sense, it\nis akin to the approach taken in many Unix-like operating systems in which\nresources are accessed through a unifying \ufb01le-system interface, effectively\nhiding the differences between \ufb01les, storage devices, and main memory, but\nalso networks.\nHowever, striving for a single coherent system introduces an important\ntrade-off. As we cannot ignore the fact that a distributed system consists of\nmultiple, networked nodes, it is inevitable that at any time only a part of the\nsystem fails. This means that unexpected behavior in which, for example,\nsome applications may continue to execute successfully while others come\nto a grinding halt, is a reality that needs to be dealt with. Although partial\nfailures are inherent to any complex system, in distributed systems they are\nparticularly dif\ufb01cult to hide. It lead Turing-Award winner Leslie Lamport, to\ndescribe a distributed system as \u201c[ . . .] one in which the failure of a computer\nyou didn\u2019t even know existed can render your own computer unusable.\u201d\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.1. WHAT IS A DISTRIBUTED SYSTEM? 5\nMiddleware and distributed systems\nTo assist the development of distributed applications, distributed systems are\noften organized to have a separate layer of software that is logically placed on\ntop of the respective operating systems of the computers that are part of the\nsystem. This organization is shown in Figure 1.1, leading to what is known as\nmiddleware [Bernstein, 1996].\nFigure 1.1: A distributed system organized in a middleware layer, which\nextends over multiple machines, offering each application the same interface.\nFigure 1.1 shows four networked computers and three applications, of\nwhich application Bis distributed across computers 2 and 3. Each application\nis offered the same interface. The distributed system provides the means for\ncomponents of a single distributed application to communicate with each\nother, but also to let different applications communicate. At the same time,\nit hides, as best and reasonably as possible, the differences in hardware and\noperating systems from each application.\nIn a sense, middleware is the same to a distributed system as what an\noperating system is to a computer: a manager of resources offering its ap-\nplications to ef\ufb01ciently share and deploy those resources across a network.\nNext to resource management, it offers services that can also be found in most\noperating systems, including:\n\u2022 Facilities for interapplication communication.\n\u2022 Security services.\n\u2022 Accounting services.\n\u2022 Masking of and recovery from failures.\nThe main difference with their operating-system equivalents, is that mid-\ndleware services are offered in a networked environment. Note also that\nmost services are useful to many applications. In this sense, middleware can\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.1. WHAT IS A DISTRIBUTED SYSTEM? 5\nMiddleware and distributed systems\nTo assist the development of distributed applications, distributed systems are\noften organized to have a separate layer of software that is logically placed on\ntop of the respective operating systems of the computers that are part of the\nsystem. This organization is shown in Figure 1.1, leading to what is known as\nmiddleware [Bernstein, 1996].\nFigure 1.1: A distributed system organized in a middleware layer, which\nextends over multiple machines, offering each application the same interface.\nFigure 1.1 shows four networked computers and three applications, of\nwhich application Bis distributed across computers 2 and 3. Each application\nis offered the same interface. The distributed system provides the means for\ncomponents of a single distributed application to communicate with each\nother, but also to let different applications communicate. At the same time,\nit hides, as best and reasonably as possible, the differences in hardware and\noperating systems from each application.\nIn a sense, middleware is the same to a distributed system as what an\noperating system is to a computer: a manager of resources offering its ap-\nplications to ef\ufb01ciently share and deploy those resources across a network.\nNext to resource management, it offers services that can also be found in most\noperating systems, including:\n\u2022 Facilities for interapplication communication.\n\u2022 Security services.\n\u2022 Accounting services.\n\u2022 Masking of and recovery from failures.\nThe main difference with their operating-system equivalents, is that mid-\ndleware services are offered in a networked environment. Note also that\nmost services are useful to many applications. In this sense, middleware can\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "6 CHAPTER 1. INTRODUCTION\nalso be viewed as a container of commonly used components and functions\nthat now no longer have to be implemented by applications separately. To\nfurther illustrate these points, let us brie\ufb02y consider a few examples of typical\nmiddleware services.\nCommunication: A common communication service is the so-called Remote\nProcedure Call (RPC ). An RPC service, to which we return in Chapter 4,\nallows an application to invoke a function that is implemented and\nexecuted on a remote computer as if it was locally available. To this\nend, a developer need merely specify the function header expressed in\na special programming language, from which the RPC subsystem can\nthen generate the necessary code that establishes remote invocations.\nTransactions: Many applications make use of multiple services that are dis-\ntributed among several computers. Middleware generally offers special\nsupport for executing such services in an all-or-nothing fashion, com-\nmonly referred to as an atomic transaction . In this case, the application\ndeveloper need only specify the remote services involved, and by fol-\nlowing a standardized protocol, the middleware makes sure that every\nservice is invoked, or none at all.\nService composition: It is becoming increasingly common to develop new\napplications by taking existing programs and gluing them together. This\nis notably the case for many Web-based applications, in particular those\nknown as Web services [Alonso et al., 2004]. Web-based middleware can\nhelp by standardizing the way Web services are accessed and providing\nthe means to generate their functions in a speci\ufb01c order. A simple\nexample of how service composition is deployed is formed by mashups :\nWeb pages that combine and aggregate data from different sources.\nWell-known mashups are those based on Google maps in which maps\nare enhanced with extra information such as trip planners or real-time\nweather forecasts.\nReliability: As a last example, there has been a wealth of research on pro-\nviding enhanced functions for building reliable distributed applications.\nThe Horus toolkit [van Renesse et al., 1994] allows a developer to build\nan application as a group of processes such that any message sent by\none process is guaranteed to be received by all or no other process. As it\nturns out, such guarantees can greatly simplify developing distributed\napplications and are typically implemented as part of the middleware.\nNote 1.1 (Historical note: The term middleware)\nAlthough the term middleware became popular in the mid 1990s, it was most\nlikely mentioned for the \ufb01rst time in a report on a NATO software engineering\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n6 CHAPTER 1. INTRODUCTION\nalso be viewed as a container of commonly used components and functions\nthat now no longer have to be implemented by applications separately. To\nfurther illustrate these points, let us brie\ufb02y consider a few examples of typical\nmiddleware services.\nCommunication: A common communication service is the so-called Remote\nProcedure Call (RPC ). An RPC service, to which we return in Chapter 4,\nallows an application to invoke a function that is implemented and\nexecuted on a remote computer as if it was locally available. To this\nend, a developer need merely specify the function header expressed in\na special programming language, from which the RPC subsystem can\nthen generate the necessary code that establishes remote invocations.\nTransactions: Many applications make use of multiple services that are dis-\ntributed among several computers. Middleware generally offers special\nsupport for executing such services in an all-or-nothing fashion, com-\nmonly referred to as an atomic transaction . In this case, the application\ndeveloper need only specify the remote services involved, and by fol-\nlowing a standardized protocol, the middleware makes sure that every\nservice is invoked, or none at all.\nService composition: It is becoming increasingly common to develop new\napplications by taking existing programs and gluing them together. This\nis notably the case for many Web-based applications, in particular those\nknown as Web services [Alonso et al., 2004]. Web-based middleware can\nhelp by standardizing the way Web services are accessed and providing\nthe means to generate their functions in a speci\ufb01c order. A simple\nexample of how service composition is deployed is formed by mashups :\nWeb pages that combine and aggregate data from different sources.\nWell-known mashups are those based on Google maps in which maps\nare enhanced with extra information such as trip planners or real-time\nweather forecasts.\nReliability: As a last example, there has been a wealth of research on pro-\nviding enhanced functions for building reliable distributed applications.\nThe Horus toolkit [van Renesse et al., 1994] allows a developer to build\nan application as a group of processes such that any message sent by\none process is guaranteed to be received by all or no other process. As it\nturns out, such guarantees can greatly simplify developing distributed\napplications and are typically implemented as part of the middleware.\nNote 1.1 (Historical note: The term middleware)\nAlthough the term middleware became popular in the mid 1990s, it was most\nlikely mentioned for the \ufb01rst time in a report on a NATO software engineering\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 7\nconference, edited by Peter Naur and Brian Randell in October 1968 [Naur and\nRandell, 1968]. Indeed, middleware was placed precisely between applications\nand service routines (the equivalent of operating systems).\n1.2 Design goals\nJust because it is possible to build distributed systems does not necessarily\nmean that it is a good idea. In this section we discuss four important goals\nthat should be met to make building a distributed system worth the effort. A\ndistributed system should make resources easily accessible; it should hide the\nfact that resources are distributed across a network; it should be open; and it\nshould be scalable.\nSupporting resource sharing\nAn important goal of a distributed system is to make it easy for users (and\napplications) to access and share remote resources. Resources can be virtually\nanything, but typical examples include peripherals, storage facilities, data,\n\ufb01les, services, and networks, to name just a few. There are many reasons for\nwanting to share resources. One obvious reason is that of economics. For\nexample, it is cheaper to have a single high-end reliable storage facility be\nshared than having to buy and maintain storage for each user separately.\nConnecting users and resources also makes it easier to collaborate and\nexchange information, as is illustrated by the success of the Internet with\nits simple protocols for exchanging \ufb01les, mail, documents, audio, and video.\nThe connectivity of the Internet has allowed geographically widely dispersed\ngroups of people to work together by means of all kinds of groupware , that is,\nsoftware for collaborative editing, teleconferencing, and so on, as is illustrated\nby multinational software-development companies that have outsourced much\nof their code production to Asia.\nHowever, resource sharing in distributed systems is perhaps best illustrated\nby the success of \ufb01le-sharing peer-to-peer networks like BitTorrent. These\ndistributed systems make it extremely simple for users to share \ufb01les across\nthe Internet. Peer-to-peer networks are often associated with distribution of\nmedia \ufb01les such as audio and video. In other cases, the technology is used for\ndistributing large amounts of data, as in the case of software updates, backup\nservices, and data synchronization across multiple servers.\nNote 1.2 (More information: Sharing folders worldwide)\nTo illustrate where we stand when it comes to seamless integration of resource-\nsharing facilities in a networked environment, Web-based services are now de-\nployed that allow a group of users to place \ufb01les into a special shared folder that is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 7\nconference, edited by Peter Naur and Brian Randell in October 1968 [Naur and\nRandell, 1968]. Indeed, middleware was placed precisely between applications\nand service routines (the equivalent of operating systems).\n1.2 Design goals\nJust because it is possible to build distributed systems does not necessarily\nmean that it is a good idea. In this section we discuss four important goals\nthat should be met to make building a distributed system worth the effort. A\ndistributed system should make resources easily accessible; it should hide the\nfact that resources are distributed across a network; it should be open; and it\nshould be scalable.\nSupporting resource sharing\nAn important goal of a distributed system is to make it easy for users (and\napplications) to access and share remote resources. Resources can be virtually\nanything, but typical examples include peripherals, storage facilities, data,\n\ufb01les, services, and networks, to name just a few. There are many reasons for\nwanting to share resources. One obvious reason is that of economics. For\nexample, it is cheaper to have a single high-end reliable storage facility be\nshared than having to buy and maintain storage for each user separately.\nConnecting users and resources also makes it easier to collaborate and\nexchange information, as is illustrated by the success of the Internet with\nits simple protocols for exchanging \ufb01les, mail, documents, audio, and video.\nThe connectivity of the Internet has allowed geographically widely dispersed\ngroups of people to work together by means of all kinds of groupware , that is,\nsoftware for collaborative editing, teleconferencing, and so on, as is illustrated\nby multinational software-development companies that have outsourced much\nof their code production to Asia.\nHowever, resource sharing in distributed systems is perhaps best illustrated\nby the success of \ufb01le-sharing peer-to-peer networks like BitTorrent. These\ndistributed systems make it extremely simple for users to share \ufb01les across\nthe Internet. Peer-to-peer networks are often associated with distribution of\nmedia \ufb01les such as audio and video. In other cases, the technology is used for\ndistributing large amounts of data, as in the case of software updates, backup\nservices, and data synchronization across multiple servers.\nNote 1.2 (More information: Sharing folders worldwide)\nTo illustrate where we stand when it comes to seamless integration of resource-\nsharing facilities in a networked environment, Web-based services are now de-\nployed that allow a group of users to place \ufb01les into a special shared folder that is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "8 CHAPTER 1. INTRODUCTION\nmaintained by a third party somewhere on the Internet. Using special software,\nthe shared folder is barely distinguishable from other folders on a user\u2019s computer.\nIn effect, these services replace the use of a shared directory on a local distributed\n\ufb01le system, making data available to users independent of the organization they\nbelong to, and independent of where they are. The service is offered for different\noperating systems. Where exactly data are stored is completely hidden from the\nend user.\nMaking distribution transparent\nAn important goal of a distributed system is to hide the fact that its processes\nand resources are physically distributed across multiple computers possibly\nseparated by large distances. In other words, it tries to make the distribution\nof processes and resources transparent , that is, invisible, to end users and\napplications.\nTypes of distribution transparency\nThe concept of transparency can be applied to several aspects of a distributed\nsystem, of which the most important ones are listed in Figure 1.2. We use the\nterm object to mean either a process or a resource.\nTransparency Description\nAccess Hide differences in data representation and how an object is\naccessed\nLocation Hide where an object is located\nRelocation Hide that an object may be moved to another location while\nin use\nMigration Hide that an object may move to another location\nReplication Hide that an object is replicated\nConcurrency Hide that an object may be shared by several independent\nusers\nFailure Hide the failure and recovery of an object\nFigure 1.2: Different forms of transparency in a distributed system (see ISO\n[1995]). An object can be a resource or a process.\nAccess transparency deals with hiding differences in data representation\nand the way that objects can be accessed. At a basic level, we want to hide\ndifferences in machine architectures, but more important is that we reach\nagreement on how data is to be represented by different machines and operat-\ning systems. For example, a distributed system may have computer systems\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n8 CHAPTER 1. INTRODUCTION\nmaintained by a third party somewhere on the Internet. Using special software,\nthe shared folder is barely distinguishable from other folders on a user\u2019s computer.\nIn effect, these services replace the use of a shared directory on a local distributed\n\ufb01le system, making data available to users independent of the organization they\nbelong to, and independent of where they are. The service is offered for different\noperating systems. Where exactly data are stored is completely hidden from the\nend user.\nMaking distribution transparent\nAn important goal of a distributed system is to hide the fact that its processes\nand resources are physically distributed across multiple computers possibly\nseparated by large distances. In other words, it tries to make the distribution\nof processes and resources transparent , that is, invisible, to end users and\napplications.\nTypes of distribution transparency\nThe concept of transparency can be applied to several aspects of a distributed\nsystem, of which the most important ones are listed in Figure 1.2. We use the\nterm object to mean either a process or a resource.\nTransparency Description\nAccess Hide differences in data representation and how an object is\naccessed\nLocation Hide where an object is located\nRelocation Hide that an object may be moved to another location while\nin use\nMigration Hide that an object may move to another location\nReplication Hide that an object is replicated\nConcurrency Hide that an object may be shared by several independent\nusers\nFailure Hide the failure and recovery of an object\nFigure 1.2: Different forms of transparency in a distributed system (see ISO\n[1995]). An object can be a resource or a process.\nAccess transparency deals with hiding differences in data representation\nand the way that objects can be accessed. At a basic level, we want to hide\ndifferences in machine architectures, but more important is that we reach\nagreement on how data is to be represented by different machines and operat-\ning systems. For example, a distributed system may have computer systems\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 9\nthat run different operating systems, each having their own \ufb01le-naming con-\nventions. Differences in naming conventions, differences in \ufb01le operations, or\ndifferences in how low-level communication with other processes is to take\nplace, are examples of access issues that should preferably be hidden from\nusers and applications.\nAn important group of transparency types concerns the location of a pro-\ncess or resource. Location transparency refers to the fact that users cannot\ntell where an object is physically located in the system. Naming plays an\nimportant role in achieving location transparency. In particular, location\ntransparency can often be achieved by assigning only logical names to re-\nsources, that is, names in which the location of a resource is not secretly\nencoded. An example of a such a name is the uniform resource locator (URL)\nhttp://www.prenhall.com/index.html , which gives no clue about the actual\nlocation of Prentice Hall\u2019s main Web server. The URL also gives no clue as\nto whether the \ufb01le index.html has always been at its current location or was\nrecently moved there. For example, the entire site may have been moved from\none data center to another, yet users should not notice. The latter is an exam-\nple of relocation transparency , which is becoming increasingly important in\nthe context of cloud computing to which we return later in this chapter.\nWhere relocation transparency refers to being moved by the distributed\nsystem, migration transparency is offered by a distributed system when it\nsupports the mobility of processes and resources initiated by users, with-\nout affecting ongoing communication and operations. A typical example\nis communication between mobile phones: regardless whether two people\nare actually moving, mobile phones will allow them to continue their con-\nversation. Other examples that come to mind include online tracking and\ntracing of goods as they are being transported from one place to another,\nand teleconferencing (partly) using devices that are equipped with mobile\nInternet.\nAs we shall see, replication plays an important role in distributed systems.\nFor example, resources may be replicated to increase availability or to im-\nprove performance by placing a copy close to the place where it is accessed.\nReplication transparency deals with hiding the fact that several copies of a\nresource exist, or that several processes are operating in some form of lockstep\nmode so that one can take over when another fails. To hide replication from\nusers, it is necessary that all replicas have the same name. Consequently,\na system that supports replication transparency should generally support\nlocation transparency as well, because it would otherwise be impossible to\nrefer to replicas at different locations.\nWe already mentioned that an important goal of distributed systems is\nto allow sharing of resources. In many cases, sharing resources is done in a\ncooperative way, as in the case of communication channels. However, there\nare also many examples of competitive sharing of resources. For example,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 9\nthat run different operating systems, each having their own \ufb01le-naming con-\nventions. Differences in naming conventions, differences in \ufb01le operations, or\ndifferences in how low-level communication with other processes is to take\nplace, are examples of access issues that should preferably be hidden from\nusers and applications.\nAn important group of transparency types concerns the location of a pro-\ncess or resource. Location transparency refers to the fact that users cannot\ntell where an object is physically located in the system. Naming plays an\nimportant role in achieving location transparency. In particular, location\ntransparency can often be achieved by assigning only logical names to re-\nsources, that is, names in which the location of a resource is not secretly\nencoded. An example of a such a name is the uniform resource locator (URL)\nhttp://www.prenhall.com/index.html , which gives no clue about the actual\nlocation of Prentice Hall\u2019s main Web server. The URL also gives no clue as\nto whether the \ufb01le index.html has always been at its current location or was\nrecently moved there. For example, the entire site may have been moved from\none data center to another, yet users should not notice. The latter is an exam-\nple of relocation transparency , which is becoming increasingly important in\nthe context of cloud computing to which we return later in this chapter.\nWhere relocation transparency refers to being moved by the distributed\nsystem, migration transparency is offered by a distributed system when it\nsupports the mobility of processes and resources initiated by users, with-\nout affecting ongoing communication and operations. A typical example\nis communication between mobile phones: regardless whether two people\nare actually moving, mobile phones will allow them to continue their con-\nversation. Other examples that come to mind include online tracking and\ntracing of goods as they are being transported from one place to another,\nand teleconferencing (partly) using devices that are equipped with mobile\nInternet.\nAs we shall see, replication plays an important role in distributed systems.\nFor example, resources may be replicated to increase availability or to im-\nprove performance by placing a copy close to the place where it is accessed.\nReplication transparency deals with hiding the fact that several copies of a\nresource exist, or that several processes are operating in some form of lockstep\nmode so that one can take over when another fails. To hide replication from\nusers, it is necessary that all replicas have the same name. Consequently,\na system that supports replication transparency should generally support\nlocation transparency as well, because it would otherwise be impossible to\nrefer to replicas at different locations.\nWe already mentioned that an important goal of distributed systems is\nto allow sharing of resources. In many cases, sharing resources is done in a\ncooperative way, as in the case of communication channels. However, there\nare also many examples of competitive sharing of resources. For example,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "10 CHAPTER 1. INTRODUCTION\ntwo independent users may each have stored their \ufb01les on the same \ufb01le server\nor may be accessing the same tables in a shared database. In such cases, it\nis important that each user does not notice that the other is making use of\nthe same resource. This phenomenon is called concurrency transparency .\nAn important issue is that concurrent access to a shared resource leaves that\nresource in a consistent state. Consistency can be achieved through locking\nmechanisms, by which users are, in turn, given exclusive access to the desired\nresource. A more re\ufb01ned mechanism is to make use of transactions, but these\nmay be dif\ufb01cult to implement in a distributed system, notably when scalability\nis an issue.\nLast, but certainly not least, it is important that a distributed system\nprovides failure transparency . This means that a user or application does not\nnotice that some piece of the system fails to work properly, and that the system\nsubsequently (and automatically) recovers from that failure. Masking failures\nis one of the hardest issues in distributed systems and is even impossible\nwhen certain apparently realistic assumptions are made, as we will discuss\nin Chapter 8. The main dif\ufb01culty in masking and transparently recovering\nfrom failures lies in the inability to distinguish between a dead process and a\npainfully slowly responding one. For example, when contacting a busy Web\nserver, a browser will eventually time out and report that the Web page is\nunavailable. At that point, the user cannot tell whether the server is actually\ndown or that the network is badly congested.\nDegree of distribution transparency\nAlthough distribution transparency is generally considered preferable for any\ndistributed system, there are situations in which attempting to blindly hide\nall distribution aspects from users is not a good idea. A simple example is\nrequesting your electronic newspaper to appear in your mailbox before 7 AM\nlocal time, as usual, while you are currently at the other end of the world\nliving in a different time zone. Your morning paper will not be the morning\npaper you are used to.\nLikewise, a wide-area distributed system that connects a process in San\nFrancisco to a process in Amsterdam cannot be expected to hide the fact\nthat Mother Nature will not allow it to send a message from one process to\nthe other in less than approximately 35 milliseconds. Practice shows that it\nactually takes several hundred milliseconds using a computer network. Signal\ntransmission is not only limited by the speed of light, but also by limited\nprocessing capacities and delays in the intermediate switches.\nThere is also a trade-off between a high degree of transparency and the\nperformance of a system. For example, many Internet applications repeatedly\ntry to contact a server before \ufb01nally giving up. Consequently, attempting to\nmask a transient server failure before trying another one may slow down the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n10 CHAPTER 1. INTRODUCTION\ntwo independent users may each have stored their \ufb01les on the same \ufb01le server\nor may be accessing the same tables in a shared database. In such cases, it\nis important that each user does not notice that the other is making use of\nthe same resource. This phenomenon is called concurrency transparency .\nAn important issue is that concurrent access to a shared resource leaves that\nresource in a consistent state. Consistency can be achieved through locking\nmechanisms, by which users are, in turn, given exclusive access to the desired\nresource. A more re\ufb01ned mechanism is to make use of transactions, but these\nmay be dif\ufb01cult to implement in a distributed system, notably when scalability\nis an issue.\nLast, but certainly not least, it is important that a distributed system\nprovides failure transparency . This means that a user or application does not\nnotice that some piece of the system fails to work properly, and that the system\nsubsequently (and automatically) recovers from that failure. Masking failures\nis one of the hardest issues in distributed systems and is even impossible\nwhen certain apparently realistic assumptions are made, as we will discuss\nin Chapter 8. The main dif\ufb01culty in masking and transparently recovering\nfrom failures lies in the inability to distinguish between a dead process and a\npainfully slowly responding one. For example, when contacting a busy Web\nserver, a browser will eventually time out and report that the Web page is\nunavailable. At that point, the user cannot tell whether the server is actually\ndown or that the network is badly congested.\nDegree of distribution transparency\nAlthough distribution transparency is generally considered preferable for any\ndistributed system, there are situations in which attempting to blindly hide\nall distribution aspects from users is not a good idea. A simple example is\nrequesting your electronic newspaper to appear in your mailbox before 7 AM\nlocal time, as usual, while you are currently at the other end of the world\nliving in a different time zone. Your morning paper will not be the morning\npaper you are used to.\nLikewise, a wide-area distributed system that connects a process in San\nFrancisco to a process in Amsterdam cannot be expected to hide the fact\nthat Mother Nature will not allow it to send a message from one process to\nthe other in less than approximately 35 milliseconds. Practice shows that it\nactually takes several hundred milliseconds using a computer network. Signal\ntransmission is not only limited by the speed of light, but also by limited\nprocessing capacities and delays in the intermediate switches.\nThere is also a trade-off between a high degree of transparency and the\nperformance of a system. For example, many Internet applications repeatedly\ntry to contact a server before \ufb01nally giving up. Consequently, attempting to\nmask a transient server failure before trying another one may slow down the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 11\nsystem as a whole. In such a case, it may have been better to give up earlier,\nor at least let the user cancel the attempts to make contact.\nAnother example is where we need to guarantee that several replicas,\nlocated on different continents, must be consistent all the time. In other words,\nif one copy is changed, that change should be propagated to all copies before\nallowing any other operation. It is clear that a single update operation may\nnow even take seconds to complete, something that cannot be hidden from\nusers.\nFinally, there are situations in which it is not at all obvious that hiding\ndistribution is a good idea. As distributed systems are expanding to devices\nthat people carry around and where the very notion of location and context\nawareness is becoming increasingly important, it may be best to actually expose\ndistribution rather than trying to hide it. An obvious example is making use\nof location-based services, which can often be found on mobile phones, such\nas \ufb01nding the nearest Chinese take-away or checking whether any of your\nfriends are nearby.\nThere are also other arguments against distribution transparency. Recog-\nnizing that full distribution transparency is simply impossible, we should ask\nourselves whether it is even wise to pretend that we can achieve it. It may\nbe much better to make distribution explicit so that the user and applica-\ntion developer are never tricked into believing that there is such a thing as\ntransparency. The result will be that users will much better understand the\n(sometimes unexpected) behavior of a distributed system, and are thus much\nbetter prepared to deal with this behavior.\nNote 1.3 (Discussion: Against distribution transparency)\nSeveral researchers have argued that hiding distribution will only lead to further\ncomplicating the development of distributed systems, exactly for the reason that\nfull distribution transparency can never be achieved. A popular technique for\nachieving access transparency is to extend procedure calls to remote servers. How-\never, Waldo et al. [1997] already pointed out that attempting to hide distribution\nby means of such remote procedure calls can lead to poorly understood semantics,\nfor the simple reason that a procedure call does change when executed over a\nfaulty communication link.\nAs an alternative, various researchers and practitioners are now arguing for\nless transparency, for example, by more explicitly using message-style commu-\nnication, or more explicitly posting requests to, and getting results from remote\nmachines, as is done in the Web when fetching pages. Such solutions will be\ndiscussed in detail in the next chapter.\nA somewhat radical standpoint is taken by Wams [2011] by stating that partial\nfailures preclude relying on the successful execution of a remote service. If such\nreliability cannot be guaranteed, it is then best to always perform only local\nexecutions, leading to the copy-before-use principle. According to this principle,\ndata can be accessed only after they have been transferred to the machine of the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 11\nsystem as a whole. In such a case, it may have been better to give up earlier,\nor at least let the user cancel the attempts to make contact.\nAnother example is where we need to guarantee that several replicas,\nlocated on different continents, must be consistent all the time. In other words,\nif one copy is changed, that change should be propagated to all copies before\nallowing any other operation. It is clear that a single update operation may\nnow even take seconds to complete, something that cannot be hidden from\nusers.\nFinally, there are situations in which it is not at all obvious that hiding\ndistribution is a good idea. As distributed systems are expanding to devices\nthat people carry around and where the very notion of location and context\nawareness is becoming increasingly important, it may be best to actually expose\ndistribution rather than trying to hide it. An obvious example is making use\nof location-based services, which can often be found on mobile phones, such\nas \ufb01nding the nearest Chinese take-away or checking whether any of your\nfriends are nearby.\nThere are also other arguments against distribution transparency. Recog-\nnizing that full distribution transparency is simply impossible, we should ask\nourselves whether it is even wise to pretend that we can achieve it. It may\nbe much better to make distribution explicit so that the user and applica-\ntion developer are never tricked into believing that there is such a thing as\ntransparency. The result will be that users will much better understand the\n(sometimes unexpected) behavior of a distributed system, and are thus much\nbetter prepared to deal with this behavior.\nNote 1.3 (Discussion: Against distribution transparency)\nSeveral researchers have argued that hiding distribution will only lead to further\ncomplicating the development of distributed systems, exactly for the reason that\nfull distribution transparency can never be achieved. A popular technique for\nachieving access transparency is to extend procedure calls to remote servers. How-\never, Waldo et al. [1997] already pointed out that attempting to hide distribution\nby means of such remote procedure calls can lead to poorly understood semantics,\nfor the simple reason that a procedure call does change when executed over a\nfaulty communication link.\nAs an alternative, various researchers and practitioners are now arguing for\nless transparency, for example, by more explicitly using message-style commu-\nnication, or more explicitly posting requests to, and getting results from remote\nmachines, as is done in the Web when fetching pages. Such solutions will be\ndiscussed in detail in the next chapter.\nA somewhat radical standpoint is taken by Wams [2011] by stating that partial\nfailures preclude relying on the successful execution of a remote service. If such\nreliability cannot be guaranteed, it is then best to always perform only local\nexecutions, leading to the copy-before-use principle. According to this principle,\ndata can be accessed only after they have been transferred to the machine of the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "12 CHAPTER 1. INTRODUCTION\nprocess wanting that data. Moreover, modifying a data item should not be done.\nInstead, it can only be updated to a new version. It is not dif\ufb01cult to imagine\nthat many other problems will surface. However, Wams shows that many existing\napplications can be retro\ufb01tted to this alternative approach without sacri\ufb01cing\nfunctionality.\nThe conclusion is that aiming for distribution transparency may be a\nnice goal when designing and implementing distributed systems, but that\nit should be considered together with other issues such as performance\nand comprehensibility. The price for achieving full transparency may be\nsurprisingly high.\nBeing open\nAnother important goal of distributed systems is openness. An open dis-\ntributed system is essentially a system that offers components that can easily\nbe used by, or integrated into other systems. At the same time, an open\ndistributed system itself will often consist of components that originate from\nelsewhere.\nInteroperability, composability, and extensibility\nTo be open means that components should adhere to standard rules that\ndescribe the syntax and semantics of what those components have to offer (i.e.,\nwhich service they provide). A general approach is to de\ufb01ne services through\ninterfaces using an Interface De\ufb01nition Language (IDL). Interface de\ufb01nitions\nwritten in an IDL nearly always capture only the syntax of services. In other\nwords, they specify precisely the names of the functions that are available\ntogether with types of the parameters, return values, possible exceptions that\ncan be raised, and so on. The hard part is specifying precisely what those\nservices do, that is, the semantics of interfaces. In practice, such speci\ufb01cations\nare given in an informal way by means of natural language.\nIf properly speci\ufb01ed, an interface de\ufb01nition allows an arbitrary process\nthat needs a certain interface, to talk to another process that provides that\ninterface. It also allows two independent parties to build completely different\nimplementations of those interfaces, leading to two separate components that\noperate in exactly the same way.\nProper speci\ufb01cations are complete and neutral. Complete means that\neverything that is necessary to make an implementation has indeed been\nspeci\ufb01ed. However, many interface de\ufb01nitions are not at all complete, so\nthat it is necessary for a developer to add implementation-speci\ufb01c details.\nJust as important is the fact that speci\ufb01cations do not prescribe what an\nimplementation should look like; they should be neutral.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n12 CHAPTER 1. INTRODUCTION\nprocess wanting that data. Moreover, modifying a data item should not be done.\nInstead, it can only be updated to a new version. It is not dif\ufb01cult to imagine\nthat many other problems will surface. However, Wams shows that many existing\napplications can be retro\ufb01tted to this alternative approach without sacri\ufb01cing\nfunctionality.\nThe conclusion is that aiming for distribution transparency may be a\nnice goal when designing and implementing distributed systems, but that\nit should be considered together with other issues such as performance\nand comprehensibility. The price for achieving full transparency may be\nsurprisingly high.\nBeing open\nAnother important goal of distributed systems is openness. An open dis-\ntributed system is essentially a system that offers components that can easily\nbe used by, or integrated into other systems. At the same time, an open\ndistributed system itself will often consist of components that originate from\nelsewhere.\nInteroperability, composability, and extensibility\nTo be open means that components should adhere to standard rules that\ndescribe the syntax and semantics of what those components have to offer (i.e.,\nwhich service they provide). A general approach is to de\ufb01ne services through\ninterfaces using an Interface De\ufb01nition Language (IDL). Interface de\ufb01nitions\nwritten in an IDL nearly always capture only the syntax of services. In other\nwords, they specify precisely the names of the functions that are available\ntogether with types of the parameters, return values, possible exceptions that\ncan be raised, and so on. The hard part is specifying precisely what those\nservices do, that is, the semantics of interfaces. In practice, such speci\ufb01cations\nare given in an informal way by means of natural language.\nIf properly speci\ufb01ed, an interface de\ufb01nition allows an arbitrary process\nthat needs a certain interface, to talk to another process that provides that\ninterface. It also allows two independent parties to build completely different\nimplementations of those interfaces, leading to two separate components that\noperate in exactly the same way.\nProper speci\ufb01cations are complete and neutral. Complete means that\neverything that is necessary to make an implementation has indeed been\nspeci\ufb01ed. However, many interface de\ufb01nitions are not at all complete, so\nthat it is necessary for a developer to add implementation-speci\ufb01c details.\nJust as important is the fact that speci\ufb01cations do not prescribe what an\nimplementation should look like; they should be neutral.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 13\nAs pointed out in Blair and Stefani [1998], completeness and neutrality are\nimportant for interoperability and portability. Interoperability characterizes\nthe extent by which two implementations of systems or components from\ndifferent manufacturers can co-exist and work together by merely relying\non each other\u2019s services as speci\ufb01ed by a common standard. Portability\ncharacterizes to what extent an application developed for a distributed system\nAcan be executed, without modi\ufb01cation, on a different distributed system B\nthat implements the same interfaces as A.\nAnother important goal for an open distributed system is that it should\nbe easy to con\ufb01gure the system out of different components (possibly from\ndifferent developers). Also, it should be easy to add new components or\nreplace existing ones without affecting those components that stay in place.\nIn other words, an open distributed system should also be extensible . For\nexample, in an extensible system, it should be relatively easy to add parts that\nrun on a different operating system, or even to replace an entire \ufb01le system.\nNote 1.4 (Discussion: Open systems in practice)\nOf course, what we have just described is an ideal situation. Practice shows that\nmany distributed systems are not as open as we would like and that still a lot\nof effort is needed to put various bits and pieces together to make a distributed\nsystem. One way out of the lack of openness is to simply reveal all the gory\ndetails of a component and to provide developers with the actual source code.\nThis approach is becoming increasingly popular, leading to so-called open source\nprojects where large groups of people contribute to improving and debugging\nsystems. Admittedly, this is as open as a system can get, but if it is the best way is\nquestionable.\nSeparating policy from mechanism\nTo achieve \ufb02exibility in open distributed systems, it is crucial that the system\nbe organized as a collection of relatively small and easily replaceable or\nadaptable components. This implies that we should provide de\ufb01nitions of not\nonly the highest-level interfaces, that is, those seen by users and applications,\nbut also de\ufb01nitions for interfaces to internal parts of the system and describe\nhow those parts interact. This approach is relatively new. Many older and\neven contemporary systems are constructed using a monolithic approach\nin which components are only logically separated but implemented as one,\nhuge program. This approach makes it hard to replace or adapt a component\nwithout affecting the entire system. Monolithic systems thus tend to be closed\ninstead of open.\nThe need for changing a distributed system is often caused by a component\nthat does not provide the optimal policy for a speci\ufb01c user or application.\nAs an example, consider caching in Web browsers. There are many different\nparameters that need to be considered:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 13\nAs pointed out in Blair and Stefani [1998], completeness and neutrality are\nimportant for interoperability and portability. Interoperability characterizes\nthe extent by which two implementations of systems or components from\ndifferent manufacturers can co-exist and work together by merely relying\non each other\u2019s services as speci\ufb01ed by a common standard. Portability\ncharacterizes to what extent an application developed for a distributed system\nAcan be executed, without modi\ufb01cation, on a different distributed system B\nthat implements the same interfaces as A.\nAnother important goal for an open distributed system is that it should\nbe easy to con\ufb01gure the system out of different components (possibly from\ndifferent developers). Also, it should be easy to add new components or\nreplace existing ones without affecting those components that stay in place.\nIn other words, an open distributed system should also be extensible . For\nexample, in an extensible system, it should be relatively easy to add parts that\nrun on a different operating system, or even to replace an entire \ufb01le system.\nNote 1.4 (Discussion: Open systems in practice)\nOf course, what we have just described is an ideal situation. Practice shows that\nmany distributed systems are not as open as we would like and that still a lot\nof effort is needed to put various bits and pieces together to make a distributed\nsystem. One way out of the lack of openness is to simply reveal all the gory\ndetails of a component and to provide developers with the actual source code.\nThis approach is becoming increasingly popular, leading to so-called open source\nprojects where large groups of people contribute to improving and debugging\nsystems. Admittedly, this is as open as a system can get, but if it is the best way is\nquestionable.\nSeparating policy from mechanism\nTo achieve \ufb02exibility in open distributed systems, it is crucial that the system\nbe organized as a collection of relatively small and easily replaceable or\nadaptable components. This implies that we should provide de\ufb01nitions of not\nonly the highest-level interfaces, that is, those seen by users and applications,\nbut also de\ufb01nitions for interfaces to internal parts of the system and describe\nhow those parts interact. This approach is relatively new. Many older and\neven contemporary systems are constructed using a monolithic approach\nin which components are only logically separated but implemented as one,\nhuge program. This approach makes it hard to replace or adapt a component\nwithout affecting the entire system. Monolithic systems thus tend to be closed\ninstead of open.\nThe need for changing a distributed system is often caused by a component\nthat does not provide the optimal policy for a speci\ufb01c user or application.\nAs an example, consider caching in Web browsers. There are many different\nparameters that need to be considered:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "14 CHAPTER 1. INTRODUCTION\nStorage: Where is data to be cached? Typically, there will be an in-memory\ncache next to storage on disk. In the latter case, the exact position in the\nlocal \ufb01le system needs to be considered.\nExemption: When the cache \ufb01lls up, which data is to be removed so that\nnewly fetched pages can be stored?\nSharing: Does each browser make use of a private cache, or is a cache to be\nshared among browsers of different users?\nRefreshing: When does a browser check if cached data is still up-to-date?\nCaches are most effective when a browser can return pages without\nhaving to contact the original Web site. However, this bears the risk of\nreturning stale data. Note also that refresh rates are highly dependent\non which data is actually cached: whereas timetables for trains hardly\nchange, this is not the case for Web pages showing current highway-\ntraf\ufb01c conditions, or worse yet, stock prices.\nWhat we need is a separation between policy and mechanism. In the case\nof Web caching, for example, a browser should ideally provide facilities for\nonly storing documents and at the same time allow users to decide which\ndocuments are stored and for how long. In practice, this can be implemented\nby offering a rich set of parameters that the user can set (dynamically). When\ntaking this a step further, a browser may even offer facilities for plugging in\npolicies that a user has implemented as a separate component.\nNote 1.5 (Discussion: Is a strict separation really what we need?)\nIn theory, strictly separating policies from mechanisms seems to be the way to go.\nHowever, there is an important trade-off to consider: the stricter the separation, the\nmore we need to make sure that we offer the appropriate collection of mechanisms.\nIn practice this means that a rich set of features is offered, in turn leading to many\ncon\ufb01guration parameters. As an example, the popular Firefox browser comes\nwith a few hundred con\ufb01guration parameters. Just imagine how the con\ufb01guration\nspace explodes when considering large distributed systems consisting of many\ncomponents. In other words, strict separation of policies and mechanisms may\nlead to highly complex con\ufb01guration problems.\nOne option to alleviate these problems is to provide reasonable defaults, and\nthis is what often happens in practice. An alternative approach is one in which\nthe system observes its own usage and dynamically changes parameter settings.\nThis leads to what are known as self-con\ufb01gurable systems . Nevertheless, the\nfact alone that many mechanisms need to be offered in order to support a wide\nrange of policies often makes coding distributed systems very complicated. Hard\ncoding policies into a distributed system may reduce complexity considerably,\nbut at the price of less \ufb02exibility.\nFinding the right balance in separating policies from mechanisms is one of the\nreasons why designing a distributed system is often more an art than a science.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n14 CHAPTER 1. INTRODUCTION\nStorage: Where is data to be cached? Typically, there will be an in-memory\ncache next to storage on disk. In the latter case, the exact position in the\nlocal \ufb01le system needs to be considered.\nExemption: When the cache \ufb01lls up, which data is to be removed so that\nnewly fetched pages can be stored?\nSharing: Does each browser make use of a private cache, or is a cache to be\nshared among browsers of different users?\nRefreshing: When does a browser check if cached data is still up-to-date?\nCaches are most effective when a browser can return pages without\nhaving to contact the original Web site. However, this bears the risk of\nreturning stale data. Note also that refresh rates are highly dependent\non which data is actually cached: whereas timetables for trains hardly\nchange, this is not the case for Web pages showing current highway-\ntraf\ufb01c conditions, or worse yet, stock prices.\nWhat we need is a separation between policy and mechanism. In the case\nof Web caching, for example, a browser should ideally provide facilities for\nonly storing documents and at the same time allow users to decide which\ndocuments are stored and for how long. In practice, this can be implemented\nby offering a rich set of parameters that the user can set (dynamically). When\ntaking this a step further, a browser may even offer facilities for plugging in\npolicies that a user has implemented as a separate component.\nNote 1.5 (Discussion: Is a strict separation really what we need?)\nIn theory, strictly separating policies from mechanisms seems to be the way to go.\nHowever, there is an important trade-off to consider: the stricter the separation, the\nmore we need to make sure that we offer the appropriate collection of mechanisms.\nIn practice this means that a rich set of features is offered, in turn leading to many\ncon\ufb01guration parameters. As an example, the popular Firefox browser comes\nwith a few hundred con\ufb01guration parameters. Just imagine how the con\ufb01guration\nspace explodes when considering large distributed systems consisting of many\ncomponents. In other words, strict separation of policies and mechanisms may\nlead to highly complex con\ufb01guration problems.\nOne option to alleviate these problems is to provide reasonable defaults, and\nthis is what often happens in practice. An alternative approach is one in which\nthe system observes its own usage and dynamically changes parameter settings.\nThis leads to what are known as self-con\ufb01gurable systems . Nevertheless, the\nfact alone that many mechanisms need to be offered in order to support a wide\nrange of policies often makes coding distributed systems very complicated. Hard\ncoding policies into a distributed system may reduce complexity considerably,\nbut at the price of less \ufb02exibility.\nFinding the right balance in separating policies from mechanisms is one of the\nreasons why designing a distributed system is often more an art than a science.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 15\nBeing scalable\nFor many of us, worldwide connectivity through the Internet is as common\nas being able to send a postcard to anyone anywhere around the world.\nMoreover, where until recently we were used to having relatively powerful\ndesktop computers for of\ufb01ce applications and storage, we are now witnessing\nthat such applications and services are being placed in what has been coined\n\u201cthe cloud,\u201d in turn leading to an increase of much smaller networked devices\nsuch as tablet computers. With this in mind, scalability has become one of the\nmost important design goals for developers of distributed systems.\nScalability dimensions\nScalability of a system can be measured along at least three different dimen-\nsions (see [Neuman, 1994]):\nSize scalability: A system can be scalable with respect to its size, meaning\nthat we can easily add more users and resources to the system without\nany noticeable loss of performance.\nGeographical scalability: A geographically scalable system is one in which\nthe users and resources may lie far apart, but the fact that communication\ndelays may be signi\ufb01cant is hardly noticed.\nAdministrative scalability: An administratively scalable system is one that\ncan still be easily managed even if it spans many independent adminis-\ntrative organizations.\nLet us take a closer look at each of these three scalability dimensions.\nSize scalability. When a system needs to scale, very different types of prob-\nlems need to be solved. Let us \ufb01rst consider scaling with respect to size.\nIf more users or resources need to be supported, we are often confronted\nwith the limitations of centralized services, although often for very different\nreasons. For example, many services are centralized in the sense that they\nare implemented by means of a single server running on a speci\ufb01c machine\nin the distributed system. In a more modern setting, we may have a group\nof collaborating servers co-located on a cluster of tightly coupled machines\nphysically placed at the same location. The problem with this scheme is\nobvious: the server, or group of servers, can simply become a bottleneck when\nit needs to process an increasing number of requests. To illustrate how this\ncan happen, let us assume that a service is implemented on a single machine.\nIn that case there are essentially three root causes for becoming a bottleneck:\n\u2022 The computational capacity, limited by the CPUs\n\u2022 The storage capacity, including the I/O transfer rate\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 15\nBeing scalable\nFor many of us, worldwide connectivity through the Internet is as common\nas being able to send a postcard to anyone anywhere around the world.\nMoreover, where until recently we were used to having relatively powerful\ndesktop computers for of\ufb01ce applications and storage, we are now witnessing\nthat such applications and services are being placed in what has been coined\n\u201cthe cloud,\u201d in turn leading to an increase of much smaller networked devices\nsuch as tablet computers. With this in mind, scalability has become one of the\nmost important design goals for developers of distributed systems.\nScalability dimensions\nScalability of a system can be measured along at least three different dimen-\nsions (see [Neuman, 1994]):\nSize scalability: A system can be scalable with respect to its size, meaning\nthat we can easily add more users and resources to the system without\nany noticeable loss of performance.\nGeographical scalability: A geographically scalable system is one in which\nthe users and resources may lie far apart, but the fact that communication\ndelays may be signi\ufb01cant is hardly noticed.\nAdministrative scalability: An administratively scalable system is one that\ncan still be easily managed even if it spans many independent adminis-\ntrative organizations.\nLet us take a closer look at each of these three scalability dimensions.\nSize scalability. When a system needs to scale, very different types of prob-\nlems need to be solved. Let us \ufb01rst consider scaling with respect to size.\nIf more users or resources need to be supported, we are often confronted\nwith the limitations of centralized services, although often for very different\nreasons. For example, many services are centralized in the sense that they\nare implemented by means of a single server running on a speci\ufb01c machine\nin the distributed system. In a more modern setting, we may have a group\nof collaborating servers co-located on a cluster of tightly coupled machines\nphysically placed at the same location. The problem with this scheme is\nobvious: the server, or group of servers, can simply become a bottleneck when\nit needs to process an increasing number of requests. To illustrate how this\ncan happen, let us assume that a service is implemented on a single machine.\nIn that case there are essentially three root causes for becoming a bottleneck:\n\u2022 The computational capacity, limited by the CPUs\n\u2022 The storage capacity, including the I/O transfer rate\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "16 CHAPTER 1. INTRODUCTION\n\u2022 The network between the user and the centralized service\nLet us \ufb01rst consider the computational capacity. Just imagine a service for\ncomputing optimal routes taking real-time traf\ufb01c information into account. It\nis not dif\ufb01cult to imagine that this may be primarily a compute-bound service\nrequiring several (tens of) seconds to complete a request. If there is only a\nsingle machine available, then even a modern high-end system will eventually\nrun into problems if the number of requests increases beyond a certain point.\nLikewise, but for different reasons, we will run into problems when having\na service that is mainly I/O bound. A typical example is a poorly designed\ncentralized search engine. The problem with content-based search queries is\nthat we essentially need to match a query against an entire data set. Even\nwith advanced indexing techniques, we may still face the problem of having\nto process a huge amount of data exceeding the main-memory capacity of\nthe machine running the service. As a consequence, much of the processing\ntime will be determined by the relatively slow disk accesses and transfer of\ndata between disk and main memory. Simply adding more or higher-speed\ndisks will prove not to be a sustainable solution as the number of requests\ncontinues to increase.\nFinally, the network between the user and the service may also be the cause\nof poor scalability. Just imagine a video-on-demand service that needs to\nstream high-quality video to multiple users. A video stream can easily require\na bandwidth of 8 to 10 Mbps, meaning that if a service sets up point-to-point\nconnections with its customers, it may soon hit the limits of the network\ncapacity of its own outgoing transmission lines.\nThere are several solutions to attack size scalability which we discuss\nbelow after having looked into geographical and administrative scalability.\nNote 1.6 (Advanced: Analyzing service capacity)\nSize scalability problems for centralized services can be formally analyzed using\nqueuing theory and making a few simplifying assumptions. At a conceptual level,\na centralized service can be modeled as the simple queuing system shown in\nFigure 1.3: requests are submitted to the service where they are queued until\nfurther notice. As soon as the process can handle a next request, it fetches it from\nthe queue, does its work, and produces a response. We largely follow Menasce\nand Almeida [2002] in explaining the performance of a centralized service.\nFigure 1.3: A simple model of a service as a queuing system.\nIn many cases, we may assume that the queue has an in\ufb01nite capacity, meaning\nthat there is no restriction on the number of requests that can be accepted for\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n16 CHAPTER 1. INTRODUCTION\n\u2022 The network between the user and the centralized service\nLet us \ufb01rst consider the computational capacity. Just imagine a service for\ncomputing optimal routes taking real-time traf\ufb01c information into account. It\nis not dif\ufb01cult to imagine that this may be primarily a compute-bound service\nrequiring several (tens of) seconds to complete a request. If there is only a\nsingle machine available, then even a modern high-end system will eventually\nrun into problems if the number of requests increases beyond a certain point.\nLikewise, but for different reasons, we will run into problems when having\na service that is mainly I/O bound. A typical example is a poorly designed\ncentralized search engine. The problem with content-based search queries is\nthat we essentially need to match a query against an entire data set. Even\nwith advanced indexing techniques, we may still face the problem of having\nto process a huge amount of data exceeding the main-memory capacity of\nthe machine running the service. As a consequence, much of the processing\ntime will be determined by the relatively slow disk accesses and transfer of\ndata between disk and main memory. Simply adding more or higher-speed\ndisks will prove not to be a sustainable solution as the number of requests\ncontinues to increase.\nFinally, the network between the user and the service may also be the cause\nof poor scalability. Just imagine a video-on-demand service that needs to\nstream high-quality video to multiple users. A video stream can easily require\na bandwidth of 8 to 10 Mbps, meaning that if a service sets up point-to-point\nconnections with its customers, it may soon hit the limits of the network\ncapacity of its own outgoing transmission lines.\nThere are several solutions to attack size scalability which we discuss\nbelow after having looked into geographical and administrative scalability.\nNote 1.6 (Advanced: Analyzing service capacity)\nSize scalability problems for centralized services can be formally analyzed using\nqueuing theory and making a few simplifying assumptions. At a conceptual level,\na centralized service can be modeled as the simple queuing system shown in\nFigure 1.3: requests are submitted to the service where they are queued until\nfurther notice. As soon as the process can handle a next request, it fetches it from\nthe queue, does its work, and produces a response. We largely follow Menasce\nand Almeida [2002] in explaining the performance of a centralized service.\nFigure 1.3: A simple model of a service as a queuing system.\nIn many cases, we may assume that the queue has an in\ufb01nite capacity, meaning\nthat there is no restriction on the number of requests that can be accepted for\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 17\nfurther processing. Strictly speaking, this means that the arrival rate of requests is\nnot in\ufb02uenced by what is currently in the queue or being processed. Assuming\nthat the arrival rate of requests is lrequests per second, and that the processing\ncapacity of the service is mrequests per second, one can compute that the fraction\nof time pkthat there are krequests in the system is equal to:\npk=\u0000\n1\u0000l\nm\u0001\u0000l\nm\u0001k\nIf we de\ufb01ne the utilization Uof a service as the fraction of time that it is busy,\nthen clearly,\nU=\u00e5\nk>0pk=1\u0000p0=l\nm)pk= (1\u0000U)Uk\nWe can then compute the average number Nof requests in the system as\nN=\u00e5\nk\u00150k\u0001pk=\u00e5\nk\u00150k\u0001(1\u0000U)Uk= (1\u0000U)\u00e5\nk\u00150k\u0001Uk=(1\u0000U)U\n(1\u0000U)2=U\n1\u0000U.\nWhat we are really interested in, is the response time R: how long does it take\nbefore the service to process a request, including the time spent in the queue.\nTo that end, we need the average throughput X. Considering that the service is\n\u201cbusy\u201d when at least one request is being processed, and that this then happens\nwith a throughput of mrequests per second, and during a fraction Uof the total\ntime, we have:\nX= U\u0001m|{z}\nserver at work+ (1\u0000U)\u00010|{z}\nserver idle=l\nm\u0001m=l\nUsing Little\u2019s formula [Trivedi, 2002], we can then derive the response time as\nR=N\nX=S\n1\u0000U)R\nS=1\n1\u0000U\nwhere S=1\nm, the actual service time. Note that if Uis very small, the response-\nto-service time ratio is close to 1, meaning that a request is virtually instantly\nprocessed, and at the maximum speed possible. However, as soon as the utilization\ncomes closer to 1, we see that the response-to-server time ratio quickly increases to\nvery high values, effectively meaning that the system is coming close to a grinding\nhalt. This is where we see scalability problems emerge. From this simple model,\nwe can see that the only solution is bringing down the service time S. We leave it\nas an exercise to the reader to explore how Smay be decreased.\nGeographical scalability. Geographical scalability has its own problems.\nOne of the main reasons why it is still dif\ufb01cult to scale existing distributed\nsystems that were designed for local-area networks is that many of them are\nbased on synchronous communication . In this form of communication, a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 17\nfurther processing. Strictly speaking, this means that the arrival rate of requests is\nnot in\ufb02uenced by what is currently in the queue or being processed. Assuming\nthat the arrival rate of requests is lrequests per second, and that the processing\ncapacity of the service is mrequests per second, one can compute that the fraction\nof time pkthat there are krequests in the system is equal to:\npk=\u0000\n1\u0000l\nm\u0001\u0000l\nm\u0001k\nIf we de\ufb01ne the utilization Uof a service as the fraction of time that it is busy,\nthen clearly,\nU=\u00e5\nk>0pk=1\u0000p0=l\nm)pk= (1\u0000U)Uk\nWe can then compute the average number Nof requests in the system as\nN=\u00e5\nk\u00150k\u0001pk=\u00e5\nk\u00150k\u0001(1\u0000U)Uk= (1\u0000U)\u00e5\nk\u00150k\u0001Uk=(1\u0000U)U\n(1\u0000U)2=U\n1\u0000U.\nWhat we are really interested in, is the response time R: how long does it take\nbefore the service to process a request, including the time spent in the queue.\nTo that end, we need the average throughput X. Considering that the service is\n\u201cbusy\u201d when at least one request is being processed, and that this then happens\nwith a throughput of mrequests per second, and during a fraction Uof the total\ntime, we have:\nX= U\u0001m|{z}\nserver at work+ (1\u0000U)\u00010|{z}\nserver idle=l\nm\u0001m=l\nUsing Little\u2019s formula [Trivedi, 2002], we can then derive the response time as\nR=N\nX=S\n1\u0000U)R\nS=1\n1\u0000U\nwhere S=1\nm, the actual service time. Note that if Uis very small, the response-\nto-service time ratio is close to 1, meaning that a request is virtually instantly\nprocessed, and at the maximum speed possible. However, as soon as the utilization\ncomes closer to 1, we see that the response-to-server time ratio quickly increases to\nvery high values, effectively meaning that the system is coming close to a grinding\nhalt. This is where we see scalability problems emerge. From this simple model,\nwe can see that the only solution is bringing down the service time S. We leave it\nas an exercise to the reader to explore how Smay be decreased.\nGeographical scalability. Geographical scalability has its own problems.\nOne of the main reasons why it is still dif\ufb01cult to scale existing distributed\nsystems that were designed for local-area networks is that many of them are\nbased on synchronous communication . In this form of communication, a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "18 CHAPTER 1. INTRODUCTION\nparty requesting service, generally referred to as a client , blocks until a reply is\nsent back from the server implementing the service. More speci\ufb01cally, we often\nsee a communication pattern consisting of many client-server interactions as\nmay be the case with database transactions. This approach generally works\n\ufb01ne in LANs where communication between two machines is often at worst\na few hundred microseconds. However, in a wide-area system, we need\nto take into account that interprocess communication may be hundreds of\nmilliseconds, three orders of magnitude slower. Building applications using\nsynchronous communication in wide-area systems requires a great deal of\ncare (and not just a little patience), notably with a rich interaction pattern\nbetween client and server.\nAnother problem that hinders geographical scalability is that communica-\ntion in wide-area networks is inherently much less reliable than in local-area\nnetworks. In addition, we also need to deal with limited bandwidth. The\neffect is that solutions developed for local-area networks cannot always be\neasily ported to a wide-area system. A typical example is streaming video. In\na home network, even when having only wireless links, ensuring a stable, fast\nstream of high-quality video frames from a media server to a display is quite\nsimple. Simply placing that same server far away and using a standard TCP\nconnection to the display will surely fail: bandwidth limitations will instantly\nsurface, but also maintaining the same level of reliability can easily cause\nheadaches.\nYet another issue that pops up when components lie far apart is the\nfact that wide-area systems generally have only very limited facilities for\nmultipoint communication. In contrast, local-area networks often support\nef\ufb01cient broadcasting mechanisms. Such mechanisms have proven to be\nextremely useful for discovering components and services, which is essential\nfrom a management point of view. In wide-area systems, we need to develop\nseparate services, such as naming and directory services to which queries can\nbe sent. These support services, in turn, need to be scalable as well and in\nmany cases no obvious solutions exist as we will encounter in later chapters.\nAdministrative scalability. Finally, a dif\ufb01cult, and in many cases open, ques-\ntion is how to scale a distributed system across multiple, independent adminis-\ntrative domains. A major problem that needs to be solved is that of con\ufb02icting\npolicies with respect to resource usage (and payment), management, and\nsecurity.\nTo illustrate, for many years scientists have been looking for solutions to\nshare their (often expensive) equipment in what is known as a computational\ngrid . In these grids, a global distributed system is constructed as a federation\nof local distributed systems, allowing a program running on a computer at\norganization Ato directly access resources at organization B.\nFor example, many components of a distributed system that reside within\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n18 CHAPTER 1. INTRODUCTION\nparty requesting service, generally referred to as a client , blocks until a reply is\nsent back from the server implementing the service. More speci\ufb01cally, we often\nsee a communication pattern consisting of many client-server interactions as\nmay be the case with database transactions. This approach generally works\n\ufb01ne in LANs where communication between two machines is often at worst\na few hundred microseconds. However, in a wide-area system, we need\nto take into account that interprocess communication may be hundreds of\nmilliseconds, three orders of magnitude slower. Building applications using\nsynchronous communication in wide-area systems requires a great deal of\ncare (and not just a little patience), notably with a rich interaction pattern\nbetween client and server.\nAnother problem that hinders geographical scalability is that communica-\ntion in wide-area networks is inherently much less reliable than in local-area\nnetworks. In addition, we also need to deal with limited bandwidth. The\neffect is that solutions developed for local-area networks cannot always be\neasily ported to a wide-area system. A typical example is streaming video. In\na home network, even when having only wireless links, ensuring a stable, fast\nstream of high-quality video frames from a media server to a display is quite\nsimple. Simply placing that same server far away and using a standard TCP\nconnection to the display will surely fail: bandwidth limitations will instantly\nsurface, but also maintaining the same level of reliability can easily cause\nheadaches.\nYet another issue that pops up when components lie far apart is the\nfact that wide-area systems generally have only very limited facilities for\nmultipoint communication. In contrast, local-area networks often support\nef\ufb01cient broadcasting mechanisms. Such mechanisms have proven to be\nextremely useful for discovering components and services, which is essential\nfrom a management point of view. In wide-area systems, we need to develop\nseparate services, such as naming and directory services to which queries can\nbe sent. These support services, in turn, need to be scalable as well and in\nmany cases no obvious solutions exist as we will encounter in later chapters.\nAdministrative scalability. Finally, a dif\ufb01cult, and in many cases open, ques-\ntion is how to scale a distributed system across multiple, independent adminis-\ntrative domains. A major problem that needs to be solved is that of con\ufb02icting\npolicies with respect to resource usage (and payment), management, and\nsecurity.\nTo illustrate, for many years scientists have been looking for solutions to\nshare their (often expensive) equipment in what is known as a computational\ngrid . In these grids, a global distributed system is constructed as a federation\nof local distributed systems, allowing a program running on a computer at\norganization Ato directly access resources at organization B.\nFor example, many components of a distributed system that reside within\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 19\na single domain can often be trusted by users that operate within that same\ndomain. In such cases, system administration may have tested and certi\ufb01ed\napplications, and may have taken special measures to ensure that such com-\nponents cannot be tampered with. In essence, the users trust their system\nadministrators. However, this trust does not expand naturally across domain\nboundaries.\nNote 1.7 (Example: A modern radio telescope)\nAs an example, consider developing a modern radio telescope, such as the Pierre\nAuger Observatory [Abraham et al., 2004]. The \ufb01nal system can be considered as\na federated distributed system:\n\u2022The radio telescope itself may be a wireless distributed system developed\nas a grid of a few thousand sensor nodes, each collecting radio signals\nand collaborating with neighboring nodes to \ufb01lter out relevant events. The\nnodes dynamically maintain a sink tree by which selected events are routed\nto a central point for further analysis.\n\u2022The central point needs to be a reasonably powerful system, capable of\nstoring and processing the events sent to it by the sensor nodes. This system\nis necessarily placed in proximity of the sensor nodes, but is otherwise to\nbe considered to operate independently. Depending on its functionality, it\nmay operate as a small local distributed system. In particular, it stores all\nrecorded events and offers access to remote systems owned by partners in\nthe consortium.\n\u2022Most partners have local distributed systems (often in the form of a cluster\nof computers) that they use to further process the data collected by the\ntelescope. In this case, the local systems directly access the central point at\nthe telescope using a standard communication protocol. Naturally, many\nresults produced within the consortium are made available to each partner.\nIt is thus seen that the complete system will cross boundaries of several adminis-\ntrative domains, and that special measures are needed to ensure that data that\nis supposed to be accessible only to (speci\ufb01c) consortium partners cannot be\ndisclosed to unauthorized parties. How to achieve administrative scalability is\nnot obvious.\nIf a distributed system expands to another domain, two types of security\nmeasures need to be taken. First, the distributed system has to protect\nitself against malicious attacks from the new domain. For example, users\nfrom the new domain may have only read access to the \ufb01le system in its\noriginal domain. Likewise, facilities such as expensive image setters or high-\nperformance computers may not be made available to unauthorized users.\nSecond, the new domain has to protect itself against malicious attacks from\nthe distributed system. A typical example is that of downloading programs\nsuch as applets in Web browsers. Basically, the new domain does not know\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 19\na single domain can often be trusted by users that operate within that same\ndomain. In such cases, system administration may have tested and certi\ufb01ed\napplications, and may have taken special measures to ensure that such com-\nponents cannot be tampered with. In essence, the users trust their system\nadministrators. However, this trust does not expand naturally across domain\nboundaries.\nNote 1.7 (Example: A modern radio telescope)\nAs an example, consider developing a modern radio telescope, such as the Pierre\nAuger Observatory [Abraham et al., 2004]. The \ufb01nal system can be considered as\na federated distributed system:\n\u2022The radio telescope itself may be a wireless distributed system developed\nas a grid of a few thousand sensor nodes, each collecting radio signals\nand collaborating with neighboring nodes to \ufb01lter out relevant events. The\nnodes dynamically maintain a sink tree by which selected events are routed\nto a central point for further analysis.\n\u2022The central point needs to be a reasonably powerful system, capable of\nstoring and processing the events sent to it by the sensor nodes. This system\nis necessarily placed in proximity of the sensor nodes, but is otherwise to\nbe considered to operate independently. Depending on its functionality, it\nmay operate as a small local distributed system. In particular, it stores all\nrecorded events and offers access to remote systems owned by partners in\nthe consortium.\n\u2022Most partners have local distributed systems (often in the form of a cluster\nof computers) that they use to further process the data collected by the\ntelescope. In this case, the local systems directly access the central point at\nthe telescope using a standard communication protocol. Naturally, many\nresults produced within the consortium are made available to each partner.\nIt is thus seen that the complete system will cross boundaries of several adminis-\ntrative domains, and that special measures are needed to ensure that data that\nis supposed to be accessible only to (speci\ufb01c) consortium partners cannot be\ndisclosed to unauthorized parties. How to achieve administrative scalability is\nnot obvious.\nIf a distributed system expands to another domain, two types of security\nmeasures need to be taken. First, the distributed system has to protect\nitself against malicious attacks from the new domain. For example, users\nfrom the new domain may have only read access to the \ufb01le system in its\noriginal domain. Likewise, facilities such as expensive image setters or high-\nperformance computers may not be made available to unauthorized users.\nSecond, the new domain has to protect itself against malicious attacks from\nthe distributed system. A typical example is that of downloading programs\nsuch as applets in Web browsers. Basically, the new domain does not know\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "20 CHAPTER 1. INTRODUCTION\nwhat to expect from such foreign code. The problem, as we shall see in\nChapter 9, is how to enforce those limitations.\nAs a counterexample of distributed systems spanning multiple adminis-\ntrative domains that apparently do not suffer from administrative scalability\nproblems, consider modern \ufb01le-sharing peer-to-peer networks. In these cases,\nend users simply install a program implementing distributed search and\ndownload functions and within minutes can start downloading \ufb01les. Other ex-\namples include peer-to-peer applications for telephony over the Internet such\nas Skype [Baset and Schulzrinne, 2006], and peer-assisted audio-streaming\napplications such as Spotify [Kreitz and Niemel\u00e4, 2010]. What these dis-\ntributed systems have in common is that end users , and not administrative\nentities, collaborate to keep the system up and running. At best, underlying\nadministrative organizations such as Internet Service Providers (ISPs ) can\npolice the network traf\ufb01c that these peer-to-peer systems cause, but so far\nsuch efforts have not been very effective.\nScaling techniques\nHaving discussed some of the scalability problems brings us to the question\nof how those problems can generally be solved. In most cases, scalability\nproblems in distributed systems appear as performance problems caused by\nlimited capacity of servers and network. Simply improving their capacity (e.g.,\nby increasing memory, upgrading CPUs, or replacing network modules) is\noften a solution, referred to as scaling up . When it comes to scaling out , that\nis, expanding the distributed system by essentially deploying more machines,\nthere are basically only three techniques we can apply: hiding communication\nlatencies, distribution of work, and replication (see also Neuman [1994]).\nHiding communication latencies. Hiding communication latencies is appli-\ncable in the case of geographical scalability. The basic idea is simple: try to\navoid waiting for responses to remote-service requests as much as possible.\nFor example, when a service has been requested at a remote machine, an\nalternative to waiting for a reply from the server is to do other useful work at\nthe requester\u2019s side. Essentially, this means constructing the requesting appli-\ncation in such a way that it uses only asynchronous communication . When\na reply comes in, the application is interrupted and a special handler is called\nto complete the previously issued request. Asynchronous communication\ncan often be used in batch-processing systems and parallel applications in\nwhich independent tasks can be scheduled for execution while another task is\nwaiting for communication to complete. Alternatively, a new thread of control\ncan be started to perform the request. Although it blocks waiting for the reply,\nother threads in the process can continue.\nHowever, there are many applications that cannot make effective use of\nasynchronous communication. For example, in interactive applications when\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n20 CHAPTER 1. INTRODUCTION\nwhat to expect from such foreign code. The problem, as we shall see in\nChapter 9, is how to enforce those limitations.\nAs a counterexample of distributed systems spanning multiple adminis-\ntrative domains that apparently do not suffer from administrative scalability\nproblems, consider modern \ufb01le-sharing peer-to-peer networks. In these cases,\nend users simply install a program implementing distributed search and\ndownload functions and within minutes can start downloading \ufb01les. Other ex-\namples include peer-to-peer applications for telephony over the Internet such\nas Skype [Baset and Schulzrinne, 2006], and peer-assisted audio-streaming\napplications such as Spotify [Kreitz and Niemel\u00e4, 2010]. What these dis-\ntributed systems have in common is that end users , and not administrative\nentities, collaborate to keep the system up and running. At best, underlying\nadministrative organizations such as Internet Service Providers (ISPs ) can\npolice the network traf\ufb01c that these peer-to-peer systems cause, but so far\nsuch efforts have not been very effective.\nScaling techniques\nHaving discussed some of the scalability problems brings us to the question\nof how those problems can generally be solved. In most cases, scalability\nproblems in distributed systems appear as performance problems caused by\nlimited capacity of servers and network. Simply improving their capacity (e.g.,\nby increasing memory, upgrading CPUs, or replacing network modules) is\noften a solution, referred to as scaling up . When it comes to scaling out , that\nis, expanding the distributed system by essentially deploying more machines,\nthere are basically only three techniques we can apply: hiding communication\nlatencies, distribution of work, and replication (see also Neuman [1994]).\nHiding communication latencies. Hiding communication latencies is appli-\ncable in the case of geographical scalability. The basic idea is simple: try to\navoid waiting for responses to remote-service requests as much as possible.\nFor example, when a service has been requested at a remote machine, an\nalternative to waiting for a reply from the server is to do other useful work at\nthe requester\u2019s side. Essentially, this means constructing the requesting appli-\ncation in such a way that it uses only asynchronous communication . When\na reply comes in, the application is interrupted and a special handler is called\nto complete the previously issued request. Asynchronous communication\ncan often be used in batch-processing systems and parallel applications in\nwhich independent tasks can be scheduled for execution while another task is\nwaiting for communication to complete. Alternatively, a new thread of control\ncan be started to perform the request. Although it blocks waiting for the reply,\nother threads in the process can continue.\nHowever, there are many applications that cannot make effective use of\nasynchronous communication. For example, in interactive applications when\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 21\n(a)\n(b)\nFigure 1.4: The difference between letting (a) a server or (b) a client check\nforms as they are being \ufb01lled.\na user sends a request he will generally have nothing better to do than to\nwait for the answer. In such cases, a much better solution is to reduce the\noverall communication, for example, by moving part of the computation that\nis normally done at the server to the client process requesting the service. A\ntypical case where this approach works is accessing databases using forms.\nFilling in forms can be done by sending a separate message for each \ufb01eld and\nwaiting for an acknowledgment from the server, as shown in Figure 1.4(a). For\nexample, the server may check for syntactic errors before accepting an entry.\nA much better solution is to ship the code for \ufb01lling in the form, and possibly\nchecking the entries, to the client, and have the client return a completed\nform, as shown in Figure 1.4(b). This approach of shipping code is widely\nsupported by the Web by means of Java applets and Javascript.\nPartitioning and distribution. Another important scaling technique is par-\ntitioning and distribution , which involves taking a component, splitting it\ninto smaller parts, and subsequently spreading those parts across the system.\nA good example of partitioning and distribution is the Internet Domain Name\nSystem (DNS). The DNS name space is hierarchically organized into a tree\nofdomains , which are divided into nonoverlapping zones , as shown for the\noriginal DNS in Figure 1.5. The names in each zone are handled by a single\nname server. Without going into too many details now (we return to DNS\nextensively in Chapter 5), one can think of each path name being the name of\na host in the Internet, and is thus associated with a network address of that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 21\n(a)\n(b)\nFigure 1.4: The difference between letting (a) a server or (b) a client check\nforms as they are being \ufb01lled.\na user sends a request he will generally have nothing better to do than to\nwait for the answer. In such cases, a much better solution is to reduce the\noverall communication, for example, by moving part of the computation that\nis normally done at the server to the client process requesting the service. A\ntypical case where this approach works is accessing databases using forms.\nFilling in forms can be done by sending a separate message for each \ufb01eld and\nwaiting for an acknowledgment from the server, as shown in Figure 1.4(a). For\nexample, the server may check for syntactic errors before accepting an entry.\nA much better solution is to ship the code for \ufb01lling in the form, and possibly\nchecking the entries, to the client, and have the client return a completed\nform, as shown in Figure 1.4(b). This approach of shipping code is widely\nsupported by the Web by means of Java applets and Javascript.\nPartitioning and distribution. Another important scaling technique is par-\ntitioning and distribution , which involves taking a component, splitting it\ninto smaller parts, and subsequently spreading those parts across the system.\nA good example of partitioning and distribution is the Internet Domain Name\nSystem (DNS). The DNS name space is hierarchically organized into a tree\nofdomains , which are divided into nonoverlapping zones , as shown for the\noriginal DNS in Figure 1.5. The names in each zone are handled by a single\nname server. Without going into too many details now (we return to DNS\nextensively in Chapter 5), one can think of each path name being the name of\na host in the Internet, and is thus associated with a network address of that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "22 CHAPTER 1. INTRODUCTION\nhost. Basically, resolving a name means returning the network address of the\nassociated host. Consider, for example, the name \rits.cs.vu.nl. To resolve this\nname, it is \ufb01rst passed to the server of zone Z1(see Figure 1.5) which returns\nthe address of the server for zone Z2, to which the rest of name, \rits.cs.vu, can\nbe handed. The server for Z2will return the address of the server for zone\nZ3, which is capable of handling the last part of the name and will return the\naddress of the associated host.\nFigure 1.5: An example of dividing the (original) DNS name space into zones.\nThis examples illustrates how the naming service , as provided by DNS, is\ndistributed across several machines, thus avoiding that a single server has to\ndeal with all requests for name resolution.\nAs another example, consider the World Wide Web. To most users, the\nWeb appears to be an enormous document-based information system in which\neach document has its own unique name in the form of a URL. Conceptually,\nit may even appear as if there is only a single server. However, the Web is\nphysically partitioned and distributed across a few hundred million servers,\neach handling a number of Web documents. The name of the server handling\na document is encoded into that document\u2019s URL. It is only because of this\ndistribution of documents that the Web has been capable of scaling to its\ncurrent size.\nReplication. Considering that scalability problems often appear in the form\nof performance degradation, it is generally a good idea to actually repli-\ncate components across a distributed system. Replication not only increases\navailability, but also helps to balance the load between components leading\nto better performance. Also, in geographically widely dispersed systems,\nhaving a copy nearby can hide much of the communication latency problems\nmentioned before.\nCaching is a special form of replication, although the distinction between\nthe two is often hard to make or even arti\ufb01cial. As in the case of replication,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n22 CHAPTER 1. INTRODUCTION\nhost. Basically, resolving a name means returning the network address of the\nassociated host. Consider, for example, the name \rits.cs.vu.nl. To resolve this\nname, it is \ufb01rst passed to the server of zone Z1(see Figure 1.5) which returns\nthe address of the server for zone Z2, to which the rest of name, \rits.cs.vu, can\nbe handed. The server for Z2will return the address of the server for zone\nZ3, which is capable of handling the last part of the name and will return the\naddress of the associated host.\nFigure 1.5: An example of dividing the (original) DNS name space into zones.\nThis examples illustrates how the naming service , as provided by DNS, is\ndistributed across several machines, thus avoiding that a single server has to\ndeal with all requests for name resolution.\nAs another example, consider the World Wide Web. To most users, the\nWeb appears to be an enormous document-based information system in which\neach document has its own unique name in the form of a URL. Conceptually,\nit may even appear as if there is only a single server. However, the Web is\nphysically partitioned and distributed across a few hundred million servers,\neach handling a number of Web documents. The name of the server handling\na document is encoded into that document\u2019s URL. It is only because of this\ndistribution of documents that the Web has been capable of scaling to its\ncurrent size.\nReplication. Considering that scalability problems often appear in the form\nof performance degradation, it is generally a good idea to actually repli-\ncate components across a distributed system. Replication not only increases\navailability, but also helps to balance the load between components leading\nto better performance. Also, in geographically widely dispersed systems,\nhaving a copy nearby can hide much of the communication latency problems\nmentioned before.\nCaching is a special form of replication, although the distinction between\nthe two is often hard to make or even arti\ufb01cial. As in the case of replication,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.2. DESIGN GOALS 23\ncaching results in making a copy of a resource, generally in the proximity of\nthe client accessing that resource. However, in contrast to replication, caching\nis a decision made by the client of a resource and not by the owner of a\nresource.\nThere is one serious drawback to caching and replication that may ad-\nversely affect scalability. Because we now have multiple copies of a resource,\nmodifying one copy makes that copy different from the others. Consequently,\ncaching and replication leads to consistency problems.\nTo what extent inconsistencies can be tolerated depends highly on the\nusage of a resource. For example, many Web users \ufb01nd it acceptable that\ntheir browser returns a cached document of which the validity has not been\nchecked for the last few minutes. However, there are also many cases in which\nstrong consistency guarantees need to be met, such as in the case of electronic\nstock exchanges and auctions. The problem with strong consistency is that\nan update must be immediately propagated to all other copies. Moreover, if\ntwo updates happen concurrently, it is often also required that updates are\nprocessed in the same order everywhere, introducing an additional global\nordering problem. To further aggravate problems, combining consistency with\nother desirable properties such as availability may simply be impossible, as\nwe discuss in Chapter 8.\nReplication therefore often requires some global synchronization mecha-\nnism. Unfortunately, such mechanisms are extremely hard or even impossible\nto implement in a scalable way, if alone because network latencies have a nat-\nural lower bound. Consequently, scaling by replication may introduce other,\ninherently nonscalable solutions. We return to replication and consistency\nextensively in Chapter 7.\nDiscussion. When considering these scaling techniques, one could argue\nthat size scalability is the least problematic from a technical point of view. In\nmany cases, increasing the capacity of a machine will save the day, although\nperhaps there is a high monetary cost to pay. Geographical scalability is a\nmuch tougher problem as network latencies are naturally bound from below.\nAs a consequence, we may be forced to copy data to locations close to where\nclients are, leading to problems of maintaining copies consistent. Practice\nshows that combining distribution, replication, and caching techniques with\ndifferent forms of consistency generally leads to acceptable solutions. Finally,\nadministrative scalability seems to be the most dif\ufb01cult problem to solve,\npartly because we need to deal with nontechnical issues, such as politics of or-\nganizations and human collaboration. The introduction and now widespread\nuse of peer-to-peer technology has successfully demonstrated what can be\nachieved if end users are put in control [Lua et al., 2005; Oram, 2001]. How-\never, peer-to-peer networks are obviously not the universal solution to all\nadministrative scalability problems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.2. DESIGN GOALS 23\ncaching results in making a copy of a resource, generally in the proximity of\nthe client accessing that resource. However, in contrast to replication, caching\nis a decision made by the client of a resource and not by the owner of a\nresource.\nThere is one serious drawback to caching and replication that may ad-\nversely affect scalability. Because we now have multiple copies of a resource,\nmodifying one copy makes that copy different from the others. Consequently,\ncaching and replication leads to consistency problems.\nTo what extent inconsistencies can be tolerated depends highly on the\nusage of a resource. For example, many Web users \ufb01nd it acceptable that\ntheir browser returns a cached document of which the validity has not been\nchecked for the last few minutes. However, there are also many cases in which\nstrong consistency guarantees need to be met, such as in the case of electronic\nstock exchanges and auctions. The problem with strong consistency is that\nan update must be immediately propagated to all other copies. Moreover, if\ntwo updates happen concurrently, it is often also required that updates are\nprocessed in the same order everywhere, introducing an additional global\nordering problem. To further aggravate problems, combining consistency with\nother desirable properties such as availability may simply be impossible, as\nwe discuss in Chapter 8.\nReplication therefore often requires some global synchronization mecha-\nnism. Unfortunately, such mechanisms are extremely hard or even impossible\nto implement in a scalable way, if alone because network latencies have a nat-\nural lower bound. Consequently, scaling by replication may introduce other,\ninherently nonscalable solutions. We return to replication and consistency\nextensively in Chapter 7.\nDiscussion. When considering these scaling techniques, one could argue\nthat size scalability is the least problematic from a technical point of view. In\nmany cases, increasing the capacity of a machine will save the day, although\nperhaps there is a high monetary cost to pay. Geographical scalability is a\nmuch tougher problem as network latencies are naturally bound from below.\nAs a consequence, we may be forced to copy data to locations close to where\nclients are, leading to problems of maintaining copies consistent. Practice\nshows that combining distribution, replication, and caching techniques with\ndifferent forms of consistency generally leads to acceptable solutions. Finally,\nadministrative scalability seems to be the most dif\ufb01cult problem to solve,\npartly because we need to deal with nontechnical issues, such as politics of or-\nganizations and human collaboration. The introduction and now widespread\nuse of peer-to-peer technology has successfully demonstrated what can be\nachieved if end users are put in control [Lua et al., 2005; Oram, 2001]. How-\never, peer-to-peer networks are obviously not the universal solution to all\nadministrative scalability problems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "24 CHAPTER 1. INTRODUCTION\nPitfalls\nIt should be clear by now that developing a distributed system is a formidable\ntask. As we will see many times throughout this book, there are so many\nissues to consider at the same time that it seems that only complexity can\nbe the result. Nevertheless, by following a number of design principles,\ndistributed systems can be developed that strongly adhere to the goals we set\nout in this chapter.\nDistributed systems differ from traditional software because components\nare dispersed across a network. Not taking this dispersion into account during\ndesign time is what makes so many systems needlessly complex and results in\n\ufb02aws that need to be patched later on. Peter Deutsch, at the time working at\nSun Microsystems, formulated these \ufb02aws as the following false assumptions\nthat everyone makes when developing a distributed application for the \ufb01rst\ntime:\n\u2022 The network is reliable\n\u2022 The network is secure\n\u2022 The network is homogeneous\n\u2022 The topology does not change\n\u2022 Latency is zero\n\u2022 Bandwidth is in\ufb01nite\n\u2022 Transport cost is zero\n\u2022 There is one administrator\nNote how these assumptions relate to properties that are unique to dis-\ntributed systems: reliability, security, heterogeneity, and topology of the\nnetwork; latency and bandwidth; transport costs; and \ufb01nally administrative\ndomains. When developing nondistributed applications, most of these issues\nwill most likely not show up.\nMost of the principles we discuss in this book relate immediately to these\nassumptions. In all cases, we will be discussing solutions to problems that\nare caused by the fact that one or more assumptions are false. For example,\nreliable networks simply do not exist and lead to the impossibility of achieving\nfailure transparency. We devote an entire chapter to deal with the fact that\nnetworked communication is inherently insecure. We have already argued\nthat distributed systems need to be open and take heterogeneity into account.\nLikewise, when discussing replication for solving scalability problems, we\nare essentially tackling latency and bandwidth problems. We will also touch\nupon management issues at various points throughout this book.\n1.3 Types of distributed systems\nBefore starting to discuss the principles of distributed systems, let us \ufb01rst\ntake a closer look at the various types of distributed systems. We make a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n24 CHAPTER 1. INTRODUCTION\nPitfalls\nIt should be clear by now that developing a distributed system is a formidable\ntask. As we will see many times throughout this book, there are so many\nissues to consider at the same time that it seems that only complexity can\nbe the result. Nevertheless, by following a number of design principles,\ndistributed systems can be developed that strongly adhere to the goals we set\nout in this chapter.\nDistributed systems differ from traditional software because components\nare dispersed across a network. Not taking this dispersion into account during\ndesign time is what makes so many systems needlessly complex and results in\n\ufb02aws that need to be patched later on. Peter Deutsch, at the time working at\nSun Microsystems, formulated these \ufb02aws as the following false assumptions\nthat everyone makes when developing a distributed application for the \ufb01rst\ntime:\n\u2022 The network is reliable\n\u2022 The network is secure\n\u2022 The network is homogeneous\n\u2022 The topology does not change\n\u2022 Latency is zero\n\u2022 Bandwidth is in\ufb01nite\n\u2022 Transport cost is zero\n\u2022 There is one administrator\nNote how these assumptions relate to properties that are unique to dis-\ntributed systems: reliability, security, heterogeneity, and topology of the\nnetwork; latency and bandwidth; transport costs; and \ufb01nally administrative\ndomains. When developing nondistributed applications, most of these issues\nwill most likely not show up.\nMost of the principles we discuss in this book relate immediately to these\nassumptions. In all cases, we will be discussing solutions to problems that\nare caused by the fact that one or more assumptions are false. For example,\nreliable networks simply do not exist and lead to the impossibility of achieving\nfailure transparency. We devote an entire chapter to deal with the fact that\nnetworked communication is inherently insecure. We have already argued\nthat distributed systems need to be open and take heterogeneity into account.\nLikewise, when discussing replication for solving scalability problems, we\nare essentially tackling latency and bandwidth problems. We will also touch\nupon management issues at various points throughout this book.\n1.3 Types of distributed systems\nBefore starting to discuss the principles of distributed systems, let us \ufb01rst\ntake a closer look at the various types of distributed systems. We make a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 25\ndistinction between distributed computing systems, distributed information\nsystems, and pervasive systems (which are naturally distributed).\nHigh performance distributed computing\nAn important class of distributed systems is the one used for high-performance\ncomputing tasks. Roughly speaking, one can make a distinction between two\nsubgroups. In cluster computing the underlying hardware consists of a\ncollection of similar workstations or PCs, closely connected by means of a\nhigh-speed local-area network. In addition, each node runs the same operating\nsystem.\nThe situation becomes very different in the case of grid computing . This\nsubgroup consists of distributed systems that are often constructed as a\nfederation of computer systems, where each system may fall under a different\nadministrative domain, and may be very different when it comes to hardware,\nsoftware, and deployed network technology.\nFrom the perspective of grid computing, a next logical step is to simply\noutsource the entire infrastructure that is needed for compute-intensive ap-\nplications. In essence, this is what cloud computing is all about: providing\nthe facilities to dynamically construct an infrastructure and compose what\nis needed from available services. Unlike grid computing, which is strongly\nassociated with high-performance computing, cloud computing is much more\nthan just providing lots of resources. We discuss it brie\ufb02y here, but will return\nto various aspects throughout the book.\nNote 1.8 (More information: Parallel processing)\nHigh-performance computing more or less started with the introduction of mul-\ntiprocessor machines . In this case, multiple CPUs are organized in such a way\nthat they all have access to the same physical memory, as shown in Figure 1.6(a).\nIn contrast, in a multicomputer system several computers are connected through\na network and there is no sharing of main memory, as shown in Figure 1.6(b).\nThe shared-memory model proved to be highly convenient for improving the\nperformance of programs and it was relatively easy to program.\nIts essence is that multiple threads of control are executing at the same time,\nwhile all threads have access to shared data. Access to that data is controlled\nthrough well-understood synchronization mechanisms like semaphores (see Ben-\nAri [2006] or Herlihy and Shavit [2008] for more information on developing\nparallel programs). Unfortunately, the model does not easily scale: so far, ma-\nchines have been developed in which only a few tens (and sometimes hundreds)\nof CPUs have ef\ufb01cient access to shared memory. To a certain extent, we are seeing\nthe same limitations for multicore processors.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 25\ndistinction between distributed computing systems, distributed information\nsystems, and pervasive systems (which are naturally distributed).\nHigh performance distributed computing\nAn important class of distributed systems is the one used for high-performance\ncomputing tasks. Roughly speaking, one can make a distinction between two\nsubgroups. In cluster computing the underlying hardware consists of a\ncollection of similar workstations or PCs, closely connected by means of a\nhigh-speed local-area network. In addition, each node runs the same operating\nsystem.\nThe situation becomes very different in the case of grid computing . This\nsubgroup consists of distributed systems that are often constructed as a\nfederation of computer systems, where each system may fall under a different\nadministrative domain, and may be very different when it comes to hardware,\nsoftware, and deployed network technology.\nFrom the perspective of grid computing, a next logical step is to simply\noutsource the entire infrastructure that is needed for compute-intensive ap-\nplications. In essence, this is what cloud computing is all about: providing\nthe facilities to dynamically construct an infrastructure and compose what\nis needed from available services. Unlike grid computing, which is strongly\nassociated with high-performance computing, cloud computing is much more\nthan just providing lots of resources. We discuss it brie\ufb02y here, but will return\nto various aspects throughout the book.\nNote 1.8 (More information: Parallel processing)\nHigh-performance computing more or less started with the introduction of mul-\ntiprocessor machines . In this case, multiple CPUs are organized in such a way\nthat they all have access to the same physical memory, as shown in Figure 1.6(a).\nIn contrast, in a multicomputer system several computers are connected through\na network and there is no sharing of main memory, as shown in Figure 1.6(b).\nThe shared-memory model proved to be highly convenient for improving the\nperformance of programs and it was relatively easy to program.\nIts essence is that multiple threads of control are executing at the same time,\nwhile all threads have access to shared data. Access to that data is controlled\nthrough well-understood synchronization mechanisms like semaphores (see Ben-\nAri [2006] or Herlihy and Shavit [2008] for more information on developing\nparallel programs). Unfortunately, the model does not easily scale: so far, ma-\nchines have been developed in which only a few tens (and sometimes hundreds)\nof CPUs have ef\ufb01cient access to shared memory. To a certain extent, we are seeing\nthe same limitations for multicore processors.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "26 CHAPTER 1. INTRODUCTION\n(a) (b)\nFigure 1.6: A comparison between (a) multiprocessor and (b) multicom-\nputer architectures.\nTo overcome the limitations of shared-memory systems, high-performance\ncomputing moved to distributed-memory systems. This shift also meant that many\nprograms had to make use of message passing instead of modifying shared data as\na means of communication and synchronization between threads. Unfortunately,\nmessage-passing models have proven to be much more dif\ufb01cult and error-prone\ncompared to the shared-memory programming models. For this reason, there\nhas been signi\ufb01cant research in attempting to build so-called distributed shared-\nmemory multicomputers , or simply DSM system [Amza et al., 1996].\nIn essence, a DSM system allows a processor to address a memory location\nat another computer as if it were local memory. This can be achieved using\nexisting techniques available to the operating system, for example, by mapping all\nmain-memory pages of the various processors into a single virtual address space.\nWhenever a processor Aaddresses a page located at another processor B, a page\nfault occurs at Aallowing the operating system at Ato fetch the content of the\nreferenced page at Bin the same way that it would normally fetch it locally from\ndisk. At the same time, processor Bwould be informed that the page is currently\nnot accessible.\nThis elegant idea of mimicking shared-memory systems using multicomputers\neventually had to be abandoned for the simple reason that performance could\nnever meet the expectations of programmers, who would rather resort to far\nmore intricate, yet better (predictably) performing message-passing programming\nmodels.\nAn important side-effect of exploring the hardware-software boundaries of\nparallel processing is a thorough understanding of consistency models, to which\nwe return extensively in Chapter 7.\nCluster computing\nCluster computing systems became popular when the price/performance\nratio of personal computers and workstations improved. At a certain point, it\nbecame \ufb01nancially and technically attractive to build a supercomputer using\noff-the-shelf technology by simply hooking up a collection of relatively simple\ncomputers in a high-speed network. In virtually all cases, cluster computing is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n26 CHAPTER 1. INTRODUCTION\n(a) (b)\nFigure 1.6: A comparison between (a) multiprocessor and (b) multicom-\nputer architectures.\nTo overcome the limitations of shared-memory systems, high-performance\ncomputing moved to distributed-memory systems. This shift also meant that many\nprograms had to make use of message passing instead of modifying shared data as\na means of communication and synchronization between threads. Unfortunately,\nmessage-passing models have proven to be much more dif\ufb01cult and error-prone\ncompared to the shared-memory programming models. For this reason, there\nhas been signi\ufb01cant research in attempting to build so-called distributed shared-\nmemory multicomputers , or simply DSM system [Amza et al., 1996].\nIn essence, a DSM system allows a processor to address a memory location\nat another computer as if it were local memory. This can be achieved using\nexisting techniques available to the operating system, for example, by mapping all\nmain-memory pages of the various processors into a single virtual address space.\nWhenever a processor Aaddresses a page located at another processor B, a page\nfault occurs at Aallowing the operating system at Ato fetch the content of the\nreferenced page at Bin the same way that it would normally fetch it locally from\ndisk. At the same time, processor Bwould be informed that the page is currently\nnot accessible.\nThis elegant idea of mimicking shared-memory systems using multicomputers\neventually had to be abandoned for the simple reason that performance could\nnever meet the expectations of programmers, who would rather resort to far\nmore intricate, yet better (predictably) performing message-passing programming\nmodels.\nAn important side-effect of exploring the hardware-software boundaries of\nparallel processing is a thorough understanding of consistency models, to which\nwe return extensively in Chapter 7.\nCluster computing\nCluster computing systems became popular when the price/performance\nratio of personal computers and workstations improved. At a certain point, it\nbecame \ufb01nancially and technically attractive to build a supercomputer using\noff-the-shelf technology by simply hooking up a collection of relatively simple\ncomputers in a high-speed network. In virtually all cases, cluster computing is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 27\nused for parallel programming in which a single (compute intensive) program\nis run in parallel on multiple machines.\nFigure 1.7: An example of a cluster computing system.\nOne widely applied example of a cluster computer is formed by Linux-\nbased Beowulf clusters, of which the general con\ufb01guration is shown in Fig-\nure 1.7. Each cluster consists of a collection of compute nodes that are con-\ntrolled and accessed by means of a single master node. The master typically\nhandles the allocation of nodes to a particular parallel program, maintains\na batch queue of submitted jobs, and provides an interface for the users of\nthe system. As such, the master actually runs the middleware needed for the\nexecution of programs and management of the cluster, while the compute\nnodes are equipped with a standard operating system extended with typical\nmiddleware functions for communication, storage, fault tolerance, and so on.\nApart from the master node, the compute nodes are thus seen to be highly\nidentical.\nAn even more symmetric approach is followed in the MOSIX system [Amar\net al., 2004]. MOSIX attempts to provide a single-system image of a cluster,\nmeaning that to a process a cluster computer offers the ultimate distribution\ntransparency by appearing to be a single computer. As we mentioned, pro-\nviding such an image under all circumstances is impossible. In the case of\nMOSIX, the high degree of transparency is provided by allowing processes\nto dynamically and preemptively migrate between the nodes that make up\nthe cluster. Process migration allows a user to start an application on any\nnode (referred to as the home node), after which it can transparently move to\nother nodes, for example, to make ef\ufb01cient use of resources. We will return to\nprocess migration in Chapter 3. Similar approaches at attempting to provide\na single-system image are compared by [Lottiaux et al., 2005].\nHowever, several modern cluster computers have been moving away from\nthese symmetric architectures to more hybrid solutions in which the middle-\nware is functionally partitioned across different nodes, as explained by En-\ngelmann et al. [2007]. The advantage of such a separation is obvious: having\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 27\nused for parallel programming in which a single (compute intensive) program\nis run in parallel on multiple machines.\nFigure 1.7: An example of a cluster computing system.\nOne widely applied example of a cluster computer is formed by Linux-\nbased Beowulf clusters, of which the general con\ufb01guration is shown in Fig-\nure 1.7. Each cluster consists of a collection of compute nodes that are con-\ntrolled and accessed by means of a single master node. The master typically\nhandles the allocation of nodes to a particular parallel program, maintains\na batch queue of submitted jobs, and provides an interface for the users of\nthe system. As such, the master actually runs the middleware needed for the\nexecution of programs and management of the cluster, while the compute\nnodes are equipped with a standard operating system extended with typical\nmiddleware functions for communication, storage, fault tolerance, and so on.\nApart from the master node, the compute nodes are thus seen to be highly\nidentical.\nAn even more symmetric approach is followed in the MOSIX system [Amar\net al., 2004]. MOSIX attempts to provide a single-system image of a cluster,\nmeaning that to a process a cluster computer offers the ultimate distribution\ntransparency by appearing to be a single computer. As we mentioned, pro-\nviding such an image under all circumstances is impossible. In the case of\nMOSIX, the high degree of transparency is provided by allowing processes\nto dynamically and preemptively migrate between the nodes that make up\nthe cluster. Process migration allows a user to start an application on any\nnode (referred to as the home node), after which it can transparently move to\nother nodes, for example, to make ef\ufb01cient use of resources. We will return to\nprocess migration in Chapter 3. Similar approaches at attempting to provide\na single-system image are compared by [Lottiaux et al., 2005].\nHowever, several modern cluster computers have been moving away from\nthese symmetric architectures to more hybrid solutions in which the middle-\nware is functionally partitioned across different nodes, as explained by En-\ngelmann et al. [2007]. The advantage of such a separation is obvious: having\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "28 CHAPTER 1. INTRODUCTION\ncompute nodes with dedicated, lightweight operating systems will most likely\nprovide optimal performance for compute-intensive applications. Likewise,\nstorage functionality can most likely be optimally handled by other specially\ncon\ufb01gured nodes such as \ufb01le and directory servers. The same holds for other\ndedicated middleware services, including job management, database services,\nand perhaps general Internet access to external services.\nGrid computing\nA characteristic feature of traditional cluster computing is its homogeneity.\nIn most cases, the computers in a cluster are largely the same, have the\nsame operating system, and are all connected through the same network.\nHowever, as we just discussed, there has been a trend towards more hybrid\narchitectures in which nodes are speci\ufb01cally con\ufb01gured for certain tasks. This\ndiversity is even more prevalent in grid computing systems: no assumptions\nare made concerning similarity of hardware, operating systems, networks,\nadministrative domains, security policies, etc.\nA key issue in a grid-computing system is that resources from different\norganizations are brought together to allow the collaboration of a group of\npeople from different institutions, indeed forming a federation of systems.\nSuch a collaboration is realized in the form of a virtual organization . The\nprocesses belonging to the same virtual organization have access rights to the\nresources that are provided to that organization. Typically, resources consist of\ncompute servers (including supercomputers, possibly implemented as cluster\ncomputers), storage facilities, and databases. In addition, special networked\ndevices such as telescopes, sensors, etc., can be provided as well.\nGiven its nature, much of the software for realizing grid computing evolves\naround providing access to resources from different administrative domains,\nand to only those users and applications that belong to a speci\ufb01c virtual\norganization. For this reason, focus is often on architectural issues. An\narchitecture initially proposed by Foster et al. [2001] is shown in Figure 1.8,\nwhich still forms the basis for many grid computing systems.\nFigure 1.8: A layered architecture for grid computing systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n28 CHAPTER 1. INTRODUCTION\ncompute nodes with dedicated, lightweight operating systems will most likely\nprovide optimal performance for compute-intensive applications. Likewise,\nstorage functionality can most likely be optimally handled by other specially\ncon\ufb01gured nodes such as \ufb01le and directory servers. The same holds for other\ndedicated middleware services, including job management, database services,\nand perhaps general Internet access to external services.\nGrid computing\nA characteristic feature of traditional cluster computing is its homogeneity.\nIn most cases, the computers in a cluster are largely the same, have the\nsame operating system, and are all connected through the same network.\nHowever, as we just discussed, there has been a trend towards more hybrid\narchitectures in which nodes are speci\ufb01cally con\ufb01gured for certain tasks. This\ndiversity is even more prevalent in grid computing systems: no assumptions\nare made concerning similarity of hardware, operating systems, networks,\nadministrative domains, security policies, etc.\nA key issue in a grid-computing system is that resources from different\norganizations are brought together to allow the collaboration of a group of\npeople from different institutions, indeed forming a federation of systems.\nSuch a collaboration is realized in the form of a virtual organization . The\nprocesses belonging to the same virtual organization have access rights to the\nresources that are provided to that organization. Typically, resources consist of\ncompute servers (including supercomputers, possibly implemented as cluster\ncomputers), storage facilities, and databases. In addition, special networked\ndevices such as telescopes, sensors, etc., can be provided as well.\nGiven its nature, much of the software for realizing grid computing evolves\naround providing access to resources from different administrative domains,\nand to only those users and applications that belong to a speci\ufb01c virtual\norganization. For this reason, focus is often on architectural issues. An\narchitecture initially proposed by Foster et al. [2001] is shown in Figure 1.8,\nwhich still forms the basis for many grid computing systems.\nFigure 1.8: A layered architecture for grid computing systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 29\nThe architecture consists of four layers. The lowest fabric layer provides\ninterfaces to local resources at a speci\ufb01c site. Note that these interfaces are\ntailored to allow sharing of resources within a virtual organization. Typically,\nthey will provide functions for querying the state and capabilities of a resource,\nalong with functions for actual resource management (e.g., locking resources).\nThe connectivity layer consists of communication protocols for supporting\ngrid transactions that span the usage of multiple resources. For example,\nprotocols are needed to transfer data between resources, or to simply access\na resource from a remote location. In addition, the connectivity layer will\ncontain security protocols to authenticate users and resources. Note that in\nmany cases human users are not authenticated; instead, programs acting on\nbehalf of the users are authenticated. In this sense, delegating rights from\na user to programs is an important function that needs to be supported in\nthe connectivity layer. We return to delegation when discussing security in\ndistributed systems in Chapter 9.\nThe resource layer is responsible for managing a single resource. It uses the\nfunctions provided by the connectivity layer and calls directly the interfaces\nmade available by the fabric layer. For example, this layer will offer functions\nfor obtaining con\ufb01guration information on a speci\ufb01c resource, or, in general,\nto perform speci\ufb01c operations such as creating a process or reading data. The\nresource layer is thus seen to be responsible for access control, and hence will\nrely on the authentication performed as part of the connectivity layer.\nThe next layer in the hierarchy is the collective layer . It deals with handling\naccess to multiple resources and typically consists of services for resource\ndiscovery, allocation and scheduling of tasks onto multiple resources, data\nreplication, and so on. Unlike the connectivity and resource layer, each\nconsisting of a relatively small, standard collection of protocols, the collective\nlayer may consist of many different protocols re\ufb02ecting the broad spectrum of\nservices it may offer to a virtual organization.\nFinally, the application layer consists of the applications that operate within a\nvirtual organization and which make use of the grid computing environment.\nTypically the collective, connectivity, and resource layer form the heart of\nwhat could be called a grid middleware layer. These layers jointly provide\naccess to and management of resources that are potentially dispersed across\nmultiple sites.\nAn important observation from a middleware perspective is that in grid\ncomputing the notion of a site (or administrative unit) is common. This\nprevalence is emphasized by the gradual shift toward a service-oriented ar-\nchitecture in which sites offer access to the various layers through a collection\nof Web services [Joseph et al., 2004]. This, by now, has lead to the de\ufb01nition\nof an alternative architecture known as the Open Grid Services Architecture\n(OGSA ) [Foster et al., 2006]. OGSA is based upon the original ideas as for-\nmulated by Foster et al. [2001], yet having gone through a standardization\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 29\nThe architecture consists of four layers. The lowest fabric layer provides\ninterfaces to local resources at a speci\ufb01c site. Note that these interfaces are\ntailored to allow sharing of resources within a virtual organization. Typically,\nthey will provide functions for querying the state and capabilities of a resource,\nalong with functions for actual resource management (e.g., locking resources).\nThe connectivity layer consists of communication protocols for supporting\ngrid transactions that span the usage of multiple resources. For example,\nprotocols are needed to transfer data between resources, or to simply access\na resource from a remote location. In addition, the connectivity layer will\ncontain security protocols to authenticate users and resources. Note that in\nmany cases human users are not authenticated; instead, programs acting on\nbehalf of the users are authenticated. In this sense, delegating rights from\na user to programs is an important function that needs to be supported in\nthe connectivity layer. We return to delegation when discussing security in\ndistributed systems in Chapter 9.\nThe resource layer is responsible for managing a single resource. It uses the\nfunctions provided by the connectivity layer and calls directly the interfaces\nmade available by the fabric layer. For example, this layer will offer functions\nfor obtaining con\ufb01guration information on a speci\ufb01c resource, or, in general,\nto perform speci\ufb01c operations such as creating a process or reading data. The\nresource layer is thus seen to be responsible for access control, and hence will\nrely on the authentication performed as part of the connectivity layer.\nThe next layer in the hierarchy is the collective layer . It deals with handling\naccess to multiple resources and typically consists of services for resource\ndiscovery, allocation and scheduling of tasks onto multiple resources, data\nreplication, and so on. Unlike the connectivity and resource layer, each\nconsisting of a relatively small, standard collection of protocols, the collective\nlayer may consist of many different protocols re\ufb02ecting the broad spectrum of\nservices it may offer to a virtual organization.\nFinally, the application layer consists of the applications that operate within a\nvirtual organization and which make use of the grid computing environment.\nTypically the collective, connectivity, and resource layer form the heart of\nwhat could be called a grid middleware layer. These layers jointly provide\naccess to and management of resources that are potentially dispersed across\nmultiple sites.\nAn important observation from a middleware perspective is that in grid\ncomputing the notion of a site (or administrative unit) is common. This\nprevalence is emphasized by the gradual shift toward a service-oriented ar-\nchitecture in which sites offer access to the various layers through a collection\nof Web services [Joseph et al., 2004]. This, by now, has lead to the de\ufb01nition\nof an alternative architecture known as the Open Grid Services Architecture\n(OGSA ) [Foster et al., 2006]. OGSA is based upon the original ideas as for-\nmulated by Foster et al. [2001], yet having gone through a standardization\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "30 CHAPTER 1. INTRODUCTION\nprocess makes it complex, to say the least. OGSA implementations generally\nfollow Web service standards.\nCloud computing\nWhile researchers were pondering on how to organize computational grids\nthat were easily accessible, organizations in charge of running data centers\nwere facing the problem of opening up their resources to customers. Eventu-\nally, this lead to the concept of utility computing by which a customer could\nupload tasks to a data center and be charged on a per-resource basis. Utility\ncomputing formed the basis for what is now called cloud computing .\nFollowing Vaquero et al. [2008], cloud computing is characterized by an\neasily usable and accessible pool of virtualized resources. Which and how\nresources are used can be con\ufb01gured dynamically, providing the basis for\nscalability: if more work needs to be done, a customer can simply acquire\nmore resources. The link to utility computing is formed by the fact that cloud\ncomputing is generally based on a pay-per-use model in which guarantees\nare offered by means of customized service level agreements (SLAs).\nFigure 1.9: The organization of clouds (adapted from Zhang et al. [2010]).\nIn practice, clouds are organized into four layers, as shown in Figure 1.9\n(see also Zhang et al. [2010]):\nHardware: The lowest layer is formed by the means to manage the necessary\nhardware: processors, routers, but also power and cooling systems. It is\ngenerally implemented at data centers and contains the resources that\ncustomers normally never get to see directly.\nInfrastructure: This is an important layer forming the backbone for most\ncloud computing platforms. It deploys virtualization techniques (dis-\ncussed in Section 3.2) to provide customers an infrastructure consisting\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n30 CHAPTER 1. INTRODUCTION\nprocess makes it complex, to say the least. OGSA implementations generally\nfollow Web service standards.\nCloud computing\nWhile researchers were pondering on how to organize computational grids\nthat were easily accessible, organizations in charge of running data centers\nwere facing the problem of opening up their resources to customers. Eventu-\nally, this lead to the concept of utility computing by which a customer could\nupload tasks to a data center and be charged on a per-resource basis. Utility\ncomputing formed the basis for what is now called cloud computing .\nFollowing Vaquero et al. [2008], cloud computing is characterized by an\neasily usable and accessible pool of virtualized resources. Which and how\nresources are used can be con\ufb01gured dynamically, providing the basis for\nscalability: if more work needs to be done, a customer can simply acquire\nmore resources. The link to utility computing is formed by the fact that cloud\ncomputing is generally based on a pay-per-use model in which guarantees\nare offered by means of customized service level agreements (SLAs).\nFigure 1.9: The organization of clouds (adapted from Zhang et al. [2010]).\nIn practice, clouds are organized into four layers, as shown in Figure 1.9\n(see also Zhang et al. [2010]):\nHardware: The lowest layer is formed by the means to manage the necessary\nhardware: processors, routers, but also power and cooling systems. It is\ngenerally implemented at data centers and contains the resources that\ncustomers normally never get to see directly.\nInfrastructure: This is an important layer forming the backbone for most\ncloud computing platforms. It deploys virtualization techniques (dis-\ncussed in Section 3.2) to provide customers an infrastructure consisting\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 31\nof virtual storage and computing resources. Indeed, nothing is what\nit seems: cloud computing evolves around allocating and managing\nvirtual storage devices and virtual servers.\nPlatform: One could argue that the platform layer provides to a cloud-\ncomputing customer what an operating system provides to application\ndevelopers, namely the means to easily develop and deploy applications\nthat need to run in a cloud. In practice, an application developer is\noffered a vendor-speci\ufb01c API, which includes calls to uploading and ex-\necuting a program in that vendor\u2019s cloud. In a sense, this is comparable\nthe Unix exec family of system calls, which take an executable \ufb01le as\nparameter and pass it to the operating system to be executed.\nAlso like operating systems, the platform layer provides higher-level\nabstractions for storage and such. For example, as we discuss in more\ndetail later, the Amazon S3 storage system [Murty, 2008] is offered to the\napplication developer in the form of an API allowing (locally created)\n\ufb01les to be organized and stored in buckets . A bucket is somewhat\ncomparable to a directory. By storing a \ufb01le in a bucket, that \ufb01le is\nautomatically uploaded to the Amazon cloud.\nApplication: Actual applications run in this layer and are offered to users\nfor further customization. Well-known examples include those found\nin of\ufb01ce suites (text processors, spreadsheet applications, presentation\napplications, and so on). It is important to realize that these applica-\ntions are again executed in the vendor\u2019s cloud. As before, they can be\ncompared to the traditional suite of applications that are shipped when\ninstalling an operating system.\nCloud-computing providers offer these layers to their customers through\nvarious interfaces (including command-line tools, programming interfaces,\nand Web interfaces), leading to three different types of services:\n\u2022Infrastructure-as-a-Service (IaaS ) covering the hardware and infrastructure\nlayer\n\u2022Platform-as-a-Service (PaaS ) covering the platform layer\n\u2022Software-as-a-Service (SaaS ) in which their applications are covered\nAs of now, making use of clouds is relatively easy, and we discuss in later\nchapters more concrete examples of interfaces to cloud providers. As a\nconsequence, cloud computing as a means for outsourcing local computing\ninfrastructures has become a serious option for many enterprises. However,\nthere are still a number of serious obstacles including provider lock-in, security\nand privacy issues, and dependency on the availability of services, to mention\na few (see also Armbrust et al. [2010]). Also, because the details on how\nspeci\ufb01c cloud computations are actually carried out are generally hidden, and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 31\nof virtual storage and computing resources. Indeed, nothing is what\nit seems: cloud computing evolves around allocating and managing\nvirtual storage devices and virtual servers.\nPlatform: One could argue that the platform layer provides to a cloud-\ncomputing customer what an operating system provides to application\ndevelopers, namely the means to easily develop and deploy applications\nthat need to run in a cloud. In practice, an application developer is\noffered a vendor-speci\ufb01c API, which includes calls to uploading and ex-\necuting a program in that vendor\u2019s cloud. In a sense, this is comparable\nthe Unix exec family of system calls, which take an executable \ufb01le as\nparameter and pass it to the operating system to be executed.\nAlso like operating systems, the platform layer provides higher-level\nabstractions for storage and such. For example, as we discuss in more\ndetail later, the Amazon S3 storage system [Murty, 2008] is offered to the\napplication developer in the form of an API allowing (locally created)\n\ufb01les to be organized and stored in buckets . A bucket is somewhat\ncomparable to a directory. By storing a \ufb01le in a bucket, that \ufb01le is\nautomatically uploaded to the Amazon cloud.\nApplication: Actual applications run in this layer and are offered to users\nfor further customization. Well-known examples include those found\nin of\ufb01ce suites (text processors, spreadsheet applications, presentation\napplications, and so on). It is important to realize that these applica-\ntions are again executed in the vendor\u2019s cloud. As before, they can be\ncompared to the traditional suite of applications that are shipped when\ninstalling an operating system.\nCloud-computing providers offer these layers to their customers through\nvarious interfaces (including command-line tools, programming interfaces,\nand Web interfaces), leading to three different types of services:\n\u2022Infrastructure-as-a-Service (IaaS ) covering the hardware and infrastructure\nlayer\n\u2022Platform-as-a-Service (PaaS ) covering the platform layer\n\u2022Software-as-a-Service (SaaS ) in which their applications are covered\nAs of now, making use of clouds is relatively easy, and we discuss in later\nchapters more concrete examples of interfaces to cloud providers. As a\nconsequence, cloud computing as a means for outsourcing local computing\ninfrastructures has become a serious option for many enterprises. However,\nthere are still a number of serious obstacles including provider lock-in, security\nand privacy issues, and dependency on the availability of services, to mention\na few (see also Armbrust et al. [2010]). Also, because the details on how\nspeci\ufb01c cloud computations are actually carried out are generally hidden, and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "32 CHAPTER 1. INTRODUCTION\neven perhaps unknown or unpredictable, meeting performance demands may\nbe impossible to arrange in advance. On top of this, Li et al. [2010] have shown\nthat different providers may easily show very different performance pro\ufb01les.\nCloud computing is no longer a hype, and certainly a serious alternative\nto maintaining huge local infrastructures, yet there is still a lot of room for\nimprovement.\nNote 1.9 (Advanced: Is cloud computing cheaper?)\nOne of the important reasons to migrate to a cloud environment is that it may be\nmuch cheaper compared to maintaining a local computing infrastructure. There\nare many ways to compute the savings, but as it turns out, only for simple and\nobvious cases will straightforward computations give a realistic perspective. Hajjat\net al. [2010] propose a more thorough approach, taking into account that part of\nan application suite is migrated to a cloud, and the other part continues to be\noperated on a local infrastructure. The crux of their method is providing the right\nmodel of a suite of enterprise applications.\nThe core of their approach is formed by a potentially large set of software\ncomponents . Each enterprise application is assumed to consist of components.\nFurthermore, each component Ciis considered to be run on Niservers. A simple\nexample is a database component to be executed by a single server. A more\nelaborate example is a Web application for computing bicycle routes, consisting\nof a Web server front end for rendering HTML pages and accepting user input, a\ncomponent for computing shortest paths (perhaps under different constraints),\nand a database component containing various maps.\nEach application is modeled as a directed graph, in which a vertex represents\na component and an arc h\u0000 !i,jithe fact that data \ufb02ows from component Cito\ncomponent Cj. Each arc has two associated weights: Ti,jrepresents the number of\ntransactions per time unit leading to data \ufb02owing from CitoCj, and Si,jthe average\nsize of those transactions (i.e., the average amount of data per transaction). They\nassume that Ti,jand Si,jare known, typically obtained through straightforward\nmeasurements.\nMigrating a suite of applications from a local infrastructure to the cloud\nthen boils down to \ufb01nding an optimal migration plan M: \ufb01guring out for each\ncomponent Ci, how many niof its Niservers should be moved to the cloud, such\nthat the monetary bene\ufb01ts resulting from M, reduced by the additional costs for\ncommunicating over the Internet, are maximal. A plan Mshould also meet the\nfollowing constraints:\n1.Policy constraints are met. For example, there may be data that is legally\nrequired to be located at an organization\u2019s local infrastructure.\n2.Because communication is now partly across long-haul Internet links, it may\nbe that certain transactions between components become much slower. A\nplan Mis acceptable only if any additional latencies do not violate speci\ufb01c\ndelay constraints.\n3.Flow balance equations should be respected: transactions continue to\noperate correctly, and requests or data are not lost during a transaction.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n32 CHAPTER 1. INTRODUCTION\neven perhaps unknown or unpredictable, meeting performance demands may\nbe impossible to arrange in advance. On top of this, Li et al. [2010] have shown\nthat different providers may easily show very different performance pro\ufb01les.\nCloud computing is no longer a hype, and certainly a serious alternative\nto maintaining huge local infrastructures, yet there is still a lot of room for\nimprovement.\nNote 1.9 (Advanced: Is cloud computing cheaper?)\nOne of the important reasons to migrate to a cloud environment is that it may be\nmuch cheaper compared to maintaining a local computing infrastructure. There\nare many ways to compute the savings, but as it turns out, only for simple and\nobvious cases will straightforward computations give a realistic perspective. Hajjat\net al. [2010] propose a more thorough approach, taking into account that part of\nan application suite is migrated to a cloud, and the other part continues to be\noperated on a local infrastructure. The crux of their method is providing the right\nmodel of a suite of enterprise applications.\nThe core of their approach is formed by a potentially large set of software\ncomponents . Each enterprise application is assumed to consist of components.\nFurthermore, each component Ciis considered to be run on Niservers. A simple\nexample is a database component to be executed by a single server. A more\nelaborate example is a Web application for computing bicycle routes, consisting\nof a Web server front end for rendering HTML pages and accepting user input, a\ncomponent for computing shortest paths (perhaps under different constraints),\nand a database component containing various maps.\nEach application is modeled as a directed graph, in which a vertex represents\na component and an arc h\u0000 !i,jithe fact that data \ufb02ows from component Cito\ncomponent Cj. Each arc has two associated weights: Ti,jrepresents the number of\ntransactions per time unit leading to data \ufb02owing from CitoCj, and Si,jthe average\nsize of those transactions (i.e., the average amount of data per transaction). They\nassume that Ti,jand Si,jare known, typically obtained through straightforward\nmeasurements.\nMigrating a suite of applications from a local infrastructure to the cloud\nthen boils down to \ufb01nding an optimal migration plan M: \ufb01guring out for each\ncomponent Ci, how many niof its Niservers should be moved to the cloud, such\nthat the monetary bene\ufb01ts resulting from M, reduced by the additional costs for\ncommunicating over the Internet, are maximal. A plan Mshould also meet the\nfollowing constraints:\n1.Policy constraints are met. For example, there may be data that is legally\nrequired to be located at an organization\u2019s local infrastructure.\n2.Because communication is now partly across long-haul Internet links, it may\nbe that certain transactions between components become much slower. A\nplan Mis acceptable only if any additional latencies do not violate speci\ufb01c\ndelay constraints.\n3.Flow balance equations should be respected: transactions continue to\noperate correctly, and requests or data are not lost during a transaction.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 33\nLet us now look into the bene\ufb01ts and Internet costs of a migration plan.\nBene\ufb01ts For each migration plan M, one can expect to have monetary savings\nexpressed as Bene\ufb01ts (M), because fewer machines or network connections need to\nbe maintained. In many organizations, such costs are known so that it may be\nrelatively simple to compute the savings. On the other hand, there are costs to\nbe made for using the cloud. Hajjat et al. [2010] make a simplifying distinction\nbetween the bene\ufb01t Bcof migrating a compute-intensive component, and the\nbene\ufb01t Bsof migrating a storage-intensive component. If there are Mccompute-\nintensive and Msstorage-intensive components, we have Bene\ufb01ts (M) =Bc\u0001Mc+\nBs\u0001Ms. Obviously, much more sophisticated models can be deployed as well.\nInternet costs To compute the increased communication costs because com-\nponents are spread across the cloud as well as the local infrastructure, we need\nto take user-initiated requests into account. To simplify matters, we make no\ndistinction between internal users (i.e., members of the enterprise), and external\nusers (as one would see in the case of Web applications). Traf\ufb01c from users before\nmigration can be expressed as:\nTrlocal ,inet=\u00e5\nCi(Tuser,iSuser,i+Ti,userSi,user)\nwhere Tuser,idenotes the number of transactions per time unit leading to data\n\ufb02owing from users to Ci. We have analogous interpretations for Ti,user,Suser i, and\nSi,user.\nFor each component Ci, let Ci,local denote the servers that continue to operate\non the local infrastructure, and Ci,cloud its servers that are placed in the cloud. Note\nthatjCi,cloudj=ni. For simplicity, assume that a server from Ci,local distributes\ntraf\ufb01c in the same proportions as a server from Ci,cloud. We are interested in\nthe rate of transactions between local servers, cloud servers, and between local\nand cloud servers, after migration. Let skbe the server for component Ckand\ndenote by fkthe fraction nk/Nk. We then have for the rate of transactions T\u0003\ni,jafter\nmigration:\nT\u0003\ni,j=8\n>>>><\n>>>>:(1\u0000fi)\u0001(1\u0000fj)\u0001Ti,jwhen si2Ci,local andsj2Cj,local\n(1\u0000fi)\u0001fj\u0001Ti,j when si2Ci,local andsj2Cj,cloud\nfi\u0001(1\u0000fj)\u0001Ti,j when si2Ci,cloud andsj2Cj,local\nfi\u0001fj\u0001Ti,j when si2Ci,cloud andsj2Cj,cloud\nS\u0003\ni,jis the amount of data associated with T\u0003\ni,j. Note that fkdenotes the fraction of\nservers of component Ckthat are moved to the cloud. In other words, (1\u0000fk)is\nthe fraction that stays in the local infrastructure. We leave it to the reader to give\nan expression for T\u0003\ni,user.\nFinally, let cost local ,inetand cost cloud ,inetdenote the per-unit Internet costs for\ntraf\ufb01c to and from the local infrastructure and cloud, respectively. Ignoring a few\nsubtleties explained in [Hajjat et al., 2010], we can then compute the local Internet\ntraf\ufb01c after migration as:\nTr\u0003\nlocal ,inet=\u00e5\nCi,local,Cj,local(T\u0003\ni,jS\u0003\ni,j+T\u0003\nj,iS\u0003\nj,i) +\u00e5\nCj,local(T\u0003\nuser,jS\u0003\nuser,j+T\u0003\nj,userS\u0003\nj,user)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 33\nLet us now look into the bene\ufb01ts and Internet costs of a migration plan.\nBene\ufb01ts For each migration plan M, one can expect to have monetary savings\nexpressed as Bene\ufb01ts (M), because fewer machines or network connections need to\nbe maintained. In many organizations, such costs are known so that it may be\nrelatively simple to compute the savings. On the other hand, there are costs to\nbe made for using the cloud. Hajjat et al. [2010] make a simplifying distinction\nbetween the bene\ufb01t Bcof migrating a compute-intensive component, and the\nbene\ufb01t Bsof migrating a storage-intensive component. If there are Mccompute-\nintensive and Msstorage-intensive components, we have Bene\ufb01ts (M) =Bc\u0001Mc+\nBs\u0001Ms. Obviously, much more sophisticated models can be deployed as well.\nInternet costs To compute the increased communication costs because com-\nponents are spread across the cloud as well as the local infrastructure, we need\nto take user-initiated requests into account. To simplify matters, we make no\ndistinction between internal users (i.e., members of the enterprise), and external\nusers (as one would see in the case of Web applications). Traf\ufb01c from users before\nmigration can be expressed as:\nTrlocal ,inet=\u00e5\nCi(Tuser,iSuser,i+Ti,userSi,user)\nwhere Tuser,idenotes the number of transactions per time unit leading to data\n\ufb02owing from users to Ci. We have analogous interpretations for Ti,user,Suser i, and\nSi,user.\nFor each component Ci, let Ci,local denote the servers that continue to operate\non the local infrastructure, and Ci,cloud its servers that are placed in the cloud. Note\nthatjCi,cloudj=ni. For simplicity, assume that a server from Ci,local distributes\ntraf\ufb01c in the same proportions as a server from Ci,cloud. We are interested in\nthe rate of transactions between local servers, cloud servers, and between local\nand cloud servers, after migration. Let skbe the server for component Ckand\ndenote by fkthe fraction nk/Nk. We then have for the rate of transactions T\u0003\ni,jafter\nmigration:\nT\u0003\ni,j=8\n>>>><\n>>>>:(1\u0000fi)\u0001(1\u0000fj)\u0001Ti,jwhen si2Ci,local andsj2Cj,local\n(1\u0000fi)\u0001fj\u0001Ti,j when si2Ci,local andsj2Cj,cloud\nfi\u0001(1\u0000fj)\u0001Ti,j when si2Ci,cloud andsj2Cj,local\nfi\u0001fj\u0001Ti,j when si2Ci,cloud andsj2Cj,cloud\nS\u0003\ni,jis the amount of data associated with T\u0003\ni,j. Note that fkdenotes the fraction of\nservers of component Ckthat are moved to the cloud. In other words, (1\u0000fk)is\nthe fraction that stays in the local infrastructure. We leave it to the reader to give\nan expression for T\u0003\ni,user.\nFinally, let cost local ,inetand cost cloud ,inetdenote the per-unit Internet costs for\ntraf\ufb01c to and from the local infrastructure and cloud, respectively. Ignoring a few\nsubtleties explained in [Hajjat et al., 2010], we can then compute the local Internet\ntraf\ufb01c after migration as:\nTr\u0003\nlocal ,inet=\u00e5\nCi,local,Cj,local(T\u0003\ni,jS\u0003\ni,j+T\u0003\nj,iS\u0003\nj,i) +\u00e5\nCj,local(T\u0003\nuser,jS\u0003\nuser,j+T\u0003\nj,userS\u0003\nj,user)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "34 CHAPTER 1. INTRODUCTION\nand, likewise, for the cloud Internet traf\ufb01c after migration:\nTr\u0003\ncloud ,inet=\u00e5\nCi,cloud,Cj,cloud(T\u0003\ni,jS\u0003\ni,j+T\u0003\nj,iS\u0003\nj,i) +\u00e5\nCj,cloud(T\u0003\nuser,jS\u0003\nuser,j+T\u0003\nj,userS\u0003\nj,user)\nTogether, this leads to a model for the increase in Internet communication costs:\ncost local ,inet(Tr\u0003\nlocal ,inet\u0000Trlocal ,inet) +cost cloud ,inetTr\u0003\ncloud ,inet\nClearly, answering the question whether moving to the cloud is cheaper requires a\nlot of detailed information and careful planning of exactly what to migrate. Hajjat\net al. [2010] provide a \ufb01rst step toward making an informed decision. Their model\nis more detailed than we are willing to explain here. An important aspect that\nwe have not touched upon is that migrating components also means that special\nattention will have to be paid to migrating security components. The interested\nreader is referred to their paper.\nDistributed information systems\nAnother important class of distributed systems is found in organizations\nthat were confronted with a wealth of networked applications, but for which\ninteroperability turned out to be a painful experience. Many of the existing\nmiddleware solutions are the result of working with an infrastructure in which\nit was easier to integrate applications into an enterprise-wide information\nsystem [Alonso et al., 2004; Bernstein, 1996; Hohpe and Woolf, 2004].\nWe can distinguish several levels at which integration can take place. In\nmany cases, a networked application simply consists of a server running that\napplication (often including a database) and making it available to remote\nprograms, called clients . Such clients send a request to the server for executing\na speci\ufb01c operation, after which a response is sent back. Integration at the\nlowest level allows clients to wrap a number of requests, possibly for different\nservers, into a single larger request and have it executed as a distributed\ntransaction . The key idea is that all, or none of the requests are executed.\nAs applications became more sophisticated and were gradually separated\ninto independent components (notably distinguishing database components\nfrom processing components), it became clear that integration should also\ntake place by letting applications communicate directly with each other. This\nhas now lead to a huge industry that concentrates on Enterprise Application\nIntegration (EAI).\nDistributed transaction processing\nTo clarify our discussion, we concentrate on database applications. In practice,\noperations on a database are carried out in the form of transactions . Pro-\ngramming using transactions requires special primitives that must either be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n34 CHAPTER 1. INTRODUCTION\nand, likewise, for the cloud Internet traf\ufb01c after migration:\nTr\u0003\ncloud ,inet=\u00e5\nCi,cloud,Cj,cloud(T\u0003\ni,jS\u0003\ni,j+T\u0003\nj,iS\u0003\nj,i) +\u00e5\nCj,cloud(T\u0003\nuser,jS\u0003\nuser,j+T\u0003\nj,userS\u0003\nj,user)\nTogether, this leads to a model for the increase in Internet communication costs:\ncost local ,inet(Tr\u0003\nlocal ,inet\u0000Trlocal ,inet) +cost cloud ,inetTr\u0003\ncloud ,inet\nClearly, answering the question whether moving to the cloud is cheaper requires a\nlot of detailed information and careful planning of exactly what to migrate. Hajjat\net al. [2010] provide a \ufb01rst step toward making an informed decision. Their model\nis more detailed than we are willing to explain here. An important aspect that\nwe have not touched upon is that migrating components also means that special\nattention will have to be paid to migrating security components. The interested\nreader is referred to their paper.\nDistributed information systems\nAnother important class of distributed systems is found in organizations\nthat were confronted with a wealth of networked applications, but for which\ninteroperability turned out to be a painful experience. Many of the existing\nmiddleware solutions are the result of working with an infrastructure in which\nit was easier to integrate applications into an enterprise-wide information\nsystem [Alonso et al., 2004; Bernstein, 1996; Hohpe and Woolf, 2004].\nWe can distinguish several levels at which integration can take place. In\nmany cases, a networked application simply consists of a server running that\napplication (often including a database) and making it available to remote\nprograms, called clients . Such clients send a request to the server for executing\na speci\ufb01c operation, after which a response is sent back. Integration at the\nlowest level allows clients to wrap a number of requests, possibly for different\nservers, into a single larger request and have it executed as a distributed\ntransaction . The key idea is that all, or none of the requests are executed.\nAs applications became more sophisticated and were gradually separated\ninto independent components (notably distinguishing database components\nfrom processing components), it became clear that integration should also\ntake place by letting applications communicate directly with each other. This\nhas now lead to a huge industry that concentrates on Enterprise Application\nIntegration (EAI).\nDistributed transaction processing\nTo clarify our discussion, we concentrate on database applications. In practice,\noperations on a database are carried out in the form of transactions . Pro-\ngramming using transactions requires special primitives that must either be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 35\nsupplied by the underlying distributed system or by the language runtime\nsystem. Typical examples of transaction primitives are shown in Figure 1.10.\nThe exact list of primitives depends on what kinds of objects are being used\nin the transaction [Gray and Reuter, 1993; Bernstein and Newcomer, 2009]. In\na mail system, there might be primitives to send, receive, and forward mail.\nIn an accounting system, they might be quite different. READ and WRITE are\ntypical examples, however. Ordinary statements, procedure calls, and so on,\nare also allowed inside a transaction. In particular, remote procedure calls\n(RPC s), that is, procedure calls to remote servers, are often also encapsulated\nin a transaction, leading to what is known as a transactional RPC . We discuss\nRPCs extensively in Section 4.2.\nPrimitive Description\nBEGIN _TRANSACTION Mark the start of a transaction\nEND _TRANSACTION Terminate the transaction and try to commit\nABORT _TRANSACTION Kill the transaction and restore the old values\nREAD Read data from a \ufb01le, a table, or otherwise\nWRITE Write data to a \ufb01le, a table, or otherwise\nFigure 1.10: Example primitives for transactions.\nBEGIN _TRANSACTION and END _TRANSACTION are used to delimit the\nscope of a transaction. The operations between them form the body of the\ntransaction. The characteristic feature of a transaction is either all of these\noperations are executed or none are executed. These may be system calls,\nlibrary procedures, or bracketing statements in a language, depending on the\nimplementation.\nThis all-or-nothing property of transactions is one of the four characteristic\nproperties that transactions have. More speci\ufb01cally, transactions adhere to the\nso-called ACID properties:\n\u2022Atomic : To the outside world, the transaction happens indivisibly\n\u2022Consistent : The transaction does not violate system invariants\n\u2022Isolated : Concurrent transactions do not interfere with each other\n\u2022Durable : Once a transaction commits, the changes are permanent\nIn distributed systems, transactions are often constructed as a number of\nsubtransactions, jointly forming a nested transaction as shown in Figure 1.11.\nThe top-level transaction may fork off children that run in parallel with one\nanother, on different machines, to gain performance or simplify programming.\nEach of these children may also execute one or more subtransactions, or fork\noff its own children.\nSubtransactions give rise to a subtle, but important, problem. Imagine\nthat a transaction starts several subtransactions in parallel, and one of these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 35\nsupplied by the underlying distributed system or by the language runtime\nsystem. Typical examples of transaction primitives are shown in Figure 1.10.\nThe exact list of primitives depends on what kinds of objects are being used\nin the transaction [Gray and Reuter, 1993; Bernstein and Newcomer, 2009]. In\na mail system, there might be primitives to send, receive, and forward mail.\nIn an accounting system, they might be quite different. READ and WRITE are\ntypical examples, however. Ordinary statements, procedure calls, and so on,\nare also allowed inside a transaction. In particular, remote procedure calls\n(RPC s), that is, procedure calls to remote servers, are often also encapsulated\nin a transaction, leading to what is known as a transactional RPC . We discuss\nRPCs extensively in Section 4.2.\nPrimitive Description\nBEGIN _TRANSACTION Mark the start of a transaction\nEND _TRANSACTION Terminate the transaction and try to commit\nABORT _TRANSACTION Kill the transaction and restore the old values\nREAD Read data from a \ufb01le, a table, or otherwise\nWRITE Write data to a \ufb01le, a table, or otherwise\nFigure 1.10: Example primitives for transactions.\nBEGIN _TRANSACTION and END _TRANSACTION are used to delimit the\nscope of a transaction. The operations between them form the body of the\ntransaction. The characteristic feature of a transaction is either all of these\noperations are executed or none are executed. These may be system calls,\nlibrary procedures, or bracketing statements in a language, depending on the\nimplementation.\nThis all-or-nothing property of transactions is one of the four characteristic\nproperties that transactions have. More speci\ufb01cally, transactions adhere to the\nso-called ACID properties:\n\u2022Atomic : To the outside world, the transaction happens indivisibly\n\u2022Consistent : The transaction does not violate system invariants\n\u2022Isolated : Concurrent transactions do not interfere with each other\n\u2022Durable : Once a transaction commits, the changes are permanent\nIn distributed systems, transactions are often constructed as a number of\nsubtransactions, jointly forming a nested transaction as shown in Figure 1.11.\nThe top-level transaction may fork off children that run in parallel with one\nanother, on different machines, to gain performance or simplify programming.\nEach of these children may also execute one or more subtransactions, or fork\noff its own children.\nSubtransactions give rise to a subtle, but important, problem. Imagine\nthat a transaction starts several subtransactions in parallel, and one of these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "36 CHAPTER 1. INTRODUCTION\nFigure 1.11: A nested transaction.\ncommits, making its results visible to the parent transaction. After further\ncomputation, the parent aborts, restoring the entire system to the state it\nhad before the top-level transaction started. Consequently, the results of\nthe subtransaction that committed must nevertheless be undone. Thus the\npermanence referred to above applies only to top-level transactions.\nSince transactions can be nested arbitrarily deep, considerable administra-\ntion is needed to get everything right. The semantics are clear, however. When\nany transaction or subtransaction starts, it is conceptually given a private copy\nof all data in the entire system for it to manipulate as it wishes. If it aborts,\nits private universe just vanishes, as if it had never existed. If it commits,\nits private universe replaces the parent\u2019s universe. Thus if a subtransaction\ncommits and then later a new subtransaction is started, the second one sees\nthe results produced by the \ufb01rst one. Likewise, if an enclosing (higher level)\ntransaction aborts, all its underlying subtransactions have to be aborted as\nwell. And if several transactions are started concurrently, the result is as if\nthey ran sequentially in some unspeci\ufb01ed order.\nNested transactions are important in distributed systems, for they provide\na natural way of distributing a transaction across multiple machines. They\nfollow a logical division of the work of the original transaction. For example,\na transaction for planning a trip by which three different \ufb02ights need to be\nreserved can be logically split up into three subtransactions. Each of these\nsubtransactions can be managed separately and independently of the other\ntwo.\nIn the early days of enterprise middleware systems, the component that\nhandled distributed (or nested) transactions formed the core for integrating\napplications at the server or database level. This component was called a\ntransaction processing monitor orTP monitor for short. Its main task was\nto allow an application to access multiple server/databases by offering it a\ntransactional programming model, as shown in Figure 1.12. Essentially, the TP\nmonitor coordinated the commitment of subtransactions following a standard\nprotocol known as distributed commit , which we discuss in Section 8.5.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n36 CHAPTER 1. INTRODUCTION\nFigure 1.11: A nested transaction.\ncommits, making its results visible to the parent transaction. After further\ncomputation, the parent aborts, restoring the entire system to the state it\nhad before the top-level transaction started. Consequently, the results of\nthe subtransaction that committed must nevertheless be undone. Thus the\npermanence referred to above applies only to top-level transactions.\nSince transactions can be nested arbitrarily deep, considerable administra-\ntion is needed to get everything right. The semantics are clear, however. When\nany transaction or subtransaction starts, it is conceptually given a private copy\nof all data in the entire system for it to manipulate as it wishes. If it aborts,\nits private universe just vanishes, as if it had never existed. If it commits,\nits private universe replaces the parent\u2019s universe. Thus if a subtransaction\ncommits and then later a new subtransaction is started, the second one sees\nthe results produced by the \ufb01rst one. Likewise, if an enclosing (higher level)\ntransaction aborts, all its underlying subtransactions have to be aborted as\nwell. And if several transactions are started concurrently, the result is as if\nthey ran sequentially in some unspeci\ufb01ed order.\nNested transactions are important in distributed systems, for they provide\na natural way of distributing a transaction across multiple machines. They\nfollow a logical division of the work of the original transaction. For example,\na transaction for planning a trip by which three different \ufb02ights need to be\nreserved can be logically split up into three subtransactions. Each of these\nsubtransactions can be managed separately and independently of the other\ntwo.\nIn the early days of enterprise middleware systems, the component that\nhandled distributed (or nested) transactions formed the core for integrating\napplications at the server or database level. This component was called a\ntransaction processing monitor orTP monitor for short. Its main task was\nto allow an application to access multiple server/databases by offering it a\ntransactional programming model, as shown in Figure 1.12. Essentially, the TP\nmonitor coordinated the commitment of subtransactions following a standard\nprotocol known as distributed commit , which we discuss in Section 8.5.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 37\nFigure 1.12: The role of a TP monitor in distributed systems.\nAn important observation is that applications wanting to coordinate sev-\neral subtransactions into a single transaction did not have to implement this\ncoordination themselves. By simply making use of a TP monitor, this coordi-\nnation was done for them. This is exactly where middleware comes into play:\nit implements services that are useful for many applications avoiding that\nsuch services have to be reimplemented over and over again by application\ndevelopers.\nEnterprise application integration\nAs mentioned, the more applications became decoupled from the databases\nthey were built upon, the more evident it became that facilities were needed\nto integrate applications independently from their databases. In particular, ap-\nplication components should be able to communicate directly with each other\nand not merely by means of the request/reply behavior that was supported\nby transaction processing systems.\nThis need for interapplication communication led to many different com-\nmunication models, The main idea was that existing applications could directly\nexchange information, as shown in Figure 1.13.\nSeveral types of communication middleware exist. With remote procedure\ncalls (RPC ), an application component can effectively send a request to another\napplication component by doing a local procedure call, which results in the\nrequest being packaged as a message and sent to the callee. Likewise, the\nresult will be sent back and returned to the application as the result of the\nprocedure call.\nAs the popularity of object technology increased, techniques were devel-\noped to allow calls to remote objects, leading to what is known as remote\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 37\nFigure 1.12: The role of a TP monitor in distributed systems.\nAn important observation is that applications wanting to coordinate sev-\neral subtransactions into a single transaction did not have to implement this\ncoordination themselves. By simply making use of a TP monitor, this coordi-\nnation was done for them. This is exactly where middleware comes into play:\nit implements services that are useful for many applications avoiding that\nsuch services have to be reimplemented over and over again by application\ndevelopers.\nEnterprise application integration\nAs mentioned, the more applications became decoupled from the databases\nthey were built upon, the more evident it became that facilities were needed\nto integrate applications independently from their databases. In particular, ap-\nplication components should be able to communicate directly with each other\nand not merely by means of the request/reply behavior that was supported\nby transaction processing systems.\nThis need for interapplication communication led to many different com-\nmunication models, The main idea was that existing applications could directly\nexchange information, as shown in Figure 1.13.\nSeveral types of communication middleware exist. With remote procedure\ncalls (RPC ), an application component can effectively send a request to another\napplication component by doing a local procedure call, which results in the\nrequest being packaged as a message and sent to the callee. Likewise, the\nresult will be sent back and returned to the application as the result of the\nprocedure call.\nAs the popularity of object technology increased, techniques were devel-\noped to allow calls to remote objects, leading to what is known as remote\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "38 CHAPTER 1. INTRODUCTION\nFigure 1.13: Middleware as a communication facilitator in enterprise applica-\ntion integration.\nmethod invocations (RMI ). An RMI is essentially the same as an RPC, except\nthat it operates on objects instead of functions.\nRPC and RMI have the disadvantage that the caller and callee both need\nto be up and running at the time of communication. In addition, they need\nto know exactly how to refer to each other. This tight coupling is often expe-\nrienced as a serious drawback, and has lead to what is known as message-\noriented middleware , or simply MOM . In this case, applications send mes-\nsages to logical contact points, often described by means of a subject. Likewise,\napplications can indicate their interest for a speci\ufb01c type of message, after\nwhich the communication middleware will take care that those messages are\ndelivered to those applications. These so-called publish/subscribe systems\nform an important and expanding class of distributed systems.\nNote 1.10 (More information: On integrating applications)\nSupporting enterprise application integration is an important goal for many mid-\ndleware products. In general, there are four ways to integrate applications [Hohpe\nand Woolf, 2004]:\nFile transfer: The essence of integration through \ufb01le transfer, is that an appli-\ncation produces a \ufb01le containing shared data that is subsequently read\nby other applications. The approach is technically very simple, making it\nappealing. The drawback, however, is that there are a lot of things that\nneed to be agreed upon:\n\u2022File format and layout: text, binary, its structure, and so on. Nowadays,\nXML has become popular as its \ufb01les are, in principle, self-describing.\n\u2022File management: where are they stored, how are they named, who is\nresponsible for deleting \ufb01les?\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n38 CHAPTER 1. INTRODUCTION\nFigure 1.13: Middleware as a communication facilitator in enterprise applica-\ntion integration.\nmethod invocations (RMI ). An RMI is essentially the same as an RPC, except\nthat it operates on objects instead of functions.\nRPC and RMI have the disadvantage that the caller and callee both need\nto be up and running at the time of communication. In addition, they need\nto know exactly how to refer to each other. This tight coupling is often expe-\nrienced as a serious drawback, and has lead to what is known as message-\noriented middleware , or simply MOM . In this case, applications send mes-\nsages to logical contact points, often described by means of a subject. Likewise,\napplications can indicate their interest for a speci\ufb01c type of message, after\nwhich the communication middleware will take care that those messages are\ndelivered to those applications. These so-called publish/subscribe systems\nform an important and expanding class of distributed systems.\nNote 1.10 (More information: On integrating applications)\nSupporting enterprise application integration is an important goal for many mid-\ndleware products. In general, there are four ways to integrate applications [Hohpe\nand Woolf, 2004]:\nFile transfer: The essence of integration through \ufb01le transfer, is that an appli-\ncation produces a \ufb01le containing shared data that is subsequently read\nby other applications. The approach is technically very simple, making it\nappealing. The drawback, however, is that there are a lot of things that\nneed to be agreed upon:\n\u2022File format and layout: text, binary, its structure, and so on. Nowadays,\nXML has become popular as its \ufb01les are, in principle, self-describing.\n\u2022File management: where are they stored, how are they named, who is\nresponsible for deleting \ufb01les?\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 39\n\u2022Update propagation: When an application produces a \ufb01le, there may\nbe several applications that need to read that \ufb01le in order to provide\nthe view of a single coherent system, as we argued in Section 1.1. As a\nconsequence, sometimes separate programs need to be implemented\nthat notify applications of \ufb01le updates.\nShared database: Many of the problems associated with integration through \ufb01les\nare alleviated when using a shared database. All applications will have ac-\ncess to the same data, and often through a high-level language such as SQL.\nAlso, it is easy to notify applications when changes occur, as triggers are\noften part of modern databases. There are, however, two major drawbacks.\nFirst, there is still a need to design a common data schema, which may be\nfar from trivial if the set of applications that need to be integrated is not\ncompletely known in advance. Second, when there are many reads and\nupdates, a shared database can easily become a performance bottleneck.\nRemote procedure call: Integration through \ufb01les or a database implicitly as-\nsumes that changes by one application can easily trigger other applications\nto take action. However, practice shows that sometimes small changes\nshould actually trigger many applications to take actions. In such cases,\nit is not really the change of data that is important, but the execution of a\nseries of actions.\nSeries of actions are best captured through the execution of a procedure\n(which may, in turn, lead to all kinds of changes in shared data). To\nprevent that every application needs to know all the internals of those\nactions (as implemented by another application), standard encapsulation\ntechniques should be used, as deployed with traditional procedure calls\nor object invocations. For such situations, an application can best offer\na procedure to other applications in the form of a remote procedure call,\nor RPC. In essence, an RPC allows an application Ato make use of the\ninformation available only to application B, without giving Adirect access\nto that information. There are many advantages and disadvantages to\nremote procedure calls, which are discussed in depth in Chapter 4.\nMessaging: A main drawback of RPCs is that caller and callee need to be up\nand running at the same time in order for the call to succeed. However, in\nmany scenarios this simultaneous activity is often dif\ufb01cult or impossible\nto guarantee. In such cases, offering a messaging system carrying requests\nfrom application Ato perform an action at application B, is what is needed.\nThe messaging system ensures that eventually the request is delivered,\nand if needed, that a response is eventually returned as well. Obviously,\nmessaging is not the panacea for application integration: it also introduces\nproblems concerning data formatting and layout, it requires an application\nto know where to send a message to, there need to be scenarios for dealing\nwith lost messages, and so on. Like RPCs, we will be discussing these\nissues extensively in Chapter 4.\nWhat these four approaches tell us, is that application integration will generally\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 39\n\u2022Update propagation: When an application produces a \ufb01le, there may\nbe several applications that need to read that \ufb01le in order to provide\nthe view of a single coherent system, as we argued in Section 1.1. As a\nconsequence, sometimes separate programs need to be implemented\nthat notify applications of \ufb01le updates.\nShared database: Many of the problems associated with integration through \ufb01les\nare alleviated when using a shared database. All applications will have ac-\ncess to the same data, and often through a high-level language such as SQL.\nAlso, it is easy to notify applications when changes occur, as triggers are\noften part of modern databases. There are, however, two major drawbacks.\nFirst, there is still a need to design a common data schema, which may be\nfar from trivial if the set of applications that need to be integrated is not\ncompletely known in advance. Second, when there are many reads and\nupdates, a shared database can easily become a performance bottleneck.\nRemote procedure call: Integration through \ufb01les or a database implicitly as-\nsumes that changes by one application can easily trigger other applications\nto take action. However, practice shows that sometimes small changes\nshould actually trigger many applications to take actions. In such cases,\nit is not really the change of data that is important, but the execution of a\nseries of actions.\nSeries of actions are best captured through the execution of a procedure\n(which may, in turn, lead to all kinds of changes in shared data). To\nprevent that every application needs to know all the internals of those\nactions (as implemented by another application), standard encapsulation\ntechniques should be used, as deployed with traditional procedure calls\nor object invocations. For such situations, an application can best offer\na procedure to other applications in the form of a remote procedure call,\nor RPC. In essence, an RPC allows an application Ato make use of the\ninformation available only to application B, without giving Adirect access\nto that information. There are many advantages and disadvantages to\nremote procedure calls, which are discussed in depth in Chapter 4.\nMessaging: A main drawback of RPCs is that caller and callee need to be up\nand running at the same time in order for the call to succeed. However, in\nmany scenarios this simultaneous activity is often dif\ufb01cult or impossible\nto guarantee. In such cases, offering a messaging system carrying requests\nfrom application Ato perform an action at application B, is what is needed.\nThe messaging system ensures that eventually the request is delivered,\nand if needed, that a response is eventually returned as well. Obviously,\nmessaging is not the panacea for application integration: it also introduces\nproblems concerning data formatting and layout, it requires an application\nto know where to send a message to, there need to be scenarios for dealing\nwith lost messages, and so on. Like RPCs, we will be discussing these\nissues extensively in Chapter 4.\nWhat these four approaches tell us, is that application integration will generally\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "40 CHAPTER 1. INTRODUCTION\nnot be simple. Middleware (in the form of a distributed system), however, can\nsigni\ufb01cantly help in integration by providing the right facilities such as support\nfor RPCs or messaging. As said, enterprise application integration is an important\ntarget \ufb01eld for many middleware products.\nPervasive systems\nThe distributed systems discussed so far are largely characterized by their\nstability: nodes are \ufb01xed and have a more or less permanent and high-quality\nconnection to a network. To a certain extent, this stability is realized through\nthe various techniques for achieving distribution transparency. For example,\nthere are many ways how we can create the illusion that only occasionally\ncomponents may fail. Likewise, there are all kinds of means to hide the actual\nnetwork location of a node, effectively allowing users and applications to\nbelieve that nodes stay put.\nHowever, matters have changed since the introduction of mobile and\nembedded computing devices, leading to what are generally referred to as\npervasive systems . As its name suggests, pervasive systems are intended to\nnaturally blend into our environment. They are naturally also distributed\nsystems, and certainly meet the characterization we gave in Section 1.1.\nWhat makes them unique in comparison to the computing and information\nsystems described so far, is that the separation between users and system\ncomponents is much more blurred. There is often no single dedicated interface,\nsuch as a screen/keyboard combination. Instead, a pervasive system is often\nequipped with many sensors that pick up various aspects of a user\u2019s behavior.\nLikewise, it may have a myriad of actuators to provide information and\nfeedback, often even purposefully aiming to steer behavior.\nMany devices in pervasive systems are characterized by being small,\nbattery-powered, mobile, and having only a wireless connection, although\nnot all these characteristics apply to all devices. These are not necessarily\nrestrictive characteristics, as is illustrated by smartphones [Roussos et al., 2005]\nand their role in what is now coined as the Internet of Things [Mattern and\nFloerkemeier, 2010; Stankovic, 2014]. Nevertheless, notably the fact that we\noften need to deal with the intricacies of wireless and mobile communication,\nwill require special solutions to make a pervasive system as transparent or\nunobtrusive as possible.\nIn the following, we make a distinction between three different types of\npervasive systems, although there is considerable overlap between the three\ntypes: ubiquitous computing systems, mobile systems, and sensor networks.\nThis distinction allows us to focus on different aspects of pervasive systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n40 CHAPTER 1. INTRODUCTION\nnot be simple. Middleware (in the form of a distributed system), however, can\nsigni\ufb01cantly help in integration by providing the right facilities such as support\nfor RPCs or messaging. As said, enterprise application integration is an important\ntarget \ufb01eld for many middleware products.\nPervasive systems\nThe distributed systems discussed so far are largely characterized by their\nstability: nodes are \ufb01xed and have a more or less permanent and high-quality\nconnection to a network. To a certain extent, this stability is realized through\nthe various techniques for achieving distribution transparency. For example,\nthere are many ways how we can create the illusion that only occasionally\ncomponents may fail. Likewise, there are all kinds of means to hide the actual\nnetwork location of a node, effectively allowing users and applications to\nbelieve that nodes stay put.\nHowever, matters have changed since the introduction of mobile and\nembedded computing devices, leading to what are generally referred to as\npervasive systems . As its name suggests, pervasive systems are intended to\nnaturally blend into our environment. They are naturally also distributed\nsystems, and certainly meet the characterization we gave in Section 1.1.\nWhat makes them unique in comparison to the computing and information\nsystems described so far, is that the separation between users and system\ncomponents is much more blurred. There is often no single dedicated interface,\nsuch as a screen/keyboard combination. Instead, a pervasive system is often\nequipped with many sensors that pick up various aspects of a user\u2019s behavior.\nLikewise, it may have a myriad of actuators to provide information and\nfeedback, often even purposefully aiming to steer behavior.\nMany devices in pervasive systems are characterized by being small,\nbattery-powered, mobile, and having only a wireless connection, although\nnot all these characteristics apply to all devices. These are not necessarily\nrestrictive characteristics, as is illustrated by smartphones [Roussos et al., 2005]\nand their role in what is now coined as the Internet of Things [Mattern and\nFloerkemeier, 2010; Stankovic, 2014]. Nevertheless, notably the fact that we\noften need to deal with the intricacies of wireless and mobile communication,\nwill require special solutions to make a pervasive system as transparent or\nunobtrusive as possible.\nIn the following, we make a distinction between three different types of\npervasive systems, although there is considerable overlap between the three\ntypes: ubiquitous computing systems, mobile systems, and sensor networks.\nThis distinction allows us to focus on different aspects of pervasive systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 41\nUbiquitous computing systems\nSo far, we have been talking about pervasive systems to emphasize that\nits elements have spread through in many parts of our environment. In a\nubiquitous computing system we go one step further: the system is pervasive\nand continuously present. The latter means that a user will be continuously\ninteracting with the system, often not even being aware that interaction is\ntaking place. Poslad [2009] describes the core requirements for a ubiquitous\ncomputing system roughly as follows:\n1.(Distribution ) Devices are networked, distributed, and accessible in a\ntransparent manner\n2.(Interaction ) Interaction between users and devices is highly unobtru-\nsive\n3.(Context awareness ) The system is aware of a user\u2019s context in order to\noptimize interaction\n4.(Autonomy ) Devices operate autonomously without human interven-\ntion, and are thus highly self-managed\n5.(Intelligence ) The system as a whole can handle a wide range of dy-\nnamic actions and interactions\nLet us brie\ufb02y consider these requirements from a distributed-systems perspec-\ntive.\nAd. 1: Distribution. As mentioned, a ubiquitous computing system is an\nexample of a distributed system: the devices and other computers forming\nthe nodes of a system are simply networked and work together to form\nthe illusion of a single coherent system. Distribution also comes naturally:\nthere will be devices close to users (such as sensors and actuators), connected\nto computers hidden from view and perhaps even operating remotely in a\ncloud. Most, if not all, of the requirements regarding distribution transparency\nmentioned in Section 1.2, should therefore hold.\nAd. 2: Interaction. When it comes to interaction with users, ubiquitous\ncomputing systems differ a lot in comparison to the systems we have been\ndiscussing so far. End users play a prominent role in the design of ubiquitous\nsystems, meaning that special attention needs to be paid to how the interac-\ntion between users and core system takes place. For ubiquitous computing\nsystems, much of the interaction by humans will be implicit, with an implicit\naction being de\ufb01ned as one \u201cthat is not primarily aimed to interact with a com-\nputerized system but which such a system understands as input\u201d [Schmidt,\n2000]. In other words, a user could be mostly unaware of the fact that input is\nbeing provided to a computer system. From a certain perspective, ubiquitous\ncomputing can be said to seemingly hide interfaces.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 41\nUbiquitous computing systems\nSo far, we have been talking about pervasive systems to emphasize that\nits elements have spread through in many parts of our environment. In a\nubiquitous computing system we go one step further: the system is pervasive\nand continuously present. The latter means that a user will be continuously\ninteracting with the system, often not even being aware that interaction is\ntaking place. Poslad [2009] describes the core requirements for a ubiquitous\ncomputing system roughly as follows:\n1.(Distribution ) Devices are networked, distributed, and accessible in a\ntransparent manner\n2.(Interaction ) Interaction between users and devices is highly unobtru-\nsive\n3.(Context awareness ) The system is aware of a user\u2019s context in order to\noptimize interaction\n4.(Autonomy ) Devices operate autonomously without human interven-\ntion, and are thus highly self-managed\n5.(Intelligence ) The system as a whole can handle a wide range of dy-\nnamic actions and interactions\nLet us brie\ufb02y consider these requirements from a distributed-systems perspec-\ntive.\nAd. 1: Distribution. As mentioned, a ubiquitous computing system is an\nexample of a distributed system: the devices and other computers forming\nthe nodes of a system are simply networked and work together to form\nthe illusion of a single coherent system. Distribution also comes naturally:\nthere will be devices close to users (such as sensors and actuators), connected\nto computers hidden from view and perhaps even operating remotely in a\ncloud. Most, if not all, of the requirements regarding distribution transparency\nmentioned in Section 1.2, should therefore hold.\nAd. 2: Interaction. When it comes to interaction with users, ubiquitous\ncomputing systems differ a lot in comparison to the systems we have been\ndiscussing so far. End users play a prominent role in the design of ubiquitous\nsystems, meaning that special attention needs to be paid to how the interac-\ntion between users and core system takes place. For ubiquitous computing\nsystems, much of the interaction by humans will be implicit, with an implicit\naction being de\ufb01ned as one \u201cthat is not primarily aimed to interact with a com-\nputerized system but which such a system understands as input\u201d [Schmidt,\n2000]. In other words, a user could be mostly unaware of the fact that input is\nbeing provided to a computer system. From a certain perspective, ubiquitous\ncomputing can be said to seemingly hide interfaces.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "42 CHAPTER 1. INTRODUCTION\nA simple example is where the settings of a car\u2019s driver\u2019s seat, steering\nwheel, and mirrors is fully personalized. If Bob takes a seat, the system will\nrecognize that it is dealing with Bob and subsequently makes the appropriate\nadjustments. The same happens when Alice uses the car, while an unknown\nuser will be steered toward making his or her own adjustments (to be remem-\nbered for later). This example already illustrates an important role of sensors\nin ubiquitous computing, namely as input devices that are used to identify a\nsituation (a speci\ufb01c person apparently wanting to drive), whose input analysis\nleads to actions (making adjustments). In turn, the actions may lead to natural\nreactions, for example that Bob slightly changes the seat settings. The system\nwill have to take all (implicit and explicit) actions by the user into account\nand react accordingly.\nAd. 3: Context awareness. Reacting to the sensory input, but also the explicit\ninput from users is more easily said than done. What a ubiquitous computing\nsystem needs to do, is to take the context in which interactions take place\ninto account. Context awareness also differentiates ubiquitous computing\nsystems from the more traditional systems we have been discussing before,\nand is described by Dey and Abowd [2000] as \u201cany information that can be\nused to characterize the situation of entities (i.e., whether a person, place or\nobject) that are considered relevant to the interaction between a user and an\napplication, including the user and the application themselves.\u201d In practice,\ncontext is often characterized by location, identity, time, and activity: the where ,\nwho,when , and what . A system will need to have the necessary (sensory) input\nto determine one or several of these context types.\nWhat is important from a distributed-systems perspective, is that raw data\nas collected by various sensors is lifted to a level of abstraction that can be\nused by applications. A concrete example is detecting where a person is,\nfor example in terms of GPS coordinates, and subsequently mapping that\ninformation to an actual location, such as the corner of a street, or a speci\ufb01c\nshop or other known facility. The question is where this processing of sensory\ninput takes place: is all data collected at a central server connected to a\ndatabase with detailed information on a city, or is it the user\u2019s smartphone\nwhere the mapping is done? Clearly, there are trade-offs to be considered.\nDey [2010] discusses more general approaches toward building context-\naware applications. When it comes to combining \ufb02exibility and potential\ndistribution, so-called shared data spaces in which processes are decoupled\nin time and space are attractive, yet as we shall see in later chapters, suffer\nfrom scalability problems. A survey on context-awareness and its relation to\nmiddleware and distributed systems is provided by Baldauf et al. [2007].\nAd. 4: Autonomy. An important aspect of most ubiquitous computing sys-\ntems is that explicit systems management has been reduced to a minimum. In\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n42 CHAPTER 1. INTRODUCTION\nA simple example is where the settings of a car\u2019s driver\u2019s seat, steering\nwheel, and mirrors is fully personalized. If Bob takes a seat, the system will\nrecognize that it is dealing with Bob and subsequently makes the appropriate\nadjustments. The same happens when Alice uses the car, while an unknown\nuser will be steered toward making his or her own adjustments (to be remem-\nbered for later). This example already illustrates an important role of sensors\nin ubiquitous computing, namely as input devices that are used to identify a\nsituation (a speci\ufb01c person apparently wanting to drive), whose input analysis\nleads to actions (making adjustments). In turn, the actions may lead to natural\nreactions, for example that Bob slightly changes the seat settings. The system\nwill have to take all (implicit and explicit) actions by the user into account\nand react accordingly.\nAd. 3: Context awareness. Reacting to the sensory input, but also the explicit\ninput from users is more easily said than done. What a ubiquitous computing\nsystem needs to do, is to take the context in which interactions take place\ninto account. Context awareness also differentiates ubiquitous computing\nsystems from the more traditional systems we have been discussing before,\nand is described by Dey and Abowd [2000] as \u201cany information that can be\nused to characterize the situation of entities (i.e., whether a person, place or\nobject) that are considered relevant to the interaction between a user and an\napplication, including the user and the application themselves.\u201d In practice,\ncontext is often characterized by location, identity, time, and activity: the where ,\nwho,when , and what . A system will need to have the necessary (sensory) input\nto determine one or several of these context types.\nWhat is important from a distributed-systems perspective, is that raw data\nas collected by various sensors is lifted to a level of abstraction that can be\nused by applications. A concrete example is detecting where a person is,\nfor example in terms of GPS coordinates, and subsequently mapping that\ninformation to an actual location, such as the corner of a street, or a speci\ufb01c\nshop or other known facility. The question is where this processing of sensory\ninput takes place: is all data collected at a central server connected to a\ndatabase with detailed information on a city, or is it the user\u2019s smartphone\nwhere the mapping is done? Clearly, there are trade-offs to be considered.\nDey [2010] discusses more general approaches toward building context-\naware applications. When it comes to combining \ufb02exibility and potential\ndistribution, so-called shared data spaces in which processes are decoupled\nin time and space are attractive, yet as we shall see in later chapters, suffer\nfrom scalability problems. A survey on context-awareness and its relation to\nmiddleware and distributed systems is provided by Baldauf et al. [2007].\nAd. 4: Autonomy. An important aspect of most ubiquitous computing sys-\ntems is that explicit systems management has been reduced to a minimum. In\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 43\na ubiquitous computing environment there is simply no room for a systems\nadministrator to keep everything up and running. As a consequence, the\nsystem as a whole should be able to act autonomously, and automatically\nreact to changes. This requires a myriad of techniques of which several will\nbe discussed throughout this book. To give a few simple examples, think of\nthe following:\nAddress allocation: In order for networked devices to communicate, they\nneed an IP address. Addresses can be allocated automatically using\nprotocols like the Dynamic Host Con\ufb01guration Protocol (DHCP) [Droms,\n1997] (which requires a server) or Zeroconf [Guttman, 2001].\nAdding devices: It should be easy to add devices to an existing system. A\nstep towards automatic con\ufb01guration is realized by the Universal Plug\nand Play Protocol (UPnP ) [UPnP Forum, 2008]. Using UPnP , devices can\ndiscover each other and make sure that they can set up communication\nchannels between them.\nAutomatic updates: Many devices in a ubiquitous computing system should\nbe able to regularly check through the Internet if their software should\nbe updated. If so, they can download new versions of their components\nand ideally continue where they left off.\nAdmittedly, these are very simple examples, but the picture should be clear\nthat manual intervention is to be kept to a minimum. We will be discussing\nmany techniques related to self-management in detail throughout the book.\nAd. 5: Intelligence. Finally, Poslad [2009] mentions that ubiquitous com-\nputing systems often use methods and techniques from the \ufb01eld of arti\ufb01cial\nintelligence. What this means, is that in many cases a wide range of ad-\nvanced algorithms and models need to be deployed to handle incomplete\ninput, quickly react to a changing environment, handle unexpected events,\nand so on. The extent to which this can or should be done in a distributed\nfashion is crucial from the perspective of distributed systems. Unfortunately,\ndistributed solutions for many problems in the \ufb01eld of arti\ufb01cial intelligence\nare yet to be found, meaning that there may be a natural tension between\nthe \ufb01rst requirement of networked and distributed devices, and advanced\ndistributed information processing.\nMobile computing systems\nAs mentioned, mobility often forms an important component of pervasive\nsystems, and many, if not all aspects that we have just discussed also apply to\nmobile computing. There are several issues that set mobile computing aside\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 43\na ubiquitous computing environment there is simply no room for a systems\nadministrator to keep everything up and running. As a consequence, the\nsystem as a whole should be able to act autonomously, and automatically\nreact to changes. This requires a myriad of techniques of which several will\nbe discussed throughout this book. To give a few simple examples, think of\nthe following:\nAddress allocation: In order for networked devices to communicate, they\nneed an IP address. Addresses can be allocated automatically using\nprotocols like the Dynamic Host Con\ufb01guration Protocol (DHCP) [Droms,\n1997] (which requires a server) or Zeroconf [Guttman, 2001].\nAdding devices: It should be easy to add devices to an existing system. A\nstep towards automatic con\ufb01guration is realized by the Universal Plug\nand Play Protocol (UPnP ) [UPnP Forum, 2008]. Using UPnP , devices can\ndiscover each other and make sure that they can set up communication\nchannels between them.\nAutomatic updates: Many devices in a ubiquitous computing system should\nbe able to regularly check through the Internet if their software should\nbe updated. If so, they can download new versions of their components\nand ideally continue where they left off.\nAdmittedly, these are very simple examples, but the picture should be clear\nthat manual intervention is to be kept to a minimum. We will be discussing\nmany techniques related to self-management in detail throughout the book.\nAd. 5: Intelligence. Finally, Poslad [2009] mentions that ubiquitous com-\nputing systems often use methods and techniques from the \ufb01eld of arti\ufb01cial\nintelligence. What this means, is that in many cases a wide range of ad-\nvanced algorithms and models need to be deployed to handle incomplete\ninput, quickly react to a changing environment, handle unexpected events,\nand so on. The extent to which this can or should be done in a distributed\nfashion is crucial from the perspective of distributed systems. Unfortunately,\ndistributed solutions for many problems in the \ufb01eld of arti\ufb01cial intelligence\nare yet to be found, meaning that there may be a natural tension between\nthe \ufb01rst requirement of networked and distributed devices, and advanced\ndistributed information processing.\nMobile computing systems\nAs mentioned, mobility often forms an important component of pervasive\nsystems, and many, if not all aspects that we have just discussed also apply to\nmobile computing. There are several issues that set mobile computing aside\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "44 CHAPTER 1. INTRODUCTION\nto pervasive systems in general (see also Adelstein et al. [2005] and Tarkoma\nand Kangasharju [2009]).\nFirst, the devices that form part of a (distributed) mobile system may\nvary widely. Typically, mobile computing is now done with devices such\nas smartphones and tablet computers. However, completely different types\nof devices are now using the Internet Protocol (IP) to communicate, placing\nmobile computing in a different perspective. Such devices include remote\ncontrols, pagers, active badges, car equipment, various GPS-enabled devices,\nand so on. A characteristic feature of all these devices is that they use wireless\ncommunication. Mobile implies wireless so it seems (although there are\nexceptions to the rules).\nSecond, in mobile computing the location of a device is assumed to change\nover time. A changing location has its effects on many issues. For example, if\nthe location of a device changes regularly, so will perhaps the services that\nare locally available. As a consequence, we may need to pay special attention\nto dynamically discovering services, but also letting services announce their\npresence. In a similar vein, we often also want to know where a device actually\nis. This may mean that we need to know the actual geographical coordinates\nof a device such as in tracking and tracing applications, but it may also require\nthat we are able to simply detect its network position (as in mobile IP [Perkins,\n2010; Perkins et al., 2011].\nChanging locations also has a profound effect on communication. To\nillustrate, consider a (wireless) mobile ad hoc network, generally abbreviated\nas a MANET . Suppose that two devices in a MANET have discovered each\nother in the sense that they know each other\u2019s network address. How do we\nroute messages between the two? Static routes are generally not sustainable\nas nodes along the routing path can easily move out of their neighbor\u2019s range,\ninvalidating the path. For large MANETs, using a priori set-up paths is not\na viable option. What we are dealing with here are so-called disruption-\ntolerant networks : networks in which connectivity between two nodes can\nsimply not be guaranteed. Getting a message from one node to another may\nthen be problematic, to say the least.\nThe trick in such cases, is not to attempt to set up a communication path\nfrom the source to the destination, but to rely on two principles. First, as we\nwill discuss in Section 4.4, using special \ufb02ooding-based techniques will allow\na message to gradually spread through a part of the network, to eventually\nreach the destination. Obviously, any type of \ufb02ooding will impose redundant\ncommunication, but this may be the price we have to pay. Second, in a\ndisruption-tolerant network, we let an intermediate node store a received\nmessage until it encounters another node to which it can pass it on. In other\nwords, a node becomes a temporary carrier of a message, as sketched in\nFigure 1.14. Eventually, the message should reach its destination.\nIt is not dif\ufb01cult to imagine that selectively passing messages to encoun-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n44 CHAPTER 1. INTRODUCTION\nto pervasive systems in general (see also Adelstein et al. [2005] and Tarkoma\nand Kangasharju [2009]).\nFirst, the devices that form part of a (distributed) mobile system may\nvary widely. Typically, mobile computing is now done with devices such\nas smartphones and tablet computers. However, completely different types\nof devices are now using the Internet Protocol (IP) to communicate, placing\nmobile computing in a different perspective. Such devices include remote\ncontrols, pagers, active badges, car equipment, various GPS-enabled devices,\nand so on. A characteristic feature of all these devices is that they use wireless\ncommunication. Mobile implies wireless so it seems (although there are\nexceptions to the rules).\nSecond, in mobile computing the location of a device is assumed to change\nover time. A changing location has its effects on many issues. For example, if\nthe location of a device changes regularly, so will perhaps the services that\nare locally available. As a consequence, we may need to pay special attention\nto dynamically discovering services, but also letting services announce their\npresence. In a similar vein, we often also want to know where a device actually\nis. This may mean that we need to know the actual geographical coordinates\nof a device such as in tracking and tracing applications, but it may also require\nthat we are able to simply detect its network position (as in mobile IP [Perkins,\n2010; Perkins et al., 2011].\nChanging locations also has a profound effect on communication. To\nillustrate, consider a (wireless) mobile ad hoc network, generally abbreviated\nas a MANET . Suppose that two devices in a MANET have discovered each\nother in the sense that they know each other\u2019s network address. How do we\nroute messages between the two? Static routes are generally not sustainable\nas nodes along the routing path can easily move out of their neighbor\u2019s range,\ninvalidating the path. For large MANETs, using a priori set-up paths is not\na viable option. What we are dealing with here are so-called disruption-\ntolerant networks : networks in which connectivity between two nodes can\nsimply not be guaranteed. Getting a message from one node to another may\nthen be problematic, to say the least.\nThe trick in such cases, is not to attempt to set up a communication path\nfrom the source to the destination, but to rely on two principles. First, as we\nwill discuss in Section 4.4, using special \ufb02ooding-based techniques will allow\na message to gradually spread through a part of the network, to eventually\nreach the destination. Obviously, any type of \ufb02ooding will impose redundant\ncommunication, but this may be the price we have to pay. Second, in a\ndisruption-tolerant network, we let an intermediate node store a received\nmessage until it encounters another node to which it can pass it on. In other\nwords, a node becomes a temporary carrier of a message, as sketched in\nFigure 1.14. Eventually, the message should reach its destination.\nIt is not dif\ufb01cult to imagine that selectively passing messages to encoun-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 45\nFigure 1.14: Passing messages in a (mobile) disruption-tolerant network.\ntered nodes may help to ensure ef\ufb01cient delivery. For example, if nodes are\nknown to belong to a certain class, and the source and destination belong to\nthe same class, we may decide to pass messages only among nodes in that\nclass. Likewise, it may prove ef\ufb01cient to pass messages only to well-connected\nnodes, that is, nodes who have been in range of many other nodes in the\nrecent past. An overview is provided by Spyropoulos et al. [2010].\nNote 1.11 (Advanced: Social networks and mobility patterns)\nNot surprisingly, mobile computing is tightly coupled to the whereabouts of\nhuman beings. With the increasing interest in complex social networks [Vega-\nRedondo, 2007; Jackson, 2008] and the explosion of the use of smartphones,\nseveral groups are seeking to combine analysis of social behavior and information\ndissemination in so-called pocket-switched networks [Hui et al., 2005]. The latter\nare networks in which nodes are formed by people (or actually, their mobile\ndevices), and links are formed when two people encounter each other, allowing\ntheir devices to exchange data.\nThe basic idea is to let information be spread using the ad hoc communications\nbetween people. In doing so, it becomes important to understand the structure of\na social group. One of the \ufb01rst to examine how social awareness can be exploited\nin mobile networks were Miklas et al. [2007]. In their approach, based on traces\non encounters between people, two people are characterized as either friends or\nstrangers. Friends interact frequently, where the number of recurring encounters\nbetween strangers is low. The goal is to make sure that a message from Alice to\nBob is eventually delivered.\nAs it turns out, when Alice adopts a strategy by which she hands out the\nmessage to each of her friends, and that each of those friends passes the message\nto Bob as soon as he is encountered, can ensure that the message reaches Bob\nwith a delay exceeding approximately 10% of the best-attainable delay. Any other\nstrategy, like forwarding the message to only 1 or 2 friends, performs much worse.\nPassing a message to a stranger has no signi\ufb01cant effect. In other words, it makes\na huge difference if nodes take friend relationships into account, but even then it\nis still necessary to judiciously adopt a forwarding strategy.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 45\nFigure 1.14: Passing messages in a (mobile) disruption-tolerant network.\ntered nodes may help to ensure ef\ufb01cient delivery. For example, if nodes are\nknown to belong to a certain class, and the source and destination belong to\nthe same class, we may decide to pass messages only among nodes in that\nclass. Likewise, it may prove ef\ufb01cient to pass messages only to well-connected\nnodes, that is, nodes who have been in range of many other nodes in the\nrecent past. An overview is provided by Spyropoulos et al. [2010].\nNote 1.11 (Advanced: Social networks and mobility patterns)\nNot surprisingly, mobile computing is tightly coupled to the whereabouts of\nhuman beings. With the increasing interest in complex social networks [Vega-\nRedondo, 2007; Jackson, 2008] and the explosion of the use of smartphones,\nseveral groups are seeking to combine analysis of social behavior and information\ndissemination in so-called pocket-switched networks [Hui et al., 2005]. The latter\nare networks in which nodes are formed by people (or actually, their mobile\ndevices), and links are formed when two people encounter each other, allowing\ntheir devices to exchange data.\nThe basic idea is to let information be spread using the ad hoc communications\nbetween people. In doing so, it becomes important to understand the structure of\na social group. One of the \ufb01rst to examine how social awareness can be exploited\nin mobile networks were Miklas et al. [2007]. In their approach, based on traces\non encounters between people, two people are characterized as either friends or\nstrangers. Friends interact frequently, where the number of recurring encounters\nbetween strangers is low. The goal is to make sure that a message from Alice to\nBob is eventually delivered.\nAs it turns out, when Alice adopts a strategy by which she hands out the\nmessage to each of her friends, and that each of those friends passes the message\nto Bob as soon as he is encountered, can ensure that the message reaches Bob\nwith a delay exceeding approximately 10% of the best-attainable delay. Any other\nstrategy, like forwarding the message to only 1 or 2 friends, performs much worse.\nPassing a message to a stranger has no signi\ufb01cant effect. In other words, it makes\na huge difference if nodes take friend relationships into account, but even then it\nis still necessary to judiciously adopt a forwarding strategy.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "46 CHAPTER 1. INTRODUCTION\nFor large groups of people, more sophisticated approaches are needed. In\nthe \ufb01rst place, it may happen that messages need to be sent between people in\ndifferent communities . What do we mean by a community? If we consider a\nsocial network (where a vertex represents a person, and a link the fact that two\npeople have a social relation), then a community is roughly speaking a group of\nvertices in which there are many links between its members and only few links\nwith vertices in other groups [Newman, 2010]. Unfortunately, many community-\ndetection algorithms require complete information on the social structure, making\nthem practically infeasible for optimizing communication in mobile networks.\nHui et al. [2007] propose a number of decentralized community detection\nalgorithms. In essence, these algorithms rely on letting a node i(1) detect the set\nof nodes it regularly encounters, called its familiar set Fi, and (2) incrementally\nexpand its local community Ci, with Fi\u0012Ci. Initially, Cias well as Fiwill be\nempty, but gradually, Fiwill grow, and with it, Ci. In the simplest case, a node jis\nadded to a community Cias follows:\nNode iadds jtoCiwhenjFj\\Cij\njFjj>lfor some l>0\nIn other words, when the fraction of j\u2019s familiar set substantially overlaps with\nthe community of i, then node ishould add jto its community. Also, we have the\nfollowing for merging communities:\nMerge two communities when jCi\\Cjj>gjCi[Cjjfor some g>0\nwhich means that two communities should be merged when they have a signi\ufb01cant\nnumber of members in common. (In their experiments, Hui et al. found that\nsetting l=g=0.6 lead to good results.)\nKnowing communities, in combination with the connectivity of a node in\neither a community, or globally, can subsequently be used to ef\ufb01ciently forward\nmessages in a disruption-tolerant network, as explained by Hui et al. [2011].\nObviously, much of the performance of a mobile computing system depends\non how nodes move. In particular, in order to pre-assess the effectiveness of new\nprotocols or algorithms, having an idea on which mobility patterns are actually\nrealistic is important. For long, there was not much data on such patterns, but\nrecent experiments have changed that.\nVarious groups have started to collect statistics on human mobility, of which\nthe traces are used to drive simulations. In addition, traces have been used to\nderive more realistic mobility models (see, e.g., Kim et al. [2006b]). However,\nunderstanding human mobility patterns in general remains a dif\ufb01cult problem.\nGonz\u00e1lez et al. [2008] report on modeling efforts based on data collected from\n100,000 cell-phone users during a six-month period. They observed that the\ndisplacement behavior could be represented by the following, relatively simple\ndistribution:\nP[Dr] = (Dr+Dr0)\u0000b\u0001e\u0000Dr/k\nin which Dris the actual displacement and Dr0=1.5kma constant initial dis-\nplacement. With b=1.75 and k=400, this leads to the distribution shown in\nFigure 1.15.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n46 CHAPTER 1. INTRODUCTION\nFor large groups of people, more sophisticated approaches are needed. In\nthe \ufb01rst place, it may happen that messages need to be sent between people in\ndifferent communities . What do we mean by a community? If we consider a\nsocial network (where a vertex represents a person, and a link the fact that two\npeople have a social relation), then a community is roughly speaking a group of\nvertices in which there are many links between its members and only few links\nwith vertices in other groups [Newman, 2010]. Unfortunately, many community-\ndetection algorithms require complete information on the social structure, making\nthem practically infeasible for optimizing communication in mobile networks.\nHui et al. [2007] propose a number of decentralized community detection\nalgorithms. In essence, these algorithms rely on letting a node i(1) detect the set\nof nodes it regularly encounters, called its familiar set Fi, and (2) incrementally\nexpand its local community Ci, with Fi\u0012Ci. Initially, Cias well as Fiwill be\nempty, but gradually, Fiwill grow, and with it, Ci. In the simplest case, a node jis\nadded to a community Cias follows:\nNode iadds jtoCiwhenjFj\\Cij\njFjj>lfor some l>0\nIn other words, when the fraction of j\u2019s familiar set substantially overlaps with\nthe community of i, then node ishould add jto its community. Also, we have the\nfollowing for merging communities:\nMerge two communities when jCi\\Cjj>gjCi[Cjjfor some g>0\nwhich means that two communities should be merged when they have a signi\ufb01cant\nnumber of members in common. (In their experiments, Hui et al. found that\nsetting l=g=0.6 lead to good results.)\nKnowing communities, in combination with the connectivity of a node in\neither a community, or globally, can subsequently be used to ef\ufb01ciently forward\nmessages in a disruption-tolerant network, as explained by Hui et al. [2011].\nObviously, much of the performance of a mobile computing system depends\non how nodes move. In particular, in order to pre-assess the effectiveness of new\nprotocols or algorithms, having an idea on which mobility patterns are actually\nrealistic is important. For long, there was not much data on such patterns, but\nrecent experiments have changed that.\nVarious groups have started to collect statistics on human mobility, of which\nthe traces are used to drive simulations. In addition, traces have been used to\nderive more realistic mobility models (see, e.g., Kim et al. [2006b]). However,\nunderstanding human mobility patterns in general remains a dif\ufb01cult problem.\nGonz\u00e1lez et al. [2008] report on modeling efforts based on data collected from\n100,000 cell-phone users during a six-month period. They observed that the\ndisplacement behavior could be represented by the following, relatively simple\ndistribution:\nP[Dr] = (Dr+Dr0)\u0000b\u0001e\u0000Dr/k\nin which Dris the actual displacement and Dr0=1.5kma constant initial dis-\nplacement. With b=1.75 and k=400, this leads to the distribution shown in\nFigure 1.15.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 47\nFigure 1.15: The distribution of displacement of (mobile) cell-phone users.\nWe can conclude that people tend to stay put. In fact, further analysis revealed\nthat people tend to return to the same place after 24, 48, or 72 hours, clearly\nshowing that people tend to go the same places. In a follow-up study, Song\net al. [2010] could indeed show that human mobility is actually remarkably well\npredictable.\nSensor networks\nOur last example of pervasive systems is sensor networks. These networks in\nmany cases form part of the enabling technology for pervasiveness and we\nsee that many solutions for sensor networks return in pervasive applications.\nWhat makes sensor networks interesting from a distributed system\u2019s perspec-\ntive is that they are more than just a collection of input devices. Instead, as\nwe shall see, sensor nodes often collaborate to ef\ufb01ciently process the sensed\ndata in an application-speci\ufb01c manner, making them very different from, for\nexample, traditional computer networks. Akyildiz et al. [2002] and Akyildiz\net al. [2005] provide an overview from a networking perspective. A more\nsystems-oriented introduction to sensor networks is given by Zhao and Guibas\n[2004], but also Karl and Willig [2005] will show to be useful.\nA sensor network generally consists of tens to hundreds or thousands of\nrelatively small nodes, each equipped with one or more sensing devices. In\naddition, nodes can often act as actuators [Akyildiz and Kasimoglu, 2004],\na typical example being the automatic activation of sprinklers when a \ufb01re\nhas been detected. Many sensor networks use wireless communication, and\nthe nodes are often battery powered. Their limited resources, restricted\ncommunication capabilities, and constrained power consumption demand\nthat ef\ufb01ciency is high on the list of design criteria.\nWhen zooming into an individual node, we see that, conceptually, they\ndo not differ a lot from \u201cnormal\u201d computers: above the hardware there is a\nsoftware layer akin to what traditional operating systems offer, including low-\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 47\nFigure 1.15: The distribution of displacement of (mobile) cell-phone users.\nWe can conclude that people tend to stay put. In fact, further analysis revealed\nthat people tend to return to the same place after 24, 48, or 72 hours, clearly\nshowing that people tend to go the same places. In a follow-up study, Song\net al. [2010] could indeed show that human mobility is actually remarkably well\npredictable.\nSensor networks\nOur last example of pervasive systems is sensor networks. These networks in\nmany cases form part of the enabling technology for pervasiveness and we\nsee that many solutions for sensor networks return in pervasive applications.\nWhat makes sensor networks interesting from a distributed system\u2019s perspec-\ntive is that they are more than just a collection of input devices. Instead, as\nwe shall see, sensor nodes often collaborate to ef\ufb01ciently process the sensed\ndata in an application-speci\ufb01c manner, making them very different from, for\nexample, traditional computer networks. Akyildiz et al. [2002] and Akyildiz\net al. [2005] provide an overview from a networking perspective. A more\nsystems-oriented introduction to sensor networks is given by Zhao and Guibas\n[2004], but also Karl and Willig [2005] will show to be useful.\nA sensor network generally consists of tens to hundreds or thousands of\nrelatively small nodes, each equipped with one or more sensing devices. In\naddition, nodes can often act as actuators [Akyildiz and Kasimoglu, 2004],\na typical example being the automatic activation of sprinklers when a \ufb01re\nhas been detected. Many sensor networks use wireless communication, and\nthe nodes are often battery powered. Their limited resources, restricted\ncommunication capabilities, and constrained power consumption demand\nthat ef\ufb01ciency is high on the list of design criteria.\nWhen zooming into an individual node, we see that, conceptually, they\ndo not differ a lot from \u201cnormal\u201d computers: above the hardware there is a\nsoftware layer akin to what traditional operating systems offer, including low-\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "48 CHAPTER 1. INTRODUCTION\nlevel network access, access to sensors and actuators, memory management,\nand so on. Normally, support for speci\ufb01c services is included, such as\nlocalization, local storage (think of additional \ufb02ash devices), and convenient\ncommunication facilities such as messaging and routing. However, similar to\nother networked computer systems, additional support is needed to effectively\ndeploy sensor network applications . In distributed systems, this takes the form\nof middleware. For sensor networks, instead of looking at middleware, it is\nbetter to see what kind of programming support is provided, which has been\nextensively surveyed by Mottola and Picco [2011].\nOne typical aspect in programming support is the scope provided by\ncommunication primitives. This scope can vary between addressing the\nphysical neighborhood of a node, and providing primitives for systemwide\ncommunication. In addition, it may also be possible to address a speci\ufb01c group\nof nodes. Likewise, computations may be restricted to an individual node, a\ngroup of nodes, or affect all nodes. To illustrate, Welsh and Mainland [2004]\nuse so-called abstract regions allowing a node to identify a neighborhood\nfrom where it can, for example, gather information:\n1 region = k_nearest_region.create(8);\n2 reading = get_sensor_reading();\n3 region.putvar(reading_key, reading);\n4 max_id = region. reduce (OP_MAXID, reading_key);\nIn line 1, a node \ufb01rst creates a region of its eight nearest neighbors, after which\nit fetches a value from its sensor(s). This reading is subsequently written to\nthe previously de\ufb01ned region to be de\ufb01ned using the key reading_key . In\nline 4, the node checks whose sensor reading in the de\ufb01ned region was the\nlargest, which is returned in the variable max_id .\nAs another related example, consider a sensor network as implementing a\ndistributed database, which is, according to Mottola and Picco [2011], one of\nfour possible ways of accessing data. This database view is quite common and\neasy to understand when realizing that many sensor networks are deployed\nfor measurement and surveillance applications [Bonnet et al., 2002]. In these\ncases, an operator would like to extract information from (a part of) the\nnetwork by simply issuing queries such as \u201cWhat is the northbound traf\ufb01c\nload on highway 1 as Santa Cruz?\u201d Such queries resemble those of traditional\ndatabases. In this case, the answer will probably need to be provided through\ncollaboration of many sensors along highway 1, while leaving other sensors\nuntouched.\nTo organize a sensor network as a distributed database, there are essentially\ntwo extremes, as shown in Figure 1.16. First, sensors do not cooperate but\nsimply send their data to a centralized database located at the operator\u2019s site.\nThe other extreme is to forward queries to relevant sensors and to let each\ncompute an answer, requiring the operator to aggregate the responses.\nNeither of these solutions is very attractive. The \ufb01rst one requires that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n48 CHAPTER 1. INTRODUCTION\nlevel network access, access to sensors and actuators, memory management,\nand so on. Normally, support for speci\ufb01c services is included, such as\nlocalization, local storage (think of additional \ufb02ash devices), and convenient\ncommunication facilities such as messaging and routing. However, similar to\nother networked computer systems, additional support is needed to effectively\ndeploy sensor network applications . In distributed systems, this takes the form\nof middleware. For sensor networks, instead of looking at middleware, it is\nbetter to see what kind of programming support is provided, which has been\nextensively surveyed by Mottola and Picco [2011].\nOne typical aspect in programming support is the scope provided by\ncommunication primitives. This scope can vary between addressing the\nphysical neighborhood of a node, and providing primitives for systemwide\ncommunication. In addition, it may also be possible to address a speci\ufb01c group\nof nodes. Likewise, computations may be restricted to an individual node, a\ngroup of nodes, or affect all nodes. To illustrate, Welsh and Mainland [2004]\nuse so-called abstract regions allowing a node to identify a neighborhood\nfrom where it can, for example, gather information:\n1 region = k_nearest_region.create(8);\n2 reading = get_sensor_reading();\n3 region.putvar(reading_key, reading);\n4 max_id = region. reduce (OP_MAXID, reading_key);\nIn line 1, a node \ufb01rst creates a region of its eight nearest neighbors, after which\nit fetches a value from its sensor(s). This reading is subsequently written to\nthe previously de\ufb01ned region to be de\ufb01ned using the key reading_key . In\nline 4, the node checks whose sensor reading in the de\ufb01ned region was the\nlargest, which is returned in the variable max_id .\nAs another related example, consider a sensor network as implementing a\ndistributed database, which is, according to Mottola and Picco [2011], one of\nfour possible ways of accessing data. This database view is quite common and\neasy to understand when realizing that many sensor networks are deployed\nfor measurement and surveillance applications [Bonnet et al., 2002]. In these\ncases, an operator would like to extract information from (a part of) the\nnetwork by simply issuing queries such as \u201cWhat is the northbound traf\ufb01c\nload on highway 1 as Santa Cruz?\u201d Such queries resemble those of traditional\ndatabases. In this case, the answer will probably need to be provided through\ncollaboration of many sensors along highway 1, while leaving other sensors\nuntouched.\nTo organize a sensor network as a distributed database, there are essentially\ntwo extremes, as shown in Figure 1.16. First, sensors do not cooperate but\nsimply send their data to a centralized database located at the operator\u2019s site.\nThe other extreme is to forward queries to relevant sensors and to let each\ncompute an answer, requiring the operator to aggregate the responses.\nNeither of these solutions is very attractive. The \ufb01rst one requires that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 49\n(a)\n(b)\nFigure 1.16: Organizing a sensor network database, while storing and pro-\ncessing data (a) only at the operator\u2019s site or (b) only at the sensors.\nsensors send all their measured data through the network, which may waste\nnetwork resources and energy. The second solution may also be wasteful as\nit discards the aggregation capabilities of sensors which would allow much\nless data to be returned to the operator. What is needed are facilities for in-\nnetwork data processing , similar to the previous example of abstract regions.\nIn-network processing can be done in numerous ways. One obvious one is\nto forward a query to all sensor nodes along a tree encompassing all nodes\nand to subsequently aggregate the results as they are propagated back to the\nroot, where the initiator is located. Aggregation will take place where two\nor more branches of the tree come together. As simple as this scheme may\nsound, it introduces dif\ufb01cult questions:\n\u2022 How do we (dynamically) set up an ef\ufb01cient tree in a sensor network?\n\u2022 How does aggregation of results take place? Can it be controlled?\n\u2022 What happens when network links fail?\nThese questions have been partly addressed in TinyDB, which implements\na declarative (database) interface to wireless sensor networks [Madden et al.,\n2005]. In essence, TinyDB can use any tree-based routing algorithm. An\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 49\n(a)\n(b)\nFigure 1.16: Organizing a sensor network database, while storing and pro-\ncessing data (a) only at the operator\u2019s site or (b) only at the sensors.\nsensors send all their measured data through the network, which may waste\nnetwork resources and energy. The second solution may also be wasteful as\nit discards the aggregation capabilities of sensors which would allow much\nless data to be returned to the operator. What is needed are facilities for in-\nnetwork data processing , similar to the previous example of abstract regions.\nIn-network processing can be done in numerous ways. One obvious one is\nto forward a query to all sensor nodes along a tree encompassing all nodes\nand to subsequently aggregate the results as they are propagated back to the\nroot, where the initiator is located. Aggregation will take place where two\nor more branches of the tree come together. As simple as this scheme may\nsound, it introduces dif\ufb01cult questions:\n\u2022 How do we (dynamically) set up an ef\ufb01cient tree in a sensor network?\n\u2022 How does aggregation of results take place? Can it be controlled?\n\u2022 What happens when network links fail?\nThese questions have been partly addressed in TinyDB, which implements\na declarative (database) interface to wireless sensor networks [Madden et al.,\n2005]. In essence, TinyDB can use any tree-based routing algorithm. An\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "50 CHAPTER 1. INTRODUCTION\nintermediate node will collect and aggregate the results from its children,\nalong with its own \ufb01ndings, and send that toward the root. To make matters\nef\ufb01cient, queries span a period of time allowing for careful scheduling of\noperations so that network resources and energy are optimally consumed.\nHowever, when queries can be initiated from different points in the net-\nwork, using single-rooted trees such as in TinyDB may not be ef\ufb01cient enough.\nAs an alternative, sensor networks may be equipped with special nodes where\nresults are forwarded to, as well as the queries related to those results. To give\na simple example, queries and results related to temperature readings may\nbe collected at a different location than those related to humidity measure-\nments. This approach corresponds directly to the notion of publish/subscribe\nsystems.\nNote 1.12 (Advanced: When energy starts to become critical)\nAs mentioned, many sensor networks need to operate on an energy budget\ncoming from the use of batteries or other limited power supplies. An approach to\nreduce energy consumption, is to let nodes be active only part of the time. More\nspeci\ufb01cally, assume that a node is repeatedly active during Tactive time units, and\nbetween these active periods, it is suspended for Tsuspended units. The fraction of\ntime that a node is active is known as its duty cycle t, that is,\nt=Tactive\nTactive+Tsuspended\nValues for tare typically in the order of 10\u000030%, but when a network needs\nto stay operational for periods exceeding many months, or even years, attaining\nvalues as low as 1% are critical.\nA problem with duty-cycled networks is that, in principle, nodes need to be\nactive at the same time for otherwise communication would simply not be possible.\nConsidering that while a node is suspended, only its local clock continues ticking,\nand that these clocks are subject to drifts, waking up at the same time may be\nproblematic. This is particularly true for networks with very low duty cycles.\nWhen a group of nodes are active at the same time, the nodes are said to\nform a synchronized group . There are essentially two problems that need to be\naddressed. First, we need to make sure that the nodes in a synchronized group\nremain active at the same time. In practice, this turns out to be relatively simple\nif each node communicates information on its current local time. Then, simple\nlocal clock adjustments will do the trick. The second problem is more dif\ufb01cult,\nnamely how two different synchronized groups can be merged into one in which\nall nodes are synchronized. Let us take a closer look at what we are facing. Most\nof the following discussion is based on material by Voulgaris et al. [2016].\nIn order to have two groups be merged, we need to \ufb01rst ensure that one group\ndetects the other. Indeed, if their respective active periods are completely disjoint,\nthere is no hope that any node in one group can pick up a message from a node\nin the other group. In an active detection method , a node will send a join message\nduring its suspended period. In other words, while it is suspended, it temporarily\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n50 CHAPTER 1. INTRODUCTION\nintermediate node will collect and aggregate the results from its children,\nalong with its own \ufb01ndings, and send that toward the root. To make matters\nef\ufb01cient, queries span a period of time allowing for careful scheduling of\noperations so that network resources and energy are optimally consumed.\nHowever, when queries can be initiated from different points in the net-\nwork, using single-rooted trees such as in TinyDB may not be ef\ufb01cient enough.\nAs an alternative, sensor networks may be equipped with special nodes where\nresults are forwarded to, as well as the queries related to those results. To give\na simple example, queries and results related to temperature readings may\nbe collected at a different location than those related to humidity measure-\nments. This approach corresponds directly to the notion of publish/subscribe\nsystems.\nNote 1.12 (Advanced: When energy starts to become critical)\nAs mentioned, many sensor networks need to operate on an energy budget\ncoming from the use of batteries or other limited power supplies. An approach to\nreduce energy consumption, is to let nodes be active only part of the time. More\nspeci\ufb01cally, assume that a node is repeatedly active during Tactive time units, and\nbetween these active periods, it is suspended for Tsuspended units. The fraction of\ntime that a node is active is known as its duty cycle t, that is,\nt=Tactive\nTactive+Tsuspended\nValues for tare typically in the order of 10\u000030%, but when a network needs\nto stay operational for periods exceeding many months, or even years, attaining\nvalues as low as 1% are critical.\nA problem with duty-cycled networks is that, in principle, nodes need to be\nactive at the same time for otherwise communication would simply not be possible.\nConsidering that while a node is suspended, only its local clock continues ticking,\nand that these clocks are subject to drifts, waking up at the same time may be\nproblematic. This is particularly true for networks with very low duty cycles.\nWhen a group of nodes are active at the same time, the nodes are said to\nform a synchronized group . There are essentially two problems that need to be\naddressed. First, we need to make sure that the nodes in a synchronized group\nremain active at the same time. In practice, this turns out to be relatively simple\nif each node communicates information on its current local time. Then, simple\nlocal clock adjustments will do the trick. The second problem is more dif\ufb01cult,\nnamely how two different synchronized groups can be merged into one in which\nall nodes are synchronized. Let us take a closer look at what we are facing. Most\nof the following discussion is based on material by Voulgaris et al. [2016].\nIn order to have two groups be merged, we need to \ufb01rst ensure that one group\ndetects the other. Indeed, if their respective active periods are completely disjoint,\nthere is no hope that any node in one group can pick up a message from a node\nin the other group. In an active detection method , a node will send a join message\nduring its suspended period. In other words, while it is suspended, it temporarily\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.3. TYPES OF DISTRIBUTED SYSTEMS 51\nwakes up to elicit nodes in other groups to join. How big is the chance that\nanother node will pick up this message? Realize that we need to consider only the\ncase when t<0.5, for otherwise two active periods will always overlap, meaning\nthat two groups can easily detect each other\u2019s presence. The probability Pdathat a\njoin message can be picked up during another node\u2019s active period, is equal to\nPda=Tactive\nTsuspended=t\n1\u0000t\nThis means that for low values of t,Pdais also very small.\nIn a passive detection method , a node skips the suspended state with (a very\nlow) probability Pdp, that is, it simply stays active during the Tsuspended time\nunits following its active period. During this time, it will be able to pick up any\nmessages sent by its neighbors, who are, by de\ufb01nition, member of a different\nsynchronized group. Experiments show that passive detection is inferior to active\ndetection.\nSimply stating that two synchronized groups need to merge is not enough:\nifAand Bhave discovered each other, which group will adapt the duty-cycle\nsettings of the other? A simple solution is to use a notion of cluster IDs. Each\nnode starts with a randomly chosen ID and effectively also a synchronized group\nhaving only itself as member. After detecting another group B, all nodes in group\nAjoinBif and only if the cluster ID of Bis larger than that of A.\nSynchronization can be improved considerably using so-called targeted join\nmessages . Whenever a node Nreceives a join message from a group Awith a\nlower cluster ID, it should obviously not join A. However, as Nnow knows when\nthe active period of Ais, it can send a join message exactly during that period.\nObviously, the chance that a node from Awill receive that message is very high,\nallowing the nodes from Ato join N\u2019s group. In addition, when a node decides to\njoin another group, it can send a special message to its group members, giving\nthe opportunity to quickly join as well.\nFigure 1.17: The speed by which different synchronized groups can\nmerge.\nFigure 1.17 shows how quickly synchronized groups can merge using two\ndifferent strategies. The experiments are based on a 4000-node mobile network\nusing realistic mobility patterns. Nodes have a duty cycle of less than 1%. These\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.3. TYPES OF DISTRIBUTED SYSTEMS 51\nwakes up to elicit nodes in other groups to join. How big is the chance that\nanother node will pick up this message? Realize that we need to consider only the\ncase when t<0.5, for otherwise two active periods will always overlap, meaning\nthat two groups can easily detect each other\u2019s presence. The probability Pdathat a\njoin message can be picked up during another node\u2019s active period, is equal to\nPda=Tactive\nTsuspended=t\n1\u0000t\nThis means that for low values of t,Pdais also very small.\nIn a passive detection method , a node skips the suspended state with (a very\nlow) probability Pdp, that is, it simply stays active during the Tsuspended time\nunits following its active period. During this time, it will be able to pick up any\nmessages sent by its neighbors, who are, by de\ufb01nition, member of a different\nsynchronized group. Experiments show that passive detection is inferior to active\ndetection.\nSimply stating that two synchronized groups need to merge is not enough:\nifAand Bhave discovered each other, which group will adapt the duty-cycle\nsettings of the other? A simple solution is to use a notion of cluster IDs. Each\nnode starts with a randomly chosen ID and effectively also a synchronized group\nhaving only itself as member. After detecting another group B, all nodes in group\nAjoinBif and only if the cluster ID of Bis larger than that of A.\nSynchronization can be improved considerably using so-called targeted join\nmessages . Whenever a node Nreceives a join message from a group Awith a\nlower cluster ID, it should obviously not join A. However, as Nnow knows when\nthe active period of Ais, it can send a join message exactly during that period.\nObviously, the chance that a node from Awill receive that message is very high,\nallowing the nodes from Ato join N\u2019s group. In addition, when a node decides to\njoin another group, it can send a special message to its group members, giving\nthe opportunity to quickly join as well.\nFigure 1.17: The speed by which different synchronized groups can\nmerge.\nFigure 1.17 shows how quickly synchronized groups can merge using two\ndifferent strategies. The experiments are based on a 4000-node mobile network\nusing realistic mobility patterns. Nodes have a duty cycle of less than 1%. These\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "52 CHAPTER 1. INTRODUCTION\nexperiments show that bringing even a large mobile, duty-cycled network to a\nstate in which all nodes are active at the same time is quite feasible. For further\ninformation, see Voulgaris et al. [2016].\n1.4 Summary\nDistributed systems consist of autonomous computers that work together to\ngive the appearance of a single coherent system. This combination of inde-\npendent, yet coherent collective behavior is achieved by collecting application-\nindependent protocols into what is known as middleware: a software layer\nlogically placed between operating systems and distributed applications. Pro-\ntocols include those for communication, transactions, service composition,\nand perhaps most important, reliability.\nDesign goals for distributed systems include sharing resources and ensur-\ning openness. In addition, designers aim at hiding many of the intricacies\nrelated to the distribution of processes, data, and control. However, this\ndistribution transparency not only comes at a performance price, in practical\nsituations it can never be fully achieved. The fact that trade-offs need to\nbe made between achieving various forms of distribution transparency is\ninherent to the design of distributed systems, and can easily complicate their\nunderstanding. One speci\ufb01c dif\ufb01cult design goal that does not always blend\nwell with achieving distribution transparency is scalability. This is particularly\ntrue for geographical scalability, in which case hiding latencies and bandwidth\nrestrictions can turn out to be dif\ufb01cult. Likewise, administrative scalability\nby which a system is designed to span multiple administrative domains, may\neasily con\ufb02ict goals for achieving distribution transparency.\nMatters are further complicated by the fact that many developers initially\nmake assumptions about the underlying network that are fundamentally\nwrong. Later, when assumptions are dropped, it may turn out to be dif\ufb01cult\nto mask unwanted behavior. A typical example is assuming that network\nlatency is not signi\ufb01cant. Other pitfalls include assuming that the network is\nreliable, static, secure, and homogeneous.\nDifferent types of distributed systems exist which can be classi\ufb01ed as\nbeing oriented toward supporting computations, information processing, and\npervasiveness. Distributed computing systems are typically deployed for\nhigh-performance applications often originating from the \ufb01eld of parallel\ncomputing. A \ufb01eld that emerged from parallel processing was initially grid\ncomputing with a strong focus on worldwide sharing of resources, in turn\nleading to what is now known as cloud computing. Cloud computing goes\nbeyond high-performance computing and also supports distributed systems\nfound in traditional of\ufb01ce environments where we see databases playing an\nimportant role. Typically, transaction processing systems are deployed in\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n52 CHAPTER 1. INTRODUCTION\nexperiments show that bringing even a large mobile, duty-cycled network to a\nstate in which all nodes are active at the same time is quite feasible. For further\ninformation, see Voulgaris et al. [2016].\n1.4 Summary\nDistributed systems consist of autonomous computers that work together to\ngive the appearance of a single coherent system. This combination of inde-\npendent, yet coherent collective behavior is achieved by collecting application-\nindependent protocols into what is known as middleware: a software layer\nlogically placed between operating systems and distributed applications. Pro-\ntocols include those for communication, transactions, service composition,\nand perhaps most important, reliability.\nDesign goals for distributed systems include sharing resources and ensur-\ning openness. In addition, designers aim at hiding many of the intricacies\nrelated to the distribution of processes, data, and control. However, this\ndistribution transparency not only comes at a performance price, in practical\nsituations it can never be fully achieved. The fact that trade-offs need to\nbe made between achieving various forms of distribution transparency is\ninherent to the design of distributed systems, and can easily complicate their\nunderstanding. One speci\ufb01c dif\ufb01cult design goal that does not always blend\nwell with achieving distribution transparency is scalability. This is particularly\ntrue for geographical scalability, in which case hiding latencies and bandwidth\nrestrictions can turn out to be dif\ufb01cult. Likewise, administrative scalability\nby which a system is designed to span multiple administrative domains, may\neasily con\ufb02ict goals for achieving distribution transparency.\nMatters are further complicated by the fact that many developers initially\nmake assumptions about the underlying network that are fundamentally\nwrong. Later, when assumptions are dropped, it may turn out to be dif\ufb01cult\nto mask unwanted behavior. A typical example is assuming that network\nlatency is not signi\ufb01cant. Other pitfalls include assuming that the network is\nreliable, static, secure, and homogeneous.\nDifferent types of distributed systems exist which can be classi\ufb01ed as\nbeing oriented toward supporting computations, information processing, and\npervasiveness. Distributed computing systems are typically deployed for\nhigh-performance applications often originating from the \ufb01eld of parallel\ncomputing. A \ufb01eld that emerged from parallel processing was initially grid\ncomputing with a strong focus on worldwide sharing of resources, in turn\nleading to what is now known as cloud computing. Cloud computing goes\nbeyond high-performance computing and also supports distributed systems\nfound in traditional of\ufb01ce environments where we see databases playing an\nimportant role. Typically, transaction processing systems are deployed in\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "1.4. SUMMARY 53\nthese environments. Finally, an emerging class of distributed systems is where\ncomponents are small, the system is composed in an ad hoc fashion, but\nmost of all is no longer managed through a system administrator. This last\nclass is typically represented by pervasive computing environments, including\nmobile-computing systems as well as sensor-rich environments.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n1.4. SUMMARY 53\nthese environments. Finally, an emerging class of distributed systems is where\ncomponents are small, the system is composed in an ad hoc fashion, but\nmost of all is no longer managed through a system administrator. This last\nclass is typically represented by pervasive computing environments, including\nmobile-computing systems as well as sensor-rich environments.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "", "Chapter 2\nArchitectures\nDistributed systems are often complex pieces of software of which the compo-\nnents are by de\ufb01nition dispersed across multiple machines. To master their\ncomplexity, it is crucial that these systems are properly organized. There are\ndifferent ways on how to view the organization of a distributed system, but\nan obvious one is to make a distinction between, on the one hand, the logical\norganization of the collection of software components, and on the other hand\nthe actual physical realization.\nThe organization of distributed systems is mostly about the software\ncomponents that constitute the system. These software architectures tell us\nhow the various software components are to be organized and how they\nshould interact. In this chapter we will \ufb01rst pay attention to some commonly\napplied architectural styles toward organizing (distributed) computer systems.\nAn important goal of distributed systems is to separate applications from\nunderlying platforms by providing a middleware layer. Adopting such a layer\nis an important architectural decision, and its main purpose is to provide\ndistribution transparency. However, trade-offs need to be made to achieve\ntransparency, which has led to various techniques to adjust the middleware\nto the needs of the applications that make use of it. We discuss some of the\nmore commonly applied techniques, as they affect the organization of the\nmiddleware itself.\nThe actual realization of a distributed system requires that we instantiate\nand place software components on real machines. There are many different\nchoices that can be made in doing so. The \ufb01nal instantiation of a software\narchitecture is also referred to as a system architecture . In this chapter we\nwill look into traditional centralized architectures in which a single server\nimplements most of the software components (and thus functionality), while\nremote clients can access that server using simple communication means. In\naddition, we consider decentralized peer-to-peer architectures in which all\nnodes more or less play equal roles. Many real-world distributed systems are\n55\nChapter 2\nArchitectures\nDistributed systems are often complex pieces of software of which the compo-\nnents are by de\ufb01nition dispersed across multiple machines. To master their\ncomplexity, it is crucial that these systems are properly organized. There are\ndifferent ways on how to view the organization of a distributed system, but\nan obvious one is to make a distinction between, on the one hand, the logical\norganization of the collection of software components, and on the other hand\nthe actual physical realization.\nThe organization of distributed systems is mostly about the software\ncomponents that constitute the system. These software architectures tell us\nhow the various software components are to be organized and how they\nshould interact. In this chapter we will \ufb01rst pay attention to some commonly\napplied architectural styles toward organizing (distributed) computer systems.\nAn important goal of distributed systems is to separate applications from\nunderlying platforms by providing a middleware layer. Adopting such a layer\nis an important architectural decision, and its main purpose is to provide\ndistribution transparency. However, trade-offs need to be made to achieve\ntransparency, which has led to various techniques to adjust the middleware\nto the needs of the applications that make use of it. We discuss some of the\nmore commonly applied techniques, as they affect the organization of the\nmiddleware itself.\nThe actual realization of a distributed system requires that we instantiate\nand place software components on real machines. There are many different\nchoices that can be made in doing so. The \ufb01nal instantiation of a software\narchitecture is also referred to as a system architecture . In this chapter we\nwill look into traditional centralized architectures in which a single server\nimplements most of the software components (and thus functionality), while\nremote clients can access that server using simple communication means. In\naddition, we consider decentralized peer-to-peer architectures in which all\nnodes more or less play equal roles. Many real-world distributed systems are\n55", "56 CHAPTER 2. ARCHITECTURES\noften organized in a hybrid fashion, combining elements from centralized and\ndecentralized architectures. We discuss a few typical examples.\nWe end this chapter by considering the organization of two widely applied\ndistributed systems: the NFS \ufb01le-sharing system and the Web.\n2.1 Architectural styles\nWe start our discussion on architectures by \ufb01rst considering the logical organi-\nzation of a distributed system into software components, also referred to as its\nsoftware architecture [Bass et al., 2003]. Research on software architectures\nhas matured considerably and it is now commonly accepted that designing\nor adopting an architecture is crucial for the successful development of large\nsoftware systems.\nFor our discussion, the notion of an architectural style is important. Such\na style is formulated in terms of components, the way that components are\nconnected to each other, the data exchanged between components, and \ufb01nally\nhow these elements are jointly con\ufb01gured into a system. A component is\na modular unit with well-de\ufb01ned required and provided interfaces that is\nreplaceable within its environment [OMG, 2004b]. That a component can be\nreplaced, and, in particular, while a system continues to operate, is important.\nThis is due to the fact that in many cases, it is not an option to shut down\na system for maintenance. At best, only parts of it may be put temporarily\nout of order. Replacing a component can be done only if its interfaces remain\nuntouched.\nA somewhat more dif\ufb01cult concept to grasp is that of a connector , which\nis generally described as a mechanism that mediates communication, coor-\ndination, or cooperation among components [Mehta et al., 2000; Shaw and\nClements, 1997]. For example, a connector can be formed by the facilities for\n(remote) procedure calls, message passing, or streaming data. In other words,\na connector allows for the \ufb02ow of control and data between components.\nUsing components and connectors, we can come to various con\ufb01gurations,\nwhich, in turn, have been classi\ufb01ed into architectural styles. Several styles\nhave by now been identi\ufb01ed, of which the most important ones for distributed\nsystems are:\n\u2022 Layered architectures\n\u2022 Object-based architectures\n\u2022 Resource-centered architectures\n\u2022 Event-based architectures\nIn the following, we discuss each of these styles separately. We note in\nadvance that in most real-world distributed systems, many different styles are\ncombined. Notably following an approach by which a system is subdivided\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n56 CHAPTER 2. ARCHITECTURES\noften organized in a hybrid fashion, combining elements from centralized and\ndecentralized architectures. We discuss a few typical examples.\nWe end this chapter by considering the organization of two widely applied\ndistributed systems: the NFS \ufb01le-sharing system and the Web.\n2.1 Architectural styles\nWe start our discussion on architectures by \ufb01rst considering the logical organi-\nzation of a distributed system into software components, also referred to as its\nsoftware architecture [Bass et al., 2003]. Research on software architectures\nhas matured considerably and it is now commonly accepted that designing\nor adopting an architecture is crucial for the successful development of large\nsoftware systems.\nFor our discussion, the notion of an architectural style is important. Such\na style is formulated in terms of components, the way that components are\nconnected to each other, the data exchanged between components, and \ufb01nally\nhow these elements are jointly con\ufb01gured into a system. A component is\na modular unit with well-de\ufb01ned required and provided interfaces that is\nreplaceable within its environment [OMG, 2004b]. That a component can be\nreplaced, and, in particular, while a system continues to operate, is important.\nThis is due to the fact that in many cases, it is not an option to shut down\na system for maintenance. At best, only parts of it may be put temporarily\nout of order. Replacing a component can be done only if its interfaces remain\nuntouched.\nA somewhat more dif\ufb01cult concept to grasp is that of a connector , which\nis generally described as a mechanism that mediates communication, coor-\ndination, or cooperation among components [Mehta et al., 2000; Shaw and\nClements, 1997]. For example, a connector can be formed by the facilities for\n(remote) procedure calls, message passing, or streaming data. In other words,\na connector allows for the \ufb02ow of control and data between components.\nUsing components and connectors, we can come to various con\ufb01gurations,\nwhich, in turn, have been classi\ufb01ed into architectural styles. Several styles\nhave by now been identi\ufb01ed, of which the most important ones for distributed\nsystems are:\n\u2022 Layered architectures\n\u2022 Object-based architectures\n\u2022 Resource-centered architectures\n\u2022 Event-based architectures\nIn the following, we discuss each of these styles separately. We note in\nadvance that in most real-world distributed systems, many different styles are\ncombined. Notably following an approach by which a system is subdivided\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 57\ninto several (logical) layers is such a universal principle that it is generally\ncombined with most other architectural styles.\nLayered architectures\nThe basic idea for the layered style is simple: components are organized in\nalayered fashion where a component at layer Ljcan make a downcall to\na component at a lower-level layer Li(with i<j) and generally expects a\nresponse. Only in exceptional cases will an upcall be made to a higher-level\ncomponent. The three common cases are shown in Figure 2.1.\n(a) (b) (c)\nFigure 2.1: (a) Pure layered organization. (b) Mixed layered organization.\n(c) Layered organization with upcalls (adopted from [Krakowiak, 2009]).\nFigure 2.1(a) shows a standard organization in which only downcalls to\nthe next lower layer are made. This organization is commonly deployed in\nthe case of network communication.\nIn many situations we also encounter the organization shown in Fig-\nure 2.1(b). Consider, for example, an application Athat makes use of a library\nLOSto interface to an operating system. At the same time, the application uses\na specialized mathematical library Lmath that has been implemented by also\nmaking use of LOS. In this case, referring to Figure 2.1(b), Ais implemented at\nlayer N\u00001,Lmath at layer N\u00002, and LOSwhich is common to both of them,\nat layer N\u00003.\nFinally, a special situation is shown in Figure 2.1(c). In some cases, it\nis convenient to have a lower layer do an upcall to its next higher layer. A\ntypical example is when an operating system signals the occurrence of an\nevent, to which end it calls a user-de\ufb01ned operation for which an application\nhad previously passed a reference (typically referred to as a handle ).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 57\ninto several (logical) layers is such a universal principle that it is generally\ncombined with most other architectural styles.\nLayered architectures\nThe basic idea for the layered style is simple: components are organized in\nalayered fashion where a component at layer Ljcan make a downcall to\na component at a lower-level layer Li(with i<j) and generally expects a\nresponse. Only in exceptional cases will an upcall be made to a higher-level\ncomponent. The three common cases are shown in Figure 2.1.\n(a) (b) (c)\nFigure 2.1: (a) Pure layered organization. (b) Mixed layered organization.\n(c) Layered organization with upcalls (adopted from [Krakowiak, 2009]).\nFigure 2.1(a) shows a standard organization in which only downcalls to\nthe next lower layer are made. This organization is commonly deployed in\nthe case of network communication.\nIn many situations we also encounter the organization shown in Fig-\nure 2.1(b). Consider, for example, an application Athat makes use of a library\nLOSto interface to an operating system. At the same time, the application uses\na specialized mathematical library Lmath that has been implemented by also\nmaking use of LOS. In this case, referring to Figure 2.1(b), Ais implemented at\nlayer N\u00001,Lmath at layer N\u00002, and LOSwhich is common to both of them,\nat layer N\u00003.\nFinally, a special situation is shown in Figure 2.1(c). In some cases, it\nis convenient to have a lower layer do an upcall to its next higher layer. A\ntypical example is when an operating system signals the occurrence of an\nevent, to which end it calls a user-de\ufb01ned operation for which an application\nhad previously passed a reference (typically referred to as a handle ).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "58 CHAPTER 2. ARCHITECTURES\nFigure 2.2: A layered communication-protocol stack, showing the difference\nbetween a service, its interface, and the protocol it deploys.\nLayered communication protocols\nA well-known and ubiquitously applied layered architecture is that of so-\ncalled communication-protocol stacks . We will concentrate here on the global\npicture only and defer a detailed discussion to Section 4.1.\nIn communication-protocol stacks, each layer implements one or several\ncommunication services allowing data to be sent from a destination to one\nor several targets. To this end, each layer offers an interface specifying the\nfunctions that can be called. In principle, the interface should completely\nhide the actual implementation of a service. Another important concept\nin the case of communication is that of a (communication) protocol , which\ndescribes the rules that parties will follow in order to exchange information.\nIt is important to understand the difference between a service offered by a\nlayer, the interface by which that service is made available, and the protocol\nthat a layer implements to establish communication. This distinction is shown\nin Figure 2.2.\nTo make this distinction clear, consider a reliable, connection-oriented\nservice, which is provided by many communication systems. In this case,\na communicating party \ufb01rst needs to set up a connection to another party\nbefore the two can send and receive messages. Being reliable means that\nstrong guarantees will be given that sent messages will indeed be delivered to\nthe other side, even when there is a high risk that messages may be lost (as,\nfor example, may be the case when using a wireless medium). In addition,\nsuch services generally also ensure that messages are delivered in the same\norder as that they were sent.\nThis kind of service is realized in the Internet by means of the Transmis-\nsion Control Protocol (TCP ). The protocol speci\ufb01es which messages are to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n58 CHAPTER 2. ARCHITECTURES\nFigure 2.2: A layered communication-protocol stack, showing the difference\nbetween a service, its interface, and the protocol it deploys.\nLayered communication protocols\nA well-known and ubiquitously applied layered architecture is that of so-\ncalled communication-protocol stacks . We will concentrate here on the global\npicture only and defer a detailed discussion to Section 4.1.\nIn communication-protocol stacks, each layer implements one or several\ncommunication services allowing data to be sent from a destination to one\nor several targets. To this end, each layer offers an interface specifying the\nfunctions that can be called. In principle, the interface should completely\nhide the actual implementation of a service. Another important concept\nin the case of communication is that of a (communication) protocol , which\ndescribes the rules that parties will follow in order to exchange information.\nIt is important to understand the difference between a service offered by a\nlayer, the interface by which that service is made available, and the protocol\nthat a layer implements to establish communication. This distinction is shown\nin Figure 2.2.\nTo make this distinction clear, consider a reliable, connection-oriented\nservice, which is provided by many communication systems. In this case,\na communicating party \ufb01rst needs to set up a connection to another party\nbefore the two can send and receive messages. Being reliable means that\nstrong guarantees will be given that sent messages will indeed be delivered to\nthe other side, even when there is a high risk that messages may be lost (as,\nfor example, may be the case when using a wireless medium). In addition,\nsuch services generally also ensure that messages are delivered in the same\norder as that they were sent.\nThis kind of service is realized in the Internet by means of the Transmis-\nsion Control Protocol (TCP ). The protocol speci\ufb01es which messages are to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 59\nbe exchanged for setting up or tearing down a connection, what needs to\nbe done to preserve the ordering of transferred data, and what both parties\nneed to do to detect and correct data that was lost during transmission. The\nservice is made available in the form of a relatively simple programming\ninterface, containing calls to set up a connection, send and receive messages,\nand to tear down the connection again. In fact, there are different interfaces\navailable, often dependent on operating system or programming language\nused. Likewise, there are many different implementations of the protocol and\nits interfaces. (All of the gory details can be found in [Stevens, 1994; Wright\nand Stevens, 1995].)\nNote 2.1 (Example: Two communicating parties)\nTo make this distinction between service, interface, and protocol more concrete,\nconsider the following two communicating parties, also known as a client and a\nserver , respectively, expressed in Python (note that some code has been removed\nfor clarity).\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3(conn, addr) = s.accept() # returns new socket and addr. client\n4while True: # forever\n5 data = conn.recv(1024) # receive data from client\n6 if not data: break # stop if client stopped\n7 conn.send( str(data)+\"*\") # return sent data plus an \"*\"\n8conn.close() # close the connection\n(a) A simple server\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.connect((HOST, PORT)) # connect to server (block until accepted)\n4s.send(\u2019Hello, world\u2019) # send some data\n5data = s.recv(1024) # receive the response\n6print data # print the result\n7s.close() # close the connection\n(b) A client\nFigure 2.3: Two communicating parties.\nIn this example, a server is created that makes use of a connection-oriented\nservice as offered by the socket library available in Python. This service allows\ntwo communicating parties to reliably send and receive data over a connection.\nThe main functions available in its interface are:\n\u2022socket() : to create an object representing the connection\n\u2022accept() : a blocking call to wait for incoming connection requests; if\nsuccessful, the call returns a new socket for a separate connection\n\u2022connect() : to set up a connection to a speci\ufb01ed party\n\u2022close() : to tear down a connection\n\u2022send() ,recv() : to send and receive data over a connection, respectively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 59\nbe exchanged for setting up or tearing down a connection, what needs to\nbe done to preserve the ordering of transferred data, and what both parties\nneed to do to detect and correct data that was lost during transmission. The\nservice is made available in the form of a relatively simple programming\ninterface, containing calls to set up a connection, send and receive messages,\nand to tear down the connection again. In fact, there are different interfaces\navailable, often dependent on operating system or programming language\nused. Likewise, there are many different implementations of the protocol and\nits interfaces. (All of the gory details can be found in [Stevens, 1994; Wright\nand Stevens, 1995].)\nNote 2.1 (Example: Two communicating parties)\nTo make this distinction between service, interface, and protocol more concrete,\nconsider the following two communicating parties, also known as a client and a\nserver , respectively, expressed in Python (note that some code has been removed\nfor clarity).\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3(conn, addr) = s.accept() # returns new socket and addr. client\n4while True: # forever\n5 data = conn.recv(1024) # receive data from client\n6 if not data: break # stop if client stopped\n7 conn.send( str(data)+\"*\") # return sent data plus an \"*\"\n8conn.close() # close the connection\n(a) A simple server\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.connect((HOST, PORT)) # connect to server (block until accepted)\n4s.send(\u2019Hello, world\u2019) # send some data\n5data = s.recv(1024) # receive the response\n6print data # print the result\n7s.close() # close the connection\n(b) A client\nFigure 2.3: Two communicating parties.\nIn this example, a server is created that makes use of a connection-oriented\nservice as offered by the socket library available in Python. This service allows\ntwo communicating parties to reliably send and receive data over a connection.\nThe main functions available in its interface are:\n\u2022socket() : to create an object representing the connection\n\u2022accept() : a blocking call to wait for incoming connection requests; if\nsuccessful, the call returns a new socket for a separate connection\n\u2022connect() : to set up a connection to a speci\ufb01ed party\n\u2022close() : to tear down a connection\n\u2022send() ,recv() : to send and receive data over a connection, respectively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "60 CHAPTER 2. ARCHITECTURES\nThe combination of constants AF_INET and SOCK_STREAM is used to specify that\nthe TCP protocol should be used in the communication between the two parties.\nThese two constants can be seen as part of the interface, whereas making use of\nTCP is part of the offered service. How TCP is implemented, or for that matter any\npart of the communication service is hidden completely from the applications.\nFinally, also note that these two programs implicitly adhere to an application-\nlevel protocol: apparently, if the client sends some data, the server will return it.\nIndeed, it operates as an echo server where the server adds an asterisk to the data\nsent by the client.\nApplication layering\nLet us now turn our attention to the logical layering of applications. Consider-\ning that a large class of distributed applications is targeted toward supporting\nuser or application access to databases, many people have advocated a distinc-\ntion between three logical levels, essentially following a layered architectural\nstyle:\n\u2022 The application-interface level\n\u2022 The processing level\n\u2022 The data level\nIn line with this layering, we see that applications can often be constructed\nfrom roughly three different pieces: a part that handles interaction with a\nuser or some external application, a part that operates on a database or \ufb01le\nsystem, and a middle part that generally contains the core functionality of the\napplication. This middle part is logically placed at the processing level. In\ncontrast to user interfaces and databases, there are not many aspects common\nto the processing level. Therefore, we shall give a number of examples to\nmake this level clearer.\nAs a \ufb01rst example, consider an Internet search engine. Ignoring all the ani-\nmated banners, images, and other fancy window dressing, the user interface\nof a search engine can be very simple: a user types in a string of keywords\nand is subsequently presented with a list of titles of Web pages. The back\nend is formed by a huge database of Web pages that have been prefetched\nand indexed. The core of the search engine is a program that transforms the\nuser\u2019s string of keywords into one or more database queries. It subsequently\nranks the results into a list, and transforms that list into a series of HTML\npages. This information retrieval part is typically placed at the processing\nlevel. Figure 2.4 shows this organization.\nAs a second example, consider a decision support system for stock bro-\nkerage. Analogous to a search engine, such a system can be divided into the\nfollowing three layers:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n60 CHAPTER 2. ARCHITECTURES\nThe combination of constants AF_INET and SOCK_STREAM is used to specify that\nthe TCP protocol should be used in the communication between the two parties.\nThese two constants can be seen as part of the interface, whereas making use of\nTCP is part of the offered service. How TCP is implemented, or for that matter any\npart of the communication service is hidden completely from the applications.\nFinally, also note that these two programs implicitly adhere to an application-\nlevel protocol: apparently, if the client sends some data, the server will return it.\nIndeed, it operates as an echo server where the server adds an asterisk to the data\nsent by the client.\nApplication layering\nLet us now turn our attention to the logical layering of applications. Consider-\ning that a large class of distributed applications is targeted toward supporting\nuser or application access to databases, many people have advocated a distinc-\ntion between three logical levels, essentially following a layered architectural\nstyle:\n\u2022 The application-interface level\n\u2022 The processing level\n\u2022 The data level\nIn line with this layering, we see that applications can often be constructed\nfrom roughly three different pieces: a part that handles interaction with a\nuser or some external application, a part that operates on a database or \ufb01le\nsystem, and a middle part that generally contains the core functionality of the\napplication. This middle part is logically placed at the processing level. In\ncontrast to user interfaces and databases, there are not many aspects common\nto the processing level. Therefore, we shall give a number of examples to\nmake this level clearer.\nAs a \ufb01rst example, consider an Internet search engine. Ignoring all the ani-\nmated banners, images, and other fancy window dressing, the user interface\nof a search engine can be very simple: a user types in a string of keywords\nand is subsequently presented with a list of titles of Web pages. The back\nend is formed by a huge database of Web pages that have been prefetched\nand indexed. The core of the search engine is a program that transforms the\nuser\u2019s string of keywords into one or more database queries. It subsequently\nranks the results into a list, and transforms that list into a series of HTML\npages. This information retrieval part is typically placed at the processing\nlevel. Figure 2.4 shows this organization.\nAs a second example, consider a decision support system for stock bro-\nkerage. Analogous to a search engine, such a system can be divided into the\nfollowing three layers:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 61\nFigure 2.4: The simpli\ufb01ed organization of an Internet search engine into three\ndifferent layers.\n\u2022A front end implementing the user interface or offering a programming\ninterface to external applications\n\u2022 A back end for accessing a database with the \ufb01nancial data\n\u2022 The analysis programs between these two.\nAnalysis of \ufb01nancial data may require sophisticated methods and tech-\nniques from statistics and arti\ufb01cial intelligence. In some cases, the core of\na \ufb01nancial decision support system may even need to be executed on high-\nperformance computers in order to achieve the throughput and responsiveness\nthat is expected from its users.\nAs a last example, consider a typical desktop package, consisting of a word\nprocessor, a spreadsheet application, communication facilities, and so on. Such\n\u201cof\ufb01ce\u201d suites are generally integrated through a common user interface that\nsupports integrated document management, and operates on \ufb01les from the\nuser\u2019s home directory. (In an of\ufb01ce environment, this home directory is often\nplaced on a remote \ufb01le server.) In this example, the processing level consists of\na relatively large collection of programs, each having rather simple processing\ncapabilities.\nThe data level contains the programs that maintain the actual data on\nwhich the applications operate. An important property of this level is that\ndata are often persistent , that is, even if no application is running, data will\nbe stored somewhere for next use. In its simplest form, the data level consists\nof a \ufb01le system, but it is also common to use a full-\ufb02edged database.\nBesides merely storing data, the data level is generally also responsible\nfor keeping data consistent across different applications. When databases\nare being used, maintaining consistency means that metadata such as table\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 61\nFigure 2.4: The simpli\ufb01ed organization of an Internet search engine into three\ndifferent layers.\n\u2022A front end implementing the user interface or offering a programming\ninterface to external applications\n\u2022 A back end for accessing a database with the \ufb01nancial data\n\u2022 The analysis programs between these two.\nAnalysis of \ufb01nancial data may require sophisticated methods and tech-\nniques from statistics and arti\ufb01cial intelligence. In some cases, the core of\na \ufb01nancial decision support system may even need to be executed on high-\nperformance computers in order to achieve the throughput and responsiveness\nthat is expected from its users.\nAs a last example, consider a typical desktop package, consisting of a word\nprocessor, a spreadsheet application, communication facilities, and so on. Such\n\u201cof\ufb01ce\u201d suites are generally integrated through a common user interface that\nsupports integrated document management, and operates on \ufb01les from the\nuser\u2019s home directory. (In an of\ufb01ce environment, this home directory is often\nplaced on a remote \ufb01le server.) In this example, the processing level consists of\na relatively large collection of programs, each having rather simple processing\ncapabilities.\nThe data level contains the programs that maintain the actual data on\nwhich the applications operate. An important property of this level is that\ndata are often persistent , that is, even if no application is running, data will\nbe stored somewhere for next use. In its simplest form, the data level consists\nof a \ufb01le system, but it is also common to use a full-\ufb02edged database.\nBesides merely storing data, the data level is generally also responsible\nfor keeping data consistent across different applications. When databases\nare being used, maintaining consistency means that metadata such as table\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "62 CHAPTER 2. ARCHITECTURES\ndescriptions, entry constraints and application-speci\ufb01c metadata are also\nstored at this level. For example, in the case of a bank, we may want to\ngenerate a noti\ufb01cation when a customer\u2019s credit card debt reaches a certain\nvalue. This type of information can be maintained through a database trigger\nthat activates a handler for that trigger at the appropriate moment.\nObject-based and service-oriented architectures\nA far more loose organization is followed in object-based architectures ,\nshown in Figure 2.5. In essence, each object corresponds to what we have\nde\ufb01ned as a component, and these components are connected through a\nprocedure call mechanism. In the case of distributed systems, a procedure\ncall can also take place over a network, that is, the calling object need not be\nexecuted on the same machine as the called object.\nFigure 2.5: An object-based architectural style.\nObject-based architectures are attractive because they provide a natural\nway of encapsulating data (called an object\u2019s state ) and the operations that can\nbe performed on that data (which are referred to as an object\u2019s methods ) into\na single entity. The interface offered by an object conceals implementation\ndetails, essentially meaning that we, in principle, can consider an object\ncompletely independent of its environment. As with components, this also\nmeans that if the interface is clearly de\ufb01ned and left otherwise untouched, an\nobject should be replaceable with one having exactly the same interface.\nThis separation between interfaces and the objects implementing these\ninterfaces allows us to place an interface at one machine, while the object itself\nresides on another machine. This organization, which is shown in Figure 2.6\nis commonly referred to as a distributed object .\nWhen a client binds to a distributed object, an implementation of the\nobject\u2019s interface, called a proxy , is then loaded into the client\u2019s address space.\nA proxy is analogous to a client stub in RPC systems. The only thing it does\nis marshal method invocations into messages and unmarshal reply messages\nto return the result of the method invocation to the client. The actual object\nresides at a server machine, where it offers the same interface as it does on the\nclient machine. Incoming invocation requests are \ufb01rst passed to a server stub,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n62 CHAPTER 2. ARCHITECTURES\ndescriptions, entry constraints and application-speci\ufb01c metadata are also\nstored at this level. For example, in the case of a bank, we may want to\ngenerate a noti\ufb01cation when a customer\u2019s credit card debt reaches a certain\nvalue. This type of information can be maintained through a database trigger\nthat activates a handler for that trigger at the appropriate moment.\nObject-based and service-oriented architectures\nA far more loose organization is followed in object-based architectures ,\nshown in Figure 2.5. In essence, each object corresponds to what we have\nde\ufb01ned as a component, and these components are connected through a\nprocedure call mechanism. In the case of distributed systems, a procedure\ncall can also take place over a network, that is, the calling object need not be\nexecuted on the same machine as the called object.\nFigure 2.5: An object-based architectural style.\nObject-based architectures are attractive because they provide a natural\nway of encapsulating data (called an object\u2019s state ) and the operations that can\nbe performed on that data (which are referred to as an object\u2019s methods ) into\na single entity. The interface offered by an object conceals implementation\ndetails, essentially meaning that we, in principle, can consider an object\ncompletely independent of its environment. As with components, this also\nmeans that if the interface is clearly de\ufb01ned and left otherwise untouched, an\nobject should be replaceable with one having exactly the same interface.\nThis separation between interfaces and the objects implementing these\ninterfaces allows us to place an interface at one machine, while the object itself\nresides on another machine. This organization, which is shown in Figure 2.6\nis commonly referred to as a distributed object .\nWhen a client binds to a distributed object, an implementation of the\nobject\u2019s interface, called a proxy , is then loaded into the client\u2019s address space.\nA proxy is analogous to a client stub in RPC systems. The only thing it does\nis marshal method invocations into messages and unmarshal reply messages\nto return the result of the method invocation to the client. The actual object\nresides at a server machine, where it offers the same interface as it does on the\nclient machine. Incoming invocation requests are \ufb01rst passed to a server stub,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 63\nFigure 2.6: Common organization of a remote object with client-side proxy.\nwhich unmarshals them to make method invocations at the object\u2019s interface\nat the server. The server stub is also responsible for marshaling replies and\nforwarding reply messages to the client-side proxy.\nThe server-side stub is often referred to as a skeleton as it provides the\nbare means for letting the server middleware access the user-de\ufb01ned objects.\nIn practice, it often contains incomplete code in the form of a language-speci\ufb01c\nclass that needs to be further specialized by the developer.\nA characteristic, but somewhat counterintuitive feature of most distributed\nobjects is that their state is notdistributed: it resides at a single machine.\nOnly the interfaces implemented by the object are made available on other\nmachines. Such objects are also referred to as remote objects . In a general\ndistributed object, the state itself may be physically distributed across multiple\nmachines, but this distribution is also hidden from clients behind the object\u2019s\ninterfaces.\nOne could argue that object-based architectures form the foundation of\nencapsulating services into independent units. Encapsulation is the keyword\nhere: the service as a whole is realized as a self-contained entity, although it\ncan possibly make use of other services. By clearly separating various services\nsuch that they can operate independently, we are paving the road toward\nservice-oriented architectures , generally abbreviated as SOA s.\nIn a service-oriented architecture, a distributed application or system is\nessentially constructed as a composition of many different services. Not all\nof these services may belong to the same administrative organization. We\nalready came across this phenomenon when discussing cloud computing: it\nmay very well be that an organization running its business application makes\nuse of storage services offered by a cloud provider. These storage services are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 63\nFigure 2.6: Common organization of a remote object with client-side proxy.\nwhich unmarshals them to make method invocations at the object\u2019s interface\nat the server. The server stub is also responsible for marshaling replies and\nforwarding reply messages to the client-side proxy.\nThe server-side stub is often referred to as a skeleton as it provides the\nbare means for letting the server middleware access the user-de\ufb01ned objects.\nIn practice, it often contains incomplete code in the form of a language-speci\ufb01c\nclass that needs to be further specialized by the developer.\nA characteristic, but somewhat counterintuitive feature of most distributed\nobjects is that their state is notdistributed: it resides at a single machine.\nOnly the interfaces implemented by the object are made available on other\nmachines. Such objects are also referred to as remote objects . In a general\ndistributed object, the state itself may be physically distributed across multiple\nmachines, but this distribution is also hidden from clients behind the object\u2019s\ninterfaces.\nOne could argue that object-based architectures form the foundation of\nencapsulating services into independent units. Encapsulation is the keyword\nhere: the service as a whole is realized as a self-contained entity, although it\ncan possibly make use of other services. By clearly separating various services\nsuch that they can operate independently, we are paving the road toward\nservice-oriented architectures , generally abbreviated as SOA s.\nIn a service-oriented architecture, a distributed application or system is\nessentially constructed as a composition of many different services. Not all\nof these services may belong to the same administrative organization. We\nalready came across this phenomenon when discussing cloud computing: it\nmay very well be that an organization running its business application makes\nuse of storage services offered by a cloud provider. These storage services are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "64 CHAPTER 2. ARCHITECTURES\nlogically completely encapsulated into a single unit, of which an interface is\nmade available to customers.\nOf course, storage is a rather basic service, but more sophisticated situa-\ntions easily come to mind. Consider, for example, a Web shop selling goods\nsuch as e-books. A simple implementation following the application layering\nwe discussed previously, may consist of an application for processing orders,\nwhich, in turn, operates on a local database containing the e-books. Order\nprocessing typically involves selecting items, registering and checking the\ndelivery channel (perhaps by making use of e-mail), but also making sure\nthat a payment takes place. The latter can be handled by a separate service,\nrun by a different organization, to which a purchasing customer is redirected\nfor the payment, after which the e-book organization is noti\ufb01ed so that it can\ncomplete the transaction.\nIn this way, we see that the problem of developing a distributed system is\npartly one of service composition , and making sure that those services operate\nin harmony. Indeed, this problem is completely analogous to the enterprise\napplication integration issues discussed in Section 1.3. Crucial is, and remains,\nthat each service offers a well-de\ufb01ned (programming) interface. In practice,\nthis also means that each service offers its own interface, in turn, possibly\nmaking the composition of services far from trivial.\nResource-based architectures\nAs an increasing number of services became available over the Web and the\ndevelopment of distributed systems through service composition became more\nimportant, researchers started to rethink the architecture of mostly Web-based\ndistributed systems. One of the problems with service composition is that\nconnecting various components can easily turn into an integration nightmare.\nAs an alternative, one can also view a distributed system as a huge collec-\ntion of resources that are individually managed by components. Resources\nmay be added or removed by (remote) applications, and likewise can be\nretrieved or modi\ufb01ed. This approach has now been widely adopted for\nthe Web and is known as Representational State Transfer (REST ) [Fielding,\n2000]. There are four key characteristics of what are known as RESTful\narchitectures [Pautasso et al., 2008]:\n1. Resources are identi\ufb01ed through a single naming scheme\n2.All services offer the same interface, consisting of at most four operations,\nas shown in Figure 2.7\n3. Messages sent to or from a service are fully self-described\n4.After executing an operation at a service, that component forgets every-\nthing about the caller\nThe last property is also referred to as a stateless execution , a concept to\nwhich we return in Section 3.4.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n64 CHAPTER 2. ARCHITECTURES\nlogically completely encapsulated into a single unit, of which an interface is\nmade available to customers.\nOf course, storage is a rather basic service, but more sophisticated situa-\ntions easily come to mind. Consider, for example, a Web shop selling goods\nsuch as e-books. A simple implementation following the application layering\nwe discussed previously, may consist of an application for processing orders,\nwhich, in turn, operates on a local database containing the e-books. Order\nprocessing typically involves selecting items, registering and checking the\ndelivery channel (perhaps by making use of e-mail), but also making sure\nthat a payment takes place. The latter can be handled by a separate service,\nrun by a different organization, to which a purchasing customer is redirected\nfor the payment, after which the e-book organization is noti\ufb01ed so that it can\ncomplete the transaction.\nIn this way, we see that the problem of developing a distributed system is\npartly one of service composition , and making sure that those services operate\nin harmony. Indeed, this problem is completely analogous to the enterprise\napplication integration issues discussed in Section 1.3. Crucial is, and remains,\nthat each service offers a well-de\ufb01ned (programming) interface. In practice,\nthis also means that each service offers its own interface, in turn, possibly\nmaking the composition of services far from trivial.\nResource-based architectures\nAs an increasing number of services became available over the Web and the\ndevelopment of distributed systems through service composition became more\nimportant, researchers started to rethink the architecture of mostly Web-based\ndistributed systems. One of the problems with service composition is that\nconnecting various components can easily turn into an integration nightmare.\nAs an alternative, one can also view a distributed system as a huge collec-\ntion of resources that are individually managed by components. Resources\nmay be added or removed by (remote) applications, and likewise can be\nretrieved or modi\ufb01ed. This approach has now been widely adopted for\nthe Web and is known as Representational State Transfer (REST ) [Fielding,\n2000]. There are four key characteristics of what are known as RESTful\narchitectures [Pautasso et al., 2008]:\n1. Resources are identi\ufb01ed through a single naming scheme\n2.All services offer the same interface, consisting of at most four operations,\nas shown in Figure 2.7\n3. Messages sent to or from a service are fully self-described\n4.After executing an operation at a service, that component forgets every-\nthing about the caller\nThe last property is also referred to as a stateless execution , a concept to\nwhich we return in Section 3.4.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 65\nOperation Description\nPUT Create a new resource\nGET Retrieve the state of a resource in some representation\nDELETE Delete a resource\nPOST Modify a resource by transferring a new state\nFigure 2.7: The four operations available in RESTful architectures.\nTo illustrate how RESTful can work in practice, consider a cloud storage\nservice, such as Amazon\u2019s Simple Storage Service (Amazon S3 ). Amazon\nS3, described in [Murty, 2008] supports only two resources: objects , which\nare essentially the equivalent of \ufb01les, and buckets , the equivalent of directo-\nries. There is no concept of placing buckets into buckets. An object named\nObjectName contained in bucket BucketName is referred to by means of the\nfollowing Uniform Resource Identi\ufb01er (URI ):\nhttp://BucketName.s3.amazonaws.com/ObjectName\nTo create a bucket, or an object for that matter, an application would essentially\nsend a PUTrequest with the URI of the bucket/object. In principle, the protocol\nthat is used with the service is HTTP . In other words, it is just another HTTP\nrequest, which will subsequently be correctly interpreted by S3. If the bucket\nor object already exists, an HTTP error message is returned.\nIn a similar fashion, to know which objects are contained in a bucket, an\napplication would send a GETrequest with the URI of that bucket. S3 will\nreturn a list of object names, again as an ordinary HTTP response.\nThe RESTful architecture has become popular because of its simplicity.\nHowever, holy wars are being fought over whether RESTful services are\nbetter than where services are speci\ufb01ed by means of service-speci\ufb01c interfaces.\nPautasso et al. [2008] have compared the two approaches, and, as to be\nexpected, they both have their advantages and disadvantages. In particular,\nthe simplicity of RESTful architectures can easily prohibit easy solutions\nto intricate communication schemes. One example is where distributed\ntransactions are needed, which generally requires that services keep track of\nthe state of execution. On the other hand, there are many examples in which\nRESTful architectures perfectly match a simple integration scheme of services,\nyet where the myriad of service interfaces will complicate matters.\nNote 2.2 (Advanced: On interfaces)\nClearly, a service cannot be made easier or more dif\ufb01cult just because of the\nparticular interface it offers. A service offers functionality, and at best the way\nthat the service is accessed is determined by the interface. Indeed, one could\nargue that the discussion on RESTful versus service-speci\ufb01c interfaces is much\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 65\nOperation Description\nPUT Create a new resource\nGET Retrieve the state of a resource in some representation\nDELETE Delete a resource\nPOST Modify a resource by transferring a new state\nFigure 2.7: The four operations available in RESTful architectures.\nTo illustrate how RESTful can work in practice, consider a cloud storage\nservice, such as Amazon\u2019s Simple Storage Service (Amazon S3 ). Amazon\nS3, described in [Murty, 2008] supports only two resources: objects , which\nare essentially the equivalent of \ufb01les, and buckets , the equivalent of directo-\nries. There is no concept of placing buckets into buckets. An object named\nObjectName contained in bucket BucketName is referred to by means of the\nfollowing Uniform Resource Identi\ufb01er (URI ):\nhttp://BucketName.s3.amazonaws.com/ObjectName\nTo create a bucket, or an object for that matter, an application would essentially\nsend a PUTrequest with the URI of the bucket/object. In principle, the protocol\nthat is used with the service is HTTP . In other words, it is just another HTTP\nrequest, which will subsequently be correctly interpreted by S3. If the bucket\nor object already exists, an HTTP error message is returned.\nIn a similar fashion, to know which objects are contained in a bucket, an\napplication would send a GETrequest with the URI of that bucket. S3 will\nreturn a list of object names, again as an ordinary HTTP response.\nThe RESTful architecture has become popular because of its simplicity.\nHowever, holy wars are being fought over whether RESTful services are\nbetter than where services are speci\ufb01ed by means of service-speci\ufb01c interfaces.\nPautasso et al. [2008] have compared the two approaches, and, as to be\nexpected, they both have their advantages and disadvantages. In particular,\nthe simplicity of RESTful architectures can easily prohibit easy solutions\nto intricate communication schemes. One example is where distributed\ntransactions are needed, which generally requires that services keep track of\nthe state of execution. On the other hand, there are many examples in which\nRESTful architectures perfectly match a simple integration scheme of services,\nyet where the myriad of service interfaces will complicate matters.\nNote 2.2 (Advanced: On interfaces)\nClearly, a service cannot be made easier or more dif\ufb01cult just because of the\nparticular interface it offers. A service offers functionality, and at best the way\nthat the service is accessed is determined by the interface. Indeed, one could\nargue that the discussion on RESTful versus service-speci\ufb01c interfaces is much\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "66 CHAPTER 2. ARCHITECTURES\nabout access transparency. To better appreciate why so many people are paying\nattention to this issue, let us zoom in to the Amazon S3 service, which offers a\nREST interface as well as a more traditional interface (referred to as the SOAP\ninterface).\nBucket operations Object operations\nListAllMyBuckets PutObjectInline\nCreateBucket PutObject\nDeleteBucket CopyObject\nListBucket GetObject\nGetBucketAccessControlPolicy GetObjectExtended\nSetBucketAccessControlPolicy DeleteObject\nGetBucketLoggingStatus GetObjectAccessControlPolicy\nSetBucketLoggingStatus SetObjectAccessControlPolicy\nFigure 2.8: The operations in Amazon\u2019s S3 SOAP interface.\nThe SOAP interface consists of approximately 16 operations, listed in Figure 2.8.\nHowever, if we were to access Amazon S3 using the Python boto library, we\nwould have close to 50 operations available. In contrast, the REST interface offers\nonly very few operations, essentially those listed in Figure 2.7. Where do these\ndifferences come from? The answer is, of course, in the parameter space. In the\ncase of RESTful architectures, an application will need to provide all that it wants\nthrough the parameters it passes by one of the operations. In Amazon\u2019s SOAP\ninterface, the number of parameters per operation is generally limited, and this is\ncertainly the case if we were to use the Python boto library.\nSticking to principles (so that we can avoid the intricacies of real code), suppose\nthat we have an interface bucket that offers an operation create , requiring an\ninput string such as mybucket , for creating a bucket with name \u201cmybucket.\u201d\nNormally, the operation would be called roughly as follows:\nimport bucket\nbucket.create(\"mybucket\")\nHowever, in a RESTful architecture, the call would need to be essentially encoded\nas a single string, such as\nPUT \"http://mybucket.s3.amazonsws.com/\"\nThe difference is striking. For example, in the \ufb01rst case, many syntactical errors\ncan often already be caught during compile time, whereas in the second case,\nchecking needs to be deferred until runtime. Secondly, one can argue that\nspecifying the semantics of an operation is much easier with speci\ufb01c interfaces\nthan with ones that offer only generic operations. On the other hand, with generic\noperations, changes are much easier to accommodate, as they would generally\ninvolve changing the layout of strings that encode what is actually required.\nPublish-subscribe architectures\nAs systems continue to grow and processes can more easily join or leave, it\nbecomes important to have an architecture in which dependencies between\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n66 CHAPTER 2. ARCHITECTURES\nabout access transparency. To better appreciate why so many people are paying\nattention to this issue, let us zoom in to the Amazon S3 service, which offers a\nREST interface as well as a more traditional interface (referred to as the SOAP\ninterface).\nBucket operations Object operations\nListAllMyBuckets PutObjectInline\nCreateBucket PutObject\nDeleteBucket CopyObject\nListBucket GetObject\nGetBucketAccessControlPolicy GetObjectExtended\nSetBucketAccessControlPolicy DeleteObject\nGetBucketLoggingStatus GetObjectAccessControlPolicy\nSetBucketLoggingStatus SetObjectAccessControlPolicy\nFigure 2.8: The operations in Amazon\u2019s S3 SOAP interface.\nThe SOAP interface consists of approximately 16 operations, listed in Figure 2.8.\nHowever, if we were to access Amazon S3 using the Python boto library, we\nwould have close to 50 operations available. In contrast, the REST interface offers\nonly very few operations, essentially those listed in Figure 2.7. Where do these\ndifferences come from? The answer is, of course, in the parameter space. In the\ncase of RESTful architectures, an application will need to provide all that it wants\nthrough the parameters it passes by one of the operations. In Amazon\u2019s SOAP\ninterface, the number of parameters per operation is generally limited, and this is\ncertainly the case if we were to use the Python boto library.\nSticking to principles (so that we can avoid the intricacies of real code), suppose\nthat we have an interface bucket that offers an operation create , requiring an\ninput string such as mybucket , for creating a bucket with name \u201cmybucket.\u201d\nNormally, the operation would be called roughly as follows:\nimport bucket\nbucket.create(\"mybucket\")\nHowever, in a RESTful architecture, the call would need to be essentially encoded\nas a single string, such as\nPUT \"http://mybucket.s3.amazonsws.com/\"\nThe difference is striking. For example, in the \ufb01rst case, many syntactical errors\ncan often already be caught during compile time, whereas in the second case,\nchecking needs to be deferred until runtime. Secondly, one can argue that\nspecifying the semantics of an operation is much easier with speci\ufb01c interfaces\nthan with ones that offer only generic operations. On the other hand, with generic\noperations, changes are much easier to accommodate, as they would generally\ninvolve changing the layout of strings that encode what is actually required.\nPublish-subscribe architectures\nAs systems continue to grow and processes can more easily join or leave, it\nbecomes important to have an architecture in which dependencies between\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 67\nprocesses become as loose as possible. A large class of distributed systems\nhave adopted an architecture in which there is a strong separation between\nprocessing and coordination . The idea is to view a system as a collection of\nautonomously operating processes. In this model, coordination encompasses\nthe communication and cooperation between processes. It forms the glue\nthat binds the activities performed by processes into a whole [Gelernter and\nCarriero, 1992].\nCabri et al. [2000] provide a taxonomy of coordination models that can\nbe applied equally to many types of distributed systems. Slightly adapting\ntheir terminology, we make a distinction between models along two different\ndimensions, temporal and referential, as shown in Figure 2.9.\nTemporally Temporally\ncoupled decoupled\nReferentially Direct Mailbox\ncoupled\nReferentially Event- Shared\ndecoupled based data space\nFigure 2.9: Examples of different forms of coordination.\nWhen processes are temporally and referentially coupled, coordination\ntakes place in a direct way, referred to as direct coordination . The referential\ncoupling generally appears in the form of explicit referencing in communi-\ncation. For example, a process can communicate only if it knows the name\nor identi\ufb01er of the other processes it wants to exchange information with.\nTemporal coupling means that processes that are communicating will both\nhave to be up and running. In real life, talking over cell phones (and assuming\nthat a cell phone has only one owner), is an example of direct communication.\nA different type of coordination occurs when processes are temporally\ndecoupled, but referentially coupled, which we refer to as mailbox coordina-\ntion. In this case, there is no need for two communicating processes to be\nexecuting at the same time in order to let communication take place. Instead,\ncommunication takes place by putting messages in a (possibly shared) mail-\nbox. Because it is necessary to explicitly address the mailbox that will hold\nthe messages that are to be exchanged, there is a referential coupling.\nThe combination of referentially decoupled and temporally coupled sys-\ntems form the group of models for event-based coordination . In referentially\ndecoupled systems, processes do not know each other explicitly. The only\nthing a process can do is publish anoti\ufb01cation describing the occurrence of an\nevent (e.g., that it wants to coordinate activities, or that it just produced some\ninteresting results). Assuming that noti\ufb01cations come in all sorts and kinds,\nprocesses may subscribe to a speci\ufb01c kind of noti\ufb01cation (see also [M\u00fchl et al.,\n2006]). In an ideal event-based coordination model, a published noti\ufb01cation\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 67\nprocesses become as loose as possible. A large class of distributed systems\nhave adopted an architecture in which there is a strong separation between\nprocessing and coordination . The idea is to view a system as a collection of\nautonomously operating processes. In this model, coordination encompasses\nthe communication and cooperation between processes. It forms the glue\nthat binds the activities performed by processes into a whole [Gelernter and\nCarriero, 1992].\nCabri et al. [2000] provide a taxonomy of coordination models that can\nbe applied equally to many types of distributed systems. Slightly adapting\ntheir terminology, we make a distinction between models along two different\ndimensions, temporal and referential, as shown in Figure 2.9.\nTemporally Temporally\ncoupled decoupled\nReferentially Direct Mailbox\ncoupled\nReferentially Event- Shared\ndecoupled based data space\nFigure 2.9: Examples of different forms of coordination.\nWhen processes are temporally and referentially coupled, coordination\ntakes place in a direct way, referred to as direct coordination . The referential\ncoupling generally appears in the form of explicit referencing in communi-\ncation. For example, a process can communicate only if it knows the name\nor identi\ufb01er of the other processes it wants to exchange information with.\nTemporal coupling means that processes that are communicating will both\nhave to be up and running. In real life, talking over cell phones (and assuming\nthat a cell phone has only one owner), is an example of direct communication.\nA different type of coordination occurs when processes are temporally\ndecoupled, but referentially coupled, which we refer to as mailbox coordina-\ntion. In this case, there is no need for two communicating processes to be\nexecuting at the same time in order to let communication take place. Instead,\ncommunication takes place by putting messages in a (possibly shared) mail-\nbox. Because it is necessary to explicitly address the mailbox that will hold\nthe messages that are to be exchanged, there is a referential coupling.\nThe combination of referentially decoupled and temporally coupled sys-\ntems form the group of models for event-based coordination . In referentially\ndecoupled systems, processes do not know each other explicitly. The only\nthing a process can do is publish anoti\ufb01cation describing the occurrence of an\nevent (e.g., that it wants to coordinate activities, or that it just produced some\ninteresting results). Assuming that noti\ufb01cations come in all sorts and kinds,\nprocesses may subscribe to a speci\ufb01c kind of noti\ufb01cation (see also [M\u00fchl et al.,\n2006]). In an ideal event-based coordination model, a published noti\ufb01cation\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "68 CHAPTER 2. ARCHITECTURES\nwill be delivered exactly to those processes that have subscribed to it. However,\nit is generally required that the subscriber is up-and-running at the time the\nnoti\ufb01cation was published.\nA well-known coordination model is the combination of referentially and\ntemporally decoupled processes, leading to what is known as a shared data\nspace . The key idea is that processes communicate entirely through tuples ,\nwhich are structured data records consisting of a number of \ufb01elds, very similar\nto a row in a database table. Processes can put any type of tuple into the\nshared data space. In order to retrieve a tuple, a process provides a search\npattern that is matched against the tuples. Any tuple that matches is returned.\nShared data spaces are thus seen to implement an associative search\nmechanism for tuples. When a process wants to extract a tuple from the data\nspace, it speci\ufb01es (some of) the values of the \ufb01elds it is interested in. Any\ntuple that matches that speci\ufb01cation is then removed from the data space and\npassed to the process.\nShared data spaces are often combined with event-based coordination: a\nprocess subscribes to certain tuples by providing a search pattern; when a\nprocess inserts a tuple into the data space, matching subscribers are noti\ufb01ed. In\nboth cases, we are dealing with a publish-subscribe architecture, and indeed,\nthe key characteristic feature is that processes have no explicit reference to\neach other. The difference between a pure event-based architectural style, and\nthat of a shared data space is shown in Figure 2.10. We have also shown an\nabstraction of the mechanism by which publishers and subscribers are matched,\nknown as an event bus .\n(a) (b)\nFigure 2.10: The (a) event-based and (b) shared data-space architectural style.\nNote 2.3 (Example: Linda tuple spaces)\nTo make matters a bit more concrete, we take a closer look at Linda , a program-\nming model developed in the 1980s [Carriero and Gelernter, 1989]. The shared\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n68 CHAPTER 2. ARCHITECTURES\nwill be delivered exactly to those processes that have subscribed to it. However,\nit is generally required that the subscriber is up-and-running at the time the\nnoti\ufb01cation was published.\nA well-known coordination model is the combination of referentially and\ntemporally decoupled processes, leading to what is known as a shared data\nspace . The key idea is that processes communicate entirely through tuples ,\nwhich are structured data records consisting of a number of \ufb01elds, very similar\nto a row in a database table. Processes can put any type of tuple into the\nshared data space. In order to retrieve a tuple, a process provides a search\npattern that is matched against the tuples. Any tuple that matches is returned.\nShared data spaces are thus seen to implement an associative search\nmechanism for tuples. When a process wants to extract a tuple from the data\nspace, it speci\ufb01es (some of) the values of the \ufb01elds it is interested in. Any\ntuple that matches that speci\ufb01cation is then removed from the data space and\npassed to the process.\nShared data spaces are often combined with event-based coordination: a\nprocess subscribes to certain tuples by providing a search pattern; when a\nprocess inserts a tuple into the data space, matching subscribers are noti\ufb01ed. In\nboth cases, we are dealing with a publish-subscribe architecture, and indeed,\nthe key characteristic feature is that processes have no explicit reference to\neach other. The difference between a pure event-based architectural style, and\nthat of a shared data space is shown in Figure 2.10. We have also shown an\nabstraction of the mechanism by which publishers and subscribers are matched,\nknown as an event bus .\n(a) (b)\nFigure 2.10: The (a) event-based and (b) shared data-space architectural style.\nNote 2.3 (Example: Linda tuple spaces)\nTo make matters a bit more concrete, we take a closer look at Linda , a program-\nming model developed in the 1980s [Carriero and Gelernter, 1989]. The shared\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.1. ARCHITECTURAL STYLES 69\ndata space in Linda is known as a tuple space , which essentially supports three\noperations:\n\u2022in(t) : remove a tuple that matches the template t\n\u2022rd(t) : obtain a copy of a tuple that matches the template t\n\u2022out(t) : add the tuple tto the tuple space\nNote that if a process would call out(t) twice in a row, we would \ufb01nd that two\ncopies of tuple twould have been stored. Formally, a tuple space is therefore\nalways modeled as a multiset . Both inand rdareblocking operations: the caller\nwill be blocked until a matching tuple is found, or has become available.\nConsider a very simple microblog application in which messages are tagged\nwith the name of its poster and a topic, followed by a short string. Each message\nis modeled as a tuple <string,string,string> where the \ufb01rst string names the\nposter, the second string represents the topic, and the third one is the actual\ncontent. Assuming that we have created a shared data space called MicroBlog ,\nFigure 2.11 shows how Alice and Bob can post messages to that space, and how\nChuck can pick a (randomly selected) message. We have omitted some code for\nclarity. Note that neither Alice nor Bob knows who will read their postings.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3blog._out((\"bob\",\"distsys\",\"I am studying chap 2\"))\n4blog._out((\"bob\",\"distsys\",\"The linda example\u2019s pretty simple\"))\n5blog._out((\"bob\",\"gtcn\",\"Cool book!\"))\n(a) Bob\u2019s code for creating a microblog and posting two messages.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3blog._out((\"alice\",\"gtcn\",\"This graph theory stuff is not easy\"))\n4blog._out((\"alice\",\"distsys\",\"I like systems more than graphs\"))\n(b) Alice\u2019s code for creating a microblog and posting two messages.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3t1 = blog._rd((\"bob\",\"distsys\", str))\n4t2 = blog._rd((\"alice\",\"gtcn\", str))\n5t3 = blog._rd((\"bob\",\"gtcn\", str))\n(c) Chuck reading a message from Bob\u2019s and Alice\u2019s microblog.\nFigure 2.11: A simple example of using a shared data space.\nIn the \ufb01rst line of each code fragment, a process looks up the tuple space\nnamed \u201c MicroBlog .\u201d Bob posts three messages: two on topic distsys , and one on\ngtcn. Alice posts two messages, one on each topic. Chuck, \ufb01nally, reads three\nmessages: one from Bob on distsys and one on gtcn, and one from Alice on gtcn.\nObviously, there is a much room for improvement. For example, we should\nensure that Alice cannot post messages under Bob\u2019s name. However, the important\nissue to note now, is that by providing only tags, a reader such as Chuck will\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.1. ARCHITECTURAL STYLES 69\ndata space in Linda is known as a tuple space , which essentially supports three\noperations:\n\u2022in(t) : remove a tuple that matches the template t\n\u2022rd(t) : obtain a copy of a tuple that matches the template t\n\u2022out(t) : add the tuple tto the tuple space\nNote that if a process would call out(t) twice in a row, we would \ufb01nd that two\ncopies of tuple twould have been stored. Formally, a tuple space is therefore\nalways modeled as a multiset . Both inand rdareblocking operations: the caller\nwill be blocked until a matching tuple is found, or has become available.\nConsider a very simple microblog application in which messages are tagged\nwith the name of its poster and a topic, followed by a short string. Each message\nis modeled as a tuple <string,string,string> where the \ufb01rst string names the\nposter, the second string represents the topic, and the third one is the actual\ncontent. Assuming that we have created a shared data space called MicroBlog ,\nFigure 2.11 shows how Alice and Bob can post messages to that space, and how\nChuck can pick a (randomly selected) message. We have omitted some code for\nclarity. Note that neither Alice nor Bob knows who will read their postings.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3blog._out((\"bob\",\"distsys\",\"I am studying chap 2\"))\n4blog._out((\"bob\",\"distsys\",\"The linda example\u2019s pretty simple\"))\n5blog._out((\"bob\",\"gtcn\",\"Cool book!\"))\n(a) Bob\u2019s code for creating a microblog and posting two messages.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3blog._out((\"alice\",\"gtcn\",\"This graph theory stuff is not easy\"))\n4blog._out((\"alice\",\"distsys\",\"I like systems more than graphs\"))\n(b) Alice\u2019s code for creating a microblog and posting two messages.\n1blog = linda.universe._rd((\"MicroBlog\",linda.TupleSpace))[1]\n2\n3t1 = blog._rd((\"bob\",\"distsys\", str))\n4t2 = blog._rd((\"alice\",\"gtcn\", str))\n5t3 = blog._rd((\"bob\",\"gtcn\", str))\n(c) Chuck reading a message from Bob\u2019s and Alice\u2019s microblog.\nFigure 2.11: A simple example of using a shared data space.\nIn the \ufb01rst line of each code fragment, a process looks up the tuple space\nnamed \u201c MicroBlog .\u201d Bob posts three messages: two on topic distsys , and one on\ngtcn. Alice posts two messages, one on each topic. Chuck, \ufb01nally, reads three\nmessages: one from Bob on distsys and one on gtcn, and one from Alice on gtcn.\nObviously, there is a much room for improvement. For example, we should\nensure that Alice cannot post messages under Bob\u2019s name. However, the important\nissue to note now, is that by providing only tags, a reader such as Chuck will\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "70 CHAPTER 2. ARCHITECTURES\nbe able to pick up messages without needing to directly reference the poster. In\nparticular, Chuck could also read a randomly selected message on topic distsys\nthrough the statement\nt = blog_rd((str,\"distsys\",str))\nWe leave it as an exercise to the reader to extend the code fragments such that a\nnext message will be selected instead of a random one.\nAn important aspect of publish-subscribe systems is that communication\ntakes place by describing the events that a subscriber is interested in. As a\nconsequence, naming plays a crucial role. We return to naming later, but for\nnow the important issue is that in many cases, data items are not explicitly\nidenti\ufb01ed by senders and receivers.\nLet us \ufb01rst assume that events are described by a series of attributes .\nA noti\ufb01cation describing an event is said to be published when it is made\navailable for other processes to read. To that end, a subscription needs to\nbe passed to the middleware, containing a description of the event that the\nsubscriber is interested in. Such a description typically consists of some\n(attribute, value ) pairs, which is common for so-called topic-based publish-\nsubscribe systems .\nAs an alternative, in content-based publish-subscribe systems , a sub-\nscription may also consist of ( attribute, range ) pairs. In this case, the speci\ufb01ed\nattribute is expected to take on values within a speci\ufb01ed range. Descriptions\ncan sometimes be given using all kinds of predicates formulated over the\nattributes, very similar in nature to SQL-like queries in the case of relational\ndatabases. Obviously, the more complex a description is, the more dif\ufb01cult it\nwill be to test whether an event matches a description.\nWe are now confronted with a situation in which subscriptions need to\nbematched against noti\ufb01cations, as shown in Figure 2.12. In many cases, an\nevent actually corresponds to data becoming available. In that case, when\nmatching succeeds, there are two possible scenarios. In the \ufb01rst case, the\nmiddleware may decide to forward the published noti\ufb01cation, along with\nthe associated data, to its current set of subscribers, that is, processes with a\nmatching subscription. As an alternative, the middleware can also forward\nonly a noti\ufb01cation at which point subscribers can execute a read operation to\nretrieve the associated data item.\nIn those cases in which data associated with an event are immediately\nforwarded to subscribers, the middleware will generally not offer storage\nof data. Storage is either explicitly handled by a separate service, or is\nthe responsibility of subscribers. In other words, we have a referentially\ndecoupled, but temporally coupled system.\nThis situation is different when noti\ufb01cations are sent so that subscribers\nneed to explicitly read the associated data. Necessarily, the middleware will\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n70 CHAPTER 2. ARCHITECTURES\nbe able to pick up messages without needing to directly reference the poster. In\nparticular, Chuck could also read a randomly selected message on topic distsys\nthrough the statement\nt = blog_rd((str,\"distsys\",str))\nWe leave it as an exercise to the reader to extend the code fragments such that a\nnext message will be selected instead of a random one.\nAn important aspect of publish-subscribe systems is that communication\ntakes place by describing the events that a subscriber is interested in. As a\nconsequence, naming plays a crucial role. We return to naming later, but for\nnow the important issue is that in many cases, data items are not explicitly\nidenti\ufb01ed by senders and receivers.\nLet us \ufb01rst assume that events are described by a series of attributes .\nA noti\ufb01cation describing an event is said to be published when it is made\navailable for other processes to read. To that end, a subscription needs to\nbe passed to the middleware, containing a description of the event that the\nsubscriber is interested in. Such a description typically consists of some\n(attribute, value ) pairs, which is common for so-called topic-based publish-\nsubscribe systems .\nAs an alternative, in content-based publish-subscribe systems , a sub-\nscription may also consist of ( attribute, range ) pairs. In this case, the speci\ufb01ed\nattribute is expected to take on values within a speci\ufb01ed range. Descriptions\ncan sometimes be given using all kinds of predicates formulated over the\nattributes, very similar in nature to SQL-like queries in the case of relational\ndatabases. Obviously, the more complex a description is, the more dif\ufb01cult it\nwill be to test whether an event matches a description.\nWe are now confronted with a situation in which subscriptions need to\nbematched against noti\ufb01cations, as shown in Figure 2.12. In many cases, an\nevent actually corresponds to data becoming available. In that case, when\nmatching succeeds, there are two possible scenarios. In the \ufb01rst case, the\nmiddleware may decide to forward the published noti\ufb01cation, along with\nthe associated data, to its current set of subscribers, that is, processes with a\nmatching subscription. As an alternative, the middleware can also forward\nonly a noti\ufb01cation at which point subscribers can execute a read operation to\nretrieve the associated data item.\nIn those cases in which data associated with an event are immediately\nforwarded to subscribers, the middleware will generally not offer storage\nof data. Storage is either explicitly handled by a separate service, or is\nthe responsibility of subscribers. In other words, we have a referentially\ndecoupled, but temporally coupled system.\nThis situation is different when noti\ufb01cations are sent so that subscribers\nneed to explicitly read the associated data. Necessarily, the middleware will\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.2. MIDDLEWARE ORGANIZATION 71\nFigure 2.12: The principle of exchanging data items between publishers and\nsubscribers.\nhave to store data items. In these situations there are additional operations\nfor data management. It is also possible to attach a lease to a data item such\nthat when the lease expires that the data item is automatically deleted.\nEvents can easily complicate the processing of subscriptions. To illustrate,\nconsider a subscription such as \u201cnotify when room ZI.1060 is unoccupied and\nthe door is unlocked.\u201d Typically, a distributed system supporting such sub-\nscriptions can be implemented by placing independent sensors for monitoring\nroom occupancy (e.g., motion sensors) and those for registering the status\nof a door lock. Following the approach sketched so far, we would need to\ncompose such primitive events into a publishable data item to which processes\ncan then subscribe. Event composition turns out to be a dif\ufb01cult task, notably\nwhen the primitive events are generated from sources dispersed across the\ndistributed system.\nClearly, in publish-subscribe systems such as these, the crucial issue is the\nef\ufb01cient and scalable implementation of matching subscriptions to noti\ufb01ca-\ntions. From the outside, the publish-subscribe architecture provides lots of\npotential for building very large-scale distributed systems due to the strong\ndecoupling of processes. On the other hand, devising scalable implementa-\ntions without losing this independence is not a trivial exercise, notably in the\ncase of content-based publish-subscribe systems.\n2.2 Middleware organization\nIn the previous section we discussed a number of architectural styles that are\noften used as general guidelines to build and organize distributed systems. Let\nus now zoom into the actual organization of middleware, that is, independent\nof the overall organization of a distributed system or application. In particular,\nthere are two important types of design patterns that are often applied to the\norganization of middleware: wrappers and interceptors. Each targets different\nproblems, yet addresses the same goal for middleware: achieving openness\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.2. MIDDLEWARE ORGANIZATION 71\nFigure 2.12: The principle of exchanging data items between publishers and\nsubscribers.\nhave to store data items. In these situations there are additional operations\nfor data management. It is also possible to attach a lease to a data item such\nthat when the lease expires that the data item is automatically deleted.\nEvents can easily complicate the processing of subscriptions. To illustrate,\nconsider a subscription such as \u201cnotify when room ZI.1060 is unoccupied and\nthe door is unlocked.\u201d Typically, a distributed system supporting such sub-\nscriptions can be implemented by placing independent sensors for monitoring\nroom occupancy (e.g., motion sensors) and those for registering the status\nof a door lock. Following the approach sketched so far, we would need to\ncompose such primitive events into a publishable data item to which processes\ncan then subscribe. Event composition turns out to be a dif\ufb01cult task, notably\nwhen the primitive events are generated from sources dispersed across the\ndistributed system.\nClearly, in publish-subscribe systems such as these, the crucial issue is the\nef\ufb01cient and scalable implementation of matching subscriptions to noti\ufb01ca-\ntions. From the outside, the publish-subscribe architecture provides lots of\npotential for building very large-scale distributed systems due to the strong\ndecoupling of processes. On the other hand, devising scalable implementa-\ntions without losing this independence is not a trivial exercise, notably in the\ncase of content-based publish-subscribe systems.\n2.2 Middleware organization\nIn the previous section we discussed a number of architectural styles that are\noften used as general guidelines to build and organize distributed systems. Let\nus now zoom into the actual organization of middleware, that is, independent\nof the overall organization of a distributed system or application. In particular,\nthere are two important types of design patterns that are often applied to the\norganization of middleware: wrappers and interceptors. Each targets different\nproblems, yet addresses the same goal for middleware: achieving openness\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "72 CHAPTER 2. ARCHITECTURES\n(as we discussed in Section 1.2). However, it can be argued that the ultimate\nopenness is achieved when we can compose middleware at runtime. We\nbrie\ufb02y discuss component-based construction as a popular means toward\nwhat Parlavantzas and Coulson [2007] refer to as modi\ufb01able middleware.\nWrappers\nWhen building a distributed system out of existing components, we immedi-\nately bump into a fundamental problem: the interfaces offered by the legacy\ncomponent are most likely not suitable for all applications. In Section 1.3 we\ndiscussed how enterprise application integration could be established through\nmiddleware as a communication facilitator, but there we still implicitly as-\nsumed that, in the end, components could be accessed through their native\ninterfaces.\nAwrapper oradapter is a special component that offers an interface\nacceptable to a client application, of which the functions are transformed\ninto those available at the component. In essence, it solves the problem of\nincompatible interfaces (see also [Gamma et al., 1994]).\nAlthough originally narrowly de\ufb01ned in the context of object-oriented\nprogramming, in the context of distributed systems wrappers are much more\nthan simple interface transformers. For example, an object adapter is a\ncomponent that allows applications to invoke remote objects, although those\nobjects may have been implemented as a combination of library functions\noperating on the tables of a relational database.\nAs another example, reconsider Amazon\u2019s S3 storage service. There are\nnow two types of interfaces available, one adhering to a RESTful architecture,\nanother following a more traditional approach. For the RESTful interface,\nclients will be using the HTTP protocol, essentially communicating with a\ntraditional Web server which now acts as an adapter to the actual storage\nservice, by partly dissecting incoming requests and subsequently handing\nthem off to specialized servers internal to S3.\nWrappers have always played an important role in extending systems with\nexisting components. Extensibility, which is crucial for achieving openness,\nused to be addressed by adding wrappers as needed. In other words, if\napplication Amanaged data that was needed by application B, one approach\nwould be to develop a wrapper speci\ufb01c for Bso that it could have access to\nA\u2019s data. Clearly, this approach does not scale well: with Napplications we\nwould, in theory, need to develop N\u0002(N\u00001) =O(N2)wrappers.\nAgain, facilitating a reduction of the number of wrappers is typically\ndone through middleware. One way of doing this is implementing a so-\ncalled broker , which is logically a centralized component that handles all\nthe accesses between different applications. An often-used type is a message\nbroker which we discuss the technicalities in Section 4.3. In the case of a\nmessage broker, applications simply send requests to the broker containing\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n72 CHAPTER 2. ARCHITECTURES\n(as we discussed in Section 1.2). However, it can be argued that the ultimate\nopenness is achieved when we can compose middleware at runtime. We\nbrie\ufb02y discuss component-based construction as a popular means toward\nwhat Parlavantzas and Coulson [2007] refer to as modi\ufb01able middleware.\nWrappers\nWhen building a distributed system out of existing components, we immedi-\nately bump into a fundamental problem: the interfaces offered by the legacy\ncomponent are most likely not suitable for all applications. In Section 1.3 we\ndiscussed how enterprise application integration could be established through\nmiddleware as a communication facilitator, but there we still implicitly as-\nsumed that, in the end, components could be accessed through their native\ninterfaces.\nAwrapper oradapter is a special component that offers an interface\nacceptable to a client application, of which the functions are transformed\ninto those available at the component. In essence, it solves the problem of\nincompatible interfaces (see also [Gamma et al., 1994]).\nAlthough originally narrowly de\ufb01ned in the context of object-oriented\nprogramming, in the context of distributed systems wrappers are much more\nthan simple interface transformers. For example, an object adapter is a\ncomponent that allows applications to invoke remote objects, although those\nobjects may have been implemented as a combination of library functions\noperating on the tables of a relational database.\nAs another example, reconsider Amazon\u2019s S3 storage service. There are\nnow two types of interfaces available, one adhering to a RESTful architecture,\nanother following a more traditional approach. For the RESTful interface,\nclients will be using the HTTP protocol, essentially communicating with a\ntraditional Web server which now acts as an adapter to the actual storage\nservice, by partly dissecting incoming requests and subsequently handing\nthem off to specialized servers internal to S3.\nWrappers have always played an important role in extending systems with\nexisting components. Extensibility, which is crucial for achieving openness,\nused to be addressed by adding wrappers as needed. In other words, if\napplication Amanaged data that was needed by application B, one approach\nwould be to develop a wrapper speci\ufb01c for Bso that it could have access to\nA\u2019s data. Clearly, this approach does not scale well: with Napplications we\nwould, in theory, need to develop N\u0002(N\u00001) =O(N2)wrappers.\nAgain, facilitating a reduction of the number of wrappers is typically\ndone through middleware. One way of doing this is implementing a so-\ncalled broker , which is logically a centralized component that handles all\nthe accesses between different applications. An often-used type is a message\nbroker which we discuss the technicalities in Section 4.3. In the case of a\nmessage broker, applications simply send requests to the broker containing\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.2. MIDDLEWARE ORGANIZATION 73\n(a) (b)\nFigure 2.13: (a) Requiring each application to have a wrapper for each other\napplication. (b) Reducing the number of wrappers by making use of a broker.\ninformation on what they need. The broker, having knowledge of all relevant\napplications, contacts the appropriate applications, possibly combines and\ntransforms the responses and returns the result to the initial application. In\nprinciple, because a broker offers a single interface to each application, we\nnow need at most 2N=O(N)wrappers instead of O(N2). This situation is\nsketched in Figure 2.13.\nInterceptors\nConceptually, an interceptor is nothing but a software construct that will\nbreak the usual \ufb02ow of control and allow other (application speci\ufb01c) code\nto be executed. Interceptors are a primary means for adapting middleware\nto the speci\ufb01c needs of an application. As such, they play an important\nrole in making middleware open. To make interceptors generic may require\na substantial implementation effort, as illustrated in Schmidt et al. [2000]\nand it is unclear whether in such cases generality should be preferred over\nrestricted applicability and simplicity. Also, in many cases having only\nlimited interception facilities will improve management of the software and\nthe distributed system as a whole.\nTo make matters concrete, consider interception as supported in many\nobject-based distributed systems. The basic idea is simple: an object Acan call\na method that belongs to an object B, while the latter resides on a different\nmachine than A. As we explain in detail later in the book, such a remote-object\ninvocation is carried out in three steps:\n1. Object Ais offered a local interface that is exactly the same as the inter-\nface offered by object B.Acalls the method available in that interface.\n2.The call by Ais transformed into a generic object invocation, made\npossible through a general object-invocation interface offered by the\nmiddleware at the machine where Aresides.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.2. MIDDLEWARE ORGANIZATION 73\n(a) (b)\nFigure 2.13: (a) Requiring each application to have a wrapper for each other\napplication. (b) Reducing the number of wrappers by making use of a broker.\ninformation on what they need. The broker, having knowledge of all relevant\napplications, contacts the appropriate applications, possibly combines and\ntransforms the responses and returns the result to the initial application. In\nprinciple, because a broker offers a single interface to each application, we\nnow need at most 2N=O(N)wrappers instead of O(N2). This situation is\nsketched in Figure 2.13.\nInterceptors\nConceptually, an interceptor is nothing but a software construct that will\nbreak the usual \ufb02ow of control and allow other (application speci\ufb01c) code\nto be executed. Interceptors are a primary means for adapting middleware\nto the speci\ufb01c needs of an application. As such, they play an important\nrole in making middleware open. To make interceptors generic may require\na substantial implementation effort, as illustrated in Schmidt et al. [2000]\nand it is unclear whether in such cases generality should be preferred over\nrestricted applicability and simplicity. Also, in many cases having only\nlimited interception facilities will improve management of the software and\nthe distributed system as a whole.\nTo make matters concrete, consider interception as supported in many\nobject-based distributed systems. The basic idea is simple: an object Acan call\na method that belongs to an object B, while the latter resides on a different\nmachine than A. As we explain in detail later in the book, such a remote-object\ninvocation is carried out in three steps:\n1. Object Ais offered a local interface that is exactly the same as the inter-\nface offered by object B.Acalls the method available in that interface.\n2.The call by Ais transformed into a generic object invocation, made\npossible through a general object-invocation interface offered by the\nmiddleware at the machine where Aresides.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "74 CHAPTER 2. ARCHITECTURES\n3.Finally, the generic object invocation is transformed into a message that\nis sent through the transport-level network interface as offered by A\u2019s\nlocal operating system.\nFigure 2.14: Using interceptors to handle remote-object invocations.\nThis scheme is shown in Figure 2.14. After the \ufb01rst step, the call B.doit(val)\nis transformed into a generic call such as invoke(B, &doit, val) with a ref-\nerence to B\u2019s method and the parameters that go along with the call. Now\nimagine that object Bis replicated. In that case, each replica should actually\nbe invoked. This is a clear point where interception can help. What the\nrequest-level interceptor will do, is simply call invoke(B, &doit, val) for\neach of the replicas. The beauty of this all is that the object Aneed not be\naware of the replication of B, but also the object middleware need not have\nspecial components that deal with this replicated call. Only the request-level\ninterceptor, which may be added to the middleware needs to know about B\u2019s\nreplication.\nIn the end, a call to a remote object will have to be sent over the network.\nIn practice, this means that the messaging interface as offered by the local\noperating system will need to be invoked. At that level, a message-level\ninterceptor may assist in transferring the invocation to the target object. For\nexample, imagine that the parameter valactually corresponds to a huge array\nof data. In that case, it may be wise to fragment the data into smaller parts to\nhave it assembled again at the destination. Such a fragmentation may improve\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n74 CHAPTER 2. ARCHITECTURES\n3.Finally, the generic object invocation is transformed into a message that\nis sent through the transport-level network interface as offered by A\u2019s\nlocal operating system.\nFigure 2.14: Using interceptors to handle remote-object invocations.\nThis scheme is shown in Figure 2.14. After the \ufb01rst step, the call B.doit(val)\nis transformed into a generic call such as invoke(B, &doit, val) with a ref-\nerence to B\u2019s method and the parameters that go along with the call. Now\nimagine that object Bis replicated. In that case, each replica should actually\nbe invoked. This is a clear point where interception can help. What the\nrequest-level interceptor will do, is simply call invoke(B, &doit, val) for\neach of the replicas. The beauty of this all is that the object Aneed not be\naware of the replication of B, but also the object middleware need not have\nspecial components that deal with this replicated call. Only the request-level\ninterceptor, which may be added to the middleware needs to know about B\u2019s\nreplication.\nIn the end, a call to a remote object will have to be sent over the network.\nIn practice, this means that the messaging interface as offered by the local\noperating system will need to be invoked. At that level, a message-level\ninterceptor may assist in transferring the invocation to the target object. For\nexample, imagine that the parameter valactually corresponds to a huge array\nof data. In that case, it may be wise to fragment the data into smaller parts to\nhave it assembled again at the destination. Such a fragmentation may improve\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.2. MIDDLEWARE ORGANIZATION 75\nperformance or reliability. Again, the middleware need not be aware of this\nfragmentation; the lower-level interceptor will transparently handle the rest of\nthe communication with the local operating system.\nModi\ufb01able middleware\nWhat wrappers and interceptors offer are means to extend and adapt the\nmiddleware. The need for adaptation comes from the fact that the environment\nin which distributed applications are executed changes continuously. Changes\ninclude those resulting from mobility, a strong variance in the quality-of-\nservice of networks, failing hardware, and battery drainage, amongst others.\nRather than making applications responsible for reacting to changes, this task\nis placed in the middleware. Moreover, as the size of a distributed system\nincreases, changing its parts can rarely be done by temporarily shutting it\ndown. What is needed is being able to make changes on-the-\ufb02y.\nThese strong in\ufb02uences from the environment have brought many de-\nsigners of middleware to consider the construction of adaptive software . We\nfollow Parlavantzas and Coulson [2007] in speaking of modi\ufb01able middleware\nto express that middleware may not only need to be adaptive, but that we\nshould be able to purposefully modify it without bringing it down. In this\ncontext, interceptors can be thought of offering a means to adapt the standard\n\ufb02ow of control. Replacing software components at runtime is an example\nof modifying a system. And indeed, perhaps one of the most popular ap-\nproaches toward modi\ufb01able middleware is that of dynamically constructing\nmiddleware from components.\nComponent-based design focuses on supporting modi\ufb01ability through\ncomposition. A system may either be con\ufb01gured statically at design time,\nor dynamically at runtime. The latter requires support for late binding,\na technique that has been successfully applied in programming language\nenvironments, but also for operating systems where modules can be loaded\nand unloaded at will. Research is now well underway to automatically select\nthe best implementation of a component during runtime [Yellin, 2003] but\nagain, the process remains complex for distributed systems, especially when\nconsidering that replacement of one component requires to know exactly what\nthe effect of that replacement on other components will be. In many cases,\ncomponents are less independent as one may think.\nThe bottom line is that in order to accommodate dynamic changes to the\nsoftware that makes up middleware, we need at least basic support to load\nand unload components at runtime. In addition, for each component explicit\nspeci\ufb01cations of the interfaces it offers, as well the interfaces it requires, are\nneeded. If state is maintained between calls to a component, then further\nspecial measures are needed. By-and-large, it should be clear that organizing\nmiddleware to be modi\ufb01able requires very special attention.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.2. MIDDLEWARE ORGANIZATION 75\nperformance or reliability. Again, the middleware need not be aware of this\nfragmentation; the lower-level interceptor will transparently handle the rest of\nthe communication with the local operating system.\nModi\ufb01able middleware\nWhat wrappers and interceptors offer are means to extend and adapt the\nmiddleware. The need for adaptation comes from the fact that the environment\nin which distributed applications are executed changes continuously. Changes\ninclude those resulting from mobility, a strong variance in the quality-of-\nservice of networks, failing hardware, and battery drainage, amongst others.\nRather than making applications responsible for reacting to changes, this task\nis placed in the middleware. Moreover, as the size of a distributed system\nincreases, changing its parts can rarely be done by temporarily shutting it\ndown. What is needed is being able to make changes on-the-\ufb02y.\nThese strong in\ufb02uences from the environment have brought many de-\nsigners of middleware to consider the construction of adaptive software . We\nfollow Parlavantzas and Coulson [2007] in speaking of modi\ufb01able middleware\nto express that middleware may not only need to be adaptive, but that we\nshould be able to purposefully modify it without bringing it down. In this\ncontext, interceptors can be thought of offering a means to adapt the standard\n\ufb02ow of control. Replacing software components at runtime is an example\nof modifying a system. And indeed, perhaps one of the most popular ap-\nproaches toward modi\ufb01able middleware is that of dynamically constructing\nmiddleware from components.\nComponent-based design focuses on supporting modi\ufb01ability through\ncomposition. A system may either be con\ufb01gured statically at design time,\nor dynamically at runtime. The latter requires support for late binding,\na technique that has been successfully applied in programming language\nenvironments, but also for operating systems where modules can be loaded\nand unloaded at will. Research is now well underway to automatically select\nthe best implementation of a component during runtime [Yellin, 2003] but\nagain, the process remains complex for distributed systems, especially when\nconsidering that replacement of one component requires to know exactly what\nthe effect of that replacement on other components will be. In many cases,\ncomponents are less independent as one may think.\nThe bottom line is that in order to accommodate dynamic changes to the\nsoftware that makes up middleware, we need at least basic support to load\nand unload components at runtime. In addition, for each component explicit\nspeci\ufb01cations of the interfaces it offers, as well the interfaces it requires, are\nneeded. If state is maintained between calls to a component, then further\nspecial measures are needed. By-and-large, it should be clear that organizing\nmiddleware to be modi\ufb01able requires very special attention.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "76 CHAPTER 2. ARCHITECTURES\n2.3 System architecture\nNow that we have brie\ufb02y discussed some commonly applied architectural\nstyles, let us take a look at how many distributed systems are actually or-\nganized by considering where software components are placed. Deciding\non software components, their interaction, and their placement leads to an\ninstance of a software architecture, also known as a system architecture [Bass\net al., 2003]. We will discuss centralized and decentralized organizations, as\nwell as various hybrid forms.\nCentralized organizations\nDespite the lack of consensus on many distributed systems issues, there is\none issue that many researchers and practitioners agree upon: thinking in\nterms of clients that request services from servers helps understanding and\nmanaging the complexity of distributed systems [Saltzer and Kaashoek, 2009].\nIn the following, we \ufb01rst consider a simple layered organization, followed by\nlooking at multi-layered organizations.\nSimple client-server architecture\nIn the basic client-server model, processes in a distributed system are divided\ninto two (possibly overlapping) groups. A server is a process implementing\na speci\ufb01c service, for example, a \ufb01le system service or a database service. A\nclient is a process that requests a service from a server by sending it a request\nand subsequently waiting for the server\u2019s reply. This client-server interaction,\nalso known as request-reply behavior is shown in Figure 2.15 in the form of\na message sequence chart.\nFigure 2.15: General interaction between a client and a server.\nCommunication between a client and a server can be implemented by\nmeans of a simple connectionless protocol when the underlying network is\nfairly reliable as in many local-area networks. In these cases, when a client\nrequests a service, it simply packages a message for the server, identifying the\nservice it wants, along with the necessary input data. The message is then\nsent to the server. The latter, in turn, will always wait for an incoming request,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n76 CHAPTER 2. ARCHITECTURES\n2.3 System architecture\nNow that we have brie\ufb02y discussed some commonly applied architectural\nstyles, let us take a look at how many distributed systems are actually or-\nganized by considering where software components are placed. Deciding\non software components, their interaction, and their placement leads to an\ninstance of a software architecture, also known as a system architecture [Bass\net al., 2003]. We will discuss centralized and decentralized organizations, as\nwell as various hybrid forms.\nCentralized organizations\nDespite the lack of consensus on many distributed systems issues, there is\none issue that many researchers and practitioners agree upon: thinking in\nterms of clients that request services from servers helps understanding and\nmanaging the complexity of distributed systems [Saltzer and Kaashoek, 2009].\nIn the following, we \ufb01rst consider a simple layered organization, followed by\nlooking at multi-layered organizations.\nSimple client-server architecture\nIn the basic client-server model, processes in a distributed system are divided\ninto two (possibly overlapping) groups. A server is a process implementing\na speci\ufb01c service, for example, a \ufb01le system service or a database service. A\nclient is a process that requests a service from a server by sending it a request\nand subsequently waiting for the server\u2019s reply. This client-server interaction,\nalso known as request-reply behavior is shown in Figure 2.15 in the form of\na message sequence chart.\nFigure 2.15: General interaction between a client and a server.\nCommunication between a client and a server can be implemented by\nmeans of a simple connectionless protocol when the underlying network is\nfairly reliable as in many local-area networks. In these cases, when a client\nrequests a service, it simply packages a message for the server, identifying the\nservice it wants, along with the necessary input data. The message is then\nsent to the server. The latter, in turn, will always wait for an incoming request,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 77\nsubsequently process it, and package the results in a reply message that is\nthen sent to the client.\nUsing a connectionless protocol has the obvious advantage of being ef\ufb01-\ncient. As long as messages do not get lost or corrupted, the request/reply\nprotocol just sketched works \ufb01ne. Unfortunately, making the protocol resistant\nto occasional transmission failures is not trivial. The only thing we can do is\npossibly let the client resend the request when no reply message comes in. The\nproblem, however, is that the client cannot detect whether the original request\nmessage was lost, or that transmission of the reply failed. If the reply was lost,\nthen resending a request may result in performing the operation twice. If the\noperation was something like \u201ctransfer $10,000 from my bank account,\u201d then\nclearly, it would have been better that we simply reported an error instead.\nOn the other hand, if the operation was \u201ctell me how much money I have left,\u201d\nit would be perfectly acceptable to resend the request. When an operation\ncan be repeated multiple times without harm, it is said to be idempotent .\nSince some requests are idempotent and others are not it should be clear that\nthere is no single solution for dealing with lost messages. We defer a detailed\ndiscussion on handling transmission failures to Section 8.3.\nAs an alternative, many client-server systems use a reliable connection-\noriented protocol. Although this solution is not entirely appropriate in a\nlocal-area network due to relatively low performance, it works perfectly \ufb01ne\nin wide-area systems in which communication is inherently unreliable. For\nexample, virtually all Internet application protocols are based on reliable\nTCP/IP connections. In this case, whenever a client requests a service, it \ufb01rst\nsets up a connection to the server before sending the request. The server\ngenerally uses that same connection to send the reply message, after which\nthe connection is torn down. The trouble may be that setting up and tearing\ndown a connection is relatively costly, especially when the request and reply\nmessages are small.\nThe client-server model has been subject to many debates and controver-\nsies over the years. One of the main issues was how to draw a clear distinction\nbetween a client and a server. Not surprisingly, there is often no clear distinc-\ntion. For example, a server for a distributed database may continuously act as\na client because it is forwarding requests to different \ufb01le servers responsible\nfor implementing the database tables. In such a case, the database server itself\nonly processes the queries.\nMultitiered Architectures\nThe distinction into three logical levels as discussed so far, suggests a number\nof possibilities for physically distributing a client-server application across\nseveral machines. The simplest organization is to have only two types of\nmachines:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 77\nsubsequently process it, and package the results in a reply message that is\nthen sent to the client.\nUsing a connectionless protocol has the obvious advantage of being ef\ufb01-\ncient. As long as messages do not get lost or corrupted, the request/reply\nprotocol just sketched works \ufb01ne. Unfortunately, making the protocol resistant\nto occasional transmission failures is not trivial. The only thing we can do is\npossibly let the client resend the request when no reply message comes in. The\nproblem, however, is that the client cannot detect whether the original request\nmessage was lost, or that transmission of the reply failed. If the reply was lost,\nthen resending a request may result in performing the operation twice. If the\noperation was something like \u201ctransfer $10,000 from my bank account,\u201d then\nclearly, it would have been better that we simply reported an error instead.\nOn the other hand, if the operation was \u201ctell me how much money I have left,\u201d\nit would be perfectly acceptable to resend the request. When an operation\ncan be repeated multiple times without harm, it is said to be idempotent .\nSince some requests are idempotent and others are not it should be clear that\nthere is no single solution for dealing with lost messages. We defer a detailed\ndiscussion on handling transmission failures to Section 8.3.\nAs an alternative, many client-server systems use a reliable connection-\noriented protocol. Although this solution is not entirely appropriate in a\nlocal-area network due to relatively low performance, it works perfectly \ufb01ne\nin wide-area systems in which communication is inherently unreliable. For\nexample, virtually all Internet application protocols are based on reliable\nTCP/IP connections. In this case, whenever a client requests a service, it \ufb01rst\nsets up a connection to the server before sending the request. The server\ngenerally uses that same connection to send the reply message, after which\nthe connection is torn down. The trouble may be that setting up and tearing\ndown a connection is relatively costly, especially when the request and reply\nmessages are small.\nThe client-server model has been subject to many debates and controver-\nsies over the years. One of the main issues was how to draw a clear distinction\nbetween a client and a server. Not surprisingly, there is often no clear distinc-\ntion. For example, a server for a distributed database may continuously act as\na client because it is forwarding requests to different \ufb01le servers responsible\nfor implementing the database tables. In such a case, the database server itself\nonly processes the queries.\nMultitiered Architectures\nThe distinction into three logical levels as discussed so far, suggests a number\nof possibilities for physically distributing a client-server application across\nseveral machines. The simplest organization is to have only two types of\nmachines:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "78 CHAPTER 2. ARCHITECTURES\n1.A client machine containing only the programs implementing (part of)\nthe user-interface level\n2.A server machine containing the rest, that is, the programs implementing\nthe processing and data level\nIn this organization everything is handled by the server while the client is\nessentially no more than a dumb terminal, possibly with only a convenient\ngraphical interface. There are, however, many other possibilities. As explained\nin Section 2.1, many distributed applications are divided into the three layers\n(1) user interface layer, (2) processing layer, and (3) data layer. One approach\nfor organizing clients and servers is then to distribute these layers across\ndifferent machines, as shown in Figure 2.16 (see also Umar [1997]). As\na \ufb01rst step, we make a distinction between only two kinds of machines:\nclient machines and server machines, leading to what is also referred to as a\n(physically) two-tiered architecture .\n(a) (b) (c) (d) (e)\nFigure 2.16: Client-server organizations in a two-tiered architecture.\nOne possible organization is to have only the terminal-dependent part\nof the user interface on the client machine, as shown in Figure 2.16(a), and\ngive the applications remote control over the presentation of their data. An\nalternative is to place the entire user-interface software on the client side, as\nshown in Figure 2.16(b). In such cases, we essentially divide the application\ninto a graphical front end, which communicates with the rest of the application\n(residing at the server) through an application-speci\ufb01c protocol. In this model,\nthe front end (the client software) does no processing other than necessary for\npresenting the application\u2019s interface.\nContinuing along this line of reasoning, we may also move part of the\napplication to the front end, as shown in Figure 2.16(c). An example where\nthis makes sense is where the application makes use of a form that needs to\nbe \ufb01lled in entirely before it can be processed. The front end can then check\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n78 CHAPTER 2. ARCHITECTURES\n1.A client machine containing only the programs implementing (part of)\nthe user-interface level\n2.A server machine containing the rest, that is, the programs implementing\nthe processing and data level\nIn this organization everything is handled by the server while the client is\nessentially no more than a dumb terminal, possibly with only a convenient\ngraphical interface. There are, however, many other possibilities. As explained\nin Section 2.1, many distributed applications are divided into the three layers\n(1) user interface layer, (2) processing layer, and (3) data layer. One approach\nfor organizing clients and servers is then to distribute these layers across\ndifferent machines, as shown in Figure 2.16 (see also Umar [1997]). As\na \ufb01rst step, we make a distinction between only two kinds of machines:\nclient machines and server machines, leading to what is also referred to as a\n(physically) two-tiered architecture .\n(a) (b) (c) (d) (e)\nFigure 2.16: Client-server organizations in a two-tiered architecture.\nOne possible organization is to have only the terminal-dependent part\nof the user interface on the client machine, as shown in Figure 2.16(a), and\ngive the applications remote control over the presentation of their data. An\nalternative is to place the entire user-interface software on the client side, as\nshown in Figure 2.16(b). In such cases, we essentially divide the application\ninto a graphical front end, which communicates with the rest of the application\n(residing at the server) through an application-speci\ufb01c protocol. In this model,\nthe front end (the client software) does no processing other than necessary for\npresenting the application\u2019s interface.\nContinuing along this line of reasoning, we may also move part of the\napplication to the front end, as shown in Figure 2.16(c). An example where\nthis makes sense is where the application makes use of a form that needs to\nbe \ufb01lled in entirely before it can be processed. The front end can then check\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 79\nthe correctness and consistency of the form, and where necessary interact\nwith the user. Another example of the organization of Figure 2.16(c), is that of\na word processor in which the basic editing functions execute on the client\nside where they operate on locally cached, or in-memory data, but where the\nadvanced support tools such as checking the spelling and grammar execute\non the server side.\nIn many client-server environments, the organizations shown in Fig-\nure 2.16(d) and Figure 2.16(e) are particularly popular. These organizations\nare used where the client machine is a PC or workstation, connected through\na network to a distributed \ufb01le system or database. Essentially, most of the\napplication is running on the client machine, but all operations on \ufb01les or\ndatabase entries go to the server. For example, many banking applications\nrun on an end-user\u2019s machine where the user prepares transactions and such.\nOnce \ufb01nished, the application contacts the database on the bank\u2019s server and\nuploads the transactions for further processing. Figure 2.16(e) represents the\nsituation where the client\u2019s local disk contains part of the data. For example,\nwhen browsing the Web, a client can gradually build a huge cache on local\ndisk of most recent inspected Web pages.\nNote 2.4 (More information: Is there a best organization?)\nWe note that there has been a strong trend to move away from the con\ufb01gurations\nshown in Figure 2.16(d) and Figure 2.16(e) in those cases that client software is\nplaced at end-user machines. Instead, most of the processing and data storage\nis handled at the server side. The reason for this is simple: although client\nmachines do a lot, they are also more problematic to manage. Having more\nfunctionality on the client machine means that a wide range of end users will\nneed to be able to handle that software. This implies that more effort needs to\nbe spent on making software resilient to end-user behavior. In addition, client-\nside software is dependent on the client\u2019s underlying platform (i.e., operating\nsystem and resources), which can easily mean that multiple versions will need\nto be maintained. From a systems-management perspective, having what are\ncalled fat clients is not optimal. Instead the thin clients as represented by the\norganizations shown in Figure 2.16(a)\u2013(c) are much easier, perhaps at the cost of\nless sophisticated user interfaces and client-perceived performance.\nDoes this mean the end of fat clients? Not in the least. For one thing, there\nare many applications for which a fat-client organization is often still the best. We\nalready mentioned of\ufb01ce suites, but also many multimedia applications require\nthat processing is done on the client\u2019s side. Second, with the advent of advanced\nWeb browsing technology, it is now much easier to dynamically place and manage\nclient-side software by simply uploading (the sometimes very sophisticated)\nscripts. Combined with the fact that this type of client-side software runs in well-\nde\ufb01ned commonly deployed environments, and thus that platform dependency\nis much less of an issue, we see that the counter-argument of management\ncomplexity is often no longer valid.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 79\nthe correctness and consistency of the form, and where necessary interact\nwith the user. Another example of the organization of Figure 2.16(c), is that of\na word processor in which the basic editing functions execute on the client\nside where they operate on locally cached, or in-memory data, but where the\nadvanced support tools such as checking the spelling and grammar execute\non the server side.\nIn many client-server environments, the organizations shown in Fig-\nure 2.16(d) and Figure 2.16(e) are particularly popular. These organizations\nare used where the client machine is a PC or workstation, connected through\na network to a distributed \ufb01le system or database. Essentially, most of the\napplication is running on the client machine, but all operations on \ufb01les or\ndatabase entries go to the server. For example, many banking applications\nrun on an end-user\u2019s machine where the user prepares transactions and such.\nOnce \ufb01nished, the application contacts the database on the bank\u2019s server and\nuploads the transactions for further processing. Figure 2.16(e) represents the\nsituation where the client\u2019s local disk contains part of the data. For example,\nwhen browsing the Web, a client can gradually build a huge cache on local\ndisk of most recent inspected Web pages.\nNote 2.4 (More information: Is there a best organization?)\nWe note that there has been a strong trend to move away from the con\ufb01gurations\nshown in Figure 2.16(d) and Figure 2.16(e) in those cases that client software is\nplaced at end-user machines. Instead, most of the processing and data storage\nis handled at the server side. The reason for this is simple: although client\nmachines do a lot, they are also more problematic to manage. Having more\nfunctionality on the client machine means that a wide range of end users will\nneed to be able to handle that software. This implies that more effort needs to\nbe spent on making software resilient to end-user behavior. In addition, client-\nside software is dependent on the client\u2019s underlying platform (i.e., operating\nsystem and resources), which can easily mean that multiple versions will need\nto be maintained. From a systems-management perspective, having what are\ncalled fat clients is not optimal. Instead the thin clients as represented by the\norganizations shown in Figure 2.16(a)\u2013(c) are much easier, perhaps at the cost of\nless sophisticated user interfaces and client-perceived performance.\nDoes this mean the end of fat clients? Not in the least. For one thing, there\nare many applications for which a fat-client organization is often still the best. We\nalready mentioned of\ufb01ce suites, but also many multimedia applications require\nthat processing is done on the client\u2019s side. Second, with the advent of advanced\nWeb browsing technology, it is now much easier to dynamically place and manage\nclient-side software by simply uploading (the sometimes very sophisticated)\nscripts. Combined with the fact that this type of client-side software runs in well-\nde\ufb01ned commonly deployed environments, and thus that platform dependency\nis much less of an issue, we see that the counter-argument of management\ncomplexity is often no longer valid.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "80 CHAPTER 2. ARCHITECTURES\nFinally, note that moving away from fat clients does not imply that we no\nlonger need distributed systems. On the contrary, what we continue to see is\nthat server-side solutions are becoming increasingly more distributed as a single\nserver is being replaced by multiple servers running on different machines. Cloud\ncomputing is a good example in this case: the complete server side is being\nexecuted in data centers, and generally on multiple servers.\nWhen distinguishing only client and server machines as we did so far, we\nmiss the point that a server may sometimes need to act as a client, as shown\nin Figure 2.17, leading to a (physically) three-tiered architecture .\nFigure 2.17: An example of a server acting as client.\nIn this architecture, traditionally programs that form part of the process-\ning layer are executed by a separate server, but may additionally be partly\ndistributed across the client and server machines. A typical example of where\na three-tiered architecture is used is in transaction processing. A separate\nprocess, called the transaction processing monitor, coordinates all transactions\nacross possibly different data servers.\nAnother, but very different example were we often see a three-tiered\narchitecture is in the organization of Web sites. In this case, a Web server acts\nas an entry point to a site, passing requests to an application server where the\nactual processing takes place. This application server, in turn, interacts with a\ndatabase server. For example, an application server may be responsible for\nrunning the code to inspect the available inventory of some goods as offered\nby an electronic bookstore. To do so, it may need to interact with a database\ncontaining the raw inventory data.\nDecentralized organizations: peer-to-peer systems\nMultitiered client-server architectures are a direct consequence of dividing\ndistributed applications into a user interface, processing components, and\ndata-management components. The different tiers correspond directly with\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n80 CHAPTER 2. ARCHITECTURES\nFinally, note that moving away from fat clients does not imply that we no\nlonger need distributed systems. On the contrary, what we continue to see is\nthat server-side solutions are becoming increasingly more distributed as a single\nserver is being replaced by multiple servers running on different machines. Cloud\ncomputing is a good example in this case: the complete server side is being\nexecuted in data centers, and generally on multiple servers.\nWhen distinguishing only client and server machines as we did so far, we\nmiss the point that a server may sometimes need to act as a client, as shown\nin Figure 2.17, leading to a (physically) three-tiered architecture .\nFigure 2.17: An example of a server acting as client.\nIn this architecture, traditionally programs that form part of the process-\ning layer are executed by a separate server, but may additionally be partly\ndistributed across the client and server machines. A typical example of where\na three-tiered architecture is used is in transaction processing. A separate\nprocess, called the transaction processing monitor, coordinates all transactions\nacross possibly different data servers.\nAnother, but very different example were we often see a three-tiered\narchitecture is in the organization of Web sites. In this case, a Web server acts\nas an entry point to a site, passing requests to an application server where the\nactual processing takes place. This application server, in turn, interacts with a\ndatabase server. For example, an application server may be responsible for\nrunning the code to inspect the available inventory of some goods as offered\nby an electronic bookstore. To do so, it may need to interact with a database\ncontaining the raw inventory data.\nDecentralized organizations: peer-to-peer systems\nMultitiered client-server architectures are a direct consequence of dividing\ndistributed applications into a user interface, processing components, and\ndata-management components. The different tiers correspond directly with\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 81\nthe logical organization of applications. In many business environments,\ndistributed processing is equivalent to organizing a client-server application\nas a multitiered architecture. We refer to this type of distribution as vertical\ndistribution . The characteristic feature of vertical distribution is that it is\nachieved by placing logically different components on different machines. The\nterm is related to the concept of vertical fragmentation as used in distributed\nrelational databases, where it means that tables are split columnwise, and\nsubsequently distributed across multiple machines [\u00d6zsu and Valduriez, 2011].\nAgain, from a systems-management perspective, having a vertical distri-\nbution can help: functions are logically and physically split across multiple\nmachines, where each machine is tailored to a speci\ufb01c group of functions.\nHowever, vertical distribution is only one way of organizing client-server\napplications. In modern architectures, it is often the distribution of the clients\nand the servers that counts, which we refer to as horizontal distribution . In\nthis type of distribution, a client or server may be physically split up into\nlogically equivalent parts, but each part is operating on its own share of the\ncomplete data set, thus balancing the load. In this section we will take a look\nat a class of modern system architectures that support horizontal distribution,\nknown as peer-to-peer systems .\nFrom a high-level perspective, the processes that constitute a peer-to-peer\nsystem are all equal. This means that the functions that need to be carried\nout are represented by every process that constitutes the distributed system.\nAs a consequence, much of the interaction between processes is symmetric:\neach process will act as a client and a server at the same time (which is also\nreferred to as acting as a servant ).\nGiven this symmetric behavior, peer-to-peer architectures evolve around\nthe question how to organize the processes in an overlay network [Tarkoma,\n2010]: a network in which the nodes are formed by the processes and the links\nrepresent the possible communication channels (which are often realized as\nTCP connections). A node may not be able to communicate directly with an\narbitrary other node, but is required to send messages through the available\ncommunication channels. Two types of overlay networks exist: those that are\nstructured and those that are not. These two types are surveyed extensively\nin Lua et al. [2005] along with numerous examples. Buford and Yu [2010]\nadditionally includes an extensive list of various peer-to-peer systems. Aberer\net al. [2005] provide a reference architecture that allows for a more formal\ncomparison of the different types of peer-to-peer systems. A survey taken\nfrom the perspective of content distribution is provided by Androutsellis-\nTheotokis and Spinellis [2004]. Finally, Buford et al. [2009], Tarkoma [2010]\nand Vu et al. [2010] go beyond the level of surveys and form adequate text\nbooks for initial or further study.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 81\nthe logical organization of applications. In many business environments,\ndistributed processing is equivalent to organizing a client-server application\nas a multitiered architecture. We refer to this type of distribution as vertical\ndistribution . The characteristic feature of vertical distribution is that it is\nachieved by placing logically different components on different machines. The\nterm is related to the concept of vertical fragmentation as used in distributed\nrelational databases, where it means that tables are split columnwise, and\nsubsequently distributed across multiple machines [\u00d6zsu and Valduriez, 2011].\nAgain, from a systems-management perspective, having a vertical distri-\nbution can help: functions are logically and physically split across multiple\nmachines, where each machine is tailored to a speci\ufb01c group of functions.\nHowever, vertical distribution is only one way of organizing client-server\napplications. In modern architectures, it is often the distribution of the clients\nand the servers that counts, which we refer to as horizontal distribution . In\nthis type of distribution, a client or server may be physically split up into\nlogically equivalent parts, but each part is operating on its own share of the\ncomplete data set, thus balancing the load. In this section we will take a look\nat a class of modern system architectures that support horizontal distribution,\nknown as peer-to-peer systems .\nFrom a high-level perspective, the processes that constitute a peer-to-peer\nsystem are all equal. This means that the functions that need to be carried\nout are represented by every process that constitutes the distributed system.\nAs a consequence, much of the interaction between processes is symmetric:\neach process will act as a client and a server at the same time (which is also\nreferred to as acting as a servant ).\nGiven this symmetric behavior, peer-to-peer architectures evolve around\nthe question how to organize the processes in an overlay network [Tarkoma,\n2010]: a network in which the nodes are formed by the processes and the links\nrepresent the possible communication channels (which are often realized as\nTCP connections). A node may not be able to communicate directly with an\narbitrary other node, but is required to send messages through the available\ncommunication channels. Two types of overlay networks exist: those that are\nstructured and those that are not. These two types are surveyed extensively\nin Lua et al. [2005] along with numerous examples. Buford and Yu [2010]\nadditionally includes an extensive list of various peer-to-peer systems. Aberer\net al. [2005] provide a reference architecture that allows for a more formal\ncomparison of the different types of peer-to-peer systems. A survey taken\nfrom the perspective of content distribution is provided by Androutsellis-\nTheotokis and Spinellis [2004]. Finally, Buford et al. [2009], Tarkoma [2010]\nand Vu et al. [2010] go beyond the level of surveys and form adequate text\nbooks for initial or further study.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "82 CHAPTER 2. ARCHITECTURES\nStructured peer-to-peer systems\nAs its name suggests, in a structured peer-to-peer system the nodes (i.e.,\nprocesses) are organized in an overlay that adheres to a speci\ufb01c, deterministic\ntopology: a ring, a binary tree, a grid, etc. This topology is used to ef\ufb01ciently\nlook up data. Characteristic for structured peer-to-peer systems, is that they\nare generally based on using a so-called semantic-free index. What this means\nis that each data item that is to be maintained by the system, is uniquely\nassociated with a key, and that this key is subsequently used as an index. To\nthis end, it is common to use a hash function, so that we get:\nkey(data item ) =hash(data item\u2019s value ).\nThe peer-to-peer system as a whole is now responsible for storing ( key,value )\npairs. To this end, each node is assigned an identi\ufb01er from the same set\nof all possible hash values, and each node is made responsible for storing\ndata associated with a speci\ufb01c subset of keys. In essence, the system is\nthus seen to implement a distributed hash table , generally abbreviated to a\nDHT [Balakrishnan et al., 2003].\nFollowing this approach now reduces the essence of structured peer-to-\npeer systems to being able to look up a data item by means of its key. That\nis, the system provides an ef\ufb01cient implementation of a function lookup that\nmaps a key to an existing node:\nexisting node =lookup (key).\nThis is where the topology of a structured peer-to-peer system plays a crucial\nrole. Any node can be asked to look up a given key, which then boils down to\nef\ufb01ciently routing that lookup request to the node responsible for storing the\ndata associated with the given key.\nTo clarify these matters, let us consider a simple peer-to-peer system with\na\ufb01xed number of nodes, organized into a hypercube. A hypercube is an\nn-dimensional cube. The hypercube shown in Figure 2.18 is four-dimensional.\nIt can be thought of as two ordinary cubes, each with 8 vertices and 12 edges.\nTo expand the hypercube to \ufb01ve dimensions, we would add another set of\ntwo interconnected cubes to the \ufb01gure, connect the corresponding edges in\nthe two halves, and so on.\nFor this (admittedly naive) system, each data item is associated with one\nof the 16 nodes. This can be achieved by hashing the value of a data item to a\nkey k2f0, 1, 2, . . ., 24\u00001g. Now suppose that the node with identi\ufb01er 0111\nis requested to look up the data having key 14, corresponding to the binary\nvalue 1110 . In this example, we assume that the node with identi\ufb01er 1110 is\nresponsible for storing all data items that have key 14. What node 0111 can\nsimply do, is forward the request to a neighbor that is closer to node 1110 .\nIn this case, this is either node 0110 or node 1111 . If it picks node 0110 , that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n82 CHAPTER 2. ARCHITECTURES\nStructured peer-to-peer systems\nAs its name suggests, in a structured peer-to-peer system the nodes (i.e.,\nprocesses) are organized in an overlay that adheres to a speci\ufb01c, deterministic\ntopology: a ring, a binary tree, a grid, etc. This topology is used to ef\ufb01ciently\nlook up data. Characteristic for structured peer-to-peer systems, is that they\nare generally based on using a so-called semantic-free index. What this means\nis that each data item that is to be maintained by the system, is uniquely\nassociated with a key, and that this key is subsequently used as an index. To\nthis end, it is common to use a hash function, so that we get:\nkey(data item ) =hash(data item\u2019s value ).\nThe peer-to-peer system as a whole is now responsible for storing ( key,value )\npairs. To this end, each node is assigned an identi\ufb01er from the same set\nof all possible hash values, and each node is made responsible for storing\ndata associated with a speci\ufb01c subset of keys. In essence, the system is\nthus seen to implement a distributed hash table , generally abbreviated to a\nDHT [Balakrishnan et al., 2003].\nFollowing this approach now reduces the essence of structured peer-to-\npeer systems to being able to look up a data item by means of its key. That\nis, the system provides an ef\ufb01cient implementation of a function lookup that\nmaps a key to an existing node:\nexisting node =lookup (key).\nThis is where the topology of a structured peer-to-peer system plays a crucial\nrole. Any node can be asked to look up a given key, which then boils down to\nef\ufb01ciently routing that lookup request to the node responsible for storing the\ndata associated with the given key.\nTo clarify these matters, let us consider a simple peer-to-peer system with\na\ufb01xed number of nodes, organized into a hypercube. A hypercube is an\nn-dimensional cube. The hypercube shown in Figure 2.18 is four-dimensional.\nIt can be thought of as two ordinary cubes, each with 8 vertices and 12 edges.\nTo expand the hypercube to \ufb01ve dimensions, we would add another set of\ntwo interconnected cubes to the \ufb01gure, connect the corresponding edges in\nthe two halves, and so on.\nFor this (admittedly naive) system, each data item is associated with one\nof the 16 nodes. This can be achieved by hashing the value of a data item to a\nkey k2f0, 1, 2, . . ., 24\u00001g. Now suppose that the node with identi\ufb01er 0111\nis requested to look up the data having key 14, corresponding to the binary\nvalue 1110 . In this example, we assume that the node with identi\ufb01er 1110 is\nresponsible for storing all data items that have key 14. What node 0111 can\nsimply do, is forward the request to a neighbor that is closer to node 1110 .\nIn this case, this is either node 0110 or node 1111 . If it picks node 0110 , that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 83\nFigure 2.18: A simple peer-to-peer system organized as a four-dimensional\nhypercube.\nnode will then forward the request directly to node 1110 from where the data\ncan be retrieved.\nNote 2.5 (Example: The Chord system)\nFigure 2.19: The organization of nodes and data items in Chord.\nThe previous example illustrates two things: (1) the use of a hashing function\nto identify the node responsible for storing some data item, and (2) the routing\nalong the topology of a peer-to-peer system when looking up a data item given\nits key. However, it is not a very realistic example, if only for the reason that we\nassumed that the total set of nodes is \ufb01xed. Let us therefore consider a more\nrealistic example of a structured peer-to-peer system that is also used in practice.\nIn the Chord system [Stoica et al., 2003] the nodes are logically organized\nin a ring such that a data item with an m-bit key kis mapped to the node with\nthe smallest (again, also mbit) identi\ufb01er id\u0015k. This node is referred to as the\nsuccessor of key kand denoted as succ(k). Keys and identi\ufb01ers are typically 128\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 83\nFigure 2.18: A simple peer-to-peer system organized as a four-dimensional\nhypercube.\nnode will then forward the request directly to node 1110 from where the data\ncan be retrieved.\nNote 2.5 (Example: The Chord system)\nFigure 2.19: The organization of nodes and data items in Chord.\nThe previous example illustrates two things: (1) the use of a hashing function\nto identify the node responsible for storing some data item, and (2) the routing\nalong the topology of a peer-to-peer system when looking up a data item given\nits key. However, it is not a very realistic example, if only for the reason that we\nassumed that the total set of nodes is \ufb01xed. Let us therefore consider a more\nrealistic example of a structured peer-to-peer system that is also used in practice.\nIn the Chord system [Stoica et al., 2003] the nodes are logically organized\nin a ring such that a data item with an m-bit key kis mapped to the node with\nthe smallest (again, also mbit) identi\ufb01er id\u0015k. This node is referred to as the\nsuccessor of key kand denoted as succ(k). Keys and identi\ufb01ers are typically 128\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "84 CHAPTER 2. ARCHITECTURES\nor 160 bits long. Figure 2.19 shows a much smaller Chord ring, where m=5and\nwith nine nodesf1,4,9,11,14,18,20,21,28g. The successor of key 7is equal to\n9. Likewise, succ(5) =9, but also succ(9) =9. In Chord, each node maintains\nshortcuts to other nodes. A shortcut appears as a directed edge from one node\nto another. How these shortcuts are constructed is explained in Chapter 5. The\nconstruction is done in such a way that the length of the shortest path between\nany pair of nodes is of order O(logN), where Nis the total number of nodes.\nTo look up a key, a node will try to forward the request \u201cas far as possible,\u201d but\nwithout passing it beyond the node responsible for that key. To clarify, suppose\nthat in our example Chord ring, node 9is asked to look up the node responsible\nfor key 3(which is node 4). Node 9has four shortcuts: to nodes 11,14,18, and 28,\nrespectively. As node 28is the farthest node 9knows about and still preceding\nthe one responsible for key 3, it will get the lookup request. Node 28has three\nshortcuts: to nodes 1,4, and 14, respectively. Note that node 28has no knowledge\nabout the existence of nodes between nodes 1and 4. For this reason, the best\nwhat it can do is forward the request to node 1. The latter knows that its successor\nin the ring is node 4, and thus that this is the node responsible for key 3, to which\nit will subsequently forward the request.\nNow suppose that a node, with the unique identi\ufb01er u, wants to join a Chord\noverlay. To that end, it contacts an arbitrary node and requests it to look up u,\nthat is, return the value v =succ(u). At that point, node uwill simply need to\ninsert itself between the predecessor of vand vitself, thus becoming the new\npredecessor of v. During this process, shortcuts from uto other nodes will be\nestablished, but also some existing ones previously directed toward vwill now be\nadjusted to point to u(again, details are deferred until later chapters). Obviously,\nany data item with key kstored at vbut for which succ(k)is now equal to uis\ntransferred from vtou.\nLeaving is just as simple: node uinforms its departure to its predecessor and\nsuccessor, and transfers its data items to succ(u).\nUnstructured peer-to-peer systems\nStructured peer-to-peer systems attempt to maintain a speci\ufb01c, deterministic\noverlay network. In contrast, in an unstructured peer-to-peer system each\nnode maintains an ad hoc list of neighbors. The resulting overlay resembles\nwhat is known as a random graph : a graph in which an edge hu,vibetween\ntwo nodes uand vexists only with a certain probability P[hu,vi]. Ideally, this\nprobability is the same for all pairs of nodes, but in practice a wide range of\ndistributions is observed.\nIn an unstructured peer-to-peer system, when a node joins it often contacts\na well-known node to obtain a starting list of other peers in the system. This\nlist can then be used to \ufb01nd more peers, and perhaps ignore others, and so\non. In practice, a node generally changes its local list almost continuously.\nFor example, a node may discover that a neighbor is no longer responsive\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n84 CHAPTER 2. ARCHITECTURES\nor 160 bits long. Figure 2.19 shows a much smaller Chord ring, where m=5and\nwith nine nodesf1,4,9,11,14,18,20,21,28g. The successor of key 7is equal to\n9. Likewise, succ(5) =9, but also succ(9) =9. In Chord, each node maintains\nshortcuts to other nodes. A shortcut appears as a directed edge from one node\nto another. How these shortcuts are constructed is explained in Chapter 5. The\nconstruction is done in such a way that the length of the shortest path between\nany pair of nodes is of order O(logN), where Nis the total number of nodes.\nTo look up a key, a node will try to forward the request \u201cas far as possible,\u201d but\nwithout passing it beyond the node responsible for that key. To clarify, suppose\nthat in our example Chord ring, node 9is asked to look up the node responsible\nfor key 3(which is node 4). Node 9has four shortcuts: to nodes 11,14,18, and 28,\nrespectively. As node 28is the farthest node 9knows about and still preceding\nthe one responsible for key 3, it will get the lookup request. Node 28has three\nshortcuts: to nodes 1,4, and 14, respectively. Note that node 28has no knowledge\nabout the existence of nodes between nodes 1and 4. For this reason, the best\nwhat it can do is forward the request to node 1. The latter knows that its successor\nin the ring is node 4, and thus that this is the node responsible for key 3, to which\nit will subsequently forward the request.\nNow suppose that a node, with the unique identi\ufb01er u, wants to join a Chord\noverlay. To that end, it contacts an arbitrary node and requests it to look up u,\nthat is, return the value v =succ(u). At that point, node uwill simply need to\ninsert itself between the predecessor of vand vitself, thus becoming the new\npredecessor of v. During this process, shortcuts from uto other nodes will be\nestablished, but also some existing ones previously directed toward vwill now be\nadjusted to point to u(again, details are deferred until later chapters). Obviously,\nany data item with key kstored at vbut for which succ(k)is now equal to uis\ntransferred from vtou.\nLeaving is just as simple: node uinforms its departure to its predecessor and\nsuccessor, and transfers its data items to succ(u).\nUnstructured peer-to-peer systems\nStructured peer-to-peer systems attempt to maintain a speci\ufb01c, deterministic\noverlay network. In contrast, in an unstructured peer-to-peer system each\nnode maintains an ad hoc list of neighbors. The resulting overlay resembles\nwhat is known as a random graph : a graph in which an edge hu,vibetween\ntwo nodes uand vexists only with a certain probability P[hu,vi]. Ideally, this\nprobability is the same for all pairs of nodes, but in practice a wide range of\ndistributions is observed.\nIn an unstructured peer-to-peer system, when a node joins it often contacts\na well-known node to obtain a starting list of other peers in the system. This\nlist can then be used to \ufb01nd more peers, and perhaps ignore others, and so\non. In practice, a node generally changes its local list almost continuously.\nFor example, a node may discover that a neighbor is no longer responsive\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 85\nand that it needs to be replaced. There may be other reasons, which we will\ndescribe shortly.\nUnlike structured peer-to-peer systems, looking up data cannot follow a\npredetermined route when lists of neighbors are constructed in an ad hoc\nfashion. Instead, in an unstructured peer-to-peer systems we really need\nto resort to searching for data [Risson and Moors, 2006]. Let us look at two\nextremes and consider the case in which we are requested to search for speci\ufb01c\ndata (e.g., identi\ufb01ed by keywords).\nFlooding: In the case of \ufb02ooding, an issuing node usimply passes a request\nfor a data item to all its neighbors. A request will be ignored when its\nreceiving node, say v, had seen it before. Otherwise, vsearches locally\nfor the requested data item. If vhas the required data, it can either\nrespond directly to the issuing node u, or send it back to the original\nforwarder, who will then return it to itsoriginal forwarder, and so on. If\nvdoes not have the requested data, it forwards the request to all of its\nown neighbors.\nObviously, \ufb02ooding can be very expensive, for which reason a request\noften has an associated time-to-live orTTL value, giving the maximum\nnumber of hops a request is allowed to be forwarded. Choosing the\nright TTL value is crucial: too small means that a request will stay close\nto the issuer and may thus not reach a node having the data. Too large\nincurs high communication costs.\nAs an alternative to setting TTL values, a node can also start a search\nwith an initial TTL value of 1, meaning that it will \ufb01rst query only its\nneighbors. If no, or not enough results are returned, the TTL is increased\nand a new search is initiated.\nRandom walks: At the other end of the search spectrum, an issuing node u\ncan simply try to \ufb01nd a data item by asking a randomly chosen neighbor,\nsayv. Ifvdoes not have the data, it forwards the request to one of its\nrandomly chosen neighbors, and so on. The result is known as a random\nwalk [Gkantsidis et al., 2006; Lv et al., 2002]. Obviously, a random walk\nimposes much less network traf\ufb01c, yet it may take much longer before\na node is reached that has the requested data. To decrease the waiting\ntime, an issuer can simply start nrandom walks simultaneously. Indeed,\nstudies show that in this case, the time it takes before reaching a node\nthat has the data drops approximately by a factor n. Lv et al. [2002]\nreport that relatively small values of n, such as 16 or 64, turn out to be\neffective.\nA random walk also needs to be stopped. To this end, we can either again\nuse a TTL, or alternatively, when a node receives a lookup request, check\nwith the issuer whether forwarding the request to another randomly\nselected neighbor is still needed.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 85\nand that it needs to be replaced. There may be other reasons, which we will\ndescribe shortly.\nUnlike structured peer-to-peer systems, looking up data cannot follow a\npredetermined route when lists of neighbors are constructed in an ad hoc\nfashion. Instead, in an unstructured peer-to-peer systems we really need\nto resort to searching for data [Risson and Moors, 2006]. Let us look at two\nextremes and consider the case in which we are requested to search for speci\ufb01c\ndata (e.g., identi\ufb01ed by keywords).\nFlooding: In the case of \ufb02ooding, an issuing node usimply passes a request\nfor a data item to all its neighbors. A request will be ignored when its\nreceiving node, say v, had seen it before. Otherwise, vsearches locally\nfor the requested data item. If vhas the required data, it can either\nrespond directly to the issuing node u, or send it back to the original\nforwarder, who will then return it to itsoriginal forwarder, and so on. If\nvdoes not have the requested data, it forwards the request to all of its\nown neighbors.\nObviously, \ufb02ooding can be very expensive, for which reason a request\noften has an associated time-to-live orTTL value, giving the maximum\nnumber of hops a request is allowed to be forwarded. Choosing the\nright TTL value is crucial: too small means that a request will stay close\nto the issuer and may thus not reach a node having the data. Too large\nincurs high communication costs.\nAs an alternative to setting TTL values, a node can also start a search\nwith an initial TTL value of 1, meaning that it will \ufb01rst query only its\nneighbors. If no, or not enough results are returned, the TTL is increased\nand a new search is initiated.\nRandom walks: At the other end of the search spectrum, an issuing node u\ncan simply try to \ufb01nd a data item by asking a randomly chosen neighbor,\nsayv. Ifvdoes not have the data, it forwards the request to one of its\nrandomly chosen neighbors, and so on. The result is known as a random\nwalk [Gkantsidis et al., 2006; Lv et al., 2002]. Obviously, a random walk\nimposes much less network traf\ufb01c, yet it may take much longer before\na node is reached that has the requested data. To decrease the waiting\ntime, an issuer can simply start nrandom walks simultaneously. Indeed,\nstudies show that in this case, the time it takes before reaching a node\nthat has the data drops approximately by a factor n. Lv et al. [2002]\nreport that relatively small values of n, such as 16 or 64, turn out to be\neffective.\nA random walk also needs to be stopped. To this end, we can either again\nuse a TTL, or alternatively, when a node receives a lookup request, check\nwith the issuer whether forwarding the request to another randomly\nselected neighbor is still needed.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "86 CHAPTER 2. ARCHITECTURES\nNote that neither method relies on a speci\ufb01c comparison technique to decide\nwhen requested data has been found. For structured peer-to-peer systems,\nwe assumed the use of keys for comparison; for the two approaches just\ndescribed, any comparison technique would suf\ufb01ce.\nBetween \ufb02ooding and random walks lie policy-based search methods .\nFor example, a node may decide to keep track of peers who responded\npositively, effectively turning them into preferred neighbors for succeeding\nqueries. Likewise, we may want to restrict \ufb02ooding to fewer neighbors, but in\nany case give preference to neighbors having many neighbors themselves.\nNote 2.6 (Advanced: Flooding versus random walks)\nWhen giving the matter some thought, it may come as a surprise that people have\neven considered a random walk as an alternative way to search. At \ufb01rst instance,\nit would seem like a technique resembling the search for a needle in a haystack.\nHowever, we need to realize that in practice we are dealing with replicated data,\nand even for very small replication factors and different replication distributions,\nstudies show that deploying random walks is not only effective, it can also be\nmuch more ef\ufb01cient in comparison to \ufb02ooding.\nTo see why, we closely follow the model described in Lv et al. [2002] and\nCohen and Shenker [2002]. Assume there are a total of Nnodes and that each data\nitem is replicated across rrandomly chosen nodes. A search consists of repeatedly\nselecting a node at random until the item is found. If P[k]is the probability that\nthe item is found after kattempts, we have\nP[k] =r\nN(1\u0000r\nN)k\u00001.\nLet the average search size Sbe the expected number of nodes that need to be\nprobed before \ufb01nding the requested data item:\nS=N\n\u00e5\nk=1k\u0001P[k] =N\n\u00e5\nk=1k\u0001r\nN(1\u0000r\nN)k\u00001\u0019N/rfor 1\u001cr\u0014N.\nBy simply replicating every data item to each node, S=1and it is clear that a\nrandom walk will always outperform \ufb02ooding even for TTL values of 1. More\nrealistically, however, is to assume that r/Nis relatively low, such as 0.1%,\nmeaning that the average search size would be approximately 1000 nodes.\nTo compare this to \ufb02ooding, assume that each node, on average, forwards a\nrequest to drandomly selected neighbors. After one step, the request will have\narrived at dnodes, each of who will forward it to another d\u00001nodes (assuming\nthat the node from where the request came is skipped), and so on. In other words,\nafter ksteps, and taking into account that a node can receive the request more\nthan once, we will have reached (at most)\nR(k) =d(d\u00001)k\u00001\nnodes. Various studies show that R(k)is a good estimate for the actual number of\nnodes reached, as long as we have only a few number of \ufb02ooding steps. Of these\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n86 CHAPTER 2. ARCHITECTURES\nNote that neither method relies on a speci\ufb01c comparison technique to decide\nwhen requested data has been found. For structured peer-to-peer systems,\nwe assumed the use of keys for comparison; for the two approaches just\ndescribed, any comparison technique would suf\ufb01ce.\nBetween \ufb02ooding and random walks lie policy-based search methods .\nFor example, a node may decide to keep track of peers who responded\npositively, effectively turning them into preferred neighbors for succeeding\nqueries. Likewise, we may want to restrict \ufb02ooding to fewer neighbors, but in\nany case give preference to neighbors having many neighbors themselves.\nNote 2.6 (Advanced: Flooding versus random walks)\nWhen giving the matter some thought, it may come as a surprise that people have\neven considered a random walk as an alternative way to search. At \ufb01rst instance,\nit would seem like a technique resembling the search for a needle in a haystack.\nHowever, we need to realize that in practice we are dealing with replicated data,\nand even for very small replication factors and different replication distributions,\nstudies show that deploying random walks is not only effective, it can also be\nmuch more ef\ufb01cient in comparison to \ufb02ooding.\nTo see why, we closely follow the model described in Lv et al. [2002] and\nCohen and Shenker [2002]. Assume there are a total of Nnodes and that each data\nitem is replicated across rrandomly chosen nodes. A search consists of repeatedly\nselecting a node at random until the item is found. If P[k]is the probability that\nthe item is found after kattempts, we have\nP[k] =r\nN(1\u0000r\nN)k\u00001.\nLet the average search size Sbe the expected number of nodes that need to be\nprobed before \ufb01nding the requested data item:\nS=N\n\u00e5\nk=1k\u0001P[k] =N\n\u00e5\nk=1k\u0001r\nN(1\u0000r\nN)k\u00001\u0019N/rfor 1\u001cr\u0014N.\nBy simply replicating every data item to each node, S=1and it is clear that a\nrandom walk will always outperform \ufb02ooding even for TTL values of 1. More\nrealistically, however, is to assume that r/Nis relatively low, such as 0.1%,\nmeaning that the average search size would be approximately 1000 nodes.\nTo compare this to \ufb02ooding, assume that each node, on average, forwards a\nrequest to drandomly selected neighbors. After one step, the request will have\narrived at dnodes, each of who will forward it to another d\u00001nodes (assuming\nthat the node from where the request came is skipped), and so on. In other words,\nafter ksteps, and taking into account that a node can receive the request more\nthan once, we will have reached (at most)\nR(k) =d(d\u00001)k\u00001\nnodes. Various studies show that R(k)is a good estimate for the actual number of\nnodes reached, as long as we have only a few number of \ufb02ooding steps. Of these\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 87\nnodes, we can expect a fraction of r/Nto have the requested data item, meaning\nthat whenr\nN\u0001R(k)\u00151, we will most likely have found a node that has the data\nitem.\nTo illustrate, let r/N=0.001 =0.1% , which means that S\u00191000 . With\n\ufb02ooding to, on average, d=10neighbors, we would require at least 4 \ufb02ooding\nsteps, reaching some 7290 nodes, which is considerably more that the 1000 nodes\nrequired when using a random walk. Only with d=33will we need to contact\napproximately also 1000 nodes in k=2\ufb02ooding steps and having r/N\u0001R(k)\u00151.\nThe obvious drawback of deploying random walks, is that it may take much\nlonger before an answer is returned.\nHierarchically organized peer-to-peer networks\nNotably in unstructured peer-to-peer systems, locating relevant data items\ncan become problematic as the network grows. The reason for this scalability\nproblem is simple: as there is no deterministic way of routing a lookup request\nto a speci\ufb01c data item, essentially the only technique a node can resort to is\nsearching for the request by means of \ufb02ooding or randomly walking through\nthe network. As an alternative many peer-to-peer systems have proposed to\nmake use of special nodes that maintain an index of data items.\nThere are other situations in which abandoning the symmetric nature\nof peer-to-peer systems is sensible. Consider a collaboration of nodes that\noffer resources to each other. For example, in a collaborative content delivery\nnetwork (CDN ), nodes may offer storage for hosting copies of Web documents\nallowing Web clients to access pages nearby, and thus to access them quickly.\nWhat is needed is a means to \ufb01nd out where documents can be stored best.\nIn that case, making use of a broker that collects data on resource usage and\navailability for a number of nodes that are in each other\u2019s proximity will allow\nto quickly select a node with suf\ufb01cient resources.\nNodes such as those maintaining an index or acting as a broker are gener-\nally referred to as super peers . As the name suggests, super peers are often\nalso organized in a peer-to-peer network, leading to a hierarchical organiza-\ntion as explained in Yang and Garcia-Molina [2003]. A simple example of such\nan organization is shown in Figure 2.20. In this organization, every regular\npeer, now referred to as a weak peer , is connected as a client to a super peer.\nAll communication from and to a weak peer proceeds through that peer\u2019s\nassociated super peer.\nIn many cases, the association between a weak peer and its super peer\nis \ufb01xed: whenever a weak peer joins the network, it attaches to one of the\nsuper peers and remains attached until it leaves the network. Obviously, it is\nexpected that super peers are long-lived processes with high availability. To\ncompensate for potential unstable behavior of a super peer, backup schemes\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 87\nnodes, we can expect a fraction of r/Nto have the requested data item, meaning\nthat whenr\nN\u0001R(k)\u00151, we will most likely have found a node that has the data\nitem.\nTo illustrate, let r/N=0.001 =0.1% , which means that S\u00191000 . With\n\ufb02ooding to, on average, d=10neighbors, we would require at least 4 \ufb02ooding\nsteps, reaching some 7290 nodes, which is considerably more that the 1000 nodes\nrequired when using a random walk. Only with d=33will we need to contact\napproximately also 1000 nodes in k=2\ufb02ooding steps and having r/N\u0001R(k)\u00151.\nThe obvious drawback of deploying random walks, is that it may take much\nlonger before an answer is returned.\nHierarchically organized peer-to-peer networks\nNotably in unstructured peer-to-peer systems, locating relevant data items\ncan become problematic as the network grows. The reason for this scalability\nproblem is simple: as there is no deterministic way of routing a lookup request\nto a speci\ufb01c data item, essentially the only technique a node can resort to is\nsearching for the request by means of \ufb02ooding or randomly walking through\nthe network. As an alternative many peer-to-peer systems have proposed to\nmake use of special nodes that maintain an index of data items.\nThere are other situations in which abandoning the symmetric nature\nof peer-to-peer systems is sensible. Consider a collaboration of nodes that\noffer resources to each other. For example, in a collaborative content delivery\nnetwork (CDN ), nodes may offer storage for hosting copies of Web documents\nallowing Web clients to access pages nearby, and thus to access them quickly.\nWhat is needed is a means to \ufb01nd out where documents can be stored best.\nIn that case, making use of a broker that collects data on resource usage and\navailability for a number of nodes that are in each other\u2019s proximity will allow\nto quickly select a node with suf\ufb01cient resources.\nNodes such as those maintaining an index or acting as a broker are gener-\nally referred to as super peers . As the name suggests, super peers are often\nalso organized in a peer-to-peer network, leading to a hierarchical organiza-\ntion as explained in Yang and Garcia-Molina [2003]. A simple example of such\nan organization is shown in Figure 2.20. In this organization, every regular\npeer, now referred to as a weak peer , is connected as a client to a super peer.\nAll communication from and to a weak peer proceeds through that peer\u2019s\nassociated super peer.\nIn many cases, the association between a weak peer and its super peer\nis \ufb01xed: whenever a weak peer joins the network, it attaches to one of the\nsuper peers and remains attached until it leaves the network. Obviously, it is\nexpected that super peers are long-lived processes with high availability. To\ncompensate for potential unstable behavior of a super peer, backup schemes\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "88 CHAPTER 2. ARCHITECTURES\nFigure 2.20: A hierarchical organization of nodes into a super-peer network.\ncan be deployed, such as pairing every super peer with another one and\nrequiring weak peers to attach to both.\nHaving a \ufb01xed association with a super peer may not always be the best\nsolution. For example, in the case of \ufb01le-sharing networks, it may be better\nfor a weak peer to attach to a super peer that maintains an index of \ufb01les\nthat the weak peer is currently interested in. In that case, chances are bigger\nthat when a weak peer is looking for a speci\ufb01c \ufb01le, its super peer will know\nwhere to \ufb01nd it. Garbacki et al. [2010] describe a relatively simple scheme in\nwhich the association between weak peer and strong peer can change as weak\npeers discover better super peers to associate with. In particular, a super peer\nreturning the result of a lookup operation is given preference over other super\npeers.\nAs we have seen, peer-to-peer networks offer a \ufb02exible means for nodes\nto join and leave the network. However, with super-peer networks a new\nproblem is introduced, namely how to select the nodes that are eligible to\nbecome super peer. This problem is closely related to the leader-election\nproblem , which we discuss in Section 6.4.\nNote 2.7 (Example: The Skype network)\nTo give a concrete example of a hierarchically organized peer-to-peer network,\nlet us take a closer look at one of the most successful ones: the Skype VOIP\nnetwork. Although there are no of\ufb01cial publications on how Skype is organized,\nBaset and Schulzrinne [2006] have experimentally analyzed the working of Skype\nfrom which they deduced an architecture. Apart from how accurate their \ufb01ndings\nare (Skype has evolved considerably since the time of the Baset and Schulzrinne\npublication), it remains an interesting case study.\nThe Skype network is organized very similar to what is shown in Figure 2.20,\nwith one important difference: there is an additional centralized Skype login\nserver , to which every peer can communicate (i.e., weak as well as super peers).\nSuper peers (known as Skype super nodes ) are crucial to the working of the\nsystem as a whole. Besides the login server, there are a number of default Skype\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n88 CHAPTER 2. ARCHITECTURES\nFigure 2.20: A hierarchical organization of nodes into a super-peer network.\ncan be deployed, such as pairing every super peer with another one and\nrequiring weak peers to attach to both.\nHaving a \ufb01xed association with a super peer may not always be the best\nsolution. For example, in the case of \ufb01le-sharing networks, it may be better\nfor a weak peer to attach to a super peer that maintains an index of \ufb01les\nthat the weak peer is currently interested in. In that case, chances are bigger\nthat when a weak peer is looking for a speci\ufb01c \ufb01le, its super peer will know\nwhere to \ufb01nd it. Garbacki et al. [2010] describe a relatively simple scheme in\nwhich the association between weak peer and strong peer can change as weak\npeers discover better super peers to associate with. In particular, a super peer\nreturning the result of a lookup operation is given preference over other super\npeers.\nAs we have seen, peer-to-peer networks offer a \ufb02exible means for nodes\nto join and leave the network. However, with super-peer networks a new\nproblem is introduced, namely how to select the nodes that are eligible to\nbecome super peer. This problem is closely related to the leader-election\nproblem , which we discuss in Section 6.4.\nNote 2.7 (Example: The Skype network)\nTo give a concrete example of a hierarchically organized peer-to-peer network,\nlet us take a closer look at one of the most successful ones: the Skype VOIP\nnetwork. Although there are no of\ufb01cial publications on how Skype is organized,\nBaset and Schulzrinne [2006] have experimentally analyzed the working of Skype\nfrom which they deduced an architecture. Apart from how accurate their \ufb01ndings\nare (Skype has evolved considerably since the time of the Baset and Schulzrinne\npublication), it remains an interesting case study.\nThe Skype network is organized very similar to what is shown in Figure 2.20,\nwith one important difference: there is an additional centralized Skype login\nserver , to which every peer can communicate (i.e., weak as well as super peers).\nSuper peers (known as Skype super nodes ) are crucial to the working of the\nsystem as a whole. Besides the login server, there are a number of default Skype\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 89\nsuper peers that can be used to get started when a weak peer starts from scratch.\nIt appears that the address of each of these default super peers is hard-coded in\nthe Skype software. An address consists of an (IP address, port number)-pair.\nEach weak peer has a local list of addresses of reachable super peers, called\nitshost cache . If none of the cached super peers is reachable, it tries to connect to\none of the default super peers. The host cache is designed to accommodate a few\nhundred addresses. To connect to the Skype network, a weak peer is required to\nestablish a TCP connection with a super peer. This is important, notably when\na peer is operating behind a (NATed) \ufb01rewall, as the super peer can assist in\nactually contacting that peer.\nLet us \ufb01rst consider the situation that one peer Awants to contact another\n(weak) peer Bfor which it has a contact address. We need to distinguish three\ncases, all related to the situation whether or not peers are behind (NATed) \ufb01rewalls.\nBoth AandBare on the public Internet: Being on the public Internet means\nthatAandBcan be directly contacted. In this case, a TCP connection is set\nup between AandBwhich is used to exchange control packets. The actual\ncall takes place using UDP packets between negotiated ports at the caller\nand callee, respectively.\nAoperates behind a \ufb01rewall, while Bis on the public Internet: In this case, A\nwill \ufb01rst set up a TCP connection to a super peer S, after which Swill set\nup a TCP connection to B. Again, the TCP connections are used to transfer\ncontrol packets between Aand B(via S), after which the actual call will\ntake place through UDP and directly between Aand Bwithout \ufb02owing\nthrough S. However, it seems that Sis needed to discover the correct pair\nof port numbers for the \ufb01rewall at Ato allow for UDP packet exchanges. In\nprinciple, this should also be possible with assistance of B.\nBoth AandBoperate behind a \ufb01rewall: This is the most challenging situation,\ncertainly if we also assume that the \ufb01rewalls restrict UDP traf\ufb01c. In this case,\nAwill connect to an online super peer Sthrough TCP , after which Swill\nset up a TCP connection to B. These connections are used for exchanging\ncontrol packets. For the actual call, another super peer is contacted that\nwill act as a relay R:Asets up a connection to R, and so will B. All voice\ntraf\ufb01c is then subsequently forwarded over the two TCP connections, and\nthrough R.\nHow do users \ufb01nd each other? As mentioned, the \ufb01rst thing a weak peer\nneeds to do is establish a TCP connection with a super peer. That super peer\nis either found in the local host cache, or obtained through one of the default\nsuper peers. In order to search for a speci\ufb01c user, a weak peer contacts its super\npeer, which will return a number of other peers to ask. If that did not lead to any\nresults, the super peer returns another (this time longer) list of peers to which\nthe search request should be forwarded. This process is repeated until the user\nis found or the requester concludes the user does not exist. Indeed, this can be\nviewed as a form of policy-based search as we mentioned above. Finding a user\nmeans that its address, or that of its associated super peer is returned. In principle,\nif the searched user is online, a VOIP connection can then be established.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 89\nsuper peers that can be used to get started when a weak peer starts from scratch.\nIt appears that the address of each of these default super peers is hard-coded in\nthe Skype software. An address consists of an (IP address, port number)-pair.\nEach weak peer has a local list of addresses of reachable super peers, called\nitshost cache . If none of the cached super peers is reachable, it tries to connect to\none of the default super peers. The host cache is designed to accommodate a few\nhundred addresses. To connect to the Skype network, a weak peer is required to\nestablish a TCP connection with a super peer. This is important, notably when\na peer is operating behind a (NATed) \ufb01rewall, as the super peer can assist in\nactually contacting that peer.\nLet us \ufb01rst consider the situation that one peer Awants to contact another\n(weak) peer Bfor which it has a contact address. We need to distinguish three\ncases, all related to the situation whether or not peers are behind (NATed) \ufb01rewalls.\nBoth AandBare on the public Internet: Being on the public Internet means\nthatAandBcan be directly contacted. In this case, a TCP connection is set\nup between AandBwhich is used to exchange control packets. The actual\ncall takes place using UDP packets between negotiated ports at the caller\nand callee, respectively.\nAoperates behind a \ufb01rewall, while Bis on the public Internet: In this case, A\nwill \ufb01rst set up a TCP connection to a super peer S, after which Swill set\nup a TCP connection to B. Again, the TCP connections are used to transfer\ncontrol packets between Aand B(via S), after which the actual call will\ntake place through UDP and directly between Aand Bwithout \ufb02owing\nthrough S. However, it seems that Sis needed to discover the correct pair\nof port numbers for the \ufb01rewall at Ato allow for UDP packet exchanges. In\nprinciple, this should also be possible with assistance of B.\nBoth AandBoperate behind a \ufb01rewall: This is the most challenging situation,\ncertainly if we also assume that the \ufb01rewalls restrict UDP traf\ufb01c. In this case,\nAwill connect to an online super peer Sthrough TCP , after which Swill\nset up a TCP connection to B. These connections are used for exchanging\ncontrol packets. For the actual call, another super peer is contacted that\nwill act as a relay R:Asets up a connection to R, and so will B. All voice\ntraf\ufb01c is then subsequently forwarded over the two TCP connections, and\nthrough R.\nHow do users \ufb01nd each other? As mentioned, the \ufb01rst thing a weak peer\nneeds to do is establish a TCP connection with a super peer. That super peer\nis either found in the local host cache, or obtained through one of the default\nsuper peers. In order to search for a speci\ufb01c user, a weak peer contacts its super\npeer, which will return a number of other peers to ask. If that did not lead to any\nresults, the super peer returns another (this time longer) list of peers to which\nthe search request should be forwarded. This process is repeated until the user\nis found or the requester concludes the user does not exist. Indeed, this can be\nviewed as a form of policy-based search as we mentioned above. Finding a user\nmeans that its address, or that of its associated super peer is returned. In principle,\nif the searched user is online, a VOIP connection can then be established.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "90 CHAPTER 2. ARCHITECTURES\nFinally, the Skype login server is used to make sure that only registered users\ncan make use of the network. It also ensures that user IDs are unique, and handles\nother administrative tasks that are dif\ufb01cult, if not practically impossible to handle\nin a decentralized manner.\nHybrid Architectures\nSo far, we have focused on client-server architectures and a number of peer-to-\npeer architectures. Many distributed systems combine architectural features,\nas we already came across in super-peer networks. In this section we take\na look at some speci\ufb01c classes of distributed systems in which client-server\nsolutions are combined with decentralized architectures.\nEdge-server systems\nAn important class of distributed systems that is organized according to\na hybrid architecture is formed by edge-server systems . These systems are\ndeployed on the Internet where servers are placed \u201cat the edge\u201d of the network.\nThis edge is formed by the boundary between enterprise networks and the\nactual Internet, for example, as provided by an Internet Service Provider\n(ISP). Likewise, where end users at home connect to the Internet through\ntheir ISP , the ISP can be considered as residing at the edge of the Internet.\nThis leads to a general organization like the one shown in Figure 2.21.\nFigure 2.21: Viewing the Internet as consisting of a collection of edge servers.\nEnd users, or clients in general, connect to the Internet by means of an\nedge server. The edge server\u2019s main purpose is to serve content, possibly after\napplying \ufb01ltering and transcoding functions. More interesting is the fact that\na collection of edge servers can be used to optimize content and application\ndistribution. The basic model is that for a speci\ufb01c organization, one edge\nserver acts as an origin server from which all content originates. That server\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n90 CHAPTER 2. ARCHITECTURES\nFinally, the Skype login server is used to make sure that only registered users\ncan make use of the network. It also ensures that user IDs are unique, and handles\nother administrative tasks that are dif\ufb01cult, if not practically impossible to handle\nin a decentralized manner.\nHybrid Architectures\nSo far, we have focused on client-server architectures and a number of peer-to-\npeer architectures. Many distributed systems combine architectural features,\nas we already came across in super-peer networks. In this section we take\na look at some speci\ufb01c classes of distributed systems in which client-server\nsolutions are combined with decentralized architectures.\nEdge-server systems\nAn important class of distributed systems that is organized according to\na hybrid architecture is formed by edge-server systems . These systems are\ndeployed on the Internet where servers are placed \u201cat the edge\u201d of the network.\nThis edge is formed by the boundary between enterprise networks and the\nactual Internet, for example, as provided by an Internet Service Provider\n(ISP). Likewise, where end users at home connect to the Internet through\ntheir ISP , the ISP can be considered as residing at the edge of the Internet.\nThis leads to a general organization like the one shown in Figure 2.21.\nFigure 2.21: Viewing the Internet as consisting of a collection of edge servers.\nEnd users, or clients in general, connect to the Internet by means of an\nedge server. The edge server\u2019s main purpose is to serve content, possibly after\napplying \ufb01ltering and transcoding functions. More interesting is the fact that\na collection of edge servers can be used to optimize content and application\ndistribution. The basic model is that for a speci\ufb01c organization, one edge\nserver acts as an origin server from which all content originates. That server\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 91\ncan use other edge servers for replicating Web pages and such [Leff and\nRay\ufb01eld, 2004; Nayate et al., 2004; Rabinovich and Spastscheck, 2002].\nThis concept of edge-server systems is now often taken a step further:\ntaking cloud computing as implemented in a data center as the core, additional\nservers at the edge of the network are used to assist in computations and\nstorage, essentially leading to distributed cloud systems. In the case of fog\ncomputing , even end-user devices form part of the system and are (partly)\ncontrolled by a cloud-service provider [Yi et al., 2015].\nCollaborative distributed systems\nHybrid structures are notably deployed in collaborative distributed systems.\nThe main issue in many of these systems is to \ufb01rst get started, for which often\na traditional client-server scheme is deployed. Once a node has joined the\nsystem, it can use a fully decentralized scheme for collaboration.\nTo make matters concrete, let us consider the widely popular BitTorrent\n\ufb01le-sharing system [Cohen, 2003]. BitTorrent is a peer-to-peer \ufb01le downloading\nsystem. Its principal working is shown in Figure 2.22. The basic idea is that\nwhen an end user is looking for a \ufb01le, he downloads chunks of the \ufb01le from\nother users until the downloaded chunks can be assembled together yielding\nthe complete \ufb01le. An important design goal was to ensure collaboration.\nIn most \ufb01le-sharing systems, a signi\ufb01cant fraction of participants merely\ndownload \ufb01les but otherwise contribute close to nothing [Adar and Huberman,\n2000; Saroiu et al., 2003; Yang et al., 2005], a phenomenon referred to as free\nriding . To prevent this situation, in BitTorrent a \ufb01le can be downloaded only\nwhen the downloading client is providing content to someone else.\nFigure 2.22: The principal working of BitTorrent [adapted with permission\nfrom Pouwelse et al. [2005]].\nTo download a \ufb01le, a user needs to access a global directory, which is\ngenerally just one of a few well-known Web sites. Such a directory contains\nreferences to what are called torrent \ufb01les. A torrent \ufb01le contains the informa-\ntion that is needed to download a speci\ufb01c \ufb01le. In particular, it contains a link\nto what is known as a tracker , which is a server that is keeping an accurate\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 91\ncan use other edge servers for replicating Web pages and such [Leff and\nRay\ufb01eld, 2004; Nayate et al., 2004; Rabinovich and Spastscheck, 2002].\nThis concept of edge-server systems is now often taken a step further:\ntaking cloud computing as implemented in a data center as the core, additional\nservers at the edge of the network are used to assist in computations and\nstorage, essentially leading to distributed cloud systems. In the case of fog\ncomputing , even end-user devices form part of the system and are (partly)\ncontrolled by a cloud-service provider [Yi et al., 2015].\nCollaborative distributed systems\nHybrid structures are notably deployed in collaborative distributed systems.\nThe main issue in many of these systems is to \ufb01rst get started, for which often\na traditional client-server scheme is deployed. Once a node has joined the\nsystem, it can use a fully decentralized scheme for collaboration.\nTo make matters concrete, let us consider the widely popular BitTorrent\n\ufb01le-sharing system [Cohen, 2003]. BitTorrent is a peer-to-peer \ufb01le downloading\nsystem. Its principal working is shown in Figure 2.22. The basic idea is that\nwhen an end user is looking for a \ufb01le, he downloads chunks of the \ufb01le from\nother users until the downloaded chunks can be assembled together yielding\nthe complete \ufb01le. An important design goal was to ensure collaboration.\nIn most \ufb01le-sharing systems, a signi\ufb01cant fraction of participants merely\ndownload \ufb01les but otherwise contribute close to nothing [Adar and Huberman,\n2000; Saroiu et al., 2003; Yang et al., 2005], a phenomenon referred to as free\nriding . To prevent this situation, in BitTorrent a \ufb01le can be downloaded only\nwhen the downloading client is providing content to someone else.\nFigure 2.22: The principal working of BitTorrent [adapted with permission\nfrom Pouwelse et al. [2005]].\nTo download a \ufb01le, a user needs to access a global directory, which is\ngenerally just one of a few well-known Web sites. Such a directory contains\nreferences to what are called torrent \ufb01les. A torrent \ufb01le contains the informa-\ntion that is needed to download a speci\ufb01c \ufb01le. In particular, it contains a link\nto what is known as a tracker , which is a server that is keeping an accurate\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "92 CHAPTER 2. ARCHITECTURES\naccount of active nodes that have (chunks of) the requested \ufb01le. An active\nnode is one that is currently downloading the \ufb01le as well. Obviously, there\nwill be many different trackers, although there will generally be only a single\ntracker per \ufb01le (or collection of \ufb01les).\nOnce the nodes have been identi\ufb01ed from where chunks can be down-\nloaded, the downloading node effectively becomes active. At that point, it\nwill be forced to help others, for example by providing chunks of the \ufb01le it\nis downloading that others do not yet have. This enforcement comes from a\nvery simple rule: if node Pnotices that node Qis downloading more than it is\nuploading, Pcan decide to decrease the rate at which it sends data to Q. This\nscheme works well provided Phas something to download from Q. For this\nreason, nodes are often supplied with references to many other nodes putting\nthem in a better position to trade data.\nClearly, BitTorrent combines centralized with decentralized solutions. As\nit turns out, the bottleneck of the system is, not surprisingly, formed by the\ntrackers. In an alternative implementation of BitTorrent, a node also joins\na separate structured peer-to-peer system (i.e., a DHT) to assist in tracking\n\ufb01le downloads. In effect, a central tracker\u2019s load is now distributed across\nthe participating nodes, with each node acting as a tracker for a relatively\nsmall set of torrent \ufb01les. The original function of the tracker coordinating\nthe collaborative downloading of a \ufb01le is retained. However, we note that in\nmany BitTorrent systems used today, the tracking functionality has actually\nbeen minimized to a one-time provisioning of peers currently involved in\ndownloading the \ufb01le. From that moment on, the newly participating peer\nwill communicate only with those peers and no longer with the initial tracker.\nThe initial tracker for the requested \ufb01le is looked up in the DHT through a\nso-called magnet link . We return to DHT-based lookups in Section 5.2.\nNote 2.8 (Advanced: BitTorrent under the hood)\nOne of the acclaimed properties of BitTorrent is the enforced collaboration between\nnodes: getting a \ufb01le chunk requires providing one. In other words, there is a\nnatural incentive for BitTorrent clients to make data available to others, thus\ncircumventing the problems of free riding that other peer-to-peer systems have.\nTo understand how this mechanism works, let us delve a bit more into the\nBitTorrent protocol.\nWhen a peer Ahas found the tracker for a \ufb01le F, the tracker returns a subset\nof all the nodes currently involved in downloading F. The complete set of\ndownloading nodes for a speci\ufb01c \ufb01le Fis known as a swarm (for that \ufb01le); the\nsubset of nodes from that swarm with whom Acollaborates is called A\u2019sneighbor\nsetNA. Note that if Bis in NA, then Awill be a member of NB: being a neighbor\nis a symmetric relation. The neighbor set of a peer Ais periodically updated by\ncontacting the tracker, or when a new peer Bjoins the swarm and the tracker\npasses NAtoB.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n92 CHAPTER 2. ARCHITECTURES\naccount of active nodes that have (chunks of) the requested \ufb01le. An active\nnode is one that is currently downloading the \ufb01le as well. Obviously, there\nwill be many different trackers, although there will generally be only a single\ntracker per \ufb01le (or collection of \ufb01les).\nOnce the nodes have been identi\ufb01ed from where chunks can be down-\nloaded, the downloading node effectively becomes active. At that point, it\nwill be forced to help others, for example by providing chunks of the \ufb01le it\nis downloading that others do not yet have. This enforcement comes from a\nvery simple rule: if node Pnotices that node Qis downloading more than it is\nuploading, Pcan decide to decrease the rate at which it sends data to Q. This\nscheme works well provided Phas something to download from Q. For this\nreason, nodes are often supplied with references to many other nodes putting\nthem in a better position to trade data.\nClearly, BitTorrent combines centralized with decentralized solutions. As\nit turns out, the bottleneck of the system is, not surprisingly, formed by the\ntrackers. In an alternative implementation of BitTorrent, a node also joins\na separate structured peer-to-peer system (i.e., a DHT) to assist in tracking\n\ufb01le downloads. In effect, a central tracker\u2019s load is now distributed across\nthe participating nodes, with each node acting as a tracker for a relatively\nsmall set of torrent \ufb01les. The original function of the tracker coordinating\nthe collaborative downloading of a \ufb01le is retained. However, we note that in\nmany BitTorrent systems used today, the tracking functionality has actually\nbeen minimized to a one-time provisioning of peers currently involved in\ndownloading the \ufb01le. From that moment on, the newly participating peer\nwill communicate only with those peers and no longer with the initial tracker.\nThe initial tracker for the requested \ufb01le is looked up in the DHT through a\nso-called magnet link . We return to DHT-based lookups in Section 5.2.\nNote 2.8 (Advanced: BitTorrent under the hood)\nOne of the acclaimed properties of BitTorrent is the enforced collaboration between\nnodes: getting a \ufb01le chunk requires providing one. In other words, there is a\nnatural incentive for BitTorrent clients to make data available to others, thus\ncircumventing the problems of free riding that other peer-to-peer systems have.\nTo understand how this mechanism works, let us delve a bit more into the\nBitTorrent protocol.\nWhen a peer Ahas found the tracker for a \ufb01le F, the tracker returns a subset\nof all the nodes currently involved in downloading F. The complete set of\ndownloading nodes for a speci\ufb01c \ufb01le Fis known as a swarm (for that \ufb01le); the\nsubset of nodes from that swarm with whom Acollaborates is called A\u2019sneighbor\nsetNA. Note that if Bis in NA, then Awill be a member of NB: being a neighbor\nis a symmetric relation. The neighbor set of a peer Ais periodically updated by\ncontacting the tracker, or when a new peer Bjoins the swarm and the tracker\npasses NAtoB.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.3. SYSTEM ARCHITECTURE 93\nA node that has all chunks of Fand continues to participate in its swarm is\nknown as a seeder ; all others in the swarm are known as leechers .\nAs mentioned, each \ufb01le is divided into a number of equally sized chunks,\ncalled pieces in BitTorrent. A piece is typically 256 Kbyte, but other sizes are also\nused. The unit of data transfer between two peers is that of a block , which is\ntypically 16 Kbyte. A node can upload a block only if it has the entire piece to\nwhich that block belongs. A neighbor Bbelongs to the potential set PAofA, if it\nhas a block that Adoes not yet have, and vice versa . Again, if B2PAandA2PB,\nthen AandBare in a position that they can trade a block.\nWith this terminology, the downloading of a \ufb01le can be described in terms of\nthree phases, as detailed by Rai et al. [2007]:\nBootstrap phase: A node Ahas just received its \ufb01rst piece, placing it in a position\nto start trading blocks. This \ufb01rst piece is obtained through a mechanism\ncalled optimistic unchoking , by which nodes from NAunsel\ufb01shly provide\nthe blocks of a piece to get a newly arrived node started. If the potential set\nPAofAis empty, Awill have to wait until new neighbors are added to the\nset who may have pieces to exchange.\nTrading phase: In this phase,jPAj>0, meaning that there is always a peer\nwith whom Acan trade blocks. In practice, this is the phase in which the\ndownloading is highly ef\ufb01cient as enough nodes can be paired to exchange\ndata.\nLast download phase: Now the size of the potential set has dropped to zero\nagain, so that node Ais entirely dependent on newly arriving peers in its\nneighbor set in order to get the last missing pieces. Note that the neighbor\nset can change only by involving the tracker.\nFigure 2.23: The development of the potential set size jPjrelative to\nthe sizejNjof the neighborhood as pieces are being downloaded in\nBitTorrent.\nMost descriptions of BitTorrent consider only the trading phase, which is\nusually the longest and most ef\ufb01cient phase. Figure 2.23 shows how the three\nphases develop in terms of the relative size of the potential set. We see clearly\nthat during the bootstrap phase, as well as the last download phase, the potential\nsetPis relatively small. Effectively, this means that it may take a while to get\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.3. SYSTEM ARCHITECTURE 93\nA node that has all chunks of Fand continues to participate in its swarm is\nknown as a seeder ; all others in the swarm are known as leechers .\nAs mentioned, each \ufb01le is divided into a number of equally sized chunks,\ncalled pieces in BitTorrent. A piece is typically 256 Kbyte, but other sizes are also\nused. The unit of data transfer between two peers is that of a block , which is\ntypically 16 Kbyte. A node can upload a block only if it has the entire piece to\nwhich that block belongs. A neighbor Bbelongs to the potential set PAofA, if it\nhas a block that Adoes not yet have, and vice versa . Again, if B2PAandA2PB,\nthen AandBare in a position that they can trade a block.\nWith this terminology, the downloading of a \ufb01le can be described in terms of\nthree phases, as detailed by Rai et al. [2007]:\nBootstrap phase: A node Ahas just received its \ufb01rst piece, placing it in a position\nto start trading blocks. This \ufb01rst piece is obtained through a mechanism\ncalled optimistic unchoking , by which nodes from NAunsel\ufb01shly provide\nthe blocks of a piece to get a newly arrived node started. If the potential set\nPAofAis empty, Awill have to wait until new neighbors are added to the\nset who may have pieces to exchange.\nTrading phase: In this phase,jPAj>0, meaning that there is always a peer\nwith whom Acan trade blocks. In practice, this is the phase in which the\ndownloading is highly ef\ufb01cient as enough nodes can be paired to exchange\ndata.\nLast download phase: Now the size of the potential set has dropped to zero\nagain, so that node Ais entirely dependent on newly arriving peers in its\nneighbor set in order to get the last missing pieces. Note that the neighbor\nset can change only by involving the tracker.\nFigure 2.23: The development of the potential set size jPjrelative to\nthe sizejNjof the neighborhood as pieces are being downloaded in\nBitTorrent.\nMost descriptions of BitTorrent consider only the trading phase, which is\nusually the longest and most ef\ufb01cient phase. Figure 2.23 shows how the three\nphases develop in terms of the relative size of the potential set. We see clearly\nthat during the bootstrap phase, as well as the last download phase, the potential\nsetPis relatively small. Effectively, this means that it may take a while to get\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "94 CHAPTER 2. ARCHITECTURES\nBitTorrent started, but also that completing a download may take a while for the\nsimple reason that it is dif\ufb01cult to \ufb01nd a trading partner.\nTo \ufb01nd a suitable trading peer, a node always selects a peer from its potential\nset having the highest upload rate. Likewise, to speed up dissemination, a\nnode generally selects to hand out blocks of the rarest piece in the swarm \ufb01rst\nbefore considering other pieces. In this way, we can expect a quick and uniform\ndissemination of pieces throughout the swarm.\n2.4 Example architectures\nThe Network File System\nMany distributed \ufb01les systems are organized along the lines of client-server\narchitectures, with Sun Microsystem\u2019s Network File System (NFS ) being one\nof the most widely-deployed ones for Unix systems. Here, we concentrate on\nNFSv3, the widely-used third version of NFS [Callaghan, 2000] and NFSv4,\nthe most recent, fourth version [Shepler et al., 2003]. We will discuss the\ndifferences between them as well.\nThe basic idea behind NFS is that each \ufb01le server provides a standardized\nview of its local \ufb01le system. In other words, it should not matter how that\nlocal \ufb01le system is implemented; each NFS server supports the same model.\nThis approach has been adopted for other distributed \ufb01les systems as well.\nNFS comes with a communication protocol that allows clients to access the\n\ufb01les stored on a server, thus allowing a heterogeneous collection of processes,\npossibly running on different operating systems and machines, to share a\ncommon \ufb01le system.\nThe model underlying NFS and similar systems is that of a remote \ufb01le\nservice . In this model, clients are offered transparent access to a \ufb01le system\nthat is managed by a remote server. However, clients are normally unaware\nof the actual location of \ufb01les. Instead, they are offered an interface to a \ufb01le\nsystem that is similar to the interface offered by a conventional local \ufb01le system.\nIn particular, the client is offered only an interface containing various \ufb01le\noperations, but the server is responsible for implementing those operations.\nThis model is therefore also referred to as the remote access model . It is\nshown in Figure 2.24(a).\nIn contrast, in the upload/download model a client accesses a \ufb01le locally\nafter having downloaded it from the server, as shown in Figure 2.24(b) When\nthe client is \ufb01nished with the \ufb01le, it is uploaded back to the server again so\nthat it can be used by another client. The Internet\u2019s FTP service can be used\nthis way when a client downloads a complete \ufb01le, modi\ufb01es it, and then puts\nit back.\nNFS has been implemented for a large number of different operating\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n94 CHAPTER 2. ARCHITECTURES\nBitTorrent started, but also that completing a download may take a while for the\nsimple reason that it is dif\ufb01cult to \ufb01nd a trading partner.\nTo \ufb01nd a suitable trading peer, a node always selects a peer from its potential\nset having the highest upload rate. Likewise, to speed up dissemination, a\nnode generally selects to hand out blocks of the rarest piece in the swarm \ufb01rst\nbefore considering other pieces. In this way, we can expect a quick and uniform\ndissemination of pieces throughout the swarm.\n2.4 Example architectures\nThe Network File System\nMany distributed \ufb01les systems are organized along the lines of client-server\narchitectures, with Sun Microsystem\u2019s Network File System (NFS ) being one\nof the most widely-deployed ones for Unix systems. Here, we concentrate on\nNFSv3, the widely-used third version of NFS [Callaghan, 2000] and NFSv4,\nthe most recent, fourth version [Shepler et al., 2003]. We will discuss the\ndifferences between them as well.\nThe basic idea behind NFS is that each \ufb01le server provides a standardized\nview of its local \ufb01le system. In other words, it should not matter how that\nlocal \ufb01le system is implemented; each NFS server supports the same model.\nThis approach has been adopted for other distributed \ufb01les systems as well.\nNFS comes with a communication protocol that allows clients to access the\n\ufb01les stored on a server, thus allowing a heterogeneous collection of processes,\npossibly running on different operating systems and machines, to share a\ncommon \ufb01le system.\nThe model underlying NFS and similar systems is that of a remote \ufb01le\nservice . In this model, clients are offered transparent access to a \ufb01le system\nthat is managed by a remote server. However, clients are normally unaware\nof the actual location of \ufb01les. Instead, they are offered an interface to a \ufb01le\nsystem that is similar to the interface offered by a conventional local \ufb01le system.\nIn particular, the client is offered only an interface containing various \ufb01le\noperations, but the server is responsible for implementing those operations.\nThis model is therefore also referred to as the remote access model . It is\nshown in Figure 2.24(a).\nIn contrast, in the upload/download model a client accesses a \ufb01le locally\nafter having downloaded it from the server, as shown in Figure 2.24(b) When\nthe client is \ufb01nished with the \ufb01le, it is uploaded back to the server again so\nthat it can be used by another client. The Internet\u2019s FTP service can be used\nthis way when a client downloads a complete \ufb01le, modi\ufb01es it, and then puts\nit back.\nNFS has been implemented for a large number of different operating\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.4. EXAMPLE ARCHITECTURES 95\nFigure 2.24: (a) The remote access model. (b) The upload/download model.\nsystems, although the Unix versions are predominant. For virtually all modern\nUnix systems, NFS is generally implemented following the layered architecture\nshown in Figure 2.25.\nFigure 2.25: The basic NFS architecture for Unix systems.\nA client accesses the \ufb01le system using the system calls provided by its local\noperating system. However, the local Unix \ufb01le system interface is replaced\nby an interface to the Virtual File System (VFS ), which by now is a de facto\nstandard for interfacing to different (distributed) \ufb01le systems [Kleiman, 1986].\nVirtually all modern operating systems provide VFS, and not doing so more\nor less forces developers to largely reimplement huge parts of an operating\nsystem when adopting a new \ufb01le-system structure. With NFS, operations on\nthe VFS interface are either passed to a local \ufb01le system, or passed to a separate\ncomponent known as the NFS client , which takes care of handling access\nto \ufb01les stored at a remote server. In NFS, all client-server communication is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.4. EXAMPLE ARCHITECTURES 95\nFigure 2.24: (a) The remote access model. (b) The upload/download model.\nsystems, although the Unix versions are predominant. For virtually all modern\nUnix systems, NFS is generally implemented following the layered architecture\nshown in Figure 2.25.\nFigure 2.25: The basic NFS architecture for Unix systems.\nA client accesses the \ufb01le system using the system calls provided by its local\noperating system. However, the local Unix \ufb01le system interface is replaced\nby an interface to the Virtual File System (VFS ), which by now is a de facto\nstandard for interfacing to different (distributed) \ufb01le systems [Kleiman, 1986].\nVirtually all modern operating systems provide VFS, and not doing so more\nor less forces developers to largely reimplement huge parts of an operating\nsystem when adopting a new \ufb01le-system structure. With NFS, operations on\nthe VFS interface are either passed to a local \ufb01le system, or passed to a separate\ncomponent known as the NFS client , which takes care of handling access\nto \ufb01les stored at a remote server. In NFS, all client-server communication is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "96 CHAPTER 2. ARCHITECTURES\ndone through so-called remote procedure calls (RPC s). An RPC is essentially\na standardized way to let a client on machine Amake an ordinary call to\na procedure that is implemented on another machine B. We discuss RPCs\nextensively in Chapter 4. The NFS client implements the NFS \ufb01le system\noperations as remote procedure calls to the server. Note that the operations\noffered by the VFS interface can be different from those offered by the NFS\nclient. The whole idea of the VFS is to hide the differences between various\n\ufb01le systems.\nOn the server side, we see a similar organization. The NFS server is\nresponsible for handling incoming client requests. The RPC component at\nthe server converts incoming requests to regular VFS \ufb01le operations that\nare subsequently passed to the VFS layer. Again, the VFS is responsible for\nimplementing a local \ufb01le system in which the actual \ufb01les are stored.\nAn important advantage of this scheme is that NFS is largely independent\nof local \ufb01le systems. In principle, it really does not matter whether the\noperating system at the client or server implements a Unix \ufb01le system, a\nWindows \ufb01le system, or even an old MS-DOS \ufb01le system. The only important\nissue is that these \ufb01le systems are compliant with the \ufb01le system model offered\nby NFS. For example, MS-DOS with its short \ufb01le names cannot be used to\nimplement an NFS server in a fully transparent way.\nNote 2.9 (More information: The NFS \ufb01le system model)\nFigure 2.26 shows the general \ufb01le operations supported by NFS versions 3 and\n4, respectively. The create operation is used to create a \ufb01le, but has somewhat\ndifferent meanings in NFSv3 and NFSv4. In version 3, the operation is used for\ncreating regular \ufb01les. Special \ufb01les are created using separate operations. The link\noperation is used to create hard links. Symlink is used to create symbolic links.\nMkdir is used to create subdirectories. Special \ufb01les, such as device \ufb01les, sockets,\nand named pipes are created by means of the mknod operation.\nThis situation is changed completely in NFSv4, where create is used for\ncreating nonregular \ufb01les, which include symbolic links, directories, and special\n\ufb01les. Hard links are still created using a separate linkoperation, but regular \ufb01les\nare created by means of the open operation, which is new to NFS and is a major\ndeviation from the approach to \ufb01le handling in older versions. Up until version 4,\nNFS was designed to allow its \ufb01le servers to be what is known as stateless : once\na request has been completed, the server will forget about the client and the\noperation it had requested. For reasons we discuss later, this design criterion\nhas been abandoned in NFSv4, in which it is assumed that servers will generally\nmaintain state between operations on the same \ufb01le.\nThe operation rename is used to change the name of an existing \ufb01le the same\nas in Unix.\nFiles are deleted by means of the remove operation. In version 4, this operation\nis used to remove any kind of \ufb01le. In previous versions, a separate rmdir operation\nwas needed to remove a subdirectory. A \ufb01le is removed by its name and has the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n96 CHAPTER 2. ARCHITECTURES\ndone through so-called remote procedure calls (RPC s). An RPC is essentially\na standardized way to let a client on machine Amake an ordinary call to\na procedure that is implemented on another machine B. We discuss RPCs\nextensively in Chapter 4. The NFS client implements the NFS \ufb01le system\noperations as remote procedure calls to the server. Note that the operations\noffered by the VFS interface can be different from those offered by the NFS\nclient. The whole idea of the VFS is to hide the differences between various\n\ufb01le systems.\nOn the server side, we see a similar organization. The NFS server is\nresponsible for handling incoming client requests. The RPC component at\nthe server converts incoming requests to regular VFS \ufb01le operations that\nare subsequently passed to the VFS layer. Again, the VFS is responsible for\nimplementing a local \ufb01le system in which the actual \ufb01les are stored.\nAn important advantage of this scheme is that NFS is largely independent\nof local \ufb01le systems. In principle, it really does not matter whether the\noperating system at the client or server implements a Unix \ufb01le system, a\nWindows \ufb01le system, or even an old MS-DOS \ufb01le system. The only important\nissue is that these \ufb01le systems are compliant with the \ufb01le system model offered\nby NFS. For example, MS-DOS with its short \ufb01le names cannot be used to\nimplement an NFS server in a fully transparent way.\nNote 2.9 (More information: The NFS \ufb01le system model)\nFigure 2.26 shows the general \ufb01le operations supported by NFS versions 3 and\n4, respectively. The create operation is used to create a \ufb01le, but has somewhat\ndifferent meanings in NFSv3 and NFSv4. In version 3, the operation is used for\ncreating regular \ufb01les. Special \ufb01les are created using separate operations. The link\noperation is used to create hard links. Symlink is used to create symbolic links.\nMkdir is used to create subdirectories. Special \ufb01les, such as device \ufb01les, sockets,\nand named pipes are created by means of the mknod operation.\nThis situation is changed completely in NFSv4, where create is used for\ncreating nonregular \ufb01les, which include symbolic links, directories, and special\n\ufb01les. Hard links are still created using a separate linkoperation, but regular \ufb01les\nare created by means of the open operation, which is new to NFS and is a major\ndeviation from the approach to \ufb01le handling in older versions. Up until version 4,\nNFS was designed to allow its \ufb01le servers to be what is known as stateless : once\na request has been completed, the server will forget about the client and the\noperation it had requested. For reasons we discuss later, this design criterion\nhas been abandoned in NFSv4, in which it is assumed that servers will generally\nmaintain state between operations on the same \ufb01le.\nThe operation rename is used to change the name of an existing \ufb01le the same\nas in Unix.\nFiles are deleted by means of the remove operation. In version 4, this operation\nis used to remove any kind of \ufb01le. In previous versions, a separate rmdir operation\nwas needed to remove a subdirectory. A \ufb01le is removed by its name and has the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.4. EXAMPLE ARCHITECTURES 97\neffect that the number of hard links to it is decreased by one. If the number of\nlinks drops to zero, the \ufb01le may be destroyed.\nOperation v3 v4 Description\ncreate Y es No Create a regular \ufb01le\ncreate No Y es Create a nonregular \ufb01le\nlink Y es Y es Create a hard link to a \ufb01le\nsymlink Y es No Create a symbolic link to a \ufb01le\nmkdir Y es No Create a subdirectory in a given directory\nmknod Y es No Create a special \ufb01le\nrename Y es Y es Change the name of a \ufb01le\nremove Y es Y es Remove a \ufb01le from a \ufb01le system\nrmdir Y es No Remove an empty subdirectory from a directory\nopen No Y es Open a \ufb01le\nclose No Y es Close a \ufb01le\nlookup Y es Y es Look up a \ufb01le by means of a \ufb01le name\nreaddir Y es Y es Read the entries in a directory\nreadlink Y es Y es Read the path name stored in a symbolic link\ngetattr Y es Y es Get the attribute values for a \ufb01le\nsetattr Y es Y es Set one or more attribute values for a \ufb01le\nread Y es Y es Read the data contained in a \ufb01le\nwrite Y es Y es Write data to a \ufb01le\nFigure 2.26: An incomplete list of NFS \ufb01le system operations.\nVersion 4 allows clients to open and close (regular) \ufb01les. Opening a nonexisting\n\ufb01le has the side effect that a new \ufb01le is created. To open a \ufb01le, a client provides a\nname, along with various values for attributes. For example, a client may specify\nthat a \ufb01le should be opened for write access. After a \ufb01le has been successfully\nopened, a client can access that \ufb01le by means of its \ufb01le handle. That handle is also\nused to close the \ufb01le, by which the client tells the server that it will no longer need\nto have access to the \ufb01le. The server, in turn, can release any state it maintained\nto provide that client access to the \ufb01le.\nThelookup operation is used to look up a \ufb01le handle for a given path name.\nIn NFSv3, the lookup operation will not resolve a name beyond what is called\namount point (which is essentially a place in a naming system that connects to\nanother naming system, to which we return Chapter 5). For example, assume\nthat the name /remote /vurefers to a mount point in a naming graph. When\nresolving the name /remote /vu/mbox , the lookup operation in NFSv3 will return\nthe \ufb01le handle for the mount point /remote /vualong with the remainder of the\npath name (i.e., mbox ). The client is then required to explicitly mount the \ufb01le\nsystem that is needed to complete the name lookup. A \ufb01le system in this context\nis the collection of \ufb01les, attributes, directories, and data blocks that are jointly\nimplemented as a logical block device [Tanenbaum and Woodhull, 2006].\nIn version 4, matters have been simpli\ufb01ed. In this case, lookup will attempt\nto resolve the entire name, even if this means crossing mount points. Note\nthat this approach is possible only if a \ufb01le system has already been mounted at\nmount points. The client is able to detect that a mount point has been crossed\nby inspecting the \ufb01le system identi\ufb01er that is later returned when the lookup\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.4. EXAMPLE ARCHITECTURES 97\neffect that the number of hard links to it is decreased by one. If the number of\nlinks drops to zero, the \ufb01le may be destroyed.\nOperation v3 v4 Description\ncreate Y es No Create a regular \ufb01le\ncreate No Y es Create a nonregular \ufb01le\nlink Y es Y es Create a hard link to a \ufb01le\nsymlink Y es No Create a symbolic link to a \ufb01le\nmkdir Y es No Create a subdirectory in a given directory\nmknod Y es No Create a special \ufb01le\nrename Y es Y es Change the name of a \ufb01le\nremove Y es Y es Remove a \ufb01le from a \ufb01le system\nrmdir Y es No Remove an empty subdirectory from a directory\nopen No Y es Open a \ufb01le\nclose No Y es Close a \ufb01le\nlookup Y es Y es Look up a \ufb01le by means of a \ufb01le name\nreaddir Y es Y es Read the entries in a directory\nreadlink Y es Y es Read the path name stored in a symbolic link\ngetattr Y es Y es Get the attribute values for a \ufb01le\nsetattr Y es Y es Set one or more attribute values for a \ufb01le\nread Y es Y es Read the data contained in a \ufb01le\nwrite Y es Y es Write data to a \ufb01le\nFigure 2.26: An incomplete list of NFS \ufb01le system operations.\nVersion 4 allows clients to open and close (regular) \ufb01les. Opening a nonexisting\n\ufb01le has the side effect that a new \ufb01le is created. To open a \ufb01le, a client provides a\nname, along with various values for attributes. For example, a client may specify\nthat a \ufb01le should be opened for write access. After a \ufb01le has been successfully\nopened, a client can access that \ufb01le by means of its \ufb01le handle. That handle is also\nused to close the \ufb01le, by which the client tells the server that it will no longer need\nto have access to the \ufb01le. The server, in turn, can release any state it maintained\nto provide that client access to the \ufb01le.\nThelookup operation is used to look up a \ufb01le handle for a given path name.\nIn NFSv3, the lookup operation will not resolve a name beyond what is called\namount point (which is essentially a place in a naming system that connects to\nanother naming system, to which we return Chapter 5). For example, assume\nthat the name /remote /vurefers to a mount point in a naming graph. When\nresolving the name /remote /vu/mbox , the lookup operation in NFSv3 will return\nthe \ufb01le handle for the mount point /remote /vualong with the remainder of the\npath name (i.e., mbox ). The client is then required to explicitly mount the \ufb01le\nsystem that is needed to complete the name lookup. A \ufb01le system in this context\nis the collection of \ufb01les, attributes, directories, and data blocks that are jointly\nimplemented as a logical block device [Tanenbaum and Woodhull, 2006].\nIn version 4, matters have been simpli\ufb01ed. In this case, lookup will attempt\nto resolve the entire name, even if this means crossing mount points. Note\nthat this approach is possible only if a \ufb01le system has already been mounted at\nmount points. The client is able to detect that a mount point has been crossed\nby inspecting the \ufb01le system identi\ufb01er that is later returned when the lookup\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "98 CHAPTER 2. ARCHITECTURES\ncompletes.\nThere is a separate operation readdir to read the entries in a directory. This\noperation returns a list of ( (name, \ufb01le handle) ), pairs along with attribute values\nthat the client requested. The client can also specify how many entries should be\nreturned. The operation returns an offset that can be used in a subsequent call to\nreaddir in order to read the next series of entries.\nOperation readlink is used to read the data associated with a symbolic link.\nNormally, this data corresponds to a path name that can be subsequently looked\nup. Note that the lookup operation cannot handle symbolic links. Instead, when\na symbolic link is reached, name resolution stops and the client is required to \ufb01rst\ncallreadlink to \ufb01nd out where name resolution should continue.\nFiles have various attributes associated with them. Again, there are important\ndifferences between NFS version 3 and 4. Typical attributes include the type of\nthe \ufb01le (telling whether we are dealing with a directory, a symbolic link, a special\n\ufb01le, etc.), the \ufb01le length, the identi\ufb01er of the \ufb01le system that contains the \ufb01le, and\nthe last time the \ufb01le was modi\ufb01ed. File attributes can be read and set using the\noperations getattr andsetattr , respectively.\nFinally, there are operations for reading data from a \ufb01le, and writing data to a\n\ufb01le. Reading data by means of the operation read is completely straightforward.\nThe client speci\ufb01es the offset and the number of bytes to be read. The client is\nreturned the actual number of bytes that have been read, along with additional\nstatus information (e.g., whether the end-of-\ufb01le has been reached).\nWriting data to a \ufb01le is done using the write operation. The client again\nspeci\ufb01es the position in the \ufb01le where writing should start, the number of bytes\nto be written, and the data. In addition, it can instruct the server to ensure that all\ndata are to be written to stable storage (we discussed stable storage in Chapter 8).\nNFS servers are required to support storage devices that can survive power supply\nfailures, operating system failures, and hardware failures.\nThe Web\nThe architecture of Web-based distributed systems is not fundamentally dif-\nferent from other distributed systems. However, it is interesting to see how\nthe initial idea of supporting distributed documents has evolved since its\ninception in 1990s. Documents turned from being purely static and pas-\nsive to dynamically generated content. Furthermore, in recent years, many\norganizations have begun supporting services instead of just documents.\nSimple Web-based systems\nMany Web-based systems are still organized as relatively simple client-server\narchitectures. The core of a Web site is formed by a process that has access to a\nlocal \ufb01le system storing documents. The simplest way to refer to a document is\nby means of a reference called a Uniform Resource Locator (URL ). It speci\ufb01es\nwhere a document is located by embedding the DNS name of its associated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n98 CHAPTER 2. ARCHITECTURES\ncompletes.\nThere is a separate operation readdir to read the entries in a directory. This\noperation returns a list of ( (name, \ufb01le handle) ), pairs along with attribute values\nthat the client requested. The client can also specify how many entries should be\nreturned. The operation returns an offset that can be used in a subsequent call to\nreaddir in order to read the next series of entries.\nOperation readlink is used to read the data associated with a symbolic link.\nNormally, this data corresponds to a path name that can be subsequently looked\nup. Note that the lookup operation cannot handle symbolic links. Instead, when\na symbolic link is reached, name resolution stops and the client is required to \ufb01rst\ncallreadlink to \ufb01nd out where name resolution should continue.\nFiles have various attributes associated with them. Again, there are important\ndifferences between NFS version 3 and 4. Typical attributes include the type of\nthe \ufb01le (telling whether we are dealing with a directory, a symbolic link, a special\n\ufb01le, etc.), the \ufb01le length, the identi\ufb01er of the \ufb01le system that contains the \ufb01le, and\nthe last time the \ufb01le was modi\ufb01ed. File attributes can be read and set using the\noperations getattr andsetattr , respectively.\nFinally, there are operations for reading data from a \ufb01le, and writing data to a\n\ufb01le. Reading data by means of the operation read is completely straightforward.\nThe client speci\ufb01es the offset and the number of bytes to be read. The client is\nreturned the actual number of bytes that have been read, along with additional\nstatus information (e.g., whether the end-of-\ufb01le has been reached).\nWriting data to a \ufb01le is done using the write operation. The client again\nspeci\ufb01es the position in the \ufb01le where writing should start, the number of bytes\nto be written, and the data. In addition, it can instruct the server to ensure that all\ndata are to be written to stable storage (we discussed stable storage in Chapter 8).\nNFS servers are required to support storage devices that can survive power supply\nfailures, operating system failures, and hardware failures.\nThe Web\nThe architecture of Web-based distributed systems is not fundamentally dif-\nferent from other distributed systems. However, it is interesting to see how\nthe initial idea of supporting distributed documents has evolved since its\ninception in 1990s. Documents turned from being purely static and pas-\nsive to dynamically generated content. Furthermore, in recent years, many\norganizations have begun supporting services instead of just documents.\nSimple Web-based systems\nMany Web-based systems are still organized as relatively simple client-server\narchitectures. The core of a Web site is formed by a process that has access to a\nlocal \ufb01le system storing documents. The simplest way to refer to a document is\nby means of a reference called a Uniform Resource Locator (URL ). It speci\ufb01es\nwhere a document is located by embedding the DNS name of its associated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.4. EXAMPLE ARCHITECTURES 99\nserver along with a \ufb01le name by which the server can look up the document\nin its local \ufb01le system. Furthermore, a URL speci\ufb01es the application-level\nprotocol for transferring the document across the network.\nA client interacts with Web servers through a browser , which is responsible\nfor properly displaying a document. Also, a browser accepts input from a\nuser mostly by letting the user select a reference to another document, which\nit then subsequently fetches and displays. The communication between a\nbrowser and Web server is standardized: they both adhere to the HyperText\nTransfer Protocol (HTTP ). This leads to the overall organization shown in\nFigure 2.27.\nFigure 2.27: The overall organization of a traditional Web site.\nLet us zoom in a bit into what a document actually is. Perhaps the simplest\nform is a standard text \ufb01le. In that case, the server and browser have barely\nanything to do: the server copies the \ufb01le from the local \ufb01le system and\ntransfers it to the browser. The latter, in turn, merely displays the content of\nthe \ufb01le ad verbatim without further ado.\nMore interesting are Web documents that have been marked up, which is\nusually done in the HyperText Markup Language , or simply HTML . In that\ncase, the document includes various instructions expressing how its content\nshould be displayed, similar to what one can expect from any decent word-\nprocessing system (although those instructions are normally hidden from the\nend user). For example, instructing text to be emphasized is done by the\nfollowing markup:\n<emph>Emphasize this text</emph>\nThere are many more of such markup instructions. The point is that the\nbrowser understands these instructions and will act accordingly when dis-\nplaying the text.\nDocuments can contain much more than just markup instructions. In\nparticular, they can have complete programs embedded of which Javascript is\nthe one most often deployed. In this case, the browser is warned that there is\nsome code to execute as in:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.4. EXAMPLE ARCHITECTURES 99\nserver along with a \ufb01le name by which the server can look up the document\nin its local \ufb01le system. Furthermore, a URL speci\ufb01es the application-level\nprotocol for transferring the document across the network.\nA client interacts with Web servers through a browser , which is responsible\nfor properly displaying a document. Also, a browser accepts input from a\nuser mostly by letting the user select a reference to another document, which\nit then subsequently fetches and displays. The communication between a\nbrowser and Web server is standardized: they both adhere to the HyperText\nTransfer Protocol (HTTP ). This leads to the overall organization shown in\nFigure 2.27.\nFigure 2.27: The overall organization of a traditional Web site.\nLet us zoom in a bit into what a document actually is. Perhaps the simplest\nform is a standard text \ufb01le. In that case, the server and browser have barely\nanything to do: the server copies the \ufb01le from the local \ufb01le system and\ntransfers it to the browser. The latter, in turn, merely displays the content of\nthe \ufb01le ad verbatim without further ado.\nMore interesting are Web documents that have been marked up, which is\nusually done in the HyperText Markup Language , or simply HTML . In that\ncase, the document includes various instructions expressing how its content\nshould be displayed, similar to what one can expect from any decent word-\nprocessing system (although those instructions are normally hidden from the\nend user). For example, instructing text to be emphasized is done by the\nfollowing markup:\n<emph>Emphasize this text</emph>\nThere are many more of such markup instructions. The point is that the\nbrowser understands these instructions and will act accordingly when dis-\nplaying the text.\nDocuments can contain much more than just markup instructions. In\nparticular, they can have complete programs embedded of which Javascript is\nthe one most often deployed. In this case, the browser is warned that there is\nsome code to execute as in:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "100 CHAPTER 2. ARCHITECTURES\n<script type=\u2019\u2019text/javascript\u2019\u2019>....</script>\nand as long as the browser as an appropriate embedded interpreter for the\nspeci\ufb01ed language, everything between \u201c <script> \u201d and \u201c </script> \u201d will be\nexecuted as any other other program. The main bene\ufb01t of including scripts\nis that it allows for much better interaction with the end user, including\nsending information back to the server. (The latter, by the way, has always\nbeen supported in HTML through forms .)\nMuch more can be said about Web documents, but this is not the place to\ndo so. A good introduction on how to build Web-based applications can be\nfound in [Sebesta, 2006].\nMultitiered architectures\nThe Web started out as the relatively simple two-tiered client-server system\nshown in Figure 2.27. By now, this simple architecture has been extended to\nsupport much more sophisticated means of documents. In fact, one could\njusta\ufb01ably argue that the term \u201cdocument\u201d is no longer appropriate. For\none, most things that we get to see in our browser has been generated on the\nspot as the result of sending a request to a Web server. Content is stored in\na database at the server\u2019s side, along with client-side scripts and such, to be\ncomposed on-the-\ufb02y into a document which is then subsequently sent to the\nclient\u2019s browser. Documents have thus become completely dynamic.\nOne of the \ufb01rst enhancements to the basic architecture was support for\nsimple user interaction by means of the Common Gateway Interface or sim-\nplyCGI . CGI de\ufb01nes a standard way by which a Web server can execute a\nprogram taking user data as input. Usually, user data come from an HTML\nform; it speci\ufb01es the program that is to be executed at the server side, along\nwith parameter values that are \ufb01lled in by the user. Once the form has been\ncompleted, the program\u2019s name and collected parameter values are sent to\nthe server, as shown in Figure 2.28.\nFigure 2.28: The principle of using server-side CGI programs.\nWhen the server sees the request, it starts the program named in the\nrequest and passes it the parameter values. At that point, the program simply\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n100 CHAPTER 2. ARCHITECTURES\n<script type=\u2019\u2019text/javascript\u2019\u2019>....</script>\nand as long as the browser as an appropriate embedded interpreter for the\nspeci\ufb01ed language, everything between \u201c <script> \u201d and \u201c </script> \u201d will be\nexecuted as any other other program. The main bene\ufb01t of including scripts\nis that it allows for much better interaction with the end user, including\nsending information back to the server. (The latter, by the way, has always\nbeen supported in HTML through forms .)\nMuch more can be said about Web documents, but this is not the place to\ndo so. A good introduction on how to build Web-based applications can be\nfound in [Sebesta, 2006].\nMultitiered architectures\nThe Web started out as the relatively simple two-tiered client-server system\nshown in Figure 2.27. By now, this simple architecture has been extended to\nsupport much more sophisticated means of documents. In fact, one could\njusta\ufb01ably argue that the term \u201cdocument\u201d is no longer appropriate. For\none, most things that we get to see in our browser has been generated on the\nspot as the result of sending a request to a Web server. Content is stored in\na database at the server\u2019s side, along with client-side scripts and such, to be\ncomposed on-the-\ufb02y into a document which is then subsequently sent to the\nclient\u2019s browser. Documents have thus become completely dynamic.\nOne of the \ufb01rst enhancements to the basic architecture was support for\nsimple user interaction by means of the Common Gateway Interface or sim-\nplyCGI . CGI de\ufb01nes a standard way by which a Web server can execute a\nprogram taking user data as input. Usually, user data come from an HTML\nform; it speci\ufb01es the program that is to be executed at the server side, along\nwith parameter values that are \ufb01lled in by the user. Once the form has been\ncompleted, the program\u2019s name and collected parameter values are sent to\nthe server, as shown in Figure 2.28.\nFigure 2.28: The principle of using server-side CGI programs.\nWhen the server sees the request, it starts the program named in the\nrequest and passes it the parameter values. At that point, the program simply\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "2.5. SUMMARY 101\ndoes its work and generally returns the results in the form of a document that\nis sent back to the user\u2019s browser to be displayed.\nCGI programs can be as sophisticated as a developer wants. For example,\nas shown in Figure 2.28 many programs operate on a database local to the\nWeb server. After processing the data, the program generates an HTML\ndocument and returns that document to the server. The server will then pass\nthe document to the client. An interesting observation is that to the server,\nit appears as if it is asking the CGI program to fetch a document. In other\nwords, the server does nothing but delegate the fetching of a document to an\nexternal program.\nThe main task of a server used to be handling client requests by simply\nfetching documents. With CGI programs, fetching a document could be\ndelegated in such a way that the server would remain unaware of whether\na document had been generated on the \ufb02y, or actually read from the local\n\ufb01le system. Note that we have just described a two-tiered organization of\nserver-side software.\nHowever, servers nowadays do much more than just fetching documents.\nOne of the most important enhancements is that servers can also process a\ndocument before passing it to the client. In particular, a document may contain\naserver-side script , which is executed by the server when the document has\nbeen fetched locally. The result of executing a script is sent along with the\nrest of the document to the client. The script itself is not sent. In other words,\nusing a server-side script changes a document by essentially replacing the\nscript with the results of its execution. To make matters concrete, take a look\nat a very simple example of dynamically generating a document. Assume a\n\ufb01le is stored at the server with the following content:\n<strong> <?php echo $_SERVER[\u2019REMOTE_ADDR\u2019]; ?> </strong>\nThe server will examine the \ufb01le and subsequently process the PHP code (be-\ntween \u201c <?php \u201d and \u201c ?>\u201d) replacing the code with the address of the requesting\nclient. Much more sophisticated settings are possible, such as accessing a\nlocal database and subsequently fetching content from that database to be\ncombined with other dynamically generated content.\n2.5 Summary\nDistributed systems can be organized in many different ways. We can make\na distinction between software architecture and system architecture. The\nlatter considers where the components that constitute a distributed system\nare placed across the various machines. The former is more concerned about\nthe logical organization of the software: how do components interact, in what\nways can they be structured, how can they be made independent, and so on.\nA keyword when talking about architectures is architectural style. A\nstyle re\ufb02ects the basic principle that is followed in organizing the interaction\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n2.5. SUMMARY 101\ndoes its work and generally returns the results in the form of a document that\nis sent back to the user\u2019s browser to be displayed.\nCGI programs can be as sophisticated as a developer wants. For example,\nas shown in Figure 2.28 many programs operate on a database local to the\nWeb server. After processing the data, the program generates an HTML\ndocument and returns that document to the server. The server will then pass\nthe document to the client. An interesting observation is that to the server,\nit appears as if it is asking the CGI program to fetch a document. In other\nwords, the server does nothing but delegate the fetching of a document to an\nexternal program.\nThe main task of a server used to be handling client requests by simply\nfetching documents. With CGI programs, fetching a document could be\ndelegated in such a way that the server would remain unaware of whether\na document had been generated on the \ufb02y, or actually read from the local\n\ufb01le system. Note that we have just described a two-tiered organization of\nserver-side software.\nHowever, servers nowadays do much more than just fetching documents.\nOne of the most important enhancements is that servers can also process a\ndocument before passing it to the client. In particular, a document may contain\naserver-side script , which is executed by the server when the document has\nbeen fetched locally. The result of executing a script is sent along with the\nrest of the document to the client. The script itself is not sent. In other words,\nusing a server-side script changes a document by essentially replacing the\nscript with the results of its execution. To make matters concrete, take a look\nat a very simple example of dynamically generating a document. Assume a\n\ufb01le is stored at the server with the following content:\n<strong> <?php echo $_SERVER[\u2019REMOTE_ADDR\u2019]; ?> </strong>\nThe server will examine the \ufb01le and subsequently process the PHP code (be-\ntween \u201c <?php \u201d and \u201c ?>\u201d) replacing the code with the address of the requesting\nclient. Much more sophisticated settings are possible, such as accessing a\nlocal database and subsequently fetching content from that database to be\ncombined with other dynamically generated content.\n2.5 Summary\nDistributed systems can be organized in many different ways. We can make\na distinction between software architecture and system architecture. The\nlatter considers where the components that constitute a distributed system\nare placed across the various machines. The former is more concerned about\nthe logical organization of the software: how do components interact, in what\nways can they be structured, how can they be made independent, and so on.\nA keyword when talking about architectures is architectural style. A\nstyle re\ufb02ects the basic principle that is followed in organizing the interaction\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "102 CHAPTER 2. ARCHITECTURES\nbetween the software components comprising a distributed system. Important\nstyles include layering, object-based styles, resource-based styles, and styles\nin which handling events are prominent.\nThere are many different organizations of distributed systems. An impor-\ntant class is where machines are divided into clients and servers. A client\nsends a request to a server, who will then produce a result that is returned to\nthe client. The client-server architecture re\ufb02ects the traditional way of modu-\nlarizing software in which a module calls the functions available in another\nmodule. By placing different components on different machines, we obtain a\nnatural physical distribution of functions across a collection of machines.\nClient-server architectures are often highly centralized. In decentralized\narchitectures we often see an equal role played by the processes that constitute\na distributed system, also known as peer-to-peer systems. In peer-to-peer\nsystems, the processes are organized into an overlay network, which is a\nlogical network in which every process has a local list of other peers that\nit can communicate with. The overlay network can be structured, in which\ncase deterministic schemes can be deployed for routing messages between\nprocesses. In unstructured networks, the list of peers is more or less random,\nimplying that search algorithms need to be deployed for locating data or other\nprocesses.\nIn hybrid architectures, elements from centralized and decentralized or-\nganizations are combined. A centralized component is often used to handle\ninitial requests, for example to redirect a client to a replica server, which, in\nturn, may be part of a peer-to-peer network as is the case in BitTorrent-based\nsystems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n102 CHAPTER 2. ARCHITECTURES\nbetween the software components comprising a distributed system. Important\nstyles include layering, object-based styles, resource-based styles, and styles\nin which handling events are prominent.\nThere are many different organizations of distributed systems. An impor-\ntant class is where machines are divided into clients and servers. A client\nsends a request to a server, who will then produce a result that is returned to\nthe client. The client-server architecture re\ufb02ects the traditional way of modu-\nlarizing software in which a module calls the functions available in another\nmodule. By placing different components on different machines, we obtain a\nnatural physical distribution of functions across a collection of machines.\nClient-server architectures are often highly centralized. In decentralized\narchitectures we often see an equal role played by the processes that constitute\na distributed system, also known as peer-to-peer systems. In peer-to-peer\nsystems, the processes are organized into an overlay network, which is a\nlogical network in which every process has a local list of other peers that\nit can communicate with. The overlay network can be structured, in which\ncase deterministic schemes can be deployed for routing messages between\nprocesses. In unstructured networks, the list of peers is more or less random,\nimplying that search algorithms need to be deployed for locating data or other\nprocesses.\nIn hybrid architectures, elements from centralized and decentralized or-\nganizations are combined. A centralized component is often used to handle\ninitial requests, for example to redirect a client to a replica server, which, in\nturn, may be part of a peer-to-peer network as is the case in BitTorrent-based\nsystems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 3\nProcesses\nIn this chapter, we take a closer look at how the different types of processes\nplay a crucial role in distributed systems. The concept of a process originates\nfrom the \ufb01eld of operating systems where it is generally de\ufb01ned as a program\nin execution. From an operating-system perspective, the management and\nscheduling of processes are perhaps the most important issues to deal with.\nHowever, when it comes to distributed systems, other issues turn out to be\nequally or more important.\nWe start with extensively discussing threads and their role in distributed\nsystems. As it turns out, threads play a crucial role in obtaining performance\nin multicore and multiprocessor environments, but also help in structuring\nclients and servers. There are many cases where we see threads being replaced\nby processes and using the underlying operating system for guaranteeing\nprotection and facilitating communication. Nevertheless, when performance\nis at stake, threads continue to play an important role.\nIn recent years, the concept of virtualization has regained much popu-\nlarity. Virtualization allows an application, and possibly also its complete\nenvironment including the operating system, to run concurrently with other\napplications, but highly independent of the underlying hardware and plat-\nforms, leading to a high degree of portability. Moreover, virtualization helps\nin isolating failures caused by errors or security problems. It is an important\nconcept for distributed systems, and we pay attention to it in a separate\nsection.\nClient-server organizations are important in distributed systems. In this\nchapter, we take a closer look at typical organizations of both clients and\nservers. We also pay attention to general design issues for servers, including\nthose typically used in object-based distributed systems. A widely used Web\nserver is Apache, to which we pay separate attention. The organization of\nserver clusters remains important, especially when they need to collaborately\nprovide the illusion of a single system. we will discuss examples of how to\n103\nChapter 3\nProcesses\nIn this chapter, we take a closer look at how the different types of processes\nplay a crucial role in distributed systems. The concept of a process originates\nfrom the \ufb01eld of operating systems where it is generally de\ufb01ned as a program\nin execution. From an operating-system perspective, the management and\nscheduling of processes are perhaps the most important issues to deal with.\nHowever, when it comes to distributed systems, other issues turn out to be\nequally or more important.\nWe start with extensively discussing threads and their role in distributed\nsystems. As it turns out, threads play a crucial role in obtaining performance\nin multicore and multiprocessor environments, but also help in structuring\nclients and servers. There are many cases where we see threads being replaced\nby processes and using the underlying operating system for guaranteeing\nprotection and facilitating communication. Nevertheless, when performance\nis at stake, threads continue to play an important role.\nIn recent years, the concept of virtualization has regained much popu-\nlarity. Virtualization allows an application, and possibly also its complete\nenvironment including the operating system, to run concurrently with other\napplications, but highly independent of the underlying hardware and plat-\nforms, leading to a high degree of portability. Moreover, virtualization helps\nin isolating failures caused by errors or security problems. It is an important\nconcept for distributed systems, and we pay attention to it in a separate\nsection.\nClient-server organizations are important in distributed systems. In this\nchapter, we take a closer look at typical organizations of both clients and\nservers. We also pay attention to general design issues for servers, including\nthose typically used in object-based distributed systems. A widely used Web\nserver is Apache, to which we pay separate attention. The organization of\nserver clusters remains important, especially when they need to collaborately\nprovide the illusion of a single system. we will discuss examples of how to\n103", "104 CHAPTER 3. PROCESSES\nachieve this perspective, including wide-area servers like PlanetLab.\nAn important issue, especially in wide-area distributed systems, is moving\nprocesses between different machines. Process migration or more speci\ufb01cally,\ncode migration, can help in achieving scalability, but can also help to dynami-\ncally con\ufb01gure clients and servers. What is actually meant by code migration\nand what its implications are is also discussed in this chapter.\n3.1 Threads\nAlthough processes form a building block in distributed systems, practice\nindicates that the granularity of processes as provided by the operating\nsystems on which distributed systems are built is not suf\ufb01cient. Instead, it\nturns out that having a \ufb01ner granularity in the form of multiple threads of\ncontrol per process makes it much easier to build distributed applications\nand to get better performance. In this section, we take a closer look at the\nrole of threads in distributed systems and explain why they are so important.\nMore on threads and how they can be used to build applications can be found\nin [Lewis and Berg, 1998; Stevens, 1999; Robbins and Robbins, 2003]. Herlihy\nand Shavit [2008] is highly recommended to learn more about multithreaded\nconcurrent programming in general.\nIntroduction to threads\nTo understand the role of threads in distributed systems, it is important to un-\nderstand what a process is, and how processes and threads relate. To execute\na program, an operating system creates a number of virtual processors , each\none for running a different program. To keep track of these virtual processors,\nthe operating system has a process table , containing entries to store CPU\nregister values, memory maps, open \ufb01les, accounting information, privileges,\netc. Jointly, these entries form a process context .\nA process context can be viewed as the software analog of the hardware\u2019s\nprocessor context . The latter consists of the minimal information that is\nautomatically stored by the hardware to handle an interrupt, and to later\nreturn to where the CPU left off. The processor context contains at least the\nprogram counter, but sometimes also other register values such as the stack\npointer.\nAprocess is often de\ufb01ned as a program in execution, that is, a program\nthat is currently being executed on one of the operating system\u2019s virtual\nprocessors. An important issue is that the operating system takes great care\nto ensure that independent processes cannot maliciously or inadvertently\naffect the correctness of each other\u2019s behavior. In other words, the fact\nthat multiple processes may be concurrently sharing the same CPU and\nother hardware resources is made transparent. Usually, the operating system\nrequires hardware support to enforce this separation.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n104 CHAPTER 3. PROCESSES\nachieve this perspective, including wide-area servers like PlanetLab.\nAn important issue, especially in wide-area distributed systems, is moving\nprocesses between different machines. Process migration or more speci\ufb01cally,\ncode migration, can help in achieving scalability, but can also help to dynami-\ncally con\ufb01gure clients and servers. What is actually meant by code migration\nand what its implications are is also discussed in this chapter.\n3.1 Threads\nAlthough processes form a building block in distributed systems, practice\nindicates that the granularity of processes as provided by the operating\nsystems on which distributed systems are built is not suf\ufb01cient. Instead, it\nturns out that having a \ufb01ner granularity in the form of multiple threads of\ncontrol per process makes it much easier to build distributed applications\nand to get better performance. In this section, we take a closer look at the\nrole of threads in distributed systems and explain why they are so important.\nMore on threads and how they can be used to build applications can be found\nin [Lewis and Berg, 1998; Stevens, 1999; Robbins and Robbins, 2003]. Herlihy\nand Shavit [2008] is highly recommended to learn more about multithreaded\nconcurrent programming in general.\nIntroduction to threads\nTo understand the role of threads in distributed systems, it is important to un-\nderstand what a process is, and how processes and threads relate. To execute\na program, an operating system creates a number of virtual processors , each\none for running a different program. To keep track of these virtual processors,\nthe operating system has a process table , containing entries to store CPU\nregister values, memory maps, open \ufb01les, accounting information, privileges,\netc. Jointly, these entries form a process context .\nA process context can be viewed as the software analog of the hardware\u2019s\nprocessor context . The latter consists of the minimal information that is\nautomatically stored by the hardware to handle an interrupt, and to later\nreturn to where the CPU left off. The processor context contains at least the\nprogram counter, but sometimes also other register values such as the stack\npointer.\nAprocess is often de\ufb01ned as a program in execution, that is, a program\nthat is currently being executed on one of the operating system\u2019s virtual\nprocessors. An important issue is that the operating system takes great care\nto ensure that independent processes cannot maliciously or inadvertently\naffect the correctness of each other\u2019s behavior. In other words, the fact\nthat multiple processes may be concurrently sharing the same CPU and\nother hardware resources is made transparent. Usually, the operating system\nrequires hardware support to enforce this separation.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 105\nThis concurrency transparency comes at a price. For example, each time a\nprocess is created, the operating system must create a complete independent\naddress space. Allocation can mean initializing memory segments by, for\nexample, zeroing a data segment, copying the associated program into a text\nsegment, and setting up a stack for temporary data. Likewise, switching the\nCPU between two processes may require some effort as well. Apart from\nsaving the data as currently stored in various registers (including the program\ncounter and stack pointer), the operating system will also have to modify\nregisters of the memory management unit (MMU) and invalidate address\ntranslation caches such as in the translation lookaside buffer (TLB). In addition,\nif the operating system supports more processes than it can simultaneously\nhold in main memory, it may have to swap processes between main memory\nand disk before the actual switch can take place.\nLike a process, a thread executes its own piece of code, independently\nfrom other threads. However, in contrast to processes, no attempt is made\nto achieve a high degree of concurrency transparency if this would result in\nperformance degradation. Therefore, a thread system generally maintains only\nthe minimum information to allow a CPU to be shared by several threads. In\nparticular, a thread context often consists of nothing more than the processor\ncontext, along with some other information for thread management. For\nexample, a thread system may keep track of the fact that a thread is currently\nblocked on a mutex variable, so as not to select it for execution. Information\nthat is not strictly necessary to manage multiple threads is generally ignored.\nFor this reason, protecting data against inappropriate access by threads within\na single process is left entirely to application developers. We thus see that a\nprocessor context is contained in a thread context, and that, in turn, a thread\ncontext is contained in a process context.\nThere are two important implications of deploying threads as we just\nsketched. First of all, the performance of a multithreaded application need\nhardly ever be worse than that of its single-threaded counterpart. In fact,\nin many cases, multithreading even leads to a performance gain. Second,\nbecause threads are not automatically protected against each other the way\nprocesses are, development of multithreaded applications requires additional\nintellectual effort. Proper design and keeping things simple, as usual, help a\nlot. Unfortunately, current practice does not demonstrate that this principle is\nequally well understood.\nThread usage in nondistributed systems\nBefore discussing the role of threads in distributed systems, let us \ufb01rst consider\ntheir usage in traditional, nondistributed systems. There are several bene\ufb01ts\nto multithreaded processes that have increased the popularity of using thread\nsystems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 105\nThis concurrency transparency comes at a price. For example, each time a\nprocess is created, the operating system must create a complete independent\naddress space. Allocation can mean initializing memory segments by, for\nexample, zeroing a data segment, copying the associated program into a text\nsegment, and setting up a stack for temporary data. Likewise, switching the\nCPU between two processes may require some effort as well. Apart from\nsaving the data as currently stored in various registers (including the program\ncounter and stack pointer), the operating system will also have to modify\nregisters of the memory management unit (MMU) and invalidate address\ntranslation caches such as in the translation lookaside buffer (TLB). In addition,\nif the operating system supports more processes than it can simultaneously\nhold in main memory, it may have to swap processes between main memory\nand disk before the actual switch can take place.\nLike a process, a thread executes its own piece of code, independently\nfrom other threads. However, in contrast to processes, no attempt is made\nto achieve a high degree of concurrency transparency if this would result in\nperformance degradation. Therefore, a thread system generally maintains only\nthe minimum information to allow a CPU to be shared by several threads. In\nparticular, a thread context often consists of nothing more than the processor\ncontext, along with some other information for thread management. For\nexample, a thread system may keep track of the fact that a thread is currently\nblocked on a mutex variable, so as not to select it for execution. Information\nthat is not strictly necessary to manage multiple threads is generally ignored.\nFor this reason, protecting data against inappropriate access by threads within\na single process is left entirely to application developers. We thus see that a\nprocessor context is contained in a thread context, and that, in turn, a thread\ncontext is contained in a process context.\nThere are two important implications of deploying threads as we just\nsketched. First of all, the performance of a multithreaded application need\nhardly ever be worse than that of its single-threaded counterpart. In fact,\nin many cases, multithreading even leads to a performance gain. Second,\nbecause threads are not automatically protected against each other the way\nprocesses are, development of multithreaded applications requires additional\nintellectual effort. Proper design and keeping things simple, as usual, help a\nlot. Unfortunately, current practice does not demonstrate that this principle is\nequally well understood.\nThread usage in nondistributed systems\nBefore discussing the role of threads in distributed systems, let us \ufb01rst consider\ntheir usage in traditional, nondistributed systems. There are several bene\ufb01ts\nto multithreaded processes that have increased the popularity of using thread\nsystems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "106 CHAPTER 3. PROCESSES\nThe most important bene\ufb01t comes from the fact that in a single-threaded\nprocess, whenever a blocking system call is executed, the process as a whole is\nblocked. To illustrate, consider an application such as a spreadsheet program,\nand assume that a user continuously and interactively wants to change values.\nAn important property of a spreadsheet program is that it maintains the func-\ntional dependencies between different cells, often from different spreadsheets.\nTherefore, whenever a cell is modi\ufb01ed, all dependent cells are automatically\nupdated. When a user changes the value in a single cell, such a modi\ufb01cation\ncan trigger a large series of computations. If there is only a single thread\nof control, computation cannot proceed while the program is waiting for\ninput. Likewise, it is not easy to provide input while dependencies are being\ncalculated. The easy solution is to have at least two threads of control: one for\nhandling interaction with the user and one for updating the spreadsheet. In\nthe meantime, a third thread could be used for backing up the spreadsheet to\ndisk while the other two are doing their work.\nAnother advantage of multithreading is that it becomes possible to exploit\nparallelism when executing the program on a multiprocessor or multicore\nsystem. In that case, each thread is assigned to a different CPU or core while\nshared data are stored in shared main memory. When properly designed,\nsuch parallelism can be transparent: the process will run equally well on a\nuniprocessor system, albeit slower. Multithreading for parallelism is becoming\nincreasingly important with the availability of relatively cheap multiprocessor\nand multicore computers. Such computer systems are typically used for\nrunning servers in client-server applications, but are by now also extensively\nused in devices such as smartphones.\nMultithreading is also useful in the context of large applications. Such\napplications are often developed as a collection of cooperating programs,\neach to be executed by a separate process. This approach is typical for a\nUnix environment. Cooperation between programs is implemented by means\nofinterprocess communication (IPC) mechanisms. For Unix systems, these\nmechanisms typically include (named) pipes, message queues, and shared\nmemory segments (see also Stevens and Rago [2005]). The major drawback of\nall IPC mechanisms is that communication often requires relatively extensive\ncontext switching, shown at three different points in Figure 3.1.\nBecause IPC requires kernel intervention, a process will generally \ufb01rst\nhave to switch from user mode to kernel mode, shown as S1in Figure 3.1.\nThis requires changing the memory map in the MMU, as well as \ufb02ushing the\nTLB. Within the kernel, a process context switch takes place ( S2in the \ufb01gure),\nafter which the other party can be activated by switching from kernel mode to\nuser mode again ( S3in Figure 3.1). The latter switch again requires changing\nthe MMU map and \ufb02ushing the TLB.\nInstead of using processes, an application can also be constructed such\nthat different parts are executed by separate threads. Communication between\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n106 CHAPTER 3. PROCESSES\nThe most important bene\ufb01t comes from the fact that in a single-threaded\nprocess, whenever a blocking system call is executed, the process as a whole is\nblocked. To illustrate, consider an application such as a spreadsheet program,\nand assume that a user continuously and interactively wants to change values.\nAn important property of a spreadsheet program is that it maintains the func-\ntional dependencies between different cells, often from different spreadsheets.\nTherefore, whenever a cell is modi\ufb01ed, all dependent cells are automatically\nupdated. When a user changes the value in a single cell, such a modi\ufb01cation\ncan trigger a large series of computations. If there is only a single thread\nof control, computation cannot proceed while the program is waiting for\ninput. Likewise, it is not easy to provide input while dependencies are being\ncalculated. The easy solution is to have at least two threads of control: one for\nhandling interaction with the user and one for updating the spreadsheet. In\nthe meantime, a third thread could be used for backing up the spreadsheet to\ndisk while the other two are doing their work.\nAnother advantage of multithreading is that it becomes possible to exploit\nparallelism when executing the program on a multiprocessor or multicore\nsystem. In that case, each thread is assigned to a different CPU or core while\nshared data are stored in shared main memory. When properly designed,\nsuch parallelism can be transparent: the process will run equally well on a\nuniprocessor system, albeit slower. Multithreading for parallelism is becoming\nincreasingly important with the availability of relatively cheap multiprocessor\nand multicore computers. Such computer systems are typically used for\nrunning servers in client-server applications, but are by now also extensively\nused in devices such as smartphones.\nMultithreading is also useful in the context of large applications. Such\napplications are often developed as a collection of cooperating programs,\neach to be executed by a separate process. This approach is typical for a\nUnix environment. Cooperation between programs is implemented by means\nofinterprocess communication (IPC) mechanisms. For Unix systems, these\nmechanisms typically include (named) pipes, message queues, and shared\nmemory segments (see also Stevens and Rago [2005]). The major drawback of\nall IPC mechanisms is that communication often requires relatively extensive\ncontext switching, shown at three different points in Figure 3.1.\nBecause IPC requires kernel intervention, a process will generally \ufb01rst\nhave to switch from user mode to kernel mode, shown as S1in Figure 3.1.\nThis requires changing the memory map in the MMU, as well as \ufb02ushing the\nTLB. Within the kernel, a process context switch takes place ( S2in the \ufb01gure),\nafter which the other party can be activated by switching from kernel mode to\nuser mode again ( S3in Figure 3.1). The latter switch again requires changing\nthe MMU map and \ufb02ushing the TLB.\nInstead of using processes, an application can also be constructed such\nthat different parts are executed by separate threads. Communication between\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 107\nFigure 3.1: Context switching as the result of IPC.\nthose parts is entirely dealt with by using shared data. Thread switching can\nsometimes be done entirely in user space, although in other implementations,\nthe kernel is aware of threads and schedules them. The effect can be a dramatic\nimprovement in performance.\nFinally, there is also a pure software engineering reason to use threads:\nmany applications are simply easier to structure as a collection of cooperating\nthreads. Think of applications that need to perform several (more or less\nindependent) tasks, like our spreadsheet example discussed previously.\nNote 3.1 (Advanced: The cost of a context switch)\nThere have been many studies on measuring the performance effects of context\nswitches. As in so many cases with measuring computer systems, \ufb01nding the\nground truth is not easy. Tsafrir [2007] notes that handling clock ticks has become\nmore or less ubiquitous in operating systems, making it an excellent candidate\nto measure overheads. A clock handler is activated once every Tmilliseconds\nby means of a clock interrupt. Common values for Trange between 0.5 and\n20 milliseconds, corresponding to interrupt frequencies of 2000 Hz and 50 Hz,\nrespectively. The handler typically assists in realizing various timing and CPU\nusage services, sends alarm signals, and assists in preempting running tasks\nfor fair CPU sharing. By simply varying the frequency by which the hardware\ngenerates an interrupt, one can easily get an impression of the incurred overhead.\nTo measure the performance effects of an interrupt, a distinction is made\nbetween direct overhead and indirect overhead . The direct overhead consists of\nthe time it takes to do the actual context switch along with the time it takes for\nthe handler to do its work and subsequently switching back to the interrupted\ntask. The indirect overhead is everything else, and is mainly caused by cache\nperturbations (to which we will return shortly). For various Intel processors,\nTsafrir [2007] found that the time to switch context is in the order of 0.5\u20131\nmicrosecond, and that the handler itself takes in the order of 0.5\u20137 microseconds\nto do its work, depending strongly on the implementation.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 107\nFigure 3.1: Context switching as the result of IPC.\nthose parts is entirely dealt with by using shared data. Thread switching can\nsometimes be done entirely in user space, although in other implementations,\nthe kernel is aware of threads and schedules them. The effect can be a dramatic\nimprovement in performance.\nFinally, there is also a pure software engineering reason to use threads:\nmany applications are simply easier to structure as a collection of cooperating\nthreads. Think of applications that need to perform several (more or less\nindependent) tasks, like our spreadsheet example discussed previously.\nNote 3.1 (Advanced: The cost of a context switch)\nThere have been many studies on measuring the performance effects of context\nswitches. As in so many cases with measuring computer systems, \ufb01nding the\nground truth is not easy. Tsafrir [2007] notes that handling clock ticks has become\nmore or less ubiquitous in operating systems, making it an excellent candidate\nto measure overheads. A clock handler is activated once every Tmilliseconds\nby means of a clock interrupt. Common values for Trange between 0.5 and\n20 milliseconds, corresponding to interrupt frequencies of 2000 Hz and 50 Hz,\nrespectively. The handler typically assists in realizing various timing and CPU\nusage services, sends alarm signals, and assists in preempting running tasks\nfor fair CPU sharing. By simply varying the frequency by which the hardware\ngenerates an interrupt, one can easily get an impression of the incurred overhead.\nTo measure the performance effects of an interrupt, a distinction is made\nbetween direct overhead and indirect overhead . The direct overhead consists of\nthe time it takes to do the actual context switch along with the time it takes for\nthe handler to do its work and subsequently switching back to the interrupted\ntask. The indirect overhead is everything else, and is mainly caused by cache\nperturbations (to which we will return shortly). For various Intel processors,\nTsafrir [2007] found that the time to switch context is in the order of 0.5\u20131\nmicrosecond, and that the handler itself takes in the order of 0.5\u20137 microseconds\nto do its work, depending strongly on the implementation.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "108 CHAPTER 3. PROCESSES\nHowever, it turns out that the direct overhead is not really that in\ufb02uential. In\na complimentary study, Liu and Solihin [2010] make clear that context switching\ncan greatly perturbate the cache resulting in a loss of performance in comparison\nto the situation before an interrupt occurred. In fact, for the simple case of clock\ninterrupts, Tsafrir [2007] measured an indirect overhead of approximately 80%.\nTo understand what is going on, consider the data organizations as sketched in\nFigure 3.2. Assume the cache is organized such that a least-recently used block of\ndata is removed from the cache when room is needed for a fresh data block.\n(a) (b) (c)\nFigure 3.2: The organization of the cache when dealing with interrupts:\n(a) before the context switch, (b) after the context switch, and (c) after\naccessing block D. (Adapted from [Liu and Solihin, 2010].)\nFigure 3.2(a) shows the situation before the interrupt occurs. After the inter-\nrupt has been handled, block Dmay have been evicted from the cache, leaving a\nhole as shown in Figure 3.2(b). Accessing block Dwill copy it back into the cache,\npossibly evicting block C, and so on. In other words, even a simple interrupt may\ncause a considerable, and relatively long-lasting reorganization of the cache, in\nturn, affecting the overall performance of an application.\nThread implementation\nThreads are often provided in the form of a thread package. Such a package\ncontains operations to create and destroy threads as well as operations on\nsynchronization variables such as mutexes and condition variables. There are\nbasically two approaches to implement a thread package. The \ufb01rst approach\nis to construct a thread library that is executed entirely in user space. The\nsecond approach is to have the kernel be aware of threads and schedule them.\nA user-level thread library has a number of advantages. First, it is cheap\nto create and destroy threads. Because all thread administration is kept in the\nuser\u2019s address space, the price of creating a thread is primarily determined\nby the cost for allocating memory to set up a thread stack. Analogously,\ndestroying a thread mainly involves freeing memory for the stack, which is\nno longer used. Both operations are cheap.\nA second advantage of user-level threads is that switching thread context\ncan often be done in just a few instructions. Basically, only the values of\nthe CPU registers need to be stored and subsequently reloaded with the\npreviously stored values of the thread to which it is being switched. There is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n108 CHAPTER 3. PROCESSES\nHowever, it turns out that the direct overhead is not really that in\ufb02uential. In\na complimentary study, Liu and Solihin [2010] make clear that context switching\ncan greatly perturbate the cache resulting in a loss of performance in comparison\nto the situation before an interrupt occurred. In fact, for the simple case of clock\ninterrupts, Tsafrir [2007] measured an indirect overhead of approximately 80%.\nTo understand what is going on, consider the data organizations as sketched in\nFigure 3.2. Assume the cache is organized such that a least-recently used block of\ndata is removed from the cache when room is needed for a fresh data block.\n(a) (b) (c)\nFigure 3.2: The organization of the cache when dealing with interrupts:\n(a) before the context switch, (b) after the context switch, and (c) after\naccessing block D. (Adapted from [Liu and Solihin, 2010].)\nFigure 3.2(a) shows the situation before the interrupt occurs. After the inter-\nrupt has been handled, block Dmay have been evicted from the cache, leaving a\nhole as shown in Figure 3.2(b). Accessing block Dwill copy it back into the cache,\npossibly evicting block C, and so on. In other words, even a simple interrupt may\ncause a considerable, and relatively long-lasting reorganization of the cache, in\nturn, affecting the overall performance of an application.\nThread implementation\nThreads are often provided in the form of a thread package. Such a package\ncontains operations to create and destroy threads as well as operations on\nsynchronization variables such as mutexes and condition variables. There are\nbasically two approaches to implement a thread package. The \ufb01rst approach\nis to construct a thread library that is executed entirely in user space. The\nsecond approach is to have the kernel be aware of threads and schedule them.\nA user-level thread library has a number of advantages. First, it is cheap\nto create and destroy threads. Because all thread administration is kept in the\nuser\u2019s address space, the price of creating a thread is primarily determined\nby the cost for allocating memory to set up a thread stack. Analogously,\ndestroying a thread mainly involves freeing memory for the stack, which is\nno longer used. Both operations are cheap.\nA second advantage of user-level threads is that switching thread context\ncan often be done in just a few instructions. Basically, only the values of\nthe CPU registers need to be stored and subsequently reloaded with the\npreviously stored values of the thread to which it is being switched. There is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 109\nno need to change memory maps, \ufb02ush the TLB, do CPU accounting, and so\non. Switching thread context is done when two threads need to synchronize,\nfor example, when entering a section of shared data. However, as discussed in\nNote 3.1, much of the overhead of context switching is caused by perturbating\nmemory caches.\nA major drawback of user-level threads comes from deploying the many-\nto-one threading model : multiple threads are mapped to a single schedulable\nentity. As a consequence, the invocation of a blocking system call will immedi-\nately block the entire process to which the thread belongs, and thus also all the\nother threads in that process. As we explained, threads are particularly useful\nto structure large applications into parts that could be logically executed at\nthe same time. In that case, blocking on I/O should not prevent other parts to\nbe executed in the meantime. For such applications, user-level threads are of\nno help.\nThese problems can be mostly circumvented by implementing threads in\nthe operating system\u2019s kernel, leading to what is known as the one-to-one\nthreading model in which every thread is a schedulable entity. The price to\npay is that every thread operation (creation, deletion, synchronization, etc.),\nwill have to be carried out by the kernel, requiring a system call. Switching\nthread contexts may now become as expensive as switching process contexts.\nHowever, in light of the fact that the performance of context switching is gen-\nerally dictated by ineffective use of memory caches, and not by the distinction\nbetween the many-to-one or one-to-one threading model, many operating\nsystems now offer the latter model, if only for its simplicity.\nNote 3.2 (Advanced: Lightweight processes)\nAn alternative to the two threading extremes is a hybrid form of user-level and\nkernel-level threads, a so-called many-to-many threading model . The model is\nimplemented in the form of lightweight processes (LWP ). An LWP runs in the\ncontext of a single (heavy-weight) process, and there can be several LWPs per\nprocess. In addition to having LWPs, a system also offers a user-level thread\npackage, offering applications the usual operations for creating and destroying\nthreads. In addition, the package provides facilities for thread synchronization,\nsuch as mutexes and condition variables. The important issue is that the thread\npackage is implemented entirely in user space. In other words, all operations on\nthreads are carried out without intervention of the kernel.\nThe thread package can be shared by multiple LWPs, as shown in Figure 3.3.\nThis means that each LWP can be running its own (user-level) thread. Mul-\ntithreaded applications are constructed by creating threads, and subsequently\nassigning each thread to an LWP . Assigning a thread to an LWP is normally\nimplicit and hidden from the programmer.\nThe combination of (user-level) threads and LWPs works as follows. The\nthread package has a single routine to schedule the next thread. When creating\nan LWP (which is done by means of a system call), the LWP is given its own stack,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 109\nno need to change memory maps, \ufb02ush the TLB, do CPU accounting, and so\non. Switching thread context is done when two threads need to synchronize,\nfor example, when entering a section of shared data. However, as discussed in\nNote 3.1, much of the overhead of context switching is caused by perturbating\nmemory caches.\nA major drawback of user-level threads comes from deploying the many-\nto-one threading model : multiple threads are mapped to a single schedulable\nentity. As a consequence, the invocation of a blocking system call will immedi-\nately block the entire process to which the thread belongs, and thus also all the\nother threads in that process. As we explained, threads are particularly useful\nto structure large applications into parts that could be logically executed at\nthe same time. In that case, blocking on I/O should not prevent other parts to\nbe executed in the meantime. For such applications, user-level threads are of\nno help.\nThese problems can be mostly circumvented by implementing threads in\nthe operating system\u2019s kernel, leading to what is known as the one-to-one\nthreading model in which every thread is a schedulable entity. The price to\npay is that every thread operation (creation, deletion, synchronization, etc.),\nwill have to be carried out by the kernel, requiring a system call. Switching\nthread contexts may now become as expensive as switching process contexts.\nHowever, in light of the fact that the performance of context switching is gen-\nerally dictated by ineffective use of memory caches, and not by the distinction\nbetween the many-to-one or one-to-one threading model, many operating\nsystems now offer the latter model, if only for its simplicity.\nNote 3.2 (Advanced: Lightweight processes)\nAn alternative to the two threading extremes is a hybrid form of user-level and\nkernel-level threads, a so-called many-to-many threading model . The model is\nimplemented in the form of lightweight processes (LWP ). An LWP runs in the\ncontext of a single (heavy-weight) process, and there can be several LWPs per\nprocess. In addition to having LWPs, a system also offers a user-level thread\npackage, offering applications the usual operations for creating and destroying\nthreads. In addition, the package provides facilities for thread synchronization,\nsuch as mutexes and condition variables. The important issue is that the thread\npackage is implemented entirely in user space. In other words, all operations on\nthreads are carried out without intervention of the kernel.\nThe thread package can be shared by multiple LWPs, as shown in Figure 3.3.\nThis means that each LWP can be running its own (user-level) thread. Mul-\ntithreaded applications are constructed by creating threads, and subsequently\nassigning each thread to an LWP . Assigning a thread to an LWP is normally\nimplicit and hidden from the programmer.\nThe combination of (user-level) threads and LWPs works as follows. The\nthread package has a single routine to schedule the next thread. When creating\nan LWP (which is done by means of a system call), the LWP is given its own stack,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "110 CHAPTER 3. PROCESSES\nand is instructed to execute the scheduling routine in search of a thread to execute.\nIf there are several LWPs, then each of them executes the scheduler. The thread\ntable, which is used to keep track of the current set of threads, is thus shared by\nthe LWPs. Protecting this table to guarantee mutually exclusive access is done by\nmeans of mutexes that are implemented entirely in user space. In other words,\nsynchronization between LWPs does not require any kernel support.\nFigure 3.3: Combining kernel-level processes and user-level threads.\nWhen an LWP \ufb01nds a runnable thread, it switches context to that thread.\nMeanwhile, other LWPs may be looking for other runnable threads as well. If a\nthread needs to block on a mutex or condition variable, it does the necessary ad-\nministration and eventually calls the scheduling routine. When another runnable\nthread has been found, a context switch is made to that thread. The beauty of all\nthis is that the LWP executing the thread need not be informed: the context switch\nis implemented completely in user space and appears to the LWP as normal\nprogram code.\nNow let us see what happens when a thread does a blocking system call. In\nthat case, execution changes from user mode to kernel mode, but still continues\nin the context of the current LWP . At the point where the current LWP can no\nlonger continue, the operating system may decide to switch context to another\nLWP , which also implies that a context switch is made back to user mode. The\nselected LWP will simply continue where it had previously left off.\nThere are several advantages to using LWPs in combination with a user-\nlevel thread package. First, creating, destroying, and synchronizing threads is\nrelatively cheap and involves no kernel intervention at all. Second, provided\nthat a process has enough LWPs, a blocking system call will not suspend the\nentire process. Third, there is no need for an application to know about the\nLWPs. All it sees are user-level threads. Fourth, LWPs can be easily used in\nmultiprocessing environments by executing different LWPs on different CPUs.\nThis multiprocessing can be hidden entirely from the application. The only\ndrawback of lightweight processes in combination with user-level threads is that\nwe still need to create and destroy LWPs, which is just as expensive as with\nkernel-level threads. However, creating and destroying LWPs needs to be done\nonly occasionally, and is often fully controlled by the operating system.\nAn alternative, but similar approach to lightweight processes, is to make use\nofscheduler activations [Anderson et al., 1991]. The most essential difference\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n110 CHAPTER 3. PROCESSES\nand is instructed to execute the scheduling routine in search of a thread to execute.\nIf there are several LWPs, then each of them executes the scheduler. The thread\ntable, which is used to keep track of the current set of threads, is thus shared by\nthe LWPs. Protecting this table to guarantee mutually exclusive access is done by\nmeans of mutexes that are implemented entirely in user space. In other words,\nsynchronization between LWPs does not require any kernel support.\nFigure 3.3: Combining kernel-level processes and user-level threads.\nWhen an LWP \ufb01nds a runnable thread, it switches context to that thread.\nMeanwhile, other LWPs may be looking for other runnable threads as well. If a\nthread needs to block on a mutex or condition variable, it does the necessary ad-\nministration and eventually calls the scheduling routine. When another runnable\nthread has been found, a context switch is made to that thread. The beauty of all\nthis is that the LWP executing the thread need not be informed: the context switch\nis implemented completely in user space and appears to the LWP as normal\nprogram code.\nNow let us see what happens when a thread does a blocking system call. In\nthat case, execution changes from user mode to kernel mode, but still continues\nin the context of the current LWP . At the point where the current LWP can no\nlonger continue, the operating system may decide to switch context to another\nLWP , which also implies that a context switch is made back to user mode. The\nselected LWP will simply continue where it had previously left off.\nThere are several advantages to using LWPs in combination with a user-\nlevel thread package. First, creating, destroying, and synchronizing threads is\nrelatively cheap and involves no kernel intervention at all. Second, provided\nthat a process has enough LWPs, a blocking system call will not suspend the\nentire process. Third, there is no need for an application to know about the\nLWPs. All it sees are user-level threads. Fourth, LWPs can be easily used in\nmultiprocessing environments by executing different LWPs on different CPUs.\nThis multiprocessing can be hidden entirely from the application. The only\ndrawback of lightweight processes in combination with user-level threads is that\nwe still need to create and destroy LWPs, which is just as expensive as with\nkernel-level threads. However, creating and destroying LWPs needs to be done\nonly occasionally, and is often fully controlled by the operating system.\nAn alternative, but similar approach to lightweight processes, is to make use\nofscheduler activations [Anderson et al., 1991]. The most essential difference\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 111\nbetween scheduler activations and LWPs is that when a thread blocks on a\nsystem call, the kernel does an upcall to the thread package, effectively calling\nthe scheduler routine to select the next runnable thread. The same procedure\nis repeated when a thread is unblocked. The advantage of this approach is that\nit saves management of LWPs by the kernel. However, the use of upcalls is\nconsidered less elegant, as it violates the structure of layered systems, in which\ncalls only to the next lower-level layer are permitted.\nBecause of the intricacies involved in deploying a many-to-many threading\nmodel, and the relatively low performance gain (which is now understood to be\ndominated by caching behavior), the simple one-to-one threading model is often\npreferred.\nAs a \ufb01nal note, it is important to realize that using threads is one way\nof organizing simultaneous and concurrent executions within an application.\nIn practice, we often see that applications are constructed as a collection of\nconcurrent processes , jointly making use of the interprocess facilities offered by\nan operating system (see also [Robbins and Robbins, 2003; Stevens, 1999]). A\ngood example of this approach is the organization of the Apache Web server,\nwhich, by default, starts with a handful of processes for handling incoming\nrequests. Each process forms a single-threaded instantiation of the server,\nyet is capable of communicating with other instances through fairly standard\nmeans.\nAs argued by Srinivasan [2010], using processes instead of threads has\nthe important advantage of separating the data space: each process works on\nits own part of data and is protected from interference from others through\nthe operating system. The advantage of this separation should not be un-\nderestimated: thread programming is considered to be notoriously dif\ufb01cult\nbecause the developer is fully responsible for managing concurrent access\nto shared data. Using processes, data spaces, in the end, are protected by\nhardware support. If a process attempts to access data outside its allocated\nmemory, the hardware will raise an exception, which is then further processed\nby the operating system. No such support is available for threads concurrently\noperating within the same process.\nThreads in distributed systems\nAn important property of threads is that they can provide a convenient means\nof allowing blocking system calls without blocking the entire process in which\nthe thread is running. This property makes threads particularly attractive to\nuse in distributed systems as it makes it much easier to express communication\nin the form of maintaining multiple logical connections at the same time. We\nillustrate this point by taking a closer look at multithreaded clients and servers,\nrespectively.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 111\nbetween scheduler activations and LWPs is that when a thread blocks on a\nsystem call, the kernel does an upcall to the thread package, effectively calling\nthe scheduler routine to select the next runnable thread. The same procedure\nis repeated when a thread is unblocked. The advantage of this approach is that\nit saves management of LWPs by the kernel. However, the use of upcalls is\nconsidered less elegant, as it violates the structure of layered systems, in which\ncalls only to the next lower-level layer are permitted.\nBecause of the intricacies involved in deploying a many-to-many threading\nmodel, and the relatively low performance gain (which is now understood to be\ndominated by caching behavior), the simple one-to-one threading model is often\npreferred.\nAs a \ufb01nal note, it is important to realize that using threads is one way\nof organizing simultaneous and concurrent executions within an application.\nIn practice, we often see that applications are constructed as a collection of\nconcurrent processes , jointly making use of the interprocess facilities offered by\nan operating system (see also [Robbins and Robbins, 2003; Stevens, 1999]). A\ngood example of this approach is the organization of the Apache Web server,\nwhich, by default, starts with a handful of processes for handling incoming\nrequests. Each process forms a single-threaded instantiation of the server,\nyet is capable of communicating with other instances through fairly standard\nmeans.\nAs argued by Srinivasan [2010], using processes instead of threads has\nthe important advantage of separating the data space: each process works on\nits own part of data and is protected from interference from others through\nthe operating system. The advantage of this separation should not be un-\nderestimated: thread programming is considered to be notoriously dif\ufb01cult\nbecause the developer is fully responsible for managing concurrent access\nto shared data. Using processes, data spaces, in the end, are protected by\nhardware support. If a process attempts to access data outside its allocated\nmemory, the hardware will raise an exception, which is then further processed\nby the operating system. No such support is available for threads concurrently\noperating within the same process.\nThreads in distributed systems\nAn important property of threads is that they can provide a convenient means\nof allowing blocking system calls without blocking the entire process in which\nthe thread is running. This property makes threads particularly attractive to\nuse in distributed systems as it makes it much easier to express communication\nin the form of maintaining multiple logical connections at the same time. We\nillustrate this point by taking a closer look at multithreaded clients and servers,\nrespectively.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "112 CHAPTER 3. PROCESSES\nMultithreaded clients\nTo establish a high degree of distribution transparency, distributed systems\nthat operate in wide-area networks may need to conceal long interprocess\nmessage propagation times. The round-trip delay in a wide-area network can\neasily be in the order of hundreds of milliseconds, or sometimes even seconds.\nThe usual way to hide communication latencies is to initiate communica-\ntion and immediately proceed with something else. A typical example where\nthis happens is in Web browsers. In many cases, a Web document consists of\nan HTML \ufb01le containing plain text along with a collection of images, icons,\netc. To fetch each element of a Web document, the browser has to set up a\nTCP/IP connection, read the incoming data, and pass it to a display compo-\nnent. Setting up a connection as well as reading incoming data are inherently\nblocking operations. When dealing with long-haul communication, we also\nhave the disadvantage that the time for each operation to complete may be\nrelatively long.\nA Web browser often starts with fetching the HTML page and subsequently\ndisplays it. To hide communication latencies as much as possible, some\nbrowsers start displaying data while it is still coming in. While the text is\nmade available to the user, including the facilities for scrolling and such, the\nbrowser continues with fetching other \ufb01les that make up the page, such as\nthe images. The latter are displayed as they are brought in. The user need\nthus not wait until all the components of the entire page are fetched before\nthe page is made available.\nIn effect, it is seen that the Web browser is doing a number of tasks\nsimultaneously. As it turns out, developing the browser as a multithreaded\nclient simpli\ufb01es matters considerably. As soon as the main HTML \ufb01le has\nbeen fetched, separate threads can be activated to take care of fetching the\nother parts. Each thread sets up a separate connection to the server and pulls\nin the data. Setting up a connection and reading data from the server can\nbe programmed using the standard (blocking) system calls, assuming that\na blocking call does not suspend the entire process. As is also illustrated\nin [Stevens, 1998], the code for each thread is the same and, above all, simple.\nMeanwhile, the user notices only delays in the display of images and such,\nbut can otherwise browse through the document.\nThere is another important bene\ufb01t to using multithreaded Web browsers\nin which several connections can be opened simultaneously. In the previous\nexample, several connections were set up to the same server. If that server is\nheavily loaded, or just plain slow, no real performance improvements will be\nnoticed compared to pulling in the \ufb01les that make up the page strictly one\nafter the other.\nHowever, in many cases, Web servers have been replicated across multiple\nmachines, where each server provides exactly the same set of Web documents.\nThe replicated servers are located at the same site, and are known under the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n112 CHAPTER 3. PROCESSES\nMultithreaded clients\nTo establish a high degree of distribution transparency, distributed systems\nthat operate in wide-area networks may need to conceal long interprocess\nmessage propagation times. The round-trip delay in a wide-area network can\neasily be in the order of hundreds of milliseconds, or sometimes even seconds.\nThe usual way to hide communication latencies is to initiate communica-\ntion and immediately proceed with something else. A typical example where\nthis happens is in Web browsers. In many cases, a Web document consists of\nan HTML \ufb01le containing plain text along with a collection of images, icons,\netc. To fetch each element of a Web document, the browser has to set up a\nTCP/IP connection, read the incoming data, and pass it to a display compo-\nnent. Setting up a connection as well as reading incoming data are inherently\nblocking operations. When dealing with long-haul communication, we also\nhave the disadvantage that the time for each operation to complete may be\nrelatively long.\nA Web browser often starts with fetching the HTML page and subsequently\ndisplays it. To hide communication latencies as much as possible, some\nbrowsers start displaying data while it is still coming in. While the text is\nmade available to the user, including the facilities for scrolling and such, the\nbrowser continues with fetching other \ufb01les that make up the page, such as\nthe images. The latter are displayed as they are brought in. The user need\nthus not wait until all the components of the entire page are fetched before\nthe page is made available.\nIn effect, it is seen that the Web browser is doing a number of tasks\nsimultaneously. As it turns out, developing the browser as a multithreaded\nclient simpli\ufb01es matters considerably. As soon as the main HTML \ufb01le has\nbeen fetched, separate threads can be activated to take care of fetching the\nother parts. Each thread sets up a separate connection to the server and pulls\nin the data. Setting up a connection and reading data from the server can\nbe programmed using the standard (blocking) system calls, assuming that\na blocking call does not suspend the entire process. As is also illustrated\nin [Stevens, 1998], the code for each thread is the same and, above all, simple.\nMeanwhile, the user notices only delays in the display of images and such,\nbut can otherwise browse through the document.\nThere is another important bene\ufb01t to using multithreaded Web browsers\nin which several connections can be opened simultaneously. In the previous\nexample, several connections were set up to the same server. If that server is\nheavily loaded, or just plain slow, no real performance improvements will be\nnoticed compared to pulling in the \ufb01les that make up the page strictly one\nafter the other.\nHowever, in many cases, Web servers have been replicated across multiple\nmachines, where each server provides exactly the same set of Web documents.\nThe replicated servers are located at the same site, and are known under the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 113\nsame name. When a request for a Web page comes in, the request is forwarded\nto one of the servers, often using a round-robin strategy or some other load-\nbalancing technique. When using a multithreaded client, connections may\nbe set up to different replicas, allowing data to be transferred in parallel,\neffectively establishing that the entire Web document is fully displayed in a\nmuch shorter time than with a nonreplicated server. This approach is possible\nonly if the client can handle truly parallel streams of incoming data. Threads\nare ideal for this purpose.\nNote 3.3 (Advanced: Exploiting client-side threads for performance)\nAlthough there are obvious opportunities for using threads to reach high perfor-\nmance, it is interesting to see whether multithreading is effectively exploited. In a\nstudy to see to what extent multiple threads put a multicore processor to work,\nBlake et al. [2010] looked at the execution of various applications on modern\narchitectures. Browsers, like many other client-side applications, are interactive\nby nature for which reason the expected processor idle time may be quite high.\nIn order to properly measure to what extent a multicore processor is being used,\nBlake et al. used a metric known as thread-level parallelism (TLP ). Let cidenote\nthe fraction of time that exactly ithreads are being executed simultaneously.\nThread-level parallelism is then de\ufb01ned as:\nTLP=\u00e5N\ni=1i\u0001ci\n1\u0000c0\nwhere Nis the maximum number of threads that (can) execute at the same\ntime. In their study, a typical Web browser had a TLP value between 1.5 and 2.5,\nmeaning that in order to effectively exploit parallelism, the client machine should\nhave two or three cores, or likewise, 2\u20133 processors.\nThese results are interesting when considering that modern Web browsers\ncreate hundreds of threads, and that tens of threads are active at the same time\n(note that an active thread is not necessarily running; it may be blocked waiting for\nan I/O request to complete). We thus see that multithreading is used to organize\nan application, but that this multithreading is not leading to dramatic performance\nimprovements through hardware exploitation. That browsers can be effectively\ndesigned for exploiting parallelism is shown, for example, by Meyerovich and\nBodik [2010]. By adapting existing algorithms, the authors manage to establish\nseveral-fold speedups.\nMultithreaded servers\nAlthough there are important bene\ufb01ts to multithreaded clients, the main use\nof multithreading in distributed systems is found at the server side. Practice\nshows that multithreading not only simpli\ufb01es server code considerably, but\nalso makes it much easier to develop servers that exploit parallelism to attain\nhigh performance, even on uniprocessor systems. However, with modern\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 113\nsame name. When a request for a Web page comes in, the request is forwarded\nto one of the servers, often using a round-robin strategy or some other load-\nbalancing technique. When using a multithreaded client, connections may\nbe set up to different replicas, allowing data to be transferred in parallel,\neffectively establishing that the entire Web document is fully displayed in a\nmuch shorter time than with a nonreplicated server. This approach is possible\nonly if the client can handle truly parallel streams of incoming data. Threads\nare ideal for this purpose.\nNote 3.3 (Advanced: Exploiting client-side threads for performance)\nAlthough there are obvious opportunities for using threads to reach high perfor-\nmance, it is interesting to see whether multithreading is effectively exploited. In a\nstudy to see to what extent multiple threads put a multicore processor to work,\nBlake et al. [2010] looked at the execution of various applications on modern\narchitectures. Browsers, like many other client-side applications, are interactive\nby nature for which reason the expected processor idle time may be quite high.\nIn order to properly measure to what extent a multicore processor is being used,\nBlake et al. used a metric known as thread-level parallelism (TLP ). Let cidenote\nthe fraction of time that exactly ithreads are being executed simultaneously.\nThread-level parallelism is then de\ufb01ned as:\nTLP=\u00e5N\ni=1i\u0001ci\n1\u0000c0\nwhere Nis the maximum number of threads that (can) execute at the same\ntime. In their study, a typical Web browser had a TLP value between 1.5 and 2.5,\nmeaning that in order to effectively exploit parallelism, the client machine should\nhave two or three cores, or likewise, 2\u20133 processors.\nThese results are interesting when considering that modern Web browsers\ncreate hundreds of threads, and that tens of threads are active at the same time\n(note that an active thread is not necessarily running; it may be blocked waiting for\nan I/O request to complete). We thus see that multithreading is used to organize\nan application, but that this multithreading is not leading to dramatic performance\nimprovements through hardware exploitation. That browsers can be effectively\ndesigned for exploiting parallelism is shown, for example, by Meyerovich and\nBodik [2010]. By adapting existing algorithms, the authors manage to establish\nseveral-fold speedups.\nMultithreaded servers\nAlthough there are important bene\ufb01ts to multithreaded clients, the main use\nof multithreading in distributed systems is found at the server side. Practice\nshows that multithreading not only simpli\ufb01es server code considerably, but\nalso makes it much easier to develop servers that exploit parallelism to attain\nhigh performance, even on uniprocessor systems. However, with modern\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "114 CHAPTER 3. PROCESSES\nmulticore processors, multithreading for parallelism is an obvious path to\nfollow.\nTo understand the bene\ufb01ts of threads for writing server code, consider the\norganization of a \ufb01le server that occasionally has to block waiting for the disk.\nThe \ufb01le server normally waits for an incoming request for a \ufb01le operation,\nsubsequently carries out the request, and then sends back the reply. One\npossible, and particularly popular organization is shown in Figure 3.4. Here\none thread, the dispatcher , reads incoming requests for a \ufb01le operation. The\nrequests are sent by clients to a well-known end point for this server. After\nexamining the request, the server chooses an idle (i.e., blocked) worker thread\nand hands it the request.\nFigure 3.4: A multithreaded server organized in a dispatcher/worker model.\nThe worker proceeds by performing a blocking read on the local \ufb01le system,\nwhich may cause the thread to be suspended until the data are fetched from\ndisk. If the thread is suspended, another thread is selected to be executed. For\nexample, the dispatcher may be selected to acquire more work. Alternatively,\nanother worker thread can be selected that is now ready to run.\nNow consider how the \ufb01le server might have been written in the absence of\nthreads. One possibility is to have it operate as a single thread. The main loop\nof the \ufb01le server gets a request, examines it, and carries it out to completion\nbefore getting the next one. While waiting for the disk, the server is idle and\ndoes not process any other requests. Consequently, requests from other clients\ncannot be handled. In addition, if the \ufb01le server is running on a dedicated\nmachine, as is commonly the case, the CPU is simply idle while the \ufb01le server\nis waiting for the disk. The net result is that many fewer requests per time\nunit can be processed. Thus threads gain considerable performance, but each\nthread is programmed sequentially, in the usual way.\nSo far we have seen two possible designs: a multithreaded \ufb01le server\nand a single-threaded \ufb01le server. A third alternative is to run the server as\na big single-threaded \ufb01nite-state machine. When a request comes in, the\none and only thread examines it. If it can be satis\ufb01ed from the in-memory\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n114 CHAPTER 3. PROCESSES\nmulticore processors, multithreading for parallelism is an obvious path to\nfollow.\nTo understand the bene\ufb01ts of threads for writing server code, consider the\norganization of a \ufb01le server that occasionally has to block waiting for the disk.\nThe \ufb01le server normally waits for an incoming request for a \ufb01le operation,\nsubsequently carries out the request, and then sends back the reply. One\npossible, and particularly popular organization is shown in Figure 3.4. Here\none thread, the dispatcher , reads incoming requests for a \ufb01le operation. The\nrequests are sent by clients to a well-known end point for this server. After\nexamining the request, the server chooses an idle (i.e., blocked) worker thread\nand hands it the request.\nFigure 3.4: A multithreaded server organized in a dispatcher/worker model.\nThe worker proceeds by performing a blocking read on the local \ufb01le system,\nwhich may cause the thread to be suspended until the data are fetched from\ndisk. If the thread is suspended, another thread is selected to be executed. For\nexample, the dispatcher may be selected to acquire more work. Alternatively,\nanother worker thread can be selected that is now ready to run.\nNow consider how the \ufb01le server might have been written in the absence of\nthreads. One possibility is to have it operate as a single thread. The main loop\nof the \ufb01le server gets a request, examines it, and carries it out to completion\nbefore getting the next one. While waiting for the disk, the server is idle and\ndoes not process any other requests. Consequently, requests from other clients\ncannot be handled. In addition, if the \ufb01le server is running on a dedicated\nmachine, as is commonly the case, the CPU is simply idle while the \ufb01le server\nis waiting for the disk. The net result is that many fewer requests per time\nunit can be processed. Thus threads gain considerable performance, but each\nthread is programmed sequentially, in the usual way.\nSo far we have seen two possible designs: a multithreaded \ufb01le server\nand a single-threaded \ufb01le server. A third alternative is to run the server as\na big single-threaded \ufb01nite-state machine. When a request comes in, the\none and only thread examines it. If it can be satis\ufb01ed from the in-memory\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.1. THREADS 115\ncache, \ufb01ne, but if not, the thread must access the disk. However, instead\nof issuing a blocking disk operation, the thread schedules an asynchronous\n(i.e., nonblocking) disk operation for which it will be later interrupted by the\noperating system. To make this work, the thread will record the status of the\nrequest (namely, that it has a pending disk operation), and continues to see if\nthere were any other incoming requests that require its attention.\nOnce a pending disk operation has been completed, the operating system\nwill notify the thread, who will then, in due time, look up the status of the\nassociated request and continue processing it. Eventually, a response will be\nsent to the originating client, again using a nonblocking call to send a message\nover the network.\nIn this design, the \u201csequential process\u201d model that we had in the \ufb01rst\ntwo cases is lost. Every time the thread needs to do a blocking operation, it\nneeds to record exactly where it was in processing the request, possibly also\nstoring additional state. Once that has been done, it can start the operation\nand continue with other work. Other work means processing newly arrived\nrequests, or postprocessing requests for which a previously started operation\nhas completed. Of course, if there is no work to be done, the thread may\nindeed block. In effect, we are simulating the behavior of multiple threads\nand their respective stacks the hard way. The process is being operated as\na \ufb01nite-state machine that gets an event and then reacts to it, depending on\nwhat is in it.\nModel Characteristics\nMultithreading Parallelism, blocking system calls\nSingle-threaded process No parallelism, blocking system calls\nFinite-state machine Parallelism, nonblocking system calls\nFigure 3.5: Three ways to construct a server.\nIt should now be clear what threads have to offer. They make it possible to\nretain the idea of sequential processes that make blocking system calls and still\nachieve parallelism. Blocking system calls make programming easier as they\nappear as just normal procedure calls. In addition, multiple threads allow for\nparallelism and thus performance improvement. The single-threaded server\nretains the ease and simplicity of blocking system calls, but may severely\nhinder performance in terms of number of requests that can be handled\nper time unit. The \ufb01nite-state machine approach achieves high performance\nthrough parallelism, but uses nonblocking calls, which is generally hard to\nprogram and thus to maintain. These models are summarized in Figure 3.5.\nAgain, note that instead of using threads, we can also use multiple pro-\ncesses to organize a server (leading to the situation that we actually have a\nmultiprocess server). The advantage is that the operating system can offer\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.1. THREADS 115\ncache, \ufb01ne, but if not, the thread must access the disk. However, instead\nof issuing a blocking disk operation, the thread schedules an asynchronous\n(i.e., nonblocking) disk operation for which it will be later interrupted by the\noperating system. To make this work, the thread will record the status of the\nrequest (namely, that it has a pending disk operation), and continues to see if\nthere were any other incoming requests that require its attention.\nOnce a pending disk operation has been completed, the operating system\nwill notify the thread, who will then, in due time, look up the status of the\nassociated request and continue processing it. Eventually, a response will be\nsent to the originating client, again using a nonblocking call to send a message\nover the network.\nIn this design, the \u201csequential process\u201d model that we had in the \ufb01rst\ntwo cases is lost. Every time the thread needs to do a blocking operation, it\nneeds to record exactly where it was in processing the request, possibly also\nstoring additional state. Once that has been done, it can start the operation\nand continue with other work. Other work means processing newly arrived\nrequests, or postprocessing requests for which a previously started operation\nhas completed. Of course, if there is no work to be done, the thread may\nindeed block. In effect, we are simulating the behavior of multiple threads\nand their respective stacks the hard way. The process is being operated as\na \ufb01nite-state machine that gets an event and then reacts to it, depending on\nwhat is in it.\nModel Characteristics\nMultithreading Parallelism, blocking system calls\nSingle-threaded process No parallelism, blocking system calls\nFinite-state machine Parallelism, nonblocking system calls\nFigure 3.5: Three ways to construct a server.\nIt should now be clear what threads have to offer. They make it possible to\nretain the idea of sequential processes that make blocking system calls and still\nachieve parallelism. Blocking system calls make programming easier as they\nappear as just normal procedure calls. In addition, multiple threads allow for\nparallelism and thus performance improvement. The single-threaded server\nretains the ease and simplicity of blocking system calls, but may severely\nhinder performance in terms of number of requests that can be handled\nper time unit. The \ufb01nite-state machine approach achieves high performance\nthrough parallelism, but uses nonblocking calls, which is generally hard to\nprogram and thus to maintain. These models are summarized in Figure 3.5.\nAgain, note that instead of using threads, we can also use multiple pro-\ncesses to organize a server (leading to the situation that we actually have a\nmultiprocess server). The advantage is that the operating system can offer\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "116 CHAPTER 3. PROCESSES\nmore protection against accidental access to shared data. However, if pro-\ncesses need to communicate a lot, we may see a noticeable adverse affect on\nperformance in comparison to using threads.\n3.2 Virtualization\nThreads and processes can be seen as a way to do more things at the same\ntime. In effect, they allow us to build (pieces of) programs that appear to be\nexecuted simultaneously. On a single-processor computer, this simultaneous\nexecution is, of course, an illusion. As there is only a single CPU, only an\ninstruction from a single thread or process will be executed at a time. By\nrapidly switching between threads and processes, the illusion of parallelism\nis created.\nThis separation between having a single CPU and being able to pretend\nthere are more can be extended to other resources as well, leading to what\nis known as resource virtualization . This virtualization has been applied for\nmany decades, but has received renewed interest as (distributed) computer\nsystems have become more commonplace and complex, leading to the sit-\nuation that application software is mostly always outliving its underlying\nsystems software and hardware.\nPrinciple of virtualization\nIn practice, every (distributed) computer system offers a programming in-\nterface to higher-level software, as shown in Figure 3.6(a). There are many\ndifferent types of interfaces, ranging from the basic instruction set as offered\nby a CPU to the vast collection of application programming interfaces that are\nshipped with many current middleware systems. In its essence, virtualization\ndeals with extending or replacing an existing interface so as to mimic the\nbehavior of another system, as shown in Figure 3.6(b). We will come to discuss\ntechnical details on virtualization shortly, but let us \ufb01rst concentrate on why\nvirtualization is important.\nVirtualization and distributed systems\nOne of the most important reasons for introducing virtualization back in the\n1970s, was to allow legacy software to run on expensive mainframe hardware.\nThe software not only included various applications, but in fact also the\noperating systems they were developed for. This approach toward supporting\nlegacy software has been successfully applied on the IBM 370 mainframes (and\ntheir successors) that offered a virtual machine to which different operating\nsystems had been ported.\nAs hardware became cheaper, computers became more powerful, and the\nnumber of different operating system \ufb02avors was reducing, virtualization\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n116 CHAPTER 3. PROCESSES\nmore protection against accidental access to shared data. However, if pro-\ncesses need to communicate a lot, we may see a noticeable adverse affect on\nperformance in comparison to using threads.\n3.2 Virtualization\nThreads and processes can be seen as a way to do more things at the same\ntime. In effect, they allow us to build (pieces of) programs that appear to be\nexecuted simultaneously. On a single-processor computer, this simultaneous\nexecution is, of course, an illusion. As there is only a single CPU, only an\ninstruction from a single thread or process will be executed at a time. By\nrapidly switching between threads and processes, the illusion of parallelism\nis created.\nThis separation between having a single CPU and being able to pretend\nthere are more can be extended to other resources as well, leading to what\nis known as resource virtualization . This virtualization has been applied for\nmany decades, but has received renewed interest as (distributed) computer\nsystems have become more commonplace and complex, leading to the sit-\nuation that application software is mostly always outliving its underlying\nsystems software and hardware.\nPrinciple of virtualization\nIn practice, every (distributed) computer system offers a programming in-\nterface to higher-level software, as shown in Figure 3.6(a). There are many\ndifferent types of interfaces, ranging from the basic instruction set as offered\nby a CPU to the vast collection of application programming interfaces that are\nshipped with many current middleware systems. In its essence, virtualization\ndeals with extending or replacing an existing interface so as to mimic the\nbehavior of another system, as shown in Figure 3.6(b). We will come to discuss\ntechnical details on virtualization shortly, but let us \ufb01rst concentrate on why\nvirtualization is important.\nVirtualization and distributed systems\nOne of the most important reasons for introducing virtualization back in the\n1970s, was to allow legacy software to run on expensive mainframe hardware.\nThe software not only included various applications, but in fact also the\noperating systems they were developed for. This approach toward supporting\nlegacy software has been successfully applied on the IBM 370 mainframes (and\ntheir successors) that offered a virtual machine to which different operating\nsystems had been ported.\nAs hardware became cheaper, computers became more powerful, and the\nnumber of different operating system \ufb02avors was reducing, virtualization\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.2. VIRTUALIZATION 117\n(a) (b)\nFigure 3.6: (a) General organization between a program, interface, and system.\n(b) General organization of virtualizing system A on top of B.\nbecame less of an issue. However, matters have changed again since the\nlate 1990s. First, while hardware and low-level systems software change\nreasonably fast, software at higher levels of abstraction (e.g., middleware and\napplications), are often much more stable. In other words, we are facing\nthe situation that legacy software cannot be maintained in the same pace as\nthe platforms it relies on. Virtualization can help here by porting the legacy\ninterfaces to the new platforms and thus immediately opening up the latter\nfor large classes of existing programs.\nNote 3.4 (Discussion: Stable software?)\nAlthough there is indeed a lot of legacy software that can bene\ufb01t from stable\ninterfaces to rapidly changing underlying hardware, it is a mistake to believe that\nthe software for widely available services hardly changes. With the increasing\nshift toward server-side computing in the form of software-as-a-service ( SaaS ),\nmuch software can be maintained for a relatively homogeneous platform, owned\nentirely by the organization offering the associated service. As a consequence,\nmaintaining software products can be much easier, as there is much lesser need\nto distribute changes to potentially millions of customers. In fact, changes may\nrapidly succeed each other following changes in available hardware and platform,\nbut without any client actually noticing downtimes [Barroso and H\u00f6lze, 2009].\nEqually important is the fact that networking has become completely\npervasive. It is hard to imagine that a modern computer is not connected to\na network. In practice, this connectivity requires that system administrators\nmaintain a large and heterogeneous collection of server computers, each\none running very different applications, which can be accessed by clients.\nAt the same time the various resources should be easily accessible to these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.2. VIRTUALIZATION 117\n(a) (b)\nFigure 3.6: (a) General organization between a program, interface, and system.\n(b) General organization of virtualizing system A on top of B.\nbecame less of an issue. However, matters have changed again since the\nlate 1990s. First, while hardware and low-level systems software change\nreasonably fast, software at higher levels of abstraction (e.g., middleware and\napplications), are often much more stable. In other words, we are facing\nthe situation that legacy software cannot be maintained in the same pace as\nthe platforms it relies on. Virtualization can help here by porting the legacy\ninterfaces to the new platforms and thus immediately opening up the latter\nfor large classes of existing programs.\nNote 3.4 (Discussion: Stable software?)\nAlthough there is indeed a lot of legacy software that can bene\ufb01t from stable\ninterfaces to rapidly changing underlying hardware, it is a mistake to believe that\nthe software for widely available services hardly changes. With the increasing\nshift toward server-side computing in the form of software-as-a-service ( SaaS ),\nmuch software can be maintained for a relatively homogeneous platform, owned\nentirely by the organization offering the associated service. As a consequence,\nmaintaining software products can be much easier, as there is much lesser need\nto distribute changes to potentially millions of customers. In fact, changes may\nrapidly succeed each other following changes in available hardware and platform,\nbut without any client actually noticing downtimes [Barroso and H\u00f6lze, 2009].\nEqually important is the fact that networking has become completely\npervasive. It is hard to imagine that a modern computer is not connected to\na network. In practice, this connectivity requires that system administrators\nmaintain a large and heterogeneous collection of server computers, each\none running very different applications, which can be accessed by clients.\nAt the same time the various resources should be easily accessible to these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "118 CHAPTER 3. PROCESSES\napplications. Virtualization can help a lot: the diversity of platforms and\nmachines can be reduced by essentially letting each application run on its\nown virtual machine, possibly including the related libraries andoperating\nsystem, which, in turn, run on a common platform.\nThis last type of virtualization provides a high degree of portability and\n\ufb02exibility. For example, in order to realize content delivery networks that\ncan easily support replication of dynamic content, Awadallah and Rosenblum\n[2002] have argued that management becomes much easier if edge servers\nwould support virtualization, allowing a complete site, including its environ-\nment to be dynamically copied. These arguments are still valid, and indeed,\nportability is perhaps the most important reason why virtualization plays\nsuch a key role in many distributed systems.\nTypes of virtualization\nThere are many different ways in which virtualization can be realized. An\noverview of these various approaches is described by Smith and Nair [2005a].\nTo understand the differences in virtualization, it is important to realize that\ncomputer systems generally offer four different types of interfaces, at three\ndifferent levels:\n1.An interface between the hardware and software, referred to as the in-\nstruction set architecture (ISA), forming the set of machine instructions.\nThis set is divided into two subsets:\n\u2022Privileged instructions, which are allowed to be executed only by\nthe operating system.\n\u2022 General instructions, which can be executed by any program.\n2.An interface consisting of system calls as offered by an operating system.\n3.An interface consisting of library calls, generally forming what is known\nas an application programming interface (API). In many cases, the\naforementioned system calls are hidden by an API.\nThese different types are shown in Figure 3.7. The essence of virtualization is\nto mimic the behavior of these interfaces.\nVirtualization can take place in two different ways. First, we can build a\nruntime system that essentially provides an abstract instruction set that is to\nbe used for executing applications. Instructions can be interpreted (as is the\ncase for the Java runtime environment), but could also be emulated as is done\nfor running Windows applications on Unix platforms. Note that in the latter\ncase, the emulator will also have to mimic the behavior of system calls, which\nhas proven to be generally far from trivial. This type of virtualization, shown\nin Figure 3.8(a), leads to what Smith and Nair [2005a] call a process virtual\nmachine , stressing that virtualization is only for a single process.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n118 CHAPTER 3. PROCESSES\napplications. Virtualization can help a lot: the diversity of platforms and\nmachines can be reduced by essentially letting each application run on its\nown virtual machine, possibly including the related libraries andoperating\nsystem, which, in turn, run on a common platform.\nThis last type of virtualization provides a high degree of portability and\n\ufb02exibility. For example, in order to realize content delivery networks that\ncan easily support replication of dynamic content, Awadallah and Rosenblum\n[2002] have argued that management becomes much easier if edge servers\nwould support virtualization, allowing a complete site, including its environ-\nment to be dynamically copied. These arguments are still valid, and indeed,\nportability is perhaps the most important reason why virtualization plays\nsuch a key role in many distributed systems.\nTypes of virtualization\nThere are many different ways in which virtualization can be realized. An\noverview of these various approaches is described by Smith and Nair [2005a].\nTo understand the differences in virtualization, it is important to realize that\ncomputer systems generally offer four different types of interfaces, at three\ndifferent levels:\n1.An interface between the hardware and software, referred to as the in-\nstruction set architecture (ISA), forming the set of machine instructions.\nThis set is divided into two subsets:\n\u2022Privileged instructions, which are allowed to be executed only by\nthe operating system.\n\u2022 General instructions, which can be executed by any program.\n2.An interface consisting of system calls as offered by an operating system.\n3.An interface consisting of library calls, generally forming what is known\nas an application programming interface (API). In many cases, the\naforementioned system calls are hidden by an API.\nThese different types are shown in Figure 3.7. The essence of virtualization is\nto mimic the behavior of these interfaces.\nVirtualization can take place in two different ways. First, we can build a\nruntime system that essentially provides an abstract instruction set that is to\nbe used for executing applications. Instructions can be interpreted (as is the\ncase for the Java runtime environment), but could also be emulated as is done\nfor running Windows applications on Unix platforms. Note that in the latter\ncase, the emulator will also have to mimic the behavior of system calls, which\nhas proven to be generally far from trivial. This type of virtualization, shown\nin Figure 3.8(a), leads to what Smith and Nair [2005a] call a process virtual\nmachine , stressing that virtualization is only for a single process.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.2. VIRTUALIZATION 119\nFigure 3.7: Various interfaces offered by computer systems.\n(a) (b) (c)\nFigure 3.8: (a) A process virtual machine. (b) A native virtual machine monitor.\n(c) A hosted virtual machine monitor.\nAn alternative approach toward virtualization, shown in Figure 3.8(b),\nis to provide a system that is implemented as a layer shielding the original\nhardware, but offering the complete instruction set of that same (or other\nhardware) as an interface. This leads to what is known as a native virtual\nmachine monitor . It is called native because it is implemented directly on\ntop of the underlying hardware. Note that the interface offered by a virtual\nmachine monitor can be offered simultaneously to different programs. As\na result, it is now possible to have multiple, and different guest operating\nsystems run independently and concurrently on the same platform.\nA native virtual machine monitor will have to provide and regulate access\nto various resources, like external storage and networks. Like any operating\nsystem, this implies that it will have to implement device drivers for those\nresources. Rather than doing all this effort anew, a hosted virtual machine\nmonitor will run on top of a trusted host operating system as shown in\nFigure 3.8(c). In this case, the virtual machine monitor can make use of existing\nfacilities provided by that host operating system. It will generally have to be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.2. VIRTUALIZATION 119\nFigure 3.7: Various interfaces offered by computer systems.\n(a) (b) (c)\nFigure 3.8: (a) A process virtual machine. (b) A native virtual machine monitor.\n(c) A hosted virtual machine monitor.\nAn alternative approach toward virtualization, shown in Figure 3.8(b),\nis to provide a system that is implemented as a layer shielding the original\nhardware, but offering the complete instruction set of that same (or other\nhardware) as an interface. This leads to what is known as a native virtual\nmachine monitor . It is called native because it is implemented directly on\ntop of the underlying hardware. Note that the interface offered by a virtual\nmachine monitor can be offered simultaneously to different programs. As\na result, it is now possible to have multiple, and different guest operating\nsystems run independently and concurrently on the same platform.\nA native virtual machine monitor will have to provide and regulate access\nto various resources, like external storage and networks. Like any operating\nsystem, this implies that it will have to implement device drivers for those\nresources. Rather than doing all this effort anew, a hosted virtual machine\nmonitor will run on top of a trusted host operating system as shown in\nFigure 3.8(c). In this case, the virtual machine monitor can make use of existing\nfacilities provided by that host operating system. It will generally have to be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "120 CHAPTER 3. PROCESSES\ngiven special privileges instead of running as a user-level application. Using\na hosted virtual machine monitor is highly popular in modern distributed\nsystems such as data centers and clouds.\nAs argued by Rosenblum and Gar\ufb01nkel [2005], virtual machines are be-\ncoming increasingly important in the context of reliability and security for\n(distributed) systems. As they allow for the isolation of a complete application\nand its environment, a failure caused by an error or security attack need no\nlonger affect a complete machine. In addition, as we also mentioned before,\nportability is greatly improved as virtual machines provide a further decou-\npling between hardware and software, allowing a complete environment to be\nmoved from one machine to another. We return to migration in Section 3.5.\nNote 3.5 (Advanced: On the performance of virtual machines)\nVirtual machines perform surprisingly well. In fact, many studies show that\nmodern virtual machines perform close to running applications directly on the\nhost operating system. Let us take a closer look at what is going under the hood\nof virtual machines. A detailed and comprehensive account of virtual machines is\nprovided by Smith and Nair [2005b].\nFigure 3.9: Applications, guest operating system, virtual machine monitor,\nand host operating system on a single hardware platform.\nPart of the answer to performance issue is shown in Figure 3.9, which forms\nan extension of Figure 3.8(c): a large part of the code constituting a virtual\nmachine monitor, guest operating system, and application is running natively on\nthe underlying hardware. In particular, all general (i.e., unprivileged) machine\ninstructions are directly executed by the underlying machine instead of being\ninterpreted or emulated.\nThis approach is not new and is founded on research by Popek and Goldberg\n[1974] who formalized the requirements for the ef\ufb01cient execution of virtual\nmachines. In a nutshell, Popek and Goldberg assumed that the underlying\nmachine provided at least two modes of operation (system and user mode), that\na subset of the instructions could be executed only in system mode, and that\nmemory addressing was relative (i.e., a physical address was obtained by adding\na relative address to an offset found in a relocation register). A distinction was\nfurther made between two types of instructions. A privileged instruction is an\ninstruction that is characterized by the fact that if and only if executed in user\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n120 CHAPTER 3. PROCESSES\ngiven special privileges instead of running as a user-level application. Using\na hosted virtual machine monitor is highly popular in modern distributed\nsystems such as data centers and clouds.\nAs argued by Rosenblum and Gar\ufb01nkel [2005], virtual machines are be-\ncoming increasingly important in the context of reliability and security for\n(distributed) systems. As they allow for the isolation of a complete application\nand its environment, a failure caused by an error or security attack need no\nlonger affect a complete machine. In addition, as we also mentioned before,\nportability is greatly improved as virtual machines provide a further decou-\npling between hardware and software, allowing a complete environment to be\nmoved from one machine to another. We return to migration in Section 3.5.\nNote 3.5 (Advanced: On the performance of virtual machines)\nVirtual machines perform surprisingly well. In fact, many studies show that\nmodern virtual machines perform close to running applications directly on the\nhost operating system. Let us take a closer look at what is going under the hood\nof virtual machines. A detailed and comprehensive account of virtual machines is\nprovided by Smith and Nair [2005b].\nFigure 3.9: Applications, guest operating system, virtual machine monitor,\nand host operating system on a single hardware platform.\nPart of the answer to performance issue is shown in Figure 3.9, which forms\nan extension of Figure 3.8(c): a large part of the code constituting a virtual\nmachine monitor, guest operating system, and application is running natively on\nthe underlying hardware. In particular, all general (i.e., unprivileged) machine\ninstructions are directly executed by the underlying machine instead of being\ninterpreted or emulated.\nThis approach is not new and is founded on research by Popek and Goldberg\n[1974] who formalized the requirements for the ef\ufb01cient execution of virtual\nmachines. In a nutshell, Popek and Goldberg assumed that the underlying\nmachine provided at least two modes of operation (system and user mode), that\na subset of the instructions could be executed only in system mode, and that\nmemory addressing was relative (i.e., a physical address was obtained by adding\na relative address to an offset found in a relocation register). A distinction was\nfurther made between two types of instructions. A privileged instruction is an\ninstruction that is characterized by the fact that if and only if executed in user\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.2. VIRTUALIZATION 121\nmode, it causes a trap to the operating system. Nonprivileged instructions are\nall other instructions.\nGiven these formal assumptions, Popek and Goldberg de\ufb01ned two classes\nof special instructions. A control-sensitive instruction is one that may affect\nthe con\ufb01guration of a machine. A typical example is an instruction that affects\nthe memory layout, for example, by changing the memory offset as stored in a\nrelocation register. Another example are instructions that affect the interrupt table\ncontaining pointers to interrupt handlers.\nAbehavior-sensitive instruction is one whose effect is partially determined\nby the context in which it is executed. For example, Intel x86processors have\ninstructions that may, or may not affect certain registers depending on whether\nthat instruction is executed in system mode or user mode. An example given\nin [Smith and Nair, 2005b] is that of the POPF instruction, which may set an\ninterrupt-enabled \ufb02ag, but only when executed in system mode.\nWe now have the following important result:\nFor any conventional computer, a virtual machine monitor may be con-\nstructed if the set of sensitive instructions for that computer is a subset of\nthe set of privileged instructions.\nWhat this says is that as long as sensitive instructions are caught when executed\nin user mode, we can safely run all nonsensitive instructions natively on the\nunderlying hardware. This also means that when designing instruction sets, if\nwe take care that the above requirement is met, we will not be unnecessarily\nobstructing ef\ufb01cient virtualization of that instruction set.\nUnfortunately, not all instruction sets have privileged-only sensitive instruc-\ntions, including perhaps the most popular one, namely the Intel x86instruction set.\nAs it turns out, this set has 17 sensitive instructions that are not privileged [Robin\nand Irvine, 2000]. In other words, each of these instructions can be executed in\nuser mode without causing a trap to the operating system, yet affect the way\nthat the operating system is managing its resources. In these cases, there are\nessentially two solutions.\nThe \ufb01rst solution is to emulate all instructions. Of course, this would have\na serious adverse effect on performance. To circumvent problems, an approach\nimplemented in VMWare [Sugerman et al., 2001], is to scan the executable and\nto insert code around the nonprivileged sensitive instructions to divert control\nto the virtual machine monitor. There, appropriate emulation will take place, for\nexample, by considering the context in which the instruction was to be executed.\nThe effect is that full virtualization can take place, meaning that execution can\ntake place without changing the guest operating system, nor the application itself.\nAn alternative solution is to apply paravirtualization , which requires the\nguest operating system to be modi\ufb01ed. In particular, the guest operating system is\nmodi\ufb01ed such that all side effects of running nonprivileged sensitive instructions\nin user mode, which would normally be executed in system mode, are dealt\nwith. For example, code can be rewritten such that these instructions simply no\nlonger occur, or if they do, that their semantics are the same regardless whether\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.2. VIRTUALIZATION 121\nmode, it causes a trap to the operating system. Nonprivileged instructions are\nall other instructions.\nGiven these formal assumptions, Popek and Goldberg de\ufb01ned two classes\nof special instructions. A control-sensitive instruction is one that may affect\nthe con\ufb01guration of a machine. A typical example is an instruction that affects\nthe memory layout, for example, by changing the memory offset as stored in a\nrelocation register. Another example are instructions that affect the interrupt table\ncontaining pointers to interrupt handlers.\nAbehavior-sensitive instruction is one whose effect is partially determined\nby the context in which it is executed. For example, Intel x86processors have\ninstructions that may, or may not affect certain registers depending on whether\nthat instruction is executed in system mode or user mode. An example given\nin [Smith and Nair, 2005b] is that of the POPF instruction, which may set an\ninterrupt-enabled \ufb02ag, but only when executed in system mode.\nWe now have the following important result:\nFor any conventional computer, a virtual machine monitor may be con-\nstructed if the set of sensitive instructions for that computer is a subset of\nthe set of privileged instructions.\nWhat this says is that as long as sensitive instructions are caught when executed\nin user mode, we can safely run all nonsensitive instructions natively on the\nunderlying hardware. This also means that when designing instruction sets, if\nwe take care that the above requirement is met, we will not be unnecessarily\nobstructing ef\ufb01cient virtualization of that instruction set.\nUnfortunately, not all instruction sets have privileged-only sensitive instruc-\ntions, including perhaps the most popular one, namely the Intel x86instruction set.\nAs it turns out, this set has 17 sensitive instructions that are not privileged [Robin\nand Irvine, 2000]. In other words, each of these instructions can be executed in\nuser mode without causing a trap to the operating system, yet affect the way\nthat the operating system is managing its resources. In these cases, there are\nessentially two solutions.\nThe \ufb01rst solution is to emulate all instructions. Of course, this would have\na serious adverse effect on performance. To circumvent problems, an approach\nimplemented in VMWare [Sugerman et al., 2001], is to scan the executable and\nto insert code around the nonprivileged sensitive instructions to divert control\nto the virtual machine monitor. There, appropriate emulation will take place, for\nexample, by considering the context in which the instruction was to be executed.\nThe effect is that full virtualization can take place, meaning that execution can\ntake place without changing the guest operating system, nor the application itself.\nAn alternative solution is to apply paravirtualization , which requires the\nguest operating system to be modi\ufb01ed. In particular, the guest operating system is\nmodi\ufb01ed such that all side effects of running nonprivileged sensitive instructions\nin user mode, which would normally be executed in system mode, are dealt\nwith. For example, code can be rewritten such that these instructions simply no\nlonger occur, or if they do, that their semantics are the same regardless whether\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "122 CHAPTER 3. PROCESSES\nbeing executed in user or system mode. Paravirtualization has been adopted by\nXen [Barham et al., 2003; Chisnall, 2007].\nApplication of virtual machines to distributed systems\nFrom the perspective of distributed systems, the most important application of\nvirtualization lies in cloud computing. As we already mentioned in Section 1.3,\ncloud providers offer roughly three different types of services:\n\u2022Infrastructure-as-a-Service (IaaS ) covering the basic infrastructure\n\u2022Platform-as-a-Service (PaaS ) covering system-level services\n\u2022Software-as-a-Service (SaaS ) containing actual applications\nVirtualization plays a key role in IaaS. Instead of renting out a physical\nmachine, a cloud provider will rent out a virtual machine (monitor) that\nmay, or may not, be sharing a physical machine with other customers. The\nbeauty of virtualization is that it allows for almost complete isolation between\ncustomers, who will indeed have the illusion that they have just rented a\ndedicated physical machine. Isolation is, however, never complete, if only\nfor the fact that the actual physical resources are shared, in turn leading to\nobservable lower performance.\nTo make matters concrete, let us consider the Amazon Elastic Compute\nCloud , or simply EC2. EC2 allows one to create an environment consisting\nof several networked virtual servers, thus jointly forming the basis of a\ndistributed system. To make life easy, there is a (large) number of pre-\ncon\ufb01gured machine images available, referred to as Amazon Machine Image s,\nor simply AMI s. An AMI is an installable software package consisting of an\noperating-system kernel along with a number of services. An example of a\nsimple, basic AMI is a LAMP image, consisting of a Linux kernel, the Apache\nWeb server, a MySQL database system, and PHP libraries. More elaborate\nimages containing additional software are also available, as well as images\nbased on other Unix kernels or Windows. In this sense, an AMI is essentially\nthe same as a boot disk (although there are few important differences to which\nwe return shortly).\nAn EC2 customer needs to select an AMI, possibly after adapting or\ncon\ufb01guring one. An AMI can then be launched resulting in what is called an\nEC2 instance : the actual virtual machine that can be used to host a customer\u2019s\napplications. An important issue is that a customer will hardly ever know\nexactly where an instance is actually being executed. Obviously, it is running\non a single physical machine, but where that machine is located remains\nhidden. The closest one can get to pinpointing the location where an instance\nshould run is by selecting one of a few regions provided by Amazon (US,\nSouth America, Europe, Asia).\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n122 CHAPTER 3. PROCESSES\nbeing executed in user or system mode. Paravirtualization has been adopted by\nXen [Barham et al., 2003; Chisnall, 2007].\nApplication of virtual machines to distributed systems\nFrom the perspective of distributed systems, the most important application of\nvirtualization lies in cloud computing. As we already mentioned in Section 1.3,\ncloud providers offer roughly three different types of services:\n\u2022Infrastructure-as-a-Service (IaaS ) covering the basic infrastructure\n\u2022Platform-as-a-Service (PaaS ) covering system-level services\n\u2022Software-as-a-Service (SaaS ) containing actual applications\nVirtualization plays a key role in IaaS. Instead of renting out a physical\nmachine, a cloud provider will rent out a virtual machine (monitor) that\nmay, or may not, be sharing a physical machine with other customers. The\nbeauty of virtualization is that it allows for almost complete isolation between\ncustomers, who will indeed have the illusion that they have just rented a\ndedicated physical machine. Isolation is, however, never complete, if only\nfor the fact that the actual physical resources are shared, in turn leading to\nobservable lower performance.\nTo make matters concrete, let us consider the Amazon Elastic Compute\nCloud , or simply EC2. EC2 allows one to create an environment consisting\nof several networked virtual servers, thus jointly forming the basis of a\ndistributed system. To make life easy, there is a (large) number of pre-\ncon\ufb01gured machine images available, referred to as Amazon Machine Image s,\nor simply AMI s. An AMI is an installable software package consisting of an\noperating-system kernel along with a number of services. An example of a\nsimple, basic AMI is a LAMP image, consisting of a Linux kernel, the Apache\nWeb server, a MySQL database system, and PHP libraries. More elaborate\nimages containing additional software are also available, as well as images\nbased on other Unix kernels or Windows. In this sense, an AMI is essentially\nthe same as a boot disk (although there are few important differences to which\nwe return shortly).\nAn EC2 customer needs to select an AMI, possibly after adapting or\ncon\ufb01guring one. An AMI can then be launched resulting in what is called an\nEC2 instance : the actual virtual machine that can be used to host a customer\u2019s\napplications. An important issue is that a customer will hardly ever know\nexactly where an instance is actually being executed. Obviously, it is running\non a single physical machine, but where that machine is located remains\nhidden. The closest one can get to pinpointing the location where an instance\nshould run is by selecting one of a few regions provided by Amazon (US,\nSouth America, Europe, Asia).\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.2. VIRTUALIZATION 123\nTo communicate, each instance obtains two IP addresses: a private one that\ncan be used for internal communication between different instances, making\nuse of EC2\u2019s internal networking facilities, and a public IP address allowing\nany Internet clients to contact an instance. The public address is mapped to the\nprivate one using standard Network Address Translation (NAT ) technology.\nA simple way to manage an instance is to make use of an SSH connection, for\nwhich Amazon provides the means for generating the appropriate keys.\nThe EC2 environment in which an instance is executed provides different\nlevels of the following services:\n\u2022CPU : allows to select the number and type of cores, including GPUs\n\u2022Memory : de\ufb01nes how much main memory is allocated to an instance\n\u2022Storage : de\ufb01nes how much local storage is allocated\n\u2022Platform : distinguishes between 32-bit or 64-bit architectures\n\u2022Networking : sets the bandwidth capacity that can be used\nIn addition, extra resources can be requested such as an additional networking\ninterface. The local storage that comes with an instance is transient : when the\ninstance stops, all the data stored locally is lost. In order to prevent data loss,\na customer will need to explicitly save data to persistent store, for example,\nby making use of Amazon\u2019s Simple Storage Service (S3). An alternative is\nto attach a storage device that is mapped to Amazon\u2019s Elastic Block Store\n(Amazon EBS ). Again, this is yet another service, but one that can be used\nin the form of a virtual block device that is simply mounted as one would\nmount an additional hard disk. When an instance is stopped, all data that\nwas stored on EBS will persist. And just as one would expect, an EBS device\ncan be (re)mounted to any other instance as well.\nIt should be clear by now that, without having gone into any signi\ufb01cant\nlevel of detail, the IaaS as offered by EC2 allows a customer to create a (po-\ntentially large) number of virtual machines, each con\ufb01gured with resources\nas needed, and capable of exchanging messages through an IP network. In\naddition, these virtual machines can be accessed from anywhere over the\nInternet (provided a client has the proper credentials). As such, Amazon\nEC2, like many other IaaS providers, offers the means to con\ufb01gure a com-\nplete distributed system consisting of networked virtual servers and running\ncustomer-supplied distributed applications. At the same time, those cus-\ntomers will not need to maintain any physical machine, which by itself is\noften already a huge gain as we will encounter at several occasions throughout\nthis text. One can indeed argue that virtualization lies at the core of modern\ncloud computing.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.2. VIRTUALIZATION 123\nTo communicate, each instance obtains two IP addresses: a private one that\ncan be used for internal communication between different instances, making\nuse of EC2\u2019s internal networking facilities, and a public IP address allowing\nany Internet clients to contact an instance. The public address is mapped to the\nprivate one using standard Network Address Translation (NAT ) technology.\nA simple way to manage an instance is to make use of an SSH connection, for\nwhich Amazon provides the means for generating the appropriate keys.\nThe EC2 environment in which an instance is executed provides different\nlevels of the following services:\n\u2022CPU : allows to select the number and type of cores, including GPUs\n\u2022Memory : de\ufb01nes how much main memory is allocated to an instance\n\u2022Storage : de\ufb01nes how much local storage is allocated\n\u2022Platform : distinguishes between 32-bit or 64-bit architectures\n\u2022Networking : sets the bandwidth capacity that can be used\nIn addition, extra resources can be requested such as an additional networking\ninterface. The local storage that comes with an instance is transient : when the\ninstance stops, all the data stored locally is lost. In order to prevent data loss,\na customer will need to explicitly save data to persistent store, for example,\nby making use of Amazon\u2019s Simple Storage Service (S3). An alternative is\nto attach a storage device that is mapped to Amazon\u2019s Elastic Block Store\n(Amazon EBS ). Again, this is yet another service, but one that can be used\nin the form of a virtual block device that is simply mounted as one would\nmount an additional hard disk. When an instance is stopped, all data that\nwas stored on EBS will persist. And just as one would expect, an EBS device\ncan be (re)mounted to any other instance as well.\nIt should be clear by now that, without having gone into any signi\ufb01cant\nlevel of detail, the IaaS as offered by EC2 allows a customer to create a (po-\ntentially large) number of virtual machines, each con\ufb01gured with resources\nas needed, and capable of exchanging messages through an IP network. In\naddition, these virtual machines can be accessed from anywhere over the\nInternet (provided a client has the proper credentials). As such, Amazon\nEC2, like many other IaaS providers, offers the means to con\ufb01gure a com-\nplete distributed system consisting of networked virtual servers and running\ncustomer-supplied distributed applications. At the same time, those cus-\ntomers will not need to maintain any physical machine, which by itself is\noften already a huge gain as we will encounter at several occasions throughout\nthis text. One can indeed argue that virtualization lies at the core of modern\ncloud computing.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "124 CHAPTER 3. PROCESSES\n3.3 Clients\nIn the previous chapters we discussed the client-server model, the roles of\nclients and servers, and the ways they interact. Let us now take a closer look\nat the anatomy of clients and servers, respectively. We start in this section\nwith a discussion of clients. Servers are discussed in the next section.\nNetworked user interfaces\nA major task of client machines is to provide the means for users to interact\nwith remote servers. There are roughly two ways in which this interaction\ncan be supported. First, for each remote service the client machine will have a\nseparate counterpart that can contact the service over the network. A typical\nexample is a calendar running on a user\u2019s smartphone that needs to synchro-\nnize with a remote, possibly shared calendar. In this case, an application-level\nprotocol will handle the synchronization, as shown in Figure 3.10(a).\n(a)\n(b)\nFigure 3.10: (a) A networked application with its own protocol. (b) A general\nsolution to allow access to remote applications.\nA second solution is to provide direct access to remote services by offer-\ning only a convenient user interface. Effectively, this means that the client\nmachine is used only as a terminal with no need for local storage, leading\nto an application-neutral solution as shown in Figure 3.10(b). In the case of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n124 CHAPTER 3. PROCESSES\n3.3 Clients\nIn the previous chapters we discussed the client-server model, the roles of\nclients and servers, and the ways they interact. Let us now take a closer look\nat the anatomy of clients and servers, respectively. We start in this section\nwith a discussion of clients. Servers are discussed in the next section.\nNetworked user interfaces\nA major task of client machines is to provide the means for users to interact\nwith remote servers. There are roughly two ways in which this interaction\ncan be supported. First, for each remote service the client machine will have a\nseparate counterpart that can contact the service over the network. A typical\nexample is a calendar running on a user\u2019s smartphone that needs to synchro-\nnize with a remote, possibly shared calendar. In this case, an application-level\nprotocol will handle the synchronization, as shown in Figure 3.10(a).\n(a)\n(b)\nFigure 3.10: (a) A networked application with its own protocol. (b) A general\nsolution to allow access to remote applications.\nA second solution is to provide direct access to remote services by offer-\ning only a convenient user interface. Effectively, this means that the client\nmachine is used only as a terminal with no need for local storage, leading\nto an application-neutral solution as shown in Figure 3.10(b). In the case of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.3. CLIENTS 125\nnetworked user interfaces, everything is processed and stored at the server.\nThis thin-client approach has received much attention with the increase of\nInternet connectivity and the use of mobile devices. Thin-client solutions are\nalso popular as they ease the task of system management.\nExample: The X window system\nPerhaps one of the oldest and still widely used networked user interfaces is the\nX Window System . The X Window System, generally referred to simply as X,\nis used to control bit-mapped terminals, which include a monitor, keyboard,\nand a pointing device such as a mouse. Next to supporting traditional\nterminals as can be found with desktop computers and workstations, X also\nsupports modern devices such a touchscreens on tablets and smartphones. In\na sense, X can be viewed as that part of an operating system that controls the\nterminal. The heart of the system is formed by what we shall call the X kernel .\nIt contains all the terminal-speci\ufb01c device drivers, and as such, is generally\nhighly hardware dependent.\nThe X kernel offers a relatively low-level interface for controlling the\nscreen, but also for capturing events from the keyboard and mouse. This\ninterface is made available to applications as a library called Xlib. This general\norganization is shown in Figure 3.11. Note that Xlib is hardly ever used directly\nby applications, which instead deploy easier to use toolkits implemented on\ntop of Xlib.\nFigure 3.11: The basic organization of the X Window System.\nThe interesting aspect of X is that the X kernel and the X applications need\nnot necessarily reside on the same machine. In particular, X provides the X\nprotocol , which is an application-level communication protocol by which an\ninstance of Xlib can exchange data and events with an X kernel. For example,\nXlib can send requests to the X kernel for creating or killing a window, setting\ncolors, and de\ufb01ning the type of cursor to display, among many other requests.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.3. CLIENTS 125\nnetworked user interfaces, everything is processed and stored at the server.\nThis thin-client approach has received much attention with the increase of\nInternet connectivity and the use of mobile devices. Thin-client solutions are\nalso popular as they ease the task of system management.\nExample: The X window system\nPerhaps one of the oldest and still widely used networked user interfaces is the\nX Window System . The X Window System, generally referred to simply as X,\nis used to control bit-mapped terminals, which include a monitor, keyboard,\nand a pointing device such as a mouse. Next to supporting traditional\nterminals as can be found with desktop computers and workstations, X also\nsupports modern devices such a touchscreens on tablets and smartphones. In\na sense, X can be viewed as that part of an operating system that controls the\nterminal. The heart of the system is formed by what we shall call the X kernel .\nIt contains all the terminal-speci\ufb01c device drivers, and as such, is generally\nhighly hardware dependent.\nThe X kernel offers a relatively low-level interface for controlling the\nscreen, but also for capturing events from the keyboard and mouse. This\ninterface is made available to applications as a library called Xlib. This general\norganization is shown in Figure 3.11. Note that Xlib is hardly ever used directly\nby applications, which instead deploy easier to use toolkits implemented on\ntop of Xlib.\nFigure 3.11: The basic organization of the X Window System.\nThe interesting aspect of X is that the X kernel and the X applications need\nnot necessarily reside on the same machine. In particular, X provides the X\nprotocol , which is an application-level communication protocol by which an\ninstance of Xlib can exchange data and events with an X kernel. For example,\nXlib can send requests to the X kernel for creating or killing a window, setting\ncolors, and de\ufb01ning the type of cursor to display, among many other requests.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "126 CHAPTER 3. PROCESSES\nIn turn, the X kernel will react to local events such as keyboard and mouse\ninput by sending event packets back to Xlib.\nSeveral applications can communicate at the same time with the X kernel.\nThere is one speci\ufb01c application that is given special rights, known as the\nwindow manager . This application can dictate the \u201clook and feel\u201d of the\ndisplay as it appears to the user. For example, the window manager can\nprescribe how each window is decorated with extra buttons, how windows\nare to be placed on the display, and so on. Other applications will have to\nadhere to these rules. In practice, this means that much of the interaction\nbetween an application and an X terminal is redirected through a window\nmanager.\nIt is interesting to note how the X window system actually \ufb01ts into client-\nserver computing. From what we have described so far, it should be clear\nthat the X kernel receives requests to manipulate the display. It gets these\nrequests from (possibly remote) applications. In this sense, the X kernel acts\nas a server, while the applications play the role of clients. This terminology\nhas been adopted by X, and although strictly speaking it is correct, it can\neasily lead to confusion.\nThin-client network computing\nObviously, applications manipulate a display using the speci\ufb01c display com-\nmands as offered by X. These commands are generally sent over the network\nwhere they are subsequently executed by the X kernel. By its nature, ap-\nplications written for X should preferably separate application logic from\nuser-interface commands. Unfortunately, this is often not the case. As re-\nported by Lai and Nieh [2002] it turns out that much of the application logic\nand user interaction are tightly coupled, meaning that an application will send\nmany requests to the X kernel for which it will expect a response before being\nable to make a next step. This synchronous behavior may adversely affect\nperformance when operating over a wide-area network with long latencies.\nThere are several solutions to this problem. One is to re-engineer the\nimplementation of the X protocol, as is done with NX [Pinzari, 2003]. An\nimportant part of this work concentrates on bandwidth reduction by reducing\nthe size of X messages. To this end, messages are considered to consist of a\n\ufb01xed part, which is treated as an identi\ufb01er, and a variable part. In many cases,\nmultiple messages will have the same identi\ufb01er in which case they will often\ncontain similar data. This property can be used to send only the differences\nbetween messages having the same identi\ufb01er. By having the sender and\nreceiver maintain identi\ufb01ers, decoding at the receiver can be readily applied.\nBandwidth reductions up to a factor 1000 have been reported, which allows X\nto also run through low-bandwidth links of only 9600 kbps.\nAs an alternative to using X, researchers and practitioners have also sought\nto let an application completely control the remote display, that is, up the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n126 CHAPTER 3. PROCESSES\nIn turn, the X kernel will react to local events such as keyboard and mouse\ninput by sending event packets back to Xlib.\nSeveral applications can communicate at the same time with the X kernel.\nThere is one speci\ufb01c application that is given special rights, known as the\nwindow manager . This application can dictate the \u201clook and feel\u201d of the\ndisplay as it appears to the user. For example, the window manager can\nprescribe how each window is decorated with extra buttons, how windows\nare to be placed on the display, and so on. Other applications will have to\nadhere to these rules. In practice, this means that much of the interaction\nbetween an application and an X terminal is redirected through a window\nmanager.\nIt is interesting to note how the X window system actually \ufb01ts into client-\nserver computing. From what we have described so far, it should be clear\nthat the X kernel receives requests to manipulate the display. It gets these\nrequests from (possibly remote) applications. In this sense, the X kernel acts\nas a server, while the applications play the role of clients. This terminology\nhas been adopted by X, and although strictly speaking it is correct, it can\neasily lead to confusion.\nThin-client network computing\nObviously, applications manipulate a display using the speci\ufb01c display com-\nmands as offered by X. These commands are generally sent over the network\nwhere they are subsequently executed by the X kernel. By its nature, ap-\nplications written for X should preferably separate application logic from\nuser-interface commands. Unfortunately, this is often not the case. As re-\nported by Lai and Nieh [2002] it turns out that much of the application logic\nand user interaction are tightly coupled, meaning that an application will send\nmany requests to the X kernel for which it will expect a response before being\nable to make a next step. This synchronous behavior may adversely affect\nperformance when operating over a wide-area network with long latencies.\nThere are several solutions to this problem. One is to re-engineer the\nimplementation of the X protocol, as is done with NX [Pinzari, 2003]. An\nimportant part of this work concentrates on bandwidth reduction by reducing\nthe size of X messages. To this end, messages are considered to consist of a\n\ufb01xed part, which is treated as an identi\ufb01er, and a variable part. In many cases,\nmultiple messages will have the same identi\ufb01er in which case they will often\ncontain similar data. This property can be used to send only the differences\nbetween messages having the same identi\ufb01er. By having the sender and\nreceiver maintain identi\ufb01ers, decoding at the receiver can be readily applied.\nBandwidth reductions up to a factor 1000 have been reported, which allows X\nto also run through low-bandwidth links of only 9600 kbps.\nAs an alternative to using X, researchers and practitioners have also sought\nto let an application completely control the remote display, that is, up the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.3. CLIENTS 127\npixel level. Changes in the bitmap are then sent over the network to the\ndisplay, where they are immediately transferred to the local frame buffer.\nA well-known example of this approach is Virtual Network Computing\n(VNC ) [Richardson et al., 1998], which has been around ever since the late\n1990s. Obviously, letting the application control the display requires sophis-\nticated encoding techniques in order to prevent bandwidth availability to\nbecome a problem. For example, consider displaying a video stream at a rate\nof 30 frames per second on a simple 320 \u0002240 screen. If each pixel is encoded\nby 24 bits, then without an ef\ufb01cient encoding scheme, we would need a band-\nwidth of approximately 53 Mbps. In practice, various encoding techniques\nare used, yet choosing the best one is generally application dependent.\nThe drawback of sending raw pixel data in comparison to higher-level\nprotocols such as X is that it is impossible to make any use of application\nsemantics, as these are effectively lost at that level. Baratto et al. [2005] propose\na different technique. In their solution, referred to as THINC, they provide a\nfew high-level display commands that operate at the level of the video device\ndrivers. These commands are thus device dependent, more powerful than\nraw pixel operations, but less powerful compared to what a protocol such as\nX offers. The result is that display servers can be much simpler, which is good\nfor CPU usage, while at the same time application-dependent optimizations\ncan be used to reduce bandwidth and synchronization.\nClient-side software for distribution transparency\nClient software comprises more than just user interfaces. In many cases, parts\nof the processing and data level in a client-server application are executed on\nthe client side as well. A special class is formed by embedded client software,\nsuch as for automatic teller machines (ATMs), cash registers, barcode readers,\nTV set-top boxes, etc. In these cases, the user interface is a relatively small part\nof the client software, in contrast to the local processing and communication\nfacilities.\nBesides the user interface and other application-related software, client soft-\nware comprises components for achieving distribution transparency. Ideally,\na client should not be aware that it is communicating with remote processes.\nIn contrast, distribution is often less transparent to servers for reasons of\nperformance and correctness.\nAccess transparency is generally handled through the generation of a\nclient stub from an interface de\ufb01nition of what the server has to offer. The\nstub provides the same interface as the one available at the server, but hides\nthe possible differences in machine architectures, as well as the actual commu-\nnication. The client stub transforms local calls to messages that are sent to the\nserver, and vice versa transforms messages from the server to return values as\none would expect when calling an ordinary procedure.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.3. CLIENTS 127\npixel level. Changes in the bitmap are then sent over the network to the\ndisplay, where they are immediately transferred to the local frame buffer.\nA well-known example of this approach is Virtual Network Computing\n(VNC ) [Richardson et al., 1998], which has been around ever since the late\n1990s. Obviously, letting the application control the display requires sophis-\nticated encoding techniques in order to prevent bandwidth availability to\nbecome a problem. For example, consider displaying a video stream at a rate\nof 30 frames per second on a simple 320 \u0002240 screen. If each pixel is encoded\nby 24 bits, then without an ef\ufb01cient encoding scheme, we would need a band-\nwidth of approximately 53 Mbps. In practice, various encoding techniques\nare used, yet choosing the best one is generally application dependent.\nThe drawback of sending raw pixel data in comparison to higher-level\nprotocols such as X is that it is impossible to make any use of application\nsemantics, as these are effectively lost at that level. Baratto et al. [2005] propose\na different technique. In their solution, referred to as THINC, they provide a\nfew high-level display commands that operate at the level of the video device\ndrivers. These commands are thus device dependent, more powerful than\nraw pixel operations, but less powerful compared to what a protocol such as\nX offers. The result is that display servers can be much simpler, which is good\nfor CPU usage, while at the same time application-dependent optimizations\ncan be used to reduce bandwidth and synchronization.\nClient-side software for distribution transparency\nClient software comprises more than just user interfaces. In many cases, parts\nof the processing and data level in a client-server application are executed on\nthe client side as well. A special class is formed by embedded client software,\nsuch as for automatic teller machines (ATMs), cash registers, barcode readers,\nTV set-top boxes, etc. In these cases, the user interface is a relatively small part\nof the client software, in contrast to the local processing and communication\nfacilities.\nBesides the user interface and other application-related software, client soft-\nware comprises components for achieving distribution transparency. Ideally,\na client should not be aware that it is communicating with remote processes.\nIn contrast, distribution is often less transparent to servers for reasons of\nperformance and correctness.\nAccess transparency is generally handled through the generation of a\nclient stub from an interface de\ufb01nition of what the server has to offer. The\nstub provides the same interface as the one available at the server, but hides\nthe possible differences in machine architectures, as well as the actual commu-\nnication. The client stub transforms local calls to messages that are sent to the\nserver, and vice versa transforms messages from the server to return values as\none would expect when calling an ordinary procedure.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "128 CHAPTER 3. PROCESSES\nThere are different ways to handle location, migration, and relocation\ntransparency. Using a convenient naming system is crucial. In many cases,\ncooperation with client-side software is also important. For example, when a\nclient is already bound to a server, the client can be directly informed when\nthe server changes location. In this case, the client\u2019s middleware can hide\nthe server\u2019s current network location from the user, and also transparently\nrebind to the server if necessary. At worst, the client\u2019s application may notice\na temporary loss of performance.\nIn a similar way, many distributed systems implement replication trans-\nparency by means of client-side solutions. For example, imagine a distributed\nsystem with replicated servers, Such replication can be achieved by forward-\ning a request to each replica, as shown in Figure 3.12. Client-side software\ncan transparently collect all responses and pass a single response to the client\napplication.\nFigure 3.12: Transparent replication of a server using a client-side solution.\nRegarding failure transparency, masking communication failures with\na server is typically done through client middleware. For example, client\nmiddleware can be con\ufb01gured to repeatedly attempt to connect to a server, or\nperhaps try another server after several attempts. There are even situations\nin which the client middleware returns data it had cached during a previous\nsession, as is sometimes done by Web browsers that fail to connect to a server.\nFinally, concurrency transparency can be handled through special interme-\ndiate servers, notably transaction monitors, and requires less support from\nclient software.\n3.4 Servers\nLet us now take a closer look at the organization of servers. In the following\npages, we \ufb01rst concentrate on a number of general design issues for servers,\nfollowed by a discussion on server clusters.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n128 CHAPTER 3. PROCESSES\nThere are different ways to handle location, migration, and relocation\ntransparency. Using a convenient naming system is crucial. In many cases,\ncooperation with client-side software is also important. For example, when a\nclient is already bound to a server, the client can be directly informed when\nthe server changes location. In this case, the client\u2019s middleware can hide\nthe server\u2019s current network location from the user, and also transparently\nrebind to the server if necessary. At worst, the client\u2019s application may notice\na temporary loss of performance.\nIn a similar way, many distributed systems implement replication trans-\nparency by means of client-side solutions. For example, imagine a distributed\nsystem with replicated servers, Such replication can be achieved by forward-\ning a request to each replica, as shown in Figure 3.12. Client-side software\ncan transparently collect all responses and pass a single response to the client\napplication.\nFigure 3.12: Transparent replication of a server using a client-side solution.\nRegarding failure transparency, masking communication failures with\na server is typically done through client middleware. For example, client\nmiddleware can be con\ufb01gured to repeatedly attempt to connect to a server, or\nperhaps try another server after several attempts. There are even situations\nin which the client middleware returns data it had cached during a previous\nsession, as is sometimes done by Web browsers that fail to connect to a server.\nFinally, concurrency transparency can be handled through special interme-\ndiate servers, notably transaction monitors, and requires less support from\nclient software.\n3.4 Servers\nLet us now take a closer look at the organization of servers. In the following\npages, we \ufb01rst concentrate on a number of general design issues for servers,\nfollowed by a discussion on server clusters.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 129\nGeneral design issues\nA server is a process implementing a speci\ufb01c service on behalf of a collection\nof clients. In essence, each server is organized in the same way: it waits for an\nincoming request from a client and subsequently ensures that the request is\ntaken care of, after which it waits for the next incoming request.\nConcurrent versus iterative servers\nThere are several ways to organize servers. In the case of an iterative server ,\nthe server itself handles the request and, if necessary, returns a response to the\nrequesting client. A concurrent server does not handle the request itself, but\npasses it to a separate thread or another process, after which it immediately\nwaits for the next incoming request. A multithreaded server is an example of\na concurrent server. An alternative implementation of a concurrent server is to\nfork a new process for each new incoming request. This approach is followed\nin many Unix systems. The thread or process that handles the request is\nresponsible for returning a response to the requesting client.\nContacting a server: end points\nAnother issue is where clients contact a server. In all cases, clients send\nrequests to an end point , also called a port , at the machine where the server\nis running. Each server listens to a speci\ufb01c end point. How do clients know\nthe end point of a service? One approach is to globally assign end points for\nwell-known services. For example, servers that handle Internet FTP requests\nalways listen to TCP port 21. Likewise, an HTTP server for the World Wide\nWeb will always listen to TCP port 80. These end points have been assigned\nby the Internet Assigned Numbers Authority (IANA), and are documented\nin [Reynolds and Postel, 1994]. With assigned end points, the client needs to\n\ufb01nd only the network address of the machine where the server is running.\nName services can be used for that purpose.\nThere are many services that do not require a preassigned end point.\nFor example, a time-of-day server may use an end point that is dynamically\nassigned to it by its local operating system. In that case, a client will \ufb01rst\nhave to look up the end point. One solution is to have a special daemon\nrunning on each machine that runs servers. The daemon keeps track of the\ncurrent end point of each service implemented by a co-located server. The\ndaemon itself listens to a well-known end point. A client will \ufb01rst contact the\ndaemon, request the end point, and then contact the speci\ufb01c server, as shown\nin Figure 3.13(a).\nIt is common to associate an end point with a speci\ufb01c service. However,\nactually implementing each service by means of a separate server may be\na waste of resources. For example, in a typical Unix system, it is common\nto have lots of servers running simultaneously, with most of them passively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 129\nGeneral design issues\nA server is a process implementing a speci\ufb01c service on behalf of a collection\nof clients. In essence, each server is organized in the same way: it waits for an\nincoming request from a client and subsequently ensures that the request is\ntaken care of, after which it waits for the next incoming request.\nConcurrent versus iterative servers\nThere are several ways to organize servers. In the case of an iterative server ,\nthe server itself handles the request and, if necessary, returns a response to the\nrequesting client. A concurrent server does not handle the request itself, but\npasses it to a separate thread or another process, after which it immediately\nwaits for the next incoming request. A multithreaded server is an example of\na concurrent server. An alternative implementation of a concurrent server is to\nfork a new process for each new incoming request. This approach is followed\nin many Unix systems. The thread or process that handles the request is\nresponsible for returning a response to the requesting client.\nContacting a server: end points\nAnother issue is where clients contact a server. In all cases, clients send\nrequests to an end point , also called a port , at the machine where the server\nis running. Each server listens to a speci\ufb01c end point. How do clients know\nthe end point of a service? One approach is to globally assign end points for\nwell-known services. For example, servers that handle Internet FTP requests\nalways listen to TCP port 21. Likewise, an HTTP server for the World Wide\nWeb will always listen to TCP port 80. These end points have been assigned\nby the Internet Assigned Numbers Authority (IANA), and are documented\nin [Reynolds and Postel, 1994]. With assigned end points, the client needs to\n\ufb01nd only the network address of the machine where the server is running.\nName services can be used for that purpose.\nThere are many services that do not require a preassigned end point.\nFor example, a time-of-day server may use an end point that is dynamically\nassigned to it by its local operating system. In that case, a client will \ufb01rst\nhave to look up the end point. One solution is to have a special daemon\nrunning on each machine that runs servers. The daemon keeps track of the\ncurrent end point of each service implemented by a co-located server. The\ndaemon itself listens to a well-known end point. A client will \ufb01rst contact the\ndaemon, request the end point, and then contact the speci\ufb01c server, as shown\nin Figure 3.13(a).\nIt is common to associate an end point with a speci\ufb01c service. However,\nactually implementing each service by means of a separate server may be\na waste of resources. For example, in a typical Unix system, it is common\nto have lots of servers running simultaneously, with most of them passively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "130 CHAPTER 3. PROCESSES\n(a)\n(b)\nFigure 3.13: (a) Client-to-server binding using a daemon. (b) Client-to-server\nbinding using a superserver.\nwaiting until a client request comes in. Instead of having to keep track of so\nmany passive processes, it is often more ef\ufb01cient to have a single superserver\nlistening to each end point associated with a speci\ufb01c service, as shown in\nFigure 3.13(b). For example, the inetd daemon in Unix listens to a number of\nwell-known ports for Internet services. When a request comes in, the daemon\nforks a process to handle it. That process will exit when \ufb01nished.\nInterrupting a server\nAnother issue that needs to be taken into account when designing a server is\nwhether and how a server can be interrupted. For example, consider a user\nwho has just decided to upload a huge \ufb01le to an FTP server. Then, suddenly\nrealizing that it is the wrong \ufb01le, he wants to interrupt the server to cancel\nfurther data transmission. There are several ways to do this. One approach\nthat works only too well in the current Internet (and is sometimes the only\nalternative) is for the user to abruptly exit the client application (which will\nautomatically break the connection to the server), immediately restart it, and\npretend nothing happened. The server will eventually tear down the old\nconnection, thinking the client has probably crashed.\nA much better approach for handling communication interrupts is to\ndevelop the client and server such that it is possible to send out-of-band\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n130 CHAPTER 3. PROCESSES\n(a)\n(b)\nFigure 3.13: (a) Client-to-server binding using a daemon. (b) Client-to-server\nbinding using a superserver.\nwaiting until a client request comes in. Instead of having to keep track of so\nmany passive processes, it is often more ef\ufb01cient to have a single superserver\nlistening to each end point associated with a speci\ufb01c service, as shown in\nFigure 3.13(b). For example, the inetd daemon in Unix listens to a number of\nwell-known ports for Internet services. When a request comes in, the daemon\nforks a process to handle it. That process will exit when \ufb01nished.\nInterrupting a server\nAnother issue that needs to be taken into account when designing a server is\nwhether and how a server can be interrupted. For example, consider a user\nwho has just decided to upload a huge \ufb01le to an FTP server. Then, suddenly\nrealizing that it is the wrong \ufb01le, he wants to interrupt the server to cancel\nfurther data transmission. There are several ways to do this. One approach\nthat works only too well in the current Internet (and is sometimes the only\nalternative) is for the user to abruptly exit the client application (which will\nautomatically break the connection to the server), immediately restart it, and\npretend nothing happened. The server will eventually tear down the old\nconnection, thinking the client has probably crashed.\nA much better approach for handling communication interrupts is to\ndevelop the client and server such that it is possible to send out-of-band\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 131\ndata, which is data that is to be processed by the server before any other data\nfrom that client. One solution is to let the server listen to a separate control\nend point to which the client sends out-of-band data, while at the same time\nlistening (with a lower priority) to the end point through which the normal\ndata passes. Another solution is to send out-of-band data across the same\nconnection through which the client is sending the original request. In TCP ,\nfor example, it is possible to transmit urgent data. When urgent data are\nreceived at the server, the latter is interrupted (e.g., through a signal in Unix\nsystems), after which it can inspect the data and handle them accordingly.\nStateless versus stateful servers\nA \ufb01nal, important design issue, is whether or not the server is stateless. A\nstateless server does not keep information on the state of its clients, and can\nchange its own state without having to inform any client [Birman, 2012]. A\nWeb server, for example, is stateless. It merely responds to incoming HTTP\nrequests, which can be either for uploading a \ufb01le to the server or (most often)\nfor fetching a \ufb01le. When the request has been processed, the Web server\nforgets the client completely. Likewise, the collection of \ufb01les that a Web server\nmanages (possibly in cooperation with a \ufb01le server), can be changed without\nclients having to be informed.\nNote that in many stateless designs, the server actually does maintain\ninformation on its clients, but crucial is the fact that if this information is\nlost, it will not lead to a disruption of the service offered by the server. For\nexample, a Web server generally logs all client requests. This information is\nuseful, for example, to decide whether certain documents should be replicated,\nand where they should be replicated to. Clearly, there is no penalty other than\nperhaps in the form of suboptimal performance if the log is lost.\nA particular form of a stateless design is where the server maintains what\nis known as soft state . In this case, the server promises to maintain state on\nbehalf of the client, but only for a limited time. After that time has expired,\nthe server falls back to default behavior, thereby discarding any information it\nkept on account of the associated client. An example of this type of state is\na server promising to keep a client informed about updates, but only for a\nlimited time. After that, the client is required to poll the server for updates.\nSoft-state approaches originate from protocol design in computer networks,\nbut can be equally applied to server design [Clark, 1989; Lui et al., 2004].\nIn contrast, a stateful server generally maintains persistent information\non its clients. This means that the information needs to be explicitly deleted\nby the server. A typical example is a \ufb01le server that allows a client to keep\na local copy of a \ufb01le, even for performing update operations. Such a server\nwould maintain a table containing (client, \ufb01le) entries. Such a table allows the\nserver to keep track of which client currently has the update permissions on\nwhich \ufb01le, and thus possibly also the most recent version of that \ufb01le.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 131\ndata, which is data that is to be processed by the server before any other data\nfrom that client. One solution is to let the server listen to a separate control\nend point to which the client sends out-of-band data, while at the same time\nlistening (with a lower priority) to the end point through which the normal\ndata passes. Another solution is to send out-of-band data across the same\nconnection through which the client is sending the original request. In TCP ,\nfor example, it is possible to transmit urgent data. When urgent data are\nreceived at the server, the latter is interrupted (e.g., through a signal in Unix\nsystems), after which it can inspect the data and handle them accordingly.\nStateless versus stateful servers\nA \ufb01nal, important design issue, is whether or not the server is stateless. A\nstateless server does not keep information on the state of its clients, and can\nchange its own state without having to inform any client [Birman, 2012]. A\nWeb server, for example, is stateless. It merely responds to incoming HTTP\nrequests, which can be either for uploading a \ufb01le to the server or (most often)\nfor fetching a \ufb01le. When the request has been processed, the Web server\nforgets the client completely. Likewise, the collection of \ufb01les that a Web server\nmanages (possibly in cooperation with a \ufb01le server), can be changed without\nclients having to be informed.\nNote that in many stateless designs, the server actually does maintain\ninformation on its clients, but crucial is the fact that if this information is\nlost, it will not lead to a disruption of the service offered by the server. For\nexample, a Web server generally logs all client requests. This information is\nuseful, for example, to decide whether certain documents should be replicated,\nand where they should be replicated to. Clearly, there is no penalty other than\nperhaps in the form of suboptimal performance if the log is lost.\nA particular form of a stateless design is where the server maintains what\nis known as soft state . In this case, the server promises to maintain state on\nbehalf of the client, but only for a limited time. After that time has expired,\nthe server falls back to default behavior, thereby discarding any information it\nkept on account of the associated client. An example of this type of state is\na server promising to keep a client informed about updates, but only for a\nlimited time. After that, the client is required to poll the server for updates.\nSoft-state approaches originate from protocol design in computer networks,\nbut can be equally applied to server design [Clark, 1989; Lui et al., 2004].\nIn contrast, a stateful server generally maintains persistent information\non its clients. This means that the information needs to be explicitly deleted\nby the server. A typical example is a \ufb01le server that allows a client to keep\na local copy of a \ufb01le, even for performing update operations. Such a server\nwould maintain a table containing (client, \ufb01le) entries. Such a table allows the\nserver to keep track of which client currently has the update permissions on\nwhich \ufb01le, and thus possibly also the most recent version of that \ufb01le.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "132 CHAPTER 3. PROCESSES\nThis approach can improve the performance of read and write operations\nas perceived by the client. Performance improvement over stateless servers\nis often an important bene\ufb01t of stateful designs. However, the example also\nillustrates the major drawback of stateful servers. If the server crashes, it has\nto recover its table of (client, \ufb01le) entries, or otherwise it cannot guarantee\nthat it has processed the most recent updates on a \ufb01le. In general, a stateful\nserver needs to recover its entire state as it was just before the crash. Enabling\nrecovery can introduce considerable complexity, as we discuss in . Chapter 8In\na stateless design, no special measures need to be taken at all for a crashed\nserver to recover. It simply starts running again, and waits for client requests\nto come in.\nLing et al. [2004] argue that one should actually make a distinction between\n(temporary) session state and permanent state . The example above is typical\nfor session state: it is associated with a series of operations by a single user\nand should be maintained for a some time, but not inde\ufb01nitely. As it turns\nout, session state is often maintained in three-tiered client-server architectures,\nwhere the application server actually needs to access a database server through\na series of queries before being able to respond to the requesting client. The\nissue here is that no real harm is done if session state is lost, provided that\nthe client can simply re-issue the original request. This observation allows for\nsimpler and less reliable storage of state.\nWhat remains for permanent state is typically information maintained\nin databases, such as customer information, keys associated with purchased\nsoftware, etc. However, for most distributed systems, maintaining session state\nalready implies a stateful design requiring special measures when failures do\nhappen and making explicit assumptions about the durability of state stored\nat the server. We will return to these matters extensively when discussing\nfault tolerance.\nWhen designing a server, the choice for a stateless or stateful design should\nnot affect the services provided by the server. For example, if \ufb01les have to\nbe opened before they can be read from, or written to, then a stateless server\nshould one way or the other mimic this behavior. A common solution is that\nthe server responds to a read or write request by \ufb01rst opening the referred\n\ufb01le, then does the actual read or write operation, and immediately closes the\n\ufb01le again.\nIn other cases, a server may want to keep a record on a client\u2019s behavior so\nthat it can more effectively respond to its requests. For example, Web servers\nsometimes offer the possibility to immediately direct a client to his favorite\npages. This approach is possible only if the server has history information on\nthat client. When the server cannot maintain state, a common solution is then\nto let the client send along additional information on its previous accesses.\nIn the case of the Web, this information is often transparently stored by the\nclient\u2019s browser in what is called a cookie , which is a small piece of data\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n132 CHAPTER 3. PROCESSES\nThis approach can improve the performance of read and write operations\nas perceived by the client. Performance improvement over stateless servers\nis often an important bene\ufb01t of stateful designs. However, the example also\nillustrates the major drawback of stateful servers. If the server crashes, it has\nto recover its table of (client, \ufb01le) entries, or otherwise it cannot guarantee\nthat it has processed the most recent updates on a \ufb01le. In general, a stateful\nserver needs to recover its entire state as it was just before the crash. Enabling\nrecovery can introduce considerable complexity, as we discuss in . Chapter 8In\na stateless design, no special measures need to be taken at all for a crashed\nserver to recover. It simply starts running again, and waits for client requests\nto come in.\nLing et al. [2004] argue that one should actually make a distinction between\n(temporary) session state and permanent state . The example above is typical\nfor session state: it is associated with a series of operations by a single user\nand should be maintained for a some time, but not inde\ufb01nitely. As it turns\nout, session state is often maintained in three-tiered client-server architectures,\nwhere the application server actually needs to access a database server through\na series of queries before being able to respond to the requesting client. The\nissue here is that no real harm is done if session state is lost, provided that\nthe client can simply re-issue the original request. This observation allows for\nsimpler and less reliable storage of state.\nWhat remains for permanent state is typically information maintained\nin databases, such as customer information, keys associated with purchased\nsoftware, etc. However, for most distributed systems, maintaining session state\nalready implies a stateful design requiring special measures when failures do\nhappen and making explicit assumptions about the durability of state stored\nat the server. We will return to these matters extensively when discussing\nfault tolerance.\nWhen designing a server, the choice for a stateless or stateful design should\nnot affect the services provided by the server. For example, if \ufb01les have to\nbe opened before they can be read from, or written to, then a stateless server\nshould one way or the other mimic this behavior. A common solution is that\nthe server responds to a read or write request by \ufb01rst opening the referred\n\ufb01le, then does the actual read or write operation, and immediately closes the\n\ufb01le again.\nIn other cases, a server may want to keep a record on a client\u2019s behavior so\nthat it can more effectively respond to its requests. For example, Web servers\nsometimes offer the possibility to immediately direct a client to his favorite\npages. This approach is possible only if the server has history information on\nthat client. When the server cannot maintain state, a common solution is then\nto let the client send along additional information on its previous accesses.\nIn the case of the Web, this information is often transparently stored by the\nclient\u2019s browser in what is called a cookie , which is a small piece of data\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 133\ncontaining client-speci\ufb01c information that is of interest to the server. Cookies\nare never executed by a browser; they are merely stored.\nThe \ufb01rst time a client accesses a server, the latter sends a cookie along with\nthe requested Web pages back to the browser, after which the browser safely\ntucks the cookie away. Each subsequent time the client accesses the server, its\ncookie for that server is sent along with the request.\nObject servers\nLet us take a look at the general organization of object servers needed for\ndistributed objects. The important difference between a general object server\nand other (more traditional) servers is that an object server by itself does not\nprovide a speci\ufb01c service. Speci\ufb01c services are implemented by the objects\nthat reside in the server. Essentially, the server provides only the means to\ninvoke local objects, based on requests from remote clients. As a consequence,\nit is relatively easy to change services by simply adding and removing objects.\nAn object server thus acts as a place where objects live. An object consists\nof two parts: data representing its state and the code for executing its methods.\nWhether or not these parts are separated, or whether method implementations\nare shared by multiple objects, depends on the object server. Also, there are\ndifferences in the way an object server invokes its objects. For example, in\na multithreaded server, each object may be assigned a separate thread, or a\nseparate thread may be used for each invocation request. These and other\nissues are discussed next.\nFor an object to be invoked, the object server needs to know which code to\nexecute, on which data it should operate, whether it should start a separate\nthread to take care of the invocation, and so on. A simple approach is to\nassume that all objects look alike and that there is only one way to invoke\nan object. Unfortunately, such an approach is generally in\ufb02exible and often\nunnecessarily constrains developers of distributed objects.\nA much better approach is for a server to support different policies. Con-\nsider, for example, a transient object : an object that exists only as long as\nits server exists, but possibly for a shorter period of time. An in-memory,\nread-only copy of a \ufb01le could typically be implemented as a transient object.\nLikewise, a calculator could also be implemented as a transient object. A\nreasonable policy is to create a transient object at the \ufb01rst invocation request\nand to destroy it as soon as no clients are bound to it anymore.\nThe advantage of this approach is that a transient object will need a server\u2019s\nresources only as long as the object is really needed. The drawback is that\nan invocation may take some time to complete, because the object needs to\nbe created \ufb01rst. Therefore, an alternative policy is sometimes to create all\ntransient objects at the time the server is initialized, at the cost of consuming\nresources even when no client is making use of the object.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 133\ncontaining client-speci\ufb01c information that is of interest to the server. Cookies\nare never executed by a browser; they are merely stored.\nThe \ufb01rst time a client accesses a server, the latter sends a cookie along with\nthe requested Web pages back to the browser, after which the browser safely\ntucks the cookie away. Each subsequent time the client accesses the server, its\ncookie for that server is sent along with the request.\nObject servers\nLet us take a look at the general organization of object servers needed for\ndistributed objects. The important difference between a general object server\nand other (more traditional) servers is that an object server by itself does not\nprovide a speci\ufb01c service. Speci\ufb01c services are implemented by the objects\nthat reside in the server. Essentially, the server provides only the means to\ninvoke local objects, based on requests from remote clients. As a consequence,\nit is relatively easy to change services by simply adding and removing objects.\nAn object server thus acts as a place where objects live. An object consists\nof two parts: data representing its state and the code for executing its methods.\nWhether or not these parts are separated, or whether method implementations\nare shared by multiple objects, depends on the object server. Also, there are\ndifferences in the way an object server invokes its objects. For example, in\na multithreaded server, each object may be assigned a separate thread, or a\nseparate thread may be used for each invocation request. These and other\nissues are discussed next.\nFor an object to be invoked, the object server needs to know which code to\nexecute, on which data it should operate, whether it should start a separate\nthread to take care of the invocation, and so on. A simple approach is to\nassume that all objects look alike and that there is only one way to invoke\nan object. Unfortunately, such an approach is generally in\ufb02exible and often\nunnecessarily constrains developers of distributed objects.\nA much better approach is for a server to support different policies. Con-\nsider, for example, a transient object : an object that exists only as long as\nits server exists, but possibly for a shorter period of time. An in-memory,\nread-only copy of a \ufb01le could typically be implemented as a transient object.\nLikewise, a calculator could also be implemented as a transient object. A\nreasonable policy is to create a transient object at the \ufb01rst invocation request\nand to destroy it as soon as no clients are bound to it anymore.\nThe advantage of this approach is that a transient object will need a server\u2019s\nresources only as long as the object is really needed. The drawback is that\nan invocation may take some time to complete, because the object needs to\nbe created \ufb01rst. Therefore, an alternative policy is sometimes to create all\ntransient objects at the time the server is initialized, at the cost of consuming\nresources even when no client is making use of the object.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "134 CHAPTER 3. PROCESSES\nIn a similar fashion, a server could follow the policy that each of its objects\nis placed in a memory segment of its own. In other words, objects share neither\ncode nor data. Such a policy may be necessary when an object implementation\ndoes not separate code and data, or when objects need to be separated for\nsecurity reasons. In the latter case, the server will need to provide special\nmeasures, or require support from the underlying operating system, to ensure\nthat segment boundaries are not violated.\nThe alternative approach is to let objects at least share their code. For\nexample, a database containing objects that belong to the same class can be\nef\ufb01ciently implemented by loading the class implementation only once into\nthe server. When a request for an object invocation comes in, the server need\nonly fetch that object\u2019s state and execute the requested method.\nLikewise, there are many different policies with respect to threading. The\nsimplest approach is to implement the server with only a single thread of\ncontrol. Alternatively, the server may have several threads, one for each of\nits objects. Whenever an invocation request comes in for an object, the server\npasses the request to the thread responsible for that object. If the thread is\ncurrently busy, the request is temporarily queued.\nThe advantage of this approach is that objects are automatically protected\nagainst concurrent access: all invocations are serialized through the single\nthread associated with the object. Neat and simple. Of course, it is also possi-\nble to use a separate thread for each invocation request, requiring that objects\nshould have already been protected against concurrent access. Independent\nof using a thread per object or thread per method is the choice of whether\nthreads are created on demand or the server maintains a pool of threads.\nGenerally there is no single best policy. Which one to use depends on whether\nthreads are available, how much performance matters, and similar factors.\nDecisions on how to invoke an object are commonly referred to as activa-\ntion policies , to emphasize that in many cases the object itself must \ufb01rst be\nbrought into the server\u2019s address space (i.e., activated) before it can actually\nbe invoked. What is needed then is a mechanism to group objects per policy.\nSuch a mechanism is sometimes called an object adapter , or alternatively\nanobject wrapper . An object adapter can best be thought of as software\nimplementing a speci\ufb01c activation policy. The main issue, however, is that\nobject adapters come as generic components to assist developers of distributed\nobjects, and which need only to be con\ufb01gured for a speci\ufb01c policy.\nAn object adapter has one or more objects under its control. Because a\nserver should be capable of simultaneously supporting objects that require\ndifferent activation policies, several object adapters may reside in the same\nserver. When an invocation request is delivered to the server, the request is\n\ufb01rst dispatched to the appropriate object adapter, as shown in Figure 3.14.\nAn important observation is that object adapters are unaware of the speci\ufb01c\ninterfaces of the objects they control. Otherwise, they could never be generic.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n134 CHAPTER 3. PROCESSES\nIn a similar fashion, a server could follow the policy that each of its objects\nis placed in a memory segment of its own. In other words, objects share neither\ncode nor data. Such a policy may be necessary when an object implementation\ndoes not separate code and data, or when objects need to be separated for\nsecurity reasons. In the latter case, the server will need to provide special\nmeasures, or require support from the underlying operating system, to ensure\nthat segment boundaries are not violated.\nThe alternative approach is to let objects at least share their code. For\nexample, a database containing objects that belong to the same class can be\nef\ufb01ciently implemented by loading the class implementation only once into\nthe server. When a request for an object invocation comes in, the server need\nonly fetch that object\u2019s state and execute the requested method.\nLikewise, there are many different policies with respect to threading. The\nsimplest approach is to implement the server with only a single thread of\ncontrol. Alternatively, the server may have several threads, one for each of\nits objects. Whenever an invocation request comes in for an object, the server\npasses the request to the thread responsible for that object. If the thread is\ncurrently busy, the request is temporarily queued.\nThe advantage of this approach is that objects are automatically protected\nagainst concurrent access: all invocations are serialized through the single\nthread associated with the object. Neat and simple. Of course, it is also possi-\nble to use a separate thread for each invocation request, requiring that objects\nshould have already been protected against concurrent access. Independent\nof using a thread per object or thread per method is the choice of whether\nthreads are created on demand or the server maintains a pool of threads.\nGenerally there is no single best policy. Which one to use depends on whether\nthreads are available, how much performance matters, and similar factors.\nDecisions on how to invoke an object are commonly referred to as activa-\ntion policies , to emphasize that in many cases the object itself must \ufb01rst be\nbrought into the server\u2019s address space (i.e., activated) before it can actually\nbe invoked. What is needed then is a mechanism to group objects per policy.\nSuch a mechanism is sometimes called an object adapter , or alternatively\nanobject wrapper . An object adapter can best be thought of as software\nimplementing a speci\ufb01c activation policy. The main issue, however, is that\nobject adapters come as generic components to assist developers of distributed\nobjects, and which need only to be con\ufb01gured for a speci\ufb01c policy.\nAn object adapter has one or more objects under its control. Because a\nserver should be capable of simultaneously supporting objects that require\ndifferent activation policies, several object adapters may reside in the same\nserver. When an invocation request is delivered to the server, the request is\n\ufb01rst dispatched to the appropriate object adapter, as shown in Figure 3.14.\nAn important observation is that object adapters are unaware of the speci\ufb01c\ninterfaces of the objects they control. Otherwise, they could never be generic.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 135\nFigure 3.14: An object server supporting different activation policies.\nThe only issue that is important to an object adapter is that it can extract an\nobject reference from an invocation request, and subsequently dispatch the\nrequest to the referenced object, but now following a speci\ufb01c activation policy.\nAs is also illustrated in Figure 3.14 rather than passing the request directly\nto the object, an adapter hands an invocation request to the server-side stub\nof that object. The stub, also called a skeleton, is normally generated from\nthe interface de\ufb01nitions of the object, unmarshals the request and invokes the\nappropriate method.\nAn object adapter can support different activation policies by simply\ncon\ufb01guring it at runtime. For example, in CORBA-compliant systems [OMG,\n2004a], it is possible to specify whether an object should continue to exist after\nits associated adapter has stopped. Likewise, an adapter can be con\ufb01gured\nto generate object identi\ufb01ers, or to let the application provide one. As a\n\ufb01nal example, an adapter can be con\ufb01gured to operate in single-threaded or\nmultithreaded mode as we explained above.\nNote that although in Figure 3.14 we have spoken about objects, we\nhave said nothing about what these objects actually are. In particular, it\nshould be stressed that as part of the implementation of such an object the\nserver may (indirectly) access databases or call special library routines. The\nimplementation details are hidden for the object adapter who communicates\nonly with a skeleton. As such, the actual implementation may have nothing\nto do with what we often see with language-level (i.e., compile-time) objects.\nFor this reason, a different terminology is generally adopted. A servant is the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 135\nFigure 3.14: An object server supporting different activation policies.\nThe only issue that is important to an object adapter is that it can extract an\nobject reference from an invocation request, and subsequently dispatch the\nrequest to the referenced object, but now following a speci\ufb01c activation policy.\nAs is also illustrated in Figure 3.14 rather than passing the request directly\nto the object, an adapter hands an invocation request to the server-side stub\nof that object. The stub, also called a skeleton, is normally generated from\nthe interface de\ufb01nitions of the object, unmarshals the request and invokes the\nappropriate method.\nAn object adapter can support different activation policies by simply\ncon\ufb01guring it at runtime. For example, in CORBA-compliant systems [OMG,\n2004a], it is possible to specify whether an object should continue to exist after\nits associated adapter has stopped. Likewise, an adapter can be con\ufb01gured\nto generate object identi\ufb01ers, or to let the application provide one. As a\n\ufb01nal example, an adapter can be con\ufb01gured to operate in single-threaded or\nmultithreaded mode as we explained above.\nNote that although in Figure 3.14 we have spoken about objects, we\nhave said nothing about what these objects actually are. In particular, it\nshould be stressed that as part of the implementation of such an object the\nserver may (indirectly) access databases or call special library routines. The\nimplementation details are hidden for the object adapter who communicates\nonly with a skeleton. As such, the actual implementation may have nothing\nto do with what we often see with language-level (i.e., compile-time) objects.\nFor this reason, a different terminology is generally adopted. A servant is the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "136 CHAPTER 3. PROCESSES\ngeneral term for a piece of code that forms the implementation of an object.\nNote 3.6 (Example: The Ice runtime system)\nLet us brie\ufb02y consider the Ice distributed-object system, which has been partly\ndeveloped in response to the intricacies of commercial object-based distributed\nsystems [Henning, 2004]. An object server in Ice is nothing but an ordinary\nprocess that simply starts with initializing the Ice runtime system (RTS). The\nbasis of the runtime environment is formed by what is called a communicator . A\ncommunicator is a component that manages a number of basic resources, of which\nthe most important one is formed by a pool of threads. Likewise, it will have\nassociated dynamically allocated memory, and so on. In addition, a communicator\nprovides the means for con\ufb01guring the environment. For example, it is possible\nto specify maximum message lengths, maximum invocation retries, and so on.\nNormally, an object server would have only a single communicator. However,\nwhen different applications need to be fully separated and protected from each\nother, a separate communicator (with possibly a different con\ufb01guration) can be\ncreated within the same process. At the very least, such an approach would\nseparate the different thread pools so that if one application has consumed all its\nthreads, then this would not affect the other application.\nA communicator can also be used to create an object adapter, such as shown\nin Figure 3.15. We note that the code is simpli\ufb01ed and incomplete. More examples\nand detailed information on Ice can be found in Henning and Spruiell [2005].\nmain( intargc, char* argv[]) {\nIce::Communicator ic;\nIce::ObjectAdapter adapter;\nIce::Object object ;\nic = Ice::initialize(argc, argv);\nadapter = ic->createObjectAdapterWithEnd Points( \"MyAdapter\",\"tcp -p 10000\");\nobject = new MyObject;\nadapter->add( object , objectID);\nadapter->activate();\nic->waitForShutdown();\n}\nFigure 3.15: Example of creating an object server in Ice.\nIn this example, we start with creating and initializing the runtime environ-\nment. When that is done, an object adapter is created. In this case, it is instructed\nto listen for incoming TCP connections on port 10000. Note that the adapter\nis created in the context of the just created communicator. We are now in the\nposition to create an object and to subsequently add that object to the adapter.\nFinally, the adapter is activated , meaning that, under the hood, a thread is activated\nthat will start listening for incoming requests.\nThis code does not yet show much differentiation in activation policies. Poli-\ncies can be changed by modifying the properties of an adapter. One family of\nproperties is related to maintaining an adapter-speci\ufb01c set of threads that are used\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n136 CHAPTER 3. PROCESSES\ngeneral term for a piece of code that forms the implementation of an object.\nNote 3.6 (Example: The Ice runtime system)\nLet us brie\ufb02y consider the Ice distributed-object system, which has been partly\ndeveloped in response to the intricacies of commercial object-based distributed\nsystems [Henning, 2004]. An object server in Ice is nothing but an ordinary\nprocess that simply starts with initializing the Ice runtime system (RTS). The\nbasis of the runtime environment is formed by what is called a communicator . A\ncommunicator is a component that manages a number of basic resources, of which\nthe most important one is formed by a pool of threads. Likewise, it will have\nassociated dynamically allocated memory, and so on. In addition, a communicator\nprovides the means for con\ufb01guring the environment. For example, it is possible\nto specify maximum message lengths, maximum invocation retries, and so on.\nNormally, an object server would have only a single communicator. However,\nwhen different applications need to be fully separated and protected from each\nother, a separate communicator (with possibly a different con\ufb01guration) can be\ncreated within the same process. At the very least, such an approach would\nseparate the different thread pools so that if one application has consumed all its\nthreads, then this would not affect the other application.\nA communicator can also be used to create an object adapter, such as shown\nin Figure 3.15. We note that the code is simpli\ufb01ed and incomplete. More examples\nand detailed information on Ice can be found in Henning and Spruiell [2005].\nmain( intargc, char* argv[]) {\nIce::Communicator ic;\nIce::ObjectAdapter adapter;\nIce::Object object ;\nic = Ice::initialize(argc, argv);\nadapter = ic->createObjectAdapterWithEnd Points( \"MyAdapter\",\"tcp -p 10000\");\nobject = new MyObject;\nadapter->add( object , objectID);\nadapter->activate();\nic->waitForShutdown();\n}\nFigure 3.15: Example of creating an object server in Ice.\nIn this example, we start with creating and initializing the runtime environ-\nment. When that is done, an object adapter is created. In this case, it is instructed\nto listen for incoming TCP connections on port 10000. Note that the adapter\nis created in the context of the just created communicator. We are now in the\nposition to create an object and to subsequently add that object to the adapter.\nFinally, the adapter is activated , meaning that, under the hood, a thread is activated\nthat will start listening for incoming requests.\nThis code does not yet show much differentiation in activation policies. Poli-\ncies can be changed by modifying the properties of an adapter. One family of\nproperties is related to maintaining an adapter-speci\ufb01c set of threads that are used\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 137\nfor handling incoming requests. For example, one can specify that there should\nalways be only one thread, effectively serializing all accesses to objects that have\nbeen added to the adapter.\nAgain, note that we have not speci\ufb01ed MyObject . Like before, this could be a\nsimple C++ object, but also one that accesses databases and other external services\nthat jointly implement an object. By registering MyObject with an adapter, such\nimplementation details are completely hidden from clients, who now believe that\nthey are invoking a remote object.\nIn the example above, an object is created as part of the application, after\nwhich it is added to an adapter. Effectively, this means that an adapter may need\nto support many objects at the same time, leading to potential scalability problems.\nAn alternative solution is to dynamically load objects into memory when they are\nneeded. To do this, Ice provides support for special objects known as locators . A\nlocator is called when the adapter receives an incoming request for an object that\nhas not been explicitly added. In that case, the request is forwarded to the locator,\nwhose job is to further handle the request.\nTo make matters more concrete, suppose a locator is handed a request for an\nobject of which the locator knows that its state is stored in a relational database\nsystem. Of course, there is no magic here: the locator has been programmed\nexplicitly to handle such requests. In this case, the object\u2019s identi\ufb01er may corre-\nspond to the key of a record in which that state is stored. The locator will then\nsimply do a lookup on that key, fetch the state, and will then be able to further\nprocess the request.\nThere can be more than one locator added to an adapter. In that case, the\nadapter would keep track of which object identi\ufb01ers would belong to the same\nlocator. Using multiple locators allows supporting many objects by a single\nadapter. Of course, objects (or rather their state) would need to be loaded at\nruntime, but this dynamic behavior would possibly make the server itself relatively\nsimple.\nNote 3.7 (Example: Enterprise Java Beans)\nThe Java programming language and associated model has formed the founda-\ntion for numerous distributed systems and applications. Its popularity can be\nattributed to the straightforward support for object orientation, combined with\nthe inherent support for remote method invocation. Java provides a high degree\nof access transparency, making it easier to use than, for example, the combination\nof C with remote procedure calling.\nEver since its introduction, there has been a strong incentive to provide\nfacilities that would ease the development of distributed applications. These\nfacilities go well beyond language support, requiring a runtime environment that\nsupports traditional multitiered client-server architectures. To this end, much\nwork has been put into the development of (Enterprise) Java Beans (EJB).\nAn EJB is essentially a Java object that is hosted by a special server offering\ndifferent ways for remote clients to invoke that object. Crucial is that this server\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 137\nfor handling incoming requests. For example, one can specify that there should\nalways be only one thread, effectively serializing all accesses to objects that have\nbeen added to the adapter.\nAgain, note that we have not speci\ufb01ed MyObject . Like before, this could be a\nsimple C++ object, but also one that accesses databases and other external services\nthat jointly implement an object. By registering MyObject with an adapter, such\nimplementation details are completely hidden from clients, who now believe that\nthey are invoking a remote object.\nIn the example above, an object is created as part of the application, after\nwhich it is added to an adapter. Effectively, this means that an adapter may need\nto support many objects at the same time, leading to potential scalability problems.\nAn alternative solution is to dynamically load objects into memory when they are\nneeded. To do this, Ice provides support for special objects known as locators . A\nlocator is called when the adapter receives an incoming request for an object that\nhas not been explicitly added. In that case, the request is forwarded to the locator,\nwhose job is to further handle the request.\nTo make matters more concrete, suppose a locator is handed a request for an\nobject of which the locator knows that its state is stored in a relational database\nsystem. Of course, there is no magic here: the locator has been programmed\nexplicitly to handle such requests. In this case, the object\u2019s identi\ufb01er may corre-\nspond to the key of a record in which that state is stored. The locator will then\nsimply do a lookup on that key, fetch the state, and will then be able to further\nprocess the request.\nThere can be more than one locator added to an adapter. In that case, the\nadapter would keep track of which object identi\ufb01ers would belong to the same\nlocator. Using multiple locators allows supporting many objects by a single\nadapter. Of course, objects (or rather their state) would need to be loaded at\nruntime, but this dynamic behavior would possibly make the server itself relatively\nsimple.\nNote 3.7 (Example: Enterprise Java Beans)\nThe Java programming language and associated model has formed the founda-\ntion for numerous distributed systems and applications. Its popularity can be\nattributed to the straightforward support for object orientation, combined with\nthe inherent support for remote method invocation. Java provides a high degree\nof access transparency, making it easier to use than, for example, the combination\nof C with remote procedure calling.\nEver since its introduction, there has been a strong incentive to provide\nfacilities that would ease the development of distributed applications. These\nfacilities go well beyond language support, requiring a runtime environment that\nsupports traditional multitiered client-server architectures. To this end, much\nwork has been put into the development of (Enterprise) Java Beans (EJB).\nAn EJB is essentially a Java object that is hosted by a special server offering\ndifferent ways for remote clients to invoke that object. Crucial is that this server\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "138 CHAPTER 3. PROCESSES\nprovides the support to separate application functionality from systems-oriented\nfunctionality. The latter includes functions for looking up objects, storing objects,\nletting objects be part of a transaction, and so on. How to develop EJBs is described\nin detail by Schildt [2010].\nFigure 3.16: General architecture of an EJB server.\nWith this separation in mind, EJBs can be pictured as shown in Figure 3.16.\nThe important issue is that an EJB is embedded inside a container which effectively\nprovides interfaces to underlying services that are implemented by the application\nserver. The container can more or less automatically bind the EJB to these services,\nmeaning that the correct references are readily available to a programmer. Typical\nservices include those for remote method invocation (RMI), database access\n(JDBC), naming (JNDI), and messaging (JMS). Making use of these services is\nmore or less automated, but does require that the programmer makes a distinction\nbetween four kinds of EJBs:\n1. Stateless session beans\n2. Stateful session beans\n3. Entity beans\n4. Message-driven beans\nAs its name suggests, a stateless session bean is an object that is invoked\nonce, does its work, after which it discards any information it needed to perform\nthe service it offered to a client. For example, a stateless session bean could be\nused to implement a service that lists the top-ranked books. In this case, the bean\nwould typically consist of an SQL query that is submitted to a database. The\nresults would be put into a special format that the client can handle, after which\nits work would have been completed and the listed books discarded.\nIn contrast, a stateful session bean maintains client-related state. The canoni-\ncal example is a bean implementing an electronic shopping cart. In this case, a\nclient would typically be able to put things in a cart, remove items, and use the\ncart to go to an electronic checkout. The bean, in turn, would typically access\ndatabases for getting current prices and information on the number of items still\nin stock. However, its lifetime would still be limited, which is why it is referred to\nas a session bean: when the client is \ufb01nished (possibly having invoked the object\nseveral times), the bean will automatically be destroyed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n138 CHAPTER 3. PROCESSES\nprovides the support to separate application functionality from systems-oriented\nfunctionality. The latter includes functions for looking up objects, storing objects,\nletting objects be part of a transaction, and so on. How to develop EJBs is described\nin detail by Schildt [2010].\nFigure 3.16: General architecture of an EJB server.\nWith this separation in mind, EJBs can be pictured as shown in Figure 3.16.\nThe important issue is that an EJB is embedded inside a container which effectively\nprovides interfaces to underlying services that are implemented by the application\nserver. The container can more or less automatically bind the EJB to these services,\nmeaning that the correct references are readily available to a programmer. Typical\nservices include those for remote method invocation (RMI), database access\n(JDBC), naming (JNDI), and messaging (JMS). Making use of these services is\nmore or less automated, but does require that the programmer makes a distinction\nbetween four kinds of EJBs:\n1. Stateless session beans\n2. Stateful session beans\n3. Entity beans\n4. Message-driven beans\nAs its name suggests, a stateless session bean is an object that is invoked\nonce, does its work, after which it discards any information it needed to perform\nthe service it offered to a client. For example, a stateless session bean could be\nused to implement a service that lists the top-ranked books. In this case, the bean\nwould typically consist of an SQL query that is submitted to a database. The\nresults would be put into a special format that the client can handle, after which\nits work would have been completed and the listed books discarded.\nIn contrast, a stateful session bean maintains client-related state. The canoni-\ncal example is a bean implementing an electronic shopping cart. In this case, a\nclient would typically be able to put things in a cart, remove items, and use the\ncart to go to an electronic checkout. The bean, in turn, would typically access\ndatabases for getting current prices and information on the number of items still\nin stock. However, its lifetime would still be limited, which is why it is referred to\nas a session bean: when the client is \ufb01nished (possibly having invoked the object\nseveral times), the bean will automatically be destroyed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 139\nAnentity bean can be considered to be a long-lived persistent object. As\nsuch, an entity bean will generally be stored in a database, and likewise, will\noften also be part of transactions. Typically, entity beans store information that\nmay be needed a next time a speci\ufb01c client accesses the server. In settings for\nelectronic commerce, an entity bean can be used to record customer information,\nfor example, shipping address, billing address, credit card information, and so on.\nIn these cases, when a client logs in, his associated entity bean will be restored\nand used for further processing.\nFinally, message-driven beans are used to program objects that should react\nto incoming messages (and likewise, be able to send messages). Message-driven\nbeans cannot be invoked directly by a client, but rather \ufb01t into a publish-subscribe\nway of communication. What it boils down to is that a message-driven bean\nis automatically called by the server when a speci\ufb01c message mis received, to\nwhich the server (or rather an application it is hosting) had previously subscribed,\ni.e., stated that it wanted to be noti\ufb01ed when such a message arrives. The bean\ncontains application code for handling the message, after which the server simply\ndiscards it. Message-driven beans are thus seen to be stateless.\nExample: The Apache Web server\nAn interesting example of a server that balances the separation between\npolicies and mechanisms is the Apache Web server. It is also an extremely\npopular server, estimated to be used to host approximately 50% of all Web sites.\nApache is a complex piece of software, and with the numerous enhancements\nto the types of documents that are now offered in the Web, it is important that\nthe server is highly con\ufb01gurable and extensible, and at the same time largely\nindependent of speci\ufb01c platforms.\nMaking the server platform independent is realized by essentially provid-\ning its own basic runtime environment, which is then subsequently imple-\nmented for different operating systems. This runtime environment, known\nas the Apache Portable Runtime (APR ), is a library that provides a platform-\nindependent interface for \ufb01le handling, networking, locking, threads, and so\non. When extending Apache, portability is largely guaranteed provided that\nonly calls to the APR are made and that calls to platform-speci\ufb01c libraries are\navoided.\nFrom a certain perspective, Apache can be considered as a completely gen-\neral server tailored to produce a response to an incoming request. Of course,\nthere are all kinds of hidden dependencies and assumptions by which Apache\nturns out to be primarily suited for handling requests for Web documents.\nFor example, as we mentioned, Web browsers and servers use HTTP as their\ncommunication protocol. HTTP is virtually always implemented on top of\nTCP , for which reason the core of Apache assumes that all incoming requests\nadhere to a TCP-based connection-oriented way of communication. Requests\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 139\nAnentity bean can be considered to be a long-lived persistent object. As\nsuch, an entity bean will generally be stored in a database, and likewise, will\noften also be part of transactions. Typically, entity beans store information that\nmay be needed a next time a speci\ufb01c client accesses the server. In settings for\nelectronic commerce, an entity bean can be used to record customer information,\nfor example, shipping address, billing address, credit card information, and so on.\nIn these cases, when a client logs in, his associated entity bean will be restored\nand used for further processing.\nFinally, message-driven beans are used to program objects that should react\nto incoming messages (and likewise, be able to send messages). Message-driven\nbeans cannot be invoked directly by a client, but rather \ufb01t into a publish-subscribe\nway of communication. What it boils down to is that a message-driven bean\nis automatically called by the server when a speci\ufb01c message mis received, to\nwhich the server (or rather an application it is hosting) had previously subscribed,\ni.e., stated that it wanted to be noti\ufb01ed when such a message arrives. The bean\ncontains application code for handling the message, after which the server simply\ndiscards it. Message-driven beans are thus seen to be stateless.\nExample: The Apache Web server\nAn interesting example of a server that balances the separation between\npolicies and mechanisms is the Apache Web server. It is also an extremely\npopular server, estimated to be used to host approximately 50% of all Web sites.\nApache is a complex piece of software, and with the numerous enhancements\nto the types of documents that are now offered in the Web, it is important that\nthe server is highly con\ufb01gurable and extensible, and at the same time largely\nindependent of speci\ufb01c platforms.\nMaking the server platform independent is realized by essentially provid-\ning its own basic runtime environment, which is then subsequently imple-\nmented for different operating systems. This runtime environment, known\nas the Apache Portable Runtime (APR ), is a library that provides a platform-\nindependent interface for \ufb01le handling, networking, locking, threads, and so\non. When extending Apache, portability is largely guaranteed provided that\nonly calls to the APR are made and that calls to platform-speci\ufb01c libraries are\navoided.\nFrom a certain perspective, Apache can be considered as a completely gen-\neral server tailored to produce a response to an incoming request. Of course,\nthere are all kinds of hidden dependencies and assumptions by which Apache\nturns out to be primarily suited for handling requests for Web documents.\nFor example, as we mentioned, Web browsers and servers use HTTP as their\ncommunication protocol. HTTP is virtually always implemented on top of\nTCP , for which reason the core of Apache assumes that all incoming requests\nadhere to a TCP-based connection-oriented way of communication. Requests\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "140 CHAPTER 3. PROCESSES\nbased on UDP cannot be handled without modifying the Apache core.\nHowever, the Apache core makes few assumptions on how incoming\nrequests should be handled. Its overall organization is shown in Figure 3.17.\nFundamental to this organization is the concept of a hook , which is nothing\nbut a placeholder for a speci\ufb01c group of functions. The Apache core assumes\nthat requests are processed in a number of phases, each phase consisting of a\nfew hooks. Each hook thus represents a group of similar actions that need to\nbe executed as part of processing a request.\nFigure 3.17: The general organization of the Apache Web server.\nFor example, there is a hook to translate a URL to a local \ufb01le name. Such a\ntranslation will almost certainly need to be done when processing a request.\nLikewise, there is a hook for writing information to a log, a hook for checking\na client\u2019s identi\ufb01cation, a hook for checking access rights, and a hook for\nchecking which MIME type the request is related to (e.g., to make sure that\nthe request can be properly handled). As shown in Figure 3.17, the hooks\nare processed in a predetermined order. It is here that we explicitly see\nthat Apache enforces a speci\ufb01c \ufb02ow of control concerning the processing of\nrequests.\nThe functions associated with a hook are all provided by separate modules .\nAlthough, in principle, a developer could change the set of hooks that will\nbe processed by Apache, it is far more common to write modules contain-\ning the functions that need to be called as part of processing the standard\nhooks provided by unmodi\ufb01ed Apache. The underlying principle is fairly\nstraightforward. Every hook can contain a set of functions that each should\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n140 CHAPTER 3. PROCESSES\nbased on UDP cannot be handled without modifying the Apache core.\nHowever, the Apache core makes few assumptions on how incoming\nrequests should be handled. Its overall organization is shown in Figure 3.17.\nFundamental to this organization is the concept of a hook , which is nothing\nbut a placeholder for a speci\ufb01c group of functions. The Apache core assumes\nthat requests are processed in a number of phases, each phase consisting of a\nfew hooks. Each hook thus represents a group of similar actions that need to\nbe executed as part of processing a request.\nFigure 3.17: The general organization of the Apache Web server.\nFor example, there is a hook to translate a URL to a local \ufb01le name. Such a\ntranslation will almost certainly need to be done when processing a request.\nLikewise, there is a hook for writing information to a log, a hook for checking\na client\u2019s identi\ufb01cation, a hook for checking access rights, and a hook for\nchecking which MIME type the request is related to (e.g., to make sure that\nthe request can be properly handled). As shown in Figure 3.17, the hooks\nare processed in a predetermined order. It is here that we explicitly see\nthat Apache enforces a speci\ufb01c \ufb02ow of control concerning the processing of\nrequests.\nThe functions associated with a hook are all provided by separate modules .\nAlthough, in principle, a developer could change the set of hooks that will\nbe processed by Apache, it is far more common to write modules contain-\ning the functions that need to be called as part of processing the standard\nhooks provided by unmodi\ufb01ed Apache. The underlying principle is fairly\nstraightforward. Every hook can contain a set of functions that each should\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 141\nmatch a speci\ufb01c function prototype (i.e., list of parameters and return type).\nA module developer will write functions for speci\ufb01c hooks. When compiling\nApache, the developer speci\ufb01es which function should be added to which\nhook. The latter is shown in Figure 3.17 as the various links between functions\nand hooks.\nBecause there may be tens of modules, each hook will generally contain\nseveral functions. Normally, modules are considered to be mutual indepen-\ndent, so that functions in the same hook will be executed in some arbitrary\norder. However, Apache can also handle module dependencies by letting\na developer specify an ordering in which functions from different modules\nshould be processed. By and large, the result is a Web server that is ex-\ntremely versatile. Detailed information on con\ufb01guring Apache, as well as an\nintroduction to how it can be extended is found in [Laurie and Laurie, 2002].\nServer clusters\nIn Chapter 1, we brie\ufb02y discussed cluster computing as one of the many\nappearances of distributed systems. We now take a closer look at the organiza-\ntion of server clusters, along with the salient design issues. We \ufb01rst consider\ncommon server clusters that are organized in local-area networks. A special\ngroup is formed by wide-area server clusters, which we subsequently discuss.\nLocal-area clusters\nSimply put, a server cluster is nothing else but a collection of machines\nconnected through a network, where each machine runs one or more servers.\nThe server clusters that we consider here, are the ones in which the machines\nare connected through a local-area network, often offering high bandwidth\nand low latency.\nGeneral organization In many cases, a server cluster is logically organized\ninto three tiers, as shown in Figure 3.18. The \ufb01rst tier consists of a (logical)\nswitch through which client requests are routed. Such a switch can vary\nwidely. For example, transport-layer switches accept incoming TCP connection\nrequests and pass requests on to one of servers in the cluster. A completely\ndifferent example is a Web server that accepts incoming HTTP requests, but\nthat partly passes requests to application servers for further processing only\nto later collect results from those servers and return an HTTP response.\nAs in any multitiered client-server architecture, many server clusters also\ncontain servers dedicated to application processing. In cluster computing,\nthese are typically servers running on high-performance hardware dedicated\nto delivering compute power. However, in the case of enterprise server\nclusters, it may be the case that applications need only run on relatively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 141\nmatch a speci\ufb01c function prototype (i.e., list of parameters and return type).\nA module developer will write functions for speci\ufb01c hooks. When compiling\nApache, the developer speci\ufb01es which function should be added to which\nhook. The latter is shown in Figure 3.17 as the various links between functions\nand hooks.\nBecause there may be tens of modules, each hook will generally contain\nseveral functions. Normally, modules are considered to be mutual indepen-\ndent, so that functions in the same hook will be executed in some arbitrary\norder. However, Apache can also handle module dependencies by letting\na developer specify an ordering in which functions from different modules\nshould be processed. By and large, the result is a Web server that is ex-\ntremely versatile. Detailed information on con\ufb01guring Apache, as well as an\nintroduction to how it can be extended is found in [Laurie and Laurie, 2002].\nServer clusters\nIn Chapter 1, we brie\ufb02y discussed cluster computing as one of the many\nappearances of distributed systems. We now take a closer look at the organiza-\ntion of server clusters, along with the salient design issues. We \ufb01rst consider\ncommon server clusters that are organized in local-area networks. A special\ngroup is formed by wide-area server clusters, which we subsequently discuss.\nLocal-area clusters\nSimply put, a server cluster is nothing else but a collection of machines\nconnected through a network, where each machine runs one or more servers.\nThe server clusters that we consider here, are the ones in which the machines\nare connected through a local-area network, often offering high bandwidth\nand low latency.\nGeneral organization In many cases, a server cluster is logically organized\ninto three tiers, as shown in Figure 3.18. The \ufb01rst tier consists of a (logical)\nswitch through which client requests are routed. Such a switch can vary\nwidely. For example, transport-layer switches accept incoming TCP connection\nrequests and pass requests on to one of servers in the cluster. A completely\ndifferent example is a Web server that accepts incoming HTTP requests, but\nthat partly passes requests to application servers for further processing only\nto later collect results from those servers and return an HTTP response.\nAs in any multitiered client-server architecture, many server clusters also\ncontain servers dedicated to application processing. In cluster computing,\nthese are typically servers running on high-performance hardware dedicated\nto delivering compute power. However, in the case of enterprise server\nclusters, it may be the case that applications need only run on relatively\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "142 CHAPTER 3. PROCESSES\nFigure 3.18: The general organization of a three-tiered server cluster.\nlow-end machines, as the required compute power is not the bottleneck, but\naccess to storage is.\nThis brings us the third tier, which consists of data-processing servers,\nnotably \ufb01le and database servers. Again, depending on the usage of the server\ncluster, these servers may be running on specialized machines, con\ufb01gured for\nhigh-speed disk access and having large server-side data caches.\nOf course, not all server clusters will follow this strict separation. It is\nfrequently the case that each machine is equipped with its own local storage,\noften integrating application and data processing in a single server leading to\na two-tiered architecture. For example, when dealing with streaming media\nby means of a server cluster, it is common to deploy a two-tiered system\narchitecture, where each machine acts as a dedicated media server [Steinmetz\nand Nahrstedt, 2004].\nWhen a server cluster offers multiple services, it may happen that different\nmachines run different application servers. As a consequence, the switch will\nhave to be able to distinguish services or otherwise it cannot forward requests\nto the proper machines. As a consequence, we may \ufb01nd that certain machines\nare temporarily idle, while others are receiving an overload of requests. What\nwould be useful is to temporarily migrate services to idle machines. A solution\nis to use virtual machines allowing a relatively easy migration of code to real\nmachines.\nRequest dispatching Let us now take a closer look at the \ufb01rst tier, consisting\nof the switch, also known as the front end . An important design goal for\nserver clusters is to hide the fact that there are multiple servers. In other\nwords, client applications running on remote machines should have no need\nto know anything about the internal organization of the cluster. This access\ntransparency is invariably offered by means of a single access point, in turn\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n142 CHAPTER 3. PROCESSES\nFigure 3.18: The general organization of a three-tiered server cluster.\nlow-end machines, as the required compute power is not the bottleneck, but\naccess to storage is.\nThis brings us the third tier, which consists of data-processing servers,\nnotably \ufb01le and database servers. Again, depending on the usage of the server\ncluster, these servers may be running on specialized machines, con\ufb01gured for\nhigh-speed disk access and having large server-side data caches.\nOf course, not all server clusters will follow this strict separation. It is\nfrequently the case that each machine is equipped with its own local storage,\noften integrating application and data processing in a single server leading to\na two-tiered architecture. For example, when dealing with streaming media\nby means of a server cluster, it is common to deploy a two-tiered system\narchitecture, where each machine acts as a dedicated media server [Steinmetz\nand Nahrstedt, 2004].\nWhen a server cluster offers multiple services, it may happen that different\nmachines run different application servers. As a consequence, the switch will\nhave to be able to distinguish services or otherwise it cannot forward requests\nto the proper machines. As a consequence, we may \ufb01nd that certain machines\nare temporarily idle, while others are receiving an overload of requests. What\nwould be useful is to temporarily migrate services to idle machines. A solution\nis to use virtual machines allowing a relatively easy migration of code to real\nmachines.\nRequest dispatching Let us now take a closer look at the \ufb01rst tier, consisting\nof the switch, also known as the front end . An important design goal for\nserver clusters is to hide the fact that there are multiple servers. In other\nwords, client applications running on remote machines should have no need\nto know anything about the internal organization of the cluster. This access\ntransparency is invariably offered by means of a single access point, in turn\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 143\nimplemented through some kind of hardware switch such as a dedicated\nmachine.\nThe switch forms the entry point for the server cluster, offering a single\nnetwork address. For scalability and availability, a server cluster may have\nmultiple access points, where each access point is then realized by a separate\ndedicated machine. We consider only the case of a single access point.\nA standard way of accessing a server cluster is to set up a TCP connection\nover which application-level requests are then sent as part of a session. A\nsession ends by tearing down the connection. In the case of transport-layer\nswitches , the switch accepts incoming TCP connection requests, and hands\noff such connections to one of the servers. There are essentially two ways how\nthe switch can operate [Cardellini et al., 2002].\nIn the \ufb01rst case, the client sets up a TCP connection such that all requests\nand responses pass through the switch. The switch, in turn, will set up a TCP\nconnection with a selected server and pass client requests to that server, and\nalso accept server responses (which it will pass on to the client). In effect,\nthe switch sits in the middle of a TCP connection between the client and a\nselected server, rewriting the source and destination addresses when passing\nTCP segments. This approach is a form of network address translation\n(NAT ) [Srisuresh and Holdrege, 1999].\nAlternatively, the switch can actually hand off the connection to a selected\nserver such that all responses are directly communicated to the client without\npassing through the server [Hunt et al., 1997; Pai et al., 1998]. The principle\nworking of what is commonly known as TCP handoff is shown in Figure 3.19.\nFigure 3.19: The principle of TCP handoff.\nWhen the switch receives a TCP connection request, it \ufb01rst identi\ufb01es the\nbest server for handling that request, and forwards the request packet to\nthat server. The server, in turn, will send an acknowledgment back to the\nrequesting client, but inserting the switch\u2019s IP address as the source \ufb01eld of\nthe header of the IP packet carrying the TCP segment. Note that this address\nrewriting is necessary for the client to continue executing the TCP protocol: it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 143\nimplemented through some kind of hardware switch such as a dedicated\nmachine.\nThe switch forms the entry point for the server cluster, offering a single\nnetwork address. For scalability and availability, a server cluster may have\nmultiple access points, where each access point is then realized by a separate\ndedicated machine. We consider only the case of a single access point.\nA standard way of accessing a server cluster is to set up a TCP connection\nover which application-level requests are then sent as part of a session. A\nsession ends by tearing down the connection. In the case of transport-layer\nswitches , the switch accepts incoming TCP connection requests, and hands\noff such connections to one of the servers. There are essentially two ways how\nthe switch can operate [Cardellini et al., 2002].\nIn the \ufb01rst case, the client sets up a TCP connection such that all requests\nand responses pass through the switch. The switch, in turn, will set up a TCP\nconnection with a selected server and pass client requests to that server, and\nalso accept server responses (which it will pass on to the client). In effect,\nthe switch sits in the middle of a TCP connection between the client and a\nselected server, rewriting the source and destination addresses when passing\nTCP segments. This approach is a form of network address translation\n(NAT ) [Srisuresh and Holdrege, 1999].\nAlternatively, the switch can actually hand off the connection to a selected\nserver such that all responses are directly communicated to the client without\npassing through the server [Hunt et al., 1997; Pai et al., 1998]. The principle\nworking of what is commonly known as TCP handoff is shown in Figure 3.19.\nFigure 3.19: The principle of TCP handoff.\nWhen the switch receives a TCP connection request, it \ufb01rst identi\ufb01es the\nbest server for handling that request, and forwards the request packet to\nthat server. The server, in turn, will send an acknowledgment back to the\nrequesting client, but inserting the switch\u2019s IP address as the source \ufb01eld of\nthe header of the IP packet carrying the TCP segment. Note that this address\nrewriting is necessary for the client to continue executing the TCP protocol: it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "144 CHAPTER 3. PROCESSES\nis expecting an answer back from the switch, not from some arbitrary server\nit has never heard of before. Clearly, a TCP-handoff implementation requires\noperating-system level modi\ufb01cations. TCP handoff is especially effective when\nresponses are much larger than requests, as in the case of Web servers.\nIt can already be seen that the switch can play an important role in dis-\ntributing the load among the various servers. By deciding where to forward a\nrequest to, the switch also decides which server is to handle further processing\nof the request. The simplest load-balancing policy that the switch can follow\nis round robin: each time it picks the next server from its list to forward a\nrequest to. Of course, the switch will have to keep track to which server it\nhanded off a TCP connection, at least until that connection is torn down. As\nit turns out, maintaining this state and handing off subsequent TCP segments\nbelonging to the same TCP connection, may actually slow down the switch.\nMore advanced server selection criteria can be deployed as well. For\nexample, assume multiple services are offered by the server cluster. If the\nswitch can distinguish those services when a request comes in, it can then take\ninformed decisions on where to forward the request to. This server selection\ncan still take place at the transport level, provided services are distinguished\nby means of a port number. In the case of transport-level switches , as we have\ndiscussed so far, decisions on where to forward an incoming request is based\non transport-level information only. One step further is to have the switch\nactually inspect the payload of the incoming request. This content-aware\nrequest distribution can be applied only if it is known what that payload\nlooks like. For example, in the case of Web servers, the switch can expect an\nHTTP request, based on which it can then decide who is to process it.\nNote 3.8 (Advanced: Ef\ufb01cient content-aware request distribution)\nObviously, the design of the switch is crucial, as it can easily become a bottleneck\nwith all traf\ufb01c passing through it. Transport-level switches are generally very\nef\ufb01cient. However, content-aware switches operating at the level of the application\nlayer that inspect the payload of an incoming request, may actually need to do a\nlot of processing.\nContent-aware request distribution has several advantages. For example, if\nthe switch always forwards requests for the same \ufb01le to the same server, that\nserver may be able to effectively cache the \ufb01le resulting in higher response times.\nLikewise, we may decide to distribute a set of \ufb01les to speci\ufb01c servers, making a\ndistinction, for example, between streaming media \ufb01les, images, text \ufb01les, but\nalso perhaps actual databases. Being able to make such a distinction will allow to\ninstall dedicated servers in the second tier.\nIdeally, the switch would be as ef\ufb01cient as a normal transport-level switch, yet\nhave the functionality for content-aware distribution. Aron et al. [2000] propose a\nscheme by which the work for inspecting the payload of an incoming request is\ndistributed across several servers, and combine this distribution with transport-\nlevel switching. The switch now has two tasks. First, when a request initially\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n144 CHAPTER 3. PROCESSES\nis expecting an answer back from the switch, not from some arbitrary server\nit has never heard of before. Clearly, a TCP-handoff implementation requires\noperating-system level modi\ufb01cations. TCP handoff is especially effective when\nresponses are much larger than requests, as in the case of Web servers.\nIt can already be seen that the switch can play an important role in dis-\ntributing the load among the various servers. By deciding where to forward a\nrequest to, the switch also decides which server is to handle further processing\nof the request. The simplest load-balancing policy that the switch can follow\nis round robin: each time it picks the next server from its list to forward a\nrequest to. Of course, the switch will have to keep track to which server it\nhanded off a TCP connection, at least until that connection is torn down. As\nit turns out, maintaining this state and handing off subsequent TCP segments\nbelonging to the same TCP connection, may actually slow down the switch.\nMore advanced server selection criteria can be deployed as well. For\nexample, assume multiple services are offered by the server cluster. If the\nswitch can distinguish those services when a request comes in, it can then take\ninformed decisions on where to forward the request to. This server selection\ncan still take place at the transport level, provided services are distinguished\nby means of a port number. In the case of transport-level switches , as we have\ndiscussed so far, decisions on where to forward an incoming request is based\non transport-level information only. One step further is to have the switch\nactually inspect the payload of the incoming request. This content-aware\nrequest distribution can be applied only if it is known what that payload\nlooks like. For example, in the case of Web servers, the switch can expect an\nHTTP request, based on which it can then decide who is to process it.\nNote 3.8 (Advanced: Ef\ufb01cient content-aware request distribution)\nObviously, the design of the switch is crucial, as it can easily become a bottleneck\nwith all traf\ufb01c passing through it. Transport-level switches are generally very\nef\ufb01cient. However, content-aware switches operating at the level of the application\nlayer that inspect the payload of an incoming request, may actually need to do a\nlot of processing.\nContent-aware request distribution has several advantages. For example, if\nthe switch always forwards requests for the same \ufb01le to the same server, that\nserver may be able to effectively cache the \ufb01le resulting in higher response times.\nLikewise, we may decide to distribute a set of \ufb01les to speci\ufb01c servers, making a\ndistinction, for example, between streaming media \ufb01les, images, text \ufb01les, but\nalso perhaps actual databases. Being able to make such a distinction will allow to\ninstall dedicated servers in the second tier.\nIdeally, the switch would be as ef\ufb01cient as a normal transport-level switch, yet\nhave the functionality for content-aware distribution. Aron et al. [2000] propose a\nscheme by which the work for inspecting the payload of an incoming request is\ndistributed across several servers, and combine this distribution with transport-\nlevel switching. The switch now has two tasks. First, when a request initially\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 145\ncomes in, it must decide which server will handle the rest of the communication\nwith the client. Second, the switch should forward the client\u2019s TCP messages\nassociated with the handed-off TCP connection.\nFigure 3.20: Content-aware request distribution deploying TCP handoff.\nThese two tasks can be distributed as shown in Figure 3.20. When a request\ncomes in, the \ufb01rst thing the switch does is pass it on to an arbitrary distributor,\nwhich is running on one of the servers. The distributor\u2019s main job is to manage\nthe TCP handoff. In turn, the distributor passes the request to a dispatcher, which\ninspects the payload to decide on the best application server. Once that server has\nbeen selected, the initial distributor will do the administration to hand off the TCP\nconnection: it communicates with the selected server to accept the connection,\nand eventually informs the switch to which server it should subsequently forward\nthe associated TCP segments.\nWide-area clusters\nA characteristic feature of local-area server clusters is that they are owned\nby a single organization. Deploying clusters across a wide-area network\nhas traditionally been quite cumbersome as one had to generally deal with\nmultiple administrative organizations such as ISPs (Internet Service Providers).\nWith the advent of cloud computing, matters have changed and we are now\nwitnessing an increase of wide-area distributed systems in which servers (or\nserver clusters) are spread across the Internet. The problems related to having\nto deal with multiple organizations are effectively circumvented by making\nuse of the facilities of a single cloud provider.\nCloud providers like Amazon and Google manage several data centers\nplaced at different locations worldwide. As such, they can offer an end user\nthe ability to build a wide-area distributed system consisting of a potentially\nlarge collection of networked virtual machines, scattered across the Internet.\nAn important reason for wanting such distributed systems is to provide\nlocality: offering data and services that are close to clients. An example where\nsuch locality is important is streaming media: the closer a video server is\nlocated to a client, the easier it becomes to provide high-quality streams.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 145\ncomes in, it must decide which server will handle the rest of the communication\nwith the client. Second, the switch should forward the client\u2019s TCP messages\nassociated with the handed-off TCP connection.\nFigure 3.20: Content-aware request distribution deploying TCP handoff.\nThese two tasks can be distributed as shown in Figure 3.20. When a request\ncomes in, the \ufb01rst thing the switch does is pass it on to an arbitrary distributor,\nwhich is running on one of the servers. The distributor\u2019s main job is to manage\nthe TCP handoff. In turn, the distributor passes the request to a dispatcher, which\ninspects the payload to decide on the best application server. Once that server has\nbeen selected, the initial distributor will do the administration to hand off the TCP\nconnection: it communicates with the selected server to accept the connection,\nand eventually informs the switch to which server it should subsequently forward\nthe associated TCP segments.\nWide-area clusters\nA characteristic feature of local-area server clusters is that they are owned\nby a single organization. Deploying clusters across a wide-area network\nhas traditionally been quite cumbersome as one had to generally deal with\nmultiple administrative organizations such as ISPs (Internet Service Providers).\nWith the advent of cloud computing, matters have changed and we are now\nwitnessing an increase of wide-area distributed systems in which servers (or\nserver clusters) are spread across the Internet. The problems related to having\nto deal with multiple organizations are effectively circumvented by making\nuse of the facilities of a single cloud provider.\nCloud providers like Amazon and Google manage several data centers\nplaced at different locations worldwide. As such, they can offer an end user\nthe ability to build a wide-area distributed system consisting of a potentially\nlarge collection of networked virtual machines, scattered across the Internet.\nAn important reason for wanting such distributed systems is to provide\nlocality: offering data and services that are close to clients. An example where\nsuch locality is important is streaming media: the closer a video server is\nlocated to a client, the easier it becomes to provide high-quality streams.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "146 CHAPTER 3. PROCESSES\nNote that if wide-area locality is not critical, it may suf\ufb01ce, or even be\nbetter, to place virtual machines in a single data center, so that interprocess\ncommunication can bene\ufb01t from low-latency local networks. The price to pay\nmay be higher latencies between clients and the service running in a remote\ndata center.\nRequest dispatching If wide-area locality is an issue, then request dispatch-\ning becomes important: if a client accesses a service, its request should be\nforwarded to a nearby server, that is, a server that will allow communication\nwith that client to be fast. Deciding which server should handle the client\u2019s\nrequest is an issue of redirection policy [Sivasubramanian et al., 2004b]. If we\nassume that a client will initially contact a request dispatcher analogous to\nthe switch in our discussion of local-area clusters, then that dispatcher will\nhave to estimate the latency between the client and several servers. How such\nan estimation can be made is discussed in Section 6.5.\nOnce a server has been selected, the dispatcher will have to inform the\nclient. Several redirection mechanisms are possible. A popular one is when\nthe dispatcher is actually a DNS name server. Internet or Web-based services\nare often looked up in the Domain Name System (DNS ). A client provides\na domain name such as service .organization .orgto a local DNS server, which\neventually returns an IP address of the associated service, possibly after\nhaving contacted other DNS servers. When sending its request to look up a\nname, a client also sends its own IP address (DNS requests are sent as UDP\npackets). In other words, the DNS server will also know the client\u2019s IP address\nwhich it can then subsequently use to select the best server for that client, and\nreturning a close-by IP address.\nUnfortunately, this scheme is not perfect for two reasons. First, rather than\nsending the client\u2019s IP address, what happens is that the local DNS server that\nis contacted by the client acts as a proxy for that client. In other words, not\nthe client\u2019s IP address, but that of the local DNS server is used to identify the\nlocation of the client. Mao et al. [2002] have shown that there may be a huge\nadditional communication cost, as the local DNS server is often not that local.\nSecondly, depending on the scheme that is used for resolving a domain\nname, it may even be the case that the address of the local DNS server is not\neven being used. Instead, it may happen that the DNS server that is deciding\non which IP address to return, may be fooled by the fact that the requester\nis yet another DNS server acting as an intermediate between the original\nclient and the deciding DNS server. In those case, locality awareness has been\ncompletely lost.\nDespite that DNS-based redirection may not always be very accurate, it\nis widely deployed if only for the fact that it is relatively easy to implement\nand also transparent to the client. In addition, there is no need to rely on\nlocation-aware client-side software.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n146 CHAPTER 3. PROCESSES\nNote that if wide-area locality is not critical, it may suf\ufb01ce, or even be\nbetter, to place virtual machines in a single data center, so that interprocess\ncommunication can bene\ufb01t from low-latency local networks. The price to pay\nmay be higher latencies between clients and the service running in a remote\ndata center.\nRequest dispatching If wide-area locality is an issue, then request dispatch-\ning becomes important: if a client accesses a service, its request should be\nforwarded to a nearby server, that is, a server that will allow communication\nwith that client to be fast. Deciding which server should handle the client\u2019s\nrequest is an issue of redirection policy [Sivasubramanian et al., 2004b]. If we\nassume that a client will initially contact a request dispatcher analogous to\nthe switch in our discussion of local-area clusters, then that dispatcher will\nhave to estimate the latency between the client and several servers. How such\nan estimation can be made is discussed in Section 6.5.\nOnce a server has been selected, the dispatcher will have to inform the\nclient. Several redirection mechanisms are possible. A popular one is when\nthe dispatcher is actually a DNS name server. Internet or Web-based services\nare often looked up in the Domain Name System (DNS ). A client provides\na domain name such as service .organization .orgto a local DNS server, which\neventually returns an IP address of the associated service, possibly after\nhaving contacted other DNS servers. When sending its request to look up a\nname, a client also sends its own IP address (DNS requests are sent as UDP\npackets). In other words, the DNS server will also know the client\u2019s IP address\nwhich it can then subsequently use to select the best server for that client, and\nreturning a close-by IP address.\nUnfortunately, this scheme is not perfect for two reasons. First, rather than\nsending the client\u2019s IP address, what happens is that the local DNS server that\nis contacted by the client acts as a proxy for that client. In other words, not\nthe client\u2019s IP address, but that of the local DNS server is used to identify the\nlocation of the client. Mao et al. [2002] have shown that there may be a huge\nadditional communication cost, as the local DNS server is often not that local.\nSecondly, depending on the scheme that is used for resolving a domain\nname, it may even be the case that the address of the local DNS server is not\neven being used. Instead, it may happen that the DNS server that is deciding\non which IP address to return, may be fooled by the fact that the requester\nis yet another DNS server acting as an intermediate between the original\nclient and the deciding DNS server. In those case, locality awareness has been\ncompletely lost.\nDespite that DNS-based redirection may not always be very accurate, it\nis widely deployed if only for the fact that it is relatively easy to implement\nand also transparent to the client. In addition, there is no need to rely on\nlocation-aware client-side software.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 147\nNote 3.9 (Advanced: An alternative for organizing wide-area server clusters)\nAs we mentioned, most server clusters offer a single access point. When that point\nfails, the cluster becomes unavailable. To eliminate this potential problem, several\naccess points can be provided, of which the addresses are made publicly available.\nThe Domain Name System (DNS ) can return several addresses, all belonging\nto the same host name, using a simple round-robin strategy. This approach still\nrequires clients to make several attempts if one of the addresses fails. Moreover,\nthis does not solve the problem of requiring static access points.\nHaving stability, like a long-living access point, is a desirable feature from a\nclient\u2019s and a server\u2019s perspective. On the other hand, it is also desirable to have\na high degree of \ufb02exibility in con\ufb01guring a server cluster, including the switch.\nThis observation has lead to a design of a distributed server which effectively is\nnothing but a possibly dynamically changing set of machines, with also possibly\nvarying access points, but which nevertheless appears to the outside world as a\nsingle, powerful machine. Szymaniak et al. [2007] provide the design of such a\ndistributed server. We describe it brie\ufb02y here.\nThe basic idea behind a distributed server is that clients bene\ufb01t from a robust,\nhigh-performing, stable server. These properties can often be provided by high-\nend mainframes, of which some have an acclaimed mean time between failure of\nmore than 40 years. However, by grouping simpler machines transparently into a\ncluster, and not relying on the availability of a single machine, it may be possible\nto achieve a better degree of stability than by each component individually. For\nexample, such a cluster could be dynamically con\ufb01gured from end-user machines,\nas in the case of a collaborative distributed system. Note also that many data\ncenters are making use of relatively cheap and simple machines [Barroso and\nH\u00f6lze, 2009].\nLet us concentrate on how a stable access point can be achieved in such a\nsystem. The main idea is to make use of available networking services, notably\nmobility support for IP version 6 (MIPv6). In MIPv6, a mobile node is assumed\nto have a home network where it normally resides and for which it has an\nassociated stable address, known as its home address (HoA ). This home network\nhas a special router attached, known as the home agent , which will take care\nof traf\ufb01c to the mobile node when it is away. To this end, when a mobile node\nattaches to a foreign network, it will receive a temporary care-of address (CoA )\nwhere it can be reached. This care-of address is reported to the node\u2019s home\nagent who will then see to it that all traf\ufb01c is forwarded to the mobile node. Note\nthat applications communicating with the mobile node will see only the address\nassociated with the node\u2019s home network. They will never see the care-of address.\nThis principle can be used to offer a stable address of a distributed server.\nIn this case, a single unique contact address is initially assigned to the server\ncluster. The contact address will be the server\u2019s life-time address to be used in all\ncommunication with the outside world. At any time, one node in the distributed\nserver will operate as an access point using that contact address, but this role\ncan easily be taken over by another node. What happens is that the access point\nrecords its own address as the care-of address at the home agent associated with\nthe distributed server. At that point, all traf\ufb01c will be directed to the access point,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 147\nNote 3.9 (Advanced: An alternative for organizing wide-area server clusters)\nAs we mentioned, most server clusters offer a single access point. When that point\nfails, the cluster becomes unavailable. To eliminate this potential problem, several\naccess points can be provided, of which the addresses are made publicly available.\nThe Domain Name System (DNS ) can return several addresses, all belonging\nto the same host name, using a simple round-robin strategy. This approach still\nrequires clients to make several attempts if one of the addresses fails. Moreover,\nthis does not solve the problem of requiring static access points.\nHaving stability, like a long-living access point, is a desirable feature from a\nclient\u2019s and a server\u2019s perspective. On the other hand, it is also desirable to have\na high degree of \ufb02exibility in con\ufb01guring a server cluster, including the switch.\nThis observation has lead to a design of a distributed server which effectively is\nnothing but a possibly dynamically changing set of machines, with also possibly\nvarying access points, but which nevertheless appears to the outside world as a\nsingle, powerful machine. Szymaniak et al. [2007] provide the design of such a\ndistributed server. We describe it brie\ufb02y here.\nThe basic idea behind a distributed server is that clients bene\ufb01t from a robust,\nhigh-performing, stable server. These properties can often be provided by high-\nend mainframes, of which some have an acclaimed mean time between failure of\nmore than 40 years. However, by grouping simpler machines transparently into a\ncluster, and not relying on the availability of a single machine, it may be possible\nto achieve a better degree of stability than by each component individually. For\nexample, such a cluster could be dynamically con\ufb01gured from end-user machines,\nas in the case of a collaborative distributed system. Note also that many data\ncenters are making use of relatively cheap and simple machines [Barroso and\nH\u00f6lze, 2009].\nLet us concentrate on how a stable access point can be achieved in such a\nsystem. The main idea is to make use of available networking services, notably\nmobility support for IP version 6 (MIPv6). In MIPv6, a mobile node is assumed\nto have a home network where it normally resides and for which it has an\nassociated stable address, known as its home address (HoA ). This home network\nhas a special router attached, known as the home agent , which will take care\nof traf\ufb01c to the mobile node when it is away. To this end, when a mobile node\nattaches to a foreign network, it will receive a temporary care-of address (CoA )\nwhere it can be reached. This care-of address is reported to the node\u2019s home\nagent who will then see to it that all traf\ufb01c is forwarded to the mobile node. Note\nthat applications communicating with the mobile node will see only the address\nassociated with the node\u2019s home network. They will never see the care-of address.\nThis principle can be used to offer a stable address of a distributed server.\nIn this case, a single unique contact address is initially assigned to the server\ncluster. The contact address will be the server\u2019s life-time address to be used in all\ncommunication with the outside world. At any time, one node in the distributed\nserver will operate as an access point using that contact address, but this role\ncan easily be taken over by another node. What happens is that the access point\nrecords its own address as the care-of address at the home agent associated with\nthe distributed server. At that point, all traf\ufb01c will be directed to the access point,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "148 CHAPTER 3. PROCESSES\nwho will then take care of distributing requests among the currently participating\nnodes. If the access point fails, a simple fail-over mechanism comes into place by\nwhich another access point reports a new care-of address.\nThis simple con\ufb01guration would make the home agent as well as the access\npoint a potential bottleneck as all traf\ufb01c would \ufb02ow through these two machines.\nThis situation can be avoided by using an MIPv6 feature known as route opti-\nmization . Route optimization works as follows. Whenever a mobile node with\nhome address HAreports its current care-of address, say CA, the home agent can\nforward CAto a client. The latter will then locally store the pair (HA,CA). From\nthat moment on, communication will be directly forwarded to CA. Although\nthe application at the client side can still use the home address, the underlying\nsupport software for MIPv6 will translate that address to CAand use that instead.\nFigure 3.21: Route optimization in a distributed server.\nRoute optimization can be used to make different clients believe they are\ncommunicating with a single server, where, in fact, each client is communicating\nwith a different member node of the distributed server, as shown in Figure 3.21.\nTo this end, when an access point of a distributed server forwards a request from\nclient C1to, say node S1(with address CA1), it passes enough information to S1\nto let it initiate the route optimization procedure by which eventually the client\nis made to believe that the care-of address is CA1. This will allow C1to store\nthe pair (HA,CA1). During this procedure, the access point (as well as the home\nagent) tunnel most of the traf\ufb01c between C1andS1. This will prevent the home\nagent from believing that the care-of address has changed, so that it will continue\nto communicate with the access point.\nOf course, while this route optimization procedure is taking place, requests\nfrom other clients may still come in. These remain in a pending state at the\naccess point until they can be forwarded. The request from another client C2\nmay then be forwarded to member node S2(with address CA2), allowing the\nlatter to let client C2store the pair HA,CA2). As a result, different clients will be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n148 CHAPTER 3. PROCESSES\nwho will then take care of distributing requests among the currently participating\nnodes. If the access point fails, a simple fail-over mechanism comes into place by\nwhich another access point reports a new care-of address.\nThis simple con\ufb01guration would make the home agent as well as the access\npoint a potential bottleneck as all traf\ufb01c would \ufb02ow through these two machines.\nThis situation can be avoided by using an MIPv6 feature known as route opti-\nmization . Route optimization works as follows. Whenever a mobile node with\nhome address HAreports its current care-of address, say CA, the home agent can\nforward CAto a client. The latter will then locally store the pair (HA,CA). From\nthat moment on, communication will be directly forwarded to CA. Although\nthe application at the client side can still use the home address, the underlying\nsupport software for MIPv6 will translate that address to CAand use that instead.\nFigure 3.21: Route optimization in a distributed server.\nRoute optimization can be used to make different clients believe they are\ncommunicating with a single server, where, in fact, each client is communicating\nwith a different member node of the distributed server, as shown in Figure 3.21.\nTo this end, when an access point of a distributed server forwards a request from\nclient C1to, say node S1(with address CA1), it passes enough information to S1\nto let it initiate the route optimization procedure by which eventually the client\nis made to believe that the care-of address is CA1. This will allow C1to store\nthe pair (HA,CA1). During this procedure, the access point (as well as the home\nagent) tunnel most of the traf\ufb01c between C1andS1. This will prevent the home\nagent from believing that the care-of address has changed, so that it will continue\nto communicate with the access point.\nOf course, while this route optimization procedure is taking place, requests\nfrom other clients may still come in. These remain in a pending state at the\naccess point until they can be forwarded. The request from another client C2\nmay then be forwarded to member node S2(with address CA2), allowing the\nlatter to let client C2store the pair HA,CA2). As a result, different clients will be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 149\ndirectly communicating with different members of the distributed server, where\neach client application still has the illusion that this server has address HA. The\nhome agent continues to communicate with the access point talking to the contact\naddress.\nCase study: PlanetLab\nLet us now take a closer look at a somewhat unusual cluster server. PlanetLab\nis a collaborative distributed system in which different organizations each\ndonate one or more computers, adding up to a total of hundreds of nodes.\nTogether, these computers form a 1-tier server cluster, where access, processing,\nand storage can all take place on each node individually. Management of\nPlanetLab is by necessity almost entirely distributed.\nGeneral organization In PlanetLab, a participating organization donates\none or more nodes (i.e., computers) that are subsequently shared among all\nPlanetLab users. Each node is organized as shown in Figure 3.22. There\nare two important components [Bavier et al., 2004; Peterson et al., 2006].\nThe \ufb01rst one is the virtual machine monitor (VMM), which is an enhanced\nLinux operating system. The enhancements mainly comprise adjustments\nfor supporting the second component, namely (Linux) Vservers . For now, a\nVserver can best be thought of as a separate environment in which a group of\nprocesses run. We return to Vservers below.\nFigure 3.22: The basic organization of a PlanetLab node.\nThe Linux VMM ensures that Vservers are separated: processes in different\nVservers are executed concurrently and independently, each making use only\nof the software packages and programs available in their own environment.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 149\ndirectly communicating with different members of the distributed server, where\neach client application still has the illusion that this server has address HA. The\nhome agent continues to communicate with the access point talking to the contact\naddress.\nCase study: PlanetLab\nLet us now take a closer look at a somewhat unusual cluster server. PlanetLab\nis a collaborative distributed system in which different organizations each\ndonate one or more computers, adding up to a total of hundreds of nodes.\nTogether, these computers form a 1-tier server cluster, where access, processing,\nand storage can all take place on each node individually. Management of\nPlanetLab is by necessity almost entirely distributed.\nGeneral organization In PlanetLab, a participating organization donates\none or more nodes (i.e., computers) that are subsequently shared among all\nPlanetLab users. Each node is organized as shown in Figure 3.22. There\nare two important components [Bavier et al., 2004; Peterson et al., 2006].\nThe \ufb01rst one is the virtual machine monitor (VMM), which is an enhanced\nLinux operating system. The enhancements mainly comprise adjustments\nfor supporting the second component, namely (Linux) Vservers . For now, a\nVserver can best be thought of as a separate environment in which a group of\nprocesses run. We return to Vservers below.\nFigure 3.22: The basic organization of a PlanetLab node.\nThe Linux VMM ensures that Vservers are separated: processes in different\nVservers are executed concurrently and independently, each making use only\nof the software packages and programs available in their own environment.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "150 CHAPTER 3. PROCESSES\nThe isolation between processes in different Vservers is strict. For example,\ntwo processes in different Vservers may have the same user ID, but this does\nnot imply that they stem from the same user. This separation considerably\neases supporting users from different organizations that want to use PlanetLab\nas, for example, a testbed to experiment with completely different distributed\nsystems and applications.\nTo support such experimentations, PlanetLab uses slices , each slice being\na set of Vservers, each Vserver running on a different node, as illustrated\nin Figure 3.23. A slice can thus be thought of as a virtual server cluster,\nimplemented by means of a collection of virtual machines.\nFigure 3.23: The principle of a PlanetLab slice, showing sets of associated\nVservers across different nodes.\nCentral to managing PlanetLab resources is the node manager . Each node\nhas such a manager, implemented by means of a separate Vserver, whose only\ntask is to create other Vservers on the node it manages and to control resource\nallocation. To create a new slice, each node will also run a slice creation\nservice (SCS ), which, in turn, can contact the node manager requesting it to\ncreate a Vserver and to allocate resources. The node manager itself cannot\nbe contacted directly over a network, allowing it to concentrate only on local\nresource management. In turn, the SCS will not accept slice-creation requests\nfrom just anybody. Only speci\ufb01c slice authorities are eligible for requesting\nthe creation of a slice. Each slice authority will have access rights to a collection\nof nodes. The simplest model is that there is only a single, centralized slice\nauthority that is allowed to request slice creation on all nodes. In practice, we\nsee that this slice authority is the one used to get a user up-and-running on\nPlanetLab.\nKeeping track of resources is done by means of a resource speci\ufb01cation,\norrspec for short. An rspec speci\ufb01es a time interval during which certain\nresources have been allocated. Resources include disk space, \ufb01le descriptors,\ninbound and outbound network bandwidth, transport-level end points, main\nmemory, and CPU usage. An rspec is identi\ufb01ed through a globally unique\n128-bit identi\ufb01er known as a resource capability ( rcap). Given an rcap, the\nnode manager can look up the associated rspec in a local table.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n150 CHAPTER 3. PROCESSES\nThe isolation between processes in different Vservers is strict. For example,\ntwo processes in different Vservers may have the same user ID, but this does\nnot imply that they stem from the same user. This separation considerably\neases supporting users from different organizations that want to use PlanetLab\nas, for example, a testbed to experiment with completely different distributed\nsystems and applications.\nTo support such experimentations, PlanetLab uses slices , each slice being\na set of Vservers, each Vserver running on a different node, as illustrated\nin Figure 3.23. A slice can thus be thought of as a virtual server cluster,\nimplemented by means of a collection of virtual machines.\nFigure 3.23: The principle of a PlanetLab slice, showing sets of associated\nVservers across different nodes.\nCentral to managing PlanetLab resources is the node manager . Each node\nhas such a manager, implemented by means of a separate Vserver, whose only\ntask is to create other Vservers on the node it manages and to control resource\nallocation. To create a new slice, each node will also run a slice creation\nservice (SCS ), which, in turn, can contact the node manager requesting it to\ncreate a Vserver and to allocate resources. The node manager itself cannot\nbe contacted directly over a network, allowing it to concentrate only on local\nresource management. In turn, the SCS will not accept slice-creation requests\nfrom just anybody. Only speci\ufb01c slice authorities are eligible for requesting\nthe creation of a slice. Each slice authority will have access rights to a collection\nof nodes. The simplest model is that there is only a single, centralized slice\nauthority that is allowed to request slice creation on all nodes. In practice, we\nsee that this slice authority is the one used to get a user up-and-running on\nPlanetLab.\nKeeping track of resources is done by means of a resource speci\ufb01cation,\norrspec for short. An rspec speci\ufb01es a time interval during which certain\nresources have been allocated. Resources include disk space, \ufb01le descriptors,\ninbound and outbound network bandwidth, transport-level end points, main\nmemory, and CPU usage. An rspec is identi\ufb01ed through a globally unique\n128-bit identi\ufb01er known as a resource capability ( rcap). Given an rcap, the\nnode manager can look up the associated rspec in a local table.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.4. SERVERS 151\nResources are bound to slices. In other words, in order to make use of\nresources, it is necessary to create a slice. Each slice is associated with a\nservice provider , which can best be seen as an entity having an account\non PlanetLab. Every slice can then be identi\ufb01ed by a ( principal _id,slice_tag)\npair, where the principal _ididenti\ufb01es the provider and slice_tagis an identi\ufb01er\nchosen by the provider.\nVservers Let us now turn our attention to PlanetLab\u2019s Vservers, which have\nbeen described and evaluated by Soltesz et al. [2007]. A Vserver is organized\naccording to what is called a container-based approach . The main difference\nwith traditional virtual machines as those discussed in Section 3.2, is that they\nrely on a single, shared operating-system kernel. This also means that resource\nmanagement is mostly done only by the underlying operating system and not\nby any of the Vservers. The primary task of a Vserver is therefore to merely\nsupport a group of processes and keep that group isolated from processes\nrunning under the jurisdiction of another Vserver. Indeed, a Vserver forms an\nisolated container for a group of processes and their allocated resources. By\nnow, containers have become quite popular for offering cloud services.\nThis isolation is technically established by the underlying VMM, for which\npurpose the Linux operating system has been adapted. One of the most\nsalient adaptations concerns the separation of independent name spaces. For\nexample, to create the illusion that a Vserver is really a single machine, the\nUnix process inittraditionally always gets process ID 1 (with its parent having\nID 0). Obviously, the Linux VMM will already have such a process running,\nyet it needs to create another initprocess for every Vserver. Those processes\nwill each also get process ID 1, while the kernel keeps track of a mapping\nbetween such a virtual process ID and the real, actually assigned process ID.\nOther examples of consistent naming across Vservers easily come to mind.\nLikewise, isolation is also established by providing each Vserver with\nits own set of libraries, yet using the directory structure that those libraries\nexpect, that is, with directories named /dev,/home ,/proc,/usr, and so on\n(which is also depicted in Figure 3.22). In principle, such a separated name\nspace can be achieved using the standard chroot command, effectively giving\neach Vserver its own root directory. However, special measures are needed at\nkernel level to prevent unauthorized access of one Vserver to the directory\ntree of another Vserver, as explained by Soltesz et al. [2007].\nAn important advantage of the container-based approach toward virtual-\nization in comparison to running separate guest operating systems, is that\nresource allocation can generally be much simpler. In particular, it is possible\nto overbook resources by allowing for dynamic resource allocation, just as is\ndone with allocating resources to normal processes. Normally, when using\na guest operating system, the guest will have to be allocated a \ufb01xed amount\nof resources in advance (notably main memory). When considering that the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.4. SERVERS 151\nResources are bound to slices. In other words, in order to make use of\nresources, it is necessary to create a slice. Each slice is associated with a\nservice provider , which can best be seen as an entity having an account\non PlanetLab. Every slice can then be identi\ufb01ed by a ( principal _id,slice_tag)\npair, where the principal _ididenti\ufb01es the provider and slice_tagis an identi\ufb01er\nchosen by the provider.\nVservers Let us now turn our attention to PlanetLab\u2019s Vservers, which have\nbeen described and evaluated by Soltesz et al. [2007]. A Vserver is organized\naccording to what is called a container-based approach . The main difference\nwith traditional virtual machines as those discussed in Section 3.2, is that they\nrely on a single, shared operating-system kernel. This also means that resource\nmanagement is mostly done only by the underlying operating system and not\nby any of the Vservers. The primary task of a Vserver is therefore to merely\nsupport a group of processes and keep that group isolated from processes\nrunning under the jurisdiction of another Vserver. Indeed, a Vserver forms an\nisolated container for a group of processes and their allocated resources. By\nnow, containers have become quite popular for offering cloud services.\nThis isolation is technically established by the underlying VMM, for which\npurpose the Linux operating system has been adapted. One of the most\nsalient adaptations concerns the separation of independent name spaces. For\nexample, to create the illusion that a Vserver is really a single machine, the\nUnix process inittraditionally always gets process ID 1 (with its parent having\nID 0). Obviously, the Linux VMM will already have such a process running,\nyet it needs to create another initprocess for every Vserver. Those processes\nwill each also get process ID 1, while the kernel keeps track of a mapping\nbetween such a virtual process ID and the real, actually assigned process ID.\nOther examples of consistent naming across Vservers easily come to mind.\nLikewise, isolation is also established by providing each Vserver with\nits own set of libraries, yet using the directory structure that those libraries\nexpect, that is, with directories named /dev,/home ,/proc,/usr, and so on\n(which is also depicted in Figure 3.22). In principle, such a separated name\nspace can be achieved using the standard chroot command, effectively giving\neach Vserver its own root directory. However, special measures are needed at\nkernel level to prevent unauthorized access of one Vserver to the directory\ntree of another Vserver, as explained by Soltesz et al. [2007].\nAn important advantage of the container-based approach toward virtual-\nization in comparison to running separate guest operating systems, is that\nresource allocation can generally be much simpler. In particular, it is possible\nto overbook resources by allowing for dynamic resource allocation, just as is\ndone with allocating resources to normal processes. Normally, when using\na guest operating system, the guest will have to be allocated a \ufb01xed amount\nof resources in advance (notably main memory). When considering that the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "152 CHAPTER 3. PROCESSES\nnodes provided by participating PlanetLab organizations are required to have\nonly few GByte of main memory, it is not hard to imagine that memory\nmay be a scarce resource. It is therefore necessary to dynamically allocate\nmemory to allow tens of virtual machines to be running at the same time\non a single node. Vservers are ideal for this type of resource management;\noperating systems are much harder to support in such cases. Of course, this\ncannot prevent a Vserver from using too much memory on a busy node. The\nPlanetLab policy in that case is simple: the Vserver hogging memory when\nswap space is almost \ufb01lled, is reset.\n3.5 Code migration\nSo far, we have been mainly concerned with distributed systems in which\ncommunication is limited to passing data. However, there are situations\nin which passing programs, sometimes even while they are being executed,\nsimpli\ufb01es the design of a distributed system. In this section, we take a detailed\nlook at what code migration actually is. We start by considering different\napproaches to code migration, followed by a discussion on how to deal with\nthe local resources that a migrating program uses. A particularly hard problem\nis migrating code in heterogeneous systems, which is also discussed.\nReasons for migrating code\nTraditionally, code migration in distributed systems took place in the form\nofprocess migration in which an entire process was moved from one node\nto another [Milojicic et al., 2000]. Moving a running process to a different\nmachine is a costly and intricate task, and there had better be a good reason\nfor doing so. That reason has always been performance. The basic idea is\nthat overall system performance can be improved if processes are moved from\nheavily loaded to lightly loaded machines. Load is often expressed in terms\nof the CPU queue length or CPU utilization, but other performance indicators\nare used as well. When completing their survey, Milojicic et al. had already\ncome to the conclusion that process migration was no longer a viable option\nfor improving distributed systems.\nHowever, instead of of\ufb02oading machines, we can now witness that code\nis moved to make sure that a machine is suf\ufb01ciently loaded. In particular,\nmigrating complete virtual machines with their suite of applications to lightly\nloaded machines in order to minimize the total number of nodes being used\nis common practice in optimizing energy usage in data centers. Interestingly\nenough, although migrating virtual machines may require more resources,\nthe task itself is far less intricate than migrating a process, as we discuss in\nNote 3.11.\nIn general, load-distribution algorithms by which decisions are made con-\ncerning the allocation and redistribution of tasks with respect to a set of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n152 CHAPTER 3. PROCESSES\nnodes provided by participating PlanetLab organizations are required to have\nonly few GByte of main memory, it is not hard to imagine that memory\nmay be a scarce resource. It is therefore necessary to dynamically allocate\nmemory to allow tens of virtual machines to be running at the same time\non a single node. Vservers are ideal for this type of resource management;\noperating systems are much harder to support in such cases. Of course, this\ncannot prevent a Vserver from using too much memory on a busy node. The\nPlanetLab policy in that case is simple: the Vserver hogging memory when\nswap space is almost \ufb01lled, is reset.\n3.5 Code migration\nSo far, we have been mainly concerned with distributed systems in which\ncommunication is limited to passing data. However, there are situations\nin which passing programs, sometimes even while they are being executed,\nsimpli\ufb01es the design of a distributed system. In this section, we take a detailed\nlook at what code migration actually is. We start by considering different\napproaches to code migration, followed by a discussion on how to deal with\nthe local resources that a migrating program uses. A particularly hard problem\nis migrating code in heterogeneous systems, which is also discussed.\nReasons for migrating code\nTraditionally, code migration in distributed systems took place in the form\nofprocess migration in which an entire process was moved from one node\nto another [Milojicic et al., 2000]. Moving a running process to a different\nmachine is a costly and intricate task, and there had better be a good reason\nfor doing so. That reason has always been performance. The basic idea is\nthat overall system performance can be improved if processes are moved from\nheavily loaded to lightly loaded machines. Load is often expressed in terms\nof the CPU queue length or CPU utilization, but other performance indicators\nare used as well. When completing their survey, Milojicic et al. had already\ncome to the conclusion that process migration was no longer a viable option\nfor improving distributed systems.\nHowever, instead of of\ufb02oading machines, we can now witness that code\nis moved to make sure that a machine is suf\ufb01ciently loaded. In particular,\nmigrating complete virtual machines with their suite of applications to lightly\nloaded machines in order to minimize the total number of nodes being used\nis common practice in optimizing energy usage in data centers. Interestingly\nenough, although migrating virtual machines may require more resources,\nthe task itself is far less intricate than migrating a process, as we discuss in\nNote 3.11.\nIn general, load-distribution algorithms by which decisions are made con-\ncerning the allocation and redistribution of tasks with respect to a set of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.5. CODE MIGRATION 153\nmachines, play an important role in compute-intensive systems. However, in\nmany modern distributed systems, optimizing computing capacity is less an\nissue than, for example, trying to minimize communication. Moreover, due to\nthe heterogeneity of the underlying platforms and computer networks, per-\nformance improvement through code migration is often based on qualitative\nreasoning instead of mathematical models.\nConsider, as an example, a client-server system in which the server man-\nages a huge database. If a client application needs to perform many database\noperations involving large quantities of data, it may be better to ship part\nof the client application to the server and send only the results across the\nnetwork. Otherwise, the network may be swamped with the transfer of data\nfrom the server to the client. In this case, code migration is based on the\nassumption that it generally makes sense to process data close to where those\ndata reside.\nThis same reason can be used for migrating parts of the server to the client.\nFor example, in many interactive database applications, clients need to \ufb01ll in\nforms that are subsequently translated into a series of database operations.\nProcessing the form at the client side, and sending only the completed form\nto the server, can sometimes avoid that a relatively large number of small\nmessages need to cross the network. The result is that the client perceives\nbetter performance, while at the same time the server spends less time on\nform processing and communication. In the case of smartphones, moving\ncode to be executed at the handheld instead of the server may be the only\nviable solution to obtain acceptable performance, both for the client and the\nserver (see Kumar et al. [2013] for a survey on of\ufb02oading computations).\nSupport for code migration can also help improve performance by ex-\nploiting parallelism, but without the usual intricacies related to parallel pro-\ngramming. A typical example is searching for information in the Web. It is\nrelatively simple to implement a search query in the form of a small mobile\nprogram, called a mobile agent , that moves from site to site. By making\nseveral copies of such a program, and sending each off to different sites, we\nmay be able to achieve a linear speed-up compared to using just a single\nprogram instance. However, Carzaniga et al. [2007] conclude that mobile\nagents have never become successful because they did not really offer an\nobvious advantage over other technologies. Moreover, and crucial, it turned\nout to be virtually impossible to let this type of mobile code operate in a\nsecure way.\nBesides improving performance, there are other reasons for supporting\ncode migration as well. The most important one is that of \ufb02exibility. The\ntraditional approach to building distributed applications is to partition the\napplication into different parts, and decide in advance where each part should\nbe executed. This approach, for example, has lead to different multitiered\nclient-server applications discussed in Section 2.3.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.5. CODE MIGRATION 153\nmachines, play an important role in compute-intensive systems. However, in\nmany modern distributed systems, optimizing computing capacity is less an\nissue than, for example, trying to minimize communication. Moreover, due to\nthe heterogeneity of the underlying platforms and computer networks, per-\nformance improvement through code migration is often based on qualitative\nreasoning instead of mathematical models.\nConsider, as an example, a client-server system in which the server man-\nages a huge database. If a client application needs to perform many database\noperations involving large quantities of data, it may be better to ship part\nof the client application to the server and send only the results across the\nnetwork. Otherwise, the network may be swamped with the transfer of data\nfrom the server to the client. In this case, code migration is based on the\nassumption that it generally makes sense to process data close to where those\ndata reside.\nThis same reason can be used for migrating parts of the server to the client.\nFor example, in many interactive database applications, clients need to \ufb01ll in\nforms that are subsequently translated into a series of database operations.\nProcessing the form at the client side, and sending only the completed form\nto the server, can sometimes avoid that a relatively large number of small\nmessages need to cross the network. The result is that the client perceives\nbetter performance, while at the same time the server spends less time on\nform processing and communication. In the case of smartphones, moving\ncode to be executed at the handheld instead of the server may be the only\nviable solution to obtain acceptable performance, both for the client and the\nserver (see Kumar et al. [2013] for a survey on of\ufb02oading computations).\nSupport for code migration can also help improve performance by ex-\nploiting parallelism, but without the usual intricacies related to parallel pro-\ngramming. A typical example is searching for information in the Web. It is\nrelatively simple to implement a search query in the form of a small mobile\nprogram, called a mobile agent , that moves from site to site. By making\nseveral copies of such a program, and sending each off to different sites, we\nmay be able to achieve a linear speed-up compared to using just a single\nprogram instance. However, Carzaniga et al. [2007] conclude that mobile\nagents have never become successful because they did not really offer an\nobvious advantage over other technologies. Moreover, and crucial, it turned\nout to be virtually impossible to let this type of mobile code operate in a\nsecure way.\nBesides improving performance, there are other reasons for supporting\ncode migration as well. The most important one is that of \ufb02exibility. The\ntraditional approach to building distributed applications is to partition the\napplication into different parts, and decide in advance where each part should\nbe executed. This approach, for example, has lead to different multitiered\nclient-server applications discussed in Section 2.3.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "154 CHAPTER 3. PROCESSES\nHowever, if code can move between different machines, it becomes possible\nto dynamically con\ufb01gure distributed systems. For example, suppose a server\nimplements a standardized interface to a \ufb01le system. To allow remote clients\nto access the \ufb01le system, the server makes use of a proprietary protocol.\nNormally, the client-side implementation of the \ufb01le system interface, which is\nbased on that protocol, would need to be linked with the client application.\nThis approach requires that the software be readily available to the client at\nthe time the client application is being developed.\nAn alternative is to let the server provide the client\u2019s implementation no\nsooner than is strictly necessary, that is, when the client binds to the server.\nAt that point, the client dynamically downloads the implementation, goes\nthrough the necessary initialization steps, and subsequently invokes the server.\nThis principle is shown in Figure 3.24 (we note that the code repository is\ngenerally located as part of the server). This model of dynamically moving\ncode from a remote site does require that the protocol for downloading and\ninitializing code is standardized. Also, it is necessary that the downloaded\ncode can be executed on the client\u2019s machine. Typically, scripts that run in\na virtual machine embedded in, for example, a Web browser, will do the\ntrick. Arguably, this form of code migration has been key to the success of\nthe dynamic Web. These and other solutions are discussed below and in later\nchapters.\nFigure 3.24: The principle of dynamically con\ufb01guring a client to communicate\nwith a server.\nThe important advantage of this model of dynamically downloading client-\nside software is that clients need not have all the software preinstalled to talk\nto servers. Instead, the software can be moved in as necessary, and likewise,\ndiscarded when no longer needed. Another advantage is that as long as\ninterfaces are standardized, we can change the client-server protocol and its\nimplementation as often as we like. Changes will not affect existing client\napplications that rely on the server.\nThere are, of course, also disadvantages. The most serious one, which\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n154 CHAPTER 3. PROCESSES\nHowever, if code can move between different machines, it becomes possible\nto dynamically con\ufb01gure distributed systems. For example, suppose a server\nimplements a standardized interface to a \ufb01le system. To allow remote clients\nto access the \ufb01le system, the server makes use of a proprietary protocol.\nNormally, the client-side implementation of the \ufb01le system interface, which is\nbased on that protocol, would need to be linked with the client application.\nThis approach requires that the software be readily available to the client at\nthe time the client application is being developed.\nAn alternative is to let the server provide the client\u2019s implementation no\nsooner than is strictly necessary, that is, when the client binds to the server.\nAt that point, the client dynamically downloads the implementation, goes\nthrough the necessary initialization steps, and subsequently invokes the server.\nThis principle is shown in Figure 3.24 (we note that the code repository is\ngenerally located as part of the server). This model of dynamically moving\ncode from a remote site does require that the protocol for downloading and\ninitializing code is standardized. Also, it is necessary that the downloaded\ncode can be executed on the client\u2019s machine. Typically, scripts that run in\na virtual machine embedded in, for example, a Web browser, will do the\ntrick. Arguably, this form of code migration has been key to the success of\nthe dynamic Web. These and other solutions are discussed below and in later\nchapters.\nFigure 3.24: The principle of dynamically con\ufb01guring a client to communicate\nwith a server.\nThe important advantage of this model of dynamically downloading client-\nside software is that clients need not have all the software preinstalled to talk\nto servers. Instead, the software can be moved in as necessary, and likewise,\ndiscarded when no longer needed. Another advantage is that as long as\ninterfaces are standardized, we can change the client-server protocol and its\nimplementation as often as we like. Changes will not affect existing client\napplications that rely on the server.\nThere are, of course, also disadvantages. The most serious one, which\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.5. CODE MIGRATION 155\nwe discuss in Chapter 9, has to do with security. Blindly trusting that the\ndownloaded code implements only the advertised interface while accessing\nyour unprotected hard disk and does not send the juiciest parts to heaven-\nknows-who may not always be such a good idea. Fortunately, it is well\nunderstood how to protect the client against malicious, downloaded code.\nNote 3.10 (More information: Moving away from thin-client computing?)\nBy now, there is much more insight and expertise concerning transparent and\nsafe dynamic migration of code to clients. As a result, the trend that we described\nin Note 2.4 of moving toward thin-client computing because managing client-\nside software often turned out to be cumbersome, has been partly reverted.\nBy dynamically migrating client-side software, yet keeping the management of\nthat software entirely at the server side (or rather, at its owner), having \u201cricher\u201d\nclient-side software has become practically feasible.\nNote 3.11 (Advanced: Models for code migration)\nAlthough code migration suggests that we move only code between machines,\nthe term actually covers a much richer area. Traditionally, communication in\ndistributed systems is concerned with exchanging data between processes. Code\nmigration in the broadest sense deals with moving programs between machines,\nwith the intention to have those programs be executed at the target. In some cases,\nas in process migration, the execution status of a program, pending signals, and\nother parts of the environment must be moved as well.\nTo get a better understanding of the different models for code migration, we\nuse a framework proposed by Fuggetta et al. [1998]. In this framework, a process\nconsists of three segments. The code segment is the part that contains the set\nof instructions that make up the program that is being executed. The resource\nsegment contains references to external resources needed by the process, such as\n\ufb01les, printers, devices, other processes, and so on. Finally, an execution segment\nis used to store the current execution state of a process, consisting of private data,\nthe stack, and, of course, the program counter.\nA further distinction can be made between sender-initiated and receiver-\ninitiated migration. In sender-initiated migration, migration is initiated at the\nmachine where the code currently resides or is being executed. Typically, sender-\ninitiated migration is done when uploading programs to a compute server. An-\nother example is sending a query, or batch of queries, to a remote database server.\nInreceiver-initiated migration, the initiative for code migration is taken by the\ntarget machine. Java applets are an example of this approach.\nReceiver-initiated migration is simpler than sender-initiated migration. In\nmany cases, code migration occurs between a client and a server, where the client\ntakes the initiative for migration. Securely uploading code to a server, as is done\nin sender-initiated migration, often requires that the client has previously been\nregistered and authenticated at that server. In other words, the server is required\nto know all its clients, the reason being is that the client will presumably want\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.5. CODE MIGRATION 155\nwe discuss in Chapter 9, has to do with security. Blindly trusting that the\ndownloaded code implements only the advertised interface while accessing\nyour unprotected hard disk and does not send the juiciest parts to heaven-\nknows-who may not always be such a good idea. Fortunately, it is well\nunderstood how to protect the client against malicious, downloaded code.\nNote 3.10 (More information: Moving away from thin-client computing?)\nBy now, there is much more insight and expertise concerning transparent and\nsafe dynamic migration of code to clients. As a result, the trend that we described\nin Note 2.4 of moving toward thin-client computing because managing client-\nside software often turned out to be cumbersome, has been partly reverted.\nBy dynamically migrating client-side software, yet keeping the management of\nthat software entirely at the server side (or rather, at its owner), having \u201cricher\u201d\nclient-side software has become practically feasible.\nNote 3.11 (Advanced: Models for code migration)\nAlthough code migration suggests that we move only code between machines,\nthe term actually covers a much richer area. Traditionally, communication in\ndistributed systems is concerned with exchanging data between processes. Code\nmigration in the broadest sense deals with moving programs between machines,\nwith the intention to have those programs be executed at the target. In some cases,\nas in process migration, the execution status of a program, pending signals, and\nother parts of the environment must be moved as well.\nTo get a better understanding of the different models for code migration, we\nuse a framework proposed by Fuggetta et al. [1998]. In this framework, a process\nconsists of three segments. The code segment is the part that contains the set\nof instructions that make up the program that is being executed. The resource\nsegment contains references to external resources needed by the process, such as\n\ufb01les, printers, devices, other processes, and so on. Finally, an execution segment\nis used to store the current execution state of a process, consisting of private data,\nthe stack, and, of course, the program counter.\nA further distinction can be made between sender-initiated and receiver-\ninitiated migration. In sender-initiated migration, migration is initiated at the\nmachine where the code currently resides or is being executed. Typically, sender-\ninitiated migration is done when uploading programs to a compute server. An-\nother example is sending a query, or batch of queries, to a remote database server.\nInreceiver-initiated migration, the initiative for code migration is taken by the\ntarget machine. Java applets are an example of this approach.\nReceiver-initiated migration is simpler than sender-initiated migration. In\nmany cases, code migration occurs between a client and a server, where the client\ntakes the initiative for migration. Securely uploading code to a server, as is done\nin sender-initiated migration, often requires that the client has previously been\nregistered and authenticated at that server. In other words, the server is required\nto know all its clients, the reason being is that the client will presumably want\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "156 CHAPTER 3. PROCESSES\naccess to the server\u2019s resources such as its disk. Protecting such resources is\nessential. In contrast, downloading code as in the receiver-initiated case, can often\nbe done anonymously. Moreover, the server is generally not interested in the\nclient\u2019s resources. Instead, code migration to the client is done only for improving\nclient-side performance. To that end, only a limited number of resources need to\nbe protected, such as memory and network connections. We return to secure code\nmigration extensively in Chapter 9.\nBefore execution After execution\nClient Server Client Server\nCScode\nexec\nresourcecode\nexec*\nresource\nREVcode\n\u0000! exec\nresource\u0000!code\nexec*\nresource\nCoD exec\nresource \u0000code code\nexec*\nresource \u0000\nMAcode\nexec\nresource\u0000!\nresource resource\u0000!code\nexec*\nresource\nCS: Client-Server REV: Remote evaluation\nCoD: Code-on-demand MA: Mobile agents\nFigure 3.25: Four different paradigms for code mobility.\nThis brings us to four different paradigms for code mobility, as shown in\nFigure 3.25. Following Fuggetta et al. [1998], we make a distinction between sim-\npleclient-server computing ,remote evaluation ,code-on-demand , and mobile\nagents . Figure 3.25 shows the situation at respectively the client and the server,\nbefore and after execution of the mobile code.\nIn the case of client-server computing, the code, execution state, and resource\nsegment are all located at the server, and after execution, only the execution state\nat the server is generally modi\ufb01ed. This state modi\ufb01cation is denoted by means\nof an asterisk. With the sender-initiated remote evaluation , the client migrates\ncode to the server where that code is executed and leading to a modi\ufb01cation of\nthe execution state at the server. Code-on-demand is a receiver-initiated scheme\nby which the client obtains code from the server with its execution modifying\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n156 CHAPTER 3. PROCESSES\naccess to the server\u2019s resources such as its disk. Protecting such resources is\nessential. In contrast, downloading code as in the receiver-initiated case, can often\nbe done anonymously. Moreover, the server is generally not interested in the\nclient\u2019s resources. Instead, code migration to the client is done only for improving\nclient-side performance. To that end, only a limited number of resources need to\nbe protected, such as memory and network connections. We return to secure code\nmigration extensively in Chapter 9.\nBefore execution After execution\nClient Server Client Server\nCScode\nexec\nresourcecode\nexec*\nresource\nREVcode\n\u0000! exec\nresource\u0000!code\nexec*\nresource\nCoD exec\nresource \u0000code code\nexec*\nresource \u0000\nMAcode\nexec\nresource\u0000!\nresource resource\u0000!code\nexec*\nresource\nCS: Client-Server REV: Remote evaluation\nCoD: Code-on-demand MA: Mobile agents\nFigure 3.25: Four different paradigms for code mobility.\nThis brings us to four different paradigms for code mobility, as shown in\nFigure 3.25. Following Fuggetta et al. [1998], we make a distinction between sim-\npleclient-server computing ,remote evaluation ,code-on-demand , and mobile\nagents . Figure 3.25 shows the situation at respectively the client and the server,\nbefore and after execution of the mobile code.\nIn the case of client-server computing, the code, execution state, and resource\nsegment are all located at the server, and after execution, only the execution state\nat the server is generally modi\ufb01ed. This state modi\ufb01cation is denoted by means\nof an asterisk. With the sender-initiated remote evaluation , the client migrates\ncode to the server where that code is executed and leading to a modi\ufb01cation of\nthe execution state at the server. Code-on-demand is a receiver-initiated scheme\nby which the client obtains code from the server with its execution modifying\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.5. CODE MIGRATION 157\nthe client-side execution state and operating on the client\u2019s resources. Finally,\nmobile agents typically follow a sender-initiated approach, moving code as well\nas execution state from the client to the server, operating on both the client\u2019s as\nwell as the server\u2019s resources. The execution of a mobile agent will generally lead\nto modi\ufb01cation of the associated execution state.\nThe bare minimum for code migration is to provide only weak mobility . In\nthis model, it is possible to transfer only the code segment, along with perhaps\nsome initialization data. A characteristic feature of weak mobility is that a\ntransferred program is always started anew. This is what happens, for example,\nwith Java applets, which start from the same initial state. In other words, no\nhistory from where the migrated code left off at a previous location is maintained\nby the underlying middleware. If such history needs to be preserved, it will have\nto be encoded as part of the mobile application itself. The bene\ufb01t of weak mobility\nis its simplicity, as it requires only that the target machine can execute the code\nsegment. In essence, this boils down to making the code portable. We return to\nthese matters when discussing migration in heterogeneous systems.\nIn contrast to weak mobility, in systems that support strong mobility the\nexecution segment can be transferred as well. The characteristic feature of strong\nmobility is that a running process can be stopped, subsequently moved to another\nmachine, and then resume execution exactly where it left off. Clearly, strong\nmobility is much more general than weak mobility, but also much more dif\ufb01cult\nto implement. In particular, when migrating a process, the execution segment\ngenerally also contains data that is highly dependent on a speci\ufb01c implementation\nof the underlying operating system. For example, it may rely on information\nnormally found in the operating system\u2019s process table. As a consequence,\nmigrating to a different operating system, even one that belongs to the same\nfamily as the source, may cause a lot of headaches.\nIn the case of weak mobility, it also makes a difference if the migrated code\nis executed by the target process, or whether a separate process is started. For\nexample, Java applets are simply downloaded by a Web browser and are executed\nin the browser\u2019s address space. The bene\ufb01t of this approach is that there is no\nneed to start a separate process, thereby avoiding interprocess communication\nat the target machine. The main drawback, obviously, is that the target process\nneeds to be protected against malicious or inadvertent code executions, which\nmay be reason enough to isolate the migrated code in a separate process.\nInstead of moving a running process, also referred to as process migration,\nstrong mobility can also be supported by remote cloning. In contrast to process\nmigration, cloning yields an exact copy of the original process, but now running\non a different machine. The cloned process is executed in parallel to the original\nprocess. In Unix systems, remote cloning takes place by forking off a child\nprocess and letting that child continue on a remote machine. The bene\ufb01t of\ncloning is that the model closely resembles the one that is already used in many\napplications. The only difference is that the cloned process is executed on a\ndifferent machine. In this sense, migration by cloning is a simple way to improve\ndistribution transparency.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.5. CODE MIGRATION 157\nthe client-side execution state and operating on the client\u2019s resources. Finally,\nmobile agents typically follow a sender-initiated approach, moving code as well\nas execution state from the client to the server, operating on both the client\u2019s as\nwell as the server\u2019s resources. The execution of a mobile agent will generally lead\nto modi\ufb01cation of the associated execution state.\nThe bare minimum for code migration is to provide only weak mobility . In\nthis model, it is possible to transfer only the code segment, along with perhaps\nsome initialization data. A characteristic feature of weak mobility is that a\ntransferred program is always started anew. This is what happens, for example,\nwith Java applets, which start from the same initial state. In other words, no\nhistory from where the migrated code left off at a previous location is maintained\nby the underlying middleware. If such history needs to be preserved, it will have\nto be encoded as part of the mobile application itself. The bene\ufb01t of weak mobility\nis its simplicity, as it requires only that the target machine can execute the code\nsegment. In essence, this boils down to making the code portable. We return to\nthese matters when discussing migration in heterogeneous systems.\nIn contrast to weak mobility, in systems that support strong mobility the\nexecution segment can be transferred as well. The characteristic feature of strong\nmobility is that a running process can be stopped, subsequently moved to another\nmachine, and then resume execution exactly where it left off. Clearly, strong\nmobility is much more general than weak mobility, but also much more dif\ufb01cult\nto implement. In particular, when migrating a process, the execution segment\ngenerally also contains data that is highly dependent on a speci\ufb01c implementation\nof the underlying operating system. For example, it may rely on information\nnormally found in the operating system\u2019s process table. As a consequence,\nmigrating to a different operating system, even one that belongs to the same\nfamily as the source, may cause a lot of headaches.\nIn the case of weak mobility, it also makes a difference if the migrated code\nis executed by the target process, or whether a separate process is started. For\nexample, Java applets are simply downloaded by a Web browser and are executed\nin the browser\u2019s address space. The bene\ufb01t of this approach is that there is no\nneed to start a separate process, thereby avoiding interprocess communication\nat the target machine. The main drawback, obviously, is that the target process\nneeds to be protected against malicious or inadvertent code executions, which\nmay be reason enough to isolate the migrated code in a separate process.\nInstead of moving a running process, also referred to as process migration,\nstrong mobility can also be supported by remote cloning. In contrast to process\nmigration, cloning yields an exact copy of the original process, but now running\non a different machine. The cloned process is executed in parallel to the original\nprocess. In Unix systems, remote cloning takes place by forking off a child\nprocess and letting that child continue on a remote machine. The bene\ufb01t of\ncloning is that the model closely resembles the one that is already used in many\napplications. The only difference is that the cloned process is executed on a\ndifferent machine. In this sense, migration by cloning is a simple way to improve\ndistribution transparency.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "158 CHAPTER 3. PROCESSES\nMigration in heterogeneous systems\nSo far, we have tacitly assumed that the migrated code can be easily executed\nat the target machine. This assumption is in order when dealing with homo-\ngeneous systems. In general, however, distributed systems are constructed\non a heterogeneous collection of platforms, each having their own operating\nsystem and machine architecture.\nThe problems coming from heterogeneity are in many respects the same\nas those of portability. Not surprisingly, solutions are also very similar. For\nexample, at the end of the 1970s, a simple solution to alleviate many of the\nproblems of porting Pascal to different machines was to generate machine-\nindependent intermediate code for an abstract virtual machine [Barron, 1981].\nThat machine, of course, would need to be implemented on many platforms,\nbut it would then allow Pascal programs to be run anywhere. Although this\nsimple idea was widely used for some years, it never really caught on as the\ngeneral solution to portability problems for other languages, notably C.\nAbout 25 years later, code migration in heterogeneous systems is being\ntackled by scripting languages and highly portable languages such as Java.\nIn essence, these solutions adopt the same approach as was done for porting\nPascal. All such solutions have in common that they rely on a (process) virtual\nmachine that either directly interprets source code (as in the case of scripting\nlanguages), or otherwise interprets intermediate code generated by a compiler\n(as in Java). Being in the right place at the right time is also important for\nlanguage developers.\nFurther developments have weakened the dependency on programming\nlanguages. In particular, solutions have been proposed to migrate not only\nprocesses, but to migrate entire computing environments. The basic idea\nis to compartmentalize the overall environment and to provide processes\nin the same part their own view on their computing environment. That\ncompartmentalization takes place in the form of virtual machine monitors\nrunning an operating system and a suite of applications.\nWith virtual machine migration, it becomes possible to decouple a com-\nputing environment from the underlying system and actually migrate it to\nanother machine. A major advantage of this approach is that processes can\nremain ignorant of the migration itself: they need not be interrupted in their\nexecution, nor should they experience any problems with used resources. The\nlatter are either migrating along with a process, or the way that a process\naccesses a resource is left unaffected (at least, for that process).\nAs an example, Clark et al. [2005] concentrated on real-time migration of a\nvirtualized operating system, typically something that would be convenient\nin a cluster of servers where a tight coupling is achieved through a single,\nshared local-area network. Under these circumstances, migration involves two\nmajor problems: migrating the entire memory image and migrating bindings\nto local resources.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n158 CHAPTER 3. PROCESSES\nMigration in heterogeneous systems\nSo far, we have tacitly assumed that the migrated code can be easily executed\nat the target machine. This assumption is in order when dealing with homo-\ngeneous systems. In general, however, distributed systems are constructed\non a heterogeneous collection of platforms, each having their own operating\nsystem and machine architecture.\nThe problems coming from heterogeneity are in many respects the same\nas those of portability. Not surprisingly, solutions are also very similar. For\nexample, at the end of the 1970s, a simple solution to alleviate many of the\nproblems of porting Pascal to different machines was to generate machine-\nindependent intermediate code for an abstract virtual machine [Barron, 1981].\nThat machine, of course, would need to be implemented on many platforms,\nbut it would then allow Pascal programs to be run anywhere. Although this\nsimple idea was widely used for some years, it never really caught on as the\ngeneral solution to portability problems for other languages, notably C.\nAbout 25 years later, code migration in heterogeneous systems is being\ntackled by scripting languages and highly portable languages such as Java.\nIn essence, these solutions adopt the same approach as was done for porting\nPascal. All such solutions have in common that they rely on a (process) virtual\nmachine that either directly interprets source code (as in the case of scripting\nlanguages), or otherwise interprets intermediate code generated by a compiler\n(as in Java). Being in the right place at the right time is also important for\nlanguage developers.\nFurther developments have weakened the dependency on programming\nlanguages. In particular, solutions have been proposed to migrate not only\nprocesses, but to migrate entire computing environments. The basic idea\nis to compartmentalize the overall environment and to provide processes\nin the same part their own view on their computing environment. That\ncompartmentalization takes place in the form of virtual machine monitors\nrunning an operating system and a suite of applications.\nWith virtual machine migration, it becomes possible to decouple a com-\nputing environment from the underlying system and actually migrate it to\nanother machine. A major advantage of this approach is that processes can\nremain ignorant of the migration itself: they need not be interrupted in their\nexecution, nor should they experience any problems with used resources. The\nlatter are either migrating along with a process, or the way that a process\naccesses a resource is left unaffected (at least, for that process).\nAs an example, Clark et al. [2005] concentrated on real-time migration of a\nvirtualized operating system, typically something that would be convenient\nin a cluster of servers where a tight coupling is achieved through a single,\nshared local-area network. Under these circumstances, migration involves two\nmajor problems: migrating the entire memory image and migrating bindings\nto local resources.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.5. CODE MIGRATION 159\nAs to the \ufb01rst problem, there are, in principle, three ways to handle\nmigration (which can be combined):\n1.Pushing memory pages to the new machine and resending the ones that\nare later modi\ufb01ed during the migration process.\n2.Stopping the current virtual machine; migrate memory, and start the\nnew virtual machine.\n3.Letting the new virtual machine pull in new pages as needed, that is,\nlet processes start on the new virtual machine immediately and copy\nmemory pages on demand.\nThe second option may lead to unacceptable downtime if the migrating\nvirtual machine is running a live service, that is, one that offers continuous\nservice. On the other hand, a pure on-demand approach as represented by\nthe third option may extensively prolong the migration period, but may also\nlead to poor performance because it takes a long time before the working set\nof the migrated processes has been moved to the new machine.\nAs an alternative, Clark et al. [2005] propose to use a pre-copy approach\nwhich combines the \ufb01rst option, along with a brief stop-and-copy phase as\nrepresented by the second option. As it turns out, this combination can lead\nto very low service downtimes (see also Note 3.12).\nConcerning local resources, matters are simpli\ufb01ed when dealing only with\na cluster server. First, because there is a single network, the only thing that\nneeds to be done is to announce the new network-to-MAC address binding, so\nthat clients can contact the migrated processes at the correct network interface.\nFinally, if it can be assumed that storage is provided as a separate tier (like we\nshowed in Figure 3.18), then migrating binding to \ufb01les is similarly simple, as\nit effectively means reestablishing network connections.\nNote 3.12 (Advanced: On the performance of live virtual machine migration)\nOne potential problem with virtual-machine migration is that it may take consid-\nerable time. This by itself need not be bad as long as the services that are running\non the migrating virtual machine can continue to operate. An approach used in\npractice was brie\ufb02y described above. First, memory pages are copied to the target\nmachine, possibly sending updates of pages that were modi\ufb01ed while copying\ntook place (remember that copying lots of memory may take tens of seconds, even\nacross a high-speed local network). Second, when most pages have been faithfully\ncopied, the current machine is stopped, the remaining dirty pages are copied to\nthe target, where the now exact copy can then be started where the original left\noff.\nThe downtime in which the remaining dirty pages need to be copied depends\non the applications running on the virtual machine. Clark et al. [2005] report\ndowntimes for speci\ufb01c con\ufb01gurations between 60 msecs and less than 4 secs.\nVoorsluys et al. [2009] come to similar values. However, what may be more\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.5. CODE MIGRATION 159\nAs to the \ufb01rst problem, there are, in principle, three ways to handle\nmigration (which can be combined):\n1.Pushing memory pages to the new machine and resending the ones that\nare later modi\ufb01ed during the migration process.\n2.Stopping the current virtual machine; migrate memory, and start the\nnew virtual machine.\n3.Letting the new virtual machine pull in new pages as needed, that is,\nlet processes start on the new virtual machine immediately and copy\nmemory pages on demand.\nThe second option may lead to unacceptable downtime if the migrating\nvirtual machine is running a live service, that is, one that offers continuous\nservice. On the other hand, a pure on-demand approach as represented by\nthe third option may extensively prolong the migration period, but may also\nlead to poor performance because it takes a long time before the working set\nof the migrated processes has been moved to the new machine.\nAs an alternative, Clark et al. [2005] propose to use a pre-copy approach\nwhich combines the \ufb01rst option, along with a brief stop-and-copy phase as\nrepresented by the second option. As it turns out, this combination can lead\nto very low service downtimes (see also Note 3.12).\nConcerning local resources, matters are simpli\ufb01ed when dealing only with\na cluster server. First, because there is a single network, the only thing that\nneeds to be done is to announce the new network-to-MAC address binding, so\nthat clients can contact the migrated processes at the correct network interface.\nFinally, if it can be assumed that storage is provided as a separate tier (like we\nshowed in Figure 3.18), then migrating binding to \ufb01les is similarly simple, as\nit effectively means reestablishing network connections.\nNote 3.12 (Advanced: On the performance of live virtual machine migration)\nOne potential problem with virtual-machine migration is that it may take consid-\nerable time. This by itself need not be bad as long as the services that are running\non the migrating virtual machine can continue to operate. An approach used in\npractice was brie\ufb02y described above. First, memory pages are copied to the target\nmachine, possibly sending updates of pages that were modi\ufb01ed while copying\ntook place (remember that copying lots of memory may take tens of seconds, even\nacross a high-speed local network). Second, when most pages have been faithfully\ncopied, the current machine is stopped, the remaining dirty pages are copied to\nthe target, where the now exact copy can then be started where the original left\noff.\nThe downtime in which the remaining dirty pages need to be copied depends\non the applications running on the virtual machine. Clark et al. [2005] report\ndowntimes for speci\ufb01c con\ufb01gurations between 60 msecs and less than 4 secs.\nVoorsluys et al. [2009] come to similar values. However, what may be more\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "160 CHAPTER 3. PROCESSES\ninteresting is to observe what the response time is of the service running on the\nvirtual machine while the latter is being migrated. The model in this case is\nthat the service continues to operate on the original machine until full migration\nhas completed. However, we cannot ignore that migration itself is resource-\nintensive operation, requiring considerable processing capacity as well as network\nbandwidth.\nFigure 3.26: The effect on the response time of a service while migrating\nits underlying virtual machine. Adapted from Voorsluys et al. [2009].\nVoorsluys et al. [2009] have observed that a complete migration may actually\ntake tens of seconds, leading to a ten- to twentyfold increase in response time. In\naddition, we need to realize that during the migration, a service will be completely\nunavailable (i.e., unresponsive) for perhaps 4 seconds. The good news is that\nthe response time goes up signi\ufb01cantly only after the downtime to complete the\nmigration, as shown in Figure 3.26.\nIn many cases, virtual machines are migrated to optimize the usage of actual\nmachines. However, it may also be desirable to clone a virtual machine, for\nexample, because the workload for the current machine is becoming too high.\nSuch cloning is very similar to using multiple processes in concurrent servers by\nwhich a dispatcher process creates worker processes to handle incoming requests.\nThis scheme was explained in Figure 3.4 when discussing multithreaded servers.\nWhen cloning for this type of performance, it often makes more sense notto\n\ufb01rst copy memory pages, but, in fact, start with as few pages as possible as the\nservice running on the cloned machine will essentially start anew. Note that this\nbehavior is very similar to the usual parent-child behavior we see when forking a\nUnix process. Namely, the child will start with loading its own executable, thereby\neffectively cleaning the memory it inherited from its parent. This analogy inspired\nLagar-Cavilla et al. [2009] to develop an analogous mechanism for forking a virtual\nmachine. However, unlike the mechanism used traditionally for migrating virtual\nmachines, their VM fork copies pages primarily on demand. The result is an\nextremely ef\ufb01cient cloning mechanism.\nIt is thus seen that there is no single best way to place copies of a virtual\nmachine on different physical machines: it very much depends on how and why\na virtual machine is being deployed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n160 CHAPTER 3. PROCESSES\ninteresting is to observe what the response time is of the service running on the\nvirtual machine while the latter is being migrated. The model in this case is\nthat the service continues to operate on the original machine until full migration\nhas completed. However, we cannot ignore that migration itself is resource-\nintensive operation, requiring considerable processing capacity as well as network\nbandwidth.\nFigure 3.26: The effect on the response time of a service while migrating\nits underlying virtual machine. Adapted from Voorsluys et al. [2009].\nVoorsluys et al. [2009] have observed that a complete migration may actually\ntake tens of seconds, leading to a ten- to twentyfold increase in response time. In\naddition, we need to realize that during the migration, a service will be completely\nunavailable (i.e., unresponsive) for perhaps 4 seconds. The good news is that\nthe response time goes up signi\ufb01cantly only after the downtime to complete the\nmigration, as shown in Figure 3.26.\nIn many cases, virtual machines are migrated to optimize the usage of actual\nmachines. However, it may also be desirable to clone a virtual machine, for\nexample, because the workload for the current machine is becoming too high.\nSuch cloning is very similar to using multiple processes in concurrent servers by\nwhich a dispatcher process creates worker processes to handle incoming requests.\nThis scheme was explained in Figure 3.4 when discussing multithreaded servers.\nWhen cloning for this type of performance, it often makes more sense notto\n\ufb01rst copy memory pages, but, in fact, start with as few pages as possible as the\nservice running on the cloned machine will essentially start anew. Note that this\nbehavior is very similar to the usual parent-child behavior we see when forking a\nUnix process. Namely, the child will start with loading its own executable, thereby\neffectively cleaning the memory it inherited from its parent. This analogy inspired\nLagar-Cavilla et al. [2009] to develop an analogous mechanism for forking a virtual\nmachine. However, unlike the mechanism used traditionally for migrating virtual\nmachines, their VM fork copies pages primarily on demand. The result is an\nextremely ef\ufb01cient cloning mechanism.\nIt is thus seen that there is no single best way to place copies of a virtual\nmachine on different physical machines: it very much depends on how and why\na virtual machine is being deployed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "3.6. SUMMARY 161\n3.6 Summary\nProcesses play a fundamental role in distributed systems as they form a\nbasis for communication between different machines. An important issue\nis how processes are internally organized and, in particular, whether or not\nthey support multiple threads of control. Threads in distributed systems are\nparticularly useful to continue using the CPU when a blocking I/O operation\nis performed. In this way, it becomes possible to build highly-ef\ufb01cient servers\nthat run multiple threads in parallel, of which several may be blocking to wait\nuntil disk I/O or network communication completes. In general, threads are\npreferred over the use of processes when performance is at stake.\nVirtualization has since long been an important \ufb01eld in computer science,\nbut in the advent of cloud computing has regained tremendous attention.\nPopular virtualization schemes allow users to run a suite of applications\non top of their favorite operating system and con\ufb01gure complete virtual\ndistributed systems in the cloud. Impressively enough, performance remains\nclose to running applications on the host operating system, unless that system\nis shared with other virtual machines. The \ufb02exible application of virtual\nmachines has led to different types of services for cloud computing, including\ninfrastructures, platforms, and software \u2014 all running in virtual environments.\nOrganizing a distributed application in terms of clients and servers has\nproven to be useful. Client processes generally implement user interfaces,\nwhich may range from very simple displays to advanced interfaces that can\nhandle compound documents. Client software is furthermore aimed at achiev-\ning distribution transparency by hiding details concerning the communication\nwith servers, where those servers are currently located, and whether or not\nservers are replicated. In addition, client software is partly responsible for\nhiding failures and recovery from failures.\nServers are often more intricate than clients, but are nevertheless subject\nto only a relatively few design issues. For example, servers can either be\niterative or concurrent, implement one or more services, and can be stateless\nor stateful. Other design issues deal with addressing services and mechanisms\nto interrupt a server after a service request has been issued and is possibly\nalready being processed.\nSpecial attention needs to be paid when organizing servers into a cluster.\nA common objective is to hide the internals of a cluster from the outside\nworld. This means that the organization of the cluster should be shielded\nfrom applications. To this end, most clusters use a single entry point that\ncan hand off messages to servers in the cluster. A challenging problem is to\ntransparently replace this single entry point by a fully distributed solution.\nAdvanced object servers have been developed for hosting remote objects.\nAn object server provides many services to basic objects, including facilities\nfor storing objects, or to ensure serialization of incoming requests. Another\nimportant role is providing the illusion to the outside world that a collection\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n3.6. SUMMARY 161\n3.6 Summary\nProcesses play a fundamental role in distributed systems as they form a\nbasis for communication between different machines. An important issue\nis how processes are internally organized and, in particular, whether or not\nthey support multiple threads of control. Threads in distributed systems are\nparticularly useful to continue using the CPU when a blocking I/O operation\nis performed. In this way, it becomes possible to build highly-ef\ufb01cient servers\nthat run multiple threads in parallel, of which several may be blocking to wait\nuntil disk I/O or network communication completes. In general, threads are\npreferred over the use of processes when performance is at stake.\nVirtualization has since long been an important \ufb01eld in computer science,\nbut in the advent of cloud computing has regained tremendous attention.\nPopular virtualization schemes allow users to run a suite of applications\non top of their favorite operating system and con\ufb01gure complete virtual\ndistributed systems in the cloud. Impressively enough, performance remains\nclose to running applications on the host operating system, unless that system\nis shared with other virtual machines. The \ufb02exible application of virtual\nmachines has led to different types of services for cloud computing, including\ninfrastructures, platforms, and software \u2014 all running in virtual environments.\nOrganizing a distributed application in terms of clients and servers has\nproven to be useful. Client processes generally implement user interfaces,\nwhich may range from very simple displays to advanced interfaces that can\nhandle compound documents. Client software is furthermore aimed at achiev-\ning distribution transparency by hiding details concerning the communication\nwith servers, where those servers are currently located, and whether or not\nservers are replicated. In addition, client software is partly responsible for\nhiding failures and recovery from failures.\nServers are often more intricate than clients, but are nevertheless subject\nto only a relatively few design issues. For example, servers can either be\niterative or concurrent, implement one or more services, and can be stateless\nor stateful. Other design issues deal with addressing services and mechanisms\nto interrupt a server after a service request has been issued and is possibly\nalready being processed.\nSpecial attention needs to be paid when organizing servers into a cluster.\nA common objective is to hide the internals of a cluster from the outside\nworld. This means that the organization of the cluster should be shielded\nfrom applications. To this end, most clusters use a single entry point that\ncan hand off messages to servers in the cluster. A challenging problem is to\ntransparently replace this single entry point by a fully distributed solution.\nAdvanced object servers have been developed for hosting remote objects.\nAn object server provides many services to basic objects, including facilities\nfor storing objects, or to ensure serialization of incoming requests. Another\nimportant role is providing the illusion to the outside world that a collection\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "162 CHAPTER 3. PROCESSES\nof data and procedures operating on that data correspond to the concept of\nan object. This role is implemented by means of object adapters. Object-based\nsystems have come to a point where we can build entire frameworks that can\nbe extended for supporting speci\ufb01c applications. Java has proven to provide\na powerful means for setting up more generic services, exempli\ufb01ed by the\nhighly popular Enterprise Java Beans concept and its implementation.\nAn exemplary server for Web-based systems is the one from Apache.\nAgain, the Apache server can be seen as a general solution for handling a\nmyriad of HTTP-based queries. By offering the right hooks, we essentially ob-\ntain a \ufb02exibly con\ufb01gurable Web server. Apache has served as an example not\nonly for traditional Web sites, but also for setting up clusters of collaborative\nWeb servers, even across wide-area networks.\nAn important topic for distributed systems is the migration of code be-\ntween different machines. Two important reasons to support code migration\nare increasing performance and \ufb02exibility. When communication is expensive,\nwe can sometimes reduce communication by shipping computations from\nthe server to the client, and let the client do as much local processing as\npossible. Flexibility is increased if a client can dynamically download software\nneeded to communicate with a speci\ufb01c server. The downloaded software can\nbe speci\ufb01cally targeted to that server, without forcing the client to have it\npreinstalled.\nCode migration brings along problems related to usage of local resources\nfor which it is required that either resources are migrated as well, new bind-\nings to local resources at the target machine are established, or for which\nsystemwide network references are used. Another problem is that code mi-\ngration requires that we take heterogeneity into account. Current practice\nindicates that the best solution to handle heterogeneity is to use virtual ma-\nchines. These can take either the form of process virtual machines as in the\ncase of, for example, Java, or through using virtual machine monitors that\neffectively allow the migration of a collection of processes along with their\nunderlying operating system.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n162 CHAPTER 3. PROCESSES\nof data and procedures operating on that data correspond to the concept of\nan object. This role is implemented by means of object adapters. Object-based\nsystems have come to a point where we can build entire frameworks that can\nbe extended for supporting speci\ufb01c applications. Java has proven to provide\na powerful means for setting up more generic services, exempli\ufb01ed by the\nhighly popular Enterprise Java Beans concept and its implementation.\nAn exemplary server for Web-based systems is the one from Apache.\nAgain, the Apache server can be seen as a general solution for handling a\nmyriad of HTTP-based queries. By offering the right hooks, we essentially ob-\ntain a \ufb02exibly con\ufb01gurable Web server. Apache has served as an example not\nonly for traditional Web sites, but also for setting up clusters of collaborative\nWeb servers, even across wide-area networks.\nAn important topic for distributed systems is the migration of code be-\ntween different machines. Two important reasons to support code migration\nare increasing performance and \ufb02exibility. When communication is expensive,\nwe can sometimes reduce communication by shipping computations from\nthe server to the client, and let the client do as much local processing as\npossible. Flexibility is increased if a client can dynamically download software\nneeded to communicate with a speci\ufb01c server. The downloaded software can\nbe speci\ufb01cally targeted to that server, without forcing the client to have it\npreinstalled.\nCode migration brings along problems related to usage of local resources\nfor which it is required that either resources are migrated as well, new bind-\nings to local resources at the target machine are established, or for which\nsystemwide network references are used. Another problem is that code mi-\ngration requires that we take heterogeneity into account. Current practice\nindicates that the best solution to handle heterogeneity is to use virtual ma-\nchines. These can take either the form of process virtual machines as in the\ncase of, for example, Java, or through using virtual machine monitors that\neffectively allow the migration of a collection of processes along with their\nunderlying operating system.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 4\nCommunication\nInterprocess communication is at the heart of all distributed systems. It makes\nno sense to study distributed systems without carefully examining the ways\nthat processes on different machines can exchange information. Communica-\ntion in distributed systems has traditionally always been based on low-level\nmessage passing as offered by the underlying network. Expressing commu-\nnication through message passing is harder than using primitives based on\nshared memory, as available for nondistributed platforms. Modern distributed\nsystems often consist of thousands or even millions of processes scattered\nacross a network with unreliable communication such as the Internet. Unless\nthe primitive communication facilities of computer networks are replaced\nby something else, development of large-scale distributed applications is\nextremely dif\ufb01cult.\nIn this chapter, we start by discussing the rules that communicating pro-\ncesses must adhere to, known as protocols, and concentrate on structuring\nthose protocols in the form of layers. We then look at two widely-used models\nfor communication: Remote Procedure Call (RPC), and Message-Oriented\nMiddleware (MOM). We also discuss the general problem of sending data to\nmultiple receivers, called multicasting.\nOur \ufb01rst model for communication in distributed systems is the remote\nprocedure call (RPC). An RPC aims at hiding most of the intricacies of message\npassing, and is ideal for client-server applications. However, realizing RPCs\nin a transparent manner is easier said than done. We look at a number of\nimportant details that cannot be ignored, while diving into actually code to\nillustrate to what extent distribution transparency can be realized such that\nperformance is still acceptable.\nIn many distributed applications, communication does not follow the\nrather strict pattern of client-server interaction. In those cases, it turns out\nthat thinking in terms of messages is more appropriate. The low-level com-\nmunication facilities of computer networks are in many ways not suitable,\n163\nChapter 4\nCommunication\nInterprocess communication is at the heart of all distributed systems. It makes\nno sense to study distributed systems without carefully examining the ways\nthat processes on different machines can exchange information. Communica-\ntion in distributed systems has traditionally always been based on low-level\nmessage passing as offered by the underlying network. Expressing commu-\nnication through message passing is harder than using primitives based on\nshared memory, as available for nondistributed platforms. Modern distributed\nsystems often consist of thousands or even millions of processes scattered\nacross a network with unreliable communication such as the Internet. Unless\nthe primitive communication facilities of computer networks are replaced\nby something else, development of large-scale distributed applications is\nextremely dif\ufb01cult.\nIn this chapter, we start by discussing the rules that communicating pro-\ncesses must adhere to, known as protocols, and concentrate on structuring\nthose protocols in the form of layers. We then look at two widely-used models\nfor communication: Remote Procedure Call (RPC), and Message-Oriented\nMiddleware (MOM). We also discuss the general problem of sending data to\nmultiple receivers, called multicasting.\nOur \ufb01rst model for communication in distributed systems is the remote\nprocedure call (RPC). An RPC aims at hiding most of the intricacies of message\npassing, and is ideal for client-server applications. However, realizing RPCs\nin a transparent manner is easier said than done. We look at a number of\nimportant details that cannot be ignored, while diving into actually code to\nillustrate to what extent distribution transparency can be realized such that\nperformance is still acceptable.\nIn many distributed applications, communication does not follow the\nrather strict pattern of client-server interaction. In those cases, it turns out\nthat thinking in terms of messages is more appropriate. The low-level com-\nmunication facilities of computer networks are in many ways not suitable,\n163", "164 CHAPTER 4. COMMUNICATION\nagain due to their lack of distribution transparency. An alternative is to use a\nhigh-level message-queuing model, in which communication proceeds much\nthe same as in e-mail systems. Message-oriented communication is a subject\nimportant enough to warrant a section of its own. We look at numerous\naspects, including application-level routing.\nFinally, since our understanding of setting up multicast facilities has im-\nproved, novel and elegant solutions for data dissemination have emerged.\nWe pay separate attention to this subject in the last section of this chapter,\ndiscussing traditional deterministic means of multicasting, as well as proba-\nbilistic approaches as used in \ufb02ooding and gossiping. The latter have been\nreceiving increased attention over the past years due to their elegance and\nsimplicity.\n4.1 Foundations\nBefore we start our discussion on communication in distributed systems, we\n\ufb01rst recapitulate some of the fundamental issues related to communication.\nIn the next section we brie\ufb02y discuss network communication protocols, as\nthese form the basis for any distributed system. After that, we take a different\napproach by classifying the different types of communication that usually\noccur in distributed systems.\nLayered Protocols\nDue to the absence of shared memory, all communication in distributed\nsystems is based on sending and receiving (low level) messages. When\nprocess Pwants to communicate with process Q, it \ufb01rst builds a message in\nits own address space. Then it executes a system call that causes the operating\nsystem to send the message over the network to Q. Although this basic idea\nsounds simple enough, in order to prevent chaos, Pand Qhave to agree on\nthe meaning of the bits being sent.\nThe OSI reference model\nTo make it easier to deal with the numerous levels and issues involved in\ncommunication, the International Standards Organization (ISO) developed\na reference model that clearly identi\ufb01es the various levels involved, gives\nthem standard names, and points out which level should do which job. This\nmodel is called the Open Systems Interconnection Reference Model [Day\nand Zimmerman, 1983] usually abbreviated as ISO OSI or sometimes just the\nOSI model . It should be emphasized that the protocols that were developed\nas part of the OSI model were never widely used and are essentially dead.\nHowever, the underlying model itself has proved to be quite useful for un-\nderstanding computer networks. Although we do not intend to give a full\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n164 CHAPTER 4. COMMUNICATION\nagain due to their lack of distribution transparency. An alternative is to use a\nhigh-level message-queuing model, in which communication proceeds much\nthe same as in e-mail systems. Message-oriented communication is a subject\nimportant enough to warrant a section of its own. We look at numerous\naspects, including application-level routing.\nFinally, since our understanding of setting up multicast facilities has im-\nproved, novel and elegant solutions for data dissemination have emerged.\nWe pay separate attention to this subject in the last section of this chapter,\ndiscussing traditional deterministic means of multicasting, as well as proba-\nbilistic approaches as used in \ufb02ooding and gossiping. The latter have been\nreceiving increased attention over the past years due to their elegance and\nsimplicity.\n4.1 Foundations\nBefore we start our discussion on communication in distributed systems, we\n\ufb01rst recapitulate some of the fundamental issues related to communication.\nIn the next section we brie\ufb02y discuss network communication protocols, as\nthese form the basis for any distributed system. After that, we take a different\napproach by classifying the different types of communication that usually\noccur in distributed systems.\nLayered Protocols\nDue to the absence of shared memory, all communication in distributed\nsystems is based on sending and receiving (low level) messages. When\nprocess Pwants to communicate with process Q, it \ufb01rst builds a message in\nits own address space. Then it executes a system call that causes the operating\nsystem to send the message over the network to Q. Although this basic idea\nsounds simple enough, in order to prevent chaos, Pand Qhave to agree on\nthe meaning of the bits being sent.\nThe OSI reference model\nTo make it easier to deal with the numerous levels and issues involved in\ncommunication, the International Standards Organization (ISO) developed\na reference model that clearly identi\ufb01es the various levels involved, gives\nthem standard names, and points out which level should do which job. This\nmodel is called the Open Systems Interconnection Reference Model [Day\nand Zimmerman, 1983] usually abbreviated as ISO OSI or sometimes just the\nOSI model . It should be emphasized that the protocols that were developed\nas part of the OSI model were never widely used and are essentially dead.\nHowever, the underlying model itself has proved to be quite useful for un-\nderstanding computer networks. Although we do not intend to give a full\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.1. FOUNDATIONS 165\ndescription of this model and all of its implications here, a short introduction\nwill be helpful. For more details see [Tanenbaum and Wetherall, 2010].\nThe OSI model is designed to allow open systems to communicate. An\nopen system is one that is prepared to communicate with any other open\nsystem by using standard rules that govern the format, contents, and meaning\nof the messages sent and received. These rules are formalized in what are\ncalled communication protocols . To allow a group of computers to commu-\nnicate over a network, they must all agree on the protocols to be used. A\nprotocol is said to provide a communication service . There are two types of\nsuch services. In the case of a connection-oriented service, before exchang-\ning data the sender and receiver \ufb01rst explicitly establish a connection, and\npossibly negotiate speci\ufb01c parameters of the protocol they will use. When\nthey are done, they release (terminate) the connection. The telephone is\na typical connection-oriented communication service. With connectionless\nservices , no setup in advance is needed. The sender just transmits the \ufb01rst\nmessage when it is ready. Dropping a letter in a mailbox is an example of\nmaking use of connectionless communication service. With computers, both\nconnection-oriented and connectionless communication are common.\nFigure 4.1: Layers, interfaces, and protocols in the OSI model.\nIn the OSI model, communication is divided into seven levels or layers, as\nshown in Figure 4.1. Each layer offers one or more speci\ufb01c communication\nservices to the layer above it. In this way, the problem of getting a message\nfrom AtoBcan be divided into manageable pieces, each of which can be\nsolved independently of the others. Each layer provides an interface to the\none above it. The interface consists of a set of operations that together de\ufb01ne\nthe service the layer is prepared to offer. The seven OSI layers are:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.1. FOUNDATIONS 165\ndescription of this model and all of its implications here, a short introduction\nwill be helpful. For more details see [Tanenbaum and Wetherall, 2010].\nThe OSI model is designed to allow open systems to communicate. An\nopen system is one that is prepared to communicate with any other open\nsystem by using standard rules that govern the format, contents, and meaning\nof the messages sent and received. These rules are formalized in what are\ncalled communication protocols . To allow a group of computers to commu-\nnicate over a network, they must all agree on the protocols to be used. A\nprotocol is said to provide a communication service . There are two types of\nsuch services. In the case of a connection-oriented service, before exchang-\ning data the sender and receiver \ufb01rst explicitly establish a connection, and\npossibly negotiate speci\ufb01c parameters of the protocol they will use. When\nthey are done, they release (terminate) the connection. The telephone is\na typical connection-oriented communication service. With connectionless\nservices , no setup in advance is needed. The sender just transmits the \ufb01rst\nmessage when it is ready. Dropping a letter in a mailbox is an example of\nmaking use of connectionless communication service. With computers, both\nconnection-oriented and connectionless communication are common.\nFigure 4.1: Layers, interfaces, and protocols in the OSI model.\nIn the OSI model, communication is divided into seven levels or layers, as\nshown in Figure 4.1. Each layer offers one or more speci\ufb01c communication\nservices to the layer above it. In this way, the problem of getting a message\nfrom AtoBcan be divided into manageable pieces, each of which can be\nsolved independently of the others. Each layer provides an interface to the\none above it. The interface consists of a set of operations that together de\ufb01ne\nthe service the layer is prepared to offer. The seven OSI layers are:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "166 CHAPTER 4. COMMUNICATION\nPhysical layer Deals with standardizing how two computers are connected\nand how 0s and 1s are represented.\nData link layer Provides the means to detect and possibly correct transmis-\nsion errors, as well as protocols to keep a sender and receiver in the\nsame pace.\nNetwork layer Contains the protocols for routing a message through a com-\nputer network, as well as protocols for handling congestion.\nTransport layer Mainly contains protocols for directly supporting applica-\ntions, such as those that establish reliable communication, or support\nreal-time streaming of data.\nSession layer Provides support for sessions between applications.\nPresentation layer Prescribes how data is represented in a way that is inde-\npendent of the hosts on which communicating applications are running.\nApplication layer Essentially, everything else: e-mail protocols, Web access\nprotocols, \ufb01le-transfer protocols, and so on.\nWhen process Pwants to communicate with some remote process Q, it\nbuilds a message and passes that message to the application layer as offered\nto it by means of an interface. This interface will typically appear in the form\nof a library procedure. The application layer software then adds a header to\nthe front of the message and passes the resulting message across the layer 6/7\ninterface to the presentation layer. The presentation layer, in turn, adds its\nown header and passes the result down to the session layer, and so on. Some\nlayers add not only a header to the front, but also a trailer to the end. When\nit hits the bottom, the physical layer actually transmits the message (which\nby now might look as shown in Figure 4.2) by putting it onto the physical\ntransmission medium.\nFigure 4.2: A typical message as it appears on the network.\nWhen the message arrives at the remote machine hosting Q, it is passed\nupward, with each layer stripping off and examining its own header. Finally,\nthe message arrives at the receiver, process Q, which may reply to it using\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n166 CHAPTER 4. COMMUNICATION\nPhysical layer Deals with standardizing how two computers are connected\nand how 0s and 1s are represented.\nData link layer Provides the means to detect and possibly correct transmis-\nsion errors, as well as protocols to keep a sender and receiver in the\nsame pace.\nNetwork layer Contains the protocols for routing a message through a com-\nputer network, as well as protocols for handling congestion.\nTransport layer Mainly contains protocols for directly supporting applica-\ntions, such as those that establish reliable communication, or support\nreal-time streaming of data.\nSession layer Provides support for sessions between applications.\nPresentation layer Prescribes how data is represented in a way that is inde-\npendent of the hosts on which communicating applications are running.\nApplication layer Essentially, everything else: e-mail protocols, Web access\nprotocols, \ufb01le-transfer protocols, and so on.\nWhen process Pwants to communicate with some remote process Q, it\nbuilds a message and passes that message to the application layer as offered\nto it by means of an interface. This interface will typically appear in the form\nof a library procedure. The application layer software then adds a header to\nthe front of the message and passes the resulting message across the layer 6/7\ninterface to the presentation layer. The presentation layer, in turn, adds its\nown header and passes the result down to the session layer, and so on. Some\nlayers add not only a header to the front, but also a trailer to the end. When\nit hits the bottom, the physical layer actually transmits the message (which\nby now might look as shown in Figure 4.2) by putting it onto the physical\ntransmission medium.\nFigure 4.2: A typical message as it appears on the network.\nWhen the message arrives at the remote machine hosting Q, it is passed\nupward, with each layer stripping off and examining its own header. Finally,\nthe message arrives at the receiver, process Q, which may reply to it using\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.1. FOUNDATIONS 167\nthe reverse path. The information in the layer- nheader is used for the layer- n\nprotocol.\nIn the OSI model, there are not two layers, but seven, as we saw in\nFigure 4.1. The collection of protocols used in a particular system is called\naprotocol suite orprotocol stack . It is important to distinguish a reference\nmodel from its actual protocols . As said, the OSI protocols were never popular,\nin contrast to protocols developed for the Internet, such as TCP and IP .\nNote 4.1 (More information: Protocols in the OSI model)\nLet us brie\ufb02y examine each of the OSI layers in turn, starting at the bottom.\nInstead of giving examples of OSI protocols, where appropriate, we will point out\nsome of the Internet protocols used in each layer.\nLower-level protocols. The three lowest layers of the OSI protocol suite imple-\nment the basic functions that encompass a computer network.\nThe physical layer is concerned with transmitting the 0s and 1s. How many\nvolts to use for 0 and 1, how many bits per second can be sent, and whether\ntransmission can take place in both directions simultaneously are key issues in\nthe physical layer. In addition, the size and shape of the network connector (plug),\nas well as the number of pins and meaning of each are of concern here.\nThe physical layer protocol deals with standardizing the electrical, optical,\nmechanical, and signaling interfaces so that when one machine sends a 0 bit it is\nactually received as a 0 bit and not a 1 bit. Many physical layer standards have\nbeen developed (for different media), for example, the USB standard for serial\ncommunication lines.\nThe physical layer just sends bits. As long as no errors occur, all is well.\nHowever, real communication networks are subject to errors, so some mechanism\nis needed to detect and correct them. This mechanism is the main task of the data\nlink layer. What it does is to group the bits into units, sometimes called frames ,\nand see that each frame is correctly received.\nThe data link layer does its work by putting a special bit pattern on the start\nand end of each frame to mark them, as well as computing a checksum by adding\nup all the bytes in the frame in a certain way. The data link layer appends the\nchecksum to the frame. When the frame arrives, the receiver recomputes the\nchecksum from the data and compares the result to the checksum following\nthe frame. If the two agree, the frame is considered correct and is accepted. If\nthey disagree, the receiver asks the sender to retransmit it. Frames are assigned\nsequence numbers (in the header), so everyone can tell which is which.\nOn a LAN, there is usually no need for the sender to locate the receiver. It just\nputs the message out on the network and the receiver takes it off. A wide-area\nnetwork, however, consists of a large number of machines, each with some number\nof lines to other machines, rather like a large-scale map showing major cities and\nroads connecting them. For a message to get from the sender to the receiver it\nmay have to make a number of hops, at each one choosing an outgoing line to use.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.1. FOUNDATIONS 167\nthe reverse path. The information in the layer- nheader is used for the layer- n\nprotocol.\nIn the OSI model, there are not two layers, but seven, as we saw in\nFigure 4.1. The collection of protocols used in a particular system is called\naprotocol suite orprotocol stack . It is important to distinguish a reference\nmodel from its actual protocols . As said, the OSI protocols were never popular,\nin contrast to protocols developed for the Internet, such as TCP and IP .\nNote 4.1 (More information: Protocols in the OSI model)\nLet us brie\ufb02y examine each of the OSI layers in turn, starting at the bottom.\nInstead of giving examples of OSI protocols, where appropriate, we will point out\nsome of the Internet protocols used in each layer.\nLower-level protocols. The three lowest layers of the OSI protocol suite imple-\nment the basic functions that encompass a computer network.\nThe physical layer is concerned with transmitting the 0s and 1s. How many\nvolts to use for 0 and 1, how many bits per second can be sent, and whether\ntransmission can take place in both directions simultaneously are key issues in\nthe physical layer. In addition, the size and shape of the network connector (plug),\nas well as the number of pins and meaning of each are of concern here.\nThe physical layer protocol deals with standardizing the electrical, optical,\nmechanical, and signaling interfaces so that when one machine sends a 0 bit it is\nactually received as a 0 bit and not a 1 bit. Many physical layer standards have\nbeen developed (for different media), for example, the USB standard for serial\ncommunication lines.\nThe physical layer just sends bits. As long as no errors occur, all is well.\nHowever, real communication networks are subject to errors, so some mechanism\nis needed to detect and correct them. This mechanism is the main task of the data\nlink layer. What it does is to group the bits into units, sometimes called frames ,\nand see that each frame is correctly received.\nThe data link layer does its work by putting a special bit pattern on the start\nand end of each frame to mark them, as well as computing a checksum by adding\nup all the bytes in the frame in a certain way. The data link layer appends the\nchecksum to the frame. When the frame arrives, the receiver recomputes the\nchecksum from the data and compares the result to the checksum following\nthe frame. If the two agree, the frame is considered correct and is accepted. If\nthey disagree, the receiver asks the sender to retransmit it. Frames are assigned\nsequence numbers (in the header), so everyone can tell which is which.\nOn a LAN, there is usually no need for the sender to locate the receiver. It just\nputs the message out on the network and the receiver takes it off. A wide-area\nnetwork, however, consists of a large number of machines, each with some number\nof lines to other machines, rather like a large-scale map showing major cities and\nroads connecting them. For a message to get from the sender to the receiver it\nmay have to make a number of hops, at each one choosing an outgoing line to use.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "168 CHAPTER 4. COMMUNICATION\nThe question of how to choose the best path is called routing , and is essentially\nthe primary task of the network layer.\nThe problem is complicated by the fact that the shortest route is not always\nthe best route. What really matters is the amount of delay on a given route, which,\nin turn, is related to the amount of traf\ufb01c and the number of messages queued up\nfor transmission over the various lines. The delay can thus change over the course\nof time. Some routing algorithms try to adapt to changing loads, whereas others\nare content to make decisions based on long-term averages.\nAt present, the most widely used network protocol is the connectionless IP\n(Internet Protocol ), which is part of the Internet protocol suite. An IP packet (the\ntechnical term for a message in the network layer) can be sent without any setup.\nEach IP packet is routed to its destination independent of all others. No internal\npath is selected and remembered.\nTransport protocols. The transport layer forms the last part of what could be\ncalled a basic network protocol stack, in the sense that it implements all those\nservices that are not provided at the interface of the network layer, but which are\nreasonably needed to build network applications. In other words, the transport\nlayer turns the underlying network into something that an application developer\ncan use.\nPackets can be lost on the way from the sender to the receiver. Although\nsome applications can handle their own error recovery, others prefer a reliable\nconnection. The job of the transport layer is to provide this service. The idea is\nthat the application layer should be able to deliver a message to the transport\nlayer with the expectation that it will be delivered without loss.\nUpon receiving a message from the application layer, the transport layer\nbreaks it into pieces small enough for transmission, assigns each one a sequence\nnumber, and then sends them all. The discussion in the transport layer header\nconcerns which packets have been sent, which have been received, how many\nmore the receiver has room to accept, which should be retransmitted, and similar\ntopics.\nReliable transport connections (which by de\ufb01nition are connection-oriented)\ncan be built on top of connection-oriented or connectionless network services. In\nthe former case all the packets will arrive in the correct sequence (if they arrive at\nall), but in the latter case it is possible for one packet to take a different route and\narrive earlier than the packet sent before it. It is up to the transport layer software\nto put everything back in order to maintain the illusion that a transport connection\nis like a big tube\u2013you put messages into it and they come out undamaged and in\nthe same order in which they went in. Providing this end-to-end communication\nbehavior is an important aspect of the transport layer.\nThe Internet transport protocol is called TCP (Transmission Control Protocol )\nand is described in detail by Comer [2013]. The combination TCP/IP is now used\nas a de facto standard for network communication. The Internet protocol suite also\nsupports a connectionless transport protocol called UDP (Universal Datagram\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n168 CHAPTER 4. COMMUNICATION\nThe question of how to choose the best path is called routing , and is essentially\nthe primary task of the network layer.\nThe problem is complicated by the fact that the shortest route is not always\nthe best route. What really matters is the amount of delay on a given route, which,\nin turn, is related to the amount of traf\ufb01c and the number of messages queued up\nfor transmission over the various lines. The delay can thus change over the course\nof time. Some routing algorithms try to adapt to changing loads, whereas others\nare content to make decisions based on long-term averages.\nAt present, the most widely used network protocol is the connectionless IP\n(Internet Protocol ), which is part of the Internet protocol suite. An IP packet (the\ntechnical term for a message in the network layer) can be sent without any setup.\nEach IP packet is routed to its destination independent of all others. No internal\npath is selected and remembered.\nTransport protocols. The transport layer forms the last part of what could be\ncalled a basic network protocol stack, in the sense that it implements all those\nservices that are not provided at the interface of the network layer, but which are\nreasonably needed to build network applications. In other words, the transport\nlayer turns the underlying network into something that an application developer\ncan use.\nPackets can be lost on the way from the sender to the receiver. Although\nsome applications can handle their own error recovery, others prefer a reliable\nconnection. The job of the transport layer is to provide this service. The idea is\nthat the application layer should be able to deliver a message to the transport\nlayer with the expectation that it will be delivered without loss.\nUpon receiving a message from the application layer, the transport layer\nbreaks it into pieces small enough for transmission, assigns each one a sequence\nnumber, and then sends them all. The discussion in the transport layer header\nconcerns which packets have been sent, which have been received, how many\nmore the receiver has room to accept, which should be retransmitted, and similar\ntopics.\nReliable transport connections (which by de\ufb01nition are connection-oriented)\ncan be built on top of connection-oriented or connectionless network services. In\nthe former case all the packets will arrive in the correct sequence (if they arrive at\nall), but in the latter case it is possible for one packet to take a different route and\narrive earlier than the packet sent before it. It is up to the transport layer software\nto put everything back in order to maintain the illusion that a transport connection\nis like a big tube\u2013you put messages into it and they come out undamaged and in\nthe same order in which they went in. Providing this end-to-end communication\nbehavior is an important aspect of the transport layer.\nThe Internet transport protocol is called TCP (Transmission Control Protocol )\nand is described in detail by Comer [2013]. The combination TCP/IP is now used\nas a de facto standard for network communication. The Internet protocol suite also\nsupports a connectionless transport protocol called UDP (Universal Datagram\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.1. FOUNDATIONS 169\nProtocol ), which is essentially just IP with some minor additions. User programs\nthat do not need a connection-oriented protocol normally use UDP .\nAdditional transport protocols are regularly proposed. For example, to sup-\nport real-time data transfer, the Real-time Transport Protocol (RTP ) has been\nde\ufb01ned. RTP is a framework protocol in the sense that it speci\ufb01es packet formats\nfor real-time data without providing the actual mechanisms for guaranteeing\ndata delivery. In addition, it speci\ufb01es a protocol for monitoring and controlling\ndata transfer of RTP packets [Schulzrinne et al., 2003]. Likewise, the Streaming\nControl Transmission Protocol (SCTP ) has been proposed as an alternative to\nTCP [Stewart, 2007]. The main difference between SCTP and TCP is that SCTP\ngroups data into messages, whereas TCP merely moves bytes between processes.\nDoing so may simplify application development.\nHigher-level protocols. Above the transport layer, OSI distinguishes three addi-\ntional layers. In practice, only the application layer is ever used. In fact, in the\nInternet protocol suite, everything above the transport layer is grouped together.\nIn the face of middleware systems, we shall see that neither the OSI nor the\nInternet approach is really appropriate.\nThe session layer is essentially an enhanced version of the transport layer. It\nprovides dialog control, to keep track of which party is currently talking, and it\nprovides synchronization facilities. The latter are useful to allow users to insert\ncheckpoints into long transfers, so that in the event of a crash, it is necessary to\ngo back only to the last checkpoint, rather than all the way back to the beginning.\nIn practice, few applications are interested in the session layer and it is rarely\nsupported. It is not even present in the Internet protocol suite. However, in\nthe context of developing middleware solutions, the concept of a session and\nits related protocols has turned out to be quite relevant, notably when de\ufb01ning\nhigher-level communication protocols.\nUnlike the lower layers, which are concerned with getting the bits from the\nsender to the receiver reliably and ef\ufb01ciently, the presentation layer is concerned\nwith the meaning of the bits. Most messages do not consist of random bit strings,\nbut more structured information such as people\u2019s names, addresses, amounts\nof money, and so on. In the presentation layer it is possible to de\ufb01ne records\ncontaining \ufb01elds like these and then have the sender notify the receiver that a\nmessage contains a particular record in a certain format. This makes it easier for\nmachines with different internal representations to communicate with each other.\nThe OSI application layer was originally intended to contain a collection of\nstandard network applications such as those for electronic mail, \ufb01le transfer, and\nterminal emulation. By now, it has become the container for all applications and\nprotocols that in one way or the other do not \ufb01t into one of the underlying layers.\nFrom the perspective of the OSI reference model, virtually all distributed systems\nare just applications.\nWhat is missing in this model is a clear distinction between applications,\napplication-speci\ufb01c protocols, and general-purpose protocols. For example, the\nInternet File Transfer Protocol (FTP) [Postel and Reynolds, 1985; Horowitz and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.1. FOUNDATIONS 169\nProtocol ), which is essentially just IP with some minor additions. User programs\nthat do not need a connection-oriented protocol normally use UDP .\nAdditional transport protocols are regularly proposed. For example, to sup-\nport real-time data transfer, the Real-time Transport Protocol (RTP ) has been\nde\ufb01ned. RTP is a framework protocol in the sense that it speci\ufb01es packet formats\nfor real-time data without providing the actual mechanisms for guaranteeing\ndata delivery. In addition, it speci\ufb01es a protocol for monitoring and controlling\ndata transfer of RTP packets [Schulzrinne et al., 2003]. Likewise, the Streaming\nControl Transmission Protocol (SCTP ) has been proposed as an alternative to\nTCP [Stewart, 2007]. The main difference between SCTP and TCP is that SCTP\ngroups data into messages, whereas TCP merely moves bytes between processes.\nDoing so may simplify application development.\nHigher-level protocols. Above the transport layer, OSI distinguishes three addi-\ntional layers. In practice, only the application layer is ever used. In fact, in the\nInternet protocol suite, everything above the transport layer is grouped together.\nIn the face of middleware systems, we shall see that neither the OSI nor the\nInternet approach is really appropriate.\nThe session layer is essentially an enhanced version of the transport layer. It\nprovides dialog control, to keep track of which party is currently talking, and it\nprovides synchronization facilities. The latter are useful to allow users to insert\ncheckpoints into long transfers, so that in the event of a crash, it is necessary to\ngo back only to the last checkpoint, rather than all the way back to the beginning.\nIn practice, few applications are interested in the session layer and it is rarely\nsupported. It is not even present in the Internet protocol suite. However, in\nthe context of developing middleware solutions, the concept of a session and\nits related protocols has turned out to be quite relevant, notably when de\ufb01ning\nhigher-level communication protocols.\nUnlike the lower layers, which are concerned with getting the bits from the\nsender to the receiver reliably and ef\ufb01ciently, the presentation layer is concerned\nwith the meaning of the bits. Most messages do not consist of random bit strings,\nbut more structured information such as people\u2019s names, addresses, amounts\nof money, and so on. In the presentation layer it is possible to de\ufb01ne records\ncontaining \ufb01elds like these and then have the sender notify the receiver that a\nmessage contains a particular record in a certain format. This makes it easier for\nmachines with different internal representations to communicate with each other.\nThe OSI application layer was originally intended to contain a collection of\nstandard network applications such as those for electronic mail, \ufb01le transfer, and\nterminal emulation. By now, it has become the container for all applications and\nprotocols that in one way or the other do not \ufb01t into one of the underlying layers.\nFrom the perspective of the OSI reference model, virtually all distributed systems\nare just applications.\nWhat is missing in this model is a clear distinction between applications,\napplication-speci\ufb01c protocols, and general-purpose protocols. For example, the\nInternet File Transfer Protocol (FTP) [Postel and Reynolds, 1985; Horowitz and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "170 CHAPTER 4. COMMUNICATION\nLunt, 1997] de\ufb01nes a protocol for transferring \ufb01les between a client and server\nmachine. The protocol should not be confused with the ftpprogram, which\nis an end-user application for transferring \ufb01les and which also (not entirely by\ncoincidence) happens to implement the Internet FTP .\nAnother example of a typical application-speci\ufb01c protocol is the HyperText\nTransfer Protocol (HTTP ) [Fielding et al., 1999] which is designed to remotely\nmanage and handle the transfer of Web pages. The protocol is implemented by\napplications such as Web browsers and Web servers. However, HTTP is now also\nused by systems that are not intrinsically tied to the Web. For example, Java\u2019s\nobject-invocation mechanism can use HTTP to request the invocation of remote\nobjects that are protected by a \ufb01rewall [Oracle, 2010].\nThere are also many general-purpose protocols that are useful to many appli-\ncations, but which cannot be quali\ufb01ed as transport protocols. In many cases, such\nprotocols fall into the category of middleware protocols.\nMiddleware protocols\nMiddleware is an application that logically lives (mostly) in the OSI application\nlayer, but which contains many general-purpose protocols that warrant their\nown layers, independent of other, more speci\ufb01c applications. Let us brie\ufb02y\nlook at some examples.\nThe Domain Name System (DNS ) [Liu and Albitz, 2006] is a distributed\nservice that is used to look up a network address associated with a name, such\nas the address of a so-called domain name like www .distributed -systems .net.\nIn terms of the OSI reference model, DNS is an application and therefore\nis logically placed in the application layer. However, it should be quite\nobvious that DNS is offering a general-purpose, application-independent\nservice. Arguably, it forms part of the middleware.\nAs another example, there are various ways to establish authentication,\nthat is, provide proof of a claimed identity. Authentication protocols are\nnot closely tied to any speci\ufb01c application, but instead, can be integrated\ninto a middleware system as a general service. Likewise, authorization\nprotocols by which authenticated users and processes are granted access only\nto those resources for which they have authorization, tend to have a general,\napplication-independent nature. Being labeled as applications in the OSI\nreference model, these are clear examples that belong in the middleware.\nDistributed commit protocols establish that in a group of processes, possi-\nbly spread out across a number of machines, either all processes carry out a\nparticular operation, or that the operation is not carried out at all. This phe-\nnomenon is also referred to as atomicity and is widely applied in transactions.\nAs it turns out, commit protocols can present an interface independently of\nspeci\ufb01c applications, thus providing a general-purpose transaction service.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n170 CHAPTER 4. COMMUNICATION\nLunt, 1997] de\ufb01nes a protocol for transferring \ufb01les between a client and server\nmachine. The protocol should not be confused with the ftpprogram, which\nis an end-user application for transferring \ufb01les and which also (not entirely by\ncoincidence) happens to implement the Internet FTP .\nAnother example of a typical application-speci\ufb01c protocol is the HyperText\nTransfer Protocol (HTTP ) [Fielding et al., 1999] which is designed to remotely\nmanage and handle the transfer of Web pages. The protocol is implemented by\napplications such as Web browsers and Web servers. However, HTTP is now also\nused by systems that are not intrinsically tied to the Web. For example, Java\u2019s\nobject-invocation mechanism can use HTTP to request the invocation of remote\nobjects that are protected by a \ufb01rewall [Oracle, 2010].\nThere are also many general-purpose protocols that are useful to many appli-\ncations, but which cannot be quali\ufb01ed as transport protocols. In many cases, such\nprotocols fall into the category of middleware protocols.\nMiddleware protocols\nMiddleware is an application that logically lives (mostly) in the OSI application\nlayer, but which contains many general-purpose protocols that warrant their\nown layers, independent of other, more speci\ufb01c applications. Let us brie\ufb02y\nlook at some examples.\nThe Domain Name System (DNS ) [Liu and Albitz, 2006] is a distributed\nservice that is used to look up a network address associated with a name, such\nas the address of a so-called domain name like www .distributed -systems .net.\nIn terms of the OSI reference model, DNS is an application and therefore\nis logically placed in the application layer. However, it should be quite\nobvious that DNS is offering a general-purpose, application-independent\nservice. Arguably, it forms part of the middleware.\nAs another example, there are various ways to establish authentication,\nthat is, provide proof of a claimed identity. Authentication protocols are\nnot closely tied to any speci\ufb01c application, but instead, can be integrated\ninto a middleware system as a general service. Likewise, authorization\nprotocols by which authenticated users and processes are granted access only\nto those resources for which they have authorization, tend to have a general,\napplication-independent nature. Being labeled as applications in the OSI\nreference model, these are clear examples that belong in the middleware.\nDistributed commit protocols establish that in a group of processes, possi-\nbly spread out across a number of machines, either all processes carry out a\nparticular operation, or that the operation is not carried out at all. This phe-\nnomenon is also referred to as atomicity and is widely applied in transactions.\nAs it turns out, commit protocols can present an interface independently of\nspeci\ufb01c applications, thus providing a general-purpose transaction service.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.1. FOUNDATIONS 171\nIn such a form, they typically belong to the middleware and not to the OSI\napplication layer.\nAs a last example, consider a distributed locking protocol by which a\nresource can be protected against simultaneous access by a collection of pro-\ncesses that are distributed across multiple machines. It is not hard to imagine\nthat such protocols can be designed in an application-independent fashion,\nand accessible through a relatively simple, again application-independent\ninterface. As such, they generally belong in the middleware.\nThese protocol examples are not directly tied to communication, yet there\nare also many middleware communication protocols. For example, with\na so-called remote procedure call , a process is offered a facility to locally\ncall a procedure that is effectively implemented on a remote machine. This\ncommunication service belongs to one of the oldest types of middleware\nservices and is used for realizing access transparency. In a similar vein, there\nare high-level communication services for setting and synchronizing streams\nfor transferring real-time data, such as needed for multimedia applications.\nAs a last example, some middleware systems offer reliable multicast services\nthat scale to thousands of receivers spread across a wide-area network.\nFigure 4.3: An adapted reference model for networked communication.\nTaking this approach to layering leads to the adapted and simpli\ufb01ed\nreference model for communication, as shown in Figure 4.3. Compared to\nthe OSI model, the session and presentation layer have been replaced by\na single middleware layer that contains application-independent protocols.\nThese protocols do not belong in the lower layers we just discussed. Network\nand transport services have been grouped into communication services as\nnormally offered by an operating system, which, in turn, manages the speci\ufb01c\nlowest-level hardware used to establish communication.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.1. FOUNDATIONS 171\nIn such a form, they typically belong to the middleware and not to the OSI\napplication layer.\nAs a last example, consider a distributed locking protocol by which a\nresource can be protected against simultaneous access by a collection of pro-\ncesses that are distributed across multiple machines. It is not hard to imagine\nthat such protocols can be designed in an application-independent fashion,\nand accessible through a relatively simple, again application-independent\ninterface. As such, they generally belong in the middleware.\nThese protocol examples are not directly tied to communication, yet there\nare also many middleware communication protocols. For example, with\na so-called remote procedure call , a process is offered a facility to locally\ncall a procedure that is effectively implemented on a remote machine. This\ncommunication service belongs to one of the oldest types of middleware\nservices and is used for realizing access transparency. In a similar vein, there\nare high-level communication services for setting and synchronizing streams\nfor transferring real-time data, such as needed for multimedia applications.\nAs a last example, some middleware systems offer reliable multicast services\nthat scale to thousands of receivers spread across a wide-area network.\nFigure 4.3: An adapted reference model for networked communication.\nTaking this approach to layering leads to the adapted and simpli\ufb01ed\nreference model for communication, as shown in Figure 4.3. Compared to\nthe OSI model, the session and presentation layer have been replaced by\na single middleware layer that contains application-independent protocols.\nThese protocols do not belong in the lower layers we just discussed. Network\nand transport services have been grouped into communication services as\nnormally offered by an operating system, which, in turn, manages the speci\ufb01c\nlowest-level hardware used to establish communication.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "172 CHAPTER 4. COMMUNICATION\nTypes of Communication\nIn the remainder of this chapter, we concentrate on high-level middleware\ncommunication services. Before doing so, there are other general criteria\nfor distinguishing (middleware) communication. To understand the various\nalternatives in communication that middleware can offer to applications, we\nview the middleware as an additional service in client-server computing, as\nshown in Figure 4.4. Consider, for example an electronic mail system. In\nprinciple, the core of the mail delivery system can be seen as a middleware\ncommunication service. Each host runs a user agent allowing users to com-\npose, send, and receive e-mail. A sending user agent passes such mail to the\nmail delivery system, expecting it, in turn, to eventually deliver the mail to\nthe intended recipient. Likewise, the user agent at the receiver\u2019s side connects\nto the mail delivery system to see whether any mail has come in. If so, the\nmessages are transferred to the user agent so that they can be displayed and\nread by the user.\nFigure 4.4: Viewing middleware as an intermediate (distributed) service in\napplication-level communication.\nAn electronic mail system is a typical example in which communication\nis persistent. With persistent communication , a message that has been sub-\nmitted for transmission is stored by the communication middleware as long\nas it takes to deliver it to the receiver. In this case, the middleware will store\nthe message at one or several of the storage facilities shown in Figure 4.4.\nAs a consequence, it is not necessary for the sending application to continue\nexecution after submitting the message. Likewise, the receiving application\nneed not be executing when the message is submitted.\nIn contrast, with transient communication , a message is stored by the\ncommunication system only as long as the sending and receiving application\nare executing. More precisely, in terms of Figure 4.4, if the middleware cannot\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n172 CHAPTER 4. COMMUNICATION\nTypes of Communication\nIn the remainder of this chapter, we concentrate on high-level middleware\ncommunication services. Before doing so, there are other general criteria\nfor distinguishing (middleware) communication. To understand the various\nalternatives in communication that middleware can offer to applications, we\nview the middleware as an additional service in client-server computing, as\nshown in Figure 4.4. Consider, for example an electronic mail system. In\nprinciple, the core of the mail delivery system can be seen as a middleware\ncommunication service. Each host runs a user agent allowing users to com-\npose, send, and receive e-mail. A sending user agent passes such mail to the\nmail delivery system, expecting it, in turn, to eventually deliver the mail to\nthe intended recipient. Likewise, the user agent at the receiver\u2019s side connects\nto the mail delivery system to see whether any mail has come in. If so, the\nmessages are transferred to the user agent so that they can be displayed and\nread by the user.\nFigure 4.4: Viewing middleware as an intermediate (distributed) service in\napplication-level communication.\nAn electronic mail system is a typical example in which communication\nis persistent. With persistent communication , a message that has been sub-\nmitted for transmission is stored by the communication middleware as long\nas it takes to deliver it to the receiver. In this case, the middleware will store\nthe message at one or several of the storage facilities shown in Figure 4.4.\nAs a consequence, it is not necessary for the sending application to continue\nexecution after submitting the message. Likewise, the receiving application\nneed not be executing when the message is submitted.\nIn contrast, with transient communication , a message is stored by the\ncommunication system only as long as the sending and receiving application\nare executing. More precisely, in terms of Figure 4.4, if the middleware cannot\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 173\ndeliver a message due to a transmission interrupt, or because the recipient is\ncurrently not active, it will simply be discarded. Typically, all transport-level\ncommunication services offer only transient communication. In this case, the\ncommunication system consists of traditional store-and-forward routers. If a\nrouter cannot deliver a message to the next one or the destination host, it will\nsimply drop the message.\nBesides being persistent or transient, communication can also be asyn-\nchronous or synchronous. The characteristic feature of asynchronous com-\nmunication is that a sender continues immediately after it has submitted its\nmessage for transmission. This means that the message is (temporarily) stored\nimmediately by the middleware upon submission. With synchronous com-\nmunication , the sender is blocked until its request is known to be accepted.\nThere are essentially three points where synchronization can take place. First,\nthe sender may be blocked until the middleware noti\ufb01es that it will take over\ntransmission of the request. Second, the sender may synchronize until its\nrequest has been delivered to the intended recipient. Third, synchronization\nmay take place by letting the sender wait until its request has been fully\nprocessed, that is, up to the time that the recipient returns a response.\nVarious combinations of persistence and synchronization occur in practice.\nPopular ones are persistence in combination with synchronization at request\nsubmission, which is a common scheme for many message-queuing systems,\nwhich we discuss later in this chapter. Likewise, transient communication\nwith synchronization after the request has been fully processed is also widely\nused. This scheme corresponds with remote procedure calls, which we discuss\nnext.\n4.2 Remote procedure call\nMany distributed systems have been based on explicit message exchange\nbetween processes. However, the operations send and receive do not conceal\ncommunication at all, which is important to achieve access transparency in\ndistributed systems. This problem has long been known, but little was done\nabout it until researchers in the 1980s [Birrell and Nelson, 1984] introduced\na completely different way of handling communication. Although the idea\nis refreshingly simple (once someone has thought of it), the implications are\noften subtle. In this section we will examine the concept, its implementation,\nits strengths, and its weaknesses.\nIn a nutshell, the proposal was to allow programs to call procedures\nlocated on other machines. When a process on machine Acalls a procedure on\nmachine B, the calling process on Ais suspended, and execution of the called\nprocedure takes place on B. Information can be transported from the caller to\nthe callee in the parameters and can come back in the procedure result. No\nmessage passing at all is visible to the programmer. This method is known as\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 173\ndeliver a message due to a transmission interrupt, or because the recipient is\ncurrently not active, it will simply be discarded. Typically, all transport-level\ncommunication services offer only transient communication. In this case, the\ncommunication system consists of traditional store-and-forward routers. If a\nrouter cannot deliver a message to the next one or the destination host, it will\nsimply drop the message.\nBesides being persistent or transient, communication can also be asyn-\nchronous or synchronous. The characteristic feature of asynchronous com-\nmunication is that a sender continues immediately after it has submitted its\nmessage for transmission. This means that the message is (temporarily) stored\nimmediately by the middleware upon submission. With synchronous com-\nmunication , the sender is blocked until its request is known to be accepted.\nThere are essentially three points where synchronization can take place. First,\nthe sender may be blocked until the middleware noti\ufb01es that it will take over\ntransmission of the request. Second, the sender may synchronize until its\nrequest has been delivered to the intended recipient. Third, synchronization\nmay take place by letting the sender wait until its request has been fully\nprocessed, that is, up to the time that the recipient returns a response.\nVarious combinations of persistence and synchronization occur in practice.\nPopular ones are persistence in combination with synchronization at request\nsubmission, which is a common scheme for many message-queuing systems,\nwhich we discuss later in this chapter. Likewise, transient communication\nwith synchronization after the request has been fully processed is also widely\nused. This scheme corresponds with remote procedure calls, which we discuss\nnext.\n4.2 Remote procedure call\nMany distributed systems have been based on explicit message exchange\nbetween processes. However, the operations send and receive do not conceal\ncommunication at all, which is important to achieve access transparency in\ndistributed systems. This problem has long been known, but little was done\nabout it until researchers in the 1980s [Birrell and Nelson, 1984] introduced\na completely different way of handling communication. Although the idea\nis refreshingly simple (once someone has thought of it), the implications are\noften subtle. In this section we will examine the concept, its implementation,\nits strengths, and its weaknesses.\nIn a nutshell, the proposal was to allow programs to call procedures\nlocated on other machines. When a process on machine Acalls a procedure on\nmachine B, the calling process on Ais suspended, and execution of the called\nprocedure takes place on B. Information can be transported from the caller to\nthe callee in the parameters and can come back in the procedure result. No\nmessage passing at all is visible to the programmer. This method is known as\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "174 CHAPTER 4. COMMUNICATION\nRemote Procedure Call , or often just RPC .\nWhile the basic idea sounds simple and elegant, subtle problems exist.\nTo start with, because the calling and called procedures run on different ma-\nchines, they execute in different address spaces, which causes complications.\nParameters and results also have to be passed, which can be complicated,\nespecially if the machines are not identical. Finally, either or both machines\ncan crash and each of the possible failures causes different problems. Still,\nmost of these can be dealt with, and RPC is a widely-used technique that\nunderlies many distributed systems.\nBasic RPC operation\nThe idea behind RPC is to make a remote procedure call look as much as\npossible like a local one. In other words, we want RPC to be transparent\u2014the\ncalling procedure should not be aware that the called procedure is executing\non a different machine or vice versa. Suppose that a program has access to a\ndatabase that allows it to append data to a stored list, after which it returns a\nreference to the modi\ufb01ed list. The operation is made available to a program\nby means of a routine append :\nnewlist = append(data, dbList)\nIn a traditional (single-processor) system, append is extracted from a library\nby the linker and inserted into the object program. In principle, it can be a\nshort procedure, which could be implemented by a few \ufb01le operations for\naccessing the database.\nEven though append eventually does only a few basic \ufb01le operations, it\nis called in the usual way, by pushing its parameters onto the stack. The\nprogrammer does not know the implementation details of append , and this is,\nof course, how it is supposed to be.\nNote 4.2 (More information: Conventional procedure calls)\nTo understand how RPC works and some of its pitfalls, it may help to \ufb01rst under-\nstand how a conventional (i.e., single machine) procedure call works. Consider\nthe following operation.\nnewlist = append(data, dbList);\nWe assume that the purpose of this call is to take a globally de\ufb01ned list object,\nreferred here to as dbList , and append a simple data element to it represented\nby the variable data . An important observation is that in various programming\nlanguages such as C, dbList is implemented as a reference to a list object (i.e.,\na pointer), whereas data may be represented directly by its value (which we\nassume to be the case here). When calling append , both the representations of data\nand dbList are pushed onto the stack, making those representations accessible\nto the implementation of append . For data , this means the variable follows a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n174 CHAPTER 4. COMMUNICATION\nRemote Procedure Call , or often just RPC .\nWhile the basic idea sounds simple and elegant, subtle problems exist.\nTo start with, because the calling and called procedures run on different ma-\nchines, they execute in different address spaces, which causes complications.\nParameters and results also have to be passed, which can be complicated,\nespecially if the machines are not identical. Finally, either or both machines\ncan crash and each of the possible failures causes different problems. Still,\nmost of these can be dealt with, and RPC is a widely-used technique that\nunderlies many distributed systems.\nBasic RPC operation\nThe idea behind RPC is to make a remote procedure call look as much as\npossible like a local one. In other words, we want RPC to be transparent\u2014the\ncalling procedure should not be aware that the called procedure is executing\non a different machine or vice versa. Suppose that a program has access to a\ndatabase that allows it to append data to a stored list, after which it returns a\nreference to the modi\ufb01ed list. The operation is made available to a program\nby means of a routine append :\nnewlist = append(data, dbList)\nIn a traditional (single-processor) system, append is extracted from a library\nby the linker and inserted into the object program. In principle, it can be a\nshort procedure, which could be implemented by a few \ufb01le operations for\naccessing the database.\nEven though append eventually does only a few basic \ufb01le operations, it\nis called in the usual way, by pushing its parameters onto the stack. The\nprogrammer does not know the implementation details of append , and this is,\nof course, how it is supposed to be.\nNote 4.2 (More information: Conventional procedure calls)\nTo understand how RPC works and some of its pitfalls, it may help to \ufb01rst under-\nstand how a conventional (i.e., single machine) procedure call works. Consider\nthe following operation.\nnewlist = append(data, dbList);\nWe assume that the purpose of this call is to take a globally de\ufb01ned list object,\nreferred here to as dbList , and append a simple data element to it represented\nby the variable data . An important observation is that in various programming\nlanguages such as C, dbList is implemented as a reference to a list object (i.e.,\na pointer), whereas data may be represented directly by its value (which we\nassume to be the case here). When calling append , both the representations of data\nand dbList are pushed onto the stack, making those representations accessible\nto the implementation of append . For data , this means the variable follows a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 175\ncopy-by-value policy, the policy for dblist iscopy-by-reference . What happens\nbefore and during the call is shown in Figure 4.5.\nSeveral things are worth noting. For one, a value parameter such as data , is\njust an initialized local variable. The called procedure may modify it, but such\nchanges do not affect the original value at the calling side.\nWhen a parameter like dbList is actually a pointer to a variable rather than\nthe value of the variable, something else happens. What is pushed onto the stack\nis the address of the list object as stored in main memory. When the value of data\nis appended to the list, a call to append does modify the list object. The difference\nbetween call-by-value and call-by-reference is quite important for RPC.\n(a) (b)\nFigure 4.5: (a) Parameter passing in a local procedure call: the stack\nbefore the call to append . (b) The stack while the called procedure is\nactive.\nOne other parameter passing mechanism also exists, although it is not used\nin most programming languages. It is called call-by-copy/restore . It consists\nof having the variable copied to the stack by the caller, as in call-by-value, and\nthen copied back after the call, overwriting the caller\u2019s original value. Under\nmost conditions, this achieves exactly the same effect as call-by-reference, but in\nsome situations, such as the same parameter being present multiple times in the\nparameter list, the semantics are different.\nThe decision of which parameter passing mechanism to use is normally made\nby the language designers and is a \ufb01xed property of the language. Sometimes\nit depends on the data type being passed. In C, for example, integers and\nother scalar types are always passed by value, whereas arrays are always passed\nby reference. Some Ada compilers use copy/restore for inout parameters, but\nothers use call-by-reference. The language de\ufb01nition permits either choice, which\nmakes the semantics a bit fuzzy. In Python, all variables are passed by reference,\nbut some actually get copied to local variables, thus mimicking the behavior of\ncopy-by-value.\nRPC achieves its transparency in an analogous way. When append is\nactually a remote procedure, a different version of append , called a client\nstub , is offered to the calling client. Like the original one, it, too, is called\nusing a normal calling sequence. However, unlike the original one, it does not\nperform an append operation. Instead, it packs the parameters into a message\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 175\ncopy-by-value policy, the policy for dblist iscopy-by-reference . What happens\nbefore and during the call is shown in Figure 4.5.\nSeveral things are worth noting. For one, a value parameter such as data , is\njust an initialized local variable. The called procedure may modify it, but such\nchanges do not affect the original value at the calling side.\nWhen a parameter like dbList is actually a pointer to a variable rather than\nthe value of the variable, something else happens. What is pushed onto the stack\nis the address of the list object as stored in main memory. When the value of data\nis appended to the list, a call to append does modify the list object. The difference\nbetween call-by-value and call-by-reference is quite important for RPC.\n(a) (b)\nFigure 4.5: (a) Parameter passing in a local procedure call: the stack\nbefore the call to append . (b) The stack while the called procedure is\nactive.\nOne other parameter passing mechanism also exists, although it is not used\nin most programming languages. It is called call-by-copy/restore . It consists\nof having the variable copied to the stack by the caller, as in call-by-value, and\nthen copied back after the call, overwriting the caller\u2019s original value. Under\nmost conditions, this achieves exactly the same effect as call-by-reference, but in\nsome situations, such as the same parameter being present multiple times in the\nparameter list, the semantics are different.\nThe decision of which parameter passing mechanism to use is normally made\nby the language designers and is a \ufb01xed property of the language. Sometimes\nit depends on the data type being passed. In C, for example, integers and\nother scalar types are always passed by value, whereas arrays are always passed\nby reference. Some Ada compilers use copy/restore for inout parameters, but\nothers use call-by-reference. The language de\ufb01nition permits either choice, which\nmakes the semantics a bit fuzzy. In Python, all variables are passed by reference,\nbut some actually get copied to local variables, thus mimicking the behavior of\ncopy-by-value.\nRPC achieves its transparency in an analogous way. When append is\nactually a remote procedure, a different version of append , called a client\nstub , is offered to the calling client. Like the original one, it, too, is called\nusing a normal calling sequence. However, unlike the original one, it does not\nperform an append operation. Instead, it packs the parameters into a message\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "176 CHAPTER 4. COMMUNICATION\nand requests that message to be sent to the server as illustrated in Figure 4.6.\nFollowing the call to send , the client stub calls receive , blocking itself until\nthe reply comes back.\nFigure 4.6: The principle of RPC between a client and server program.\nWhen the message arrives at the server, the server\u2019s operating system\npasses it to a server stub . A server stub is the server-side equivalent of a client\nstub: it is a piece of code that transforms requests coming in over the network\ninto local procedure calls. Typically the server stub will have called receive\nand be blocked waiting for incoming messages. The server stub unpacks the\nparameters from the message and then calls the server procedure in the usual\nway. From the server\u2019s point of view, it is as though it is being called directly\nby the client\u2014the parameters and return address are all on the stack where\nthey belong and nothing seems unusual. The server performs its work and\nthen returns the result to the caller (in this case the server stub) in the usual\nway.\nWhen the server stub gets control back after the call has completed, it\npacks the result in a message and calls send to return it to the client. After\nthat, the server stub usually does a call to receive again, to wait for the next\nincoming request.\nWhen the result message arrives at the client\u2019s machine, the operating\nsystem passes it through the receive operation, which had been called pre-\nviously, to the client stub, and the client process is subsequently unblocked.\nThe client stub inspects the message, unpacks the result, copies it to its caller,\nand returns in the usual way. When the caller gets control following the call\ntoappend , all it knows is that it appended some data to a list. It has no idea\nthat the work was done remotely at another machine.\nThis blissful ignorance on the part of the client is the beauty of the whole\nscheme. As far as it is concerned, remote services are accessed by making\nordinary (i.e., local) procedure calls, not by calling send and receive . All\nthe details of the message passing are hidden away in the two library proce-\ndures, just as the details of actually making system calls are hidden away in\ntraditional libraries.\nTo summarize, a remote procedure call occurs in the following steps:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n176 CHAPTER 4. COMMUNICATION\nand requests that message to be sent to the server as illustrated in Figure 4.6.\nFollowing the call to send , the client stub calls receive , blocking itself until\nthe reply comes back.\nFigure 4.6: The principle of RPC between a client and server program.\nWhen the message arrives at the server, the server\u2019s operating system\npasses it to a server stub . A server stub is the server-side equivalent of a client\nstub: it is a piece of code that transforms requests coming in over the network\ninto local procedure calls. Typically the server stub will have called receive\nand be blocked waiting for incoming messages. The server stub unpacks the\nparameters from the message and then calls the server procedure in the usual\nway. From the server\u2019s point of view, it is as though it is being called directly\nby the client\u2014the parameters and return address are all on the stack where\nthey belong and nothing seems unusual. The server performs its work and\nthen returns the result to the caller (in this case the server stub) in the usual\nway.\nWhen the server stub gets control back after the call has completed, it\npacks the result in a message and calls send to return it to the client. After\nthat, the server stub usually does a call to receive again, to wait for the next\nincoming request.\nWhen the result message arrives at the client\u2019s machine, the operating\nsystem passes it through the receive operation, which had been called pre-\nviously, to the client stub, and the client process is subsequently unblocked.\nThe client stub inspects the message, unpacks the result, copies it to its caller,\nand returns in the usual way. When the caller gets control following the call\ntoappend , all it knows is that it appended some data to a list. It has no idea\nthat the work was done remotely at another machine.\nThis blissful ignorance on the part of the client is the beauty of the whole\nscheme. As far as it is concerned, remote services are accessed by making\nordinary (i.e., local) procedure calls, not by calling send and receive . All\nthe details of the message passing are hidden away in the two library proce-\ndures, just as the details of actually making system calls are hidden away in\ntraditional libraries.\nTo summarize, a remote procedure call occurs in the following steps:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 177\nFigure 4.7: The steps involved in calling a remote procedure doit(a,b) . The\nreturn path for the result is not shown.\n1. The client procedure calls the client stub in the normal way.\n2. The client stub builds a message and calls the local operating system.\n3. The client\u2019s OS sends the message to the remote OS.\n4. The remote OS gives the message to the server stub.\n5. The server stub unpacks the parameter(s) and calls the server.\n6. The server does the work and returns the result to the stub.\n7. The server stub packs the result in a message and calls its local OS.\n8. The server\u2019s OS sends the message to the client\u2019s OS.\n9. The client\u2019s OS gives the message to the client stub.\n10. The stub unpacks the result and returns it to the client.\nThe \ufb01rst steps are shown in Figure 4.7 for an abstract two-parameter\nprocedure doit(a,b) , where we assume that parameter ais of type type1 , and\nbof type type2 . The net effect of all these steps is to convert the local call by\nthe client procedure to the client stub, to a local call to the server procedure\nwithout either client or server being aware of the intermediate steps or the\nexistence of the network.\nNote 4.3 (More information: An example in Python)\nTo make matters concrete, let us consider how a remote procedure call could\nbe implemented for the operation append discussed previously. Take a look at\nthe Python code shown in Figure 4.8 (from which we omit nonessential code\nfragments).\nThe class DBList is a simple representation of a list object, mimicking what one\nwould expect to see in a version that would be found in a database environment.\nThe client stub, represented by the class Client , consists of an implementation\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 177\nFigure 4.7: The steps involved in calling a remote procedure doit(a,b) . The\nreturn path for the result is not shown.\n1. The client procedure calls the client stub in the normal way.\n2. The client stub builds a message and calls the local operating system.\n3. The client\u2019s OS sends the message to the remote OS.\n4. The remote OS gives the message to the server stub.\n5. The server stub unpacks the parameter(s) and calls the server.\n6. The server does the work and returns the result to the stub.\n7. The server stub packs the result in a message and calls its local OS.\n8. The server\u2019s OS sends the message to the client\u2019s OS.\n9. The client\u2019s OS gives the message to the client stub.\n10. The stub unpacks the result and returns it to the client.\nThe \ufb01rst steps are shown in Figure 4.7 for an abstract two-parameter\nprocedure doit(a,b) , where we assume that parameter ais of type type1 , and\nbof type type2 . The net effect of all these steps is to convert the local call by\nthe client procedure to the client stub, to a local call to the server procedure\nwithout either client or server being aware of the intermediate steps or the\nexistence of the network.\nNote 4.3 (More information: An example in Python)\nTo make matters concrete, let us consider how a remote procedure call could\nbe implemented for the operation append discussed previously. Take a look at\nthe Python code shown in Figure 4.8 (from which we omit nonessential code\nfragments).\nThe class DBList is a simple representation of a list object, mimicking what one\nwould expect to see in a version that would be found in a database environment.\nThe client stub, represented by the class Client , consists of an implementation\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "178 CHAPTER 4. COMMUNICATION\nofappend . When called with parameters data and dbList , the following happens.\nThe call is transformed into a tuple (APPEND, data, dbList) containing all the\ninformation the server would need to do its work. The client stub then sends\nthe request off to the server, and subsequently waits for the response. When the\nresponse comes in, it \ufb01nishes by passing the result to the program that initially\ncalled the stub.\n1import channel, pickle\n2\n3class DBList:\n4 defappend(self, data):\n5 self.value = self.value + [data]\n6 return self\n7\n8class Client:\n9 defappend(self, data, dbList):\n10 msglst = (APPEND, data, dbList) # message payload\n11 self.chan.sendTo(self.server, msglst) # send msg to server\n12 msgrcv = self.chan.recvFrom(self.server) # wait for response\n13 return msgrcv[1] # pass it to caller\n14\n15class Server:\n16 defappend(self, data, dbList):\n17 return dbList.append(data)\n18\n19 defrun(self):\n20 while True:\n21 msgreq = self.chan.recvFromAny() # wait for any request\n22 client = msgreq[0] # see who is the caller\n23 msgrpc = msgreq[1] # fetch call & parameters\n24 ifAPPEND == msgrpc[0]: # check what is being requested\n25 result = self.append(msgrpc[1], msgrpc[2]) # do local call\n26 self.chan.sendTo([client],result) # return response\nFigure 4.8: A simple RPC example for operation append .\nOn the server side, we see that in the server stub, the server waits for any\nincoming message, and inspects which operation it is required to call. Assuming\nit received a request to call append, it then simply does a local call to its imple-\nmentation of append with the appropriate parameters as also found in the request\ntuple. The result is then sent off to the client.\nParameter passing\nThe function of the client stub is to take its parameters, pack them into a\nmessage, and send them to the server stub. While this sounds straightforward,\nit is not quite as simple as it at \ufb01rst appears.\nPacking parameters into a message is called parameter marshaling . Re-\nturning to our append operation, we thus need to ensure that its two parame-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n178 CHAPTER 4. COMMUNICATION\nofappend . When called with parameters data and dbList , the following happens.\nThe call is transformed into a tuple (APPEND, data, dbList) containing all the\ninformation the server would need to do its work. The client stub then sends\nthe request off to the server, and subsequently waits for the response. When the\nresponse comes in, it \ufb01nishes by passing the result to the program that initially\ncalled the stub.\n1import channel, pickle\n2\n3class DBList:\n4 defappend(self, data):\n5 self.value = self.value + [data]\n6 return self\n7\n8class Client:\n9 defappend(self, data, dbList):\n10 msglst = (APPEND, data, dbList) # message payload\n11 self.chan.sendTo(self.server, msglst) # send msg to server\n12 msgrcv = self.chan.recvFrom(self.server) # wait for response\n13 return msgrcv[1] # pass it to caller\n14\n15class Server:\n16 defappend(self, data, dbList):\n17 return dbList.append(data)\n18\n19 defrun(self):\n20 while True:\n21 msgreq = self.chan.recvFromAny() # wait for any request\n22 client = msgreq[0] # see who is the caller\n23 msgrpc = msgreq[1] # fetch call & parameters\n24 ifAPPEND == msgrpc[0]: # check what is being requested\n25 result = self.append(msgrpc[1], msgrpc[2]) # do local call\n26 self.chan.sendTo([client],result) # return response\nFigure 4.8: A simple RPC example for operation append .\nOn the server side, we see that in the server stub, the server waits for any\nincoming message, and inspects which operation it is required to call. Assuming\nit received a request to call append, it then simply does a local call to its imple-\nmentation of append with the appropriate parameters as also found in the request\ntuple. The result is then sent off to the client.\nParameter passing\nThe function of the client stub is to take its parameters, pack them into a\nmessage, and send them to the server stub. While this sounds straightforward,\nit is not quite as simple as it at \ufb01rst appears.\nPacking parameters into a message is called parameter marshaling . Re-\nturning to our append operation, we thus need to ensure that its two parame-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 179\nters ( data and dbList ) are sent over the network and correctly interpreted by\nthe server. The thing to realize here is that, in the end, the server will just be\nseeing a series of bytes coming in that constitute the original message sent by\nthe client. However, no additional information on what those bytes mean is\nnormally provided with the message, let alone that we would be facing the\nsame problem again: how should the meta-information be recognized as such\nby the server?\nBesides this interpretation problem, we also need to handle the case that\nthe placement of bytes in memory may differ between machine architectures.\nIn particular, we need to account for the fact that some machines, such as the\nIntel Pentium, number their bytes from right to left, whereas many others,\nsuch as the older ARM processors, number them the other way (ARM now\nsupports both). The Intel format is called little endian and the (older) ARM\nformat is called big endian . Byte ordering is also important for networking:\nalso here we can witness that machines may use a different ordering when\ntransmitting (and thus receiving) bits and bytes. However, big endian is what\nis normally used for transferring bytes across a network.\nThe solution to this problem is to transform data that is to be sent to a\nmachine- and network-independent format, next to making sure that both\ncommunicating parties expect the same message data type to be transmitted.\nThe latter can typically be solved at the level of programming languages. The\nformer is accomplished by using machine- dependent routines that transform\ndata to and from machine- and network-independent formats.\nMarshaling and unmarshaling is all about this transformation to neutral\nformats and forms an essential part of remote procedure calls.\nWe now come to a dif\ufb01cult problem: How are pointers, or in general,\nreferences passed? The answer is: only with the greatest of dif\ufb01culty, if at\nall. A pointer is meaningful only within the address space of the process\nin which it is being used. Getting back to our append example, we stated\nthat the second parameter, dbList , is implemented by means of a reference to\na list stored in a database. If that reference is just a pointer to a local data\nstructure somewhere in the caller\u2019s main memory, we cannot simply pass it\nto the server. The transferred pointer value will most likely be referring to\nsomething completely different.\nNote 4.4 (More information: An example in Python revisited)\nIt is not dif\ufb01cult to see that the solution to remote procedure calling as shown in\nFigure 4.8 will not work in general. Only if the client and server are operating\non machines that obey they same byte-ordering rules andhave the same machine\nrepresentations for data structures, will the exchange of messages as shown lead\nto correct interpretations. A robust solution is shown in Figure 4.9 (where we\nagain have omitted code for brevity).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 179\nters ( data and dbList ) are sent over the network and correctly interpreted by\nthe server. The thing to realize here is that, in the end, the server will just be\nseeing a series of bytes coming in that constitute the original message sent by\nthe client. However, no additional information on what those bytes mean is\nnormally provided with the message, let alone that we would be facing the\nsame problem again: how should the meta-information be recognized as such\nby the server?\nBesides this interpretation problem, we also need to handle the case that\nthe placement of bytes in memory may differ between machine architectures.\nIn particular, we need to account for the fact that some machines, such as the\nIntel Pentium, number their bytes from right to left, whereas many others,\nsuch as the older ARM processors, number them the other way (ARM now\nsupports both). The Intel format is called little endian and the (older) ARM\nformat is called big endian . Byte ordering is also important for networking:\nalso here we can witness that machines may use a different ordering when\ntransmitting (and thus receiving) bits and bytes. However, big endian is what\nis normally used for transferring bytes across a network.\nThe solution to this problem is to transform data that is to be sent to a\nmachine- and network-independent format, next to making sure that both\ncommunicating parties expect the same message data type to be transmitted.\nThe latter can typically be solved at the level of programming languages. The\nformer is accomplished by using machine- dependent routines that transform\ndata to and from machine- and network-independent formats.\nMarshaling and unmarshaling is all about this transformation to neutral\nformats and forms an essential part of remote procedure calls.\nWe now come to a dif\ufb01cult problem: How are pointers, or in general,\nreferences passed? The answer is: only with the greatest of dif\ufb01culty, if at\nall. A pointer is meaningful only within the address space of the process\nin which it is being used. Getting back to our append example, we stated\nthat the second parameter, dbList , is implemented by means of a reference to\na list stored in a database. If that reference is just a pointer to a local data\nstructure somewhere in the caller\u2019s main memory, we cannot simply pass it\nto the server. The transferred pointer value will most likely be referring to\nsomething completely different.\nNote 4.4 (More information: An example in Python revisited)\nIt is not dif\ufb01cult to see that the solution to remote procedure calling as shown in\nFigure 4.8 will not work in general. Only if the client and server are operating\non machines that obey they same byte-ordering rules andhave the same machine\nrepresentations for data structures, will the exchange of messages as shown lead\nto correct interpretations. A robust solution is shown in Figure 4.9 (where we\nagain have omitted code for brevity).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "180 CHAPTER 4. COMMUNICATION\nIn this example, we use the Python pickle library for marshaling and unmar-\nshaling data structures. Note that the code hardly changes in comparison to what\nwe have shown in Figure 4.8. The only changes occur just before sending, and\nafter receiving a message. Also note that both client and server are programmed\nto work on the same data structures, as we discussed above.\n1import channel, pickle\n2\n3class Client:\n4 defappend(self, data, dbList):\n5 msglst = (APPEND, data, dbList) # message payload\n6 msgsnd = pickle.dumps(msglst) # wrap call\n7 self.chan.sendTo(self.server, msgsnd) # send request to server\n8 msgrcv = self.chan.recvFrom(self.server) # wait for response\n9 retval = pickle.loads(msgrcv[1]) # unwrap return value\n10 return retval # pass it to caller\n11\n12class Server:\n13 defrun(self):\n14 while True:\n15 msgreq = self.chan.recvFromAny() # wait for any request\n16 client = msgreq[0] # see who is the caller\n17 msgrpc = pickle.loads(msgreq[1]) # unwrap the call\n18 ifAPPEND == msgrpc[0]: # check what is being requested\n19 result = self.append(msgrpc[1], msgrpc[2]) # do local call\n20 msgres = pickle.dumps(result) # wrap the result\n21 self.chan.sendTo([client],msgres) # send response\nFigure 4.9: A simple RPC example for operation append , but now with\nproper marshaling.\nOne solution is just to forbid pointers and reference parameters in general.\nHowever, these are so important that this solution is highly undesirable. In\nfact, it is often not necessary either. First, reference parameters are often used\nwith \ufb01xed-sized data types, such as static arrays, or with dynamic data types\nfor which it is easy to compute their size at runtime, such as strings or dynamic\narrays. In such cases, we can simply copy the entire data structure to which the\nparameter is referring, effectively replacing the copy-by-reference mechanism\nby copy-by-value/restore. Although this is semantically not always identical,\nit frequently is good enough. An obvious optimization is that when the client\nstub knows the referred data will be only read, there is no need to copy it\nback when the call has \ufb01nished. Copy-by-value is thus good enough.\nMore intricate data types can often be supported as well, and certainly if a\nprogramming language supports those data types. For example, a language\nsuch as Python or Java supports user-de\ufb01ned classes, allowing a language\nsystem to provide fully automated marshaling and unmarshaling of those data\ntypes. Note, however, that as soon as we are dealing with very large, nested,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n180 CHAPTER 4. COMMUNICATION\nIn this example, we use the Python pickle library for marshaling and unmar-\nshaling data structures. Note that the code hardly changes in comparison to what\nwe have shown in Figure 4.8. The only changes occur just before sending, and\nafter receiving a message. Also note that both client and server are programmed\nto work on the same data structures, as we discussed above.\n1import channel, pickle\n2\n3class Client:\n4 defappend(self, data, dbList):\n5 msglst = (APPEND, data, dbList) # message payload\n6 msgsnd = pickle.dumps(msglst) # wrap call\n7 self.chan.sendTo(self.server, msgsnd) # send request to server\n8 msgrcv = self.chan.recvFrom(self.server) # wait for response\n9 retval = pickle.loads(msgrcv[1]) # unwrap return value\n10 return retval # pass it to caller\n11\n12class Server:\n13 defrun(self):\n14 while True:\n15 msgreq = self.chan.recvFromAny() # wait for any request\n16 client = msgreq[0] # see who is the caller\n17 msgrpc = pickle.loads(msgreq[1]) # unwrap the call\n18 ifAPPEND == msgrpc[0]: # check what is being requested\n19 result = self.append(msgrpc[1], msgrpc[2]) # do local call\n20 msgres = pickle.dumps(result) # wrap the result\n21 self.chan.sendTo([client],msgres) # send response\nFigure 4.9: A simple RPC example for operation append , but now with\nproper marshaling.\nOne solution is just to forbid pointers and reference parameters in general.\nHowever, these are so important that this solution is highly undesirable. In\nfact, it is often not necessary either. First, reference parameters are often used\nwith \ufb01xed-sized data types, such as static arrays, or with dynamic data types\nfor which it is easy to compute their size at runtime, such as strings or dynamic\narrays. In such cases, we can simply copy the entire data structure to which the\nparameter is referring, effectively replacing the copy-by-reference mechanism\nby copy-by-value/restore. Although this is semantically not always identical,\nit frequently is good enough. An obvious optimization is that when the client\nstub knows the referred data will be only read, there is no need to copy it\nback when the call has \ufb01nished. Copy-by-value is thus good enough.\nMore intricate data types can often be supported as well, and certainly if a\nprogramming language supports those data types. For example, a language\nsuch as Python or Java supports user-de\ufb01ned classes, allowing a language\nsystem to provide fully automated marshaling and unmarshaling of those data\ntypes. Note, however, that as soon as we are dealing with very large, nested,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 181\nor otherwise intricate dynamic data structures, automatic (un)marshaling may\nnot be available, or even desirable.\nThe problem with pointers and references as discussed so far, is that\nthey make only locally sense: they refer to memory locations that have\nmeaning only to the calling process. Problems can be alleviated by using\nglobal references: references that are meaningful to the calling and the called\nprocess. For example, if the client and the server have access to the same \ufb01le\nsystem, passing a \ufb01le handle instead of a pointer may do the trick. There\nis one important observation: both processes need to know exactly what\nto do when a global reference is passed. In other words, if we consider a\nglobal reference having an associated data type, the calling and called process\nshould have exactly the same picture of the operations that can be performed.\nMoreover, both processes should have agreement on exactly what to do when\na \ufb01le handle is passed. Again, these are typically issues that can be solved by\nproper programming-language support.\nNote 4.5 (Advanced: Parameter passing in object-based systems)\nObject-based systems often use global references. Consider the situation that all\nobjects in the system can be accessed from remote machines. In that case, we can\nconsistently use object references as parameters in method invocations. References\nare passed by value, and thus copied from one machine to the other. When a\nprocess is given an object reference as the result of a method invocation, it can\nsimply bind to the object referred to when needed later (see also Section 2.1).\nUnfortunately, using only distributed objects can be highly inef\ufb01cient, espe-\ncially when objects are small, such as integers, or worse yet, Booleans. Each\ninvocation by a client that is not co-located in the same server as the object, gener-\nates a request between different address spaces or, even worse, between different\nmachines. Therefore, references to remote objects and those to local objects are\noften treated differently.\nFigure 4.10: Passing an object by reference or by value.\nWhen invoking a method with an object reference as parameter, that reference\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 181\nor otherwise intricate dynamic data structures, automatic (un)marshaling may\nnot be available, or even desirable.\nThe problem with pointers and references as discussed so far, is that\nthey make only locally sense: they refer to memory locations that have\nmeaning only to the calling process. Problems can be alleviated by using\nglobal references: references that are meaningful to the calling and the called\nprocess. For example, if the client and the server have access to the same \ufb01le\nsystem, passing a \ufb01le handle instead of a pointer may do the trick. There\nis one important observation: both processes need to know exactly what\nto do when a global reference is passed. In other words, if we consider a\nglobal reference having an associated data type, the calling and called process\nshould have exactly the same picture of the operations that can be performed.\nMoreover, both processes should have agreement on exactly what to do when\na \ufb01le handle is passed. Again, these are typically issues that can be solved by\nproper programming-language support.\nNote 4.5 (Advanced: Parameter passing in object-based systems)\nObject-based systems often use global references. Consider the situation that all\nobjects in the system can be accessed from remote machines. In that case, we can\nconsistently use object references as parameters in method invocations. References\nare passed by value, and thus copied from one machine to the other. When a\nprocess is given an object reference as the result of a method invocation, it can\nsimply bind to the object referred to when needed later (see also Section 2.1).\nUnfortunately, using only distributed objects can be highly inef\ufb01cient, espe-\ncially when objects are small, such as integers, or worse yet, Booleans. Each\ninvocation by a client that is not co-located in the same server as the object, gener-\nates a request between different address spaces or, even worse, between different\nmachines. Therefore, references to remote objects and those to local objects are\noften treated differently.\nFigure 4.10: Passing an object by reference or by value.\nWhen invoking a method with an object reference as parameter, that reference\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "182 CHAPTER 4. COMMUNICATION\nis copied and passed as a value parameter only when it refers to a remote object.\nIn this case, the object is literally passed by reference. However, when the reference\nrefers to a local object, that is an object in the same address space as the client,\nthe referred object is copied as a whole and passed along with the invocation. In\nother words, the object is passed by value.\nThese two situations are illustrated in Figure 4.10 which shows a client pro-\ngram running on machine A, and a server program on machine C. The client has\na reference to a local object O1that it uses as a parameter when calling the server\nprogram on machine C. In addition, it holds a reference to a remote object O2\nresiding at machine B, which is also used as a parameter. When calling the server,\na copy of O1is passed to the server on machine C, along with only a copy of the\nreference to O2.\nNote that whether we are dealing with a reference to a local object or a\nreference to a remote object can be highly transparent, such as in Java. In Java, the\ndistinction is visible only because local objects are essentially of a different data\ntype than remote objects. Otherwise, both types of references are treated very\nmuch the same (see also [Wollrath et al., 1996]). On the other hand, when using\nconventional programming languages such as C, a reference to a local object can\nbe as simple as a pointer, which can never be used to refer to a remote object.\nThe side effect of invoking a method with an object reference as parameter is\nthat we may be copying an object. Obviously, hiding this aspect is unacceptable,\nso that we are consequently forced to make an explicit distinction between local\nand distributed objects. Clearly, this distinction not only violates distribution\ntransparency, but also makes it harder to write distributed applications.\nWe can now also easily explain how global references can be implemented\nwhen using portable, interpreted languages such as Python or Java: use the entire\nclient stub as a reference. The key observation is that a client stub is often just\nanother data structure that is compiled into (portable) bytecode. That compiled\ncode can actually be transferred across the network and executed at the receiver\u2019s\nside. In other words, there is no need for explicit binding anymore; simply\ncopying the client stub to the recipient is enough to allow the latter to invoke the\nassociated server-side object.\nRPC-based application support\nFrom what we have explained so far, it is clear that hiding a remote procedure\ncall requires that the caller and the callee agree on the format of the messages\nthey exchange and that they follow the same steps when it comes to, for\nexample, passing complex data structures. In other words, both sides in an\nRPC should follow the same protocol or the RPC will not work correctly.\nThere are at least two ways in which RPC-based application development can\nbe supported. The \ufb01rst one is to let a developer specify exactly what needs\nto be called remotely, from which complete client-side and server-side stubs\ncan be generated. A second approach is to embed remote procedure calling\nas part of a programming-language environment.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n182 CHAPTER 4. COMMUNICATION\nis copied and passed as a value parameter only when it refers to a remote object.\nIn this case, the object is literally passed by reference. However, when the reference\nrefers to a local object, that is an object in the same address space as the client,\nthe referred object is copied as a whole and passed along with the invocation. In\nother words, the object is passed by value.\nThese two situations are illustrated in Figure 4.10 which shows a client pro-\ngram running on machine A, and a server program on machine C. The client has\na reference to a local object O1that it uses as a parameter when calling the server\nprogram on machine C. In addition, it holds a reference to a remote object O2\nresiding at machine B, which is also used as a parameter. When calling the server,\na copy of O1is passed to the server on machine C, along with only a copy of the\nreference to O2.\nNote that whether we are dealing with a reference to a local object or a\nreference to a remote object can be highly transparent, such as in Java. In Java, the\ndistinction is visible only because local objects are essentially of a different data\ntype than remote objects. Otherwise, both types of references are treated very\nmuch the same (see also [Wollrath et al., 1996]). On the other hand, when using\nconventional programming languages such as C, a reference to a local object can\nbe as simple as a pointer, which can never be used to refer to a remote object.\nThe side effect of invoking a method with an object reference as parameter is\nthat we may be copying an object. Obviously, hiding this aspect is unacceptable,\nso that we are consequently forced to make an explicit distinction between local\nand distributed objects. Clearly, this distinction not only violates distribution\ntransparency, but also makes it harder to write distributed applications.\nWe can now also easily explain how global references can be implemented\nwhen using portable, interpreted languages such as Python or Java: use the entire\nclient stub as a reference. The key observation is that a client stub is often just\nanother data structure that is compiled into (portable) bytecode. That compiled\ncode can actually be transferred across the network and executed at the receiver\u2019s\nside. In other words, there is no need for explicit binding anymore; simply\ncopying the client stub to the recipient is enough to allow the latter to invoke the\nassociated server-side object.\nRPC-based application support\nFrom what we have explained so far, it is clear that hiding a remote procedure\ncall requires that the caller and the callee agree on the format of the messages\nthey exchange and that they follow the same steps when it comes to, for\nexample, passing complex data structures. In other words, both sides in an\nRPC should follow the same protocol or the RPC will not work correctly.\nThere are at least two ways in which RPC-based application development can\nbe supported. The \ufb01rst one is to let a developer specify exactly what needs\nto be called remotely, from which complete client-side and server-side stubs\ncan be generated. A second approach is to embed remote procedure calling\nas part of a programming-language environment.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 183\nStub generation\nConsider the function someFunction of Figure 4.11(a). It has three parameters,\na character, a \ufb02oating-point number, and an array of \ufb01ve integers. Assuming a\nword is four bytes, the RPC protocol might prescribe that we should transmit a\ncharacter in the rightmost byte of a word (leaving the next three bytes empty),\na \ufb02oat as a whole word, and an array as a group of words equal to the array\nlength, preceded by a word giving the length, as shown in Figure 4.11(b).\nThus given these rules, the client stub for someFunction knows that it must\nuse the format of Figure 4.11(b), and the server stub knows that incoming\nmessages for someFunction will have the format of Figure 4.11(b).\nvoid someFunction(char x; float y; int z[5])\n(a)\n(b)\nFigure 4.11: (a) A function. (b) The corresponding message, and the order in\nwhich bytes and words are sent across the network.\nDe\ufb01ning the message format is one aspect of an RPC protocol, but it\nis not suf\ufb01cient. What we also need is the client and the server to agree\non the representation of simple data structures, such as integers, characters,\nBooleans, etc. For example, the protocol could prescribe that integers are\nrepresented in two\u2019s complement, characters in 16-bit Unicode, and \ufb02oats in\nthe IEEE standard #754 format, with everything stored in little endian. With\nthis additional information, messages can be unambiguously interpreted.\nWith the encoding rules now pinned down to the last bit, the only thing\nthat remains to be done is that the caller and callee agree on the actual\nexchange of messages. For example, it may be decided to use a connection-\noriented transport service such as TCP/IP . An alternative is to use an unreli-\nable datagram service and let the client and server implement an error control\nscheme as part of the RPC protocol. In practice, several variants exist, and it\nis up to the developer to indicate the preferred underlying communication\nservice.\nOnce the RPC protocol has been fully de\ufb01ned, the client and server stubs\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 183\nStub generation\nConsider the function someFunction of Figure 4.11(a). It has three parameters,\na character, a \ufb02oating-point number, and an array of \ufb01ve integers. Assuming a\nword is four bytes, the RPC protocol might prescribe that we should transmit a\ncharacter in the rightmost byte of a word (leaving the next three bytes empty),\na \ufb02oat as a whole word, and an array as a group of words equal to the array\nlength, preceded by a word giving the length, as shown in Figure 4.11(b).\nThus given these rules, the client stub for someFunction knows that it must\nuse the format of Figure 4.11(b), and the server stub knows that incoming\nmessages for someFunction will have the format of Figure 4.11(b).\nvoid someFunction(char x; float y; int z[5])\n(a)\n(b)\nFigure 4.11: (a) A function. (b) The corresponding message, and the order in\nwhich bytes and words are sent across the network.\nDe\ufb01ning the message format is one aspect of an RPC protocol, but it\nis not suf\ufb01cient. What we also need is the client and the server to agree\non the representation of simple data structures, such as integers, characters,\nBooleans, etc. For example, the protocol could prescribe that integers are\nrepresented in two\u2019s complement, characters in 16-bit Unicode, and \ufb02oats in\nthe IEEE standard #754 format, with everything stored in little endian. With\nthis additional information, messages can be unambiguously interpreted.\nWith the encoding rules now pinned down to the last bit, the only thing\nthat remains to be done is that the caller and callee agree on the actual\nexchange of messages. For example, it may be decided to use a connection-\noriented transport service such as TCP/IP . An alternative is to use an unreli-\nable datagram service and let the client and server implement an error control\nscheme as part of the RPC protocol. In practice, several variants exist, and it\nis up to the developer to indicate the preferred underlying communication\nservice.\nOnce the RPC protocol has been fully de\ufb01ned, the client and server stubs\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "184 CHAPTER 4. COMMUNICATION\nneed to be implemented. Fortunately, stubs for the same protocol but different\nprocedures normally differ only in their interface to the applications. An\ninterface consists of a collection of procedures that can be called by a client,\nand which are implemented by a server. An interface is usually available in\nthe same programming language as the one in which the client or server is\nwritten (although this is strictly speaking, not necessary). To simplify matters,\ninterfaces are often speci\ufb01ed by means of an Interface De\ufb01nition Language\n(IDL). An interface speci\ufb01ed in such an IDL is then subsequently compiled\ninto a client stub and a server stub, along with the appropriate compile-time\nor run-time interfaces.\nPractice shows that using an interface de\ufb01nition language considerably\nsimpli\ufb01es client-server applications based on RPCs. Because it is easy to fully\ngenerate client and server stubs, all RPC-based middleware systems offer an\nIDL to support application development. In some cases, using the IDL is even\nmandatory.\nLanguage-based support\nThe approach described up until now is largely independent of a speci\ufb01c\nprogramming language. As an alternative, we can also embed remote pro-\ncedure calling into a language itself. The main bene\ufb01t is that application\ndevelopment often becomes much simpler. Also, reaching a high degree\nof access transparency is often simpler as many issues related to parameter\npassing can be circumvented altogether.\nA well-known example in which remote procedure calling is fully em-\nbedded is Java, where an RPC is referred to as a remote method invocation\n(RMI ). In essence, a client being executed by its own (Java) virtual machine\ncan invoke a method of an object managed by another virtual machine. By\nsimply reading an application\u2019s source code, it may be hard or even impossible\nto see whether a method invocation is to a local or to a remote object.\nNote 4.6 (More information: Language-based RPC in Python)\nLet us see by an example of how remote procedure calling can be integrated in\na language. We have been using the Python language for most of our examples,\nand will continue to do so now as well. In Figure 4.12(a) we show a simple\nserver for our DBList data structure. In this case, it has two exposed operations:\nexposed_append for appending elements, and exposed_value to display what is\ncurrently in the list. We use the Python RPyC package for embedding RPCs.\nThe client is shown in Figure 4.12(b). When a connection is made to the server,\na new instance of DBList will be created and the client can immediately append\nvalues to the list. The exposed operations can be called without further ado.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n184 CHAPTER 4. COMMUNICATION\nneed to be implemented. Fortunately, stubs for the same protocol but different\nprocedures normally differ only in their interface to the applications. An\ninterface consists of a collection of procedures that can be called by a client,\nand which are implemented by a server. An interface is usually available in\nthe same programming language as the one in which the client or server is\nwritten (although this is strictly speaking, not necessary). To simplify matters,\ninterfaces are often speci\ufb01ed by means of an Interface De\ufb01nition Language\n(IDL). An interface speci\ufb01ed in such an IDL is then subsequently compiled\ninto a client stub and a server stub, along with the appropriate compile-time\nor run-time interfaces.\nPractice shows that using an interface de\ufb01nition language considerably\nsimpli\ufb01es client-server applications based on RPCs. Because it is easy to fully\ngenerate client and server stubs, all RPC-based middleware systems offer an\nIDL to support application development. In some cases, using the IDL is even\nmandatory.\nLanguage-based support\nThe approach described up until now is largely independent of a speci\ufb01c\nprogramming language. As an alternative, we can also embed remote pro-\ncedure calling into a language itself. The main bene\ufb01t is that application\ndevelopment often becomes much simpler. Also, reaching a high degree\nof access transparency is often simpler as many issues related to parameter\npassing can be circumvented altogether.\nA well-known example in which remote procedure calling is fully em-\nbedded is Java, where an RPC is referred to as a remote method invocation\n(RMI ). In essence, a client being executed by its own (Java) virtual machine\ncan invoke a method of an object managed by another virtual machine. By\nsimply reading an application\u2019s source code, it may be hard or even impossible\nto see whether a method invocation is to a local or to a remote object.\nNote 4.6 (More information: Language-based RPC in Python)\nLet us see by an example of how remote procedure calling can be integrated in\na language. We have been using the Python language for most of our examples,\nand will continue to do so now as well. In Figure 4.12(a) we show a simple\nserver for our DBList data structure. In this case, it has two exposed operations:\nexposed_append for appending elements, and exposed_value to display what is\ncurrently in the list. We use the Python RPyC package for embedding RPCs.\nThe client is shown in Figure 4.12(b). When a connection is made to the server,\na new instance of DBList will be created and the client can immediately append\nvalues to the list. The exposed operations can be called without further ado.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 185\n1import rpyc\n2from rpyc.utils.server import ForkingServer\n3\n4class DBList(rpyc.Service):\n5 value = []\n6\n7 defexposed_append(self, data):\n8 self.value = self.value + [data]\n9 return self.value\n10\n11 defexposed_value(self):\n12 return self.value\n13\n14if__name__ == \"__main__\":\n15 server = ForkingServer(DBList, port = 12345)\n16 server.start()\nFigure 4.12: (a) Embedding RPCs in a language: a server.\n1import rpyc\n2\n3class Client:\n4 conn = rpyc.connect(SERVER, PORT) # Connect to the server\n5 conn.root.exposed_append(2) # Call an exposed operation,\n6 conn.root.exposed_append(4) # and append two elements\n7 print conn.root.exposed_value() # Print the result\nFigure 4.12: (b) Embedding RPCs in a language: a client.\nThis example also illustrates a subtle issue: apparently the instance of DBList\ncreated for the client is new, and unique. In other words, as soon as the client\nbreaks the connection to the server, the list will be lost. It is a transient object so\nto say, and special measures will need to be taken to make it a persistent object .\nVariations on RPC\nAs in conventional procedure calls, when a client calls a remote procedure, the\nclient will block until a reply is returned. This strict request-reply behavior\nis unnecessary when there is no result to return, or may hinder ef\ufb01ciency\nwhen multiple RPCs need to be performed. In the following we look at two\nvariations on the RPC scheme we have discussed so far.\nAsynchronous RPC\nTo support situations in which there is simply no result to return to the client,\nRPC systems may provide facilities for what are called asynchronous RPCs .\nWith asynchronous RPCs, the server, in principle, immediately sends a reply\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 185\n1import rpyc\n2from rpyc.utils.server import ForkingServer\n3\n4class DBList(rpyc.Service):\n5 value = []\n6\n7 defexposed_append(self, data):\n8 self.value = self.value + [data]\n9 return self.value\n10\n11 defexposed_value(self):\n12 return self.value\n13\n14if__name__ == \"__main__\":\n15 server = ForkingServer(DBList, port = 12345)\n16 server.start()\nFigure 4.12: (a) Embedding RPCs in a language: a server.\n1import rpyc\n2\n3class Client:\n4 conn = rpyc.connect(SERVER, PORT) # Connect to the server\n5 conn.root.exposed_append(2) # Call an exposed operation,\n6 conn.root.exposed_append(4) # and append two elements\n7 print conn.root.exposed_value() # Print the result\nFigure 4.12: (b) Embedding RPCs in a language: a client.\nThis example also illustrates a subtle issue: apparently the instance of DBList\ncreated for the client is new, and unique. In other words, as soon as the client\nbreaks the connection to the server, the list will be lost. It is a transient object so\nto say, and special measures will need to be taken to make it a persistent object .\nVariations on RPC\nAs in conventional procedure calls, when a client calls a remote procedure, the\nclient will block until a reply is returned. This strict request-reply behavior\nis unnecessary when there is no result to return, or may hinder ef\ufb01ciency\nwhen multiple RPCs need to be performed. In the following we look at two\nvariations on the RPC scheme we have discussed so far.\nAsynchronous RPC\nTo support situations in which there is simply no result to return to the client,\nRPC systems may provide facilities for what are called asynchronous RPCs .\nWith asynchronous RPCs, the server, in principle, immediately sends a reply\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "186 CHAPTER 4. COMMUNICATION\nback to the client the moment the RPC request is received, after which it\nlocally calls the requested procedure. The reply acts as an acknowledgment\nto the client that the server is going to process the RPC. The client will\ncontinue without further blocking as soon as it has received the server\u2019s\nacknowledgment. Figure 4.13(b) shows how client and server interact in the\ncase of asynchronous RPCs. For comparison, Figure 4.13(a) shows the normal\nrequest-reply behavior.\n(a) (b)\nFigure 4.13: (a) The interaction between client and server in a traditional RPC.\n(b) The interaction using asynchronous RPC.\nAsynchronous RPCs can also be useful when a reply will be returned but\nthe client is not prepared to wait for it and do nothing in the meantime. A\ntypical case is when a client needs to contact several servers independently.\nIn that case, it can send the call requests one after the other, effectively\nestablishing that the servers operate more or less in parallel. After all call\nrequests have been sent, the client can start waiting for the various results to be\nreturned. In cases such as these, it makes sense to organize the communication\nbetween the client and server through an asynchronous RPC combined with a\ncallback , as shown in Figure 4.14. In this scheme, also referred to as deferred\nsynchronous RPC , the client \ufb01rst calls the server, waits for the acceptance,\nand continues. When the results become available, the server sends a response\nmessage that leads to a callback at the client\u2019s side. A callback is a user-de\ufb01ned\nfunction that is invoked when a special event happens, such as an incoming\nmessage. A straightforward implementation is to spawn a separate thread and\nlet it block on the occurrence of the event while the main process continues.\nWhen the event occurs, the thread is unblocked and calls the function.\nIt should be noted that variants of asynchronous RPCs exist in which\nthe client continues executing immediately after sending the request to the\nserver. In other words, the client does not wait for an acknowledgment of the\nserver\u2019s acceptance of the request. We refer to such RPCs as one-way RPCs .\nThe problem with this approach is that when reliability is not guaranteed,\nthe client cannot know for sure whether or not its request will be processed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n186 CHAPTER 4. COMMUNICATION\nback to the client the moment the RPC request is received, after which it\nlocally calls the requested procedure. The reply acts as an acknowledgment\nto the client that the server is going to process the RPC. The client will\ncontinue without further blocking as soon as it has received the server\u2019s\nacknowledgment. Figure 4.13(b) shows how client and server interact in the\ncase of asynchronous RPCs. For comparison, Figure 4.13(a) shows the normal\nrequest-reply behavior.\n(a) (b)\nFigure 4.13: (a) The interaction between client and server in a traditional RPC.\n(b) The interaction using asynchronous RPC.\nAsynchronous RPCs can also be useful when a reply will be returned but\nthe client is not prepared to wait for it and do nothing in the meantime. A\ntypical case is when a client needs to contact several servers independently.\nIn that case, it can send the call requests one after the other, effectively\nestablishing that the servers operate more or less in parallel. After all call\nrequests have been sent, the client can start waiting for the various results to be\nreturned. In cases such as these, it makes sense to organize the communication\nbetween the client and server through an asynchronous RPC combined with a\ncallback , as shown in Figure 4.14. In this scheme, also referred to as deferred\nsynchronous RPC , the client \ufb01rst calls the server, waits for the acceptance,\nand continues. When the results become available, the server sends a response\nmessage that leads to a callback at the client\u2019s side. A callback is a user-de\ufb01ned\nfunction that is invoked when a special event happens, such as an incoming\nmessage. A straightforward implementation is to spawn a separate thread and\nlet it block on the occurrence of the event while the main process continues.\nWhen the event occurs, the thread is unblocked and calls the function.\nIt should be noted that variants of asynchronous RPCs exist in which\nthe client continues executing immediately after sending the request to the\nserver. In other words, the client does not wait for an acknowledgment of the\nserver\u2019s acceptance of the request. We refer to such RPCs as one-way RPCs .\nThe problem with this approach is that when reliability is not guaranteed,\nthe client cannot know for sure whether or not its request will be processed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 187\nFigure 4.14: A client and server interacting through asynchronous RPCs.\nWe return to these matters in Chapter 8. Likewise, in the case of deferred\nsynchronous RPC, the client may poll the server to see whether the results are\navailable yet, instead of letting the server calling back the client.\nMulticast RPC\nAsynchronous and deferred synchronous RPCs facilitate another alternative\nto remote procedure calls, namely executing multiple RPCs at the same time.\nAdopting the one-way RPCs (i.e., when a server does not tell the client it has\naccepted its call request but immediately starts processing it), a multicast RPC\nboils down to sending an RPC request to a group of servers. This principle\nis shown in Figure 4.15. In this example, the client sends a request to two\nservers, who subsequently process that request independently and in parallel.\nWhen done, the result is returned to the client where a callback takes place.\nFigure 4.15: The principle of a multicast RPC.\nThere are several issues that we need to consider. First, as before, the\nclient application may be unaware of the fact that an RPC is actually being\nforwarded to more than one server. For example, to increase fault tolerance,\nwe may decide to have all operations executed by a backup server who can\ntake over when the main server fails. That a server has been replicated can be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 187\nFigure 4.14: A client and server interacting through asynchronous RPCs.\nWe return to these matters in Chapter 8. Likewise, in the case of deferred\nsynchronous RPC, the client may poll the server to see whether the results are\navailable yet, instead of letting the server calling back the client.\nMulticast RPC\nAsynchronous and deferred synchronous RPCs facilitate another alternative\nto remote procedure calls, namely executing multiple RPCs at the same time.\nAdopting the one-way RPCs (i.e., when a server does not tell the client it has\naccepted its call request but immediately starts processing it), a multicast RPC\nboils down to sending an RPC request to a group of servers. This principle\nis shown in Figure 4.15. In this example, the client sends a request to two\nservers, who subsequently process that request independently and in parallel.\nWhen done, the result is returned to the client where a callback takes place.\nFigure 4.15: The principle of a multicast RPC.\nThere are several issues that we need to consider. First, as before, the\nclient application may be unaware of the fact that an RPC is actually being\nforwarded to more than one server. For example, to increase fault tolerance,\nwe may decide to have all operations executed by a backup server who can\ntake over when the main server fails. That a server has been replicated can be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "188 CHAPTER 4. COMMUNICATION\ncompletely hidden from a client application by an appropriate stub. Yet even\nthe stub need not be aware that the server is replicated, for example, because\nwe are using a transport-level multicast address.\nSecond, we need to consider what to do with the responses. In particular,\nwill the client proceed after all responses have been received, or wait just for\none? It all depends. When the server has been replicated for fault tolerance,\nwe may decide to wait for just the \ufb01rst response, or perhaps until a majority\nof the servers returns the same result. On the other hand, if the servers have\nbeen replicated to do the same work but on different parts of the input, their\nresults may need to be merged before the client can continue. Again, such\nmatters can be hidden in the client-side stub, yet the application developer\nwill, at the very least, have to specify the purpose of the multicast RPC.\nExample: DCE RPC\nRemote procedure calls have been widely adopted as the basis of middleware\nand distributed systems in general. In this section, we take a closer look at the\nDistributed Computing Environment (DCE ) which was developed by the\nOpen Software Foundation (OSF), now called The Open Group. It forms the\nbasis for Microsoft\u2019s distributed computing environment DCOM [Eddon and\nEddon, 1998] and used in Samba, a \ufb01le server and accompanying protocol suite\nallowing the Windows \ufb01le system to be accessed through remote procedure\ncalls from non-Windows systems.\nAlthough DCE RPC is arguably not the most modern way of managing\nRPCs, it is worthwhile discussing some of its details, notably because it\nis representative for most traditional RPC systems that use a combination\nof interface speci\ufb01cations and explicit bindings to various programming\nlanguages. We start with a brief introduction to DCE, after which we consider\nits principal workings. Details on how to develop RPC-based applications can\nbe found in [Stevens, 1999].\nIntroduction to DCE\nDCE is a true middleware system in that it is designed to execute as a layer\nof abstraction between existing (network) operating systems and distributed\napplications. Initially designed for Unix, it has now been ported to all major\noperating systems. The idea is that the customer can take a collection of\nexisting machines, add the DCE software, and then be able to run distributed\napplications, all without disturbing existing (nondistributed) applications.\nAlthough most of the DCE package runs in user space, in some con\ufb01gurations\na piece (part of the distributed \ufb01le system) must be added to the kernel of the\nunderlying operating system.\nThe programming model underlying DCE is the client-server model. User\nprocesses act as clients to access remote services provided by server processes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n188 CHAPTER 4. COMMUNICATION\ncompletely hidden from a client application by an appropriate stub. Yet even\nthe stub need not be aware that the server is replicated, for example, because\nwe are using a transport-level multicast address.\nSecond, we need to consider what to do with the responses. In particular,\nwill the client proceed after all responses have been received, or wait just for\none? It all depends. When the server has been replicated for fault tolerance,\nwe may decide to wait for just the \ufb01rst response, or perhaps until a majority\nof the servers returns the same result. On the other hand, if the servers have\nbeen replicated to do the same work but on different parts of the input, their\nresults may need to be merged before the client can continue. Again, such\nmatters can be hidden in the client-side stub, yet the application developer\nwill, at the very least, have to specify the purpose of the multicast RPC.\nExample: DCE RPC\nRemote procedure calls have been widely adopted as the basis of middleware\nand distributed systems in general. In this section, we take a closer look at the\nDistributed Computing Environment (DCE ) which was developed by the\nOpen Software Foundation (OSF), now called The Open Group. It forms the\nbasis for Microsoft\u2019s distributed computing environment DCOM [Eddon and\nEddon, 1998] and used in Samba, a \ufb01le server and accompanying protocol suite\nallowing the Windows \ufb01le system to be accessed through remote procedure\ncalls from non-Windows systems.\nAlthough DCE RPC is arguably not the most modern way of managing\nRPCs, it is worthwhile discussing some of its details, notably because it\nis representative for most traditional RPC systems that use a combination\nof interface speci\ufb01cations and explicit bindings to various programming\nlanguages. We start with a brief introduction to DCE, after which we consider\nits principal workings. Details on how to develop RPC-based applications can\nbe found in [Stevens, 1999].\nIntroduction to DCE\nDCE is a true middleware system in that it is designed to execute as a layer\nof abstraction between existing (network) operating systems and distributed\napplications. Initially designed for Unix, it has now been ported to all major\noperating systems. The idea is that the customer can take a collection of\nexisting machines, add the DCE software, and then be able to run distributed\napplications, all without disturbing existing (nondistributed) applications.\nAlthough most of the DCE package runs in user space, in some con\ufb01gurations\na piece (part of the distributed \ufb01le system) must be added to the kernel of the\nunderlying operating system.\nThe programming model underlying DCE is the client-server model. User\nprocesses act as clients to access remote services provided by server processes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 189\nSome of these services are part of DCE itself, but others belong to the applica-\ntions and are written by the application programmers. All communication\nbetween clients and servers takes place by means of RPCs.\nGoals of DCE RPC\nThe goals of the DCE RPC system are relatively traditional. First and foremost,\nthe RPC system makes it possible for a client to access a remote service by\nsimply calling a local procedure. This interface makes it possible for client\n(i.e., application) programs to be written in a simple way, familiar to most\nprogrammers. It also makes it easy to have large volumes of existing code run\nin a distributed environment with few, if any, changes.\nIt is up to the RPC system to hide all the details from the clients, and,\nto some extent, from the servers as well. To start with, the RPC system\ncan automatically locate the correct server, and subsequently set up the\ncommunication between client and server software (generally called binding ).\nIt can also handle the message transport in both directions, fragmenting and\nreassembling them as needed (e.g., if one of the parameters is a large array).\nFinally, the RPC system can automatically handle data type conversions\nbetween the client and the server, even if they run on different architectures\nand have a different byte ordering.\nAs a consequence of the RPC system\u2019s ability to hide the details, clients\nand servers are highly independent of one another. A client can be written\nin Java and a server in C, or vice versa. A client and server can run on\ndifferent hardware platforms and use different operating systems. A variety\nof network protocols and data representations are also supported, all without\nany intervention from the client or server.\nWriting a Client and a Server\nThe DCE RPC system consists of a number of components, including lan-\nguages, libraries, daemons, and utility programs, among others. Together\nthese make it possible to write clients and servers. In this section we will\ndescribe the pieces and how they \ufb01t together. The entire process of writing\nand using an RPC client and server is summarized in Figure 4.16.\nIn a client-server system, the glue that holds everything together is the\ninterface de\ufb01nition, as speci\ufb01ed in the Interface De\ufb01nition Language , or\nIDL. It permits procedure declarations in a form closely resembling function\nprototypes in ANSI C. IDL \ufb01les can also contain type de\ufb01nitions, constant\ndeclarations, and other information needed to correctly marshal parameters\nand unmarshal results. Ideally, the interface de\ufb01nition should also contain a\nformal de\ufb01nition of what the procedures do, but such a de\ufb01nition is beyond\nthe current state of the art, so the interface de\ufb01nition just de\ufb01nes the syntax\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 189\nSome of these services are part of DCE itself, but others belong to the applica-\ntions and are written by the application programmers. All communication\nbetween clients and servers takes place by means of RPCs.\nGoals of DCE RPC\nThe goals of the DCE RPC system are relatively traditional. First and foremost,\nthe RPC system makes it possible for a client to access a remote service by\nsimply calling a local procedure. This interface makes it possible for client\n(i.e., application) programs to be written in a simple way, familiar to most\nprogrammers. It also makes it easy to have large volumes of existing code run\nin a distributed environment with few, if any, changes.\nIt is up to the RPC system to hide all the details from the clients, and,\nto some extent, from the servers as well. To start with, the RPC system\ncan automatically locate the correct server, and subsequently set up the\ncommunication between client and server software (generally called binding ).\nIt can also handle the message transport in both directions, fragmenting and\nreassembling them as needed (e.g., if one of the parameters is a large array).\nFinally, the RPC system can automatically handle data type conversions\nbetween the client and the server, even if they run on different architectures\nand have a different byte ordering.\nAs a consequence of the RPC system\u2019s ability to hide the details, clients\nand servers are highly independent of one another. A client can be written\nin Java and a server in C, or vice versa. A client and server can run on\ndifferent hardware platforms and use different operating systems. A variety\nof network protocols and data representations are also supported, all without\nany intervention from the client or server.\nWriting a Client and a Server\nThe DCE RPC system consists of a number of components, including lan-\nguages, libraries, daemons, and utility programs, among others. Together\nthese make it possible to write clients and servers. In this section we will\ndescribe the pieces and how they \ufb01t together. The entire process of writing\nand using an RPC client and server is summarized in Figure 4.16.\nIn a client-server system, the glue that holds everything together is the\ninterface de\ufb01nition, as speci\ufb01ed in the Interface De\ufb01nition Language , or\nIDL. It permits procedure declarations in a form closely resembling function\nprototypes in ANSI C. IDL \ufb01les can also contain type de\ufb01nitions, constant\ndeclarations, and other information needed to correctly marshal parameters\nand unmarshal results. Ideally, the interface de\ufb01nition should also contain a\nformal de\ufb01nition of what the procedures do, but such a de\ufb01nition is beyond\nthe current state of the art, so the interface de\ufb01nition just de\ufb01nes the syntax\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "190 CHAPTER 4. COMMUNICATION\nFigure 4.16: The steps in writing a client and a server in DCE RPC.\nof the calls, not their semantics. At best the writer can add a few comments\ndescribing what the procedures do.\nA crucial element in every IDL \ufb01le is a globally unique identi\ufb01er for the\nspeci\ufb01ed interface. The client sends this identi\ufb01er in the \ufb01rst RPC message\nand the server veri\ufb01es that it is correct. In this way, if a client inadvertently\ntries to bind to the wrong server, or even to an older version of the right server,\nthe server will detect the error and the binding will not take place.\nInterface de\ufb01nitions and unique identi\ufb01ers are closely related in DCE. As\nillustrated in Figure 4.16, the \ufb01rst step in writing a client/server application\nis usually calling the uuidgen program, asking it to generate a prototype IDL\n\ufb01le containing an interface identi\ufb01er guaranteed never to be used again in any\ninterface generated anywhere by uuidgen . Uniqueness is ensured by encoding\nin it the location and time of creation. It consists of a 128-bit binary number\nrepresented in the IDL \ufb01le as an ASCII string in hexadecimal.\nThe next step is editing the IDL \ufb01le, \ufb01lling in the names of the remote\nprocedures and their parameters. It is worth noting that RPC is not totally\ntransparent. For example, the client and server cannot share global variables.\nThe IDL rules make it impossible to express constructs that are not supported.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n190 CHAPTER 4. COMMUNICATION\nFigure 4.16: The steps in writing a client and a server in DCE RPC.\nof the calls, not their semantics. At best the writer can add a few comments\ndescribing what the procedures do.\nA crucial element in every IDL \ufb01le is a globally unique identi\ufb01er for the\nspeci\ufb01ed interface. The client sends this identi\ufb01er in the \ufb01rst RPC message\nand the server veri\ufb01es that it is correct. In this way, if a client inadvertently\ntries to bind to the wrong server, or even to an older version of the right server,\nthe server will detect the error and the binding will not take place.\nInterface de\ufb01nitions and unique identi\ufb01ers are closely related in DCE. As\nillustrated in Figure 4.16, the \ufb01rst step in writing a client/server application\nis usually calling the uuidgen program, asking it to generate a prototype IDL\n\ufb01le containing an interface identi\ufb01er guaranteed never to be used again in any\ninterface generated anywhere by uuidgen . Uniqueness is ensured by encoding\nin it the location and time of creation. It consists of a 128-bit binary number\nrepresented in the IDL \ufb01le as an ASCII string in hexadecimal.\nThe next step is editing the IDL \ufb01le, \ufb01lling in the names of the remote\nprocedures and their parameters. It is worth noting that RPC is not totally\ntransparent. For example, the client and server cannot share global variables.\nThe IDL rules make it impossible to express constructs that are not supported.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.2. REMOTE PROCEDURE CALL 191\nWhen the IDL \ufb01le is complete, the IDL compiler is called to process it. The\noutput of the IDL compiler consists of three \ufb01les:\n\u2022 A header \ufb01le (e.g., interface.h , in C terms).\n\u2022 The client stub.\n\u2022 The server stub.\nThe header \ufb01le contains the unique identi\ufb01er, type de\ufb01nitions, constant\nde\ufb01nitions, and function prototypes. It should be included (using #include ) in\nboth the client and server code. The client stub contains the actual procedures\nthat the client program will call. These procedures are the ones responsible\nfor collecting and packing the parameters into the outgoing message and then\ncalling the runtime system to send it. The client stub also handles unpacking\nthe reply and returning values to the client. The server stub contains the\nprocedures called by the runtime system on the server machine when an\nincoming message arrives. These, in turn, call the actual server procedures\nthat do the work.\nThe next step is for the application writer to write the client and server\ncode. Both of these are then compiled, as are the two stub procedures. The\nresulting client code and client stub object \ufb01les are then linked with the\nruntime library to produce the executable binary for the client. Similarly, the\nserver code and server stub are compiled and linked to produce the server\u2019s\nbinary. At runtime, the client and server are started so that the application is\nactually executed as well.\nBinding a client to a server\nTo allow a client to call a server, it is necessary that the server has been\nregistered and is prepared to accept incoming calls. Registration of a server\nmakes it possible for a client to locate the server and bind to it. Finding the\nlocation of the server is done in two steps:\n1. Locate the server\u2019s machine.\n2. Locate the server (i.e., the correct process) on that machine.\nThe second step is somewhat subtle. Basically, what it comes down to is\nthat to communicate with a server, the client needs to know a port on the\nserver\u2019s machine to which it can send messages. A port is used by the server\u2019s\noperating system to distinguish incoming messages for different processes.\nIn DCE, a table of ( server ,port) pairs is maintained on each server machine by\na process called the DCE daemon . Before it becomes available for incoming\nrequests, the server must ask the operating system for a port. It then registers\nthis port with the DCE daemon. The DCE daemon records this information\n(including which protocols the server speaks) in the port table for future use.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.2. REMOTE PROCEDURE CALL 191\nWhen the IDL \ufb01le is complete, the IDL compiler is called to process it. The\noutput of the IDL compiler consists of three \ufb01les:\n\u2022 A header \ufb01le (e.g., interface.h , in C terms).\n\u2022 The client stub.\n\u2022 The server stub.\nThe header \ufb01le contains the unique identi\ufb01er, type de\ufb01nitions, constant\nde\ufb01nitions, and function prototypes. It should be included (using #include ) in\nboth the client and server code. The client stub contains the actual procedures\nthat the client program will call. These procedures are the ones responsible\nfor collecting and packing the parameters into the outgoing message and then\ncalling the runtime system to send it. The client stub also handles unpacking\nthe reply and returning values to the client. The server stub contains the\nprocedures called by the runtime system on the server machine when an\nincoming message arrives. These, in turn, call the actual server procedures\nthat do the work.\nThe next step is for the application writer to write the client and server\ncode. Both of these are then compiled, as are the two stub procedures. The\nresulting client code and client stub object \ufb01les are then linked with the\nruntime library to produce the executable binary for the client. Similarly, the\nserver code and server stub are compiled and linked to produce the server\u2019s\nbinary. At runtime, the client and server are started so that the application is\nactually executed as well.\nBinding a client to a server\nTo allow a client to call a server, it is necessary that the server has been\nregistered and is prepared to accept incoming calls. Registration of a server\nmakes it possible for a client to locate the server and bind to it. Finding the\nlocation of the server is done in two steps:\n1. Locate the server\u2019s machine.\n2. Locate the server (i.e., the correct process) on that machine.\nThe second step is somewhat subtle. Basically, what it comes down to is\nthat to communicate with a server, the client needs to know a port on the\nserver\u2019s machine to which it can send messages. A port is used by the server\u2019s\noperating system to distinguish incoming messages for different processes.\nIn DCE, a table of ( server ,port) pairs is maintained on each server machine by\na process called the DCE daemon . Before it becomes available for incoming\nrequests, the server must ask the operating system for a port. It then registers\nthis port with the DCE daemon. The DCE daemon records this information\n(including which protocols the server speaks) in the port table for future use.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "192 CHAPTER 4. COMMUNICATION\nThe server also registers with the directory service by providing it the\nnetwork address of the server\u2019s machine and a name under which the server\ncan be looked up. Binding a client to a server proceeds as shown in Figure 4.17.\nFigure 4.17: Client-to-server binding in DCE.\nLet us assume that the client wants to bind to a video server that is locally\nknown under the name /local /multimedia /video /movies . It passes this name\nto the directory server, which returns the network address of the machine\nrunning the video server. The client then goes to the DCE daemon on that\nmachine (which has a well-known port), and asks it to look up the port of the\nvideo server in its port table. Armed with this information, the RPC can now\ntake place. On subsequent RPCs this lookup is not needed. DCE also gives\nclients the ability to do more sophisticated searches for a suitable server when\nthat is needed. Secure RPC is also an option where con\ufb01dentiality or data\nintegrity is crucial.\nPerforming an RPC\nThe actual RPC is carried out transparently and in the usual way. The client\nstub marshals the parameters to the runtime library for transmission using the\nprotocol chosen at binding time. When a message arrives at the server side, it\nis routed to the correct server based on the port contained in the incoming\nmessage. The runtime library passes the message to the server stub, which\nunmarshals the parameters and calls the server. The reply goes back by the\nreverse route.\nDCE provides several semantic options. The default is at-most-once oper-\nation , in which case no call is ever carried out more than once, even in the\npresence of system crashes. In practice, what this means is that if a server\ncrashes during an RPC and then recovers quickly, the client does not repeat\nthe operation, for fear that it might already have been carried out once.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n192 CHAPTER 4. COMMUNICATION\nThe server also registers with the directory service by providing it the\nnetwork address of the server\u2019s machine and a name under which the server\ncan be looked up. Binding a client to a server proceeds as shown in Figure 4.17.\nFigure 4.17: Client-to-server binding in DCE.\nLet us assume that the client wants to bind to a video server that is locally\nknown under the name /local /multimedia /video /movies . It passes this name\nto the directory server, which returns the network address of the machine\nrunning the video server. The client then goes to the DCE daemon on that\nmachine (which has a well-known port), and asks it to look up the port of the\nvideo server in its port table. Armed with this information, the RPC can now\ntake place. On subsequent RPCs this lookup is not needed. DCE also gives\nclients the ability to do more sophisticated searches for a suitable server when\nthat is needed. Secure RPC is also an option where con\ufb01dentiality or data\nintegrity is crucial.\nPerforming an RPC\nThe actual RPC is carried out transparently and in the usual way. The client\nstub marshals the parameters to the runtime library for transmission using the\nprotocol chosen at binding time. When a message arrives at the server side, it\nis routed to the correct server based on the port contained in the incoming\nmessage. The runtime library passes the message to the server stub, which\nunmarshals the parameters and calls the server. The reply goes back by the\nreverse route.\nDCE provides several semantic options. The default is at-most-once oper-\nation , in which case no call is ever carried out more than once, even in the\npresence of system crashes. In practice, what this means is that if a server\ncrashes during an RPC and then recovers quickly, the client does not repeat\nthe operation, for fear that it might already have been carried out once.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 193\nAlternatively, it is possible to mark a remote procedure as idempotent (in\nthe IDL \ufb01le), in which case it can be repeated multiple times without harm.\nFor example, reading a speci\ufb01ed block from a \ufb01le can be tried over and over\nuntil it succeeds. When an idempotent RPC fails due to a server crash, the\nclient can wait until the server reboots and then try again. Other semantics\nare also available (but rarely used), including broadcasting the RPC to all the\nmachines on the local network. We return to RPC semantics in Section 8.3\nwhen discussing RPC in the presence of failures.\n4.3 Message-oriented communication\nRemote procedure calls and remote object invocations contribute to hiding\ncommunication in distributed systems, that is, they enhance access trans-\nparency. Unfortunately, neither mechanism is always appropriate. In particu-\nlar, when it cannot be assumed that the receiving side is executing at the time\na request is issued, alternative communication services are needed. Likewise,\nthe inherent synchronous nature of RPCs, by which a client is blocked until\nits request has been processed, may need to be replaced by something else.\nThat something else is messaging. In this section we concentrate on\nmessage-oriented communication in distributed systems by \ufb01rst taking a closer\nlook at what exactly synchronous behavior is and what its implications are.\nThen, we discuss messaging systems that assume that parties are executing\nat the time of communication. Finally, we will examine message-queuing\nsystems that allow processes to exchange information, even if the other party\nis not executing at the time communication is initiated.\nSimple transient messaging with sockets\nMany distributed systems and applications are built directly on top of the\nsimple message-oriented model offered by the transport layer. To better un-\nderstand and appreciate the message-oriented systems as part of middleware\nsolutions, we \ufb01rst discuss messaging through transport-level sockets.\nSpecial attention has been paid to standardizing the interface of the trans-\nport layer to allow programmers to make use of its entire suite of (messaging)\nprotocols through a simple set of operations. Also, standard interfaces make\nit easier to port an application to a different machine. As an example, we\nbrie\ufb02y discuss the socket interface as introduced in the 1970s in Berkeley\nUnix, and which has been adopted as a POSIX standard (with only very few\nadaptations).\nConceptually, a socket is a communication end point to which an applica-\ntion can write data that are to be sent out over the underlying network, and\nfrom which incoming data can be read. A socket forms an abstraction over the\nactual port that is used by the local operating system for a speci\ufb01c transport\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 193\nAlternatively, it is possible to mark a remote procedure as idempotent (in\nthe IDL \ufb01le), in which case it can be repeated multiple times without harm.\nFor example, reading a speci\ufb01ed block from a \ufb01le can be tried over and over\nuntil it succeeds. When an idempotent RPC fails due to a server crash, the\nclient can wait until the server reboots and then try again. Other semantics\nare also available (but rarely used), including broadcasting the RPC to all the\nmachines on the local network. We return to RPC semantics in Section 8.3\nwhen discussing RPC in the presence of failures.\n4.3 Message-oriented communication\nRemote procedure calls and remote object invocations contribute to hiding\ncommunication in distributed systems, that is, they enhance access trans-\nparency. Unfortunately, neither mechanism is always appropriate. In particu-\nlar, when it cannot be assumed that the receiving side is executing at the time\na request is issued, alternative communication services are needed. Likewise,\nthe inherent synchronous nature of RPCs, by which a client is blocked until\nits request has been processed, may need to be replaced by something else.\nThat something else is messaging. In this section we concentrate on\nmessage-oriented communication in distributed systems by \ufb01rst taking a closer\nlook at what exactly synchronous behavior is and what its implications are.\nThen, we discuss messaging systems that assume that parties are executing\nat the time of communication. Finally, we will examine message-queuing\nsystems that allow processes to exchange information, even if the other party\nis not executing at the time communication is initiated.\nSimple transient messaging with sockets\nMany distributed systems and applications are built directly on top of the\nsimple message-oriented model offered by the transport layer. To better un-\nderstand and appreciate the message-oriented systems as part of middleware\nsolutions, we \ufb01rst discuss messaging through transport-level sockets.\nSpecial attention has been paid to standardizing the interface of the trans-\nport layer to allow programmers to make use of its entire suite of (messaging)\nprotocols through a simple set of operations. Also, standard interfaces make\nit easier to port an application to a different machine. As an example, we\nbrie\ufb02y discuss the socket interface as introduced in the 1970s in Berkeley\nUnix, and which has been adopted as a POSIX standard (with only very few\nadaptations).\nConceptually, a socket is a communication end point to which an applica-\ntion can write data that are to be sent out over the underlying network, and\nfrom which incoming data can be read. A socket forms an abstraction over the\nactual port that is used by the local operating system for a speci\ufb01c transport\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "194 CHAPTER 4. COMMUNICATION\nprotocol. In the following text, we concentrate on the socket operations for\nTCP , which are shown in Figure 4.18.\nOperation Description\nsocket Create a new communication end point\nbind Attach a local address to a socket\nlisten Tell operating system what the maximum number of pending\nconnection requests should be\naccept Block caller until a connection request arrives\nconnect Actively attempt to establish a connection\nsend Send some data over the connection\nreceive Receive some data over the connection\nclose Release the connection\nFigure 4.18: The socket operations for TCP/IP .\nServers generally execute the \ufb01rst four operations, normally in the order\ngiven. When calling the socket operation, the caller creates a new commu-\nnication end point for a speci\ufb01c transport protocol. Internally, creating a\ncommunication end point means that the local operating system reserves\nresources for sending and receiving messages for the speci\ufb01ed protocol.\nThe bind operation associates a local address with the newly created socket.\nFor example, a server should bind the IP address of its machine together with\na (possibly well-known) port number to a socket. Binding tells the operating\nsystem that the server wants to receive messages only on the speci\ufb01ed address\nand port. In the case of connection-oriented communication, the address is\nused to receive incoming connection requests.\nThe listen operation is called only in the case of connection-oriented\ncommunication. It is a nonblocking call that allows the local operating sys-\ntem to reserve enough buffers for a speci\ufb01ed maximum number of pending\nconnection requests that the caller is willing to accept.\nA call to accept blocks the caller until a connection request arrives. When\na request arrives, the local operating system creates a new socket with the\nsame properties as the original one, and returns it to the caller. This approach\nwill allow the server to, for example, fork off a process that will subsequently\nhandle the actual communication through the new connection. The server can\ngo back and wait for another connection request on the original socket.\nLet us now take a look at the client side. Here, too, a socket must \ufb01rst be\ncreated using the socket operation, but explicitly binding the socket to a local\naddress is not necessary, since the operating system can dynamically allocate\na port when the connection is set up. The connect operation requires that the\ncaller speci\ufb01es the transport-level address to which a connection request is to\nbe sent. The client is blocked until a connection has been set up successfully,\nafter which both sides can start exchanging information through the send and\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n194 CHAPTER 4. COMMUNICATION\nprotocol. In the following text, we concentrate on the socket operations for\nTCP , which are shown in Figure 4.18.\nOperation Description\nsocket Create a new communication end point\nbind Attach a local address to a socket\nlisten Tell operating system what the maximum number of pending\nconnection requests should be\naccept Block caller until a connection request arrives\nconnect Actively attempt to establish a connection\nsend Send some data over the connection\nreceive Receive some data over the connection\nclose Release the connection\nFigure 4.18: The socket operations for TCP/IP .\nServers generally execute the \ufb01rst four operations, normally in the order\ngiven. When calling the socket operation, the caller creates a new commu-\nnication end point for a speci\ufb01c transport protocol. Internally, creating a\ncommunication end point means that the local operating system reserves\nresources for sending and receiving messages for the speci\ufb01ed protocol.\nThe bind operation associates a local address with the newly created socket.\nFor example, a server should bind the IP address of its machine together with\na (possibly well-known) port number to a socket. Binding tells the operating\nsystem that the server wants to receive messages only on the speci\ufb01ed address\nand port. In the case of connection-oriented communication, the address is\nused to receive incoming connection requests.\nThe listen operation is called only in the case of connection-oriented\ncommunication. It is a nonblocking call that allows the local operating sys-\ntem to reserve enough buffers for a speci\ufb01ed maximum number of pending\nconnection requests that the caller is willing to accept.\nA call to accept blocks the caller until a connection request arrives. When\na request arrives, the local operating system creates a new socket with the\nsame properties as the original one, and returns it to the caller. This approach\nwill allow the server to, for example, fork off a process that will subsequently\nhandle the actual communication through the new connection. The server can\ngo back and wait for another connection request on the original socket.\nLet us now take a look at the client side. Here, too, a socket must \ufb01rst be\ncreated using the socket operation, but explicitly binding the socket to a local\naddress is not necessary, since the operating system can dynamically allocate\na port when the connection is set up. The connect operation requires that the\ncaller speci\ufb01es the transport-level address to which a connection request is to\nbe sent. The client is blocked until a connection has been set up successfully,\nafter which both sides can start exchanging information through the send and\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 195\nFigure 4.19: Connection-oriented communication pattern using sockets.\nreceive operations. Finally, closing a connection is symmetric when using\nsockets, and is established by having both the client and server call the close\noperation. Although there are many exceptions to the rule, the general pattern\nfollowed by a client and server for connection-oriented communication using\nsockets is as shown in Figure 4.19. Details on network programming using\nsockets and other interfaces in Unix can be found in [Stevens, 1998].\nNote 4.7 (Example: A simple socket-based client-server system)\nAs an illustration of the recurring pattern in Figure 4.19, consider the simple\nsocket-based client-server system shown below (see also Note 2.1). We see the\nserver [Figure 4.20(a)] starting by creating a socket, and subsequently binding an\naddress to that socket. It calls the listen operation, and waits for an incoming\nconnection request. When the server accepts a connection, the socket library\ncreates a separate connection, conn , which is used to receive data and send a\nresponse to the connected client. The server enters a loop receiving and sending\nmessages, until no more data has been received. It then closes the connection.\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.bind((HOST, PORT))\n4s.listen(1)\n5(conn, addr) = s.accept() # returns new socket and addr. client\n6while True: # forever\n7 data = conn.recv(1024) # receive data from client\n8 if not data: break # stop if client stopped\n9 conn.send( str(data)+\"*\") # return sent data plus an \"*\"\n10conn.close() # close the connection\nFigure 4.20: (a) A simple socket-based client-server system: the server.\nThe client, shown in Figure 4.20(b), again follows the pattern from Figure 4.19.\nIt creates a socket, and calls connect to request a connection with the server. Once\nthe connection has been established, it sends a single message, waits for the\nresponse, and after printing the result, closes the connection.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 195\nFigure 4.19: Connection-oriented communication pattern using sockets.\nreceive operations. Finally, closing a connection is symmetric when using\nsockets, and is established by having both the client and server call the close\noperation. Although there are many exceptions to the rule, the general pattern\nfollowed by a client and server for connection-oriented communication using\nsockets is as shown in Figure 4.19. Details on network programming using\nsockets and other interfaces in Unix can be found in [Stevens, 1998].\nNote 4.7 (Example: A simple socket-based client-server system)\nAs an illustration of the recurring pattern in Figure 4.19, consider the simple\nsocket-based client-server system shown below (see also Note 2.1). We see the\nserver [Figure 4.20(a)] starting by creating a socket, and subsequently binding an\naddress to that socket. It calls the listen operation, and waits for an incoming\nconnection request. When the server accepts a connection, the socket library\ncreates a separate connection, conn , which is used to receive data and send a\nresponse to the connected client. The server enters a loop receiving and sending\nmessages, until no more data has been received. It then closes the connection.\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.bind((HOST, PORT))\n4s.listen(1)\n5(conn, addr) = s.accept() # returns new socket and addr. client\n6while True: # forever\n7 data = conn.recv(1024) # receive data from client\n8 if not data: break # stop if client stopped\n9 conn.send( str(data)+\"*\") # return sent data plus an \"*\"\n10conn.close() # close the connection\nFigure 4.20: (a) A simple socket-based client-server system: the server.\nThe client, shown in Figure 4.20(b), again follows the pattern from Figure 4.19.\nIt creates a socket, and calls connect to request a connection with the server. Once\nthe connection has been established, it sends a single message, waits for the\nresponse, and after printing the result, closes the connection.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "196 CHAPTER 4. COMMUNICATION\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.connect((HOST, PORT)) # connect to server (block until accepted)\n4s.send(\u2019Hello, world\u2019) # send same data\n5data = s.recv(1024) # receive the response\n6print data # print the result\n7s.close() # close the connection\nFigure 4.20: (b) A simple socket-based client-server system: the client.\nNote 4.8 (Advanced: Implementing stubs as global references revisited)\nTo provide a deeper insight in the working of sockets, let us look at a more\nelaborate example, namely the use of stubs as global references. We return to\nour example of implementing a shared list, which we now do by means of a\nlist server, implemented in the form of the Python class shown in Figure 4.21(b).\nFigure 4.21(a) shows the stub implementation of a shared list. Again, we have\nomitted code for readability.\n1class DBClient:\n2 defsendrecv(self, message):\n3 sock = socket() # create a socket\n4 sock.connect((self.host, self.port)) # connect to server\n5 sock.send(pickle.dumps(message)) # send some data\n6 result = pickle.loads(sock.recv(1024)) # receive the response\n7 sock.close() # close the connection\n8 return result\n9\n10 defcreate(self):\n11 self.listID = self.sendrecv([CREATE])\n12 return self.listID\n13\n14 defgetValue(self):\n15 return self.sendrecv([GETVALUE, self.listID])\n16\n17 defappendData(self, data):\n18 return self.sendrecv([APPEND, data, self.listID])\nFigure 4.21: (a) Implementing a list server in Python: the list structure.\nThe DBClient class represents a client-side stub that, once marshaled, can be\npassed between processes. It provides three operations associated with a list:\ncreate ,getValue , and append , with obvious semantics. A DBClient is assumed to\nbe associated with one speci\ufb01c list as managed by the server. An identi\ufb01er for\nthat list is returned when the list is created. Note how the (internal) sendrecv\noperation follows the client-side pattern explained in Figure 4.19.\nThe server maintains lists, as shown in Figure 4.21(b). Its internal data\nstructure is a setOfLists with each element being a previously created list. The\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n196 CHAPTER 4. COMMUNICATION\n1from socket import *\n2s = socket(AF_INET, SOCK_STREAM)\n3s.connect((HOST, PORT)) # connect to server (block until accepted)\n4s.send(\u2019Hello, world\u2019) # send same data\n5data = s.recv(1024) # receive the response\n6print data # print the result\n7s.close() # close the connection\nFigure 4.20: (b) A simple socket-based client-server system: the client.\nNote 4.8 (Advanced: Implementing stubs as global references revisited)\nTo provide a deeper insight in the working of sockets, let us look at a more\nelaborate example, namely the use of stubs as global references. We return to\nour example of implementing a shared list, which we now do by means of a\nlist server, implemented in the form of the Python class shown in Figure 4.21(b).\nFigure 4.21(a) shows the stub implementation of a shared list. Again, we have\nomitted code for readability.\n1class DBClient:\n2 defsendrecv(self, message):\n3 sock = socket() # create a socket\n4 sock.connect((self.host, self.port)) # connect to server\n5 sock.send(pickle.dumps(message)) # send some data\n6 result = pickle.loads(sock.recv(1024)) # receive the response\n7 sock.close() # close the connection\n8 return result\n9\n10 defcreate(self):\n11 self.listID = self.sendrecv([CREATE])\n12 return self.listID\n13\n14 defgetValue(self):\n15 return self.sendrecv([GETVALUE, self.listID])\n16\n17 defappendData(self, data):\n18 return self.sendrecv([APPEND, data, self.listID])\nFigure 4.21: (a) Implementing a list server in Python: the list structure.\nThe DBClient class represents a client-side stub that, once marshaled, can be\npassed between processes. It provides three operations associated with a list:\ncreate ,getValue , and append , with obvious semantics. A DBClient is assumed to\nbe associated with one speci\ufb01c list as managed by the server. An identi\ufb01er for\nthat list is returned when the list is created. Note how the (internal) sendrecv\noperation follows the client-side pattern explained in Figure 4.19.\nThe server maintains lists, as shown in Figure 4.21(b). Its internal data\nstructure is a setOfLists with each element being a previously created list. The\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 197\nserver simply waits for incoming requests, unmarshals the request, and checks\nwhich operation is being requested. Results are sent back to the requesting client\n(which always issues the sendrecv operation implemented as part of DBClient ).\nAgain, we see that the server follows the pattern shown in Figure 4.19: it creates\na socket, binds an address to it, informs the operating system to how many\nconnections it should listen, and then waits to accept an incoming connection\nrequest. Once a connection has been established, the server receives data, sends a\nresponse, and closes the connection again.\n1class Server:\n2 def__init__(self, port=PORT):\n3 self.host = \u2019localhost\u2019 # this machine\n4 self.port = port # the port it will listen to\n5 self.sock = socket() # socket for incoming calls\n6 self.sock.bind((self.host,self.port)) # bind socket to an address\n7 self.sock.listen(5) # max num of connections\n8 self.setOfLists = {} # init: no lists to manage\n9\n10 defrun(self):\n11 while True:\n12 (conn, addr) = self.sock.accept() # accept incoming call\n13 data = conn.recv(1024) # fetch data from client\n14 request = pickle.loads(data) # unwrap the request\n15 ifrequest[0] == CREATE: # create a list\n16 listID = len(self.setOfLists) + 1 # allocate listID\n17 self.setOfLists[listID] = [] # initialize to empty\n18 conn.send(pickle.dumps(listID)) # return ID\n19\n20 elif request[0] == APPEND: # append request\n21 listID = request[2] # fetch listID\n22 data = request[1] # fetch data to append\n23 self.setOfLists[listID].append(data) # append it to the list\n24 conn.send(pickle.dumps(OK)) # return an OK\n25\n26 elif request[0] == GETVALUE: # read request\n27 listID = request[1] # fetch listID\n28 result = self.setOfLists[listID] # get the elements\n29 conn.send(pickle.dumps(result)) # return the list\n30 conn.close() # close the connection\nFigure 4.21: (b) Implementing a list server in Python: the server.\nTo use a stub as a global reference, we represent each client application by\nmeans of the class Client shown in Figure 4.21(c). The class is instantiated in the\nsame process running the application (exempli\ufb01ed by the value of self.host ),\nand will be listening on a speci\ufb01c port for messages from other applications,\nas well as the server. Otherwise, it merely sends and receives messages, coded\nthrough the operations sendTo and recvAny , respectively.\nNow consider the code shown in Figure 4.21(d), which mimics two client\napplications. The \ufb01rst one creates a new list and appends data to it. Then note\nhow dbClient1 is simply sent to the other client. Under the hood, we now know\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 197\nserver simply waits for incoming requests, unmarshals the request, and checks\nwhich operation is being requested. Results are sent back to the requesting client\n(which always issues the sendrecv operation implemented as part of DBClient ).\nAgain, we see that the server follows the pattern shown in Figure 4.19: it creates\na socket, binds an address to it, informs the operating system to how many\nconnections it should listen, and then waits to accept an incoming connection\nrequest. Once a connection has been established, the server receives data, sends a\nresponse, and closes the connection again.\n1class Server:\n2 def__init__(self, port=PORT):\n3 self.host = \u2019localhost\u2019 # this machine\n4 self.port = port # the port it will listen to\n5 self.sock = socket() # socket for incoming calls\n6 self.sock.bind((self.host,self.port)) # bind socket to an address\n7 self.sock.listen(5) # max num of connections\n8 self.setOfLists = {} # init: no lists to manage\n9\n10 defrun(self):\n11 while True:\n12 (conn, addr) = self.sock.accept() # accept incoming call\n13 data = conn.recv(1024) # fetch data from client\n14 request = pickle.loads(data) # unwrap the request\n15 ifrequest[0] == CREATE: # create a list\n16 listID = len(self.setOfLists) + 1 # allocate listID\n17 self.setOfLists[listID] = [] # initialize to empty\n18 conn.send(pickle.dumps(listID)) # return ID\n19\n20 elif request[0] == APPEND: # append request\n21 listID = request[2] # fetch listID\n22 data = request[1] # fetch data to append\n23 self.setOfLists[listID].append(data) # append it to the list\n24 conn.send(pickle.dumps(OK)) # return an OK\n25\n26 elif request[0] == GETVALUE: # read request\n27 listID = request[1] # fetch listID\n28 result = self.setOfLists[listID] # get the elements\n29 conn.send(pickle.dumps(result)) # return the list\n30 conn.close() # close the connection\nFigure 4.21: (b) Implementing a list server in Python: the server.\nTo use a stub as a global reference, we represent each client application by\nmeans of the class Client shown in Figure 4.21(c). The class is instantiated in the\nsame process running the application (exempli\ufb01ed by the value of self.host ),\nand will be listening on a speci\ufb01c port for messages from other applications,\nas well as the server. Otherwise, it merely sends and receives messages, coded\nthrough the operations sendTo and recvAny , respectively.\nNow consider the code shown in Figure 4.21(d), which mimics two client\napplications. The \ufb01rst one creates a new list and appends data to it. Then note\nhow dbClient1 is simply sent to the other client. Under the hood, we now know\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "198 CHAPTER 4. COMMUNICATION\nthat it is marshaled in the operation sendTo (line 12) of class Client shown in\nFigure 4.21(c).\n1class Client:\n2 def__init__(self, port):\n3 self.host = \u2019localhost\u2019 # this machine\n4 self.port = port # port it will listen to\n5 self.sock = socket() # socket for incoming calls\n6 self.sock.bind((self.host, self.port)) # bind socket to an address\n7 self.sock.listen(2) # max num connections\n8\n9 defsendTo(self, host, port, data):\n10 sock = socket()\n11 sock.connect((host, port)) # connect to server (blocking call)\n12 sock.send(pickle.dumps(data)) # send some data\n13 sock.close()\n14\n15 defrecvAny(self):\n16 (conn, addr) = self.sock.accept()\n17 return conn.recv(1024)\nFigure 4.21: (c) Implementing a list server in Python: the client.\n1pid = os.fork()\n2ifpid == 0:\n3 client1 = Client(CLIENT1) # create client\n4 dbClient1 = DBClient(HOST,PORT) # create reference\n5 dbClient1.create() # create new list\n6 dbClient1.appendData(\u2019Client 1\u2019) # append some data\n7 client1.sendTo(HOSTCL2,CLIENT2,dbClient1) # send to other client\n8\n9pid = os.fork()\n10ifpid == 0:\n11 client2 = Client(CLIENT2) # create a new client\n12 data = client2.recvAny() # block until data is sent\n13 dbClient2 = pickle.loads(data) # receive reference\n14 dbClient2.appendData(\u2019Client 2\u2019) # append data to same list\nFigure 4.21: (d) Passing stubs as references.\nThe second client simply waits for an incoming message (line 12), unmarshals\nthe result, knowing that it is a DBClient instance, and subsequently appends some\nmore data to the same list as the one the \ufb01rst client appended data. Indeed, an\ninstance of DBClient is seen to be passed as global reference, seemingly along\nwith all the operations that go with the associated class.\nAdvanced transient messaging\nThe standard socket-based approach toward transient messaging is very basic\nand as such, rather brittle: a mistake is easily made. Furthermore, sockets\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n198 CHAPTER 4. COMMUNICATION\nthat it is marshaled in the operation sendTo (line 12) of class Client shown in\nFigure 4.21(c).\n1class Client:\n2 def__init__(self, port):\n3 self.host = \u2019localhost\u2019 # this machine\n4 self.port = port # port it will listen to\n5 self.sock = socket() # socket for incoming calls\n6 self.sock.bind((self.host, self.port)) # bind socket to an address\n7 self.sock.listen(2) # max num connections\n8\n9 defsendTo(self, host, port, data):\n10 sock = socket()\n11 sock.connect((host, port)) # connect to server (blocking call)\n12 sock.send(pickle.dumps(data)) # send some data\n13 sock.close()\n14\n15 defrecvAny(self):\n16 (conn, addr) = self.sock.accept()\n17 return conn.recv(1024)\nFigure 4.21: (c) Implementing a list server in Python: the client.\n1pid = os.fork()\n2ifpid == 0:\n3 client1 = Client(CLIENT1) # create client\n4 dbClient1 = DBClient(HOST,PORT) # create reference\n5 dbClient1.create() # create new list\n6 dbClient1.appendData(\u2019Client 1\u2019) # append some data\n7 client1.sendTo(HOSTCL2,CLIENT2,dbClient1) # send to other client\n8\n9pid = os.fork()\n10ifpid == 0:\n11 client2 = Client(CLIENT2) # create a new client\n12 data = client2.recvAny() # block until data is sent\n13 dbClient2 = pickle.loads(data) # receive reference\n14 dbClient2.appendData(\u2019Client 2\u2019) # append data to same list\nFigure 4.21: (d) Passing stubs as references.\nThe second client simply waits for an incoming message (line 12), unmarshals\nthe result, knowing that it is a DBClient instance, and subsequently appends some\nmore data to the same list as the one the \ufb01rst client appended data. Indeed, an\ninstance of DBClient is seen to be passed as global reference, seemingly along\nwith all the operations that go with the associated class.\nAdvanced transient messaging\nThe standard socket-based approach toward transient messaging is very basic\nand as such, rather brittle: a mistake is easily made. Furthermore, sockets\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 199\nessentially support only TCP or UDP , meaning that any extra facility for\nmessaging needs to be implemented separately by an application programmer.\nIn practice, we do often need more advanced approaches for message-oriented\ncommunication to make network programming easier, to expand beyond the\nfunctionality offered by existing networking protocols, to make better use of\nlocal resources, and so on.\nUsing messaging patterns: ZeroMQ\nOne approach toward making network programming easier is based on the\nobservation that many messaging applications, or their components, can be\neffectively organized according to a few simple communication patterns. By\nsubsequently providing enhancements to sockets for each of these patterns,\nit may become easier to develop a networked, distributed application. This\napproach has been followed in ZeroMQ and documented in [Hintjens, 2013;\nAkgul, 2013].\nLike in the Berkeley approach, ZeroMQ also provides sockets through\nwhich all communication takes place. Actual message transmission gener-\nally takes place over TCP connections, and like TCP , all communication is\nessentially connection-oriented, meaning that a connection will \ufb01rst be set up\nbetween a sender and receiver before message transmission can take place.\nHowever, setting up, and maintaining connections is kept mostly under the\nhood: an application programmer need not bother with those issues. To\nfurther simplify matters, a socket may be bound to multiple addresses, ef-\nfectively allowing a server to handle messages from very different sources\nthrough a single interface. For example, a server can listen to multiple ports\nusing a single blocking receive operation. ZeroMQ sockets can thus support\nmany-to-one communication instead of just one-to-one communication as is the\ncase with standard Berkeley sockets. To complete the story: ZeroMQ sockets\nalso support one-to-many communication, i.e., multicasting.\nEssential to ZeroMQ is that communication is asynchronous: a sender will\nnormally continue after having submitted a message to the underlying com-\nmunication subsystem. An interesting side effect of combining asynchronous\nwith connection-oriented communication, is that a process can request a con-\nnection setup, and subsequently send a message even if the recipient is not yet\nup-and-running and ready to accept incoming connection requests, let alone\nincoming messages. What happens, of course, is that a connection request and\nsubsequent messages are queued at the sender\u2019s side, while a separate thread\nas part of ZeroMQ \u2019s library will take care that eventually the connection is set\nup and messages are transmitted to the recipient.\nSimplifying matters, ZeroMQ establishes a higher level of abstraction in\nsocket-based communication by pairing sockets: a speci\ufb01c type of socket used\nfor sending messages is paired with a corresponding socket type for receiving\nmessages. Each pair of socket types corresponds to a communication pattern.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 199\nessentially support only TCP or UDP , meaning that any extra facility for\nmessaging needs to be implemented separately by an application programmer.\nIn practice, we do often need more advanced approaches for message-oriented\ncommunication to make network programming easier, to expand beyond the\nfunctionality offered by existing networking protocols, to make better use of\nlocal resources, and so on.\nUsing messaging patterns: ZeroMQ\nOne approach toward making network programming easier is based on the\nobservation that many messaging applications, or their components, can be\neffectively organized according to a few simple communication patterns. By\nsubsequently providing enhancements to sockets for each of these patterns,\nit may become easier to develop a networked, distributed application. This\napproach has been followed in ZeroMQ and documented in [Hintjens, 2013;\nAkgul, 2013].\nLike in the Berkeley approach, ZeroMQ also provides sockets through\nwhich all communication takes place. Actual message transmission gener-\nally takes place over TCP connections, and like TCP , all communication is\nessentially connection-oriented, meaning that a connection will \ufb01rst be set up\nbetween a sender and receiver before message transmission can take place.\nHowever, setting up, and maintaining connections is kept mostly under the\nhood: an application programmer need not bother with those issues. To\nfurther simplify matters, a socket may be bound to multiple addresses, ef-\nfectively allowing a server to handle messages from very different sources\nthrough a single interface. For example, a server can listen to multiple ports\nusing a single blocking receive operation. ZeroMQ sockets can thus support\nmany-to-one communication instead of just one-to-one communication as is the\ncase with standard Berkeley sockets. To complete the story: ZeroMQ sockets\nalso support one-to-many communication, i.e., multicasting.\nEssential to ZeroMQ is that communication is asynchronous: a sender will\nnormally continue after having submitted a message to the underlying com-\nmunication subsystem. An interesting side effect of combining asynchronous\nwith connection-oriented communication, is that a process can request a con-\nnection setup, and subsequently send a message even if the recipient is not yet\nup-and-running and ready to accept incoming connection requests, let alone\nincoming messages. What happens, of course, is that a connection request and\nsubsequent messages are queued at the sender\u2019s side, while a separate thread\nas part of ZeroMQ \u2019s library will take care that eventually the connection is set\nup and messages are transmitted to the recipient.\nSimplifying matters, ZeroMQ establishes a higher level of abstraction in\nsocket-based communication by pairing sockets: a speci\ufb01c type of socket used\nfor sending messages is paired with a corresponding socket type for receiving\nmessages. Each pair of socket types corresponds to a communication pattern.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "200 CHAPTER 4. COMMUNICATION\nThe three most important communication patterns supported by ZeroMQ are\nrequest-reply ,publish-subscribe , and pipeline .\nThe request-reply pattern is used in traditional client-server communi-\ncation, like the ones normally used for remote procedure calls. A client\napplication uses a request socket (of type REQ) to send a request message\nto a server and expects the latter to respond with an appropriate response.\nThe server is assumed to use a reply socket (of type REP). The request-reply\npattern simpli\ufb01es matters for developers by avoiding the need to call the\nlisten operation, as well as the accept operation. Moreover, when a server\nreceives a message, a subsequent call to send is automatically targeted toward\nthe original sender. Likewise, when a client calls the recv operation (for re-\nceiving a message) after having sent a message, ZeroMQ assumes the client is\nwaiting for a response from the original recipient. Note that this approach was\neffectively encoded in the local sendrecv operation of Figure 4.21 discussed in\nNote 4.8.\nNote 4.9 (Example: The request-reply pattern)\nLet us look at a simple programming example to illustrate the request-reply\npattern. Figure 4.22(a) shows a server that appends an asterisk to a received\nmessage. As before, it creates a socket, and binds it to a combination of a protocol\n(in this case TCP), and a host and port. In our example, the server is willing\nto accept incoming connection requests on two different ports. It then waits for\nincoming messages. The request-reply pattern effectively ties the receipt of a\nmessage to the subsequent response. In other words, when the server calls send , it\nwill transmit a message to the same client from which it previously had received\na message. Of course, this simplicity can be achieved only if the programmer\nindeed abides to the request-reply pattern.\n1import zmq\n2context = zmq.Context()\n3\n4p1 = \"tcp://\"+ HOST +\":\"+ PORT1 # how and where to connect\n5p2 = \"tcp://\"+ HOST +\":\"+ PORT2 # how and where to connect\n6s = context.socket(zmq.REP) # create reply socket\n7\n8s.bind(p1) # bind socket to address\n9s.bind(p2) # bind socket to address\n10while True:\n11 message = s.recv() # wait for incoming message\n12 if not \"STOP\" inmessage: # if not to stop...\n13 s.send(message + \"*\") # append \"*\" to message\n14 else: # else...\n15 break # break out of loop and end\nFigure 4.22: (a) A ZeroMQ client-server system based: the server.\nThe client, shown in Figure 4.22(b) does what is expected: it creates a socket\nand connects to the associated server. When it sends a message, it can expect to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n200 CHAPTER 4. COMMUNICATION\nThe three most important communication patterns supported by ZeroMQ are\nrequest-reply ,publish-subscribe , and pipeline .\nThe request-reply pattern is used in traditional client-server communi-\ncation, like the ones normally used for remote procedure calls. A client\napplication uses a request socket (of type REQ) to send a request message\nto a server and expects the latter to respond with an appropriate response.\nThe server is assumed to use a reply socket (of type REP). The request-reply\npattern simpli\ufb01es matters for developers by avoiding the need to call the\nlisten operation, as well as the accept operation. Moreover, when a server\nreceives a message, a subsequent call to send is automatically targeted toward\nthe original sender. Likewise, when a client calls the recv operation (for re-\nceiving a message) after having sent a message, ZeroMQ assumes the client is\nwaiting for a response from the original recipient. Note that this approach was\neffectively encoded in the local sendrecv operation of Figure 4.21 discussed in\nNote 4.8.\nNote 4.9 (Example: The request-reply pattern)\nLet us look at a simple programming example to illustrate the request-reply\npattern. Figure 4.22(a) shows a server that appends an asterisk to a received\nmessage. As before, it creates a socket, and binds it to a combination of a protocol\n(in this case TCP), and a host and port. In our example, the server is willing\nto accept incoming connection requests on two different ports. It then waits for\nincoming messages. The request-reply pattern effectively ties the receipt of a\nmessage to the subsequent response. In other words, when the server calls send , it\nwill transmit a message to the same client from which it previously had received\na message. Of course, this simplicity can be achieved only if the programmer\nindeed abides to the request-reply pattern.\n1import zmq\n2context = zmq.Context()\n3\n4p1 = \"tcp://\"+ HOST +\":\"+ PORT1 # how and where to connect\n5p2 = \"tcp://\"+ HOST +\":\"+ PORT2 # how and where to connect\n6s = context.socket(zmq.REP) # create reply socket\n7\n8s.bind(p1) # bind socket to address\n9s.bind(p2) # bind socket to address\n10while True:\n11 message = s.recv() # wait for incoming message\n12 if not \"STOP\" inmessage: # if not to stop...\n13 s.send(message + \"*\") # append \"*\" to message\n14 else: # else...\n15 break # break out of loop and end\nFigure 4.22: (a) A ZeroMQ client-server system based: the server.\nThe client, shown in Figure 4.22(b) does what is expected: it creates a socket\nand connects to the associated server. When it sends a message, it can expect to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 201\nreceive, from that same server, a response. By sending the string \u201c STOP \u201d, it tells\nthe server it is done, after which the server will actually stop.\n1import zmq\n2context = zmq.Context()\n3\n4p1 = \"tcp://\"+ HOST +\":\"+ PORT1 # how and where to connect\n5s = context.socket(zmq.REQ) # create request socket\n6\n7s.connect(p1) # block until connected\n8s.send(\"Hello world 1\") # send message\n9message = s.recv() # block until response\n10s.send(\"STOP\") # tell server to stop\n11print message # print result\nFigure 4.22: (b) A ZeroMQ client-server system: the client.\nInterestingly, the asynchronous nature of ZeroMQ allows one to start the client\nbefore starting the server. An implication is that if, in this example, we would\nstart the server, then a client, and after a while a second client, that the latter\nwill be blocked until the server is restarted. Furthermore, note that ZeroMQ\ndoes not require the programmer to specify how many bytes are expected to\nbe received. Unlike TCP , ZeroMQ uses messages instead of bytestreams as its\nunderlying model of communication.\nIn the case of a publish-subscribe pattern , clients subscribe to speci\ufb01c\nmessages that are published by servers. We came across this pattern brie\ufb02y\nin Chapter 1 when discussing event-based coordination. In effect, only the\nmessages to which the client has subscribed will be transmitted. If a server is\npublishing messages to which no one has subscribed, these messages will be\nlost. In its simplest form, this pattern establishes multicasting messages from\na server to several clients. The server is assumed to use a socket of type PUB,\nwhile each client must use SUBtype sockets. Each client socket is connected to\nthe socket of the server. By default, a client subscribes to no speci\ufb01c message.\nThis means that as long as no explicit subscription is provided, a client will\nnot receive a message published by the server.\nNote 4.10 (Example: The publish-subscribe pattern)\nAgain, let us make this pattern more concrete through a simple example. Fig-\nure 4.23(a) shows an admittedly naive time server that publishes its current, local\ntime, through a PUBsocket. The local time is published every \ufb01ve seconds, for any\ninterested client.\nA client is equally simple, as shown in Figure 4.23(b). It \ufb01rst creates a SUB\nsocket which it connects to the corresponding PUBsocket of the server. In order\nto receive the appropriate messages, it needs to subscribe to messages that have\nTIME as their tag. In our example, a client will simply print the \ufb01rst \ufb01ve messages\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 201\nreceive, from that same server, a response. By sending the string \u201c STOP \u201d, it tells\nthe server it is done, after which the server will actually stop.\n1import zmq\n2context = zmq.Context()\n3\n4p1 = \"tcp://\"+ HOST +\":\"+ PORT1 # how and where to connect\n5s = context.socket(zmq.REQ) # create request socket\n6\n7s.connect(p1) # block until connected\n8s.send(\"Hello world 1\") # send message\n9message = s.recv() # block until response\n10s.send(\"STOP\") # tell server to stop\n11print message # print result\nFigure 4.22: (b) A ZeroMQ client-server system: the client.\nInterestingly, the asynchronous nature of ZeroMQ allows one to start the client\nbefore starting the server. An implication is that if, in this example, we would\nstart the server, then a client, and after a while a second client, that the latter\nwill be blocked until the server is restarted. Furthermore, note that ZeroMQ\ndoes not require the programmer to specify how many bytes are expected to\nbe received. Unlike TCP , ZeroMQ uses messages instead of bytestreams as its\nunderlying model of communication.\nIn the case of a publish-subscribe pattern , clients subscribe to speci\ufb01c\nmessages that are published by servers. We came across this pattern brie\ufb02y\nin Chapter 1 when discussing event-based coordination. In effect, only the\nmessages to which the client has subscribed will be transmitted. If a server is\npublishing messages to which no one has subscribed, these messages will be\nlost. In its simplest form, this pattern establishes multicasting messages from\na server to several clients. The server is assumed to use a socket of type PUB,\nwhile each client must use SUBtype sockets. Each client socket is connected to\nthe socket of the server. By default, a client subscribes to no speci\ufb01c message.\nThis means that as long as no explicit subscription is provided, a client will\nnot receive a message published by the server.\nNote 4.10 (Example: The publish-subscribe pattern)\nAgain, let us make this pattern more concrete through a simple example. Fig-\nure 4.23(a) shows an admittedly naive time server that publishes its current, local\ntime, through a PUBsocket. The local time is published every \ufb01ve seconds, for any\ninterested client.\nA client is equally simple, as shown in Figure 4.23(b). It \ufb01rst creates a SUB\nsocket which it connects to the corresponding PUBsocket of the server. In order\nto receive the appropriate messages, it needs to subscribe to messages that have\nTIME as their tag. In our example, a client will simply print the \ufb01rst \ufb01ve messages\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "202 CHAPTER 4. COMMUNICATION\nreceived from the server. Note that we can have as many clients as we want: the\nserver\u2019s message will be multicast to all subscribers.\n1import zmq, time\n2\n3context = zmq.Context()\n4s = context.socket(zmq.PUB) # create a publisher socket\n5p = \"tcp://\"+ HOST +\":\"+ PORT # how and where to communicate\n6s.bind(p) # bind socket to the address\n7while True:\n8 time.sleep(5) # wait every 5 seconds\n9 s.send(\"TIME \" + time.asctime()) # publish the current time\nFigure 4.23: (a) A multicasting socket-based time server\n1import zmq\n2\n3context = zmq.Context()\n4s = context.socket(zmq.SUB) # create a subscriber socket\n5p = \"tcp://\"+ HOST +\":\"+ PORT # how and where to communicate\n6s.connect(p) # connect to the server\n7s.setsockopt(zmq.SUBSCRIBE, \"TIME\") # subscribe to TIME messages\n8\n9foriin range (5): # Five iterations\n10 time = s.recv() # receive a message\n11 print time\nFigure 4.23: (b) A client for the multicasting socket-based time server.\nFinally, the pipeline pattern is characterized by the fact that a process\nwants to push out its results, assuming that there are other processes that\nwant to pull in those results. The essence of the pipeline pattern is that a\npushing process does not really care which other process pulls in its results:\nthe \ufb01rst available one will do just \ufb01ne. Likewise, any process pulling in\nresults from multiple other processes will do so from the \ufb01rst pushing process\nmaking its results available. The intention of the pipeline pattern is thus seen\nto keep as many processes working as possible, pushing results through a\npipeline of processes as quickly as possible.\nNote 4.11 (Example: The pipeline pattern)\nAs our last example, consider the following template for keeping a collection of\nworker tasks busy. Figure 4.24(a) shows the code for a so-called farmer task : a\nprocess generating tasks to be picked up by others. In this example, we simulate\nthe task by letting the farmer pick a random number modeling the duration, or\nload, of the work to be done. This workload is then sent to the push socket,\neffectively being queued until another process picks it up.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n202 CHAPTER 4. COMMUNICATION\nreceived from the server. Note that we can have as many clients as we want: the\nserver\u2019s message will be multicast to all subscribers.\n1import zmq, time\n2\n3context = zmq.Context()\n4s = context.socket(zmq.PUB) # create a publisher socket\n5p = \"tcp://\"+ HOST +\":\"+ PORT # how and where to communicate\n6s.bind(p) # bind socket to the address\n7while True:\n8 time.sleep(5) # wait every 5 seconds\n9 s.send(\"TIME \" + time.asctime()) # publish the current time\nFigure 4.23: (a) A multicasting socket-based time server\n1import zmq\n2\n3context = zmq.Context()\n4s = context.socket(zmq.SUB) # create a subscriber socket\n5p = \"tcp://\"+ HOST +\":\"+ PORT # how and where to communicate\n6s.connect(p) # connect to the server\n7s.setsockopt(zmq.SUBSCRIBE, \"TIME\") # subscribe to TIME messages\n8\n9foriin range (5): # Five iterations\n10 time = s.recv() # receive a message\n11 print time\nFigure 4.23: (b) A client for the multicasting socket-based time server.\nFinally, the pipeline pattern is characterized by the fact that a process\nwants to push out its results, assuming that there are other processes that\nwant to pull in those results. The essence of the pipeline pattern is that a\npushing process does not really care which other process pulls in its results:\nthe \ufb01rst available one will do just \ufb01ne. Likewise, any process pulling in\nresults from multiple other processes will do so from the \ufb01rst pushing process\nmaking its results available. The intention of the pipeline pattern is thus seen\nto keep as many processes working as possible, pushing results through a\npipeline of processes as quickly as possible.\nNote 4.11 (Example: The pipeline pattern)\nAs our last example, consider the following template for keeping a collection of\nworker tasks busy. Figure 4.24(a) shows the code for a so-called farmer task : a\nprocess generating tasks to be picked up by others. In this example, we simulate\nthe task by letting the farmer pick a random number modeling the duration, or\nload, of the work to be done. This workload is then sent to the push socket,\neffectively being queued until another process picks it up.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 203\n1import zmq, time, pickle, sys, random\n2\n3context = zmq.Context()\n4me = str(sys.argv[1])\n5s = context.socket(zmq.PUSH) # create a push socket\n6src = SRC1 ifme == \u20191\u2019 else SRC2 # check task source host\n7prt = PORT1 ifme == \u20191\u2019 else PORT2 # check task source port\n8p = \"tcp://\"+ src +\":\"+ prt # how and where to connect\n9s.bind(p) # bind socket to address\n10\n11foriin range (100): # generate 100 workloads\n12 workload = random.randint(1, 100) # compute workload\n13 s.send(pickle.dumps((me,workload))) # send workload to worker\nFigure 4.24: (a) A task simulating the generation of work.\n1import zmq, time, pickle, sys\n2\n3context = zmq.Context()\n4me = str(sys.argv[1])\n5r = context.socket(zmq.PULL) # create a pull socket\n6p1 = \"tcp://\"+ SRC1 +\":\"+ PORT1 # address first task source\n7p2 = \"tcp://\"+ SRC2 +\":\"+ PORT2 # address second task source\n8r.connect(p1) # connect to task source 1\n9r.connect(p2) # connect to task source 2\n10\n11while True:\n12 work = pickle.loads(r.recv()) # receive work from a source\n13 time.sleep(work[1]*0.01) # pretend to work\nFigure 4.24: (b) A worker task.\nSuch other processes are known as worker tasks , of which a sketch is given\nin Figure 4.24(b). A worker task connects a single pull socket to, in this case, two\nspeci\ufb01c farmer tasks. Once it picks up some work, it simulates that it is actually\ndoing something by sleeping for a number of tens of milliseconds proportional to\nthe received workload.\nThe semantics of this push-pull pattern is such that the \ufb01rst available worker\nwill pick up work from either farmer, and likewise, if there are multiple workers\nready to pick up work, each one of them will be provided with a task. How work\ndistribution is actually done in a fair manner requires some speci\ufb01c attention\nwhich we will not discuss further here.\nThe Message-Passing Interface (MPI)\nWith the advent of high-performance multicomputers, developers have been\nlooking for message-oriented operations that would allow them to easily write\nhighly ef\ufb01cient applications. This means that the operations should be at a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 203\n1import zmq, time, pickle, sys, random\n2\n3context = zmq.Context()\n4me = str(sys.argv[1])\n5s = context.socket(zmq.PUSH) # create a push socket\n6src = SRC1 ifme == \u20191\u2019 else SRC2 # check task source host\n7prt = PORT1 ifme == \u20191\u2019 else PORT2 # check task source port\n8p = \"tcp://\"+ src +\":\"+ prt # how and where to connect\n9s.bind(p) # bind socket to address\n10\n11foriin range (100): # generate 100 workloads\n12 workload = random.randint(1, 100) # compute workload\n13 s.send(pickle.dumps((me,workload))) # send workload to worker\nFigure 4.24: (a) A task simulating the generation of work.\n1import zmq, time, pickle, sys\n2\n3context = zmq.Context()\n4me = str(sys.argv[1])\n5r = context.socket(zmq.PULL) # create a pull socket\n6p1 = \"tcp://\"+ SRC1 +\":\"+ PORT1 # address first task source\n7p2 = \"tcp://\"+ SRC2 +\":\"+ PORT2 # address second task source\n8r.connect(p1) # connect to task source 1\n9r.connect(p2) # connect to task source 2\n10\n11while True:\n12 work = pickle.loads(r.recv()) # receive work from a source\n13 time.sleep(work[1]*0.01) # pretend to work\nFigure 4.24: (b) A worker task.\nSuch other processes are known as worker tasks , of which a sketch is given\nin Figure 4.24(b). A worker task connects a single pull socket to, in this case, two\nspeci\ufb01c farmer tasks. Once it picks up some work, it simulates that it is actually\ndoing something by sleeping for a number of tens of milliseconds proportional to\nthe received workload.\nThe semantics of this push-pull pattern is such that the \ufb01rst available worker\nwill pick up work from either farmer, and likewise, if there are multiple workers\nready to pick up work, each one of them will be provided with a task. How work\ndistribution is actually done in a fair manner requires some speci\ufb01c attention\nwhich we will not discuss further here.\nThe Message-Passing Interface (MPI)\nWith the advent of high-performance multicomputers, developers have been\nlooking for message-oriented operations that would allow them to easily write\nhighly ef\ufb01cient applications. This means that the operations should be at a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "204 CHAPTER 4. COMMUNICATION\nconvenient level of abstraction (to ease application development), and that\ntheir implementation incurs only minimal overhead. Sockets were deemed\ninsuf\ufb01cient for two reasons. First, they were at the wrong level of abstraction\nby supporting only simple send and receive operations. Second, sockets\nhad been designed to communicate across networks using general-purpose\nprotocol stacks such as TCP/IP . They were not considered suitable for the\nproprietary protocols developed for high-speed interconnection networks,\nsuch as those used in high-performance server clusters. Those protocols\nrequired an interface that could handle more advanced features, such as\ndifferent forms of buffering and synchronization.\nThe result was that most interconnection networks and high-performance\nmulticomputers were shipped with proprietary communication libraries.\nThese libraries offered a wealth of high-level and generally ef\ufb01cient com-\nmunication operations. Of course, all libraries were mutually incompatible, so\nthat application developers now had a portability problem.\nThe need to be hardware and platform independent eventually lead to\nthe de\ufb01nition of a standard for message passing, simply called the Message-\nPassing Interface orMPI . MPI is designed for parallel applications and\nas such is tailored to transient communication. It makes direct use of the\nunderlying network. Also, it assumes that serious failures such as process\ncrashes or network partitions are fatal and do not require automatic recovery.\nMPI assumes communication takes place within a known group of pro-\ncesses. Each group is assigned an identi\ufb01er. Each process within a group is\nalso assigned a (local) identi\ufb01er. A ( groupID ,processID ) pair therefore uniquely\nidenti\ufb01es the source or destination of a message, and is used instead of a\ntransport-level address. There may be several, possibly overlapping groups of\nprocesses involved in a computation and that are all executing at the same\ntime.\nAt the core of MPI are messaging operations to support transient commu-\nnication, of which the most intuitive ones are summarized in Figure 4.25.\nOperation Description\nMPI_bsend Append outgoing message to a local send buffer\nMPI_send Send a message and wait until copied to local or remote\nbuffer\nMPI_ssend Send a message and wait until transmission starts\nMPI_sendrecv Send a message and wait for reply\nMPI_isend Pass reference to outgoing message, and continue\nMPI_issend Pass reference to outgoing message, and wait until receipt\nstarts\nMPI_recv Receive a message; block if there is none\nMPI_irecv Check if there is an incoming message, but do not block\nFigure 4.25: Some of the most intuitive message-passing operations of MPI.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n204 CHAPTER 4. COMMUNICATION\nconvenient level of abstraction (to ease application development), and that\ntheir implementation incurs only minimal overhead. Sockets were deemed\ninsuf\ufb01cient for two reasons. First, they were at the wrong level of abstraction\nby supporting only simple send and receive operations. Second, sockets\nhad been designed to communicate across networks using general-purpose\nprotocol stacks such as TCP/IP . They were not considered suitable for the\nproprietary protocols developed for high-speed interconnection networks,\nsuch as those used in high-performance server clusters. Those protocols\nrequired an interface that could handle more advanced features, such as\ndifferent forms of buffering and synchronization.\nThe result was that most interconnection networks and high-performance\nmulticomputers were shipped with proprietary communication libraries.\nThese libraries offered a wealth of high-level and generally ef\ufb01cient com-\nmunication operations. Of course, all libraries were mutually incompatible, so\nthat application developers now had a portability problem.\nThe need to be hardware and platform independent eventually lead to\nthe de\ufb01nition of a standard for message passing, simply called the Message-\nPassing Interface orMPI . MPI is designed for parallel applications and\nas such is tailored to transient communication. It makes direct use of the\nunderlying network. Also, it assumes that serious failures such as process\ncrashes or network partitions are fatal and do not require automatic recovery.\nMPI assumes communication takes place within a known group of pro-\ncesses. Each group is assigned an identi\ufb01er. Each process within a group is\nalso assigned a (local) identi\ufb01er. A ( groupID ,processID ) pair therefore uniquely\nidenti\ufb01es the source or destination of a message, and is used instead of a\ntransport-level address. There may be several, possibly overlapping groups of\nprocesses involved in a computation and that are all executing at the same\ntime.\nAt the core of MPI are messaging operations to support transient commu-\nnication, of which the most intuitive ones are summarized in Figure 4.25.\nOperation Description\nMPI_bsend Append outgoing message to a local send buffer\nMPI_send Send a message and wait until copied to local or remote\nbuffer\nMPI_ssend Send a message and wait until transmission starts\nMPI_sendrecv Send a message and wait for reply\nMPI_isend Pass reference to outgoing message, and continue\nMPI_issend Pass reference to outgoing message, and wait until receipt\nstarts\nMPI_recv Receive a message; block if there is none\nMPI_irecv Check if there is an incoming message, but do not block\nFigure 4.25: Some of the most intuitive message-passing operations of MPI.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 205\nTransient asynchronous communication is supported by means of the\nMPI_bsend operation. The sender submits a message for transmission, which\nis generally \ufb01rst copied to a local buffer in the MPI runtime system. When the\nmessage has been copied, the sender continues. The local MPI runtime system\nwill remove the message from its local buffer and take care of transmission as\nsoon as a receiver has called a receive operation.\nThere is also a blocking send operation, called MPI_send , of which the\nsemantics are implementation dependent. The operation MPI_send may either\nblock the caller until the speci\ufb01ed message has been copied to the MPI\nruntime system at the sender\u2019s side, or until the receiver has initiated a receive\noperation. Synchronous communication by which the sender blocks until its\nrequest is accepted for further processing is available through the MPI_ssend\noperation. Finally, the strongest form of synchronous communication is also\nsupported: when a sender calls MPI_sendrecv , it sends a request to the receiver\nand blocks until the latter returns a reply. Basically, this operation corresponds\nto a normal RPC.\nBoth MPI_send and MPI_ssend have variants that avoid copying messages\nfrom user buffers to buffers internal to the local MPI runtime system. These\nvariants essentially correspond to a form of asynchronous communication.\nWith MPI_isend , a sender passes a pointer to the message after which the MPI\nruntime system takes care of communication. The sender immediately contin-\nues. To prevent overwriting the message before communication completes,\nMPI offers operations to check for completion, or even to block if required.\nAs with MPI_send , whether the message has actually been transferred to the\nreceiver or that it has merely been copied by the local MPI runtime system to\nan internal buffer is left unspeci\ufb01ed.\nLikewise, with MPI_issend , a sender also passes only a pointer to the MPI\nruntime system. When the runtime system indicates it has processed the\nmessage, the sender is then guaranteed that the receiver has accepted the\nmessage and is now working on it.\nThe operation MPI_recv is called to receive a message; it blocks the\ncaller until a message arrives. There is also an asynchronous variant, called\nMPI_irecv , by which a receiver indicates that it is prepared to accept a mes-\nsage. The receiver can check whether or not a message has indeed arrived, or\nblock until one does.\nThe semantics of MPI communication operations are not always straight-\nforward, and different operations can sometimes be interchanged without\naffecting the correctness of a program. The of\ufb01cial reason why so many differ-\nent forms of communication are supported is that it gives implementers of\nMPI systems enough possibilities for optimizing performance. Cynics might\nsay the committee could not make up its collective mind, so it threw in every-\nthing. By now, MPI is in its third version with over 440 operations available.\nBeing designed for high-performance parallel applications, it is perhaps easier\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 205\nTransient asynchronous communication is supported by means of the\nMPI_bsend operation. The sender submits a message for transmission, which\nis generally \ufb01rst copied to a local buffer in the MPI runtime system. When the\nmessage has been copied, the sender continues. The local MPI runtime system\nwill remove the message from its local buffer and take care of transmission as\nsoon as a receiver has called a receive operation.\nThere is also a blocking send operation, called MPI_send , of which the\nsemantics are implementation dependent. The operation MPI_send may either\nblock the caller until the speci\ufb01ed message has been copied to the MPI\nruntime system at the sender\u2019s side, or until the receiver has initiated a receive\noperation. Synchronous communication by which the sender blocks until its\nrequest is accepted for further processing is available through the MPI_ssend\noperation. Finally, the strongest form of synchronous communication is also\nsupported: when a sender calls MPI_sendrecv , it sends a request to the receiver\nand blocks until the latter returns a reply. Basically, this operation corresponds\nto a normal RPC.\nBoth MPI_send and MPI_ssend have variants that avoid copying messages\nfrom user buffers to buffers internal to the local MPI runtime system. These\nvariants essentially correspond to a form of asynchronous communication.\nWith MPI_isend , a sender passes a pointer to the message after which the MPI\nruntime system takes care of communication. The sender immediately contin-\nues. To prevent overwriting the message before communication completes,\nMPI offers operations to check for completion, or even to block if required.\nAs with MPI_send , whether the message has actually been transferred to the\nreceiver or that it has merely been copied by the local MPI runtime system to\nan internal buffer is left unspeci\ufb01ed.\nLikewise, with MPI_issend , a sender also passes only a pointer to the MPI\nruntime system. When the runtime system indicates it has processed the\nmessage, the sender is then guaranteed that the receiver has accepted the\nmessage and is now working on it.\nThe operation MPI_recv is called to receive a message; it blocks the\ncaller until a message arrives. There is also an asynchronous variant, called\nMPI_irecv , by which a receiver indicates that it is prepared to accept a mes-\nsage. The receiver can check whether or not a message has indeed arrived, or\nblock until one does.\nThe semantics of MPI communication operations are not always straight-\nforward, and different operations can sometimes be interchanged without\naffecting the correctness of a program. The of\ufb01cial reason why so many differ-\nent forms of communication are supported is that it gives implementers of\nMPI systems enough possibilities for optimizing performance. Cynics might\nsay the committee could not make up its collective mind, so it threw in every-\nthing. By now, MPI is in its third version with over 440 operations available.\nBeing designed for high-performance parallel applications, it is perhaps easier\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "206 CHAPTER 4. COMMUNICATION\nto understand its diversity. More on MPI can be found in [Gropp et al., 2016].\nThe complete MPI-3 reference can be found in [MPI Forum, 2012].\nMessage-oriented persistent communication\nWe now come to an important class of message-oriented middleware services,\ngenerally known as message-queuing systems , or just Message-Oriented\nMiddleware (MOM ). Message-queuing systems provide extensive support for\npersistent asynchronous communication. The essence of these systems is that\nthey offer intermediate-term storage capacity for messages, without requiring\neither the sender or receiver to be active during message transmission. An\nimportant difference with sockets and MPI is that message-queuing systems\nare typically targeted to support message transfers that are allowed to take\nminutes instead of seconds or milliseconds.\nMessage-queuing model\nThe basic idea behind a message-queuing system is that applications commu-\nnicate by inserting messages in speci\ufb01c queues. These messages are forwarded\nover a series of communication servers and are eventually delivered to the\ndestination, even if it was down when the message was sent. In practice, most\ncommunication servers are directly connected to each other. In other words, a\nmessage is generally transferred directly to a destination server. In principle,\neach application has its own private queue to which other applications can\nsend messages. A queue can be read only by its associated application, but it\nis also possible for multiple applications to share a single queue.\nAn important aspect of message-queuing systems is that a sender is gen-\nerally given only the guarantees that its message will eventually be inserted\nin the recipient\u2019s queue. No guarantees are given about when, or even if\nthe message will actually be read, which is completely determined by the\nbehavior of the recipient.\nThese semantics permit communication to be loosely coupled in time.\nThere is thus no need for the receiver to be executing when a message is being\nsent to its queue. Likewise, there is no need for the sender to be executing\nat the moment its message is picked up by the receiver. The sender and\nreceiver can execute completely independently of each other. In fact, once a\nmessage has been deposited in a queue, it will remain there until it is removed,\nirrespective of whether its sender or receiver is executing. This gives us four\ncombinations with respect to the execution mode of the sender and receiver,\nas shown in Figure 4.26.\nIn Figure 4.26(a), both the sender and receiver execute during the entire\ntransmission of a message. In Figure 4.26(b), only the sender is executing,\nwhile the receiver is passive, that is, in a state in which message delivery is not\npossible. Nevertheless, the sender can still send messages. The combination of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n206 CHAPTER 4. COMMUNICATION\nto understand its diversity. More on MPI can be found in [Gropp et al., 2016].\nThe complete MPI-3 reference can be found in [MPI Forum, 2012].\nMessage-oriented persistent communication\nWe now come to an important class of message-oriented middleware services,\ngenerally known as message-queuing systems , or just Message-Oriented\nMiddleware (MOM ). Message-queuing systems provide extensive support for\npersistent asynchronous communication. The essence of these systems is that\nthey offer intermediate-term storage capacity for messages, without requiring\neither the sender or receiver to be active during message transmission. An\nimportant difference with sockets and MPI is that message-queuing systems\nare typically targeted to support message transfers that are allowed to take\nminutes instead of seconds or milliseconds.\nMessage-queuing model\nThe basic idea behind a message-queuing system is that applications commu-\nnicate by inserting messages in speci\ufb01c queues. These messages are forwarded\nover a series of communication servers and are eventually delivered to the\ndestination, even if it was down when the message was sent. In practice, most\ncommunication servers are directly connected to each other. In other words, a\nmessage is generally transferred directly to a destination server. In principle,\neach application has its own private queue to which other applications can\nsend messages. A queue can be read only by its associated application, but it\nis also possible for multiple applications to share a single queue.\nAn important aspect of message-queuing systems is that a sender is gen-\nerally given only the guarantees that its message will eventually be inserted\nin the recipient\u2019s queue. No guarantees are given about when, or even if\nthe message will actually be read, which is completely determined by the\nbehavior of the recipient.\nThese semantics permit communication to be loosely coupled in time.\nThere is thus no need for the receiver to be executing when a message is being\nsent to its queue. Likewise, there is no need for the sender to be executing\nat the moment its message is picked up by the receiver. The sender and\nreceiver can execute completely independently of each other. In fact, once a\nmessage has been deposited in a queue, it will remain there until it is removed,\nirrespective of whether its sender or receiver is executing. This gives us four\ncombinations with respect to the execution mode of the sender and receiver,\nas shown in Figure 4.26.\nIn Figure 4.26(a), both the sender and receiver execute during the entire\ntransmission of a message. In Figure 4.26(b), only the sender is executing,\nwhile the receiver is passive, that is, in a state in which message delivery is not\npossible. Nevertheless, the sender can still send messages. The combination of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 207\nFigure 4.26: Four combinations for loosely-coupled communication using\nqueues.\na passive sender and an executing receiver is shown in Figure 4.26(c). In this\ncase, the receiver can read messages that were sent to it, but it is not necessary\nthat their respective senders are executing as well. Finally, in Figure 4.26(d),\nwe see the situation that the system is storing (and possibly transmitting)\nmessages even while sender and receiver are passive. One may argue that\nonly if this last con\ufb01guration is supported, the message-queuing system truly\nprovides persistent messaging.\nMessages can, in principle, contain any data. The only important aspect\nfrom the perspective of middleware is that messages are properly addressed.\nIn practice, addressing is done by providing a systemwide unique name of the\ndestination queue. In some cases, message size may be limited, although it is\nalso possible that the underlying system takes care of fragmenting and assem-\nbling large messages in a way that is completely transparent to applications.\nAn effect of this approach is that the basic interface offered to applications\ncan be extremely simple, as shown in Figure 4.27.\nThe putoperation is called by a sender to pass a message to the underlying\nsystem that is to be appended to the speci\ufb01ed queue. As we explained, this is a\nnonblocking call. The getoperation is a blocking call by which an authorized\nprocess can remove the longest pending message in the speci\ufb01ed queue. The\nprocess is blocked only if the queue is empty. Variations on this call allow\nsearching for a speci\ufb01c message in the queue, for example, using a priority, or\na matching pattern. The nonblocking variant is given by the poll operation.\nIf the queue is empty, or if a speci\ufb01c message could not be found, the calling\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 207\nFigure 4.26: Four combinations for loosely-coupled communication using\nqueues.\na passive sender and an executing receiver is shown in Figure 4.26(c). In this\ncase, the receiver can read messages that were sent to it, but it is not necessary\nthat their respective senders are executing as well. Finally, in Figure 4.26(d),\nwe see the situation that the system is storing (and possibly transmitting)\nmessages even while sender and receiver are passive. One may argue that\nonly if this last con\ufb01guration is supported, the message-queuing system truly\nprovides persistent messaging.\nMessages can, in principle, contain any data. The only important aspect\nfrom the perspective of middleware is that messages are properly addressed.\nIn practice, addressing is done by providing a systemwide unique name of the\ndestination queue. In some cases, message size may be limited, although it is\nalso possible that the underlying system takes care of fragmenting and assem-\nbling large messages in a way that is completely transparent to applications.\nAn effect of this approach is that the basic interface offered to applications\ncan be extremely simple, as shown in Figure 4.27.\nThe putoperation is called by a sender to pass a message to the underlying\nsystem that is to be appended to the speci\ufb01ed queue. As we explained, this is a\nnonblocking call. The getoperation is a blocking call by which an authorized\nprocess can remove the longest pending message in the speci\ufb01ed queue. The\nprocess is blocked only if the queue is empty. Variations on this call allow\nsearching for a speci\ufb01c message in the queue, for example, using a priority, or\na matching pattern. The nonblocking variant is given by the poll operation.\nIf the queue is empty, or if a speci\ufb01c message could not be found, the calling\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "208 CHAPTER 4. COMMUNICATION\nOperation Description\nput Append a message to a speci\ufb01ed queue\nget Block until the speci\ufb01ed queue is nonempty, and remove the\n\ufb01rst message\npoll Check a speci\ufb01ed queue for messages, and remove the \ufb01rst.\nNever block\nnotify Install a handler to be called when a message is put into the\nspeci\ufb01ed queue\nFigure 4.27: Basic interface to a queue in a message-queuing system.\nprocess simply continues.\nFinally, most queuing systems also allow a process to install a handler as a\ncallback function , which is automatically invoked whenever a message is put\ninto the queue. Callbacks can also be used to automatically start a process that\nwill fetch messages from the queue if no process is currently executing. This\napproach is often implemented by means of a daemon on the receiver\u2019s side\nthat continuously monitors the queue for incoming messages and handles\naccordingly.\nGeneral architecture of a message-queuing system\nLet us now take a closer look at what a general message-queuing system looks\nlike. First of all, queues are managed by queue managers . A queue manager\nis either a separate process, or is implemented by means of a library that is\nlinked with an application. Secondly, as a rule of thumb, an application can\nput messages only into a local queue. Likewise, getting a message is possible\nby extracting it from a local queue only. As a consequence, if a queue manager\nQMAhandling the queues for an application Aruns as a separate process,\nboth processes QMAand Awill generally be placed on the same machine, or\nat worst on the same LAN. Also note that if allqueue managers are linked\ninto their respective applications, we can no longer speak of a persistent\nasynchronous messaging system.\nIf applications can put messages only into local queues, then clearly each\nmessage will have to carry information concerning its destination. It is the\nqueue manager\u2019s task to make sure that a message reaches its destination.\nThis brings us to a number of issues.\nIn the \ufb01rst place, we need to consider how the destination queue is ad-\ndressed. Obviously, to enhance location transparency, it is preferable that\nqueues have logical, location-independent names. Assuming that a queue man-\nager is implemented as a separate process, using logical names implies that\neach name should be associated with a contact address , such as a ( host,port)-\npair, and that the name-to-address mapping is readily available to a queue\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n208 CHAPTER 4. COMMUNICATION\nOperation Description\nput Append a message to a speci\ufb01ed queue\nget Block until the speci\ufb01ed queue is nonempty, and remove the\n\ufb01rst message\npoll Check a speci\ufb01ed queue for messages, and remove the \ufb01rst.\nNever block\nnotify Install a handler to be called when a message is put into the\nspeci\ufb01ed queue\nFigure 4.27: Basic interface to a queue in a message-queuing system.\nprocess simply continues.\nFinally, most queuing systems also allow a process to install a handler as a\ncallback function , which is automatically invoked whenever a message is put\ninto the queue. Callbacks can also be used to automatically start a process that\nwill fetch messages from the queue if no process is currently executing. This\napproach is often implemented by means of a daemon on the receiver\u2019s side\nthat continuously monitors the queue for incoming messages and handles\naccordingly.\nGeneral architecture of a message-queuing system\nLet us now take a closer look at what a general message-queuing system looks\nlike. First of all, queues are managed by queue managers . A queue manager\nis either a separate process, or is implemented by means of a library that is\nlinked with an application. Secondly, as a rule of thumb, an application can\nput messages only into a local queue. Likewise, getting a message is possible\nby extracting it from a local queue only. As a consequence, if a queue manager\nQMAhandling the queues for an application Aruns as a separate process,\nboth processes QMAand Awill generally be placed on the same machine, or\nat worst on the same LAN. Also note that if allqueue managers are linked\ninto their respective applications, we can no longer speak of a persistent\nasynchronous messaging system.\nIf applications can put messages only into local queues, then clearly each\nmessage will have to carry information concerning its destination. It is the\nqueue manager\u2019s task to make sure that a message reaches its destination.\nThis brings us to a number of issues.\nIn the \ufb01rst place, we need to consider how the destination queue is ad-\ndressed. Obviously, to enhance location transparency, it is preferable that\nqueues have logical, location-independent names. Assuming that a queue man-\nager is implemented as a separate process, using logical names implies that\neach name should be associated with a contact address , such as a ( host,port)-\npair, and that the name-to-address mapping is readily available to a queue\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 209\nmanager, as shown in Figure 4.28. In practice, a contact address carries more\ninformation, notably the protocol to be used, such as TCP or UDP . We came\nacross such contact addresses in our examples of advanced sockets in, for\nexample, Note 4.9.\nFigure 4.28: The relationship between queue-level naming and network-level\naddressing.\nA second issue that we need to consider is how the name-to-address\nmapping is actually made available to a queue manager. A common approach\nis to simply implement the mapping as a lookup table and copy that table\nto all managers. Obviously, this leads to a maintenance problem, for every\ntime that a new queue is added or named, many, if not all tables need to be\nupdated. There are various ways to alleviate such problems, which we will\ndiscuss in Chapter 5.\nThis brings us to a third issue, related to the problems of ef\ufb01ciently\nmaintaining name-to-address mappings. We have implicitly assumed that\nif a destination queue at manager QMBis known to queue manager QMA,\nthen QMAcan directly contact QMBto transfer messages. In effect, this means\nthat (the contact address of) each queue manager should be known to all\nothers. Obviously, when dealing with very large message-queuing systems,\nwe will have a scalability problem. In practice, there are often special queue\nmanagers that operate as routers : they forward incoming messages to other\nqueue managers. In this way, a message-queuing system may gradually grow\ninto a complete, application-level, overlay network .\nIf only a few routers need to know about the network topology, then a\nsource queue manager need only to know to which adjacent router, say R\nit should forward a message, given a destination queue. Router R, in turn,\nmay only need to keep track of its adjacent routers to see where to forward\nthe message to, and so on. Of course, we still need to have name-to-address\nmappings for all queue managers, including the routers, but it is not dif\ufb01cult\nto imagine that such tables can be much smaller and easier to maintain.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 209\nmanager, as shown in Figure 4.28. In practice, a contact address carries more\ninformation, notably the protocol to be used, such as TCP or UDP . We came\nacross such contact addresses in our examples of advanced sockets in, for\nexample, Note 4.9.\nFigure 4.28: The relationship between queue-level naming and network-level\naddressing.\nA second issue that we need to consider is how the name-to-address\nmapping is actually made available to a queue manager. A common approach\nis to simply implement the mapping as a lookup table and copy that table\nto all managers. Obviously, this leads to a maintenance problem, for every\ntime that a new queue is added or named, many, if not all tables need to be\nupdated. There are various ways to alleviate such problems, which we will\ndiscuss in Chapter 5.\nThis brings us to a third issue, related to the problems of ef\ufb01ciently\nmaintaining name-to-address mappings. We have implicitly assumed that\nif a destination queue at manager QMBis known to queue manager QMA,\nthen QMAcan directly contact QMBto transfer messages. In effect, this means\nthat (the contact address of) each queue manager should be known to all\nothers. Obviously, when dealing with very large message-queuing systems,\nwe will have a scalability problem. In practice, there are often special queue\nmanagers that operate as routers : they forward incoming messages to other\nqueue managers. In this way, a message-queuing system may gradually grow\ninto a complete, application-level, overlay network .\nIf only a few routers need to know about the network topology, then a\nsource queue manager need only to know to which adjacent router, say R\nit should forward a message, given a destination queue. Router R, in turn,\nmay only need to keep track of its adjacent routers to see where to forward\nthe message to, and so on. Of course, we still need to have name-to-address\nmappings for all queue managers, including the routers, but it is not dif\ufb01cult\nto imagine that such tables can be much smaller and easier to maintain.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "210 CHAPTER 4. COMMUNICATION\nMessage brokers\nAn important application area of message-queuing systems is integrating\nexisting and new applications into a single, coherent distributed information\nsystem. If we assume that communication with an application takes place\nthrough messages, then integration requires that applications can understand\nthe messages they receive. In practice, this requires the sender to have its\noutgoing messages in the same format as that of the receiver, but also that\nits messages adhere to the same semantics as those expected by the receiver.\nSender and receiver essentially need to speak the same language, that is,\nadhere to the same messaging protocol.\nThe problem with this approach is that each time an application Ais\nadded to the system having its own messaging protocol, then for each other\napplication Bthat is to communicate with Awe will need to provide the means\nfor converting their respective messages. In a system with Napplications, we\nwill thus need N\u0002Nmessaging protocol converters.\nAn alternative is to agree on a common messaging protocol, as is done\nwith traditional network protocols. Unfortunately, this approach will generally\nnot work for message-queuing systems. The problem is the level of abstraction\nat which these systems operate. A common messaging protocol makes sense\nonly if the collection of processes that make use of that protocol indeed\nhave enough in common. If the collection of applications that make up a\ndistributed information system is highly diverse (which it often is), then\ninventing a one-size-\ufb01ts-all solution is simply not going to work.\nIf we focus only on the format and meaning of messages, commonality can\nbe achieved by lifting the level of abstraction as is done with XML messages.\nIn this case, messages carry information on their own organization, and what\nhas been standardized is the way that they can describe their content. As a\nconsequence, an application can provide information on the organization of\nits messages that can be automatically processed. Of course, this information\nis generally not enough: we also need to make sure that the semantics of\nmessages are well understood.\nGiven these problems, the general approach is to learn to live with differ-\nences, and try to provide the means to make conversions as simple as possible.\nIn message-queuing systems, conversions are handled by special nodes in a\nqueuing network, known as message brokers . A message broker acts as an\napplication-level gateway in a message-queuing system. Its main purpose is to\nconvert incoming messages so that they can be understood by the destination\napplication. Note that to a message-queuing system, a message broker is just\nanother application, as shown in Figure 4.29. In other words, a message broker\nis generally not considered to be an integral part of the queuing system.\nA message broker can be as simple as a reformatter for messages. For\nexample, assume an incoming message contains a table from a database in\nwhich records are separated by a special end-of-record delimiter and \ufb01elds\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n210 CHAPTER 4. COMMUNICATION\nMessage brokers\nAn important application area of message-queuing systems is integrating\nexisting and new applications into a single, coherent distributed information\nsystem. If we assume that communication with an application takes place\nthrough messages, then integration requires that applications can understand\nthe messages they receive. In practice, this requires the sender to have its\noutgoing messages in the same format as that of the receiver, but also that\nits messages adhere to the same semantics as those expected by the receiver.\nSender and receiver essentially need to speak the same language, that is,\nadhere to the same messaging protocol.\nThe problem with this approach is that each time an application Ais\nadded to the system having its own messaging protocol, then for each other\napplication Bthat is to communicate with Awe will need to provide the means\nfor converting their respective messages. In a system with Napplications, we\nwill thus need N\u0002Nmessaging protocol converters.\nAn alternative is to agree on a common messaging protocol, as is done\nwith traditional network protocols. Unfortunately, this approach will generally\nnot work for message-queuing systems. The problem is the level of abstraction\nat which these systems operate. A common messaging protocol makes sense\nonly if the collection of processes that make use of that protocol indeed\nhave enough in common. If the collection of applications that make up a\ndistributed information system is highly diverse (which it often is), then\ninventing a one-size-\ufb01ts-all solution is simply not going to work.\nIf we focus only on the format and meaning of messages, commonality can\nbe achieved by lifting the level of abstraction as is done with XML messages.\nIn this case, messages carry information on their own organization, and what\nhas been standardized is the way that they can describe their content. As a\nconsequence, an application can provide information on the organization of\nits messages that can be automatically processed. Of course, this information\nis generally not enough: we also need to make sure that the semantics of\nmessages are well understood.\nGiven these problems, the general approach is to learn to live with differ-\nences, and try to provide the means to make conversions as simple as possible.\nIn message-queuing systems, conversions are handled by special nodes in a\nqueuing network, known as message brokers . A message broker acts as an\napplication-level gateway in a message-queuing system. Its main purpose is to\nconvert incoming messages so that they can be understood by the destination\napplication. Note that to a message-queuing system, a message broker is just\nanother application, as shown in Figure 4.29. In other words, a message broker\nis generally not considered to be an integral part of the queuing system.\nA message broker can be as simple as a reformatter for messages. For\nexample, assume an incoming message contains a table from a database in\nwhich records are separated by a special end-of-record delimiter and \ufb01elds\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 211\nFigure 4.29: The general organization of a message broker in a message-\nqueuing system.\nwithin a record have a known, \ufb01xed length. If the destination application\nexpects a different delimiter between records, and also expects that \ufb01elds have\nvariable lengths, a message broker can be used to convert messages to the\nformat expected by the destination.\nIn a more advanced setting, a message broker may act as an application-\nlevel gateway, in which information on the messaging protocol of several\napplications has been encoded. In general, for each pair of applications, we\nwill have a separate subprogram capable of converting messages between the\ntwo applications. In Figure 4.29, this subprogram is drawn as a plugin to\nemphasize that such parts can be dynamically plugged in, or removed from a\nbroker.\nFinally, note that in many cases a message broker is used for advanced\nenterprise application integration (EAI), as we discussed in Section 1.3. In\nthis case, rather than (only) converting messages, a broker is responsible for\nmatching applications based on the messages that are being exchanged. In\nsuch a publish-subscribe model, applications send messages in the form of\npublishing . In particular, they may publish a message on topic X, which is then\nsent to the broker. Applications that have stated their interest in messages\non topic X, that is, who have subscribed to those messages, will then receive\nthese messages from the broker. More advanced forms of mediation are also\npossible.\nAt the heart of a message broker lies a repository of rules for transforming\na message of one type to another. The problem is de\ufb01ning the rules and de-\nveloping the plugins. Most message broker products come with sophisticated\ndevelopment tools, but the bottom line is still that the repository needs to be\n\ufb01lled by experts. Here we see a perfect example where commercial products\nare often misleadingly said to provide \u201cintelligence,\u201d where, in fact, the only\nintelligence is to be found in the heads of those experts.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 211\nFigure 4.29: The general organization of a message broker in a message-\nqueuing system.\nwithin a record have a known, \ufb01xed length. If the destination application\nexpects a different delimiter between records, and also expects that \ufb01elds have\nvariable lengths, a message broker can be used to convert messages to the\nformat expected by the destination.\nIn a more advanced setting, a message broker may act as an application-\nlevel gateway, in which information on the messaging protocol of several\napplications has been encoded. In general, for each pair of applications, we\nwill have a separate subprogram capable of converting messages between the\ntwo applications. In Figure 4.29, this subprogram is drawn as a plugin to\nemphasize that such parts can be dynamically plugged in, or removed from a\nbroker.\nFinally, note that in many cases a message broker is used for advanced\nenterprise application integration (EAI), as we discussed in Section 1.3. In\nthis case, rather than (only) converting messages, a broker is responsible for\nmatching applications based on the messages that are being exchanged. In\nsuch a publish-subscribe model, applications send messages in the form of\npublishing . In particular, they may publish a message on topic X, which is then\nsent to the broker. Applications that have stated their interest in messages\non topic X, that is, who have subscribed to those messages, will then receive\nthese messages from the broker. More advanced forms of mediation are also\npossible.\nAt the heart of a message broker lies a repository of rules for transforming\na message of one type to another. The problem is de\ufb01ning the rules and de-\nveloping the plugins. Most message broker products come with sophisticated\ndevelopment tools, but the bottom line is still that the repository needs to be\n\ufb01lled by experts. Here we see a perfect example where commercial products\nare often misleadingly said to provide \u201cintelligence,\u201d where, in fact, the only\nintelligence is to be found in the heads of those experts.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "212 CHAPTER 4. COMMUNICATION\nNote 4.12 (More information: A note on message-queuing systems)\nConsidering what we have said about message-queuing systems, it would appear\nthat they have long existed in the form of implementations for e-mail services.\nE-mail systems are generally implemented through a collection of mail servers\nthat store and forward messages on behalf of the users on hosts directly connected\nto the server. Routing is generally left out, as e-mail systems can make direct use\nof the underlying transport services. For example, in the mail protocol for the\nInternet, SMTP [Postel, 1982], a message is transferred by setting up a direct TCP\nconnection to the destination mail server.\nWhat makes e-mail systems special compared to message-queuing systems\nis that they are primarily aimed at providing direct support for end users. This\nexplains, for example, why a number of groupware applications are based directly\non an e-mail system [Khosha\ufb01an and Buckiewicz, 1995]. In addition, e-mail\nsystems may have very speci\ufb01c requirements such as automatic message \ufb01ltering,\nsupport for advanced messaging databases (e.g., to easily retrieve previously\nstored messages), and so on.\nGeneral message-queuing systems are not aimed at supporting only end users.\nAn important issue is that they are set up to enable persistent communication\nbetween processes, regardless of whether a process is running a user application,\nhandling access to a database, performing computations, and so on. This approach\nleads to a different set of requirements for message-queuing systems than pure e-\nmail systems. For example, e-mail systems generally need not provide guaranteed\nmessage delivery, message priorities, logging facilities, ef\ufb01cient multicasting, load\nbalancing, fault tolerance, and so on for general usage.\nGeneral-purpose message-queuing systems, therefore, have a wide range\nof applications, including e-mail, work\ufb02ow, groupware, and batch processing.\nHowever, as we have stated before, the most important application area is the in-\ntegration of a (possibly widely-dispersed) collection of databases and applications\ninto a federated information system [Hohpe and Woolf, 2004]. For example, a\nquery expanding several databases may need to be split into subqueries that are\nforwarded to individual databases. Message-queuing systems assist by providing\nthe basic means to package each subquery into a message and routing it to the\nappropriate database. Other communication facilities we have discussed in this\nchapter are far less appropriate.\nExample: IBM\u2019s WebSphere message-queuing system\nTo help understand how message-queuing systems work in practice, let us\ntake a look at the message-queuing system that is part of IBM\u2019s WebSphere.\nFormerly known as MQSeries, it is now referred to as WebSphere MQ . There\nis a wealth of documentation on WebSphere MQ, and in the following we can\nonly resort to the basic principles. Taylor [2012] provides a gentle introduction\nto WebSphere MQ, whereas further details can be found in [Davies and\nBroadhurst, 2005; Aranha et al., 2013].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n212 CHAPTER 4. COMMUNICATION\nNote 4.12 (More information: A note on message-queuing systems)\nConsidering what we have said about message-queuing systems, it would appear\nthat they have long existed in the form of implementations for e-mail services.\nE-mail systems are generally implemented through a collection of mail servers\nthat store and forward messages on behalf of the users on hosts directly connected\nto the server. Routing is generally left out, as e-mail systems can make direct use\nof the underlying transport services. For example, in the mail protocol for the\nInternet, SMTP [Postel, 1982], a message is transferred by setting up a direct TCP\nconnection to the destination mail server.\nWhat makes e-mail systems special compared to message-queuing systems\nis that they are primarily aimed at providing direct support for end users. This\nexplains, for example, why a number of groupware applications are based directly\non an e-mail system [Khosha\ufb01an and Buckiewicz, 1995]. In addition, e-mail\nsystems may have very speci\ufb01c requirements such as automatic message \ufb01ltering,\nsupport for advanced messaging databases (e.g., to easily retrieve previously\nstored messages), and so on.\nGeneral message-queuing systems are not aimed at supporting only end users.\nAn important issue is that they are set up to enable persistent communication\nbetween processes, regardless of whether a process is running a user application,\nhandling access to a database, performing computations, and so on. This approach\nleads to a different set of requirements for message-queuing systems than pure e-\nmail systems. For example, e-mail systems generally need not provide guaranteed\nmessage delivery, message priorities, logging facilities, ef\ufb01cient multicasting, load\nbalancing, fault tolerance, and so on for general usage.\nGeneral-purpose message-queuing systems, therefore, have a wide range\nof applications, including e-mail, work\ufb02ow, groupware, and batch processing.\nHowever, as we have stated before, the most important application area is the in-\ntegration of a (possibly widely-dispersed) collection of databases and applications\ninto a federated information system [Hohpe and Woolf, 2004]. For example, a\nquery expanding several databases may need to be split into subqueries that are\nforwarded to individual databases. Message-queuing systems assist by providing\nthe basic means to package each subquery into a message and routing it to the\nappropriate database. Other communication facilities we have discussed in this\nchapter are far less appropriate.\nExample: IBM\u2019s WebSphere message-queuing system\nTo help understand how message-queuing systems work in practice, let us\ntake a look at the message-queuing system that is part of IBM\u2019s WebSphere.\nFormerly known as MQSeries, it is now referred to as WebSphere MQ . There\nis a wealth of documentation on WebSphere MQ, and in the following we can\nonly resort to the basic principles. Taylor [2012] provides a gentle introduction\nto WebSphere MQ, whereas further details can be found in [Davies and\nBroadhurst, 2005; Aranha et al., 2013].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 213\nOverview\nThe basic architecture of an MQ queuing network is quite straightforward,\nand is shown in Figure 4.30. All queues are managed by queue managers . A\nqueue manager is responsible for removing messages from its send queues,\nand forwarding those to other queue managers. Likewise, a queue manager\nis responsible for handling incoming messages by picking them up from the\nunderlying network and subsequently storing each message in the appropriate\ninput queue. To give an impression of what messaging can mean: a message\nhas a maximum default size of 4 MB, but this can be increased up to 100\nMB. A queue is normally restricted to 2 GB of data, but depending on the\nunderlying operating system, this maximum can be easily set higher.\nQueue managers are pairwise connected through message channels , which\nare an abstraction of transport-level connections. A message channel is a uni-\ndirectional, reliable connection between a sending and a receiving queue\nmanager, through which queued messages are transported. For example, an\nInternet-based message channel is implemented as a TCP connection. Each of\nthe two ends of a message channel is managed by a message channel agent\n(MCA ). A sending MCA is basically doing nothing else than checking send\nqueues for a message, wrapping it into a transport-level packet, and sending\nit along the connection to its associated receiving MCA. Likewise, the basic\ntask of a receiving MCA is listening for an incoming packet, unwrapping it,\nand subsequently storing the unwrapped message into the appropriate queue.\nFigure 4.30: General organization of IBM\u2019s message-queuing system.\nQueue managers can be linked into the same process as the application\nfor which it manages the queues. In that case, the queues are hidden from\nthe application behind a standard interface, but effectively can be directly\nmanipulated by the application. An alternative organization is one in which\nqueue managers and applications run on separate machines. In that case,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 213\nOverview\nThe basic architecture of an MQ queuing network is quite straightforward,\nand is shown in Figure 4.30. All queues are managed by queue managers . A\nqueue manager is responsible for removing messages from its send queues,\nand forwarding those to other queue managers. Likewise, a queue manager\nis responsible for handling incoming messages by picking them up from the\nunderlying network and subsequently storing each message in the appropriate\ninput queue. To give an impression of what messaging can mean: a message\nhas a maximum default size of 4 MB, but this can be increased up to 100\nMB. A queue is normally restricted to 2 GB of data, but depending on the\nunderlying operating system, this maximum can be easily set higher.\nQueue managers are pairwise connected through message channels , which\nare an abstraction of transport-level connections. A message channel is a uni-\ndirectional, reliable connection between a sending and a receiving queue\nmanager, through which queued messages are transported. For example, an\nInternet-based message channel is implemented as a TCP connection. Each of\nthe two ends of a message channel is managed by a message channel agent\n(MCA ). A sending MCA is basically doing nothing else than checking send\nqueues for a message, wrapping it into a transport-level packet, and sending\nit along the connection to its associated receiving MCA. Likewise, the basic\ntask of a receiving MCA is listening for an incoming packet, unwrapping it,\nand subsequently storing the unwrapped message into the appropriate queue.\nFigure 4.30: General organization of IBM\u2019s message-queuing system.\nQueue managers can be linked into the same process as the application\nfor which it manages the queues. In that case, the queues are hidden from\nthe application behind a standard interface, but effectively can be directly\nmanipulated by the application. An alternative organization is one in which\nqueue managers and applications run on separate machines. In that case,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "214 CHAPTER 4. COMMUNICATION\nthe application is offered the same interface as when the queue manager is\nco-located on the same machine. However, the interface is implemented as\na proxy that communicates with the queue manager using traditional RPC-\nbased synchronous communication. In this way, MQ basically retains the\nmodel that only queues local to an application can be accessed.\nChannels\nAn important component of MQ is formed by the message channels. Each\nmessage channel has exactly one associated send queue from which it fetches\nthe messages it should transfer to the other end. Transfer along the channel\ncan take place only if both its sending and receiving MCA are up and running.\nApart from starting both MCAs manually, there are several alternative ways\nto start a channel, some of which we discuss next.\nOne alternative is to have an application directly start its end of a channel\nby activating the sending or receiving MCA. However, from a transparency\npoint of view, this is not a very attractive alternative. A better approach to\nstart a sending MCA is to con\ufb01gure the channel\u2019s send queue to set off a\ntrigger when a message is \ufb01rst put into the queue. That trigger is associated\nwith a handler to start the sending MCA so that it can remove messages from\nthe send queue.\nAnother alternative is to start an MCA over the network. In particular,\nif one side of a channel is already active, it can send a control message\nrequesting that the other MCA is to be started. Such a control message is sent\nto a daemon listening to a well-known address on the same machine as where\nthe other MCA is to be started.\nChannels are stopped automatically after a speci\ufb01ed time has expired\nduring which no more messages were dropped into the send queue.\nEach MCA has a set of associated attributes that determine the overall\nbehavior of a channel. Some of the attributes are listed in Figure 4.31. Attribute\nvalues of the sending and receiving MCA should be compatible and perhaps\nnegotiated \ufb01rst before a channel can be set up. For example, both MCAs\nshould obviously support the same transport protocol. An example of a\nnonnegotiable attribute is whether or not messages are to be delivered in the\nsame order as they are put into the send queue. If one MCA wants FIFO\ndelivery, the other must comply. An example of a negotiable attribute value is\nthe maximum message length, which will simply be chosen as the minimum\nvalue speci\ufb01ed by either MCA.\nMessage transfer\nTo transfer a message from one queue manager to another (possibly remote)\nqueue manager, it is necessary that each message carries its destination ad-\ndress, for which a transmission header is used. An address in MQ consists of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n214 CHAPTER 4. COMMUNICATION\nthe application is offered the same interface as when the queue manager is\nco-located on the same machine. However, the interface is implemented as\na proxy that communicates with the queue manager using traditional RPC-\nbased synchronous communication. In this way, MQ basically retains the\nmodel that only queues local to an application can be accessed.\nChannels\nAn important component of MQ is formed by the message channels. Each\nmessage channel has exactly one associated send queue from which it fetches\nthe messages it should transfer to the other end. Transfer along the channel\ncan take place only if both its sending and receiving MCA are up and running.\nApart from starting both MCAs manually, there are several alternative ways\nto start a channel, some of which we discuss next.\nOne alternative is to have an application directly start its end of a channel\nby activating the sending or receiving MCA. However, from a transparency\npoint of view, this is not a very attractive alternative. A better approach to\nstart a sending MCA is to con\ufb01gure the channel\u2019s send queue to set off a\ntrigger when a message is \ufb01rst put into the queue. That trigger is associated\nwith a handler to start the sending MCA so that it can remove messages from\nthe send queue.\nAnother alternative is to start an MCA over the network. In particular,\nif one side of a channel is already active, it can send a control message\nrequesting that the other MCA is to be started. Such a control message is sent\nto a daemon listening to a well-known address on the same machine as where\nthe other MCA is to be started.\nChannels are stopped automatically after a speci\ufb01ed time has expired\nduring which no more messages were dropped into the send queue.\nEach MCA has a set of associated attributes that determine the overall\nbehavior of a channel. Some of the attributes are listed in Figure 4.31. Attribute\nvalues of the sending and receiving MCA should be compatible and perhaps\nnegotiated \ufb01rst before a channel can be set up. For example, both MCAs\nshould obviously support the same transport protocol. An example of a\nnonnegotiable attribute is whether or not messages are to be delivered in the\nsame order as they are put into the send queue. If one MCA wants FIFO\ndelivery, the other must comply. An example of a negotiable attribute value is\nthe maximum message length, which will simply be chosen as the minimum\nvalue speci\ufb01ed by either MCA.\nMessage transfer\nTo transfer a message from one queue manager to another (possibly remote)\nqueue manager, it is necessary that each message carries its destination ad-\ndress, for which a transmission header is used. An address in MQ consists of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 215\nAttribute Description\nTransport type Determines the transport protocol to be used\nFIFO delivery Indicates that messages are to be delivered in the order they\nare sent\nMessage length Maximum length of a single message\nSetup retry count Maximum number of retries to start up the remote MCA\nDelivery retries Maximum times MCA will try to put received message into\nqueue\nFigure 4.31: Some attributes associated with message channel agents.\ntwo parts. The \ufb01rst part consists of the name of the queue manager to which\nthe message is to be delivered. The second part is the name of the destination\nqueue resorting under that manager to which the message is to be appended.\nBesides the destination address, it is also necessary to specify the route\nthat a message should follow. Route speci\ufb01cation is done by providing the\nname of the local send queue to which a message is to be appended. Thus\nit is not necessary to provide the full route in a message. Recall that each\nmessage channel has exactly one send queue. By telling to which send queue\na message is to be appended, we effectively specify to which queue manager\na message is to be forwarded.\nIn most cases, routes are explicitly stored inside a queue manager in a\nrouting table. An entry in a routing table is a pair ( destQM ,sendQ ), where\ndestQM is the name of the destination queue manager, and sendQ is the name\nof the local send queue to which a message for that queue manager should be\nappended. (A routing table entry is called an alias in MQ.)\nIt is possible that a message needs to be transferred across multiple queue\nmanagers before reaching its destination. Whenever such an intermediate\nqueue manager receives the message, it simply extracts the name of the\ndestination queue manager from the message header, and does a routing-\ntable look-up to \ufb01nd the local send queue to which the message should be\nappended.\nIt is important to realize that each queue manager has a systemwide\nunique name that is effectively used as an identi\ufb01er for that queue manager.\nThe problem with using these names is that replacing a queue manager,\nor changing its name, will affect all applications that send messages to it.\nProblems can be alleviated by using a local alias for queue manager names.\nAn alias de\ufb01ned within a queue manager QM1is another name for a queue\nmanager QM2, but which is available only to applications interfacing to QM1.\nAn alias allows the use of the same (logical) name for a queue, even if the\nqueue manager of that queue changes. Changing the name of a queue manager\nrequires that we change its alias in all queue managers. However, applications\ncan be left unaffected.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 215\nAttribute Description\nTransport type Determines the transport protocol to be used\nFIFO delivery Indicates that messages are to be delivered in the order they\nare sent\nMessage length Maximum length of a single message\nSetup retry count Maximum number of retries to start up the remote MCA\nDelivery retries Maximum times MCA will try to put received message into\nqueue\nFigure 4.31: Some attributes associated with message channel agents.\ntwo parts. The \ufb01rst part consists of the name of the queue manager to which\nthe message is to be delivered. The second part is the name of the destination\nqueue resorting under that manager to which the message is to be appended.\nBesides the destination address, it is also necessary to specify the route\nthat a message should follow. Route speci\ufb01cation is done by providing the\nname of the local send queue to which a message is to be appended. Thus\nit is not necessary to provide the full route in a message. Recall that each\nmessage channel has exactly one send queue. By telling to which send queue\na message is to be appended, we effectively specify to which queue manager\na message is to be forwarded.\nIn most cases, routes are explicitly stored inside a queue manager in a\nrouting table. An entry in a routing table is a pair ( destQM ,sendQ ), where\ndestQM is the name of the destination queue manager, and sendQ is the name\nof the local send queue to which a message for that queue manager should be\nappended. (A routing table entry is called an alias in MQ.)\nIt is possible that a message needs to be transferred across multiple queue\nmanagers before reaching its destination. Whenever such an intermediate\nqueue manager receives the message, it simply extracts the name of the\ndestination queue manager from the message header, and does a routing-\ntable look-up to \ufb01nd the local send queue to which the message should be\nappended.\nIt is important to realize that each queue manager has a systemwide\nunique name that is effectively used as an identi\ufb01er for that queue manager.\nThe problem with using these names is that replacing a queue manager,\nor changing its name, will affect all applications that send messages to it.\nProblems can be alleviated by using a local alias for queue manager names.\nAn alias de\ufb01ned within a queue manager QM1is another name for a queue\nmanager QM2, but which is available only to applications interfacing to QM1.\nAn alias allows the use of the same (logical) name for a queue, even if the\nqueue manager of that queue changes. Changing the name of a queue manager\nrequires that we change its alias in all queue managers. However, applications\ncan be left unaffected.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "216 CHAPTER 4. COMMUNICATION\nFigure 4.32: The general organization of an MQ queuing network using\nrouting tables and aliases.\nThe principle of using routing tables and aliases is shown in Figure 4.32.\nFor example, an application linked to queue manager QMA can refer to a\nremote queue manager using the local alias LA1. The queue manager will \ufb01rst\nlook up the actual destination in the alias table to \ufb01nd it is queue manager\nQMC . The route to QMC is found in the routing table, which states that\nmessages for QMC should be appended to the outgoing queue SQ1, which\nis used to transfer messages to queue manager QMB . The latter will use its\nrouting table to forward the message to QMC .\nFollowing this approach of routing and aliasing leads to a programming\ninterface that is relatively simple, called the Message Queue Interface (MQI ).\nThe most important operations of MQI are summarized in Figure 4.33.\nOperation Description\nMQOPEN Open a (possibly remote) queue\nMQCLOSE Close a queue\nMQPUT Put a message into an opened queue\nMQGET Get a message from a (local) queue\nFigure 4.33: Operations available in the message-queuing interface.\nTo put messages into a queue, an application calls MQOPEN , specifying a\ndestination queue in a speci\ufb01c queue manager. The queue manager can be\nnamed using the locally-available alias. Whether the destination queue is\nactually remote or not is completely transparent to the application. MQOPEN\nshould also be called if the application wants to get messages from its local\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n216 CHAPTER 4. COMMUNICATION\nFigure 4.32: The general organization of an MQ queuing network using\nrouting tables and aliases.\nThe principle of using routing tables and aliases is shown in Figure 4.32.\nFor example, an application linked to queue manager QMA can refer to a\nremote queue manager using the local alias LA1. The queue manager will \ufb01rst\nlook up the actual destination in the alias table to \ufb01nd it is queue manager\nQMC . The route to QMC is found in the routing table, which states that\nmessages for QMC should be appended to the outgoing queue SQ1, which\nis used to transfer messages to queue manager QMB . The latter will use its\nrouting table to forward the message to QMC .\nFollowing this approach of routing and aliasing leads to a programming\ninterface that is relatively simple, called the Message Queue Interface (MQI ).\nThe most important operations of MQI are summarized in Figure 4.33.\nOperation Description\nMQOPEN Open a (possibly remote) queue\nMQCLOSE Close a queue\nMQPUT Put a message into an opened queue\nMQGET Get a message from a (local) queue\nFigure 4.33: Operations available in the message-queuing interface.\nTo put messages into a queue, an application calls MQOPEN , specifying a\ndestination queue in a speci\ufb01c queue manager. The queue manager can be\nnamed using the locally-available alias. Whether the destination queue is\nactually remote or not is completely transparent to the application. MQOPEN\nshould also be called if the application wants to get messages from its local\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 217\nqueue. Only local queues can be opened for reading incoming messages.\nWhen an application is \ufb01nished with accessing a queue, it should close it by\ncalling MQCLOSE .\nMessages can be written to, or read from, a queue using MQPUT and MQGET ,\nrespectively. In principle, messages are removed from a queue on a priority\nbasis. Messages with the same priority are removed on a \ufb01rst-in, \ufb01rst-out\nbasis, that is, the longest pending message is removed \ufb01rst. It is also possible\nto request for speci\ufb01c messages. Furthermore, MQ provides facilities to signal\napplications when messages have arrived, thus avoiding that an application\nwill continuously have to poll a message queue for incoming messages.\nAn interesting observation, and one that is common to modern message-\nqueuing systems, is that persistent messaging is not obtained by simply making\nqueues persistent. Instead, a message is marked as being persistent , and it is a\nqueue manager\u2019s job to see to it that the message can survive a crash. As a\nconsequence, a queue can simultaneously store persistent and nonpersistent\nmessages.\nManaging overlay networks\nFrom the description so far, it should be clear that an important part of\nmanaging MQ systems is connecting the various queue managers into a\nconsistent overlay network. Moreover, this network needs to be maintained\nover time. For small networks, this maintenance will not require much more\nthan average administrative work, but matters become complicated when\nmessage queuing is used to integrate and disintegrate large existing systems.\nA major issue with MQ is that overlay networks need to be manually ad-\nministrated. This administration not only involves creating channels between\nqueue managers, but also \ufb01lling in the routing tables. Obviously, this can\ngrow into a nightmare. Unfortunately, management support for MQ systems\nis advanced only in the sense that an administrator can set virtually every pos-\nsible attribute, and tweak any thinkable con\ufb01guration. However, the bottom\nline is that channels and routing tables need to be manually maintained.\nAt the heart of overlay management is the channel control function com-\nponent, which logically sits between message channel agents. This component\nallows an operator to monitor exactly what is going on at two end points of a\nchannel. In addition, it is used to create channels and routing tables, but also\nto manage the queue managers that host the message channel agents. In a way,\nthis approach to overlay management strongly resembles the management\nof cluster servers where a single administration server is used. In the latter\ncase, the server essentially offers only a remote shell to each machine in the\ncluster, along with a few collective operations to handle groups of machines.\nThe good news about distributed-systems management is that it offers lots\nof opportunities if you are looking for an area to explore new solutions to\nserious problems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 217\nqueue. Only local queues can be opened for reading incoming messages.\nWhen an application is \ufb01nished with accessing a queue, it should close it by\ncalling MQCLOSE .\nMessages can be written to, or read from, a queue using MQPUT and MQGET ,\nrespectively. In principle, messages are removed from a queue on a priority\nbasis. Messages with the same priority are removed on a \ufb01rst-in, \ufb01rst-out\nbasis, that is, the longest pending message is removed \ufb01rst. It is also possible\nto request for speci\ufb01c messages. Furthermore, MQ provides facilities to signal\napplications when messages have arrived, thus avoiding that an application\nwill continuously have to poll a message queue for incoming messages.\nAn interesting observation, and one that is common to modern message-\nqueuing systems, is that persistent messaging is not obtained by simply making\nqueues persistent. Instead, a message is marked as being persistent , and it is a\nqueue manager\u2019s job to see to it that the message can survive a crash. As a\nconsequence, a queue can simultaneously store persistent and nonpersistent\nmessages.\nManaging overlay networks\nFrom the description so far, it should be clear that an important part of\nmanaging MQ systems is connecting the various queue managers into a\nconsistent overlay network. Moreover, this network needs to be maintained\nover time. For small networks, this maintenance will not require much more\nthan average administrative work, but matters become complicated when\nmessage queuing is used to integrate and disintegrate large existing systems.\nA major issue with MQ is that overlay networks need to be manually ad-\nministrated. This administration not only involves creating channels between\nqueue managers, but also \ufb01lling in the routing tables. Obviously, this can\ngrow into a nightmare. Unfortunately, management support for MQ systems\nis advanced only in the sense that an administrator can set virtually every pos-\nsible attribute, and tweak any thinkable con\ufb01guration. However, the bottom\nline is that channels and routing tables need to be manually maintained.\nAt the heart of overlay management is the channel control function com-\nponent, which logically sits between message channel agents. This component\nallows an operator to monitor exactly what is going on at two end points of a\nchannel. In addition, it is used to create channels and routing tables, but also\nto manage the queue managers that host the message channel agents. In a way,\nthis approach to overlay management strongly resembles the management\nof cluster servers where a single administration server is used. In the latter\ncase, the server essentially offers only a remote shell to each machine in the\ncluster, along with a few collective operations to handle groups of machines.\nThe good news about distributed-systems management is that it offers lots\nof opportunities if you are looking for an area to explore new solutions to\nserious problems.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "218 CHAPTER 4. COMMUNICATION\nExample: Advanced Message Queuing Protocol (AMQP)\nAn interesting observation about message-queuing systems is that they have\nbeen developed in part to allow legacy applications to interoperate, yet at\nthe same time we see that when it comes to operations between different\nmessage-queuing systems, we often hit a wall. As a consequence, once an\norganization chooses to use a message-queuing system from manufacturer X,\nthey may have to settle for solutions that only X provides. Message-queuing\nsolutions are thus in large part proprietary solutions. So much for openness.\nIn 2006, a working group was formed to change this situation, which\nresulted in the speci\ufb01cation of the Advanced Message-Queuing Protocol , or\nsimply AMQP . There are different versions of AMQP , with version 1.0 being\nthe most recent one. There are also various implementations of AMQP , notably\nof versions prior to 1.0, which by the time version 1.0 was established, had\ngained considerable popularity. Because a pre-1.0 version is so different from\nthe 1.0 version, yet has also a steady user base, we may see various pre-1.0\nAMQP servers exist next to (their undeniably incompatible) 1.0 servers. So\nmuch for openness.\nIn this section, we will describe AMQP , but will more or less deliberately\nmix the pre-1.0 and 1.0 versions, sticking to the essentials and spirit of AMQP .\nDetails can be found in the speci\ufb01cations [Group, 2008; OASIS, 2011]. Im-\nplementations of AMQP include RabbitMQ [Videla and Williams, 2012] and\nApache\u2019s Qpid.\nBasics\nAMQP evolves around applications, queue managers, and queues. Taking\nan approach that is common for many networking situations, we make a\ndistinction between AMQP as a messaging service , the actual messaging protocol ,\nand, \ufb01nally, the messaging interface as offered to applications. To this end, it\nis easiest to consider the situation of having only a single queue manager,\nrunning as a single, separate server forming the implementation of AMQP as\na service. An application communicates with this queue manager through a\nlocal interface. Between an application and the queue manager communication\nproceeds according to the AMQP protocol.\nThis situation is shown in Figure 4.34 and should look familiar. The AMQP\nstub shields the application (as well as the queue manager) from the details\nconcerning message transfer and communication in general. At the same\ntime, it implements a message-queuing interface, allowing the application to\nmake use of AMQP as a message-queuing service. Although the distinction\nbetween AMQP stub and queue manager is made explicit for queue managers,\nthe strictness of the separation is left to an implementation. Nevertheless, if\nnot strict, conceptually there is a distinction between handling queues and\nhandling related communication as we shall make clear shortly.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n218 CHAPTER 4. COMMUNICATION\nExample: Advanced Message Queuing Protocol (AMQP)\nAn interesting observation about message-queuing systems is that they have\nbeen developed in part to allow legacy applications to interoperate, yet at\nthe same time we see that when it comes to operations between different\nmessage-queuing systems, we often hit a wall. As a consequence, once an\norganization chooses to use a message-queuing system from manufacturer X,\nthey may have to settle for solutions that only X provides. Message-queuing\nsolutions are thus in large part proprietary solutions. So much for openness.\nIn 2006, a working group was formed to change this situation, which\nresulted in the speci\ufb01cation of the Advanced Message-Queuing Protocol , or\nsimply AMQP . There are different versions of AMQP , with version 1.0 being\nthe most recent one. There are also various implementations of AMQP , notably\nof versions prior to 1.0, which by the time version 1.0 was established, had\ngained considerable popularity. Because a pre-1.0 version is so different from\nthe 1.0 version, yet has also a steady user base, we may see various pre-1.0\nAMQP servers exist next to (their undeniably incompatible) 1.0 servers. So\nmuch for openness.\nIn this section, we will describe AMQP , but will more or less deliberately\nmix the pre-1.0 and 1.0 versions, sticking to the essentials and spirit of AMQP .\nDetails can be found in the speci\ufb01cations [Group, 2008; OASIS, 2011]. Im-\nplementations of AMQP include RabbitMQ [Videla and Williams, 2012] and\nApache\u2019s Qpid.\nBasics\nAMQP evolves around applications, queue managers, and queues. Taking\nan approach that is common for many networking situations, we make a\ndistinction between AMQP as a messaging service , the actual messaging protocol ,\nand, \ufb01nally, the messaging interface as offered to applications. To this end, it\nis easiest to consider the situation of having only a single queue manager,\nrunning as a single, separate server forming the implementation of AMQP as\na service. An application communicates with this queue manager through a\nlocal interface. Between an application and the queue manager communication\nproceeds according to the AMQP protocol.\nThis situation is shown in Figure 4.34 and should look familiar. The AMQP\nstub shields the application (as well as the queue manager) from the details\nconcerning message transfer and communication in general. At the same\ntime, it implements a message-queuing interface, allowing the application to\nmake use of AMQP as a message-queuing service. Although the distinction\nbetween AMQP stub and queue manager is made explicit for queue managers,\nthe strictness of the separation is left to an implementation. Nevertheless, if\nnot strict, conceptually there is a distinction between handling queues and\nhandling related communication as we shall make clear shortly.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.3. MESSAGE-ORIENTED COMMUNICATION 219\nFigure 4.34: An overview of a single-server AMQP instance.\nAMQP communication\nAMQP allows an application to set up a connection to a queue manager; a\nconnection is a container for a number of one-way channels . Whereas the\nlifetime of a channel can be highly dynamic, connections are assumed to\nbe relatively stable. This difference between connection and channel allows\nfor ef\ufb01cient implementations, notably by using a single transport-layer TCP\nconnection to multiplex lots of different channels between an application and\na queue manager. In practice, AMQP assumes TCP is used for establishing\nAMQP connections.\nBidirectional communication is established through sessions : a logical\ngrouping of two channels. A connection may have multiple sessions, but note\nthat a channel need not necessarily be part of a session.\nFinally, to actually transfer messages, a link is needed. Conceptually, a\nlink, or rather its end points, keep track of the status of messages that are\nbeing transferred. It thus provides \ufb01ne-grained \ufb02ow control between an\napplication and a queue manager, and, indeed, different control policies can\nbe put simultaneously in place for different messages that are transferred\nthrough the same session of connection. Flow control is established through\ncredits: a receiver can tell the sender how many messages it is allowed to send\nover a speci\ufb01c link.\nWhen a message is to be transferred, the application passes it to its local\nAMQP stub. As mentioned, each message transfer is associated with one\nspeci\ufb01c link. Message transfer normally proceeds in three steps.\n1.At the sender\u2019s side, the message is assigned a unique identi\ufb01er and\nis recorded locally to be in an unsettled state . The stub subsequently\ntransfers the message to the server, where the AMQP stub also records it\nas being in an unsettled state. At that point, the server-side stub passes\nit to the queue manager.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.3. MESSAGE-ORIENTED COMMUNICATION 219\nFigure 4.34: An overview of a single-server AMQP instance.\nAMQP communication\nAMQP allows an application to set up a connection to a queue manager; a\nconnection is a container for a number of one-way channels . Whereas the\nlifetime of a channel can be highly dynamic, connections are assumed to\nbe relatively stable. This difference between connection and channel allows\nfor ef\ufb01cient implementations, notably by using a single transport-layer TCP\nconnection to multiplex lots of different channels between an application and\na queue manager. In practice, AMQP assumes TCP is used for establishing\nAMQP connections.\nBidirectional communication is established through sessions : a logical\ngrouping of two channels. A connection may have multiple sessions, but note\nthat a channel need not necessarily be part of a session.\nFinally, to actually transfer messages, a link is needed. Conceptually, a\nlink, or rather its end points, keep track of the status of messages that are\nbeing transferred. It thus provides \ufb01ne-grained \ufb02ow control between an\napplication and a queue manager, and, indeed, different control policies can\nbe put simultaneously in place for different messages that are transferred\nthrough the same session of connection. Flow control is established through\ncredits: a receiver can tell the sender how many messages it is allowed to send\nover a speci\ufb01c link.\nWhen a message is to be transferred, the application passes it to its local\nAMQP stub. As mentioned, each message transfer is associated with one\nspeci\ufb01c link. Message transfer normally proceeds in three steps.\n1.At the sender\u2019s side, the message is assigned a unique identi\ufb01er and\nis recorded locally to be in an unsettled state . The stub subsequently\ntransfers the message to the server, where the AMQP stub also records it\nas being in an unsettled state. At that point, the server-side stub passes\nit to the queue manager.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "220 CHAPTER 4. COMMUNICATION\n2.The receiving application (in this case the queue manager), is assumed\nto handle the message and normally reports back to its stub that it is\n\ufb01nished. The stub passes this information to the original sender, at\nwhich point the message at the original sender\u2019s AMQP stub enters a\nsettled state .\n3.The AMQP stub of the original sender now tells the stub of the original\nreceiver that message transfer has been settled (meaning that the original\nsender will forget about the message from now on). The receiver\u2019s stub\ncan now also discard anything about the message, formally recording it\nas being settled as well.\nNote that because the receiving application can indicate to the underlying\nAMQP communication layer that it is done with a message, AMQP enables\ntrue end-to-end communication reliability. In particular, the application, be\nit a client application or an actual queue manager, can instruct the AMQP\ncommunication layer to keep hold of a message (i.e., a message stays in the\nunsettled state).\nAMQP messaging\nMessaging in AMQP logically takes place at the layer above the one handling\ncommunication. It is here that an application can indicate what needs to be\ndone with a message, but can also see what has happened so far. Messaging\nformally takes place between two nodes , of which there are three types: a\nproducer, a consumer, or a queue. Typically, producer and consumer nodes\nrepresent regular applications, whereas queues are used to store and forward\nmessages. Indeed, a queue manager will typically consist of multiple queue\nnodes. In order for message transfer to take place, two nodes will have to\nestablish a link between them.\nThe receiver can indicate to the sender whether its message was accepted\n(meaning that it was successfully processed), or rejected. Note that this\nmeans that a noti\ufb01cation is returned to the original sender. AMQP also\nsupports fragmentation and assembly of large messages for which additional\nnoti\ufb01cations are sent.\nOf course, an important aspect of AMQP is its support for persistent\nmessaging. Achieving persistence is handled through several mechanisms.\nFirst and foremost important, a message can be marked as durable , indicating\nthat the source expects any intermediate node, such as a queue, to be able to\nrecover in the case of a failure. An intermediate node that cannot guarantee\nsuch durability will have to reject a message. Second, a source or target node\ncan also indicate its durability: if durable, will it maintain its state, or will it\nalso maintain the (unsettled) state of durable messages? Combining the latter\nwith durable messages effectively establishes reliable message transfer and\npersistent messaging.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n220 CHAPTER 4. COMMUNICATION\n2.The receiving application (in this case the queue manager), is assumed\nto handle the message and normally reports back to its stub that it is\n\ufb01nished. The stub passes this information to the original sender, at\nwhich point the message at the original sender\u2019s AMQP stub enters a\nsettled state .\n3.The AMQP stub of the original sender now tells the stub of the original\nreceiver that message transfer has been settled (meaning that the original\nsender will forget about the message from now on). The receiver\u2019s stub\ncan now also discard anything about the message, formally recording it\nas being settled as well.\nNote that because the receiving application can indicate to the underlying\nAMQP communication layer that it is done with a message, AMQP enables\ntrue end-to-end communication reliability. In particular, the application, be\nit a client application or an actual queue manager, can instruct the AMQP\ncommunication layer to keep hold of a message (i.e., a message stays in the\nunsettled state).\nAMQP messaging\nMessaging in AMQP logically takes place at the layer above the one handling\ncommunication. It is here that an application can indicate what needs to be\ndone with a message, but can also see what has happened so far. Messaging\nformally takes place between two nodes , of which there are three types: a\nproducer, a consumer, or a queue. Typically, producer and consumer nodes\nrepresent regular applications, whereas queues are used to store and forward\nmessages. Indeed, a queue manager will typically consist of multiple queue\nnodes. In order for message transfer to take place, two nodes will have to\nestablish a link between them.\nThe receiver can indicate to the sender whether its message was accepted\n(meaning that it was successfully processed), or rejected. Note that this\nmeans that a noti\ufb01cation is returned to the original sender. AMQP also\nsupports fragmentation and assembly of large messages for which additional\nnoti\ufb01cations are sent.\nOf course, an important aspect of AMQP is its support for persistent\nmessaging. Achieving persistence is handled through several mechanisms.\nFirst and foremost important, a message can be marked as durable , indicating\nthat the source expects any intermediate node, such as a queue, to be able to\nrecover in the case of a failure. An intermediate node that cannot guarantee\nsuch durability will have to reject a message. Second, a source or target node\ncan also indicate its durability: if durable, will it maintain its state, or will it\nalso maintain the (unsettled) state of durable messages? Combining the latter\nwith durable messages effectively establishes reliable message transfer and\npersistent messaging.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 221\nAMQP is truly a messaging protocol in the sense that it does not by itself\nsupport, for example, publish-subscribe primitives. It expects that such issues\nare handled by more advanced, proprietary queue managers, akin to the\nmessage brokers discussed in Section 4.3.\nFinally, there is no reason why a queue manager cannot be connected to\nanother queue manager. In fact, it is quite common to organize queue man-\nagers into an overlay network in which messages are routed from producers to\ntheir consumers. AMQP does not specify how the overlay network should be\nconstructed and managed, and, indeed, different providers of AMQP-based\nsystems offer different solutions. Of particular importance is specifying how\nmessages should be routed through the network. The bottom line is that ad-\nministrators will need to do a lot of this speci\ufb01cation manually. Only in cases\nwhere overlays have regular structures, such as cycles or trees, it becomes\neasier to provide the necessary routing details.\n4.4 Multicast communication\nAn important topic in communication in distributed systems is the support for\nsending data to multiple receivers, also known as multicast communication.\nFor many years, this topic has belonged to the domain of network protocols,\nwhere numerous proposals for network-level and transport-level solutions\nhave been implemented and evaluated [Janic, 2005; Obraczka, 1998]. A major\nissue in all solutions was setting up the communication paths for informa-\ntion dissemination. In practice, this involved a huge management effort, in\nmany cases requiring human intervention. In addition, as long as there is\nno convergence of proposals, ISPs have shown to be reluctant to support\nmulticasting [Diot et al., 2000].\nWith the advent of peer-to-peer technology, and notably structured overlay\nmanagement, it became easier to set up communication paths. As peer-to-peer\nsolutions are typically deployed at the application layer, various application-\nlevel multicasting techniques have been introduced. In this section, we will\ntake a brief look at these techniques.\nMulticast communication can also be accomplished in other ways than\nsetting up explicit communication paths. As we also explore in this sec-\ntion, gossip-based information dissemination provides simple (yet often less\nef\ufb01cient) ways for multicasting.\nApplication-level tree-based multicasting\nThe basic idea in application-level multicasting is that nodes organize into\nan overlay network, which is then used to disseminate information to its\nmembers. An important observation is that network routers are not involved\nin group membership. As a consequence, the connections between nodes in\nthe overlay network may cross several physical links, and as such, routing\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 221\nAMQP is truly a messaging protocol in the sense that it does not by itself\nsupport, for example, publish-subscribe primitives. It expects that such issues\nare handled by more advanced, proprietary queue managers, akin to the\nmessage brokers discussed in Section 4.3.\nFinally, there is no reason why a queue manager cannot be connected to\nanother queue manager. In fact, it is quite common to organize queue man-\nagers into an overlay network in which messages are routed from producers to\ntheir consumers. AMQP does not specify how the overlay network should be\nconstructed and managed, and, indeed, different providers of AMQP-based\nsystems offer different solutions. Of particular importance is specifying how\nmessages should be routed through the network. The bottom line is that ad-\nministrators will need to do a lot of this speci\ufb01cation manually. Only in cases\nwhere overlays have regular structures, such as cycles or trees, it becomes\neasier to provide the necessary routing details.\n4.4 Multicast communication\nAn important topic in communication in distributed systems is the support for\nsending data to multiple receivers, also known as multicast communication.\nFor many years, this topic has belonged to the domain of network protocols,\nwhere numerous proposals for network-level and transport-level solutions\nhave been implemented and evaluated [Janic, 2005; Obraczka, 1998]. A major\nissue in all solutions was setting up the communication paths for informa-\ntion dissemination. In practice, this involved a huge management effort, in\nmany cases requiring human intervention. In addition, as long as there is\nno convergence of proposals, ISPs have shown to be reluctant to support\nmulticasting [Diot et al., 2000].\nWith the advent of peer-to-peer technology, and notably structured overlay\nmanagement, it became easier to set up communication paths. As peer-to-peer\nsolutions are typically deployed at the application layer, various application-\nlevel multicasting techniques have been introduced. In this section, we will\ntake a brief look at these techniques.\nMulticast communication can also be accomplished in other ways than\nsetting up explicit communication paths. As we also explore in this sec-\ntion, gossip-based information dissemination provides simple (yet often less\nef\ufb01cient) ways for multicasting.\nApplication-level tree-based multicasting\nThe basic idea in application-level multicasting is that nodes organize into\nan overlay network, which is then used to disseminate information to its\nmembers. An important observation is that network routers are not involved\nin group membership. As a consequence, the connections between nodes in\nthe overlay network may cross several physical links, and as such, routing\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "222 CHAPTER 4. COMMUNICATION\nmessages within the overlay may not be optimal in comparison to what could\nhave been achieved by network-level routing.\nA crucial design issue is the construction of the overlay network. In\nessence, there are two approaches [El-Sayed et al., 2003; Hosseini et al., 2007;\nAllani et al., 2009]. First, nodes may organize themselves directly into a tree,\nmeaning that there is a unique (overlay) path between every pair of nodes.\nAn alternative approach is that nodes organize into a mesh network in which\nevery node will have multiple neighbors and, in general, there exist multiple\npaths between every pair of nodes. The main difference between the two is\nthat the latter generally provides higher robustness: if a connection breaks\n(e.g., because a node fails), there will still be an opportunity to disseminate\ninformation without having to immediately reorganize the entire overlay\nnetwork.\nNote 4.13 (Advanced: Constructing a multicast tree in Chord)\nTo make matters concrete, let us consider a relatively simple scheme for construct-\ning a multicast tree in Chord, which we described in Note 2.5. This scheme was\noriginally proposed for Scribe [Castro et al., 2002b] which is an application-level\nmulticasting scheme built on top of Pastry [Rowstron and Druschel, 2001]. The\nlatter is also a DHT-based peer-to-peer system.\nAssume a node wants to start a multicast session. To this end, it simply\ngenerates a multicast identi\ufb01er, say mid which is just a randomly-chosen 160-bit\nkey. It then looks up succ(mid), which is the node responsible for that key, and\npromotes it to become the root of the multicast tree that will be used to sending\ndata to interested nodes. In order to join the tree, a node Psimply executes the\noperation lookup (mid)having the effect that a lookup message with the request\nto join the multicast group midwill be routed from Ptosucc(mid). The routing\nalgorithm itself will be explained in detail in Chapter 5.\nOn its way toward the root, the join request will pass several nodes. Assume\nit \ufb01rst reaches node Q. IfQhad never seen a join request for midbefore, it will\nbecome a forwarder for that group. At that point, Pwill become a child of Q\nwhereas the latter will continue to forward the join request to the root. If the next\nnode on the root, say Ris also not yet a forwarder, it will become one and record\nQas its child and continue to send the join request.\nOn the other hand, if Q(orR) is already a forwarder for mid, it will also record\nthe previous sender as its child (i.e., PorQ, respectively), but there will not be a\nneed to send the join request to the root anymore, as Q(orR) will already be a\nmember of the multicast tree.\nNodes such as Pthat have explicitly requested to join the multicast tree are, by\nde\ufb01nition, also forwarders. The result of this scheme is that we construct a multi-\ncast tree across the overlay network with two types of nodes: pure forwarders that\nact as helpers, and nodes that are also forwarders, but have explicitly requested\nto join the tree. Multicasting is now simple: a node merely sends a multicast\nmessage toward the root of the tree by again executing the lookup (mid)operation,\nafter which that message can be sent along the tree.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n222 CHAPTER 4. COMMUNICATION\nmessages within the overlay may not be optimal in comparison to what could\nhave been achieved by network-level routing.\nA crucial design issue is the construction of the overlay network. In\nessence, there are two approaches [El-Sayed et al., 2003; Hosseini et al., 2007;\nAllani et al., 2009]. First, nodes may organize themselves directly into a tree,\nmeaning that there is a unique (overlay) path between every pair of nodes.\nAn alternative approach is that nodes organize into a mesh network in which\nevery node will have multiple neighbors and, in general, there exist multiple\npaths between every pair of nodes. The main difference between the two is\nthat the latter generally provides higher robustness: if a connection breaks\n(e.g., because a node fails), there will still be an opportunity to disseminate\ninformation without having to immediately reorganize the entire overlay\nnetwork.\nNote 4.13 (Advanced: Constructing a multicast tree in Chord)\nTo make matters concrete, let us consider a relatively simple scheme for construct-\ning a multicast tree in Chord, which we described in Note 2.5. This scheme was\noriginally proposed for Scribe [Castro et al., 2002b] which is an application-level\nmulticasting scheme built on top of Pastry [Rowstron and Druschel, 2001]. The\nlatter is also a DHT-based peer-to-peer system.\nAssume a node wants to start a multicast session. To this end, it simply\ngenerates a multicast identi\ufb01er, say mid which is just a randomly-chosen 160-bit\nkey. It then looks up succ(mid), which is the node responsible for that key, and\npromotes it to become the root of the multicast tree that will be used to sending\ndata to interested nodes. In order to join the tree, a node Psimply executes the\noperation lookup (mid)having the effect that a lookup message with the request\nto join the multicast group midwill be routed from Ptosucc(mid). The routing\nalgorithm itself will be explained in detail in Chapter 5.\nOn its way toward the root, the join request will pass several nodes. Assume\nit \ufb01rst reaches node Q. IfQhad never seen a join request for midbefore, it will\nbecome a forwarder for that group. At that point, Pwill become a child of Q\nwhereas the latter will continue to forward the join request to the root. If the next\nnode on the root, say Ris also not yet a forwarder, it will become one and record\nQas its child and continue to send the join request.\nOn the other hand, if Q(orR) is already a forwarder for mid, it will also record\nthe previous sender as its child (i.e., PorQ, respectively), but there will not be a\nneed to send the join request to the root anymore, as Q(orR) will already be a\nmember of the multicast tree.\nNodes such as Pthat have explicitly requested to join the multicast tree are, by\nde\ufb01nition, also forwarders. The result of this scheme is that we construct a multi-\ncast tree across the overlay network with two types of nodes: pure forwarders that\nact as helpers, and nodes that are also forwarders, but have explicitly requested\nto join the tree. Multicasting is now simple: a node merely sends a multicast\nmessage toward the root of the tree by again executing the lookup (mid)operation,\nafter which that message can be sent along the tree.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 223\nWe note that this high-level description of multicasting in Scribe does not do\ncomplete justice to its original design. The interested reader is encouraged to take\na look at the details, which can be found in [Castro et al., 2002b].\nPerformance issues in overlays\nFrom the high-level description given above, it should be clear that although\nbuilding a tree by itself is not that dif\ufb01cult once we have organized the nodes\ninto an overlay, building an ef\ufb01cient tree may be a different story. Note that\nin our description so far, the selection of nodes that participate in the tree\ndoes not take into account any performance metrics: it is purely based on the\n(logical) routing of messages through the overlay.\nFigure 4.35: The relation between links in an overlay and actual network-level\nroutes.\nTo understand the problem at hand, take a look at Figure 4.35 which\nshows a small set of \ufb01ve nodes that are organized in a simple overlay network,\nwith node Aforming the root of a multicast tree. The costs for traversing\na physical link are also shown. Now, whenever Amulticasts a message to\nthe other nodes, it is seen that this message will traverse each of the links\nhB,Rbi,hRa,Rbi,hE,Rei,hRc,Rdi, andhD,Rditwice. The overlay network\nwould have been more ef\ufb01cient if we had not constructed overlay linkshB,Ei,\nandhD,Ei, but insteadhA,EiandhC,Ei. Such a con\ufb01guration would have\nsaved the double traversal across physical links hRa,RbiandhRc,Rdi.\nThe quality of an application-level multicast tree is generally measured\nby three different metrics: link stress, stretch, and tree cost. Link stress is\nde\ufb01ned per link and counts how often a packet crosses the same link [Chu\net al., 2002]. A link stress greater than 1 comes from the fact that although at a\nlogical level a packet may be forwarded along two different connections, part\nof those connections may actually correspond to the same physical link, as we\nshowed in Figure 4.35.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 223\nWe note that this high-level description of multicasting in Scribe does not do\ncomplete justice to its original design. The interested reader is encouraged to take\na look at the details, which can be found in [Castro et al., 2002b].\nPerformance issues in overlays\nFrom the high-level description given above, it should be clear that although\nbuilding a tree by itself is not that dif\ufb01cult once we have organized the nodes\ninto an overlay, building an ef\ufb01cient tree may be a different story. Note that\nin our description so far, the selection of nodes that participate in the tree\ndoes not take into account any performance metrics: it is purely based on the\n(logical) routing of messages through the overlay.\nFigure 4.35: The relation between links in an overlay and actual network-level\nroutes.\nTo understand the problem at hand, take a look at Figure 4.35 which\nshows a small set of \ufb01ve nodes that are organized in a simple overlay network,\nwith node Aforming the root of a multicast tree. The costs for traversing\na physical link are also shown. Now, whenever Amulticasts a message to\nthe other nodes, it is seen that this message will traverse each of the links\nhB,Rbi,hRa,Rbi,hE,Rei,hRc,Rdi, andhD,Rditwice. The overlay network\nwould have been more ef\ufb01cient if we had not constructed overlay linkshB,Ei,\nandhD,Ei, but insteadhA,EiandhC,Ei. Such a con\ufb01guration would have\nsaved the double traversal across physical links hRa,RbiandhRc,Rdi.\nThe quality of an application-level multicast tree is generally measured\nby three different metrics: link stress, stretch, and tree cost. Link stress is\nde\ufb01ned per link and counts how often a packet crosses the same link [Chu\net al., 2002]. A link stress greater than 1 comes from the fact that although at a\nlogical level a packet may be forwarded along two different connections, part\nof those connections may actually correspond to the same physical link, as we\nshowed in Figure 4.35.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "224 CHAPTER 4. COMMUNICATION\nThe stretch orRelative Delay Penalty (RDP ) measures the ratio in the\ndelay between two nodes in the overlay, and the delay that those two nodes\nwould experience in the underlying network. For example, messages from\nBtoCfollow the route B!Rb!Ra!Re!E!Re!Rc!Rd!\nD!Rd!Rc!Cin the overlay network, having a total cost of 73 units.\nHowever, messages would have been routed in the underlying network along\nthe path B!Rb!Rd!Rc!C, with a total cost of 47 units, leading to a\nstretch of 1.55. Obviously, when constructing an overlay network, the goal is\nto minimize the aggregated stretch, or similarly, the average RDP measured\nover all node pairs.\nFinally, the tree cost is a global metric, generally related to minimizing\nthe aggregated link costs. For example, if the cost of a link is taken to be the\ndelay between its two end nodes, then optimizing the tree cost boils down\nto \ufb01nding a minimal spanning tree in which the total time for disseminating\ninformation to all nodes is minimal.\nTo simplify matters somewhat, assume that a multicast group has an\nassociated and well-known node that keeps track of the nodes that have joined\nthe tree. When a new node issues a join request, it contacts this rendezvous\nnode to obtain a (potentially partial) list of members. The goal is to select\nthe best member that can operate as the new node\u2019s parent in the tree. Who\nshould it select? There are many alternatives and different proposals often\nfollow very different solutions.\nConsider, for example, a multicast group with only a single source. In\nthis case, the selection of the best node is obvious: it should be the source\n(because in that case we can be assured that the stretch will be equal to 1).\nHowever, in doing so, we would introduce a star topology with the source\nin the middle. Although simple, it is not dif\ufb01cult to imagine the source may\neasily become overloaded. In other words, selection of a node will generally\nbe constrained in such a way that only those nodes may be chosen who have\nkor less neighbors, with kbeing a design parameter. This constraint severely\ncomplicates the tree-establishment algorithm, as a good solution may require\nthat part of the existing tree is recon\ufb01gured. Tan et al. [2003] provide an\nextensive overview and evaluation of various solutions to this problem.\nNote 4.14 (Advanced: Switch-trees)\nAs an illustration, let us take a closer look at one speci\ufb01c family, known as switch-\ntrees [Helder and Jamin, 2002]. The basic idea is simple. Assume we already have\na multicast tree with a single source as root. In this tree, a node Pcan switch\nparents by dropping the link to its current parent in favor of a link to another\nnode. The only constraints imposed on switching links is that the new parent can\nnever be a member of the subtree rooted at P(as this would partition the tree\nand create a loop), and that the new parent will not have too many immediate\nchildren. This last requirement is needed to limit the load of forwarding messages\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n224 CHAPTER 4. COMMUNICATION\nThe stretch orRelative Delay Penalty (RDP ) measures the ratio in the\ndelay between two nodes in the overlay, and the delay that those two nodes\nwould experience in the underlying network. For example, messages from\nBtoCfollow the route B!Rb!Ra!Re!E!Re!Rc!Rd!\nD!Rd!Rc!Cin the overlay network, having a total cost of 73 units.\nHowever, messages would have been routed in the underlying network along\nthe path B!Rb!Rd!Rc!C, with a total cost of 47 units, leading to a\nstretch of 1.55. Obviously, when constructing an overlay network, the goal is\nto minimize the aggregated stretch, or similarly, the average RDP measured\nover all node pairs.\nFinally, the tree cost is a global metric, generally related to minimizing\nthe aggregated link costs. For example, if the cost of a link is taken to be the\ndelay between its two end nodes, then optimizing the tree cost boils down\nto \ufb01nding a minimal spanning tree in which the total time for disseminating\ninformation to all nodes is minimal.\nTo simplify matters somewhat, assume that a multicast group has an\nassociated and well-known node that keeps track of the nodes that have joined\nthe tree. When a new node issues a join request, it contacts this rendezvous\nnode to obtain a (potentially partial) list of members. The goal is to select\nthe best member that can operate as the new node\u2019s parent in the tree. Who\nshould it select? There are many alternatives and different proposals often\nfollow very different solutions.\nConsider, for example, a multicast group with only a single source. In\nthis case, the selection of the best node is obvious: it should be the source\n(because in that case we can be assured that the stretch will be equal to 1).\nHowever, in doing so, we would introduce a star topology with the source\nin the middle. Although simple, it is not dif\ufb01cult to imagine the source may\neasily become overloaded. In other words, selection of a node will generally\nbe constrained in such a way that only those nodes may be chosen who have\nkor less neighbors, with kbeing a design parameter. This constraint severely\ncomplicates the tree-establishment algorithm, as a good solution may require\nthat part of the existing tree is recon\ufb01gured. Tan et al. [2003] provide an\nextensive overview and evaluation of various solutions to this problem.\nNote 4.14 (Advanced: Switch-trees)\nAs an illustration, let us take a closer look at one speci\ufb01c family, known as switch-\ntrees [Helder and Jamin, 2002]. The basic idea is simple. Assume we already have\na multicast tree with a single source as root. In this tree, a node Pcan switch\nparents by dropping the link to its current parent in favor of a link to another\nnode. The only constraints imposed on switching links is that the new parent can\nnever be a member of the subtree rooted at P(as this would partition the tree\nand create a loop), and that the new parent will not have too many immediate\nchildren. This last requirement is needed to limit the load of forwarding messages\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 225\nby any single node.\nThere are different criteria for deciding to switch parents. A simple one is to\noptimize the route to the source, effectively minimizing the delay when a message\nis to be multicast. To this end, each node regularly receives information on other\nnodes (we will explain one speci\ufb01c way of doing this below). At that point, the\nnode can evaluate whether another node would be a better parent in terms of\ndelay along the route to the source, and if so, initiates a switch.\nAnother criteria could be whether the delay to the potential other parent is\nlower than to the current parent. If every node takes this as a criterion, then\nthe aggregated delays of the resulting tree should ideally be minimal. In other\nwords, this is an example of optimizing the cost of the tree as we explained\nabove. However, more information would be needed to construct such a tree, but\nas it turns out, this simple scheme is a reasonable heuristic leading to a good\napproximation of a minimal spanning tree.\nAs an example, consider the case where a node Preceives information on\nthe neighbors of its parent. Note that the neighbors consist of P\u2019s grandparent,\nalong with the other siblings of P\u2019s parent. Node Pcan then evaluate the delays\nto each of these nodes and subsequently choose the one with the lowest delay,\nsayQ, as its new parent. To that end, it sends a switch request to Q. To prevent\nloops from being formed due to concurrent switching requests, a node that has an\noutstanding switch request will simply refuse to process any incoming requests.\nIn effect, this leads to a situation where only completely independent switches\ncan be carried out simultaneously. Furthermore, Pwill provide Qwith enough\ninformation to allow the latter to conclude that both nodes have the same parent,\nor that Qis the grandparent.\nAn important problem that we have not yet addressed is node failure. In\nthe case of switch-trees, a simple solution is proposed: whenever a node notices\nthat its parent has failed, it simply attaches itself to the root. At that point, the\noptimization protocol can proceed as usual and will eventually place the node at\na good point in the multicast tree. Experiments described in [Helder and Jamin,\n2002] show that the resulting tree is indeed close to a minimal spanning one.\nFlooding-based multicasting\nSo far, we have assumed that when a message is to be multicast, it is to\nbe received by every node in the overlay network. Strictly speaking, this\ncorresponds to broadcasting . In general, multicasting refers to sending a\nmessage to a subset of all the nodes, that is, a speci\ufb01c group of nodes . A\nkey design issue when it comes to multicasting is to minimize the use of\nintermediate nodes for which the message is not intended. To make this clear,\nif the overlay is organized as a multi-level tree, yet only the leaf nodes are the\nones who should receive a multicast message, then clearly there may be quite\nsome nodes who need to store and subsequently forward a message that is\nnot meant for them.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 225\nby any single node.\nThere are different criteria for deciding to switch parents. A simple one is to\noptimize the route to the source, effectively minimizing the delay when a message\nis to be multicast. To this end, each node regularly receives information on other\nnodes (we will explain one speci\ufb01c way of doing this below). At that point, the\nnode can evaluate whether another node would be a better parent in terms of\ndelay along the route to the source, and if so, initiates a switch.\nAnother criteria could be whether the delay to the potential other parent is\nlower than to the current parent. If every node takes this as a criterion, then\nthe aggregated delays of the resulting tree should ideally be minimal. In other\nwords, this is an example of optimizing the cost of the tree as we explained\nabove. However, more information would be needed to construct such a tree, but\nas it turns out, this simple scheme is a reasonable heuristic leading to a good\napproximation of a minimal spanning tree.\nAs an example, consider the case where a node Preceives information on\nthe neighbors of its parent. Note that the neighbors consist of P\u2019s grandparent,\nalong with the other siblings of P\u2019s parent. Node Pcan then evaluate the delays\nto each of these nodes and subsequently choose the one with the lowest delay,\nsayQ, as its new parent. To that end, it sends a switch request to Q. To prevent\nloops from being formed due to concurrent switching requests, a node that has an\noutstanding switch request will simply refuse to process any incoming requests.\nIn effect, this leads to a situation where only completely independent switches\ncan be carried out simultaneously. Furthermore, Pwill provide Qwith enough\ninformation to allow the latter to conclude that both nodes have the same parent,\nor that Qis the grandparent.\nAn important problem that we have not yet addressed is node failure. In\nthe case of switch-trees, a simple solution is proposed: whenever a node notices\nthat its parent has failed, it simply attaches itself to the root. At that point, the\noptimization protocol can proceed as usual and will eventually place the node at\na good point in the multicast tree. Experiments described in [Helder and Jamin,\n2002] show that the resulting tree is indeed close to a minimal spanning one.\nFlooding-based multicasting\nSo far, we have assumed that when a message is to be multicast, it is to\nbe received by every node in the overlay network. Strictly speaking, this\ncorresponds to broadcasting . In general, multicasting refers to sending a\nmessage to a subset of all the nodes, that is, a speci\ufb01c group of nodes . A\nkey design issue when it comes to multicasting is to minimize the use of\nintermediate nodes for which the message is not intended. To make this clear,\nif the overlay is organized as a multi-level tree, yet only the leaf nodes are the\nones who should receive a multicast message, then clearly there may be quite\nsome nodes who need to store and subsequently forward a message that is\nnot meant for them.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "226 CHAPTER 4. COMMUNICATION\nOne simple way to avoid such inef\ufb01ciency, is to construct an overlay\nnetwork per multicast group . As a consequence, multicasting a message mto a\ngroup Gis the same as broadcasting mtoG. The drawback of this solution is\nthat a node belonging to several groups, will, in principle, need to maintain a\nseparate list of its neighbors for each group of which it is a member.\nIf we assume that an overlay corresponds to a multicast group, and thus\nthat we need to broadcast a message, a naive way of doing so is to apply\n\ufb02ooding . In this case, each node simply forwards a message mto each of its\nneighbors, except to the one from which it received m. Furthermore, if a node\nkeeps track of the messages it received and forwarded, it can simply ignore\nduplicates. We will roughly see twice as many messages being sent as there\nare links in the overlay network, making \ufb02ooding quite inef\ufb01cient.\nTo understand the performance of \ufb02ooding, we model an overlay network\nas a connected graph Gwith Nnodes and Medges. Keep in mind that\n\ufb02ooding means that we need to send (at least) Mmessages. Only if Gis a\ntree, will \ufb02ooding be optimal, for in that case, M=N\u00001. In the worst case,\nwhen Gis fully conneced, we will have to send out M=(N\n2)=1\n2\u0001N\u0001(N\u00001)\nmessages.\nSuppose now that we have no information on the structure of the overlay\nnetwork and that the best we can assume is that it can be represented as\narandom graph , which (when keeping matters simple) is a graph having\na probability pedgethat two vertices are joined by an edge, also known as\nanErd\u00f6s-R\u00e9nyi graph [Erd\u00f6s and R\u00e9nyi, 1959]. Note that we are actually\nconsidering our overlay network to be an unstructured peer-to-peer network,\nand that we do not have any information on how it is being constructed. With\na probability pedgethat two nodes are joined, and a total of (N\n2)edges, it is not\ndif\ufb01cult to see that we can expect our overlay to have M=1\n2\u0001pedge\u0001N\u0001(N\u00001)\nedges. To give an impression of what we are dealing with, Figure 4.36 shows\nthe relationship between the number of nodes and edges for different values\nofpedge.\nFigure 4.36: The size of a random overlay as function of the number of nodes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n226 CHAPTER 4. COMMUNICATION\nOne simple way to avoid such inef\ufb01ciency, is to construct an overlay\nnetwork per multicast group . As a consequence, multicasting a message mto a\ngroup Gis the same as broadcasting mtoG. The drawback of this solution is\nthat a node belonging to several groups, will, in principle, need to maintain a\nseparate list of its neighbors for each group of which it is a member.\nIf we assume that an overlay corresponds to a multicast group, and thus\nthat we need to broadcast a message, a naive way of doing so is to apply\n\ufb02ooding . In this case, each node simply forwards a message mto each of its\nneighbors, except to the one from which it received m. Furthermore, if a node\nkeeps track of the messages it received and forwarded, it can simply ignore\nduplicates. We will roughly see twice as many messages being sent as there\nare links in the overlay network, making \ufb02ooding quite inef\ufb01cient.\nTo understand the performance of \ufb02ooding, we model an overlay network\nas a connected graph Gwith Nnodes and Medges. Keep in mind that\n\ufb02ooding means that we need to send (at least) Mmessages. Only if Gis a\ntree, will \ufb02ooding be optimal, for in that case, M=N\u00001. In the worst case,\nwhen Gis fully conneced, we will have to send out M=(N\n2)=1\n2\u0001N\u0001(N\u00001)\nmessages.\nSuppose now that we have no information on the structure of the overlay\nnetwork and that the best we can assume is that it can be represented as\narandom graph , which (when keeping matters simple) is a graph having\na probability pedgethat two vertices are joined by an edge, also known as\nanErd\u00f6s-R\u00e9nyi graph [Erd\u00f6s and R\u00e9nyi, 1959]. Note that we are actually\nconsidering our overlay network to be an unstructured peer-to-peer network,\nand that we do not have any information on how it is being constructed. With\na probability pedgethat two nodes are joined, and a total of (N\n2)edges, it is not\ndif\ufb01cult to see that we can expect our overlay to have M=1\n2\u0001pedge\u0001N\u0001(N\u00001)\nedges. To give an impression of what we are dealing with, Figure 4.36 shows\nthe relationship between the number of nodes and edges for different values\nofpedge.\nFigure 4.36: The size of a random overlay as function of the number of nodes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 227\nTo reduce the number of messages, we can also use probabilistic \ufb02ooding\nas introduced by Banaei-Kashani and Shahab [2003] and formally analyzed\nby Oikonomou and Stavrakakis [2007]. The idea is very simple: when a node\nis \ufb02ooding a message mand needs to forward mto a speci\ufb01c neighbor, it will\ndo so with a probability p\ufb02ood. The effect can be dramatic: the total number\nof messages sent will drop linearly in p\ufb02ood. However, there is also a risk: the\nlower p\ufb02ood, the higher the chance that not all nodes in the network will be\nreached. This risk is caused by the simple fact that all neighbors of a speci\ufb01c\nnode Qhave decided notto forward mtoQ. IfQhasnneighbors, then this\ncan happen roughly with a probability of (1\u0000p\ufb02ood)n. Clearly, the number\nof neighbors plays an important role in deciding whether or not to forward\na message, and, indeed, we can replace the static probability of forwarding\nwith one that takes the degree of the neighbor into account. This heuristic has\nbeen further developed and analyzed by Sereno and Gaeta [2011]. To give an\nidea of the ef\ufb01ciency of probabilistic broadcasting: in a random network of\n10,000 nodes and pedge=0.1, we need only set p\ufb02ood=0.01 to establish a more\nthan 50-fold reduction in the number of messages sent in comparison to full\n\ufb02ooding.\nWhen dealing with a structured overlay, that is, one having a more or less\ndeterministic topology, designing ef\ufb01cient \ufb02ooding schemes is simpler. As an\nexample, consider an n-dimensional hypercube, shown in Figure 4.37 for the\ncase n=4, as also discussed in Chapter 2.\nFigure 4.37: A simple peer-to-peer system organized as a four-dimensional\nhypercube.\nA simple and ef\ufb01cient broadcast scheme has been designed by Schlosser\net al. [2002] and relies on keeping track of neighbors per dimension . This is\nbest explained by considering that every node in an n-dimensional hypercube\nis represented by a bit string of length n. Each edge in the overlay is labeled\nwith its dimension. For the case n=4, node 0000 will have as its neighbors\nthe setf0001 ,0010 ,0100 ,1000g. The edge between 0000 and 0001 is labeled\n\u201c4\u201d corresponding to changing the 4th bit when comparing 0000 to0001 and\nvice versa . Likewise, the edge h0000 ,0100iis labeled \u201c2,\u201d and so forth. A node\ninitially broadcasts a message mto all of its neighbors, and tags mwith the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 227\nTo reduce the number of messages, we can also use probabilistic \ufb02ooding\nas introduced by Banaei-Kashani and Shahab [2003] and formally analyzed\nby Oikonomou and Stavrakakis [2007]. The idea is very simple: when a node\nis \ufb02ooding a message mand needs to forward mto a speci\ufb01c neighbor, it will\ndo so with a probability p\ufb02ood. The effect can be dramatic: the total number\nof messages sent will drop linearly in p\ufb02ood. However, there is also a risk: the\nlower p\ufb02ood, the higher the chance that not all nodes in the network will be\nreached. This risk is caused by the simple fact that all neighbors of a speci\ufb01c\nnode Qhave decided notto forward mtoQ. IfQhasnneighbors, then this\ncan happen roughly with a probability of (1\u0000p\ufb02ood)n. Clearly, the number\nof neighbors plays an important role in deciding whether or not to forward\na message, and, indeed, we can replace the static probability of forwarding\nwith one that takes the degree of the neighbor into account. This heuristic has\nbeen further developed and analyzed by Sereno and Gaeta [2011]. To give an\nidea of the ef\ufb01ciency of probabilistic broadcasting: in a random network of\n10,000 nodes and pedge=0.1, we need only set p\ufb02ood=0.01 to establish a more\nthan 50-fold reduction in the number of messages sent in comparison to full\n\ufb02ooding.\nWhen dealing with a structured overlay, that is, one having a more or less\ndeterministic topology, designing ef\ufb01cient \ufb02ooding schemes is simpler. As an\nexample, consider an n-dimensional hypercube, shown in Figure 4.37 for the\ncase n=4, as also discussed in Chapter 2.\nFigure 4.37: A simple peer-to-peer system organized as a four-dimensional\nhypercube.\nA simple and ef\ufb01cient broadcast scheme has been designed by Schlosser\net al. [2002] and relies on keeping track of neighbors per dimension . This is\nbest explained by considering that every node in an n-dimensional hypercube\nis represented by a bit string of length n. Each edge in the overlay is labeled\nwith its dimension. For the case n=4, node 0000 will have as its neighbors\nthe setf0001 ,0010 ,0100 ,1000g. The edge between 0000 and 0001 is labeled\n\u201c4\u201d corresponding to changing the 4th bit when comparing 0000 to0001 and\nvice versa . Likewise, the edge h0000 ,0100iis labeled \u201c2,\u201d and so forth. A node\ninitially broadcasts a message mto all of its neighbors, and tags mwith the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "228 CHAPTER 4. COMMUNICATION\nlabel of the edge over which it sends the message. In our example, if node\n1001 broadcasts a message, it will send the following:\n\u2022 (m,1) to 0001\n\u2022 (m,2) to 1101\n\u2022 (m,3) to 1011\n\u2022 (m,4) to 1000\nWhen a node receives a broadcast message, it will forward it only along edges\nthat have a higher dimension. In other words, in our example, node 1101 will\nforward monly to nodes 1111 (joined to 1101 by an edge labeled \u201c3\u201d) and\n1100 (joined by an edge with label \u201c4\u201d). Using this scheme, it can be shown\nthat every broadcast requires precisely N\u00001messages, where N=2n, that is\nthe number of nodes in a n-dimensional hypercube. This broadcasting scheme\nis therefore optimal in terms of the number of messages sent.\nNote 4.15 (Advanced: Ring-based \ufb02ooding)\nA hypercube is a straightforward example of how we can effectively use knowl-\nedge of the structure of an overlay network to establish ef\ufb01cient \ufb02ooding. In the\ncase of Chord, we can follow an approach proposed by Ghodsi [2010]. Recall that\nin Chord each node is identi\ufb01ed by a number p, and each resource (typically a \ufb01le),\nis assigned a key kfrom the same space as used for node identi\ufb01ers. The successor\nsucc(k)of a key kis the node with the smallest identi\ufb01er p\u0015k. Consider the\nsmall Chord ring shown in Figure 4.38 and assume that node 9wants to \ufb02ood a\nmessage to all other nodes.\nFigure 4.38: A Chord ring in which node 9broadcasts a message.\nIn our example, node 9divides the identi\ufb01er space into four segments (one\nfor each of its neighbors). Node 28is requested to make sure that the message\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n228 CHAPTER 4. COMMUNICATION\nlabel of the edge over which it sends the message. In our example, if node\n1001 broadcasts a message, it will send the following:\n\u2022 (m,1) to 0001\n\u2022 (m,2) to 1101\n\u2022 (m,3) to 1011\n\u2022 (m,4) to 1000\nWhen a node receives a broadcast message, it will forward it only along edges\nthat have a higher dimension. In other words, in our example, node 1101 will\nforward monly to nodes 1111 (joined to 1101 by an edge labeled \u201c3\u201d) and\n1100 (joined by an edge with label \u201c4\u201d). Using this scheme, it can be shown\nthat every broadcast requires precisely N\u00001messages, where N=2n, that is\nthe number of nodes in a n-dimensional hypercube. This broadcasting scheme\nis therefore optimal in terms of the number of messages sent.\nNote 4.15 (Advanced: Ring-based \ufb02ooding)\nA hypercube is a straightforward example of how we can effectively use knowl-\nedge of the structure of an overlay network to establish ef\ufb01cient \ufb02ooding. In the\ncase of Chord, we can follow an approach proposed by Ghodsi [2010]. Recall that\nin Chord each node is identi\ufb01ed by a number p, and each resource (typically a \ufb01le),\nis assigned a key kfrom the same space as used for node identi\ufb01ers. The successor\nsucc(k)of a key kis the node with the smallest identi\ufb01er p\u0015k. Consider the\nsmall Chord ring shown in Figure 4.38 and assume that node 9wants to \ufb02ood a\nmessage to all other nodes.\nFigure 4.38: A Chord ring in which node 9broadcasts a message.\nIn our example, node 9divides the identi\ufb01er space into four segments (one\nfor each of its neighbors). Node 28is requested to make sure that the message\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 229\nreaches all nodes with identi\ufb01ers 28\u0014k<9(recall that we are applying modulo\narithmetic); node 18takes care of nodes with identi\ufb01ers 18\u0014k<28; node 14for\n14\u0014k<18; and 11for identi\ufb01ers 11\u0014k<14.\nNode 28will subsequently divide the part of the identi\ufb01er space it is requested\nto handle into two subsegments: one for its neighboring node 1and another for 4.\nLikewise, node 18, responsible for segment [18,28)will \u201csplit\u201d that segment into\nonly one part: it has only one neighbor to delegate the \ufb02ood to, and forwards the\nmessage to node 20, telling it that it should handle segment [20,28).\nIn the last step, only node 20has work to do. It forwards the message to node\n21, telling it to forward it to nodes known to it in the segment [21,28). As there\nare no such nodes anymore, the broadcast completes.\nAs in the case of our hypercube example, we see that \ufb02ooding is done with\nN\u00001 message, with Nbeing the number of nodes in the system.\nGossip-based data dissemination\nAn important technique for disseminating information is to rely on epidemic\nbehavior , also referred to as gossiping . Observing how diseases spread\namong people, researchers have since long investigated whether simple tech-\nniques could be developed for spreading information in very large-scale\ndistributed systems. The main goal of these epidemic protocols is to rapidly\npropagate information among a large collection of nodes using only local infor-\nmation. In other words, there is no central component by which information\ndissemination is coordinated.\nTo explain the general principles of these algorithms, we assume that all\nupdates for a speci\ufb01c data item are initiated at a single node. In this way, we\nsimply avoid write-write con\ufb02icts. The following presentation is based on the\nclassical paper by Demers et al. [1987] on epidemic algorithms. An overview\nof epidemic information dissemination can be found in [Eugster et al., 2004].\nInformation dissemination models\nAs the name suggests, epidemic algorithms are based on the theory of epi-\ndemics, which studies the spreading of infectious diseases. In the case of\nlarge-scale distributed systems, instead of spreading diseases, they spread\ninformation. Research on epidemics for distributed systems also aims at a\ncompletely different goal: whereas health organizations will do their utmost\nbest to prevent infectious diseases from spreading across large groups of\npeople, designers of epidemic algorithms for distributed systems will try to\n\u201cinfect\u201d all nodes with new information as fast as possible.\nUsing the terminology from epidemics, a node that is part of a distributed\nsystem is called infected if it holds data that it is willing to spread to other\nnodes. A node that has not yet seen this data is called susceptible . Finally, an\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 229\nreaches all nodes with identi\ufb01ers 28\u0014k<9(recall that we are applying modulo\narithmetic); node 18takes care of nodes with identi\ufb01ers 18\u0014k<28; node 14for\n14\u0014k<18; and 11for identi\ufb01ers 11\u0014k<14.\nNode 28will subsequently divide the part of the identi\ufb01er space it is requested\nto handle into two subsegments: one for its neighboring node 1and another for 4.\nLikewise, node 18, responsible for segment [18,28)will \u201csplit\u201d that segment into\nonly one part: it has only one neighbor to delegate the \ufb02ood to, and forwards the\nmessage to node 20, telling it that it should handle segment [20,28).\nIn the last step, only node 20has work to do. It forwards the message to node\n21, telling it to forward it to nodes known to it in the segment [21,28). As there\nare no such nodes anymore, the broadcast completes.\nAs in the case of our hypercube example, we see that \ufb02ooding is done with\nN\u00001 message, with Nbeing the number of nodes in the system.\nGossip-based data dissemination\nAn important technique for disseminating information is to rely on epidemic\nbehavior , also referred to as gossiping . Observing how diseases spread\namong people, researchers have since long investigated whether simple tech-\nniques could be developed for spreading information in very large-scale\ndistributed systems. The main goal of these epidemic protocols is to rapidly\npropagate information among a large collection of nodes using only local infor-\nmation. In other words, there is no central component by which information\ndissemination is coordinated.\nTo explain the general principles of these algorithms, we assume that all\nupdates for a speci\ufb01c data item are initiated at a single node. In this way, we\nsimply avoid write-write con\ufb02icts. The following presentation is based on the\nclassical paper by Demers et al. [1987] on epidemic algorithms. An overview\nof epidemic information dissemination can be found in [Eugster et al., 2004].\nInformation dissemination models\nAs the name suggests, epidemic algorithms are based on the theory of epi-\ndemics, which studies the spreading of infectious diseases. In the case of\nlarge-scale distributed systems, instead of spreading diseases, they spread\ninformation. Research on epidemics for distributed systems also aims at a\ncompletely different goal: whereas health organizations will do their utmost\nbest to prevent infectious diseases from spreading across large groups of\npeople, designers of epidemic algorithms for distributed systems will try to\n\u201cinfect\u201d all nodes with new information as fast as possible.\nUsing the terminology from epidemics, a node that is part of a distributed\nsystem is called infected if it holds data that it is willing to spread to other\nnodes. A node that has not yet seen this data is called susceptible . Finally, an\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "230 CHAPTER 4. COMMUNICATION\nupdated node that is not willing or able to spread its data is said to have been\nremoved . Note that we assume we can distinguish old from new data, for\nexample, because it has been timestamped or versioned. In this light, nodes\nare also said to spread updates.\nA popular propagation model is that of anti-entropy . In this model, a\nnode Ppicks another node Qat random, and subsequently exchanges updates\nwith Q. There are three approaches to exchanging updates:\n1.Ponly pulls in new updates from Q\n2.Ponly pushes its own updates to Q\n3.Pand Qsend updates to each other (i.e., a push-pull approach)\nWhen it comes to rapidly spreading updates, only pushing updates turns\nout to be a bad choice. Intuitively, this can be understood as follows. First,\nnote that in a pure push-based approach, updates can be propagated only by\ninfected nodes. However, if many nodes are infected, the probability of each\none selecting a susceptible node is relatively small. Consequently, chances are\nthat a particular node remains susceptible for a long period simply because it\nis not selected by an infected node.\nIn contrast, the pull-based approach works much better when many nodes\nare infected. In that case, spreading updates is essentially triggered by suscep-\ntible nodes. Chances are big that such a node will contact an infected one to\nsubsequently pull in the updates and become infected as well.\nIf only a single node is infected, updates will rapidly spread across all\nnodes using either form of anti-entropy, although push-pull remains the best\nstrategy [Jelasity et al., 2007]. De\ufb01ne a round as spanning a period in which\nevery node will have taken the initiative once to exchange updates with\na randomly chosen other node. It can then be shown that the number of\nrounds to propagate a single update to all nodes takes O(log(N)), where N\nis the number of nodes in the system. This indicates indeed that propagating\nupdates is fast, but above all scalable.\nNote 4.16 (Advanced: An analysis of anti-entropy)\nA simple and straightforward analysis will give some idea on how well anti-\nentropy works. Consider a system with Nnodes. One of these nodes initiates the\nspreading of a message mto all other nodes. Let pidenote the probability that a\nnode Phas not yet received mafter the ithround. We distinguish the following\nthree cases:\n\u2022With a pure pull-based approach, pi+1= (pi)2: not only had Pnot yet\nbeen updated in the previous round, also the node Pcontacts had not yet\nreceived m.\n\u2022With a pure push-based approach, pi+1=pi\u0001(1\u00001\nN\u00001)N(1\u0000pi): again, P\nshould not have been updated in the previous round, but also none of the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n230 CHAPTER 4. COMMUNICATION\nupdated node that is not willing or able to spread its data is said to have been\nremoved . Note that we assume we can distinguish old from new data, for\nexample, because it has been timestamped or versioned. In this light, nodes\nare also said to spread updates.\nA popular propagation model is that of anti-entropy . In this model, a\nnode Ppicks another node Qat random, and subsequently exchanges updates\nwith Q. There are three approaches to exchanging updates:\n1.Ponly pulls in new updates from Q\n2.Ponly pushes its own updates to Q\n3.Pand Qsend updates to each other (i.e., a push-pull approach)\nWhen it comes to rapidly spreading updates, only pushing updates turns\nout to be a bad choice. Intuitively, this can be understood as follows. First,\nnote that in a pure push-based approach, updates can be propagated only by\ninfected nodes. However, if many nodes are infected, the probability of each\none selecting a susceptible node is relatively small. Consequently, chances are\nthat a particular node remains susceptible for a long period simply because it\nis not selected by an infected node.\nIn contrast, the pull-based approach works much better when many nodes\nare infected. In that case, spreading updates is essentially triggered by suscep-\ntible nodes. Chances are big that such a node will contact an infected one to\nsubsequently pull in the updates and become infected as well.\nIf only a single node is infected, updates will rapidly spread across all\nnodes using either form of anti-entropy, although push-pull remains the best\nstrategy [Jelasity et al., 2007]. De\ufb01ne a round as spanning a period in which\nevery node will have taken the initiative once to exchange updates with\na randomly chosen other node. It can then be shown that the number of\nrounds to propagate a single update to all nodes takes O(log(N)), where N\nis the number of nodes in the system. This indicates indeed that propagating\nupdates is fast, but above all scalable.\nNote 4.16 (Advanced: An analysis of anti-entropy)\nA simple and straightforward analysis will give some idea on how well anti-\nentropy works. Consider a system with Nnodes. One of these nodes initiates the\nspreading of a message mto all other nodes. Let pidenote the probability that a\nnode Phas not yet received mafter the ithround. We distinguish the following\nthree cases:\n\u2022With a pure pull-based approach, pi+1= (pi)2: not only had Pnot yet\nbeen updated in the previous round, also the node Pcontacts had not yet\nreceived m.\n\u2022With a pure push-based approach, pi+1=pi\u0001(1\u00001\nN\u00001)N(1\u0000pi): again, P\nshould not have been updated in the previous round, but also none of the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 231\nupdated nodes should contact P. The probability that a node contacts Pis\n1\u00001\nN\u00001; we can expect that there are N(1\u0000pi)updated nodes in round i.\n\u2022In a push-pull approach, we can simply combine the two: Pshould not\ncontact an updated node, and should not be contacted by one.\nFigure 4.39: The probability of not yet having been updated as a function\nof the number of dissemination rounds.\nFigure 4.39 shows how quickly the probability of not yet being updated drops\nas a function of the number of rounds. Indeed, assuming that nodes are up-\nand-running all the time, it turns out that anti-entropy is an extremely effective\ndissemination protocol.\nOne speci\ufb01c variant of epidemic protocols is called rumor spreading . It\nworks as follows. If node Phas just been updated for data item x, it contacts\nan arbitrary other node Qand tries to push the update to Q. However, it is\npossible that Qwas already updated by another node. In that case, Pmay\nlose interest in spreading the update any further, say with probability pstop. In\nother words, it then becomes removed.\nRumor spreading is gossiping analogous to real life. When Bob has some\nhot news to spread around, he may phone his friend Alice telling her all about\nit. Alice, like Bob, will be really excited to spread the rumor to her friends\nas well. However, she will become disappointed when phoning a friend, say\nChuck, only to hear that the news has already reached him. Chances are that\nshe will stop phoning other friends, for what good is it if they already know?\nRumor spreading turns out to be an excellent way of rapidly spread-\ning news. However, it cannot guarantee that all nodes will actually be up-\ndated [Demers et al., 1987]. In fact, when there is a large number of nodes that\nparticipate in the epidemics, the fraction sof nodes that will remain ignorant\nof an update, that is, remain susceptible, satis\ufb01es the equation:\ns=e\u0000(1/pstop+1)(1\u0000s)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 231\nupdated nodes should contact P. The probability that a node contacts Pis\n1\u00001\nN\u00001; we can expect that there are N(1\u0000pi)updated nodes in round i.\n\u2022In a push-pull approach, we can simply combine the two: Pshould not\ncontact an updated node, and should not be contacted by one.\nFigure 4.39: The probability of not yet having been updated as a function\nof the number of dissemination rounds.\nFigure 4.39 shows how quickly the probability of not yet being updated drops\nas a function of the number of rounds. Indeed, assuming that nodes are up-\nand-running all the time, it turns out that anti-entropy is an extremely effective\ndissemination protocol.\nOne speci\ufb01c variant of epidemic protocols is called rumor spreading . It\nworks as follows. If node Phas just been updated for data item x, it contacts\nan arbitrary other node Qand tries to push the update to Q. However, it is\npossible that Qwas already updated by another node. In that case, Pmay\nlose interest in spreading the update any further, say with probability pstop. In\nother words, it then becomes removed.\nRumor spreading is gossiping analogous to real life. When Bob has some\nhot news to spread around, he may phone his friend Alice telling her all about\nit. Alice, like Bob, will be really excited to spread the rumor to her friends\nas well. However, she will become disappointed when phoning a friend, say\nChuck, only to hear that the news has already reached him. Chances are that\nshe will stop phoning other friends, for what good is it if they already know?\nRumor spreading turns out to be an excellent way of rapidly spread-\ning news. However, it cannot guarantee that all nodes will actually be up-\ndated [Demers et al., 1987]. In fact, when there is a large number of nodes that\nparticipate in the epidemics, the fraction sof nodes that will remain ignorant\nof an update, that is, remain susceptible, satis\ufb01es the equation:\ns=e\u0000(1/pstop+1)(1\u0000s)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "232 CHAPTER 4. COMMUNICATION\nNote 4.17 (Advanced: Analysis of rumor spreading)\nIn order to formally analyze the situation for rumor spreading, we let sdenote the\nfraction of nodes that have not yet been updated, i.e., the fraction of susceptible\nnodes. Likewise, let idenote the fraction of infected nodes: the ones that have\nbeen updated and are still contacting other nodes in order to spread the news.\nFinally, ris the fraction of nodes that have been updated, but have given up, i.e.,\nthey are no passive and play no more part in disseminating news. Obviously,\ns+i+r=1. Using theory from epidemics, it is not dif\ufb01cult to see the following:\n(1)ds/dt=\u0000s\u0001i\n(2)di/dt= s\u0001i\u0000pstop\u0001(1\u0000s)\u0001i\n) di/ds=\u0000(1+pstop) +pstop\ns\n) i(s) =\u0000(1+pstop)\u0001s+pstop\u0001ln(s) +C\nwhere we use the notation i(s)to express ias a function of s. When s=1, no\nnodes have yet been infected, meaning that i(1) =0. This allows us to derive that\nC=1+pstop, and thus\ni(s) = ( 1+pstop)\u0001(1\u0000s) +pstop\u0001ln(s)\nWe are looking for the situation that there is no more rumor spreading, i.e., when\ni(s) =0. Having a closed expression for i(s)then leads to\ns=e\u0000(1/pstop+1)(1\u0000s)\nTo get an idea of what this means, take a look at Figure 4.40, which\nshows sas a function of pstop. Even for high values of pstopwe see that the\nfraction of nodes that remains ignorant is relatively low, and always less that\napproximately 0.2. For pstop=0.20 it can be shown that s=0.0025 . However,\nin those cases when pstopis relatively high, additional measures will need to\nbe taken to ensure that allnodes are updated.\nOne of the main advantages of epidemic algorithms is their scalability, due\nto the fact that the number of synchronizations between processes is relatively\nsmall compared to other propagation methods. For wide-area systems, Lin\nand Marzullo [1999] have shown that it makes sense to take the actual network\ntopology into account to achieve better results. In that case, nodes that are\nconnected to only a few other nodes are contacted with a relatively high\nprobability. The underlying assumption is that such nodes form a bridge to\nother remote parts of the network; therefore, they should be contacted as soon\nas possible. This approach is referred to as directional gossiping and comes\nin different variants.\nThis problem touches upon an important assumption that most epidemic\nsolutions make, namely that a node can randomly select any other node to\ngossip with. This implies that, in principle, the complete set of nodes should\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n232 CHAPTER 4. COMMUNICATION\nNote 4.17 (Advanced: Analysis of rumor spreading)\nIn order to formally analyze the situation for rumor spreading, we let sdenote the\nfraction of nodes that have not yet been updated, i.e., the fraction of susceptible\nnodes. Likewise, let idenote the fraction of infected nodes: the ones that have\nbeen updated and are still contacting other nodes in order to spread the news.\nFinally, ris the fraction of nodes that have been updated, but have given up, i.e.,\nthey are no passive and play no more part in disseminating news. Obviously,\ns+i+r=1. Using theory from epidemics, it is not dif\ufb01cult to see the following:\n(1)ds/dt=\u0000s\u0001i\n(2)di/dt= s\u0001i\u0000pstop\u0001(1\u0000s)\u0001i\n) di/ds=\u0000(1+pstop) +pstop\ns\n) i(s) =\u0000(1+pstop)\u0001s+pstop\u0001ln(s) +C\nwhere we use the notation i(s)to express ias a function of s. When s=1, no\nnodes have yet been infected, meaning that i(1) =0. This allows us to derive that\nC=1+pstop, and thus\ni(s) = ( 1+pstop)\u0001(1\u0000s) +pstop\u0001ln(s)\nWe are looking for the situation that there is no more rumor spreading, i.e., when\ni(s) =0. Having a closed expression for i(s)then leads to\ns=e\u0000(1/pstop+1)(1\u0000s)\nTo get an idea of what this means, take a look at Figure 4.40, which\nshows sas a function of pstop. Even for high values of pstopwe see that the\nfraction of nodes that remains ignorant is relatively low, and always less that\napproximately 0.2. For pstop=0.20 it can be shown that s=0.0025 . However,\nin those cases when pstopis relatively high, additional measures will need to\nbe taken to ensure that allnodes are updated.\nOne of the main advantages of epidemic algorithms is their scalability, due\nto the fact that the number of synchronizations between processes is relatively\nsmall compared to other propagation methods. For wide-area systems, Lin\nand Marzullo [1999] have shown that it makes sense to take the actual network\ntopology into account to achieve better results. In that case, nodes that are\nconnected to only a few other nodes are contacted with a relatively high\nprobability. The underlying assumption is that such nodes form a bridge to\nother remote parts of the network; therefore, they should be contacted as soon\nas possible. This approach is referred to as directional gossiping and comes\nin different variants.\nThis problem touches upon an important assumption that most epidemic\nsolutions make, namely that a node can randomly select any other node to\ngossip with. This implies that, in principle, the complete set of nodes should\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.4. MULTICAST COMMUNICATION 233\nFigure 4.40: The relation between the fraction sof update-ignorant nodes and\nthe probability pstopthat a node will stop gossiping once it contacts a node\nthat has already been updated.\nbe known to each member. In a large system, this assumption can never hold\nand special measures will need to be taken to mimic such properties. We\nreturn to this issue in Section 6.7 when we discuss a peer-sampling service.\nRemoving data\nEpidemic algorithms are extremely good for spreading updates. However,\nthey have a rather strange side-effect: spreading the deletion of a data item\nis hard. The essence of the problem lies in the fact that deletion of a data\nitem destroys all information on that item. Consequently, when a data item\nis simply removed from a node, that node will eventually receive old copies\nof the data item and interpret those as updates on something it did not have\nbefore.\nThe trick is to record the deletion of a data item as just another update, and\nkeep a record of that deletion. In this way, old copies will not be interpreted\nas something new, but merely treated as versions that have been updated by\na delete operation. The recording of a deletion is done by spreading death\ncerti\ufb01cates .\nOf course, the problem with death certi\ufb01cates is that they should eventually\nbe cleaned up, or otherwise each node will gradually build a huge local\ndatabase of historical information on deleted data items that is otherwise not\nused. Demers et al. [1987] propose to use what are called dormant death\ncerti\ufb01cates. Each death certi\ufb01cate is timestamped when it is created. If it can\nbe assumed that updates propagate to all nodes within a known \ufb01nite time,\nthen death certi\ufb01cates can be removed after this maximum propagation time\nhas elapsed.\nHowever, to provide hard guarantees that deletions are indeed spread to\nall nodes, only a very few nodes maintain dormant death certi\ufb01cates that are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.4. MULTICAST COMMUNICATION 233\nFigure 4.40: The relation between the fraction sof update-ignorant nodes and\nthe probability pstopthat a node will stop gossiping once it contacts a node\nthat has already been updated.\nbe known to each member. In a large system, this assumption can never hold\nand special measures will need to be taken to mimic such properties. We\nreturn to this issue in Section 6.7 when we discuss a peer-sampling service.\nRemoving data\nEpidemic algorithms are extremely good for spreading updates. However,\nthey have a rather strange side-effect: spreading the deletion of a data item\nis hard. The essence of the problem lies in the fact that deletion of a data\nitem destroys all information on that item. Consequently, when a data item\nis simply removed from a node, that node will eventually receive old copies\nof the data item and interpret those as updates on something it did not have\nbefore.\nThe trick is to record the deletion of a data item as just another update, and\nkeep a record of that deletion. In this way, old copies will not be interpreted\nas something new, but merely treated as versions that have been updated by\na delete operation. The recording of a deletion is done by spreading death\ncerti\ufb01cates .\nOf course, the problem with death certi\ufb01cates is that they should eventually\nbe cleaned up, or otherwise each node will gradually build a huge local\ndatabase of historical information on deleted data items that is otherwise not\nused. Demers et al. [1987] propose to use what are called dormant death\ncerti\ufb01cates. Each death certi\ufb01cate is timestamped when it is created. If it can\nbe assumed that updates propagate to all nodes within a known \ufb01nite time,\nthen death certi\ufb01cates can be removed after this maximum propagation time\nhas elapsed.\nHowever, to provide hard guarantees that deletions are indeed spread to\nall nodes, only a very few nodes maintain dormant death certi\ufb01cates that are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "234 CHAPTER 4. COMMUNICATION\nnever thrown away. Assume node Phas such a certi\ufb01cate for data item x.\nIf by any chance an obsolete update for xreaches P,Pwill react by simply\nspreading the death certi\ufb01cate for xagain.\n4.5 Summary\nHaving powerful and \ufb02exible facilities for communication between processes\nis essential for any distributed system. In traditional network applications,\ncommunication is often based on the low-level message-passing primitives\noffered by the transport layer. An important issue in middleware systems\nis to offer a higher level of abstraction that will make it easier to express\ncommunication between processes than the support offered by the interface\nto the transport layer.\nOne of the most widely used abstractions is the Remote Procedure Call\n(RPC). The essence of an RPC is that a service is implemented by means\nof a procedure, of which the body is executed at a server. The client is\noffered only the signature of the procedure, that is, the procedure\u2019s name\nalong with its parameters. When the client calls the procedure, the client-\nside implementation, called a stub, takes care of wrapping the parameter\nvalues into a message and sending that to the server. The latter calls the\nactual procedure and returns the results, again in a message. The client\u2019s stub\nextracts the result values from the return message and passes it back to the\ncalling client application.\nRPCs offer synchronous communication facilities, by which a client is\nblocked until the server has sent a reply. Although variations of either\nmechanism exist by which this strict synchronous model is relaxed, it turns\nout that general-purpose, high-level message-oriented models are often more\nconvenient.\nIn message-oriented models, the issues are whether or not communication\nis persistent, and whether or not communication is synchronous. The essence\nof persistent communication is that a message that is submitted for transmis-\nsion, is stored by the communication system as long as it takes to deliver\nit. In other words, neither the sender nor the receiver needs to be up and\nrunning for message transmission to take place. In transient communication,\nno storage facilities are offered, so that the receiver must be prepared to accept\nthe message when it is sent.\nIn asynchronous communication, the sender is allowed to continue imme-\ndiately after the message has been submitted for transmission, possibly before\nit has even been sent. In synchronous communication, the sender is blocked\nat least until a message has been received. Alternatively, the sender may be\nblocked until message delivery has taken place or even until the receiver has\nresponded as with RPCs.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n234 CHAPTER 4. COMMUNICATION\nnever thrown away. Assume node Phas such a certi\ufb01cate for data item x.\nIf by any chance an obsolete update for xreaches P,Pwill react by simply\nspreading the death certi\ufb01cate for xagain.\n4.5 Summary\nHaving powerful and \ufb02exible facilities for communication between processes\nis essential for any distributed system. In traditional network applications,\ncommunication is often based on the low-level message-passing primitives\noffered by the transport layer. An important issue in middleware systems\nis to offer a higher level of abstraction that will make it easier to express\ncommunication between processes than the support offered by the interface\nto the transport layer.\nOne of the most widely used abstractions is the Remote Procedure Call\n(RPC). The essence of an RPC is that a service is implemented by means\nof a procedure, of which the body is executed at a server. The client is\noffered only the signature of the procedure, that is, the procedure\u2019s name\nalong with its parameters. When the client calls the procedure, the client-\nside implementation, called a stub, takes care of wrapping the parameter\nvalues into a message and sending that to the server. The latter calls the\nactual procedure and returns the results, again in a message. The client\u2019s stub\nextracts the result values from the return message and passes it back to the\ncalling client application.\nRPCs offer synchronous communication facilities, by which a client is\nblocked until the server has sent a reply. Although variations of either\nmechanism exist by which this strict synchronous model is relaxed, it turns\nout that general-purpose, high-level message-oriented models are often more\nconvenient.\nIn message-oriented models, the issues are whether or not communication\nis persistent, and whether or not communication is synchronous. The essence\nof persistent communication is that a message that is submitted for transmis-\nsion, is stored by the communication system as long as it takes to deliver\nit. In other words, neither the sender nor the receiver needs to be up and\nrunning for message transmission to take place. In transient communication,\nno storage facilities are offered, so that the receiver must be prepared to accept\nthe message when it is sent.\nIn asynchronous communication, the sender is allowed to continue imme-\ndiately after the message has been submitted for transmission, possibly before\nit has even been sent. In synchronous communication, the sender is blocked\nat least until a message has been received. Alternatively, the sender may be\nblocked until message delivery has taken place or even until the receiver has\nresponded as with RPCs.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "4.5. SUMMARY 235\nMessage-oriented middleware models generally offer persistent asyn-\nchronous communication, and are used where RPCs are not appropriate.\nThey are often used to assist the integration of (widely dispersed) collections\nof databases into large-scale information systems.\nFinally, an important class of communication protocols in distributed\nsystems is multicasting. The basic idea is to disseminate information from\none sender to multiple receivers. We have discussed two different approaches.\nFirst, multicasting can be achieved by setting up a tree from the sender to\nthe receivers. Considering that it is now well understood how nodes can self-\norganize into peer-to-peer system, solutions have also appeared to dynamically\nset up trees in a decentralized fashion. Second, \ufb02ooding messages across\nthe network is extremely robust, yet requires special attention if we want to\navoid severe waste of resources as nodes may see messages multiple times.\nProbabilistic \ufb02ooding by which a node forwards a message with a certain\nprobability often proves to combine simplicity and ef\ufb01ciency, while being\nhighly effective.\nAnother important class of dissemination solutions deploys epidemic\nprotocols. These protocols have proven to be very simple and extremely\nrobust. Apart from merely spreading messages, epidemic protocols can also\nbe ef\ufb01ciently deployed for aggregating information across a large distributed\nsystem.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n4.5. SUMMARY 235\nMessage-oriented middleware models generally offer persistent asyn-\nchronous communication, and are used where RPCs are not appropriate.\nThey are often used to assist the integration of (widely dispersed) collections\nof databases into large-scale information systems.\nFinally, an important class of communication protocols in distributed\nsystems is multicasting. The basic idea is to disseminate information from\none sender to multiple receivers. We have discussed two different approaches.\nFirst, multicasting can be achieved by setting up a tree from the sender to\nthe receivers. Considering that it is now well understood how nodes can self-\norganize into peer-to-peer system, solutions have also appeared to dynamically\nset up trees in a decentralized fashion. Second, \ufb02ooding messages across\nthe network is extremely robust, yet requires special attention if we want to\navoid severe waste of resources as nodes may see messages multiple times.\nProbabilistic \ufb02ooding by which a node forwards a message with a certain\nprobability often proves to combine simplicity and ef\ufb01ciency, while being\nhighly effective.\nAnother important class of dissemination solutions deploys epidemic\nprotocols. These protocols have proven to be very simple and extremely\nrobust. Apart from merely spreading messages, epidemic protocols can also\nbe ef\ufb01ciently deployed for aggregating information across a large distributed\nsystem.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "", "Chapter 5\nNaming\nNames play an important role in all computer systems. They are used to share\nresources, to uniquely identify entities, to refer to locations, and more. An\nimportant issue with naming is that a name can be resolved to the entity it\nrefers to. Name resolution thus allows a process to access the named entity. To\nresolve names, it is necessary to implement a naming system. The difference\nbetween naming in distributed systems and nondistributed systems lies in the\nway naming systems are implemented.\nIn a distributed system, the implementation of a naming system is itself\noften distributed across multiple machines. How this distribution is done\nplays a key role in the ef\ufb01ciency and scalability of the naming system. In this\nchapter, we concentrate on three different, important ways that names are\nused in distributed systems.\nFirst, we consider so-called \ufb02at-naming systems. In such systems, entities\nare referred to by an identi\ufb01er that, in principle, has no meaning at all. In addi-\ntion, \ufb02at names bare no structure, implying that we need special mechanisms\nto trace the location of such entities. We discuss various approaches, ranging\nfrom chains of forwarding links, to distributed hash tables, to hierarchical\nlocation services.\nIn practice, humans prefer to use readable names. Such names are of-\nten structured, as is well known from the way Web pages are referred to.\nStructured names allow for a highly systematic way of \ufb01nding the server\nresponsible for the named entity, as exempli\ufb01ed by the Domain Name System.\nWe discuss the general principles, as well as scalability issues.\nFinally, humans often prefer to describe entities by means of various\ncharacteristics, leading to a situation in which we need to resolve a description\nby means of the attributes assigned to an entity. As we shall see, this type\nof name resolution is notoriously dif\ufb01cult, especially in combination with\nsearching.\n237\nChapter 5\nNaming\nNames play an important role in all computer systems. They are used to share\nresources, to uniquely identify entities, to refer to locations, and more. An\nimportant issue with naming is that a name can be resolved to the entity it\nrefers to. Name resolution thus allows a process to access the named entity. To\nresolve names, it is necessary to implement a naming system. The difference\nbetween naming in distributed systems and nondistributed systems lies in the\nway naming systems are implemented.\nIn a distributed system, the implementation of a naming system is itself\noften distributed across multiple machines. How this distribution is done\nplays a key role in the ef\ufb01ciency and scalability of the naming system. In this\nchapter, we concentrate on three different, important ways that names are\nused in distributed systems.\nFirst, we consider so-called \ufb02at-naming systems. In such systems, entities\nare referred to by an identi\ufb01er that, in principle, has no meaning at all. In addi-\ntion, \ufb02at names bare no structure, implying that we need special mechanisms\nto trace the location of such entities. We discuss various approaches, ranging\nfrom chains of forwarding links, to distributed hash tables, to hierarchical\nlocation services.\nIn practice, humans prefer to use readable names. Such names are of-\nten structured, as is well known from the way Web pages are referred to.\nStructured names allow for a highly systematic way of \ufb01nding the server\nresponsible for the named entity, as exempli\ufb01ed by the Domain Name System.\nWe discuss the general principles, as well as scalability issues.\nFinally, humans often prefer to describe entities by means of various\ncharacteristics, leading to a situation in which we need to resolve a description\nby means of the attributes assigned to an entity. As we shall see, this type\nof name resolution is notoriously dif\ufb01cult, especially in combination with\nsearching.\n237", "238 CHAPTER 5. NAMING\n5.1 Names, identi\ufb01ers, and addresses\nLet us start by taking a closer look at what a name actually is. A name in a\ndistributed system is a string of bits or characters that is used to refer to an\nentity. An entity in a distributed system can be practically anything. Typical\nexamples include resources such as hosts, printers, disks, and \ufb01les. Other\nwell-known examples of entities that are often explicitly named are processes,\nusers, mailboxes, newsgroups, Web pages, graphical windows, messages,\nnetwork connections, and so on.\nEntities can be operated on. For example, a resource such as a printer\noffers an interface containing operations for printing a document, requesting\nthe status of a print job, and the like. Furthermore, an entity such as a network\nconnection may provide operations for sending and receiving data, setting\nquality-of-service parameters, requesting the status, and so forth.\nTo operate on an entity, it is necessary to access it, for which we need an\naccess point . An access point is yet another, but special, kind of entity in a\ndistributed system. The name of an access point is called an address . The\naddress of an access point of an entity is also simply called an address of that\nentity.\nAn entity can offer more than one access point. As a comparison, a\ntelephone can be viewed as an access point of a person, whereas the telephone\nnumber corresponds to an address. Indeed, many people nowadays have\nseveral telephone numbers, each number corresponding to a point where they\ncan be reached. In a distributed system, a typical example of an access point is\na host running a speci\ufb01c server, with its address formed by the combination of,\nfor example, an IP address and port number (i.e., the server\u2019s transport-level\naddress).\nAn entity may change its access points in the course of time. For example,\nwhen a mobile computer moves to another location, it is often assigned a\ndifferent IP address than the one it had before. Likewise, when a person\nmoves to another city or country, it is often necessary to change telephone\nnumbers as well. In a similar fashion, changing jobs or Internet Service\nProviders, means changing your e-mail address.\nAn address is thus just a special kind of name: it refers to an access point\nof an entity. Because an access point is tightly associated with an entity, it\nwould seem convenient to use the address of an access point as a regular\nname for the associated entity. Nevertheless, this is hardly ever done as such\nnaming is generally very in\ufb02exible and often human unfriendly.\nFor example, it is not uncommon to regularly reorganize a distributed\nsystem so that a speci\ufb01c server is now running on a different host than\npreviously. The old machine on which the server used to be running may\nbe reassigned to a completely different server. In other words, an entity\nmay easily change an access point, or an access point may be reassigned to\na different entity. If an address is used to refer to an entity, we will have\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n238 CHAPTER 5. NAMING\n5.1 Names, identi\ufb01ers, and addresses\nLet us start by taking a closer look at what a name actually is. A name in a\ndistributed system is a string of bits or characters that is used to refer to an\nentity. An entity in a distributed system can be practically anything. Typical\nexamples include resources such as hosts, printers, disks, and \ufb01les. Other\nwell-known examples of entities that are often explicitly named are processes,\nusers, mailboxes, newsgroups, Web pages, graphical windows, messages,\nnetwork connections, and so on.\nEntities can be operated on. For example, a resource such as a printer\noffers an interface containing operations for printing a document, requesting\nthe status of a print job, and the like. Furthermore, an entity such as a network\nconnection may provide operations for sending and receiving data, setting\nquality-of-service parameters, requesting the status, and so forth.\nTo operate on an entity, it is necessary to access it, for which we need an\naccess point . An access point is yet another, but special, kind of entity in a\ndistributed system. The name of an access point is called an address . The\naddress of an access point of an entity is also simply called an address of that\nentity.\nAn entity can offer more than one access point. As a comparison, a\ntelephone can be viewed as an access point of a person, whereas the telephone\nnumber corresponds to an address. Indeed, many people nowadays have\nseveral telephone numbers, each number corresponding to a point where they\ncan be reached. In a distributed system, a typical example of an access point is\na host running a speci\ufb01c server, with its address formed by the combination of,\nfor example, an IP address and port number (i.e., the server\u2019s transport-level\naddress).\nAn entity may change its access points in the course of time. For example,\nwhen a mobile computer moves to another location, it is often assigned a\ndifferent IP address than the one it had before. Likewise, when a person\nmoves to another city or country, it is often necessary to change telephone\nnumbers as well. In a similar fashion, changing jobs or Internet Service\nProviders, means changing your e-mail address.\nAn address is thus just a special kind of name: it refers to an access point\nof an entity. Because an access point is tightly associated with an entity, it\nwould seem convenient to use the address of an access point as a regular\nname for the associated entity. Nevertheless, this is hardly ever done as such\nnaming is generally very in\ufb02exible and often human unfriendly.\nFor example, it is not uncommon to regularly reorganize a distributed\nsystem so that a speci\ufb01c server is now running on a different host than\npreviously. The old machine on which the server used to be running may\nbe reassigned to a completely different server. In other words, an entity\nmay easily change an access point, or an access point may be reassigned to\na different entity. If an address is used to refer to an entity, we will have\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.1. NAMES, IDENTIFIERS, AND ADDRESSES 239\nan invalid reference the instant the access point changes or is reassigned to\nanother entity. Therefore, it is much better to let a service be known by a\nseparate name independent of the address of the associated server.\nLikewise, if an entity offers more than one access point, it is not clear which\naddress to use as a reference. For instance, many organizations distribute their\nWeb service across several servers. If we would use the addresses of those\nservers as a reference for the Web service, it is not obvious which address\nshould be chosen as the best one. Again, a much better solution is to have\na single name for the Web service independent from the addresses of the\ndifferent Web servers.\nThese examples illustrate that a name for an entity that is independent\nfrom its addresses is often much easier and more \ufb02exible to use. Such a name\nis called location independent .\nIn addition to addresses, there are other types of names that deserve\nspecial treatment, such as names that are used to uniquely identify an entity.\nA true identi\ufb01er is a name that has the following properties [Wieringa and\nde Jonge, 1995]:\n1. An identi\ufb01er refers to at most one entity.\n2. Each entity is referred to by at most one identi\ufb01er.\n3. An identi\ufb01er always refers to the same entity (i.e., it is never reused).\nBy using identi\ufb01ers, it becomes much easier to unambiguously refer to an\nentity. For example, assume two processes each refer to an entity by means\nof an identi\ufb01er. To check if the processes are referring to the same entity, it\nis suf\ufb01cient to test if the two identi\ufb01ers are equal. Such a test would not be\nsuf\ufb01cient if the two processes were using regular, nonunique, nonidentifying\nnames. For example, the name \u201cJohn Smith\u201d cannot be taken as a unique\nreference to just a single person.\nLikewise, if an address can be reassigned to a different entity, we cannot\nuse an address as an identi\ufb01er. Consider the use of telephone numbers, which\nare reasonably stable in the sense that a telephone number will often for some\ntime refer to the same person or organization. However, using a telephone\nnumber as an identi\ufb01er will not work, as it can be reassigned in the course\nof time. Consequently, Bob\u2019s new bakery may be receiving phone calls for\nAlice\u2019s old antique store for a long time. In this case, it would have been better\nto use a true identi\ufb01er for Alice instead of her phone number.\nAddresses and identi\ufb01ers are two important types of names that are each\nused for very different purposes. In many computer systems, addresses and\nidenti\ufb01ers are represented in machine-readable form only, that is, in the form\nof bit strings. For example, an Ethernet address is essentially a random string\nof 48 bits. Likewise, memory addresses are typically represented as 32-bit or\n64-bit strings.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.1. NAMES, IDENTIFIERS, AND ADDRESSES 239\nan invalid reference the instant the access point changes or is reassigned to\nanother entity. Therefore, it is much better to let a service be known by a\nseparate name independent of the address of the associated server.\nLikewise, if an entity offers more than one access point, it is not clear which\naddress to use as a reference. For instance, many organizations distribute their\nWeb service across several servers. If we would use the addresses of those\nservers as a reference for the Web service, it is not obvious which address\nshould be chosen as the best one. Again, a much better solution is to have\na single name for the Web service independent from the addresses of the\ndifferent Web servers.\nThese examples illustrate that a name for an entity that is independent\nfrom its addresses is often much easier and more \ufb02exible to use. Such a name\nis called location independent .\nIn addition to addresses, there are other types of names that deserve\nspecial treatment, such as names that are used to uniquely identify an entity.\nA true identi\ufb01er is a name that has the following properties [Wieringa and\nde Jonge, 1995]:\n1. An identi\ufb01er refers to at most one entity.\n2. Each entity is referred to by at most one identi\ufb01er.\n3. An identi\ufb01er always refers to the same entity (i.e., it is never reused).\nBy using identi\ufb01ers, it becomes much easier to unambiguously refer to an\nentity. For example, assume two processes each refer to an entity by means\nof an identi\ufb01er. To check if the processes are referring to the same entity, it\nis suf\ufb01cient to test if the two identi\ufb01ers are equal. Such a test would not be\nsuf\ufb01cient if the two processes were using regular, nonunique, nonidentifying\nnames. For example, the name \u201cJohn Smith\u201d cannot be taken as a unique\nreference to just a single person.\nLikewise, if an address can be reassigned to a different entity, we cannot\nuse an address as an identi\ufb01er. Consider the use of telephone numbers, which\nare reasonably stable in the sense that a telephone number will often for some\ntime refer to the same person or organization. However, using a telephone\nnumber as an identi\ufb01er will not work, as it can be reassigned in the course\nof time. Consequently, Bob\u2019s new bakery may be receiving phone calls for\nAlice\u2019s old antique store for a long time. In this case, it would have been better\nto use a true identi\ufb01er for Alice instead of her phone number.\nAddresses and identi\ufb01ers are two important types of names that are each\nused for very different purposes. In many computer systems, addresses and\nidenti\ufb01ers are represented in machine-readable form only, that is, in the form\nof bit strings. For example, an Ethernet address is essentially a random string\nof 48 bits. Likewise, memory addresses are typically represented as 32-bit or\n64-bit strings.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "240 CHAPTER 5. NAMING\nAnother important type of name is that which is tailored to be used by\nhumans, also referred to as human-friendly names . In contrast to addresses\nand identi\ufb01ers, a human-friendly name is generally represented as a character\nstring. These names appear in many different forms. For example, \ufb01les in\nUnix systems have character-string names that can generally be as long as 255\ncharacters, and which are de\ufb01ned entirely by the user. Similarly, DNS names\nare represented as relatively simple case-insensitive character strings.\nHaving names, identi\ufb01ers, and addresses brings us to the central theme of\nthis chapter: how do we resolve names and identi\ufb01ers to addresses? Before\nwe go into various solutions, it is important to realize that there is often a close\nrelationship between name resolution in distributed systems and message\nrouting [Shoch, 1978]. In principle, a naming system maintains a name-to-\naddress binding which in its simplest form is just a table of (name, address)\npairs. However, in distributed systems that span large networks and for which\nmany resources need to be named, a centralized table is not going to work.\nInstead, what often happens is that a name is decomposed into several\nparts such as ftp.cs.vu.nl.and that name resolution takes place through a\nrecursive lookup of those parts. For example, a client needing to know the\naddress of the FTP server named by ftp.cs.vu.nl.would \ufb01rst resolve nlto \ufb01nd\nthe server NS(nl)responsible for names that end with nl, after which the rest\nof the name is passed to server NS(nl). This server may then resolve the name\nvuto the server NS(vu.nl)responsible for names that end with vu.nl.who can\nfurther handle the remaining name ftp.cs. Eventually, this leads to routing the\nname resolution request as:\nNS(.)!NS(nl)!NS(vu.nl)!address of ftp.cs.vu.nl\nwhere NS(.)denotes the server that can return the address of NS(nl), also\nknown as the root server .NS(vu.nl)will return the actual address of the FTP\nserver. It is interesting to note that the boundaries between name resolution\nand message routing are starting to blur.\nNote 5.1 (More information: Information-centric networking)\nName resolution and message routing play a central role in information-centric\nnetworking , or simply ICN [Ahlgren et al., 2012]. This type of networking re-\nvolves around the principle that applications are not really interested to know\nwhere an entity is stored, but rather that they can get a copy in order to access it\nlocally when needed. To this end, much research has been spent since approxi-\nmately 2007 on designing an alternative to the host-based addressing schemes that\nare common in today\u2019s Internet. In particular, the main idea is that an application\ncan retrieve an entity from the network by using that entity\u2019s name. The network\ntakes that name as input, and subsequently routes a request to an appropriate\nlocation where the entity is stored, to return a copy the requester.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n240 CHAPTER 5. NAMING\nAnother important type of name is that which is tailored to be used by\nhumans, also referred to as human-friendly names . In contrast to addresses\nand identi\ufb01ers, a human-friendly name is generally represented as a character\nstring. These names appear in many different forms. For example, \ufb01les in\nUnix systems have character-string names that can generally be as long as 255\ncharacters, and which are de\ufb01ned entirely by the user. Similarly, DNS names\nare represented as relatively simple case-insensitive character strings.\nHaving names, identi\ufb01ers, and addresses brings us to the central theme of\nthis chapter: how do we resolve names and identi\ufb01ers to addresses? Before\nwe go into various solutions, it is important to realize that there is often a close\nrelationship between name resolution in distributed systems and message\nrouting [Shoch, 1978]. In principle, a naming system maintains a name-to-\naddress binding which in its simplest form is just a table of (name, address)\npairs. However, in distributed systems that span large networks and for which\nmany resources need to be named, a centralized table is not going to work.\nInstead, what often happens is that a name is decomposed into several\nparts such as ftp.cs.vu.nl.and that name resolution takes place through a\nrecursive lookup of those parts. For example, a client needing to know the\naddress of the FTP server named by ftp.cs.vu.nl.would \ufb01rst resolve nlto \ufb01nd\nthe server NS(nl)responsible for names that end with nl, after which the rest\nof the name is passed to server NS(nl). This server may then resolve the name\nvuto the server NS(vu.nl)responsible for names that end with vu.nl.who can\nfurther handle the remaining name ftp.cs. Eventually, this leads to routing the\nname resolution request as:\nNS(.)!NS(nl)!NS(vu.nl)!address of ftp.cs.vu.nl\nwhere NS(.)denotes the server that can return the address of NS(nl), also\nknown as the root server .NS(vu.nl)will return the actual address of the FTP\nserver. It is interesting to note that the boundaries between name resolution\nand message routing are starting to blur.\nNote 5.1 (More information: Information-centric networking)\nName resolution and message routing play a central role in information-centric\nnetworking , or simply ICN [Ahlgren et al., 2012]. This type of networking re-\nvolves around the principle that applications are not really interested to know\nwhere an entity is stored, but rather that they can get a copy in order to access it\nlocally when needed. To this end, much research has been spent since approxi-\nmately 2007 on designing an alternative to the host-based addressing schemes that\nare common in today\u2019s Internet. In particular, the main idea is that an application\ncan retrieve an entity from the network by using that entity\u2019s name. The network\ntakes that name as input, and subsequently routes a request to an appropriate\nlocation where the entity is stored, to return a copy the requester.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 241\nKey to the success of this approach is, of course, a form of name-based\nrouting, which is essentially a means for resolving a name to an address where\nthe associated entity is to be found. How that routing takes place depends on\nthe organization of names. Flat names may require solutions such as those for\ndistributed hash tables, which we discuss in Section 5.2. Structured names can be\nef\ufb01ciently resolved using hierarchical solutions as we discuss in Section 5.3.\nNote 5.2 (More information: Self-certifying names)\nIf a name is being used to refer to an entity, how do we know that we are actually\naccessing the intended entity? In other words, how can we ensure that an entity\nand its name are uniquely bound to each other? In the case of immutable data\nobjects, there is a simple solution: take a hash of the data object and use that hash\nvalue as its name. When the data object is retrieved, the receiving process can\nhash the object and see whether the hash value corresponds to the one it used as\nthe object\u2019s name. This is a simple version of a self-certifying name , which is a\nname that can be checked locally to see if it truly belongs to the entity it refers to.\nWe return to secure naming in Section 9.4.\nIn the following sections we will consider three different classes of naming\nsystems. First, we will take a look at how identi\ufb01ers can be resolved to\naddresses. In this case, we will also see an example where name resolution\nis actually indistinguishable from message routing. After that, we consider\nhuman-friendly names and descriptive names (i.e., entities that are described\nby a collection of names).\n5.2 Flat naming\nAbove, we explained that identi\ufb01ers are convenient to uniquely represent\nentities. In many cases, identi\ufb01ers are simply random bit strings, which we\nconveniently refer to as unstructured, or \ufb02at names. An important property\nof such a name is that it does not contain any information whatsoever on how\nto locate the access point of its associated entity. In the following, we will take\na look at how \ufb02at names can be resolved, or, equivalently, how we can locate\nan entity when given only its identi\ufb01er.\nSimple solutions\nWe \ufb01rst consider two simple solutions for locating an entity: broadcasting and\nforwarding pointers. Both solutions are mainly applicable only to local-area\nnetworks. Nevertheless, in that environment, they often do the job well, mak-\ning their simplicity particularly attractive. However, the use of broadcasting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 241\nKey to the success of this approach is, of course, a form of name-based\nrouting, which is essentially a means for resolving a name to an address where\nthe associated entity is to be found. How that routing takes place depends on\nthe organization of names. Flat names may require solutions such as those for\ndistributed hash tables, which we discuss in Section 5.2. Structured names can be\nef\ufb01ciently resolved using hierarchical solutions as we discuss in Section 5.3.\nNote 5.2 (More information: Self-certifying names)\nIf a name is being used to refer to an entity, how do we know that we are actually\naccessing the intended entity? In other words, how can we ensure that an entity\nand its name are uniquely bound to each other? In the case of immutable data\nobjects, there is a simple solution: take a hash of the data object and use that hash\nvalue as its name. When the data object is retrieved, the receiving process can\nhash the object and see whether the hash value corresponds to the one it used as\nthe object\u2019s name. This is a simple version of a self-certifying name , which is a\nname that can be checked locally to see if it truly belongs to the entity it refers to.\nWe return to secure naming in Section 9.4.\nIn the following sections we will consider three different classes of naming\nsystems. First, we will take a look at how identi\ufb01ers can be resolved to\naddresses. In this case, we will also see an example where name resolution\nis actually indistinguishable from message routing. After that, we consider\nhuman-friendly names and descriptive names (i.e., entities that are described\nby a collection of names).\n5.2 Flat naming\nAbove, we explained that identi\ufb01ers are convenient to uniquely represent\nentities. In many cases, identi\ufb01ers are simply random bit strings, which we\nconveniently refer to as unstructured, or \ufb02at names. An important property\nof such a name is that it does not contain any information whatsoever on how\nto locate the access point of its associated entity. In the following, we will take\na look at how \ufb02at names can be resolved, or, equivalently, how we can locate\nan entity when given only its identi\ufb01er.\nSimple solutions\nWe \ufb01rst consider two simple solutions for locating an entity: broadcasting and\nforwarding pointers. Both solutions are mainly applicable only to local-area\nnetworks. Nevertheless, in that environment, they often do the job well, mak-\ning their simplicity particularly attractive. However, the use of broadcasting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "242 CHAPTER 5. NAMING\nand forwarding pointers imposes scalability problems. Broadcasting or multi-\ncasting is dif\ufb01cult to implement ef\ufb01ciently in large-scale networks whereas\nlong chains of forwarding pointers introduce performance problems and are\nsusceptible to broken links.\nBroadcasting\nConsider a distributed system built on a computer network that offers ef\ufb01cient\nbroadcasting facilities. Typically, such facilities are offered by local-area\nnetworks in which all machines are connected to a single cable or the logical\nequivalent thereof. Also, local-area wireless networks fall into this category.\nLocating an entity in such an environment is simple: a message containing\nthe identi\ufb01er of the entity is broadcast to each machine and each machine is\nrequested to check whether it has that entity. Only the machines that can offer\nan access point for the entity send a reply message containing the address of\nthat access point.\nThis principle is used in the Internet Address Resolution Protocol (ARP )\nto \ufb01nd the data-link address of a machine when given only an IP address [Plum-\nmer, 1982]. In essence, a machine broadcasts a packet on the local network\nasking who is the owner of a given IP address. When the message arrives at\na machine, the receiver checks whether it should listen to the requested IP\naddress. If so, it sends a reply packet containing, for example, its Ethernet\naddress.\nBroadcasting becomes inef\ufb01cient when the network grows. Not only is\nnetwork bandwidth wasted by request messages, but, more seriously, too\nmany hosts may be interrupted by requests they cannot answer. One possible\nsolution is to switch to multicasting, by which only a restricted group of hosts\nreceives the request. For example, Ethernet networks support data-link level\nmulticasting directly in hardware.\nMulticasting can also be used to locate entities in point-to-point networks.\nFor example, the Internet supports network-level multicasting by allowing\nhosts to join a speci\ufb01c multicast group. Such groups are identi\ufb01ed by a\nmulticast address. When a host sends a message to a multicast address, the\nnetwork layer provides a best-effort service to deliver that message to all\ngroup members. Ef\ufb01cient implementations for multicasting in the Internet are\ndiscussed in Deering and Cheriton [1990] and Deering et al. [1996].\nA multicast address can be used as a general location service for multiple\nentities. For example, consider an organization where each employee has his\nor her own mobile computer. When such a computer connects to the locally\navailable network, it is dynamically assigned an IP address. In addition, it\njoins a speci\ufb01c multicast group. When a process wants to locate computer A,\nit sends a \u201cwhere is A?\u201d request to the multicast group. If Ais connected, it\nresponds with its current IP address.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n242 CHAPTER 5. NAMING\nand forwarding pointers imposes scalability problems. Broadcasting or multi-\ncasting is dif\ufb01cult to implement ef\ufb01ciently in large-scale networks whereas\nlong chains of forwarding pointers introduce performance problems and are\nsusceptible to broken links.\nBroadcasting\nConsider a distributed system built on a computer network that offers ef\ufb01cient\nbroadcasting facilities. Typically, such facilities are offered by local-area\nnetworks in which all machines are connected to a single cable or the logical\nequivalent thereof. Also, local-area wireless networks fall into this category.\nLocating an entity in such an environment is simple: a message containing\nthe identi\ufb01er of the entity is broadcast to each machine and each machine is\nrequested to check whether it has that entity. Only the machines that can offer\nan access point for the entity send a reply message containing the address of\nthat access point.\nThis principle is used in the Internet Address Resolution Protocol (ARP )\nto \ufb01nd the data-link address of a machine when given only an IP address [Plum-\nmer, 1982]. In essence, a machine broadcasts a packet on the local network\nasking who is the owner of a given IP address. When the message arrives at\na machine, the receiver checks whether it should listen to the requested IP\naddress. If so, it sends a reply packet containing, for example, its Ethernet\naddress.\nBroadcasting becomes inef\ufb01cient when the network grows. Not only is\nnetwork bandwidth wasted by request messages, but, more seriously, too\nmany hosts may be interrupted by requests they cannot answer. One possible\nsolution is to switch to multicasting, by which only a restricted group of hosts\nreceives the request. For example, Ethernet networks support data-link level\nmulticasting directly in hardware.\nMulticasting can also be used to locate entities in point-to-point networks.\nFor example, the Internet supports network-level multicasting by allowing\nhosts to join a speci\ufb01c multicast group. Such groups are identi\ufb01ed by a\nmulticast address. When a host sends a message to a multicast address, the\nnetwork layer provides a best-effort service to deliver that message to all\ngroup members. Ef\ufb01cient implementations for multicasting in the Internet are\ndiscussed in Deering and Cheriton [1990] and Deering et al. [1996].\nA multicast address can be used as a general location service for multiple\nentities. For example, consider an organization where each employee has his\nor her own mobile computer. When such a computer connects to the locally\navailable network, it is dynamically assigned an IP address. In addition, it\njoins a speci\ufb01c multicast group. When a process wants to locate computer A,\nit sends a \u201cwhere is A?\u201d request to the multicast group. If Ais connected, it\nresponds with its current IP address.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 243\nAnother way to use a multicast address is to associate it with a replicated\nentity, and to use multicasting to locate the nearest replica. When sending\na request to the multicast address, each replica responds with its current\n(normal) IP address. A crude way to select the nearest replica is to choose the\none whose reply comes in \ufb01rst, but as it turns out, selecting a nearest replica\nis generally not that easy.\nForwarding pointers\nAnother popular approach to locating mobile entities is to make use of for-\nwarding pointers [Fowler, 1985]. The principle is simple: when an entity\nmoves from AtoB, it leaves behind in Aa reference to its new location at B.\nThe main advantage of this approach is its simplicity: as soon as an entity has\nbeen located, for example by using a traditional naming service, a client can\nlook up the current address by following the chain of forwarding pointers.\nThere are also drawbacks. First, if no special measures are taken, a chain\nfor a highly mobile entity can become so long that locating that entity is\nprohibitively expensive. Second, all intermediate locations in a chain will have\nto maintain their part of the chain of forwarding pointers as long as needed.\nA third (and related) drawback is the vulnerability to broken links. As soon\nas any forwarding pointer is lost, the entity can no longer be reached. An\nimportant issue is, therefore, to keep chains relatively short, and to ensure\nthat forwarding pointers are robust.\nNote 5.3 (More information: SSP chains)\nTo better understand how forwarding pointers work, consider their use with\nrespect to remote objects: objects that can be accessed by means of a remote\nprocedure call. Following the approach in SSP chains [Shapiro et al., 1992], each\nforwarding pointer is implemented as a (client stub, server stub) pair as shown\nin Figure 5.1 (We note that in the original terminology, a server stub was called\na scion, leading to (stub,scion) pairs, which explains its name.) A server stub\ncontains either a local reference to the actual object or a local reference to a remote\nclient stub for that object.\nWhenever an object moves from address space AtoB, it leaves behind a client\nstub in its place in Aand installs a server stub that refers to the client stub in B.\nAn interesting aspect of this approach is that migration is completely transparent\nto a client. The only thing the client sees of an object is a client stub. How, and to\nwhich location that client stub forwards its invocations, are hidden from the client.\nAlso note that this use of forwarding pointers is not like looking up an address.\nInstead, a client\u2019s request is forwarded along the chain to the actual object.\nNow suppose that process P1in Figure 5.1 passes its reference to process P2.\nReference passing is done by installing a copy p0of client stub pin the address\nspace of process P2. Client stub p0refers to the same server stub as p, so that the\nforwarding invocation mechanism works the same as before.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 243\nAnother way to use a multicast address is to associate it with a replicated\nentity, and to use multicasting to locate the nearest replica. When sending\na request to the multicast address, each replica responds with its current\n(normal) IP address. A crude way to select the nearest replica is to choose the\none whose reply comes in \ufb01rst, but as it turns out, selecting a nearest replica\nis generally not that easy.\nForwarding pointers\nAnother popular approach to locating mobile entities is to make use of for-\nwarding pointers [Fowler, 1985]. The principle is simple: when an entity\nmoves from AtoB, it leaves behind in Aa reference to its new location at B.\nThe main advantage of this approach is its simplicity: as soon as an entity has\nbeen located, for example by using a traditional naming service, a client can\nlook up the current address by following the chain of forwarding pointers.\nThere are also drawbacks. First, if no special measures are taken, a chain\nfor a highly mobile entity can become so long that locating that entity is\nprohibitively expensive. Second, all intermediate locations in a chain will have\nto maintain their part of the chain of forwarding pointers as long as needed.\nA third (and related) drawback is the vulnerability to broken links. As soon\nas any forwarding pointer is lost, the entity can no longer be reached. An\nimportant issue is, therefore, to keep chains relatively short, and to ensure\nthat forwarding pointers are robust.\nNote 5.3 (More information: SSP chains)\nTo better understand how forwarding pointers work, consider their use with\nrespect to remote objects: objects that can be accessed by means of a remote\nprocedure call. Following the approach in SSP chains [Shapiro et al., 1992], each\nforwarding pointer is implemented as a (client stub, server stub) pair as shown\nin Figure 5.1 (We note that in the original terminology, a server stub was called\na scion, leading to (stub,scion) pairs, which explains its name.) A server stub\ncontains either a local reference to the actual object or a local reference to a remote\nclient stub for that object.\nWhenever an object moves from address space AtoB, it leaves behind a client\nstub in its place in Aand installs a server stub that refers to the client stub in B.\nAn interesting aspect of this approach is that migration is completely transparent\nto a client. The only thing the client sees of an object is a client stub. How, and to\nwhich location that client stub forwards its invocations, are hidden from the client.\nAlso note that this use of forwarding pointers is not like looking up an address.\nInstead, a client\u2019s request is forwarded along the chain to the actual object.\nNow suppose that process P1in Figure 5.1 passes its reference to process P2.\nReference passing is done by installing a copy p0of client stub pin the address\nspace of process P2. Client stub p0refers to the same server stub as p, so that the\nforwarding invocation mechanism works the same as before.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "244 CHAPTER 5. NAMING\nFigure 5.1: The principle of forwarding pointers using\n(client stub, server stub) pairs.\nTo shortcut a chain of (client stub, server stub) pairs, an object invocation carries\nthe identi\ufb01cation of the client stub from where that invocation was initiated. A\nclient-stub identi\ufb01cation consists of the client\u2019s transport-level address, combined\nwith a locally generated number to identify that stub. When the invocation\nreaches the object at its current location, a response is sent back to the client stub\nwhere the invocation was initiated (often without going back up the chain). The\ncurrent location is piggybacked with this response, and the client stub adjusts its\ncompanion server stub to the one in the object\u2019s current location. This principle is\nshown in Figure 5.2.\n(a) (b)\nFigure 5.2: Redirecting a forwarding pointer by storing a shortcut in a\nclient stub.\nThere is a trade-off between sending the response directly to the initiating\nclient stub, or along the reverse path of forwarding pointers. In the former case,\ncommunication is faster because fewer processes may need to be passed. On the\nother hand, only the initiating client stub can be adjusted, whereas sending the\nresponse along the reverse path allows adjustment of all intermediate stubs.\nWhen a server stub is no longer referred to by any client, it can be removed.\nThis by itself is strongly related to distributed garbage collection, a generally far\nfrom trivial problem, to which we refer the interested reader to [Abdullahi and\nRingwood, 1998], [Plainfosse and Shapiro, 1995], and [Veiga and Ferreira, 2005].\nProblems arise when a process in a chain of (client stub, server stub) pairs\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n244 CHAPTER 5. NAMING\nFigure 5.1: The principle of forwarding pointers using\n(client stub, server stub) pairs.\nTo shortcut a chain of (client stub, server stub) pairs, an object invocation carries\nthe identi\ufb01cation of the client stub from where that invocation was initiated. A\nclient-stub identi\ufb01cation consists of the client\u2019s transport-level address, combined\nwith a locally generated number to identify that stub. When the invocation\nreaches the object at its current location, a response is sent back to the client stub\nwhere the invocation was initiated (often without going back up the chain). The\ncurrent location is piggybacked with this response, and the client stub adjusts its\ncompanion server stub to the one in the object\u2019s current location. This principle is\nshown in Figure 5.2.\n(a) (b)\nFigure 5.2: Redirecting a forwarding pointer by storing a shortcut in a\nclient stub.\nThere is a trade-off between sending the response directly to the initiating\nclient stub, or along the reverse path of forwarding pointers. In the former case,\ncommunication is faster because fewer processes may need to be passed. On the\nother hand, only the initiating client stub can be adjusted, whereas sending the\nresponse along the reverse path allows adjustment of all intermediate stubs.\nWhen a server stub is no longer referred to by any client, it can be removed.\nThis by itself is strongly related to distributed garbage collection, a generally far\nfrom trivial problem, to which we refer the interested reader to [Abdullahi and\nRingwood, 1998], [Plainfosse and Shapiro, 1995], and [Veiga and Ferreira, 2005].\nProblems arise when a process in a chain of (client stub, server stub) pairs\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 245\ncrashes or becomes otherwise unreachable. Several solutions are possible. One\npossibility, as followed in Emerald [Jul et al., 1988], and in the LII system [Black\nand Artsy, 1990], is to let the machine where an object was created (called the\nobject\u2019s home location ), always keep a reference to its current location. That\nreference is stored and maintained in a fault-tolerant way. When a chain is broken,\nthe object\u2019s home location is asked where the object is now. To allow an object\u2019s\nhome location to change, a traditional naming service can be used to record the\ncurrent home location.\nHome-based approaches\nA popular approach to supporting mobile entities in large-scale networks is\nto introduce a home location , which keeps track of the current location of an\nentity. Special techniques may be applied to safeguard against network or\nprocess failures. In practice, the home location is often chosen to be the place\nwhere an entity was created.\nThe home-based approach is used as a fall-back mechanism for location\nservices based on forwarding pointers. Another example where the home-\nbased approach is followed is in Mobile IP [Johnson et al., 2004], which we\nbrie\ufb02y explained in Note 3.9. Each mobile host uses a \ufb01xed IP address. All\ncommunication to that IP address is initially directed to the mobile host\u2019s\nhome agent . This home agent is located on the local-area network correspond-\ning to the network address contained in the mobile host\u2019s IP address. In the\ncase of IPv6, it is realized as a network-layer component. Whenever the mobile\nhost moves to another network, it requests a temporary address that it can\nuse for communication. This care-of address is registered at the home agent.\nWhen the home agent receives a packet for the mobile host, it looks up the\nhost\u2019s current location. If the host is on the current local network, the packet\nis simply forwarded. Otherwise, it is tunneled to the host\u2019s current location,\nthat is, wrapped as data in an IP packet and sent to the care-of address. At the\nsame time, the sender of the packet is informed of the host\u2019s current location.\nThis principle is shown in Figure 5.3 Note that the IP address is effectively\nused as an identi\ufb01er for the mobile host.\nAn important aspect is that this whole mechanism is largely hidden for\napplications. In other words, the original IP address associated with the\nmobile host can be used by an application without further ado. Client-side\nsoftware that is part of the application-independent communication layer will\nhandle the redirection to the target\u2019s current location. Likewise, at the target\u2019s\nlocation, a message that has been tunneled will be unpacked and handed\nto the application on the mobile host as if it were using its original address.\nIndeed, Mobile IP establishes a high degree of location transparency.\nFigure 5.3 also illustrates a drawback of home-based approaches in large-\nscale networks. To communicate with a mobile entity, a client \ufb01rst has to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 245\ncrashes or becomes otherwise unreachable. Several solutions are possible. One\npossibility, as followed in Emerald [Jul et al., 1988], and in the LII system [Black\nand Artsy, 1990], is to let the machine where an object was created (called the\nobject\u2019s home location ), always keep a reference to its current location. That\nreference is stored and maintained in a fault-tolerant way. When a chain is broken,\nthe object\u2019s home location is asked where the object is now. To allow an object\u2019s\nhome location to change, a traditional naming service can be used to record the\ncurrent home location.\nHome-based approaches\nA popular approach to supporting mobile entities in large-scale networks is\nto introduce a home location , which keeps track of the current location of an\nentity. Special techniques may be applied to safeguard against network or\nprocess failures. In practice, the home location is often chosen to be the place\nwhere an entity was created.\nThe home-based approach is used as a fall-back mechanism for location\nservices based on forwarding pointers. Another example where the home-\nbased approach is followed is in Mobile IP [Johnson et al., 2004], which we\nbrie\ufb02y explained in Note 3.9. Each mobile host uses a \ufb01xed IP address. All\ncommunication to that IP address is initially directed to the mobile host\u2019s\nhome agent . This home agent is located on the local-area network correspond-\ning to the network address contained in the mobile host\u2019s IP address. In the\ncase of IPv6, it is realized as a network-layer component. Whenever the mobile\nhost moves to another network, it requests a temporary address that it can\nuse for communication. This care-of address is registered at the home agent.\nWhen the home agent receives a packet for the mobile host, it looks up the\nhost\u2019s current location. If the host is on the current local network, the packet\nis simply forwarded. Otherwise, it is tunneled to the host\u2019s current location,\nthat is, wrapped as data in an IP packet and sent to the care-of address. At the\nsame time, the sender of the packet is informed of the host\u2019s current location.\nThis principle is shown in Figure 5.3 Note that the IP address is effectively\nused as an identi\ufb01er for the mobile host.\nAn important aspect is that this whole mechanism is largely hidden for\napplications. In other words, the original IP address associated with the\nmobile host can be used by an application without further ado. Client-side\nsoftware that is part of the application-independent communication layer will\nhandle the redirection to the target\u2019s current location. Likewise, at the target\u2019s\nlocation, a message that has been tunneled will be unpacked and handed\nto the application on the mobile host as if it were using its original address.\nIndeed, Mobile IP establishes a high degree of location transparency.\nFigure 5.3 also illustrates a drawback of home-based approaches in large-\nscale networks. To communicate with a mobile entity, a client \ufb01rst has to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "246 CHAPTER 5. NAMING\nFigure 5.3: The principle of Mobile IP .\ncontact the home, which may be at a completely different location than the\nentity itself. The result is an increase in communication latency.\nAnother drawback of the home-based approach is the use of a \ufb01xed home\nlocation. For one thing, it must be ensured that the home location always\nexists. Otherwise, contacting the entity will become impossible. Problems\nare aggravated when a long-lived entity decides to move permanently to a\ncompletely different part of the network than where its home is located. In\nthat case, it would have been better if the home could have moved along with\nthe host.\nA solution to this problem is to register the home at a traditional naming\nservice and to let a client \ufb01rst look up the location of the home. Because the\nhome location can be assumed to be relatively stable, that location can be\neffectively cached after it has been looked up.\nDistributed hash tables\nLet us now take a closer look at how to resolve an identi\ufb01er to the address of\nthe associated entity. We have already mentioned distributed hash tables a\nnumber of times, but have deferred discussion on how they actually work. In\nthis section we correct this situation by \ufb01rst considering the Chord system as\nan easy-to-explain DHT-based system.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n246 CHAPTER 5. NAMING\nFigure 5.3: The principle of Mobile IP .\ncontact the home, which may be at a completely different location than the\nentity itself. The result is an increase in communication latency.\nAnother drawback of the home-based approach is the use of a \ufb01xed home\nlocation. For one thing, it must be ensured that the home location always\nexists. Otherwise, contacting the entity will become impossible. Problems\nare aggravated when a long-lived entity decides to move permanently to a\ncompletely different part of the network than where its home is located. In\nthat case, it would have been better if the home could have moved along with\nthe host.\nA solution to this problem is to register the home at a traditional naming\nservice and to let a client \ufb01rst look up the location of the home. Because the\nhome location can be assumed to be relatively stable, that location can be\neffectively cached after it has been looked up.\nDistributed hash tables\nLet us now take a closer look at how to resolve an identi\ufb01er to the address of\nthe associated entity. We have already mentioned distributed hash tables a\nnumber of times, but have deferred discussion on how they actually work. In\nthis section we correct this situation by \ufb01rst considering the Chord system as\nan easy-to-explain DHT-based system.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 247\nGeneral mechanism\nMany DHT-based systems have been developed in the past decade, with the\nChord system [Stoica et al., 2003] being a typical representative. Chord uses\nanm-bit identi\ufb01er space to assign randomly chosen identi\ufb01ers to nodes as\nwell as keys to speci\ufb01c entities. The latter can be virtually anything: \ufb01les,\nprocesses, etc. The number mof bits is usually 128 or 160, depending on\nwhich hash function is used. An entity with key kfalls under the jurisdiction\nof the node with the smallest identi\ufb01er id\u0015k. This node is referred to as\nthesuccessor ofkand denoted as succ(k). To keep our notation simple and\nconsistent, in the following we refer to a node with identi\ufb01er pas node p.\nThe main issue in DHT-based systems is to ef\ufb01ciently resolve a key kto\nthe address of succ(k). An obvious nonscalable approach is to let each node\npkeep track of the successor succ(p+1)as well as its predecessor pred(p).\nIn that case, whenever a node preceives a request to resolve key k, it will\nsimply forward the request to one of its two neighbors\u2013whichever one is\nappropriate\u2013unless pred(p)<k\u0014pin which case node pshould return its\nown address to the process that initiated the resolution of key k.\nInstead of this linear approach toward key lookup, each Chord node\nmaintains a \ufb01nger table containing s\u0014mentries. If FTpdenotes the \ufb01nger\ntable of node p, then\nFTp[i] =succ(p+2i\u00001)\nPut in other words, the i-th entry points to the \ufb01rst node succeeding pby at\nleast 2i\u00001. Note that these references are actually shortcuts to existing nodes\nin the identi\ufb01er space, where the short-cutted distance from node pincreases\nexponentially as the index in the \ufb01nger table increases. To look up a key k,\nnode pwill then immediately forward the request to node qwith index jin\np\u2019s \ufb01nger table where:\nq=FTp[j]\u0014k<FTp[j+1]\norq=FTp[1]when p<k<FTp[1]. (For clarity, we ignore modulo arith-\nmetic.) Note that when the \ufb01nger-table size sis equal to 1, a Chord lookup\ncorresponds to naively traversing the ring linearly as we just discussed.\nTo illustrate this lookup, consider resolving k=26from node 1as shown in\nFigure 5.4. First, node 1will look up k=26in its \ufb01nger table to discover that\nthis value is larger than FT1[5], meaning that the request will be forwarded\nto node 18=FT1[5]. Node 18, in turn, will select node 20, asFT18[2]\u0014k<\nFT18[3]. Finally, the request is forwarded from node 20to node 21and from\nthere to node 28, which is responsible for k=26. At that point, the address\nof node 28is returned to node 1and the key has been resolved. For similar\nreasons, when node 28is requested to resolve the key k=12, a request will\nbe routed as shown by the dashed line in Figure 5.4. It can be shown that a\nlookup will generally require O(log(N))steps, with Nbeing the number of\nnodes in the system.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 247\nGeneral mechanism\nMany DHT-based systems have been developed in the past decade, with the\nChord system [Stoica et al., 2003] being a typical representative. Chord uses\nanm-bit identi\ufb01er space to assign randomly chosen identi\ufb01ers to nodes as\nwell as keys to speci\ufb01c entities. The latter can be virtually anything: \ufb01les,\nprocesses, etc. The number mof bits is usually 128 or 160, depending on\nwhich hash function is used. An entity with key kfalls under the jurisdiction\nof the node with the smallest identi\ufb01er id\u0015k. This node is referred to as\nthesuccessor ofkand denoted as succ(k). To keep our notation simple and\nconsistent, in the following we refer to a node with identi\ufb01er pas node p.\nThe main issue in DHT-based systems is to ef\ufb01ciently resolve a key kto\nthe address of succ(k). An obvious nonscalable approach is to let each node\npkeep track of the successor succ(p+1)as well as its predecessor pred(p).\nIn that case, whenever a node preceives a request to resolve key k, it will\nsimply forward the request to one of its two neighbors\u2013whichever one is\nappropriate\u2013unless pred(p)<k\u0014pin which case node pshould return its\nown address to the process that initiated the resolution of key k.\nInstead of this linear approach toward key lookup, each Chord node\nmaintains a \ufb01nger table containing s\u0014mentries. If FTpdenotes the \ufb01nger\ntable of node p, then\nFTp[i] =succ(p+2i\u00001)\nPut in other words, the i-th entry points to the \ufb01rst node succeeding pby at\nleast 2i\u00001. Note that these references are actually shortcuts to existing nodes\nin the identi\ufb01er space, where the short-cutted distance from node pincreases\nexponentially as the index in the \ufb01nger table increases. To look up a key k,\nnode pwill then immediately forward the request to node qwith index jin\np\u2019s \ufb01nger table where:\nq=FTp[j]\u0014k<FTp[j+1]\norq=FTp[1]when p<k<FTp[1]. (For clarity, we ignore modulo arith-\nmetic.) Note that when the \ufb01nger-table size sis equal to 1, a Chord lookup\ncorresponds to naively traversing the ring linearly as we just discussed.\nTo illustrate this lookup, consider resolving k=26from node 1as shown in\nFigure 5.4. First, node 1will look up k=26in its \ufb01nger table to discover that\nthis value is larger than FT1[5], meaning that the request will be forwarded\nto node 18=FT1[5]. Node 18, in turn, will select node 20, asFT18[2]\u0014k<\nFT18[3]. Finally, the request is forwarded from node 20to node 21and from\nthere to node 28, which is responsible for k=26. At that point, the address\nof node 28is returned to node 1and the key has been resolved. For similar\nreasons, when node 28is requested to resolve the key k=12, a request will\nbe routed as shown by the dashed line in Figure 5.4. It can be shown that a\nlookup will generally require O(log(N))steps, with Nbeing the number of\nnodes in the system.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "248 CHAPTER 5. NAMING\nFigure 5.4: Resolving key 26from node 1and key 12from node 28ina Chord\nsystem.\nIn large distributed systems the collection of participating nodes can be\nexpected to change all the time. Not only will nodes join and leave voluntarily,\nwe also need to consider the case of nodes failing (and thus effectively leaving\nthe system), to later recover again (at which point they rejoin).\nJoining a DHT-based system such as Chord is relatively simple. Suppose\nnode pwants to join. It simply contacts an arbitrary node in the existing\nsystem and requests a lookup for succ(p+1). Once this node has been\nidenti\ufb01ed, pcan insert itself into the ring. Likewise, leaving can be just as\nsimple. Note that nodes also keep track of their predecessor.\nObviously, the complexity comes from keeping the \ufb01nger tables up-to-\ndate. Most important is that for every node q,FTq[1]is correct as this entry\nrefers to the next node in the ring, that is, the successor of q+1. In order to\nachieve this goal, each node qregularly runs a simple procedure that contacts\nsucc(q+1)and requests to return pred(succ(q+1)). Ifq=pred(succ(q+1))\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n248 CHAPTER 5. NAMING\nFigure 5.4: Resolving key 26from node 1and key 12from node 28ina Chord\nsystem.\nIn large distributed systems the collection of participating nodes can be\nexpected to change all the time. Not only will nodes join and leave voluntarily,\nwe also need to consider the case of nodes failing (and thus effectively leaving\nthe system), to later recover again (at which point they rejoin).\nJoining a DHT-based system such as Chord is relatively simple. Suppose\nnode pwants to join. It simply contacts an arbitrary node in the existing\nsystem and requests a lookup for succ(p+1). Once this node has been\nidenti\ufb01ed, pcan insert itself into the ring. Likewise, leaving can be just as\nsimple. Note that nodes also keep track of their predecessor.\nObviously, the complexity comes from keeping the \ufb01nger tables up-to-\ndate. Most important is that for every node q,FTq[1]is correct as this entry\nrefers to the next node in the ring, that is, the successor of q+1. In order to\nachieve this goal, each node qregularly runs a simple procedure that contacts\nsucc(q+1)and requests to return pred(succ(q+1)). Ifq=pred(succ(q+1))\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 249\nthen qknows its information is consistent with that of its successor. Otherwise,\nifq\u2019s successor has updated its predecessor, then apparently a new node phad\nentered the system, with q<p\u0014succ(q+1), so that qwill adjust FTq[1]top.\nAt that point, it will also check whether phas recorded qas its predecessor. If\nnot, another adjustment of FTq[1]is needed.\nIn a similar way, to update a \ufb01nger table, node qsimply needs to \ufb01nd the\nsuccessor for k=q+2i\u00001for each entry i. Again, this can be done by issuing\na request to resolve succ(k). In Chord, such requests are issued regularly by\nmeans of a background process.\nLikewise, each node qwill regularly check whether its predecessor is alive.\nIf the predecessor has failed, the only thing that qcan do is record the fact by\nsetting pred(q)to \u201cunknown.\u201d On the other hand, when node qis updating\nits link to the next known node in the ring, and \ufb01nds that the predecessor of\nsucc(q+1)has been set to \u201cunknown,\u201d it will simply notify succ(q+1)that it\nsuspects it to be the predecessor. By and large, these simple procedures ensure\nthat a Chord system is generally consistent, only perhaps with exception of a\nfew nodes. The details can be found in [Stoica et al., 2003].\nNote 5.4 (Advanced: Chord in Python)\n1class ChordNode:\n2 deffinger(self, i):\n3 succ = (self.nodeID + pow(2, i-1)) % self.MAXPROC # succ(p+2^(i-1))\n4 lwbi = self.nodeSet.index(self.nodeID) # self in nodeset\n5 upbi = (lwbi + 1) % len(self.nodeSet) # next neighbor\n6 forkin range (len(self.nodeSet)): # process segments\n7 ifself.inbetween(succ, self.nodeSet[lwbi]+1, self.nodeSet[upbi]+1):\n8 return self.nodeSet[upbi] # found successor\n9 (lwbi,upbi) = (upbi, (upbi+1) % len(self.nodeSet)) # next segment\n10\n11 defrecomputeFingerTable(self):\n12 self.FT[0] = self.nodeSet[self.nodeSet.index(self.nodeID)-1] # Pred.\n13 self.FT[1:] = [self.finger(i) foriin range (1,self.nBits+1)] # Succ.\n14\n15 deflocalSuccNode(self, key):\n16 ifself.inbetween(key, self.FT[0]+1, self.nodeID+1): # in (FT[0],self]\n17 return self.nodeID # responsible node\n18 elif self.inbetween(key, self.nodeID+1, self.FT[1]): # in (self,FT[1]]\n19 return self.FT[1] # succ. responsible\n20 foriin range (1, self.nBits+1): # rest of FT\n21 ifself.inbetween(key, self.FT[i], self.FT[(i+1) % self.nBits]):\n22 return self.FT[i] # in [FT[i],FT[i+1])\nFigure 5.5: The essence of a Chord node expressed in Python.\nCoding Chord in Python is remarkably simple. Again, omitting many of\nthe nonessential coding details, the core of the behavior of a Chord node can\nbe described as shown in Figure 5.5. The function finger(i) computes succ(i)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 249\nthen qknows its information is consistent with that of its successor. Otherwise,\nifq\u2019s successor has updated its predecessor, then apparently a new node phad\nentered the system, with q<p\u0014succ(q+1), so that qwill adjust FTq[1]top.\nAt that point, it will also check whether phas recorded qas its predecessor. If\nnot, another adjustment of FTq[1]is needed.\nIn a similar way, to update a \ufb01nger table, node qsimply needs to \ufb01nd the\nsuccessor for k=q+2i\u00001for each entry i. Again, this can be done by issuing\na request to resolve succ(k). In Chord, such requests are issued regularly by\nmeans of a background process.\nLikewise, each node qwill regularly check whether its predecessor is alive.\nIf the predecessor has failed, the only thing that qcan do is record the fact by\nsetting pred(q)to \u201cunknown.\u201d On the other hand, when node qis updating\nits link to the next known node in the ring, and \ufb01nds that the predecessor of\nsucc(q+1)has been set to \u201cunknown,\u201d it will simply notify succ(q+1)that it\nsuspects it to be the predecessor. By and large, these simple procedures ensure\nthat a Chord system is generally consistent, only perhaps with exception of a\nfew nodes. The details can be found in [Stoica et al., 2003].\nNote 5.4 (Advanced: Chord in Python)\n1class ChordNode:\n2 deffinger(self, i):\n3 succ = (self.nodeID + pow(2, i-1)) % self.MAXPROC # succ(p+2^(i-1))\n4 lwbi = self.nodeSet.index(self.nodeID) # self in nodeset\n5 upbi = (lwbi + 1) % len(self.nodeSet) # next neighbor\n6 forkin range (len(self.nodeSet)): # process segments\n7 ifself.inbetween(succ, self.nodeSet[lwbi]+1, self.nodeSet[upbi]+1):\n8 return self.nodeSet[upbi] # found successor\n9 (lwbi,upbi) = (upbi, (upbi+1) % len(self.nodeSet)) # next segment\n10\n11 defrecomputeFingerTable(self):\n12 self.FT[0] = self.nodeSet[self.nodeSet.index(self.nodeID)-1] # Pred.\n13 self.FT[1:] = [self.finger(i) foriin range (1,self.nBits+1)] # Succ.\n14\n15 deflocalSuccNode(self, key):\n16 ifself.inbetween(key, self.FT[0]+1, self.nodeID+1): # in (FT[0],self]\n17 return self.nodeID # responsible node\n18 elif self.inbetween(key, self.nodeID+1, self.FT[1]): # in (self,FT[1]]\n19 return self.FT[1] # succ. responsible\n20 foriin range (1, self.nBits+1): # rest of FT\n21 ifself.inbetween(key, self.FT[i], self.FT[(i+1) % self.nBits]):\n22 return self.FT[i] # in [FT[i],FT[i+1])\nFigure 5.5: The essence of a Chord node expressed in Python.\nCoding Chord in Python is remarkably simple. Again, omitting many of\nthe nonessential coding details, the core of the behavior of a Chord node can\nbe described as shown in Figure 5.5. The function finger(i) computes succ(i)\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "250 CHAPTER 5. NAMING\nfor the given node. All nodes known to a speci\ufb01c Chord node are collected in\na local set nodeSet , which is sorted by node identi\ufb01er. The node \ufb01rst looks up\nits own position in this set, and that of its right-hand neighbor. The operation\ninbetween(k,l,u) computes if k2[l,u), taking modulo arithmetic into account.\nComputing inbetween(k,l+1,u+1) is therefore the same as testing whether k2(l,\nu]. We thus see that finger(i) returns the largest existing node identi\ufb01er less or\nequal to i.\nEvery time a node learns about a new node in the system (or discovers that\none has left), it simply adjusts the local nodeSet and recomputes its \ufb01nger table\nby calling recomputeFingerTable . The \ufb01nger table itself is implemented as a\nlocal table FT, with FT[0] pointing to the node\u2019s predecessor. nBits indicates the\nnumber of bits used for node identi\ufb01ers and keys.\nThe core of what a node does during a lookup is encoded in localSuccNode(k) .\nWhen handed a key k, it will either return itself, its immediate successor FT[1] ,\nor go through the \ufb01nger table to search the entry satisfying FT[i]\u0014k<FT[i+1] .\nThe code does not show what is done with the returned value (which is a node\nidenti\ufb01er), but typically in an iterative scheme , the referenced node will be contacted\nto continue looking up k, unless the node had returned itself as the one being\nresponsible for k. In a recursive scheme , the node itself will contact the referenced\nnode.\nNote 5.5 (Advanced: Exploiting network proximity)\nOne of the potential problems with systems such as Chord is that requests may\nbe routed erratically across the Internet. For example, assume that node 1in\nFigure 5.5 is placed in Amsterdam, The Netherlands; node 18in San Diego,\nCalifornia; node 20in Amsterdam again; and node 21in San Diego. The result\nof resolving key 26will then incur three wide-area message transfers which\narguably could have been reduced to at most one. To minimize these pathological\ncases, designing a DHT-based system requires taking the underlying network into\naccount.\nCastro et al. [2002a] distinguish three different ways for making a DHT-\nbased system aware of the underlying network. In the case of topology-based\nassignment of node identi\ufb01ers the idea is to assign identi\ufb01ers such that two\nnearby nodes will have identi\ufb01ers that are also close to each other. It is not\ndif\ufb01cult to imagine that this approach may impose severe problems in the case of\nrelatively simple systems such as Chord. In the case where node identi\ufb01ers are\nsampled from a one-dimensional space, mapping a logical ring to the Internet is\nfar from trivial. Moreover, such a mapping can easily expose correlated failures:\nnodes on the same enterprise network will have identi\ufb01ers from a relatively small\ninterval. When that network becomes unreachable, we suddenly have a gap in\nthe otherwise uniform distribution of identi\ufb01ers.\nWith proximity routing , nodes maintain a list of alternatives to forward a\nrequest to. For example, instead of having only a single successor, each node in\nChord could equally well keep track of rsuccessors. In fact, this redundancy can\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n250 CHAPTER 5. NAMING\nfor the given node. All nodes known to a speci\ufb01c Chord node are collected in\na local set nodeSet , which is sorted by node identi\ufb01er. The node \ufb01rst looks up\nits own position in this set, and that of its right-hand neighbor. The operation\ninbetween(k,l,u) computes if k2[l,u), taking modulo arithmetic into account.\nComputing inbetween(k,l+1,u+1) is therefore the same as testing whether k2(l,\nu]. We thus see that finger(i) returns the largest existing node identi\ufb01er less or\nequal to i.\nEvery time a node learns about a new node in the system (or discovers that\none has left), it simply adjusts the local nodeSet and recomputes its \ufb01nger table\nby calling recomputeFingerTable . The \ufb01nger table itself is implemented as a\nlocal table FT, with FT[0] pointing to the node\u2019s predecessor. nBits indicates the\nnumber of bits used for node identi\ufb01ers and keys.\nThe core of what a node does during a lookup is encoded in localSuccNode(k) .\nWhen handed a key k, it will either return itself, its immediate successor FT[1] ,\nor go through the \ufb01nger table to search the entry satisfying FT[i]\u0014k<FT[i+1] .\nThe code does not show what is done with the returned value (which is a node\nidenti\ufb01er), but typically in an iterative scheme , the referenced node will be contacted\nto continue looking up k, unless the node had returned itself as the one being\nresponsible for k. In a recursive scheme , the node itself will contact the referenced\nnode.\nNote 5.5 (Advanced: Exploiting network proximity)\nOne of the potential problems with systems such as Chord is that requests may\nbe routed erratically across the Internet. For example, assume that node 1in\nFigure 5.5 is placed in Amsterdam, The Netherlands; node 18in San Diego,\nCalifornia; node 20in Amsterdam again; and node 21in San Diego. The result\nof resolving key 26will then incur three wide-area message transfers which\narguably could have been reduced to at most one. To minimize these pathological\ncases, designing a DHT-based system requires taking the underlying network into\naccount.\nCastro et al. [2002a] distinguish three different ways for making a DHT-\nbased system aware of the underlying network. In the case of topology-based\nassignment of node identi\ufb01ers the idea is to assign identi\ufb01ers such that two\nnearby nodes will have identi\ufb01ers that are also close to each other. It is not\ndif\ufb01cult to imagine that this approach may impose severe problems in the case of\nrelatively simple systems such as Chord. In the case where node identi\ufb01ers are\nsampled from a one-dimensional space, mapping a logical ring to the Internet is\nfar from trivial. Moreover, such a mapping can easily expose correlated failures:\nnodes on the same enterprise network will have identi\ufb01ers from a relatively small\ninterval. When that network becomes unreachable, we suddenly have a gap in\nthe otherwise uniform distribution of identi\ufb01ers.\nWith proximity routing , nodes maintain a list of alternatives to forward a\nrequest to. For example, instead of having only a single successor, each node in\nChord could equally well keep track of rsuccessors. In fact, this redundancy can\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 251\nbe applied for every entry in a \ufb01nger table. For node p,FTp[i]normally points to\nthe \ufb01rst node in the range [p+2i\u00001,p+2i\u00001]. Whenever it needs to look up key\nk, it tries to prevent \u201covershooting\u201d by passing the request to a node qwith k<q\nwithout knowing for sure if there is a node q0with k\u0014q0<q. For this reason, p\npasses kto the node known to pwith the largest identi\ufb01er smaller or equal to k.\nHowever, there is no reason why pcannot keep track of rnodes in range\n[p+2i\u00001,p+2i\u00001]: each node qin this range can be used to route a lookup\nrequest for a key kas long as q\u0014k. In that case, when choosing to forward a\nlookup request, a node can pick one of the rsuccessors that is closest to itself while\nmaking sure not to \u201covershoot.\u201d An additional advantage of having multiple\nsuccessors for every table entry is that node failures need not immediately lead to\nfailures of lookups, as multiple routes can be explored.\nFinally, in proximity neighbor selection the idea is to optimize routing tables\nsuch that the nearest node is selected as neighbor. This selection works only when\nthere are more nodes to choose from. In Chord, this is normally not the case.\nHowever, in other protocols such as Pastry [Rowstron and Druschel, 2001], when\na node joins it receives information about the current overlay from multiple other\nnodes. This information is used by the new node to construct a routing table.\nObviously, when there are alternative nodes to choose from, proximity neighbor\nselection will allow the joining node to choose the best one.\nNote that it may not be that easy to draw a line between proximity routing\nand proximity neighbor selection. In fact, when Chord is modi\ufb01ed to include\nrsuccessors for each \ufb01nger table entry, proximity neighbor selection resorts to\nidentifying the closest rneighbors, which comes very close to proximity routing\nas we just explained [Dabek et al., 2004b].\nHierarchical approaches\nWe now discuss a general approach to a hierarchical location scheme, includ-\ning a number of optimizations. The approach we present is based on the\nGlobe location service [van Steen et al., 1998]. A detailed description can be\nfound in [Ballintijn, 2003]. This is a general-purpose location service that is\nrepresentative of many hierarchical location services proposed for what are\ncalled Personal Communication Systems, of which a general overview can be\nfound in Pitoura and Samaras [2001].\nIn a hierarchical scheme, a network is divided into a collection of domains .\nThere is a single top-level domain that spans the entire network. Each domain\ncan be subdivided into multiple, smaller subdomains. A lowest-level domain,\ncalled a leaf domain , typically corresponds to a local-area network in a\ncomputer network or a cell in a mobile telephone network. The general\nassumption is that within a smaller domain the average time it takes to\ntransfer a message from one node to another is less than in a large domain.\nEach domain Dhas an associated directory node dir(D)that keeps track\nof the entities in that domain. This leads to a tree of directory nodes. The\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 251\nbe applied for every entry in a \ufb01nger table. For node p,FTp[i]normally points to\nthe \ufb01rst node in the range [p+2i\u00001,p+2i\u00001]. Whenever it needs to look up key\nk, it tries to prevent \u201covershooting\u201d by passing the request to a node qwith k<q\nwithout knowing for sure if there is a node q0with k\u0014q0<q. For this reason, p\npasses kto the node known to pwith the largest identi\ufb01er smaller or equal to k.\nHowever, there is no reason why pcannot keep track of rnodes in range\n[p+2i\u00001,p+2i\u00001]: each node qin this range can be used to route a lookup\nrequest for a key kas long as q\u0014k. In that case, when choosing to forward a\nlookup request, a node can pick one of the rsuccessors that is closest to itself while\nmaking sure not to \u201covershoot.\u201d An additional advantage of having multiple\nsuccessors for every table entry is that node failures need not immediately lead to\nfailures of lookups, as multiple routes can be explored.\nFinally, in proximity neighbor selection the idea is to optimize routing tables\nsuch that the nearest node is selected as neighbor. This selection works only when\nthere are more nodes to choose from. In Chord, this is normally not the case.\nHowever, in other protocols such as Pastry [Rowstron and Druschel, 2001], when\na node joins it receives information about the current overlay from multiple other\nnodes. This information is used by the new node to construct a routing table.\nObviously, when there are alternative nodes to choose from, proximity neighbor\nselection will allow the joining node to choose the best one.\nNote that it may not be that easy to draw a line between proximity routing\nand proximity neighbor selection. In fact, when Chord is modi\ufb01ed to include\nrsuccessors for each \ufb01nger table entry, proximity neighbor selection resorts to\nidentifying the closest rneighbors, which comes very close to proximity routing\nas we just explained [Dabek et al., 2004b].\nHierarchical approaches\nWe now discuss a general approach to a hierarchical location scheme, includ-\ning a number of optimizations. The approach we present is based on the\nGlobe location service [van Steen et al., 1998]. A detailed description can be\nfound in [Ballintijn, 2003]. This is a general-purpose location service that is\nrepresentative of many hierarchical location services proposed for what are\ncalled Personal Communication Systems, of which a general overview can be\nfound in Pitoura and Samaras [2001].\nIn a hierarchical scheme, a network is divided into a collection of domains .\nThere is a single top-level domain that spans the entire network. Each domain\ncan be subdivided into multiple, smaller subdomains. A lowest-level domain,\ncalled a leaf domain , typically corresponds to a local-area network in a\ncomputer network or a cell in a mobile telephone network. The general\nassumption is that within a smaller domain the average time it takes to\ntransfer a message from one node to another is less than in a large domain.\nEach domain Dhas an associated directory node dir(D)that keeps track\nof the entities in that domain. This leads to a tree of directory nodes. The\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "252 CHAPTER 5. NAMING\ndirectory node of the top-level domain, called the root (directory) node ,\nknows about all entities. This general organization of a network into domains\nand directory nodes is illustrated in Figure 5.6\nFigure 5.6: Hierarchical organization of a location service into domains, each\nhaving an associated directory node.\nTo keep track of the whereabouts of an entity, each entity currently located\nin a domain Dis represented by a location record in the directory node\ndir(D). A location record for entity Ein the directory node Nfor a leaf\ndomain Dcontains the entity\u2019s current address in that domain. In contrast,\nthe directory node N0for the next higher-level domain D0that contains D,\nwill have a location record for Econtaining only a pointer to N. Likewise,\nthe parent node of N0will store a location record for Econtaining only a\npointer to N0. Consequently, the root node will have a location record for each\nentity, where each location record stores a pointer to the directory node of the\nnext lower-level subdomain where that record\u2019s associated entity is currently\nlocated.\nAn entity may have multiple addresses, for example if it is replicated.\nIf an entity has an address in leaf domain D1and D2respectively, then the\ndirectory node of the smallest domain containing both D1and D2, will have\ntwo pointers, one for each subdomain containing an address. This leads to\nthe general organization of the tree as shown in Figure 5.7.\nLet us now consider how a lookup operation proceeds in such a hierarchi-\ncal location service. As is shown in Figure 5.8, a client wishing to locate an\nentity E, issues a lookup request to the directory node of the leaf domain Din\nwhich the client resides. If the directory node does not store a location record\nfor the entity, then the entity is currently not located in D. Consequently, the\nnode forwards the request to its parent. Note that the parent node represents\na larger domain than its child. If the parent also has no location record for E,\nthe lookup request is forwarded to a next level higher, and so on.\nAs soon as the request reaches a directory node Mthat stores a location\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n252 CHAPTER 5. NAMING\ndirectory node of the top-level domain, called the root (directory) node ,\nknows about all entities. This general organization of a network into domains\nand directory nodes is illustrated in Figure 5.6\nFigure 5.6: Hierarchical organization of a location service into domains, each\nhaving an associated directory node.\nTo keep track of the whereabouts of an entity, each entity currently located\nin a domain Dis represented by a location record in the directory node\ndir(D). A location record for entity Ein the directory node Nfor a leaf\ndomain Dcontains the entity\u2019s current address in that domain. In contrast,\nthe directory node N0for the next higher-level domain D0that contains D,\nwill have a location record for Econtaining only a pointer to N. Likewise,\nthe parent node of N0will store a location record for Econtaining only a\npointer to N0. Consequently, the root node will have a location record for each\nentity, where each location record stores a pointer to the directory node of the\nnext lower-level subdomain where that record\u2019s associated entity is currently\nlocated.\nAn entity may have multiple addresses, for example if it is replicated.\nIf an entity has an address in leaf domain D1and D2respectively, then the\ndirectory node of the smallest domain containing both D1and D2, will have\ntwo pointers, one for each subdomain containing an address. This leads to\nthe general organization of the tree as shown in Figure 5.7.\nLet us now consider how a lookup operation proceeds in such a hierarchi-\ncal location service. As is shown in Figure 5.8, a client wishing to locate an\nentity E, issues a lookup request to the directory node of the leaf domain Din\nwhich the client resides. If the directory node does not store a location record\nfor the entity, then the entity is currently not located in D. Consequently, the\nnode forwards the request to its parent. Note that the parent node represents\na larger domain than its child. If the parent also has no location record for E,\nthe lookup request is forwarded to a next level higher, and so on.\nAs soon as the request reaches a directory node Mthat stores a location\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 253\nFigure 5.7: An example of storing information of an entity having two ad-\ndresses in different leaf domains.\nFigure 5.8: Looking up a location in a hierarchically organized location service.\nrecord for entity E, we know that Eis somewhere in the domain dom(M)\nrepresented by node M. In Figure 5.8, Mis shown to store a location record\ncontaining a pointer to one of its subdomains. The lookup request is then\nforwarded to the directory node of that subdomain, which in turn forwards\nit further down the tree, until the request \ufb01nally reaches a leaf node. The\nlocation record stored in the leaf node will contain the address of Ein that\nleaf domain. This address can then be returned to the client that initially\nrequested the lookup to take place.\nAn important observation with respect to hierarchical location services is\nthat the lookup operation exploits locality. In principle, the entity is searched\nin a gradually increasing ring centered around the requesting client. The\nsearch area is expanded each time the lookup request is forwarded to a next\nhigher-level directory node. In the worst case, the search continues until the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 253\nFigure 5.7: An example of storing information of an entity having two ad-\ndresses in different leaf domains.\nFigure 5.8: Looking up a location in a hierarchically organized location service.\nrecord for entity E, we know that Eis somewhere in the domain dom(M)\nrepresented by node M. In Figure 5.8, Mis shown to store a location record\ncontaining a pointer to one of its subdomains. The lookup request is then\nforwarded to the directory node of that subdomain, which in turn forwards\nit further down the tree, until the request \ufb01nally reaches a leaf node. The\nlocation record stored in the leaf node will contain the address of Ein that\nleaf domain. This address can then be returned to the client that initially\nrequested the lookup to take place.\nAn important observation with respect to hierarchical location services is\nthat the lookup operation exploits locality. In principle, the entity is searched\nin a gradually increasing ring centered around the requesting client. The\nsearch area is expanded each time the lookup request is forwarded to a next\nhigher-level directory node. In the worst case, the search continues until the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "254 CHAPTER 5. NAMING\nrequest reaches the root node. Because the root node has a location record\nfor each entity, the request can then simply be forwarded along a downward\npath of pointers to one of the leaf nodes.\nUpdate operations exploit locality in a similar fashion, as shown in Fig-\nure 5.9. Consider an entity Ethat has created a replica in leaf domain Dfor\nwhich it needs to insert its address. The insertion is initiated at the leaf node\ndir(D)ofDwhich immediately forwards the insert request to its parent. The\nparent will forward the insert request as well, until it reaches a directory node\nMthat already stores a location record for E.\nNode Mwill then store a pointer in the location record for E, referring to\nthe child node from where the insert request was forwarded. At that point,\nthe child node creates a location record for E, containing a pointer to the next\nlower-level node from where the request came. This process continues until\nwe reach the leaf node from which the insert was initiated. The leaf node,\n\ufb01nally, creates a record with the entity\u2019s address in the associated leaf domain.\nInserting an address as just described leads to installing the chain of\npointers in a top-down fashion starting at the lowest-level directory node\nthat has a location record for entity E. An alternative is to create a location\nrecord before passing the insert request to the parent node. In other words,\nthe chain of pointers is constructed from the bottom up. The advantage of\nthe latter is that an address becomes available for lookups as soon as possible.\nConsequently, if a parent node is temporarily unreachable, the address can\nstill be looked up within the domain represented by the current node.\nA delete operation is analogous to an insert operation. When an address\nfor entity Ein leaf domain Dneeds to be removed, directory node dir(D)\nis requested to remove that address from its location record for E. If that\nlocation record becomes empty, that is, it contains no other addresses for Ein\nD, the record can be removed. In that case, the parent node of dir(D)wants\nto remove its pointer to dir(D). If the location record for Eat the parent now\nalso becomes empty, that record should be removed as well and the next\nhigher-level directory node should be informed. Again, this process continues\nuntil a pointer is removed from a location record that remains nonempty\nafterward or until the root is reached.\nNote 5.6 (Advanced: Scalability issues)\nOne question that immediately comes to mind is whether the hierarchical ap-\nproach just described can actually scale. A seemingly obvious design \ufb02aw, is\nthat the root node needs to keep track of allidenti\ufb01ers. However, it is important\nto make a distinction between a logical design and its physical implementation.\nLet us make this distinction here and see how we can actually come to a highly\nscalable implementation of a hierarchical location service.\nTo this end, we assume that each entity is assigned a unique identi\ufb01er uniform\nat random from a large space of m-bit identi\ufb01ers, just as in Chord. Furthermore,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n254 CHAPTER 5. NAMING\nrequest reaches the root node. Because the root node has a location record\nfor each entity, the request can then simply be forwarded along a downward\npath of pointers to one of the leaf nodes.\nUpdate operations exploit locality in a similar fashion, as shown in Fig-\nure 5.9. Consider an entity Ethat has created a replica in leaf domain Dfor\nwhich it needs to insert its address. The insertion is initiated at the leaf node\ndir(D)ofDwhich immediately forwards the insert request to its parent. The\nparent will forward the insert request as well, until it reaches a directory node\nMthat already stores a location record for E.\nNode Mwill then store a pointer in the location record for E, referring to\nthe child node from where the insert request was forwarded. At that point,\nthe child node creates a location record for E, containing a pointer to the next\nlower-level node from where the request came. This process continues until\nwe reach the leaf node from which the insert was initiated. The leaf node,\n\ufb01nally, creates a record with the entity\u2019s address in the associated leaf domain.\nInserting an address as just described leads to installing the chain of\npointers in a top-down fashion starting at the lowest-level directory node\nthat has a location record for entity E. An alternative is to create a location\nrecord before passing the insert request to the parent node. In other words,\nthe chain of pointers is constructed from the bottom up. The advantage of\nthe latter is that an address becomes available for lookups as soon as possible.\nConsequently, if a parent node is temporarily unreachable, the address can\nstill be looked up within the domain represented by the current node.\nA delete operation is analogous to an insert operation. When an address\nfor entity Ein leaf domain Dneeds to be removed, directory node dir(D)\nis requested to remove that address from its location record for E. If that\nlocation record becomes empty, that is, it contains no other addresses for Ein\nD, the record can be removed. In that case, the parent node of dir(D)wants\nto remove its pointer to dir(D). If the location record for Eat the parent now\nalso becomes empty, that record should be removed as well and the next\nhigher-level directory node should be informed. Again, this process continues\nuntil a pointer is removed from a location record that remains nonempty\nafterward or until the root is reached.\nNote 5.6 (Advanced: Scalability issues)\nOne question that immediately comes to mind is whether the hierarchical ap-\nproach just described can actually scale. A seemingly obvious design \ufb02aw, is\nthat the root node needs to keep track of allidenti\ufb01ers. However, it is important\nto make a distinction between a logical design and its physical implementation.\nLet us make this distinction here and see how we can actually come to a highly\nscalable implementation of a hierarchical location service.\nTo this end, we assume that each entity is assigned a unique identi\ufb01er uniform\nat random from a large space of m-bit identi\ufb01ers, just as in Chord. Furthermore,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.2. FLAT NAMING 255\n(a)\n(b)\nFigure 5.9: (a) An insert request is forwarded to the \ufb01rst node that knows\nabout entity E. (b) A chain of forwarding pointers to the leaf node is created.\nlet us assume that there are a total of Nphysical hostsfH1,H2,. . .,HNgthat can\naccommodate the lookup service, spread across the Internet. Each host is capable\nof running one or more location servers. Typically, two servers running on the\nsame host will represent two nodes at different levels of the logical tree. Let Dk(A)\ndenote the domain at level kthat contains address A, with k=0denoting the\nroot domain. Likewise, let LSk(E,A)denote the unique location server in Dk(A)\nresponsible for keeping track of the whereabouts of entity E.\nWe can now make a distinction between a logical root and its implementation.\nLetDk=fDk,1,Dk,2,. . .,Dk,Nkgdenote the Nkdomains at level k, with, obviously,\nN0=jD0j=1. For each level k, the set of hosts is partitioned into Nksubsets,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.2. FLAT NAMING 255\n(a)\n(b)\nFigure 5.9: (a) An insert request is forwarded to the \ufb01rst node that knows\nabout entity E. (b) A chain of forwarding pointers to the leaf node is created.\nlet us assume that there are a total of Nphysical hostsfH1,H2,. . .,HNgthat can\naccommodate the lookup service, spread across the Internet. Each host is capable\nof running one or more location servers. Typically, two servers running on the\nsame host will represent two nodes at different levels of the logical tree. Let Dk(A)\ndenote the domain at level kthat contains address A, with k=0denoting the\nroot domain. Likewise, let LSk(E,A)denote the unique location server in Dk(A)\nresponsible for keeping track of the whereabouts of entity E.\nWe can now make a distinction between a logical root and its implementation.\nLetDk=fDk,1,Dk,2,. . .,Dk,Nkgdenote the Nkdomains at level k, with, obviously,\nN0=jD0j=1. For each level k, the set of hosts is partitioned into Nksubsets,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "256 CHAPTER 5. NAMING\nwith each host running a location server representing exactly one of the domains\nDk,ifrom Dk. This principle is shown in Figure 5.10.\nFigure 5.10: The principle of distributing logical location servers over\nphysical hosts.\nIn this example, we consider a simple tree with four levels and nine hosts.\nThere are two level-1 domains, four level-2 domains, and eight leaf domains. We\nalso show a tree for one speci\ufb01c entity E: any contact address associated with E\nwill be stored in one of the eight level-3 location servers, depending, of course,\non the domain to which that address belongs. The root location server for Eis\nrunning on host H3. Note that this host also runs a leaf-level location server for E.\nAs explained by [van Steen and Ballintijn, 2002], by judiciously choosing\nwhich host should run a location server for E, we can combine the principle of\nlocal operations (which is good for geographical scalability) and full distribution\nof higher level servers (which is good for size scalability).\n5.3 Structured naming\nFlat names are good for machines, but are generally not very convenient\nfor humans to use. As an alternative, naming systems generally support\nstructured names that are composed from simple, human-readable names.\nNot only \ufb01le naming, but also host naming on the Internet follows this\napproach. In this section, we concentrate on structured names and the way\nthat these names are resolved to addresses.\nName spaces\nNames are commonly organized into what is called a name space . Name\nspaces for structured names can be represented as a labeled, directed graph\nwith two types of nodes. A leaf node represents a named entity and has\nthe property that it has no outgoing edges. A leaf node generally stores\ninformation on the entity it is representing\u2013for example, its address\u2013so that a\nclient can access it. Alternatively, it can store the state of that entity, such as in\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n256 CHAPTER 5. NAMING\nwith each host running a location server representing exactly one of the domains\nDk,ifrom Dk. This principle is shown in Figure 5.10.\nFigure 5.10: The principle of distributing logical location servers over\nphysical hosts.\nIn this example, we consider a simple tree with four levels and nine hosts.\nThere are two level-1 domains, four level-2 domains, and eight leaf domains. We\nalso show a tree for one speci\ufb01c entity E: any contact address associated with E\nwill be stored in one of the eight level-3 location servers, depending, of course,\non the domain to which that address belongs. The root location server for Eis\nrunning on host H3. Note that this host also runs a leaf-level location server for E.\nAs explained by [van Steen and Ballintijn, 2002], by judiciously choosing\nwhich host should run a location server for E, we can combine the principle of\nlocal operations (which is good for geographical scalability) and full distribution\nof higher level servers (which is good for size scalability).\n5.3 Structured naming\nFlat names are good for machines, but are generally not very convenient\nfor humans to use. As an alternative, naming systems generally support\nstructured names that are composed from simple, human-readable names.\nNot only \ufb01le naming, but also host naming on the Internet follows this\napproach. In this section, we concentrate on structured names and the way\nthat these names are resolved to addresses.\nName spaces\nNames are commonly organized into what is called a name space . Name\nspaces for structured names can be represented as a labeled, directed graph\nwith two types of nodes. A leaf node represents a named entity and has\nthe property that it has no outgoing edges. A leaf node generally stores\ninformation on the entity it is representing\u2013for example, its address\u2013so that a\nclient can access it. Alternatively, it can store the state of that entity, such as in\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 257\nthe case of \ufb01le systems in which a leaf node actually contains the complete\n\ufb01le it is representing. We return to the contents of nodes below.\nIn contrast to a leaf node, a directory node has a number of outgoing\nedges, each labeled with a name, as shown in Figure 5.11 Each node in a\nnaming graph is considered as yet another entity in a distributed system,\nand, in particular, has an associated identi\ufb01er. A directory node stores a table\nin which an outgoing edge is represented as a pair (node identi\ufb01er, edge label) .\nSuch a table is called a directory table .\nFigure 5.11: A general naming graph with a single root node.\nThe naming graph shown in Figure 5.11 has one node, namely n0, which\nhas only outgoing and no incoming edges. Such a node is called the root\n(node) of the naming graph. Although it is possible for a naming graph to\nhave several root nodes, for simplicity, many naming systems have only one.\nEach path in a naming graph can be referred to by the sequence of labels\ncorresponding to the edges in that path, such as N:[label 1,label 2, ...,label n],\nwhere Nrefers to the \ufb01rst node in the path. Such a sequence is called a path\nname . If the \ufb01rst node in a path name is the root of the naming graph, it is\ncalled an absolute path name . Otherwise, it is called a relative path name .\nIt is important to realize that names are always organized in a name space.\nAs a consequence, a name is always de\ufb01ned relative only to a directory node.\nIn this sense, the term \u201cabsolute name\u201d is somewhat misleading. Likewise, the\ndifference between global and local names can often be confusing. A global\nname is a name that denotes the same entity, no matter where that name is\nused in a system. In other words, a global name is always interpreted with\nrespect to the same directory node. In contrast, a local name is a name whose\ninterpretation depends on where that name is being used. Put differently,\na local name is essentially a relative name whose directory in which it is\ncontained is (implicitly) known.\nThis description of a naming graph comes close to what is implemented\nin many \ufb01le systems. However, instead of writing the sequence of edge\nlabels to represent a path name, path names in \ufb01le systems are generally\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 257\nthe case of \ufb01le systems in which a leaf node actually contains the complete\n\ufb01le it is representing. We return to the contents of nodes below.\nIn contrast to a leaf node, a directory node has a number of outgoing\nedges, each labeled with a name, as shown in Figure 5.11 Each node in a\nnaming graph is considered as yet another entity in a distributed system,\nand, in particular, has an associated identi\ufb01er. A directory node stores a table\nin which an outgoing edge is represented as a pair (node identi\ufb01er, edge label) .\nSuch a table is called a directory table .\nFigure 5.11: A general naming graph with a single root node.\nThe naming graph shown in Figure 5.11 has one node, namely n0, which\nhas only outgoing and no incoming edges. Such a node is called the root\n(node) of the naming graph. Although it is possible for a naming graph to\nhave several root nodes, for simplicity, many naming systems have only one.\nEach path in a naming graph can be referred to by the sequence of labels\ncorresponding to the edges in that path, such as N:[label 1,label 2, ...,label n],\nwhere Nrefers to the \ufb01rst node in the path. Such a sequence is called a path\nname . If the \ufb01rst node in a path name is the root of the naming graph, it is\ncalled an absolute path name . Otherwise, it is called a relative path name .\nIt is important to realize that names are always organized in a name space.\nAs a consequence, a name is always de\ufb01ned relative only to a directory node.\nIn this sense, the term \u201cabsolute name\u201d is somewhat misleading. Likewise, the\ndifference between global and local names can often be confusing. A global\nname is a name that denotes the same entity, no matter where that name is\nused in a system. In other words, a global name is always interpreted with\nrespect to the same directory node. In contrast, a local name is a name whose\ninterpretation depends on where that name is being used. Put differently,\na local name is essentially a relative name whose directory in which it is\ncontained is (implicitly) known.\nThis description of a naming graph comes close to what is implemented\nin many \ufb01le systems. However, instead of writing the sequence of edge\nlabels to represent a path name, path names in \ufb01le systems are generally\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "258 CHAPTER 5. NAMING\nrepresented as a single string in which the labels are separated by a special\nseparator character, such as a slash (\u201c/\u201d). This character is also used to\nindicate whether a path name is absolute. For example, in Figure 5.11 instead\nof using n0:[home ,steen ,mbox ], that is, the actual path name, it is common\npractice to use its string representation /home /steen /mbox . Note also that\nwhen there are several paths that lead to the same node, that node can be\nrepresented by different path names. For example, node n5in Figure 5.11 can\nbe referred to by /home /steen /keys as well as /keys. The string representation\nof path names can be equally well applied to naming graphs other than those\nused for only \ufb01le systems. In Plan 9 [Pike et al., 1995], all resources, such as\nprocesses, hosts, I/O devices, and network interfaces, are named in the same\nfashion as traditional \ufb01les. This approach is analogous to implementing a\nsingle naming graph for all resources in a distributed system.\nThere are many different ways to organize a name space. As we mentioned,\nmost name spaces have only a single root node. In many cases, a name space\nis also strictly hierarchical in the sense that the naming graph is organized as\na tree. This means that each node except the root has exactly one incoming\nedge; the root has no incoming edges. As a consequence, each node also has\nexactly one associated (absolute) path name.\nThe naming graph shown in Figure 5.11 is an example of directed acyclic\ngraph . In such an organization, a node can have more than one incoming\nedge, but the graph is not permitted to have a cycle. There are also name\nspaces that do not have this restriction.\nNote 5.7 (More information: Implementing the Unix naming graph)\nTo make matters more concrete, consider the way that \ufb01les in a traditional Unix\n\ufb01le system are named. In a naming graph for Unix a directory node represents a\n\ufb01le directory, whereas a leaf node represents a \ufb01le. There is a single root directory,\nrepresented in the naming graph by the root node. The implementation of the\nnaming graph is an integral part of the complete implementation of the \ufb01le system.\nThat implementation consists of a contiguous series of blocks from a logical disk,\ngenerally divided into a boot block, a superblock, a series of index nodes (called\ninodes), and \ufb01le data blocks. See also Silberschatz et al. [2012] or Tanenbaum\n[2001]. This organization is shown in Figure 5.12.\nThe boot block is a special block of data and instructions that are automatically\nloaded into main memory when the system is booted. The boot block is used to\nload the operating system into main memory.\nThe superblock contains information on the entire \ufb01le system, such as its size,\nwhich blocks on disk are not yet allocated, which inodes are not yet used, and so\non. Inodes are referred to by an index number, starting at number zero, which is\nreserved for the inode representing the root directory.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n258 CHAPTER 5. NAMING\nrepresented as a single string in which the labels are separated by a special\nseparator character, such as a slash (\u201c/\u201d). This character is also used to\nindicate whether a path name is absolute. For example, in Figure 5.11 instead\nof using n0:[home ,steen ,mbox ], that is, the actual path name, it is common\npractice to use its string representation /home /steen /mbox . Note also that\nwhen there are several paths that lead to the same node, that node can be\nrepresented by different path names. For example, node n5in Figure 5.11 can\nbe referred to by /home /steen /keys as well as /keys. The string representation\nof path names can be equally well applied to naming graphs other than those\nused for only \ufb01le systems. In Plan 9 [Pike et al., 1995], all resources, such as\nprocesses, hosts, I/O devices, and network interfaces, are named in the same\nfashion as traditional \ufb01les. This approach is analogous to implementing a\nsingle naming graph for all resources in a distributed system.\nThere are many different ways to organize a name space. As we mentioned,\nmost name spaces have only a single root node. In many cases, a name space\nis also strictly hierarchical in the sense that the naming graph is organized as\na tree. This means that each node except the root has exactly one incoming\nedge; the root has no incoming edges. As a consequence, each node also has\nexactly one associated (absolute) path name.\nThe naming graph shown in Figure 5.11 is an example of directed acyclic\ngraph . In such an organization, a node can have more than one incoming\nedge, but the graph is not permitted to have a cycle. There are also name\nspaces that do not have this restriction.\nNote 5.7 (More information: Implementing the Unix naming graph)\nTo make matters more concrete, consider the way that \ufb01les in a traditional Unix\n\ufb01le system are named. In a naming graph for Unix a directory node represents a\n\ufb01le directory, whereas a leaf node represents a \ufb01le. There is a single root directory,\nrepresented in the naming graph by the root node. The implementation of the\nnaming graph is an integral part of the complete implementation of the \ufb01le system.\nThat implementation consists of a contiguous series of blocks from a logical disk,\ngenerally divided into a boot block, a superblock, a series of index nodes (called\ninodes), and \ufb01le data blocks. See also Silberschatz et al. [2012] or Tanenbaum\n[2001]. This organization is shown in Figure 5.12.\nThe boot block is a special block of data and instructions that are automatically\nloaded into main memory when the system is booted. The boot block is used to\nload the operating system into main memory.\nThe superblock contains information on the entire \ufb01le system, such as its size,\nwhich blocks on disk are not yet allocated, which inodes are not yet used, and so\non. Inodes are referred to by an index number, starting at number zero, which is\nreserved for the inode representing the root directory.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 259\nFigure 5.12: The general organization of the Unix \ufb01le system implemen-\ntation on a logical disk of contiguous disk blocks.\nEach inode contains information on where the data of its associated \ufb01le can\nbe found on disk. In addition, an inode contains information on its owner, time\nof creation and last modi\ufb01cation, protection, and the like. Consequently, when\ngiven the index number of an inode, it is possible to access its associated \ufb01le.\nEach directory is implemented as a \ufb01le as well. This is also the case for the root\ndirectory, which contains a mapping between \ufb01le names and index numbers of\ninodes. It is thus seen that the index number of an inode corresponds to a node\nidenti\ufb01er in the naming graph.\nName resolution\nName spaces offer a convenient mechanism for storing and retrieving infor-\nmation about entities by means of names. More generally, given a path name,\nit should be possible to look up any information stored in the node referred\nto by that name. The process of looking up a name is called name resolution .\nTo explain how name resolution works, let us consider a path name such\nasN:[label 1,label 2, ...,label n]. Resolution of this name starts at node Nof the\nnaming graph, where the name label 1is looked up in the directory table, and\nwhich returns the identi\ufb01er of the node to which label 1refers. Resolution then\ncontinues at the identi\ufb01ed node by looking up the name label 2in its directory\ntable, and so on. Assuming that the named path actually exists, resolution\nstops at the last node referred to by label n, by returning that node\u2019s content.\nNote 5.8 (More information: The Unix naming graph again)\nA name lookup returns the identi\ufb01er of a node from where the name resolution\nprocess continues. In particular, it is necessary to access the directory table of\nthe identi\ufb01ed node. Consider again a naming graph for a Unix \ufb01le system. As\nmentioned, a node identi\ufb01er is implemented as the index number of an inode.\nAccessing a directory table means that \ufb01rst the inode has to be read to \ufb01nd out\nwhere the actual data are stored on disk, and then subsequently to read the data\nblocks containing the directory table.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 259\nFigure 5.12: The general organization of the Unix \ufb01le system implemen-\ntation on a logical disk of contiguous disk blocks.\nEach inode contains information on where the data of its associated \ufb01le can\nbe found on disk. In addition, an inode contains information on its owner, time\nof creation and last modi\ufb01cation, protection, and the like. Consequently, when\ngiven the index number of an inode, it is possible to access its associated \ufb01le.\nEach directory is implemented as a \ufb01le as well. This is also the case for the root\ndirectory, which contains a mapping between \ufb01le names and index numbers of\ninodes. It is thus seen that the index number of an inode corresponds to a node\nidenti\ufb01er in the naming graph.\nName resolution\nName spaces offer a convenient mechanism for storing and retrieving infor-\nmation about entities by means of names. More generally, given a path name,\nit should be possible to look up any information stored in the node referred\nto by that name. The process of looking up a name is called name resolution .\nTo explain how name resolution works, let us consider a path name such\nasN:[label 1,label 2, ...,label n]. Resolution of this name starts at node Nof the\nnaming graph, where the name label 1is looked up in the directory table, and\nwhich returns the identi\ufb01er of the node to which label 1refers. Resolution then\ncontinues at the identi\ufb01ed node by looking up the name label 2in its directory\ntable, and so on. Assuming that the named path actually exists, resolution\nstops at the last node referred to by label n, by returning that node\u2019s content.\nNote 5.8 (More information: The Unix naming graph again)\nA name lookup returns the identi\ufb01er of a node from where the name resolution\nprocess continues. In particular, it is necessary to access the directory table of\nthe identi\ufb01ed node. Consider again a naming graph for a Unix \ufb01le system. As\nmentioned, a node identi\ufb01er is implemented as the index number of an inode.\nAccessing a directory table means that \ufb01rst the inode has to be read to \ufb01nd out\nwhere the actual data are stored on disk, and then subsequently to read the data\nblocks containing the directory table.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "260 CHAPTER 5. NAMING\nClosure mechanism\nName resolution can take place only if we know how and where to start. In\nour example, the starting node was given, and we assumed we had access to its\ndirectory table. Knowing how and where to start name resolution is generally\nreferred to as a closure mechanism . Essentially, a closure mechanism deals\nwith selecting the initial node in a name space from which name resolution\nis to start [Radia, 1989]. What makes closure mechanisms sometimes hard\nto understand is that they are necessarily partly implicit and may be very\ndifferent when comparing them to each other.\nConsider, for example, the string \u201c00312059837784\u201d. Many people will not\nknow what to do with these numbers, unless they are told that the sequence is\na telephone number. That information is enough to start the resolution process,\nin particular, by dialing the number. The telephone system subsequently does\nthe rest.\nAs another example, consider the use of global and local names in dis-\ntributed systems. A typical example of a local name is an environment variable.\nFor example, in Unix systems, the variable named HOME is used to refer to\nthe home directory of a user. Each user has its own copy of this variable,\nwhich is initialized to the global, systemwide name corresponding to the\nuser\u2019s home directory. The closure mechanism associated with environment\nvariables ensures that the name of the variable is properly resolved by looking\nit up in a user-speci\ufb01c table.\nNote 5.9 (More information: The Unix naming graph and its closure mechanism)\nName resolution in the naming graph for a Unix \ufb01le system makes use of the fact\nthat the inode of the root directory is the \ufb01rst inode in the logical disk representing\nthe \ufb01le system. Its actual byte offset is calculated from the values in other \ufb01elds\nof the superblock, together with hard-coded information in the operating system\nitself on the internal organization of the superblock.\nTo make this point clear, consider the string representation of a \ufb01le name\nsuch as /home /steen /mbox . To resolve this name, it is necessary to already\nhave access to the directory table of the root node of the appropriate naming\ngraph. Being a root node, the node itself cannot have been looked up unless it is\nimplemented as a different node in a another naming graph, say G. But in that\ncase, it would have been necessary to already have access to the root node of G.\nConsequently, resolving a \ufb01le name requires that some mechanism has already\nbeen implemented by which the resolution process can start.\nLinking and mounting\nStrongly related to name resolution is the use of aliases . An alias is another\nname for the same entity. An environment variable is an example of an alias.\nIn terms of naming graphs, there are basically two different ways to implement\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n260 CHAPTER 5. NAMING\nClosure mechanism\nName resolution can take place only if we know how and where to start. In\nour example, the starting node was given, and we assumed we had access to its\ndirectory table. Knowing how and where to start name resolution is generally\nreferred to as a closure mechanism . Essentially, a closure mechanism deals\nwith selecting the initial node in a name space from which name resolution\nis to start [Radia, 1989]. What makes closure mechanisms sometimes hard\nto understand is that they are necessarily partly implicit and may be very\ndifferent when comparing them to each other.\nConsider, for example, the string \u201c00312059837784\u201d. Many people will not\nknow what to do with these numbers, unless they are told that the sequence is\na telephone number. That information is enough to start the resolution process,\nin particular, by dialing the number. The telephone system subsequently does\nthe rest.\nAs another example, consider the use of global and local names in dis-\ntributed systems. A typical example of a local name is an environment variable.\nFor example, in Unix systems, the variable named HOME is used to refer to\nthe home directory of a user. Each user has its own copy of this variable,\nwhich is initialized to the global, systemwide name corresponding to the\nuser\u2019s home directory. The closure mechanism associated with environment\nvariables ensures that the name of the variable is properly resolved by looking\nit up in a user-speci\ufb01c table.\nNote 5.9 (More information: The Unix naming graph and its closure mechanism)\nName resolution in the naming graph for a Unix \ufb01le system makes use of the fact\nthat the inode of the root directory is the \ufb01rst inode in the logical disk representing\nthe \ufb01le system. Its actual byte offset is calculated from the values in other \ufb01elds\nof the superblock, together with hard-coded information in the operating system\nitself on the internal organization of the superblock.\nTo make this point clear, consider the string representation of a \ufb01le name\nsuch as /home /steen /mbox . To resolve this name, it is necessary to already\nhave access to the directory table of the root node of the appropriate naming\ngraph. Being a root node, the node itself cannot have been looked up unless it is\nimplemented as a different node in a another naming graph, say G. But in that\ncase, it would have been necessary to already have access to the root node of G.\nConsequently, resolving a \ufb01le name requires that some mechanism has already\nbeen implemented by which the resolution process can start.\nLinking and mounting\nStrongly related to name resolution is the use of aliases . An alias is another\nname for the same entity. An environment variable is an example of an alias.\nIn terms of naming graphs, there are basically two different ways to implement\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 261\nan alias. The \ufb01rst approach is to simply allow multiple absolute paths names\nto refer to the same node in a naming graph. This approach is illustrated\nin Figure 5.13, in which node n5can be referred to by two different path\nnames. In Unix terminology, both path names /keys and /home /steen /keys\nin Figure 5.9 are called hard links to node n5.\nThe second approach is to represent an entity by a leaf node, say N, but\ninstead of storing the address or state of that entity, the node stores an absolute\npath name. When \ufb01rst resolving an absolute path name that leads to N, name\nresolution will return the path name stored in N, at which point it can continue\nwith resolving that new path name. This principle corresponds to the use of\nsymbolic links in Unix \ufb01le systems, and is illustrated in Figure 5.13 In this\nexample, the path name /home /steen /keys, which refers to a node containing\nthe absolute path name / keys, is a symbolic link to node n5.\nFigure 5.13: The concept of a symbolic link explained in a naming graph.\nName resolution as described so far takes place completely within a single\nname space. However, name resolution can also be used to merge different\nname spaces in a transparent way. Let us \ufb01rst consider a mounted \ufb01le system.\nIn terms of our naming model, a mounted \ufb01le system corresponds to letting a\ndirectory node store the identi\ufb01er of a directory node from a different name\nspace, which we refer to as a foreign name space . The directory node storing\nthe node identi\ufb01er is called a mount point . Accordingly, the directory node in\nthe foreign name space is called a mounting point . Normally, the mounting\npoint is the root of a name space. During name resolution, the mounting point\nis looked up and resolution proceeds by accessing its directory table.\nThe principle of mounting can be generalized to other name spaces as\nwell. In particular, what is needed is a directory node that acts as a mount\npoint and stores all the necessary information for identifying and accessing\nthe mounting point in the foreign name space. This approach is followed in\nmany distributed \ufb01le systems.\nConsider a collection of name spaces that is distributed across different\nmachines. In particular, each name space is implemented by a different server,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 261\nan alias. The \ufb01rst approach is to simply allow multiple absolute paths names\nto refer to the same node in a naming graph. This approach is illustrated\nin Figure 5.13, in which node n5can be referred to by two different path\nnames. In Unix terminology, both path names /keys and /home /steen /keys\nin Figure 5.9 are called hard links to node n5.\nThe second approach is to represent an entity by a leaf node, say N, but\ninstead of storing the address or state of that entity, the node stores an absolute\npath name. When \ufb01rst resolving an absolute path name that leads to N, name\nresolution will return the path name stored in N, at which point it can continue\nwith resolving that new path name. This principle corresponds to the use of\nsymbolic links in Unix \ufb01le systems, and is illustrated in Figure 5.13 In this\nexample, the path name /home /steen /keys, which refers to a node containing\nthe absolute path name / keys, is a symbolic link to node n5.\nFigure 5.13: The concept of a symbolic link explained in a naming graph.\nName resolution as described so far takes place completely within a single\nname space. However, name resolution can also be used to merge different\nname spaces in a transparent way. Let us \ufb01rst consider a mounted \ufb01le system.\nIn terms of our naming model, a mounted \ufb01le system corresponds to letting a\ndirectory node store the identi\ufb01er of a directory node from a different name\nspace, which we refer to as a foreign name space . The directory node storing\nthe node identi\ufb01er is called a mount point . Accordingly, the directory node in\nthe foreign name space is called a mounting point . Normally, the mounting\npoint is the root of a name space. During name resolution, the mounting point\nis looked up and resolution proceeds by accessing its directory table.\nThe principle of mounting can be generalized to other name spaces as\nwell. In particular, what is needed is a directory node that acts as a mount\npoint and stores all the necessary information for identifying and accessing\nthe mounting point in the foreign name space. This approach is followed in\nmany distributed \ufb01le systems.\nConsider a collection of name spaces that is distributed across different\nmachines. In particular, each name space is implemented by a different server,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "262 CHAPTER 5. NAMING\neach possibly running on a separate machine. Consequently, if we want to\nmount a foreign name space NS2into a name space NS1, it may be necessary\nto communicate over a network with the server of NS2, as that server may be\nrunning on a different machine than the server for NS1. To mount a foreign\nname space in a distributed system requires at least the following information:\n1. The name of an access protocol.\n2. The name of the server.\n3. The name of the mounting point in the foreign name space.\nNote that each of these names needs to be resolved. The name of an access\nprotocol needs to be resolved to the implementation of a protocol by which\ncommunication with the server of the foreign name space can take place. The\nname of the server needs to be resolved to an address where that server can\nbe reached. As the last part in name resolution, the name of the mounting\npoint needs to be resolved to a node identi\ufb01er in the foreign name space.\nIn nondistributed systems, none of the three points may actually be needed.\nFor example, in Unix there is no access protocol and no server. Also, the name\nof the mounting point is not necessary, as it is simply the root directory of the\nforeign name space.\nThe name of the mounting point is to be resolved by the server of the\nforeign name space. However, we also need name spaces and implementations\nfor the access protocol and the server name. One possibility is to represent\nthe three names listed above as a URL.\nTo make matters concrete, consider a situation in which a user with a\nlaptop computer wants to access \ufb01les that are stored on a remote \ufb01le server.\nThe client machine and the \ufb01le server are both con\ufb01gured with the Network\nFile System (NFS ). In particular, to allow NFS to work across the Internet,\na client can specify exactly which \ufb01le it wants to access by means of an\nNFS URL, for example, nfs: / /\rits.cs.vu.nl/home /steen . This URL names a\n\ufb01le (which happens to be a directory) called /home /steen on an NFS \ufb01le\nserver \rits.cs.vu.nl, which can be accessed by a client by means of the NFS\nprotocol [Shepler et al., 2003].\nThe name nfsis a well-known name in the sense that worldwide agreement\nexists on how to interpret that name. Given that we are dealing with a URL,\nthe name nfswill be resolved to an implementation of the NFS protocol. The\nserver name is resolved to its address using DNS, which is discussed in a\nlater section. As we said, /home /steen is resolved by the server of the foreign\nname space.\nThe organization of a \ufb01le system on the client machine is partly shown in\nFigure 5.14 The root directory has a number of user-de\ufb01ned entries, including\na subdirectory called /remote . This subdirectory is intended to include mount\npoints for foreign name spaces such as the user\u2019s home directory at VU\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n262 CHAPTER 5. NAMING\neach possibly running on a separate machine. Consequently, if we want to\nmount a foreign name space NS2into a name space NS1, it may be necessary\nto communicate over a network with the server of NS2, as that server may be\nrunning on a different machine than the server for NS1. To mount a foreign\nname space in a distributed system requires at least the following information:\n1. The name of an access protocol.\n2. The name of the server.\n3. The name of the mounting point in the foreign name space.\nNote that each of these names needs to be resolved. The name of an access\nprotocol needs to be resolved to the implementation of a protocol by which\ncommunication with the server of the foreign name space can take place. The\nname of the server needs to be resolved to an address where that server can\nbe reached. As the last part in name resolution, the name of the mounting\npoint needs to be resolved to a node identi\ufb01er in the foreign name space.\nIn nondistributed systems, none of the three points may actually be needed.\nFor example, in Unix there is no access protocol and no server. Also, the name\nof the mounting point is not necessary, as it is simply the root directory of the\nforeign name space.\nThe name of the mounting point is to be resolved by the server of the\nforeign name space. However, we also need name spaces and implementations\nfor the access protocol and the server name. One possibility is to represent\nthe three names listed above as a URL.\nTo make matters concrete, consider a situation in which a user with a\nlaptop computer wants to access \ufb01les that are stored on a remote \ufb01le server.\nThe client machine and the \ufb01le server are both con\ufb01gured with the Network\nFile System (NFS ). In particular, to allow NFS to work across the Internet,\na client can specify exactly which \ufb01le it wants to access by means of an\nNFS URL, for example, nfs: / /\rits.cs.vu.nl/home /steen . This URL names a\n\ufb01le (which happens to be a directory) called /home /steen on an NFS \ufb01le\nserver \rits.cs.vu.nl, which can be accessed by a client by means of the NFS\nprotocol [Shepler et al., 2003].\nThe name nfsis a well-known name in the sense that worldwide agreement\nexists on how to interpret that name. Given that we are dealing with a URL,\nthe name nfswill be resolved to an implementation of the NFS protocol. The\nserver name is resolved to its address using DNS, which is discussed in a\nlater section. As we said, /home /steen is resolved by the server of the foreign\nname space.\nThe organization of a \ufb01le system on the client machine is partly shown in\nFigure 5.14 The root directory has a number of user-de\ufb01ned entries, including\na subdirectory called /remote . This subdirectory is intended to include mount\npoints for foreign name spaces such as the user\u2019s home directory at VU\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 263\nFigure 5.14: Mounting remote name spaces through a speci\ufb01c protocol.\nUniversity. To this end, a directory node named /remote /vuis used to store\nthe URL nfs: / /\rits.cs.vu.nl/home /steen .\nNow consider the name /remote /vu/mbox . This name is resolved by\nstarting in the root directory on the client\u2019s machine and continues until the\nnode /remote /vuis reached. The process of name resolution then continues\nby returning the URL nfs: / /\rits.cs.vu.nl/home /steen , in turn leading the client\nmachine to contact the \ufb01le server \rits.cs.vu.nlby means of the NFS protocol,\nand to subsequently access directory /home /steen . Name resolution can then\nbe continued by reading the \ufb01le named mbox in that directory, after which the\nresolution process stops.\nDistributed systems that allow mounting a remote \ufb01le system as just\ndescribed allow a client machine to, for example, execute the following com-\nmands (assume the client machine is named horton ):\nhorton$ cd /remote/vu\nhorton$ ls -l\nwhich subsequently lists the \ufb01les in the directory /home /steen on the remote\n\ufb01le server. The beauty of all this is that the user is spared the details of the\nactual access to the remote server. Ideally, only some loss in performance is\nnoticed compared to accessing locally available \ufb01les. In effect, to the client it\nappears that the name space rooted on the local machine, and the one rooted\nat /home /steen on the remote machine, form a single name space.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 263\nFigure 5.14: Mounting remote name spaces through a speci\ufb01c protocol.\nUniversity. To this end, a directory node named /remote /vuis used to store\nthe URL nfs: / /\rits.cs.vu.nl/home /steen .\nNow consider the name /remote /vu/mbox . This name is resolved by\nstarting in the root directory on the client\u2019s machine and continues until the\nnode /remote /vuis reached. The process of name resolution then continues\nby returning the URL nfs: / /\rits.cs.vu.nl/home /steen , in turn leading the client\nmachine to contact the \ufb01le server \rits.cs.vu.nlby means of the NFS protocol,\nand to subsequently access directory /home /steen . Name resolution can then\nbe continued by reading the \ufb01le named mbox in that directory, after which the\nresolution process stops.\nDistributed systems that allow mounting a remote \ufb01le system as just\ndescribed allow a client machine to, for example, execute the following com-\nmands (assume the client machine is named horton ):\nhorton$ cd /remote/vu\nhorton$ ls -l\nwhich subsequently lists the \ufb01les in the directory /home /steen on the remote\n\ufb01le server. The beauty of all this is that the user is spared the details of the\nactual access to the remote server. Ideally, only some loss in performance is\nnoticed compared to accessing locally available \ufb01les. In effect, to the client it\nappears that the name space rooted on the local machine, and the one rooted\nat /home /steen on the remote machine, form a single name space.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "264 CHAPTER 5. NAMING\nNote 5.10 (More information: Mounting across a network in Unix)\nThere are many ways in which mounting across a network can take place. One\npractical solution and adopted by many small-scale distributed systems, is to\nsimply assign \ufb01xed IP addresses to machines and subsequently offer mounting\npoints to clients. Consider the following example. Suppose we have a Unix\nmachine named coltrane using the private address 192.168.2.3, storing a collection\nof music \ufb01les under the local directory /audio . This directory can be exported as a\nmounting point, and as a consequence can be imported by another machine.\nLetquandar be such a machine, and suppose it wants to mount the collection\nof audio \ufb01les at the local mount point /home /maarten /Music . The following\ncommand will do the job (assuming the correct privileges have been set):\nquandar$ mount -t nfs 192.168.2.3:/audio /home/maarten/Music\nFrom that moment on, all \ufb01les available on coltrane in its directory /audio can be\naccessed by quandar in the directory /home /maarten /Music . The beauty of this\nscheme is that once mounted, there is no need to think of remote access anymore\n(until something fails, of course).\nThe implementation of a name space\nA name space forms the heart of a naming service, that is, a service that\nallows users and processes to add, remove, and look up names. A naming\nservice is implemented by name servers. If a distributed system is restricted\nto a local-area network, it is often feasible to implement a naming service\nby means of only a single name server. However, in large-scale distributed\nsystems with many entities, possibly spread across a large geographical area,\nit is necessary to distribute the implementation of a name space over multiple\nname servers.\nName space distribution\nName spaces for a large-scale, possibly worldwide distributed system, are\nusually organized hierarchically. As before, assume such a name space has\nonly a single root node. To effectively implement such a name space, it\nis convenient to partition it into logical layers. Cheriton and Mann [1989]\ndistinguish the following three layers.\nThe global layer is formed by highest-level nodes, that is, the root node\nand other directory nodes logically close to the root, namely its children.\nNodes in the global layer are often characterized by their stability, in the\nsense that directory tables are rarely changed. Such nodes may represent\norganizations, or groups of organizations, for which names are stored in the\nname space.\nThe administrational layer is formed by directory nodes that together are\nmanaged within a single organization. A characteristic feature of the directory\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n264 CHAPTER 5. NAMING\nNote 5.10 (More information: Mounting across a network in Unix)\nThere are many ways in which mounting across a network can take place. One\npractical solution and adopted by many small-scale distributed systems, is to\nsimply assign \ufb01xed IP addresses to machines and subsequently offer mounting\npoints to clients. Consider the following example. Suppose we have a Unix\nmachine named coltrane using the private address 192.168.2.3, storing a collection\nof music \ufb01les under the local directory /audio . This directory can be exported as a\nmounting point, and as a consequence can be imported by another machine.\nLetquandar be such a machine, and suppose it wants to mount the collection\nof audio \ufb01les at the local mount point /home /maarten /Music . The following\ncommand will do the job (assuming the correct privileges have been set):\nquandar$ mount -t nfs 192.168.2.3:/audio /home/maarten/Music\nFrom that moment on, all \ufb01les available on coltrane in its directory /audio can be\naccessed by quandar in the directory /home /maarten /Music . The beauty of this\nscheme is that once mounted, there is no need to think of remote access anymore\n(until something fails, of course).\nThe implementation of a name space\nA name space forms the heart of a naming service, that is, a service that\nallows users and processes to add, remove, and look up names. A naming\nservice is implemented by name servers. If a distributed system is restricted\nto a local-area network, it is often feasible to implement a naming service\nby means of only a single name server. However, in large-scale distributed\nsystems with many entities, possibly spread across a large geographical area,\nit is necessary to distribute the implementation of a name space over multiple\nname servers.\nName space distribution\nName spaces for a large-scale, possibly worldwide distributed system, are\nusually organized hierarchically. As before, assume such a name space has\nonly a single root node. To effectively implement such a name space, it\nis convenient to partition it into logical layers. Cheriton and Mann [1989]\ndistinguish the following three layers.\nThe global layer is formed by highest-level nodes, that is, the root node\nand other directory nodes logically close to the root, namely its children.\nNodes in the global layer are often characterized by their stability, in the\nsense that directory tables are rarely changed. Such nodes may represent\norganizations, or groups of organizations, for which names are stored in the\nname space.\nThe administrational layer is formed by directory nodes that together are\nmanaged within a single organization. A characteristic feature of the directory\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 265\nnodes in the administrational layer is that they represent groups of entities\nthat belong to the same organization or administrational unit. For example,\nthere may be a directory node for each department in an organization, or a\ndirectory node from which all hosts can be found. Another directory node\nmay be used as the starting point for naming all users, and so forth. The\nnodes in the administrational layer are relatively stable, although changes\ngenerally occur more frequently than to nodes in the global layer.\nFinally, the managerial layer consists of nodes that may typically change\nregularly. For example, nodes representing hosts in the local network belong\nto this layer. For the same reason, the layer includes nodes representing shared\n\ufb01les such as those for libraries or binaries. Another important class of nodes\nincludes those that represent user-de\ufb01ned directories and \ufb01les. In contrast\nto the global and administrational layer, the nodes in the managerial layer\nare maintained not only by system administrators, but also by individual end\nusers of a distributed system.\nFigure 5.15: An example partitioning of the DNS name space, including\nInternet-accessible \ufb01les, into three layers.\nTo make matters more concrete, Figure 5.15 shows an example of the\npartitioning of part of the DNS name space, including the names of \ufb01les within\nan organization that can be accessed through the Internet, for example, Web\npages and transferable \ufb01les. The name space is divided into nonoverlapping\nparts, called zones in DNS [Mockapetris, 1987]. A zone is a part of the name\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 265\nnodes in the administrational layer is that they represent groups of entities\nthat belong to the same organization or administrational unit. For example,\nthere may be a directory node for each department in an organization, or a\ndirectory node from which all hosts can be found. Another directory node\nmay be used as the starting point for naming all users, and so forth. The\nnodes in the administrational layer are relatively stable, although changes\ngenerally occur more frequently than to nodes in the global layer.\nFinally, the managerial layer consists of nodes that may typically change\nregularly. For example, nodes representing hosts in the local network belong\nto this layer. For the same reason, the layer includes nodes representing shared\n\ufb01les such as those for libraries or binaries. Another important class of nodes\nincludes those that represent user-de\ufb01ned directories and \ufb01les. In contrast\nto the global and administrational layer, the nodes in the managerial layer\nare maintained not only by system administrators, but also by individual end\nusers of a distributed system.\nFigure 5.15: An example partitioning of the DNS name space, including\nInternet-accessible \ufb01les, into three layers.\nTo make matters more concrete, Figure 5.15 shows an example of the\npartitioning of part of the DNS name space, including the names of \ufb01les within\nan organization that can be accessed through the Internet, for example, Web\npages and transferable \ufb01les. The name space is divided into nonoverlapping\nparts, called zones in DNS [Mockapetris, 1987]. A zone is a part of the name\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "266 CHAPTER 5. NAMING\nspace that is implemented by a separate name server. Some of these zones are\nillustrated in Figure 5.15.\nIf we take a look at availability and performance, name servers in each\nlayer have to meet different requirements. High availability is especially\ncritical for name servers in the global layer. If a name server fails, a large\npart of the name space will be unreachable because name resolution cannot\nproceed beyond the failing server.\nPerformance is somewhat subtle. Due to the low rate of change of nodes\nin the global layer, the results of lookup operations generally remain valid for\na long time. Consequently, those results can be effectively cached (i.e., stored\nlocally) by the clients. The next time the same lookup operation is performed,\nthe results can be retrieved from the client\u2019s cache instead of letting the name\nserver return the results. As a result, name servers in the global layer do\nnot have to respond quickly to a single lookup request. On the other hand,\nthroughput may be important, especially in large-scale systems with millions\nof users.\nThe availability and performance requirements for name servers in the\nglobal layer can be met by replicating servers, in combination with client-side\ncaching. Updates in this layer generally do not have to come into effect\nimmediately, making it much easier to keep replicas consistent.\nAvailability for a name server in the administrational layer is primarily\nimportant for clients in the same organization as the name server. If the name\nserver fails, many resources within the organization become unreachable\nbecause they cannot be looked up. On the other hand, it may be less important\nthat resources in an organization are temporarily unreachable for users outside\nthat organization.\nWith respect to performance, name servers in the administrational layer\nhave similar characteristics as those in the global layer. Because changes to\nnodes do not occur all that often, caching lookup results can be highly effective,\nmaking performance less critical. However, in contrast to the global layer,\nthe administrational layer should take care that lookup results are returned\nwithin a few milliseconds, either directly from the server or from the client\u2019s\nlocal cache. Likewise, updates should generally be processed quicker than\nthose of the global layer. For example, it is unacceptable that an account for a\nnew user takes hours to become effective.\nThese requirements can often be met by using relatively powerful machines\nto run name servers. In addition, client-side caching should be applied,\ncombined with replication for increased overall availability.\nAvailability requirements for name servers at the managerial level are\ngenerally less demanding. In particular, it often suf\ufb01ces to use a single\nmachine to run name servers at the risk of temporary unavailability. However,\nperformance is crucial: operations must take place immediately. Because\nupdates occur regularly, client-side caching is often less effective.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n266 CHAPTER 5. NAMING\nspace that is implemented by a separate name server. Some of these zones are\nillustrated in Figure 5.15.\nIf we take a look at availability and performance, name servers in each\nlayer have to meet different requirements. High availability is especially\ncritical for name servers in the global layer. If a name server fails, a large\npart of the name space will be unreachable because name resolution cannot\nproceed beyond the failing server.\nPerformance is somewhat subtle. Due to the low rate of change of nodes\nin the global layer, the results of lookup operations generally remain valid for\na long time. Consequently, those results can be effectively cached (i.e., stored\nlocally) by the clients. The next time the same lookup operation is performed,\nthe results can be retrieved from the client\u2019s cache instead of letting the name\nserver return the results. As a result, name servers in the global layer do\nnot have to respond quickly to a single lookup request. On the other hand,\nthroughput may be important, especially in large-scale systems with millions\nof users.\nThe availability and performance requirements for name servers in the\nglobal layer can be met by replicating servers, in combination with client-side\ncaching. Updates in this layer generally do not have to come into effect\nimmediately, making it much easier to keep replicas consistent.\nAvailability for a name server in the administrational layer is primarily\nimportant for clients in the same organization as the name server. If the name\nserver fails, many resources within the organization become unreachable\nbecause they cannot be looked up. On the other hand, it may be less important\nthat resources in an organization are temporarily unreachable for users outside\nthat organization.\nWith respect to performance, name servers in the administrational layer\nhave similar characteristics as those in the global layer. Because changes to\nnodes do not occur all that often, caching lookup results can be highly effective,\nmaking performance less critical. However, in contrast to the global layer,\nthe administrational layer should take care that lookup results are returned\nwithin a few milliseconds, either directly from the server or from the client\u2019s\nlocal cache. Likewise, updates should generally be processed quicker than\nthose of the global layer. For example, it is unacceptable that an account for a\nnew user takes hours to become effective.\nThese requirements can often be met by using relatively powerful machines\nto run name servers. In addition, client-side caching should be applied,\ncombined with replication for increased overall availability.\nAvailability requirements for name servers at the managerial level are\ngenerally less demanding. In particular, it often suf\ufb01ces to use a single\nmachine to run name servers at the risk of temporary unavailability. However,\nperformance is crucial: operations must take place immediately. Because\nupdates occur regularly, client-side caching is often less effective.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 267\nIssue Global Administrational Managerial\nGeographical scale Worldwide Organization Department\nNumber of nodes Few Many Vast numbers\nResponsiveness to lookups Seconds Milliseconds Immediate\nUpdate propagation Lazy Immediate Immediate\nNumber of replicas Many None or few None\nClient-side caching Y es Y es Sometimes\nFigure 5.16: A comparison between name servers for implementing nodes\nfrom a large-scale name space partitioned into a global layer, an administra-\ntional layer, and a managerial layer.\nA comparison between name servers at different layers is shown in Fig-\nure 5.16. In distributed systems, name servers in the global and administra-\ntional layer are the most dif\ufb01cult to implement. Dif\ufb01culties are caused by\nreplication and caching, which are needed for availability and performance,\nbut which also introduce consistency problems. Some of the problems are\naggravated by the fact that caches and replicas are spread across a wide-area\nnetwork, which may introduce long communication delays during lookups.\nImplementation of name resolution\nThe distribution of a name space across multiple name servers affects the\nimplementation of name resolution. To explain the implementation of name\nresolution in large-scale name services, we assume for the moment that name\nservers are not replicated and that no client-side caches are used. Each\nclient has access to a local name resolver , which is responsible for ensuring\nthat the name resolution process is carried out. Referring to Figure 5.15,\nassume the (absolute) path name root:[nl,vu,cs,ftp,pub,globe ,index .html ]is to\nbe resolved. Using a URL notation, this path name would correspond to\nftp: / /ftp.cs.vu.nl/pub/globe /index .html . There are now two ways to imple-\nment name resolution.\nIniterative name resolution , a name resolver hands over the complete\nname to the root name server. It is assumed that the address where the root\nserver can be contacted is well known. The root server will resolve the path\nname as far as it can, and return the result to the client. In our example, the\nroot server can resolve only the label nl, for which it will return the address of\nthe associated name server.\nAt that point, the client passes the remaining path name (i.e., nl:[vu,cs,ftp,\npub,globe ,index .html ]) to that name server. This server can resolve only the\nlabel vu, and returns the address of the associated name server, along with\nthe remaining path name vu:[cs,ftp,pub,globe ,index .html ].\nThe client\u2019s name resolver will then contact this next name server, which\nresponds by resolving the label cs, and subsequently also ftp, returning the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 267\nIssue Global Administrational Managerial\nGeographical scale Worldwide Organization Department\nNumber of nodes Few Many Vast numbers\nResponsiveness to lookups Seconds Milliseconds Immediate\nUpdate propagation Lazy Immediate Immediate\nNumber of replicas Many None or few None\nClient-side caching Y es Y es Sometimes\nFigure 5.16: A comparison between name servers for implementing nodes\nfrom a large-scale name space partitioned into a global layer, an administra-\ntional layer, and a managerial layer.\nA comparison between name servers at different layers is shown in Fig-\nure 5.16. In distributed systems, name servers in the global and administra-\ntional layer are the most dif\ufb01cult to implement. Dif\ufb01culties are caused by\nreplication and caching, which are needed for availability and performance,\nbut which also introduce consistency problems. Some of the problems are\naggravated by the fact that caches and replicas are spread across a wide-area\nnetwork, which may introduce long communication delays during lookups.\nImplementation of name resolution\nThe distribution of a name space across multiple name servers affects the\nimplementation of name resolution. To explain the implementation of name\nresolution in large-scale name services, we assume for the moment that name\nservers are not replicated and that no client-side caches are used. Each\nclient has access to a local name resolver , which is responsible for ensuring\nthat the name resolution process is carried out. Referring to Figure 5.15,\nassume the (absolute) path name root:[nl,vu,cs,ftp,pub,globe ,index .html ]is to\nbe resolved. Using a URL notation, this path name would correspond to\nftp: / /ftp.cs.vu.nl/pub/globe /index .html . There are now two ways to imple-\nment name resolution.\nIniterative name resolution , a name resolver hands over the complete\nname to the root name server. It is assumed that the address where the root\nserver can be contacted is well known. The root server will resolve the path\nname as far as it can, and return the result to the client. In our example, the\nroot server can resolve only the label nl, for which it will return the address of\nthe associated name server.\nAt that point, the client passes the remaining path name (i.e., nl:[vu,cs,ftp,\npub,globe ,index .html ]) to that name server. This server can resolve only the\nlabel vu, and returns the address of the associated name server, along with\nthe remaining path name vu:[cs,ftp,pub,globe ,index .html ].\nThe client\u2019s name resolver will then contact this next name server, which\nresponds by resolving the label cs, and subsequently also ftp, returning the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "268 CHAPTER 5. NAMING\naddress of the FTP server along with the path name ftp:[pub,globe ,index .html ].\nThe client then contacts the FTP server, requesting it to resolve the last part of\nthe original path name. The FTP server will subsequently resolve the labels\npub,globe , and index .html , and transfer the requested \ufb01le (in this case using\nFTP). This process of iterative name resolution is shown in Figure 5.17. (The\nnotation # [cs]is used to indicate the address of the server responsible for\nhandling the node referred to by [ cs].)\nFigure 5.17: The principle of iterative name resolution.\nIn practice, the last step, namely contacting the FTP server and requesting\nit to transfer the \ufb01le with path name ftp:[pub,globe ,index .html ], is carried out\nseparately by the client process. In other words, the client would normally\nhand only the path name root:[nl,vu,cs,ftp]to the name resolver, from which\nit would expect the address where it can contact the FTP server, as is also\nshown in Figure 5.17\nAn alternative to iterative name resolution is to use recursion during\nname resolution. Instead of returning each intermediate result back to\nthe client\u2019s name resolver, with recursive name resolution , a name server\npasses the result to the next name server it \ufb01nds. So, for example, when\nthe root name server \ufb01nds the address of the name server implementing\nthe node named nl, it requests that name server to resolve the path name\nnl:[vu,cs,ftp,pub,globe ,index .html ]. Using recursive name resolution as well,\nthis next server will resolve the complete path and eventually return the \ufb01le\nindex .html . to the root server, which, in turn, will pass that \ufb01le to the client\u2019s\nname resolver.\nRecursive name resolution is shown in Figure 5.18. As in iterative name\nresolution, the last step (contacting the FTP server and asking it to transfer\nthe indicated \ufb01le) is generally carried out as a separate process by the client.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n268 CHAPTER 5. NAMING\naddress of the FTP server along with the path name ftp:[pub,globe ,index .html ].\nThe client then contacts the FTP server, requesting it to resolve the last part of\nthe original path name. The FTP server will subsequently resolve the labels\npub,globe , and index .html , and transfer the requested \ufb01le (in this case using\nFTP). This process of iterative name resolution is shown in Figure 5.17. (The\nnotation # [cs]is used to indicate the address of the server responsible for\nhandling the node referred to by [ cs].)\nFigure 5.17: The principle of iterative name resolution.\nIn practice, the last step, namely contacting the FTP server and requesting\nit to transfer the \ufb01le with path name ftp:[pub,globe ,index .html ], is carried out\nseparately by the client process. In other words, the client would normally\nhand only the path name root:[nl,vu,cs,ftp]to the name resolver, from which\nit would expect the address where it can contact the FTP server, as is also\nshown in Figure 5.17\nAn alternative to iterative name resolution is to use recursion during\nname resolution. Instead of returning each intermediate result back to\nthe client\u2019s name resolver, with recursive name resolution , a name server\npasses the result to the next name server it \ufb01nds. So, for example, when\nthe root name server \ufb01nds the address of the name server implementing\nthe node named nl, it requests that name server to resolve the path name\nnl:[vu,cs,ftp,pub,globe ,index .html ]. Using recursive name resolution as well,\nthis next server will resolve the complete path and eventually return the \ufb01le\nindex .html . to the root server, which, in turn, will pass that \ufb01le to the client\u2019s\nname resolver.\nRecursive name resolution is shown in Figure 5.18. As in iterative name\nresolution, the last step (contacting the FTP server and asking it to transfer\nthe indicated \ufb01le) is generally carried out as a separate process by the client.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 269\nFigure 5.18: The principle of recursive name resolution.\nThe main drawback of recursive name resolution is that it puts a higher\nperformance demand on each name server. Basically, a name server is required\nto handle the complete resolution of a path name, although it may do so in\ncooperation with other name servers. This additional burden is generally\nso high that name servers in the global layer of a name space support only\niterative name resolution.\nThere are two important advantages to recursive name resolution. The \ufb01rst\nadvantage is that caching results is more effective compared to iterative name\nresolution. The second advantage is that communication costs may be reduced.\nTo explain these advantages, assume that a client\u2019s name resolver will accept\npath names referring only to nodes in the global or administrational layer\nof the name space. To resolve that part of a path name that corresponds to\nnodes in the managerial layer, a client will separately contact the name server\nreturned by its name resolver, as we discussed above.\nRecursive name resolution allows each name server to gradually learn\nthe address of each name server responsible for implementing lower-level\nnodes. As a result, caching can be effectively used to enhance performance.\nFor example, when the root server is requested to resolve the path name\nroot:[nl,vu,cs,ftp], it will eventually get the address of the name server imple-\nmenting the node referred to by that path name. To come to that point, the\nname server for the nlnode has to look up the address of the name server for\nthevunode, whereas the latter has to look up the address of the name server\nhandling the csnode.\nBecause changes to nodes in the global and administrational layer do not\noccur often, the root name server can effectively cache the returned address.\nMoreover, because the address is also returned, by recursion, to the name\nserver responsible for implementing the vunode and to the one implementing\nthenlnode, it might as well be cached at those servers too.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 269\nFigure 5.18: The principle of recursive name resolution.\nThe main drawback of recursive name resolution is that it puts a higher\nperformance demand on each name server. Basically, a name server is required\nto handle the complete resolution of a path name, although it may do so in\ncooperation with other name servers. This additional burden is generally\nso high that name servers in the global layer of a name space support only\niterative name resolution.\nThere are two important advantages to recursive name resolution. The \ufb01rst\nadvantage is that caching results is more effective compared to iterative name\nresolution. The second advantage is that communication costs may be reduced.\nTo explain these advantages, assume that a client\u2019s name resolver will accept\npath names referring only to nodes in the global or administrational layer\nof the name space. To resolve that part of a path name that corresponds to\nnodes in the managerial layer, a client will separately contact the name server\nreturned by its name resolver, as we discussed above.\nRecursive name resolution allows each name server to gradually learn\nthe address of each name server responsible for implementing lower-level\nnodes. As a result, caching can be effectively used to enhance performance.\nFor example, when the root server is requested to resolve the path name\nroot:[nl,vu,cs,ftp], it will eventually get the address of the name server imple-\nmenting the node referred to by that path name. To come to that point, the\nname server for the nlnode has to look up the address of the name server for\nthevunode, whereas the latter has to look up the address of the name server\nhandling the csnode.\nBecause changes to nodes in the global and administrational layer do not\noccur often, the root name server can effectively cache the returned address.\nMoreover, because the address is also returned, by recursion, to the name\nserver responsible for implementing the vunode and to the one implementing\nthenlnode, it might as well be cached at those servers too.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "270 CHAPTER 5. NAMING\nLikewise, the results of intermediate name lookups can also be returned\nand cached. For example, the server for the nlnode will have to look up the\naddress of the vunode server. That address can be returned to the root server\nwhen the nlserver returns the result of the original name lookup. A complete\noverview of the resolution process, and the results that can be cached by each\nname server is shown in Figure 5.19.\nServer Should Looks up Passes to Receives Returns\nfor node resolve child and caches to requester\ncs [ftp] #[ftp] \u2014 \u2014 #[ftp]\nvu [cs,ftp] #[cs] [ftp] #[ftp] #[cs]\n#[cs,ftp]\nnl [vu,cs,ftp] #[vu] [cs,ftp] #[cs] #[vu]\n#[cs,ftp] #[vu,cs]\n#[vu,cs,ftp]\nroot [nl,vu,cs,ftp]#[nl] [vu,cs,ftp] #[vu] #[nl]\n#[vu,cs] #[nl,vu]\n#[vu,cs,ftp]#[nl,vu,cs]\n#[nl,vu,cs,ftp]\nFigure 5.19: Recursive name resolution of [nl,vu,cs,ftp]. Name servers cache\nintermediate results for subsequent lookups.\nThe main bene\ufb01t of this approach is that, eventually, lookup operations\ncan be handled quite ef\ufb01ciently. For example, suppose that another client later\nrequests resolution of the path name root:[nl,vu,cs,\rits]. This name is passed\nto the root, which can immediately forward it to the name server for the cs\nnode, and request it to resolve the remaining path name cs:[\rits].\nWith iterative name resolution, caching is necessarily restricted to the\nclient\u2019s name resolver. Consequently, if a client Arequests the resolution of a\nname, and another client Blater requests that same name to be resolved, name\nresolution will have to pass through the same name servers as was done for\nclient A. As a compromise, many organizations use a local, intermediate name\nserver that is shared by all clients. This local name server handles all naming\nrequests and caches results. Such an intermediate server is also convenient\nfrom a management point of view. For example, only that server needs to\nknow where the root name server is located; other machines do not require\nthis information.\nThe second advantage of recursive name resolution is that it is often\ncheaper with respect to communication. Again, consider the resolution of the\npath name root:[nl,vu,cs,ftp]and assume the client is located in San Francisco.\nAssuming that the client knows the address of the server for the nlnode,\nwith recursive name resolution, communication follows the route from the\nclient\u2019s host in San Francisco to the nlserver in The Netherlands, shown\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n270 CHAPTER 5. NAMING\nLikewise, the results of intermediate name lookups can also be returned\nand cached. For example, the server for the nlnode will have to look up the\naddress of the vunode server. That address can be returned to the root server\nwhen the nlserver returns the result of the original name lookup. A complete\noverview of the resolution process, and the results that can be cached by each\nname server is shown in Figure 5.19.\nServer Should Looks up Passes to Receives Returns\nfor node resolve child and caches to requester\ncs [ftp] #[ftp] \u2014 \u2014 #[ftp]\nvu [cs,ftp] #[cs] [ftp] #[ftp] #[cs]\n#[cs,ftp]\nnl [vu,cs,ftp] #[vu] [cs,ftp] #[cs] #[vu]\n#[cs,ftp] #[vu,cs]\n#[vu,cs,ftp]\nroot [nl,vu,cs,ftp]#[nl] [vu,cs,ftp] #[vu] #[nl]\n#[vu,cs] #[nl,vu]\n#[vu,cs,ftp]#[nl,vu,cs]\n#[nl,vu,cs,ftp]\nFigure 5.19: Recursive name resolution of [nl,vu,cs,ftp]. Name servers cache\nintermediate results for subsequent lookups.\nThe main bene\ufb01t of this approach is that, eventually, lookup operations\ncan be handled quite ef\ufb01ciently. For example, suppose that another client later\nrequests resolution of the path name root:[nl,vu,cs,\rits]. This name is passed\nto the root, which can immediately forward it to the name server for the cs\nnode, and request it to resolve the remaining path name cs:[\rits].\nWith iterative name resolution, caching is necessarily restricted to the\nclient\u2019s name resolver. Consequently, if a client Arequests the resolution of a\nname, and another client Blater requests that same name to be resolved, name\nresolution will have to pass through the same name servers as was done for\nclient A. As a compromise, many organizations use a local, intermediate name\nserver that is shared by all clients. This local name server handles all naming\nrequests and caches results. Such an intermediate server is also convenient\nfrom a management point of view. For example, only that server needs to\nknow where the root name server is located; other machines do not require\nthis information.\nThe second advantage of recursive name resolution is that it is often\ncheaper with respect to communication. Again, consider the resolution of the\npath name root:[nl,vu,cs,ftp]and assume the client is located in San Francisco.\nAssuming that the client knows the address of the server for the nlnode,\nwith recursive name resolution, communication follows the route from the\nclient\u2019s host in San Francisco to the nlserver in The Netherlands, shown\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 271\nasR1in Figure 5.20 From there on, communication is subsequently needed\nbetween the nlserver and the name server of VU University on the campus in\nAmsterdam, The Netherlands. This communication is shown as R2. Finally,\ncommunication is needed between the vuserver and the name server in the\nComputer Science Department, shown as R3. The route for the reply is the\nsame, but in the opposite direction. Clearly, communication costs are dictated\nby the message exchange between the client\u2019s host and the nlserver.\nIn contrast, with iterative name resolution, the client\u2019s host has to commu-\nnicate separately with the nlserver, the vuserver, and the csserver, of which\nthe total costs may be roughly three times that of recursive name resolution.\nThe arrows in Figure 5.20 labeled I1,I2, and I3show the communication path\nfor iterative name resolution.\nFigure 5.20: The comparison between recursive and iterative name resolution\nwith respect to communication costs.\nExample: The Domain Name System\nOne of the largest distributed naming services in use today is the Internet\nDomain Name System (DNS ). DNS is primarily used for looking up IP\naddresses of hosts and mail servers. In the following pages, we concentrate\non the organization of the DNS name space and the information stored in its\nnodes. Also, we take a closer look at the actual implementation of DNS. More\ninformation can be found in [Mockapetris, 1987] and [Liu and Albitz, 2006].\nAn assessment of DNS, notably concerning whether it still \ufb01ts the needs of\nthe current Internet, can be found in [Levien, 2005]. From this report, one can\ndraw the somewhat surprising conclusion that even after more than 30 years,\nDNS gives no indication that it needs to be replaced. We would argue that the\nmain cause lies in the designer\u2019s deep understanding of how to keep matters\nsimple. Practice in other \ufb01elds of distributed systems indicates that not many\nare gifted with such an understanding.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 271\nasR1in Figure 5.20 From there on, communication is subsequently needed\nbetween the nlserver and the name server of VU University on the campus in\nAmsterdam, The Netherlands. This communication is shown as R2. Finally,\ncommunication is needed between the vuserver and the name server in the\nComputer Science Department, shown as R3. The route for the reply is the\nsame, but in the opposite direction. Clearly, communication costs are dictated\nby the message exchange between the client\u2019s host and the nlserver.\nIn contrast, with iterative name resolution, the client\u2019s host has to commu-\nnicate separately with the nlserver, the vuserver, and the csserver, of which\nthe total costs may be roughly three times that of recursive name resolution.\nThe arrows in Figure 5.20 labeled I1,I2, and I3show the communication path\nfor iterative name resolution.\nFigure 5.20: The comparison between recursive and iterative name resolution\nwith respect to communication costs.\nExample: The Domain Name System\nOne of the largest distributed naming services in use today is the Internet\nDomain Name System (DNS ). DNS is primarily used for looking up IP\naddresses of hosts and mail servers. In the following pages, we concentrate\non the organization of the DNS name space and the information stored in its\nnodes. Also, we take a closer look at the actual implementation of DNS. More\ninformation can be found in [Mockapetris, 1987] and [Liu and Albitz, 2006].\nAn assessment of DNS, notably concerning whether it still \ufb01ts the needs of\nthe current Internet, can be found in [Levien, 2005]. From this report, one can\ndraw the somewhat surprising conclusion that even after more than 30 years,\nDNS gives no indication that it needs to be replaced. We would argue that the\nmain cause lies in the designer\u2019s deep understanding of how to keep matters\nsimple. Practice in other \ufb01elds of distributed systems indicates that not many\nare gifted with such an understanding.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "272 CHAPTER 5. NAMING\nThe DNS name space\nThe DNS name space is hierarchically organized as a rooted tree. A label is\na case-insensitive string made up of alphanumeric characters. A label has\na maximum length of 63 characters; the length of a complete path name is\nrestricted to 255 characters. The string representation of a path name consists\nof listing its labels, starting with the rightmost one, and separating the labels\nby a dot (\u201c.\u201d). The root is represented by a dot. So, for example, the path\nname root:[nl,vu,cs,\rits], is represented by the string \u201c \rits.cs.vu.nl.\u201d, which\nincludes the rightmost dot to indicate the root node. We generally omit this\ndot for readability.\nBecause each node in the DNS name space has exactly one incoming edge\n(with the exception of the root node, which has no incoming edges), the label\nattached to a node\u2019s incoming edge is also used as the name for that node. A\nsubtree is called a domain a path name to its root node is called a domain\nname . Note that, just like a path name, a domain name can be either absolute\nor relative.\nThe contents of a node is formed by a collection of resource records .\nThere are different types of resource records. The major ones are shown in\nFigure 5.21.\nType Refers to Description\nSOA Zone Holds info on the represented zone\nA Host IP addr. of host this node represents\nMX Domain Mail server to handle mail for this node\nSRV Domain Server handling a speci\ufb01c service\nNS Zone Name server for the represented zone\nCNAME Node Symbolic link\nPTR Host Canonical name of a host\nHINFO Host Info on this host\nTXT Any kind Any info considered useful\nFigure 5.21: The most important types of resource records forming the con-\ntents of nodes in the DNS name space.\nA node in the DNS name space will often represent several entities at the\nsame time. For example, a domain name such as vu.nlis used to represent\na domain and a zone. In this case, the domain is implemented by means of\nseveral (nonoverlapping) zones.\nAnSOA (start of authority) resource record contains information such as\nan e-mail address of the system administrator responsible for the represented\nzone, the name of the host where data on the zone can be fetched, and so on.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n272 CHAPTER 5. NAMING\nThe DNS name space\nThe DNS name space is hierarchically organized as a rooted tree. A label is\na case-insensitive string made up of alphanumeric characters. A label has\na maximum length of 63 characters; the length of a complete path name is\nrestricted to 255 characters. The string representation of a path name consists\nof listing its labels, starting with the rightmost one, and separating the labels\nby a dot (\u201c.\u201d). The root is represented by a dot. So, for example, the path\nname root:[nl,vu,cs,\rits], is represented by the string \u201c \rits.cs.vu.nl.\u201d, which\nincludes the rightmost dot to indicate the root node. We generally omit this\ndot for readability.\nBecause each node in the DNS name space has exactly one incoming edge\n(with the exception of the root node, which has no incoming edges), the label\nattached to a node\u2019s incoming edge is also used as the name for that node. A\nsubtree is called a domain a path name to its root node is called a domain\nname . Note that, just like a path name, a domain name can be either absolute\nor relative.\nThe contents of a node is formed by a collection of resource records .\nThere are different types of resource records. The major ones are shown in\nFigure 5.21.\nType Refers to Description\nSOA Zone Holds info on the represented zone\nA Host IP addr. of host this node represents\nMX Domain Mail server to handle mail for this node\nSRV Domain Server handling a speci\ufb01c service\nNS Zone Name server for the represented zone\nCNAME Node Symbolic link\nPTR Host Canonical name of a host\nHINFO Host Info on this host\nTXT Any kind Any info considered useful\nFigure 5.21: The most important types of resource records forming the con-\ntents of nodes in the DNS name space.\nA node in the DNS name space will often represent several entities at the\nsame time. For example, a domain name such as vu.nlis used to represent\na domain and a zone. In this case, the domain is implemented by means of\nseveral (nonoverlapping) zones.\nAnSOA (start of authority) resource record contains information such as\nan e-mail address of the system administrator responsible for the represented\nzone, the name of the host where data on the zone can be fetched, and so on.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 273\nAnA(address) record, represents a particular host in the Internet. The A\nrecord contains an IP address for that host to allow communication. If a host\nhas several IP addresses, as is the case with multi-homed machines, the node\nwill contain an Arecord for each address.\nAnother type of record is the MX(mail exchange) record, which is like\na symbolic link to a node representing a mail server. For example, the\nnode representing the domain cs.vu.nlhas an MXrecord containing the name\nzephyr .cs.vu.nlwhich refers to a mail server. That server will handle all incom-\ning mail addressed to users in the cs.vu.nldomain. There may be several MX\nrecords stored in a node.\nRelated to MXrecords are SRV records, which contain the name of a server\nfor a speci\ufb01c service. The service itself is identi\ufb01ed by means of a name\nalong with the name of a protocol. For example, the Web server in the cs.vu.nl\ndomain could be named by means of an SRV record such as _http_tcp.cs.vu.nl.\nThis record would then refer to the actual name of the server (which is\nsoling .cs.vu.nl). An important advantage of SRV records is that clients need no\nlonger know the DNS name of the host providing a speci\ufb01c service. Instead,\nonly service names need to be standardized, after which the providing host\ncan be looked up.\nNodes that represent a zone, contain one or more NS(name server) records.\nLike MX records, an NSrecord contains the name of a name server that\nimplements the zone represented by the node. In principle, each node in\nthe name space can store an NSrecord referring to the name server that\nimplements it. However, as we discuss below, the implementation of the\nDNS name space is such that only nodes representing zones need to store NS\nrecords.\nDNS distinguishes aliases from what are called canonical names . Each\nhost is assumed to have a canonical, or primary name. An alias is implemented\nby means of node storing a CNAME record containing the canonical name\nof a host. The name of the node storing such a record is thus the same as a\nsymbolic link, as was shown in Figure 5.13.\nDNS maintains an inverse mapping of IP addresses to host names by\nmeans of PTR (pointer) records. To accommodate the lookups of host names\nwhen given only an IP address, DNS maintains a domain named in-addr.arpa,\nwhich contains nodes that represent Internet hosts and which are named by\nthe IP address of the represented host. For example, host www .cs.vu.nlhas\nIP address 130.37.20.20. DNS creates a node named 20.20.37.130.in-addr.arpa,\nwhich is used to store the canonical name of that host (which happens to be\nsoling .cs.vu.nlin a PTR record).\nFinally, an HINFO (host info) record is used to store additional information\non a host such as its machine type and operating system. In a similar fashion,\nTXT records are used for any other kind of data that a user \ufb01nds useful to\nstore about the entity represented by the node.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 273\nAnA(address) record, represents a particular host in the Internet. The A\nrecord contains an IP address for that host to allow communication. If a host\nhas several IP addresses, as is the case with multi-homed machines, the node\nwill contain an Arecord for each address.\nAnother type of record is the MX(mail exchange) record, which is like\na symbolic link to a node representing a mail server. For example, the\nnode representing the domain cs.vu.nlhas an MXrecord containing the name\nzephyr .cs.vu.nlwhich refers to a mail server. That server will handle all incom-\ning mail addressed to users in the cs.vu.nldomain. There may be several MX\nrecords stored in a node.\nRelated to MXrecords are SRV records, which contain the name of a server\nfor a speci\ufb01c service. The service itself is identi\ufb01ed by means of a name\nalong with the name of a protocol. For example, the Web server in the cs.vu.nl\ndomain could be named by means of an SRV record such as _http_tcp.cs.vu.nl.\nThis record would then refer to the actual name of the server (which is\nsoling .cs.vu.nl). An important advantage of SRV records is that clients need no\nlonger know the DNS name of the host providing a speci\ufb01c service. Instead,\nonly service names need to be standardized, after which the providing host\ncan be looked up.\nNodes that represent a zone, contain one or more NS(name server) records.\nLike MX records, an NSrecord contains the name of a name server that\nimplements the zone represented by the node. In principle, each node in\nthe name space can store an NSrecord referring to the name server that\nimplements it. However, as we discuss below, the implementation of the\nDNS name space is such that only nodes representing zones need to store NS\nrecords.\nDNS distinguishes aliases from what are called canonical names . Each\nhost is assumed to have a canonical, or primary name. An alias is implemented\nby means of node storing a CNAME record containing the canonical name\nof a host. The name of the node storing such a record is thus the same as a\nsymbolic link, as was shown in Figure 5.13.\nDNS maintains an inverse mapping of IP addresses to host names by\nmeans of PTR (pointer) records. To accommodate the lookups of host names\nwhen given only an IP address, DNS maintains a domain named in-addr.arpa,\nwhich contains nodes that represent Internet hosts and which are named by\nthe IP address of the represented host. For example, host www .cs.vu.nlhas\nIP address 130.37.20.20. DNS creates a node named 20.20.37.130.in-addr.arpa,\nwhich is used to store the canonical name of that host (which happens to be\nsoling .cs.vu.nlin a PTR record).\nFinally, an HINFO (host info) record is used to store additional information\non a host such as its machine type and operating system. In a similar fashion,\nTXT records are used for any other kind of data that a user \ufb01nds useful to\nstore about the entity represented by the node.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "274 CHAPTER 5. NAMING\nDNS implementation\nIn essence, the DNS name space can be divided into a global layer and an\nadministrational layer as shown in Figure 5.15. The managerial layer, which\nis generally formed by local \ufb01le systems, is formally not part of DNS and is\ntherefore also not managed by it.\nEach zone is implemented by a name server, which is virtually always\nreplicated for availability. Updates for a zone are normally handled by the\nprimary name server. Updates take place by modifying the DNS database\nlocal to the primary server. Secondary name servers do not access the database\ndirectly, but, instead, request the primary server to transfer its content. The\nlatter is called a zone transfer in DNS terminology.\nA DNS database is implemented as a (small) collection of \ufb01les, of which\nthe most important one contains all the resource records for allthe nodes in a\nparticular zone. This approach allows nodes to be simply identi\ufb01ed by means\nof their domain name, by which the notion of a node identi\ufb01er reduces to an\n(implicit) index into a \ufb01le.\nNote 5.11 (More information: An example DNS database)\nTo better understand these implementation issues, Figure 5.22 shows a small part\nof the \ufb01le that contains most of the information for a previous organization of the\ncs.vu.nldomain. Note that we have deliberately chosen an outdated version for\nsecurity reasons. The \ufb01le has been edited for readability. It shows the content of\nseveral nodes that are part of the cs.vu.nldomain, where each node is identi\ufb01ed\nby means of its domain name.\nThe node cs.vu.nlrepresents the domain as well as the zone. Its SOA resource\nrecord contains speci\ufb01c information on the validity of this \ufb01le, which will not\nconcern us further. There are four name servers for this zone, referred to by their\ncanonical host names in the NSrecords. The TXT record is used to give some\nadditional information on this zone, but cannot be automatically processed by any\nname server. Furthermore, there is a single mail server that can handle incoming\nmail addressed to users in this domain. The number preceding the name of a\nmail server speci\ufb01es a selection priority. A sending mail server should always\n\ufb01rst attempt to contact the mail server with the lowest number.\nThe host star.cs.vu.nloperates as a name server for this zone. Name servers\nare critical to any naming service. What can be seen about this name server\nis that additional robustness has been created by giving two separate network\ninterfaces, each represented by a separate Aresource record. In this way, the\neffects of a broken network link can be somewhat alleviated as the server will\nremain accessible.\nThe next four lines (for zephyr .cs.vu.nl) give the necessary information about\none of the department\u2019s mail servers. Note that this mail server is also backed up\nby another mail server, whose path is tornado .cs.vu.nl.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n274 CHAPTER 5. NAMING\nDNS implementation\nIn essence, the DNS name space can be divided into a global layer and an\nadministrational layer as shown in Figure 5.15. The managerial layer, which\nis generally formed by local \ufb01le systems, is formally not part of DNS and is\ntherefore also not managed by it.\nEach zone is implemented by a name server, which is virtually always\nreplicated for availability. Updates for a zone are normally handled by the\nprimary name server. Updates take place by modifying the DNS database\nlocal to the primary server. Secondary name servers do not access the database\ndirectly, but, instead, request the primary server to transfer its content. The\nlatter is called a zone transfer in DNS terminology.\nA DNS database is implemented as a (small) collection of \ufb01les, of which\nthe most important one contains all the resource records for allthe nodes in a\nparticular zone. This approach allows nodes to be simply identi\ufb01ed by means\nof their domain name, by which the notion of a node identi\ufb01er reduces to an\n(implicit) index into a \ufb01le.\nNote 5.11 (More information: An example DNS database)\nTo better understand these implementation issues, Figure 5.22 shows a small part\nof the \ufb01le that contains most of the information for a previous organization of the\ncs.vu.nldomain. Note that we have deliberately chosen an outdated version for\nsecurity reasons. The \ufb01le has been edited for readability. It shows the content of\nseveral nodes that are part of the cs.vu.nldomain, where each node is identi\ufb01ed\nby means of its domain name.\nThe node cs.vu.nlrepresents the domain as well as the zone. Its SOA resource\nrecord contains speci\ufb01c information on the validity of this \ufb01le, which will not\nconcern us further. There are four name servers for this zone, referred to by their\ncanonical host names in the NSrecords. The TXT record is used to give some\nadditional information on this zone, but cannot be automatically processed by any\nname server. Furthermore, there is a single mail server that can handle incoming\nmail addressed to users in this domain. The number preceding the name of a\nmail server speci\ufb01es a selection priority. A sending mail server should always\n\ufb01rst attempt to contact the mail server with the lowest number.\nThe host star.cs.vu.nloperates as a name server for this zone. Name servers\nare critical to any naming service. What can be seen about this name server\nis that additional robustness has been created by giving two separate network\ninterfaces, each represented by a separate Aresource record. In this way, the\neffects of a broken network link can be somewhat alleviated as the server will\nremain accessible.\nThe next four lines (for zephyr .cs.vu.nl) give the necessary information about\none of the department\u2019s mail servers. Note that this mail server is also backed up\nby another mail server, whose path is tornado .cs.vu.nl.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 275\nName Record type Record value\ncs.vu.nl. SOA star.cs.vu.nl.hostmaster .cs.vu.nl.\n2005092900 7200 3600 2419200 3600\ncs.vu.nl. TXT \u201cVU University - Computer Science\u201d\ncs.vu.nl. MX 1mail.few.vu.nl.\ncs.vu.nl. NS ns.vu.nl.\ncs.vu.nl. NS top.cs.vu.nl.\ncs.vu.nl. NS solo.cs.vu.nl.\ncs.vu.nl. NS star.cs.vu.nl.\nstar.cs.vu.nl. A 130.37.24.6\nstar.cs.vu.nl. A 192.31.231.42\nstar.cs.vu.nl. MX 1star.cs.vu.nl.\nstar.cs.vu.nl. MX 666zephyr .cs.vu.nl.\nstar.cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nzephyr .cs.vu.nl. A 130.37.20.10\nzephyr .cs.vu.nl. MX 1zephyr .cs.vu.nl.\nzephyr .cs.vu.nl. MX 2tornado .cs.vu.nl.\nzephyr .cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nftp.cs.vu.nl. CNAME soling .cs.vu.nl.\nwww .cs.vu.nl. CNAME soling .cs.vu.nl.\nsoling .cs.vu.nl. A 130.37.20.20\nsoling .cs.vu.nl. MX 1soling .cs.vu.nl.\nsoling .cs.vu.nl. MX 666zephyr .cs.vu.nl.\nsoling .cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nvucs-das1 .cs.vu.nl.PTR 0.198.37.130.in-addr .arpa.\nvucs-das1 .cs.vu.nl.A 130.37.198.0\ninkt.cs.vu.nl. HINFO \u201cOCE\u201d \u201cProprietary\u201d\ninkt.cs.vu.nl. A 192.168.4.3\npen.cs.vu.nl. HINFO \u201cOCE\u201d \u201cProprietary\u201d\npen.cs.vu.nl. A 192.168.4.2\nlocalhost .cs.vu.nl.A 127.0.0.1\nFigure 5.22: An excerpt from an (old) DNS database for the zone cs.vu.nl.\nThe next six lines show a typical con\ufb01guration in which the department\u2019s\nWeb server, as well as the department\u2019s FTP server are implemented by a single\nmachine, called soling .cs.vu.nl. By executing both servers on the same machine\n(and essentially using that machine only for Internet services and not anything\nelse), system management becomes easier. For example, both servers will have\nthe same view of the \ufb01le system, and for ef\ufb01ciency, part of the \ufb01le system may\nbe implemented on soling .cs.vu.nl. This approach is often applied in the case of\nWWW and FTP services.\nThe following two lines show information on one of the department\u2019s older\nserver clusters. In this case, it tells us that the address 130.37.198.0is associated\nwith the host name vucs-das1 .cs.vu.nl.\nThe next four lines show information on two major printers connected to the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 275\nName Record type Record value\ncs.vu.nl. SOA star.cs.vu.nl.hostmaster .cs.vu.nl.\n2005092900 7200 3600 2419200 3600\ncs.vu.nl. TXT \u201cVU University - Computer Science\u201d\ncs.vu.nl. MX 1mail.few.vu.nl.\ncs.vu.nl. NS ns.vu.nl.\ncs.vu.nl. NS top.cs.vu.nl.\ncs.vu.nl. NS solo.cs.vu.nl.\ncs.vu.nl. NS star.cs.vu.nl.\nstar.cs.vu.nl. A 130.37.24.6\nstar.cs.vu.nl. A 192.31.231.42\nstar.cs.vu.nl. MX 1star.cs.vu.nl.\nstar.cs.vu.nl. MX 666zephyr .cs.vu.nl.\nstar.cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nzephyr .cs.vu.nl. A 130.37.20.10\nzephyr .cs.vu.nl. MX 1zephyr .cs.vu.nl.\nzephyr .cs.vu.nl. MX 2tornado .cs.vu.nl.\nzephyr .cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nftp.cs.vu.nl. CNAME soling .cs.vu.nl.\nwww .cs.vu.nl. CNAME soling .cs.vu.nl.\nsoling .cs.vu.nl. A 130.37.20.20\nsoling .cs.vu.nl. MX 1soling .cs.vu.nl.\nsoling .cs.vu.nl. MX 666zephyr .cs.vu.nl.\nsoling .cs.vu.nl. HINFO \u201cSun\u201d \u201cUnix\u201d\nvucs-das1 .cs.vu.nl.PTR 0.198.37.130.in-addr .arpa.\nvucs-das1 .cs.vu.nl.A 130.37.198.0\ninkt.cs.vu.nl. HINFO \u201cOCE\u201d \u201cProprietary\u201d\ninkt.cs.vu.nl. A 192.168.4.3\npen.cs.vu.nl. HINFO \u201cOCE\u201d \u201cProprietary\u201d\npen.cs.vu.nl. A 192.168.4.2\nlocalhost .cs.vu.nl.A 127.0.0.1\nFigure 5.22: An excerpt from an (old) DNS database for the zone cs.vu.nl.\nThe next six lines show a typical con\ufb01guration in which the department\u2019s\nWeb server, as well as the department\u2019s FTP server are implemented by a single\nmachine, called soling .cs.vu.nl. By executing both servers on the same machine\n(and essentially using that machine only for Internet services and not anything\nelse), system management becomes easier. For example, both servers will have\nthe same view of the \ufb01le system, and for ef\ufb01ciency, part of the \ufb01le system may\nbe implemented on soling .cs.vu.nl. This approach is often applied in the case of\nWWW and FTP services.\nThe following two lines show information on one of the department\u2019s older\nserver clusters. In this case, it tells us that the address 130.37.198.0is associated\nwith the host name vucs-das1 .cs.vu.nl.\nThe next four lines show information on two major printers connected to the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "276 CHAPTER 5. NAMING\nlocal network. Note that addresses in the range 192.168.0.0to192.168.255.255\nare private: they can be accessed only from inside the local network and are not\naccessible from an arbitrary Internet host.\nName Record type Record value\ncs.vu.nl. NS solo.cs.vu.nl.\ncs.vu.nl. NS star.cs.vu.nl.\ncs.vu.nl. NS ns.vu.nl.\ncs.vu.nl. NS top.cs.vu.nl.\nns.vu.nl. A 130.37.129.4\ntop.cs.vu.nl.A 130.37.20.4\nsolo.cs.vu.nl.A 130.37.20.5\nstar.cs.vu.nl.A 130.37.24.6\nstar.cs.vu.nl.A 192.31.231.42\nFigure 5.23: Part of the description for the vu.nldomain which contains\nthecs.vu.nldomain.\nBecause the cs.vu.nldomain is implemented as a single zone, Figure 5.22 does\nnot include references to other zones. The way to refer to nodes in a subdomain\nthat are implemented in a different zone is shown in Figure 5.23. What needs\nto be done is to specify a name server for the subdomain by simply giving its\ndomain name and IP address. When resolving a name for a node that lies in the\ncs.vu.nldomain, name resolution will continue at a certain point by reading the\nDNS database stored by the name server for the cs.vu.nldomain.\nNote 5.12 (Advanced: Decentralized versus hierarchical DNS implementations)\nThe implementation of DNS we described so far is the standard one. It follows\na hierarchy of servers with 13 well-known root nodes and ending in millions of\nservers at the leaves (but read on). An important observation is that higher-level\nnodes receive many more requests than lower-level nodes. Only by caching the\nname-to-address bindings of these higher levels, it becomes possible to avoid\nsending requests to them and thus swamping them.\nThese scalability problems can, in principle, be avoided altogether with fully\ndecentralized solutions. In particular, we can compute the hash of a DNS name,\nand subsequently take that hash as a key value to be looked up in a distributed\nhash table or a hierarchical location service with a fully partitioned root node.\nThe obvious drawback of this approach is that we lose the structure of the original\nname. This loss may prevent ef\ufb01cient implementations of, for example, \ufb01nding all\nchildren in a speci\ufb01c domain.\nAs argued by Wal\ufb01sh et al. [2004], when there is a need for many names,\nusing identi\ufb01ers as a semantic-free way of accessing data will allow different\nsystems to make use of a single naming system. The reason is simple: by now\nit is well understood how a huge collection of (\ufb02at) names can be ef\ufb01ciently\nsupported. What needs to be done is to maintain the mapping of identi\ufb01er-to-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n276 CHAPTER 5. NAMING\nlocal network. Note that addresses in the range 192.168.0.0to192.168.255.255\nare private: they can be accessed only from inside the local network and are not\naccessible from an arbitrary Internet host.\nName Record type Record value\ncs.vu.nl. NS solo.cs.vu.nl.\ncs.vu.nl. NS star.cs.vu.nl.\ncs.vu.nl. NS ns.vu.nl.\ncs.vu.nl. NS top.cs.vu.nl.\nns.vu.nl. A 130.37.129.4\ntop.cs.vu.nl.A 130.37.20.4\nsolo.cs.vu.nl.A 130.37.20.5\nstar.cs.vu.nl.A 130.37.24.6\nstar.cs.vu.nl.A 192.31.231.42\nFigure 5.23: Part of the description for the vu.nldomain which contains\nthecs.vu.nldomain.\nBecause the cs.vu.nldomain is implemented as a single zone, Figure 5.22 does\nnot include references to other zones. The way to refer to nodes in a subdomain\nthat are implemented in a different zone is shown in Figure 5.23. What needs\nto be done is to specify a name server for the subdomain by simply giving its\ndomain name and IP address. When resolving a name for a node that lies in the\ncs.vu.nldomain, name resolution will continue at a certain point by reading the\nDNS database stored by the name server for the cs.vu.nldomain.\nNote 5.12 (Advanced: Decentralized versus hierarchical DNS implementations)\nThe implementation of DNS we described so far is the standard one. It follows\na hierarchy of servers with 13 well-known root nodes and ending in millions of\nservers at the leaves (but read on). An important observation is that higher-level\nnodes receive many more requests than lower-level nodes. Only by caching the\nname-to-address bindings of these higher levels, it becomes possible to avoid\nsending requests to them and thus swamping them.\nThese scalability problems can, in principle, be avoided altogether with fully\ndecentralized solutions. In particular, we can compute the hash of a DNS name,\nand subsequently take that hash as a key value to be looked up in a distributed\nhash table or a hierarchical location service with a fully partitioned root node.\nThe obvious drawback of this approach is that we lose the structure of the original\nname. This loss may prevent ef\ufb01cient implementations of, for example, \ufb01nding all\nchildren in a speci\ufb01c domain.\nAs argued by Wal\ufb01sh et al. [2004], when there is a need for many names,\nusing identi\ufb01ers as a semantic-free way of accessing data will allow different\nsystems to make use of a single naming system. The reason is simple: by now\nit is well understood how a huge collection of (\ufb02at) names can be ef\ufb01ciently\nsupported. What needs to be done is to maintain the mapping of identi\ufb01er-to-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 277\nname information, where in this case a name may come from the DNS space,\nbe a URL, and so on. Using identi\ufb01ers can be made easier by letting users or\norganizations use a strict local name space. The latter is completely analogous to\nmaintaining a private setting of environment variables on a computer.\nNevertheless, stating that a decentralized implementation of DNS will cir-\ncumvent many of its scalability problems is too simple. In a comparative study,\nPappas et al. [2006] showed that there are many trade-offs to consider and that\nactually the current, hierarchical design of DNS is not so bad for at least two\nreasons:\n\u2022In a hierarchical design, not all nodes are equal and in the case of DNS,\nnotably the higher-level nodes are engineered differently. For example,\ndespite that there are of\ufb01cially 13 root nodes, each of these nodes is highly\ndistributed and replicated for performance and availability. To illustrate,\nthe root node provided by RIPE NCC is implemented at some 25 different\nsites (all using the same IP address), each implemented as a highly robust\nand replicated server cluster.\nAgain, we see the important difference between a logical and physical\ndesign. Exploiting this difference is crucial for the operation of a distributed\nsystem such as DNS. However, in virtually all DHT-based systems, making\nthis distinction can be much more dif\ufb01cult when dealing with a logical\nnaming hierarchy, as all names are necessarily treated to be equal. In such\ncases, it becomes much more dif\ufb01cult to engineer the system so that, for\nexample, top-level domains are separated out by special (physical) nodes.\nOf course, the obvious drawback of not having all nodes being equal, is\nthat special measures need to be taken to protect the more important parts\nof a system against abuse. We have already mentioned that top-level nodes\nin DNS are implemented as distributed and replicated server (clusters), but\nalso that an associated server will not provide recursive name resolution.\nSuch implementation decisions are necessary also from a perspective of\nrobustness.\n\u2022DNS caches are highly effective and driven almost entirely by the local\ndistribution of queries: if domain Dis queried often at server S, then the\nreferences for name servers of Dwill be cached at S. The behavior at another\nserver S0is determined by what is queried at S0. This important feature has\nbeen con\ufb01rmed in a more recent study that also shows how dif\ufb01cult it can\nbe to understand the effectiveness of caching and the locality principles of\nDNS resolvers. In particular, an ISP\u2019s DNS resolver may be very effective in\nredirecting traf\ufb01c to content that is localized in that ISP [Ager et al., 2010].\nIn contrast, caching and replication in DHT-based systems generally does\nnot show such principles of locality: results are simply cached at nodes on\nthe return path of a lookup and have very little to do with the fact that a\nlookup was locally initiated at a speci\ufb01c node in the DHT, or a resolver for\nwhich the local ISP can assist in looking up content.\nThe fact remains that replacing DNS by a decentralized implementation is not\nnecessarily a good idea. DNS as it stands today, is a well-engineered system that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 277\nname information, where in this case a name may come from the DNS space,\nbe a URL, and so on. Using identi\ufb01ers can be made easier by letting users or\norganizations use a strict local name space. The latter is completely analogous to\nmaintaining a private setting of environment variables on a computer.\nNevertheless, stating that a decentralized implementation of DNS will cir-\ncumvent many of its scalability problems is too simple. In a comparative study,\nPappas et al. [2006] showed that there are many trade-offs to consider and that\nactually the current, hierarchical design of DNS is not so bad for at least two\nreasons:\n\u2022In a hierarchical design, not all nodes are equal and in the case of DNS,\nnotably the higher-level nodes are engineered differently. For example,\ndespite that there are of\ufb01cially 13 root nodes, each of these nodes is highly\ndistributed and replicated for performance and availability. To illustrate,\nthe root node provided by RIPE NCC is implemented at some 25 different\nsites (all using the same IP address), each implemented as a highly robust\nand replicated server cluster.\nAgain, we see the important difference between a logical and physical\ndesign. Exploiting this difference is crucial for the operation of a distributed\nsystem such as DNS. However, in virtually all DHT-based systems, making\nthis distinction can be much more dif\ufb01cult when dealing with a logical\nnaming hierarchy, as all names are necessarily treated to be equal. In such\ncases, it becomes much more dif\ufb01cult to engineer the system so that, for\nexample, top-level domains are separated out by special (physical) nodes.\nOf course, the obvious drawback of not having all nodes being equal, is\nthat special measures need to be taken to protect the more important parts\nof a system against abuse. We have already mentioned that top-level nodes\nin DNS are implemented as distributed and replicated server (clusters), but\nalso that an associated server will not provide recursive name resolution.\nSuch implementation decisions are necessary also from a perspective of\nrobustness.\n\u2022DNS caches are highly effective and driven almost entirely by the local\ndistribution of queries: if domain Dis queried often at server S, then the\nreferences for name servers of Dwill be cached at S. The behavior at another\nserver S0is determined by what is queried at S0. This important feature has\nbeen con\ufb01rmed in a more recent study that also shows how dif\ufb01cult it can\nbe to understand the effectiveness of caching and the locality principles of\nDNS resolvers. In particular, an ISP\u2019s DNS resolver may be very effective in\nredirecting traf\ufb01c to content that is localized in that ISP [Ager et al., 2010].\nIn contrast, caching and replication in DHT-based systems generally does\nnot show such principles of locality: results are simply cached at nodes on\nthe return path of a lookup and have very little to do with the fact that a\nlookup was locally initiated at a speci\ufb01c node in the DHT, or a resolver for\nwhich the local ISP can assist in looking up content.\nThe fact remains that replacing DNS by a decentralized implementation is not\nnecessarily a good idea. DNS as it stands today, is a well-engineered system that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "278 CHAPTER 5. NAMING\nis dif\ufb01cult to beat when it comes to performance and robustness [Vixie, 2009;\n2014].\nExample: The Network File System\nAs another, and very different example, consider naming in NFS. The funda-\nmental idea underlying the NFS naming model is to provide clients complete\ntransparent access to a remote \ufb01le system as maintained by a server. This\ntransparency is achieved by letting a client be able to mount a remote \ufb01le\nsystem into its own local \ufb01le system, as shown in Figure 5.24.\nFigure 5.24: Mounting (part of) a remote \ufb01le system in NFS.\nInstead of mounting an entire \ufb01le system, NFS allows clients to mount\nonly part of a \ufb01le system, as also shown in Figure 5.24. A server is said to\nexport a directory when it makes that directory and its entries available to\nclients. An exported directory can be mounted into a client\u2019s local name space.\nThis design approach has a serious implication: in principle, users do not\nshare name spaces. As shown in Figure 5.24 the \ufb01le named /remote /vu/mbox\nat client Ais named /work /me/mbox at client B. A \ufb01le\u2019s name therefore\ndepends on how clients organize their own local name space, and where\nexported directories are mounted. The drawback of this approach in a dis-\ntributed \ufb01le system is that sharing \ufb01les becomes much harder. For example,\nAlice cannot tell Bob about a \ufb01le using the name she assigned to that \ufb01le, for\nthat name may have a completely different meaning in Bob\u2019s name space of\n\ufb01les.\nThere are several ways to solve this problem, but the most common one\nis to provide each client with a name space that is partly standardized. For\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n278 CHAPTER 5. NAMING\nis dif\ufb01cult to beat when it comes to performance and robustness [Vixie, 2009;\n2014].\nExample: The Network File System\nAs another, and very different example, consider naming in NFS. The funda-\nmental idea underlying the NFS naming model is to provide clients complete\ntransparent access to a remote \ufb01le system as maintained by a server. This\ntransparency is achieved by letting a client be able to mount a remote \ufb01le\nsystem into its own local \ufb01le system, as shown in Figure 5.24.\nFigure 5.24: Mounting (part of) a remote \ufb01le system in NFS.\nInstead of mounting an entire \ufb01le system, NFS allows clients to mount\nonly part of a \ufb01le system, as also shown in Figure 5.24. A server is said to\nexport a directory when it makes that directory and its entries available to\nclients. An exported directory can be mounted into a client\u2019s local name space.\nThis design approach has a serious implication: in principle, users do not\nshare name spaces. As shown in Figure 5.24 the \ufb01le named /remote /vu/mbox\nat client Ais named /work /me/mbox at client B. A \ufb01le\u2019s name therefore\ndepends on how clients organize their own local name space, and where\nexported directories are mounted. The drawback of this approach in a dis-\ntributed \ufb01le system is that sharing \ufb01les becomes much harder. For example,\nAlice cannot tell Bob about a \ufb01le using the name she assigned to that \ufb01le, for\nthat name may have a completely different meaning in Bob\u2019s name space of\n\ufb01les.\nThere are several ways to solve this problem, but the most common one\nis to provide each client with a name space that is partly standardized. For\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 279\nexample, each client may be using the local directory /usr/binto mount a\n\ufb01le system containing a standard collection of programs that are available to\neveryone. Likewise, the directory /local may be used as a standard to mount\na local \ufb01le system that is located on the client\u2019s host.\nAn NFS server can itself mount directories that are exported by other\nservers. However, it is not allowed to export those directories to its own\nclients. Instead, a client will have to explicitly mount such a directory from\nthe server that maintains it, as shown in Figure 5.25. This restriction comes\npartly from simplicity. If a server could export a directory that it mounted\nfrom another server, it would have to return special \ufb01le handles that include\nan identi\ufb01er for a server. NFS does not support such \ufb01le handles.\nFigure 5.25: Mounting nested directories from multiple servers in NFS.\nTo explain this point in more detail, assume that server Ahosts a \ufb01le system\nFSAfrom which it exports the directory /packages . This directory contains\na subdirectory /draw that acts as a mount point for a \ufb01le system FSBthat is\nexported by server Band mounted by A. Let Aalso export /packages /draw to\nits own clients, and assume that a client has mounted /packages into its local\ndirectory / binas shown in Figure 5.25.\nIf name resolution is iterative, then to resolve the name /bin/draw /install ,\nthe client contacts server Awhen it has locally resolved /binand requests Ato\nreturn a \ufb01le handle for directory /draw . In that case, server Ashould return a\n\ufb01le handle that includes an identi\ufb01er for server B, for only Bcan resolve the\nrest of the path name, in this case /install . As we have said, this kind of name\nresolution is not supported by NFS.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 279\nexample, each client may be using the local directory /usr/binto mount a\n\ufb01le system containing a standard collection of programs that are available to\neveryone. Likewise, the directory /local may be used as a standard to mount\na local \ufb01le system that is located on the client\u2019s host.\nAn NFS server can itself mount directories that are exported by other\nservers. However, it is not allowed to export those directories to its own\nclients. Instead, a client will have to explicitly mount such a directory from\nthe server that maintains it, as shown in Figure 5.25. This restriction comes\npartly from simplicity. If a server could export a directory that it mounted\nfrom another server, it would have to return special \ufb01le handles that include\nan identi\ufb01er for a server. NFS does not support such \ufb01le handles.\nFigure 5.25: Mounting nested directories from multiple servers in NFS.\nTo explain this point in more detail, assume that server Ahosts a \ufb01le system\nFSAfrom which it exports the directory /packages . This directory contains\na subdirectory /draw that acts as a mount point for a \ufb01le system FSBthat is\nexported by server Band mounted by A. Let Aalso export /packages /draw to\nits own clients, and assume that a client has mounted /packages into its local\ndirectory / binas shown in Figure 5.25.\nIf name resolution is iterative, then to resolve the name /bin/draw /install ,\nthe client contacts server Awhen it has locally resolved /binand requests Ato\nreturn a \ufb01le handle for directory /draw . In that case, server Ashould return a\n\ufb01le handle that includes an identi\ufb01er for server B, for only Bcan resolve the\nrest of the path name, in this case /install . As we have said, this kind of name\nresolution is not supported by NFS.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "280 CHAPTER 5. NAMING\nName resolution in earlier versions of NFS is strictly iterative in the sense\nthat only a single \ufb01le name at a time can be looked up. In other words,\nresolving a name such as /bin/draw /install requires three separate calls to the\nNFS server. Moreover, the client is fully responsible for implementing the\nresolution of a path name. NFSv4 also supports recursive name lookups. In\nthis case, a client can pass a complete path name to a server and request that\nserver to resolve it.\nThere is another peculiarity with NFS name lookups that has been solved\nwith the most recent version (NFSv4). Consider a \ufb01le server hosting several\n\ufb01le systems. With the strict iterative name resolution, whenever a lookup is\ndone for a directory on which another \ufb01le system was mounted, the lookup\nwould return the \ufb01le handle of the directory. Subsequently reading that\ndirectory would return its original content, not that of the root directory of the\nmounted \ufb01le system.\nTo explain, assume that in our previous example that both \ufb01le systems FSA\nand FSBare hosted by a single server. If the client has mounted /packages\ninto its local directory /bin, then looking up the \ufb01le name draw at the server\nwould return the \ufb01le handle for draw . A subsequent call to the server for\nlisting the directory entries of draw by means of readdir would then return\nthe list of directory entries that were originally stored in FSAin subdirectory\n/packages /draw . Only if the client had also mounted \ufb01le system FSB, would\nit be possible to properly resolve the path name draw /install relative to / bin.\nNFSv4 solves this problem by allowing lookups to cross mount points at a\nserver. In particular, lookup returns the \ufb01le handle of the mounted directory\ninstead of that of the original directory. The client can detect that the lookup\nhas crossed a mount point by inspecting the \ufb01le system identi\ufb01er of the looked\nup \ufb01le. If required, the client can locally mount that \ufb01le system as well.\nA \ufb01le handle is a reference to a \ufb01le within a \ufb01le system. It is independent\nof the name of the \ufb01le it refers to. A \ufb01le handle is created by the server that is\nhosting the \ufb01le system and is unique with respect to all \ufb01le systems exported\nby the server. It is created when the \ufb01le is created. The client is kept ignorant\nof the actual content of a \ufb01le handle; it is completely opaque. File handles\nwere 32 bytes in NFS version 2, but were variable up to 64 bytes in version 3\nand 128 bytes in version 4. Of course, the length of a \ufb01le handle is not opaque.\nIdeally, a \ufb01le handle is implemented as a true identi\ufb01er for a \ufb01le relative to\na \ufb01le system. For one thing, this means that as long as the \ufb01le exists, it should\nhave one and the same \ufb01le handle. This persistence requirement allows a\nclient to store a \ufb01le handle locally once the associated \ufb01le has been looked\nup by means of its name. One bene\ufb01t is performance: as most \ufb01le operations\nrequire a \ufb01le handle instead of a name, the client can avoid having to look\nup a name repeatedly before every \ufb01le operation. Another bene\ufb01t of this\napproach is that the client can now access the \ufb01le regardless which (current)\nname it has.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n280 CHAPTER 5. NAMING\nName resolution in earlier versions of NFS is strictly iterative in the sense\nthat only a single \ufb01le name at a time can be looked up. In other words,\nresolving a name such as /bin/draw /install requires three separate calls to the\nNFS server. Moreover, the client is fully responsible for implementing the\nresolution of a path name. NFSv4 also supports recursive name lookups. In\nthis case, a client can pass a complete path name to a server and request that\nserver to resolve it.\nThere is another peculiarity with NFS name lookups that has been solved\nwith the most recent version (NFSv4). Consider a \ufb01le server hosting several\n\ufb01le systems. With the strict iterative name resolution, whenever a lookup is\ndone for a directory on which another \ufb01le system was mounted, the lookup\nwould return the \ufb01le handle of the directory. Subsequently reading that\ndirectory would return its original content, not that of the root directory of the\nmounted \ufb01le system.\nTo explain, assume that in our previous example that both \ufb01le systems FSA\nand FSBare hosted by a single server. If the client has mounted /packages\ninto its local directory /bin, then looking up the \ufb01le name draw at the server\nwould return the \ufb01le handle for draw . A subsequent call to the server for\nlisting the directory entries of draw by means of readdir would then return\nthe list of directory entries that were originally stored in FSAin subdirectory\n/packages /draw . Only if the client had also mounted \ufb01le system FSB, would\nit be possible to properly resolve the path name draw /install relative to / bin.\nNFSv4 solves this problem by allowing lookups to cross mount points at a\nserver. In particular, lookup returns the \ufb01le handle of the mounted directory\ninstead of that of the original directory. The client can detect that the lookup\nhas crossed a mount point by inspecting the \ufb01le system identi\ufb01er of the looked\nup \ufb01le. If required, the client can locally mount that \ufb01le system as well.\nA \ufb01le handle is a reference to a \ufb01le within a \ufb01le system. It is independent\nof the name of the \ufb01le it refers to. A \ufb01le handle is created by the server that is\nhosting the \ufb01le system and is unique with respect to all \ufb01le systems exported\nby the server. It is created when the \ufb01le is created. The client is kept ignorant\nof the actual content of a \ufb01le handle; it is completely opaque. File handles\nwere 32 bytes in NFS version 2, but were variable up to 64 bytes in version 3\nand 128 bytes in version 4. Of course, the length of a \ufb01le handle is not opaque.\nIdeally, a \ufb01le handle is implemented as a true identi\ufb01er for a \ufb01le relative to\na \ufb01le system. For one thing, this means that as long as the \ufb01le exists, it should\nhave one and the same \ufb01le handle. This persistence requirement allows a\nclient to store a \ufb01le handle locally once the associated \ufb01le has been looked\nup by means of its name. One bene\ufb01t is performance: as most \ufb01le operations\nrequire a \ufb01le handle instead of a name, the client can avoid having to look\nup a name repeatedly before every \ufb01le operation. Another bene\ufb01t of this\napproach is that the client can now access the \ufb01le regardless which (current)\nname it has.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.3. STRUCTURED NAMING 281\nBecause a \ufb01le handle can be locally stored by a client, it is also important\nthat a server does not reuse a \ufb01le handle after deleting a \ufb01le. Otherwise, a\nclient may mistakenly access the wrong \ufb01le when it uses its locally stored \ufb01le\nhandle.\nNote that the combination of iterative name lookups and not letting a\nlookup operation allow crossing a mount point introduces a problem with\ngetting an initial \ufb01le handle. In order to access \ufb01les in a remote \ufb01le system,\na client will need to provide the server with a \ufb01le handle of the directory\nwhere the lookup should take place, along with the name of the \ufb01le or\ndirectory that is to be resolved. NFSv3 solves this problem through a separate\nmount protocol, by which a client actually mounts a remote \ufb01le system. After\nmounting, the client is passed back the root \ufb01le handle of the mounted \ufb01le\nsystem, which it can subsequently use as a starting point for looking up\nnames.\nIn NFSv4, this problem is solved by providing a separate operation\nputrootfh that tells the server to solve all \ufb01le names relative to the root \ufb01le\nhandle of the \ufb01le system it manages. The root \ufb01le handle can be used to\nlook up any other \ufb01le handle in the server\u2019s \ufb01le system. This approach has\nthe additional bene\ufb01t that there is no need for a separate mount protocol.\nInstead, mounting can be integrated into the regular protocol for looking up\n\ufb01les. A client can simply mount a remote \ufb01le system by requesting the server\nto resolve names relative to the \ufb01le system\u2019s root \ufb01le handle using putrootfh .\nNote 5.13 (Advanced: Automounting)\nAs we mentioned, the NFS naming model essentially provides users with their\nown name space. Sharing in this model may become dif\ufb01cult if users name the\nsame \ufb01le differently. One solution to this problem is to provide each user with a\nlocal name space that is partly standardized, and subsequently mounting remote\n\ufb01le systems the same for each user.\nAnother problem with the NFS naming model has to do with deciding when a\nremote \ufb01le system should be mounted. Consider a large system with thousands\nof users. Assume that each user has a local directory /home that is used to mount\nthe home directories of other users. For example, Alice\u2019s home directory may\nbe locally available to her as /home /alice , although the actual \ufb01les are stored on\na remote server. This directory can be automatically mounted when Alice logs\ninto her workstation. In addition, she may have access to Bob\u2019s public \ufb01les by\naccessing Bob\u2019s directory through / home /bob.\nThe question, however, is whether Bob\u2019s home directory should also be\nmounted automatically when Alice logs in. The bene\ufb01t of this approach would be\nthat the whole business of mounting \ufb01le systems would be transparent to Alice.\nHowever, if this policy were followed for every user, logging in could incur a lot\nof communication and administrative overhead. In addition, it would require\nthat all users are known in advance. A much better approach is to transparently\nmount another user\u2019s home directory on demand, that is, when it is \ufb01rst needed.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.3. STRUCTURED NAMING 281\nBecause a \ufb01le handle can be locally stored by a client, it is also important\nthat a server does not reuse a \ufb01le handle after deleting a \ufb01le. Otherwise, a\nclient may mistakenly access the wrong \ufb01le when it uses its locally stored \ufb01le\nhandle.\nNote that the combination of iterative name lookups and not letting a\nlookup operation allow crossing a mount point introduces a problem with\ngetting an initial \ufb01le handle. In order to access \ufb01les in a remote \ufb01le system,\na client will need to provide the server with a \ufb01le handle of the directory\nwhere the lookup should take place, along with the name of the \ufb01le or\ndirectory that is to be resolved. NFSv3 solves this problem through a separate\nmount protocol, by which a client actually mounts a remote \ufb01le system. After\nmounting, the client is passed back the root \ufb01le handle of the mounted \ufb01le\nsystem, which it can subsequently use as a starting point for looking up\nnames.\nIn NFSv4, this problem is solved by providing a separate operation\nputrootfh that tells the server to solve all \ufb01le names relative to the root \ufb01le\nhandle of the \ufb01le system it manages. The root \ufb01le handle can be used to\nlook up any other \ufb01le handle in the server\u2019s \ufb01le system. This approach has\nthe additional bene\ufb01t that there is no need for a separate mount protocol.\nInstead, mounting can be integrated into the regular protocol for looking up\n\ufb01les. A client can simply mount a remote \ufb01le system by requesting the server\nto resolve names relative to the \ufb01le system\u2019s root \ufb01le handle using putrootfh .\nNote 5.13 (Advanced: Automounting)\nAs we mentioned, the NFS naming model essentially provides users with their\nown name space. Sharing in this model may become dif\ufb01cult if users name the\nsame \ufb01le differently. One solution to this problem is to provide each user with a\nlocal name space that is partly standardized, and subsequently mounting remote\n\ufb01le systems the same for each user.\nAnother problem with the NFS naming model has to do with deciding when a\nremote \ufb01le system should be mounted. Consider a large system with thousands\nof users. Assume that each user has a local directory /home that is used to mount\nthe home directories of other users. For example, Alice\u2019s home directory may\nbe locally available to her as /home /alice , although the actual \ufb01les are stored on\na remote server. This directory can be automatically mounted when Alice logs\ninto her workstation. In addition, she may have access to Bob\u2019s public \ufb01les by\naccessing Bob\u2019s directory through / home /bob.\nThe question, however, is whether Bob\u2019s home directory should also be\nmounted automatically when Alice logs in. The bene\ufb01t of this approach would be\nthat the whole business of mounting \ufb01le systems would be transparent to Alice.\nHowever, if this policy were followed for every user, logging in could incur a lot\nof communication and administrative overhead. In addition, it would require\nthat all users are known in advance. A much better approach is to transparently\nmount another user\u2019s home directory on demand, that is, when it is \ufb01rst needed.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "282 CHAPTER 5. NAMING\nOn-demand mounting of a remote \ufb01le system (or actually an exported direc-\ntory) is handled in NFS by an automounter , which runs as a separate process\non the client\u2019s machine. The principle underlying an automounter is relatively\nsimple. Consider a simple automounter implemented as a user-level NFS server\non a Unix operating system. (For other implementations, see [Callaghan, 2000]).\nAssume that for each user, the home directories of all users are available\nthrough the local directory /home , as described above. When a client machine\nboots, the automounter starts with mounting this directory. The effect of this local\nmount is that whenever a program attempts to access /home , the Unix kernel will\nforward a lookup operation to the NFS client, which in this case, will forward the\nrequest to the automounter in its role as NFS server, as shown in Figure 5.26.\nFigure 5.26: A simple automounter for NFS.\nFor example, suppose that Alice logs in. The login program will attempt to\nread the directory /home /alice to \ufb01nd information such as login scripts. The\nautomounter will thus receive the request to look up subdirectory /home /alice ,\nfor which reason it \ufb01rst creates a subdirectory /alice in/home . It then looks up\nthe NFS server that exports Alice\u2019s home directory to subsequently mount that\ndirectory in / home /alice . At that point, the login program can proceed.\nThe problem with this approach is that the automounter will have to be\ninvolved in all \ufb01le operations to guarantee transparency. If a referenced \ufb01le is not\nlocally available because the corresponding \ufb01le system has not yet been mounted,\nthe automounter will have to know. In particular, it will need to handle all read\nand write requests, even for \ufb01le systems that have already been mounted. This\napproach may incur a large performance problem. It would be better to have\nthe automounter only mount/unmount directories, but otherwise stay out of the\nloop.\nA simple solution is to let the automounter mount directories in a special\nsubdirectory, and install a symbolic link to each mounted directory. This approach\nis shown in Figure 5.27.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n282 CHAPTER 5. NAMING\nOn-demand mounting of a remote \ufb01le system (or actually an exported direc-\ntory) is handled in NFS by an automounter , which runs as a separate process\non the client\u2019s machine. The principle underlying an automounter is relatively\nsimple. Consider a simple automounter implemented as a user-level NFS server\non a Unix operating system. (For other implementations, see [Callaghan, 2000]).\nAssume that for each user, the home directories of all users are available\nthrough the local directory /home , as described above. When a client machine\nboots, the automounter starts with mounting this directory. The effect of this local\nmount is that whenever a program attempts to access /home , the Unix kernel will\nforward a lookup operation to the NFS client, which in this case, will forward the\nrequest to the automounter in its role as NFS server, as shown in Figure 5.26.\nFigure 5.26: A simple automounter for NFS.\nFor example, suppose that Alice logs in. The login program will attempt to\nread the directory /home /alice to \ufb01nd information such as login scripts. The\nautomounter will thus receive the request to look up subdirectory /home /alice ,\nfor which reason it \ufb01rst creates a subdirectory /alice in/home . It then looks up\nthe NFS server that exports Alice\u2019s home directory to subsequently mount that\ndirectory in / home /alice . At that point, the login program can proceed.\nThe problem with this approach is that the automounter will have to be\ninvolved in all \ufb01le operations to guarantee transparency. If a referenced \ufb01le is not\nlocally available because the corresponding \ufb01le system has not yet been mounted,\nthe automounter will have to know. In particular, it will need to handle all read\nand write requests, even for \ufb01le systems that have already been mounted. This\napproach may incur a large performance problem. It would be better to have\nthe automounter only mount/unmount directories, but otherwise stay out of the\nloop.\nA simple solution is to let the automounter mount directories in a special\nsubdirectory, and install a symbolic link to each mounted directory. This approach\nis shown in Figure 5.27.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 283\nFigure 5.27: Using symbolic links with automounting.\nIn our example, the user home directories are mounted as subdirectories of\n/tmp_mnt. When Alice logs in, the automounter mounts her home directory\nin/tmp_mnt/home /alice and creates a symbolic link /home /alice that refers to\nthat subdirectory. In this case, whenever Alice executes a command such as ls\n--l /home/alice the NFS server that exports Alice\u2019s home directory is contacted\ndirectly without further involvement of the automounter.\n5.4 Attribute-based naming\nFlat and structured names generally provide a unique and location-independent\nway of referring to entities. Moreover, structured names have been partly\ndesigned to provide a human-friendly way to name entities so that they can be\nconveniently accessed. In most cases, it is assumed that the name refers to only\na single entity. However, location independence and human friendliness are\nnot the only criterion for naming entities. In particular, as more information\nis being made available it becomes important to effectively search for entities.\nThis approach requires that a user can provide merely a description of what\nhe is looking for.\nThere are many ways in which descriptions can be provided, but a popular\none in distributed systems is to describe an entity in terms of ( attribute, value )\npairs, generally referred to as attribute-based naming . In this approach,\nan entity is assumed to have an associated collection of attributes. Each\nattribute says something about that entity. By specifying which values a\nspeci\ufb01c attribute should have, a user essentially constrains the set of entities\nthat he is interested in. It is up to the naming system to return one or more\nentities that meet the user\u2019s description. In this section we take a closer look\nat attribute-based naming systems.\nDirectory services\nAttribute-based naming systems are also known as directory services , whereas\nsystems that support structured naming are generally called naming systems .\nWith directory services, entities have a set of associated attributes that can be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 283\nFigure 5.27: Using symbolic links with automounting.\nIn our example, the user home directories are mounted as subdirectories of\n/tmp_mnt. When Alice logs in, the automounter mounts her home directory\nin/tmp_mnt/home /alice and creates a symbolic link /home /alice that refers to\nthat subdirectory. In this case, whenever Alice executes a command such as ls\n--l /home/alice the NFS server that exports Alice\u2019s home directory is contacted\ndirectly without further involvement of the automounter.\n5.4 Attribute-based naming\nFlat and structured names generally provide a unique and location-independent\nway of referring to entities. Moreover, structured names have been partly\ndesigned to provide a human-friendly way to name entities so that they can be\nconveniently accessed. In most cases, it is assumed that the name refers to only\na single entity. However, location independence and human friendliness are\nnot the only criterion for naming entities. In particular, as more information\nis being made available it becomes important to effectively search for entities.\nThis approach requires that a user can provide merely a description of what\nhe is looking for.\nThere are many ways in which descriptions can be provided, but a popular\none in distributed systems is to describe an entity in terms of ( attribute, value )\npairs, generally referred to as attribute-based naming . In this approach,\nan entity is assumed to have an associated collection of attributes. Each\nattribute says something about that entity. By specifying which values a\nspeci\ufb01c attribute should have, a user essentially constrains the set of entities\nthat he is interested in. It is up to the naming system to return one or more\nentities that meet the user\u2019s description. In this section we take a closer look\nat attribute-based naming systems.\nDirectory services\nAttribute-based naming systems are also known as directory services , whereas\nsystems that support structured naming are generally called naming systems .\nWith directory services, entities have a set of associated attributes that can be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "284 CHAPTER 5. NAMING\nused for searching. In some cases, the choice of attributes can be relatively\nsimple. For example, in an e-mail system, messages can be tagged with\nattributes for the sender, recipient, subject, and so on. However, even in the\ncase of e-mail, matters become dif\ufb01cult when other types of descriptors are\nneeded, as is illustrated by the dif\ufb01culty of developing \ufb01lters that will allow\nonly certain messages (based on their descriptors) to be passed through.\nWhat it all boils down to is that designing an appropriate set of attributes\nis not trivial. In most cases, attribute design has to be done manually. Even if\nthere is consensus on the set of attributes to use, practice shows that setting\nthe values consistently by a diverse group of people is a problem by itself, as\nmany will have experienced when accessing music and video databases on\nthe Internet.\nTo alleviate some of these problems, research has been conducted on\nunifying the ways that resources can be described. In the context of distributed\nsystems, one particularly relevant development is the resource description\nframework (RDF ). Fundamental to the RDF model is that resources are\ndescribed as triplets consisting of a subject, a predicate, and an object. For\nexample, ( Person ,name ,Alice ) describes a resource named Person whose name\nisAlice . In RDF, each subject, predicate, or object can be a resource itself.\nThis means that Alice may be implemented as a reference to a \ufb01le that can\nbe subsequently retrieved. In the case of a predicate, such a resource could\ncontain a textual description of that predicate. Resources associated with\nsubjects and objects can be anything. References in RDF are essentially URLs.\nIf resource descriptions are stored, it becomes possible to query that\nstorage in a way that is common for many attribute-based naming systems.\nFor example, an application could ask for the information associated with a\nperson named Alice. Such a query would return a reference to the person\nresource associated with Alice. This resource can then subsequently be fetched\nby the application.\nIn this example, the resource descriptions are stored at a central location.\nThere is no reason why the resources should reside at the same location as\nwell. However, not having the descriptions in the same place may incur a\nserious performance problem. Unlike structured naming systems, looking up\nvalues in an attribute-based naming system essentially requires an exhaustive\nsearch through all descriptors. (Various techniques can be applied to avoid\nsuch exhaustive searches, one obvious being indexing.) When considering\nperformance, an exhaustive search may be less of problem within a single,\nnondistributed data store, but simply sending a search query to hundreds of\nservers that jointly implement a distributed data store is generally not such\na good idea. In the following, we will take a look at different approaches to\nsolving this problem in distributed systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n284 CHAPTER 5. NAMING\nused for searching. In some cases, the choice of attributes can be relatively\nsimple. For example, in an e-mail system, messages can be tagged with\nattributes for the sender, recipient, subject, and so on. However, even in the\ncase of e-mail, matters become dif\ufb01cult when other types of descriptors are\nneeded, as is illustrated by the dif\ufb01culty of developing \ufb01lters that will allow\nonly certain messages (based on their descriptors) to be passed through.\nWhat it all boils down to is that designing an appropriate set of attributes\nis not trivial. In most cases, attribute design has to be done manually. Even if\nthere is consensus on the set of attributes to use, practice shows that setting\nthe values consistently by a diverse group of people is a problem by itself, as\nmany will have experienced when accessing music and video databases on\nthe Internet.\nTo alleviate some of these problems, research has been conducted on\nunifying the ways that resources can be described. In the context of distributed\nsystems, one particularly relevant development is the resource description\nframework (RDF ). Fundamental to the RDF model is that resources are\ndescribed as triplets consisting of a subject, a predicate, and an object. For\nexample, ( Person ,name ,Alice ) describes a resource named Person whose name\nisAlice . In RDF, each subject, predicate, or object can be a resource itself.\nThis means that Alice may be implemented as a reference to a \ufb01le that can\nbe subsequently retrieved. In the case of a predicate, such a resource could\ncontain a textual description of that predicate. Resources associated with\nsubjects and objects can be anything. References in RDF are essentially URLs.\nIf resource descriptions are stored, it becomes possible to query that\nstorage in a way that is common for many attribute-based naming systems.\nFor example, an application could ask for the information associated with a\nperson named Alice. Such a query would return a reference to the person\nresource associated with Alice. This resource can then subsequently be fetched\nby the application.\nIn this example, the resource descriptions are stored at a central location.\nThere is no reason why the resources should reside at the same location as\nwell. However, not having the descriptions in the same place may incur a\nserious performance problem. Unlike structured naming systems, looking up\nvalues in an attribute-based naming system essentially requires an exhaustive\nsearch through all descriptors. (Various techniques can be applied to avoid\nsuch exhaustive searches, one obvious being indexing.) When considering\nperformance, an exhaustive search may be less of problem within a single,\nnondistributed data store, but simply sending a search query to hundreds of\nservers that jointly implement a distributed data store is generally not such\na good idea. In the following, we will take a look at different approaches to\nsolving this problem in distributed systems.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 285\nHierarchical implementations: LDAP\nA common approach to tackling distributed directory services is to combine\nstructured naming with attribute-based naming. This approach has been\nwidely adopted, for example, in Microsoft\u2019s Active Directory service and other\nsystems. Many of these systems use, or rely on the lightweight directory\naccess protocol commonly referred simply as LDAP . The LDAP directory\nservice has been derived from OSI\u2019s X.500 directory service. As with many OSI\nservices, the quality of their associated implementations hindered widespread\nuse, and simpli\ufb01cations were needed to make it useful. Detailed information\non LDAP can be found in Arkills [2003].\nConceptually, an LDAP directory service consists of a number of records,\nusually referred to as directory entries. A directory entry is comparable\nto a resource record in DNS. Each record is made up of a collection of ( at-\ntribute, value ) pairs, where each attribute has an associated type. A distinction\nis made between single-valued attributes and multiple-valued attributes. The\nlatter typically represent arrays and lists. As an example, a simple direc-\ntory entry identifying the network addresses of some general servers from\nFigure 5.23 is shown in Figure 5.28.\nAttribute Abbr. Value\nCountry C NL\nLocality L Amsterdam\nOrganization O VU University\nOrganizationalUnit OU Computer Science\nCommonName CN Main server\nMail_Servers \u2013 137.37.20.3, 130.37.24.6, 137.37.20.10\nFTP_Server \u2013 130.37.20.20\nWWW_Server \u2013 130.37.20.20\nFigure 5.28: A simple example of an LDAP directory entry using LDAP\nnaming conventions.\nIn our example, we have used a naming convention described in the\nLDAP standards, which applies to the \ufb01rst \ufb01ve attributes. The attributes\nOrganization and OrganizationUnit describe, respectively, the organization and\nthe department associated with the data that are stored in the record. Likewise,\nthe attributes Locality and Country provide additional information on where\nthe entry is stored. The CommonName attribute is often used as an (ambiguous)\nname to identify an entry within a limited part of the directory. For example,\nthe name \u201cMain server\u201d may be enough to \ufb01nd our example entry given the\nspeci\ufb01c values for the other four attributes Country ,Locality ,Organization , and\nOrganizationalUnit . In our example, only attribute Mail_Servers has multiple\nvalues associated with it. All other attributes have only a single value.\nThe collection of all directory entries in an LDAP directory service is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 285\nHierarchical implementations: LDAP\nA common approach to tackling distributed directory services is to combine\nstructured naming with attribute-based naming. This approach has been\nwidely adopted, for example, in Microsoft\u2019s Active Directory service and other\nsystems. Many of these systems use, or rely on the lightweight directory\naccess protocol commonly referred simply as LDAP . The LDAP directory\nservice has been derived from OSI\u2019s X.500 directory service. As with many OSI\nservices, the quality of their associated implementations hindered widespread\nuse, and simpli\ufb01cations were needed to make it useful. Detailed information\non LDAP can be found in Arkills [2003].\nConceptually, an LDAP directory service consists of a number of records,\nusually referred to as directory entries. A directory entry is comparable\nto a resource record in DNS. Each record is made up of a collection of ( at-\ntribute, value ) pairs, where each attribute has an associated type. A distinction\nis made between single-valued attributes and multiple-valued attributes. The\nlatter typically represent arrays and lists. As an example, a simple direc-\ntory entry identifying the network addresses of some general servers from\nFigure 5.23 is shown in Figure 5.28.\nAttribute Abbr. Value\nCountry C NL\nLocality L Amsterdam\nOrganization O VU University\nOrganizationalUnit OU Computer Science\nCommonName CN Main server\nMail_Servers \u2013 137.37.20.3, 130.37.24.6, 137.37.20.10\nFTP_Server \u2013 130.37.20.20\nWWW_Server \u2013 130.37.20.20\nFigure 5.28: A simple example of an LDAP directory entry using LDAP\nnaming conventions.\nIn our example, we have used a naming convention described in the\nLDAP standards, which applies to the \ufb01rst \ufb01ve attributes. The attributes\nOrganization and OrganizationUnit describe, respectively, the organization and\nthe department associated with the data that are stored in the record. Likewise,\nthe attributes Locality and Country provide additional information on where\nthe entry is stored. The CommonName attribute is often used as an (ambiguous)\nname to identify an entry within a limited part of the directory. For example,\nthe name \u201cMain server\u201d may be enough to \ufb01nd our example entry given the\nspeci\ufb01c values for the other four attributes Country ,Locality ,Organization , and\nOrganizationalUnit . In our example, only attribute Mail_Servers has multiple\nvalues associated with it. All other attributes have only a single value.\nThe collection of all directory entries in an LDAP directory service is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "286 CHAPTER 5. NAMING\ncalled a directory information base (DIB). An important aspect of a DIB\nis that each record is uniquely named so that it can be looked up. Such a\nglobally unique name appears as a sequence of naming attributes in each\nrecord. Each naming attribute is called a relative distinguished name , or\nRDN for short. In our example in Figure 5.28 the \ufb01rst \ufb01ve attributes are\nall naming attributes. Using the conventional abbreviations for representing\nnaming attributes in LDAP , as shown in Figure 5.28 the attributes Country ,\nOrganization , and OrganizationalUnit could be used to form the globally unique\nname /C=NL/O=VU University /OU=Computer Science . analogous to the\nDNS name nl.vu.cs.\nAs in DNS, the use of globally unique names by listing RDNs in sequence,\nleads to a hierarchy of the collection of directory entries, which is referred to\nas a directory information tree (DIT). A DIT essentially forms the naming\ngraph of an LDAP directory service in which each node represents a directory\nentry. In addition, a node may also act as a directory in the traditional sense,\nin that there may be several children for which the node acts as parent. To\nexplain, consider the naming graph as partly shown in Figure 5.29. (Recall\nthat labels are associated with edges.)\n(a)\nAttribute Value Attribute Value\nLocality Amsterdam Locality Amsterdam\nOrganization VUUniversity Organization VUUniversity\nOrganizationalUnit ComputerScience OrganizationalUnit ComputerScience\nCommonName Mainserver CommonName Mainserver\nHostName star HostName zephyr\nHostAddress 192 .31.231.42 HostAddress 137 .37.20.10\n(b)\nFigure 5.29: (a) Part of a directory information tree. (b) Two directory entries\nhaving HostName as RDN.\nNode Ncorresponds to the directory entry shown in Figure 5.28. At the\nsame time, this node acts as a parent to a number of other directory entries\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n286 CHAPTER 5. NAMING\ncalled a directory information base (DIB). An important aspect of a DIB\nis that each record is uniquely named so that it can be looked up. Such a\nglobally unique name appears as a sequence of naming attributes in each\nrecord. Each naming attribute is called a relative distinguished name , or\nRDN for short. In our example in Figure 5.28 the \ufb01rst \ufb01ve attributes are\nall naming attributes. Using the conventional abbreviations for representing\nnaming attributes in LDAP , as shown in Figure 5.28 the attributes Country ,\nOrganization , and OrganizationalUnit could be used to form the globally unique\nname /C=NL/O=VU University /OU=Computer Science . analogous to the\nDNS name nl.vu.cs.\nAs in DNS, the use of globally unique names by listing RDNs in sequence,\nleads to a hierarchy of the collection of directory entries, which is referred to\nas a directory information tree (DIT). A DIT essentially forms the naming\ngraph of an LDAP directory service in which each node represents a directory\nentry. In addition, a node may also act as a directory in the traditional sense,\nin that there may be several children for which the node acts as parent. To\nexplain, consider the naming graph as partly shown in Figure 5.29. (Recall\nthat labels are associated with edges.)\n(a)\nAttribute Value Attribute Value\nLocality Amsterdam Locality Amsterdam\nOrganization VUUniversity Organization VUUniversity\nOrganizationalUnit ComputerScience OrganizationalUnit ComputerScience\nCommonName Mainserver CommonName Mainserver\nHostName star HostName zephyr\nHostAddress 192 .31.231.42 HostAddress 137 .37.20.10\n(b)\nFigure 5.29: (a) Part of a directory information tree. (b) Two directory entries\nhaving HostName as RDN.\nNode Ncorresponds to the directory entry shown in Figure 5.28. At the\nsame time, this node acts as a parent to a number of other directory entries\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 287\nthat have an additional naming attribute HostName that is used as an RDN. For\nexample, such entries may be used to represent hosts as shown in Figure 5.29.\nA node in an LDAP naming graph can thus simultaneously represent a\ndirectory in the traditional sense as we discussed previously, as well as an\nLDAP record. This distinction is supported by two different lookup operations.\nThe read operation is used to read a single record given its path name in the\nDIT. In contrast, the list operation is used to list the names of all outgoing\nedges of a given node in the DIT. Each name corresponds to a child node of\nthe given node. Note that the list operation does not return any records; it\nmerely returns names. In other words, calling read with as input the name\n/C=NL/O=VU University /OU=Computer Science /CN=Main server\nwill return the record shown in Figure 5.29, whereas calling list will return\nthe names starand zephyr from the entries shown in Figure 5.29 as well as the\nnames of other hosts that have been registered in a similar way.\nImplementing an LDAP directory service proceeds in much the same way\nas implementing a naming service such as DNS, except that LDAP supports\nmore lookup operations as we will discuss shortly. When dealing with a large-\nscale directory, the DIT is usually partitioned and distributed across several\nservers, known as directory service agents (DSA ). Each part of a partitioned\nDIT thus corresponds to a zone in DNS. Likewise, each DSA behaves very\nmuch the same as a normal name server, except that it implements a number\nof typical directory services, such as advanced search operations.\nClients are represented by what are called directory user agents , or simply\nDUA . A DUA is similar to a name resolver in structured-naming services. A\nDUA exchanges information with a DSA according to a standardized access\nprotocol.\nWhat makes an LDAP implementation different from a DNS implementa-\ntion are the facilities for searching through a DIB. In particular, facilities are\nprovided to search for a directory entry given a set of criteria that attributes\nof the searched entries should meet. For example, suppose that we want a list\nof all main servers at VU University. Using the notation de\ufb01ned in Howes\n[1997], such a list can be returned using a search operation like\nsearch(\u2018\u2018(C=NL)(O=VU University)(OU=*)(CN=Main server)\u2019\u2019)\nIn this example, we have speci\ufb01ed that the place to look for main servers\nis the organization named VU_University in country NL, but that we are not\ninterested in a particular organizational unit. However, each returned result\nshould have the CNattribute equal to Main _server .\nAs we already mentioned, searching in a directory service is generally an\nexpensive operation. For example, to \ufb01nd all main servers at VU University\nrequires searching all entries at each department and combining the results\nin a single answer. In other words, we will generally need to access several\nleaf nodes of a DIT in order to get an answer. In practice, this also means that\nseveral DSAs need to be accessed. In contrast, naming services can often be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 287\nthat have an additional naming attribute HostName that is used as an RDN. For\nexample, such entries may be used to represent hosts as shown in Figure 5.29.\nA node in an LDAP naming graph can thus simultaneously represent a\ndirectory in the traditional sense as we discussed previously, as well as an\nLDAP record. This distinction is supported by two different lookup operations.\nThe read operation is used to read a single record given its path name in the\nDIT. In contrast, the list operation is used to list the names of all outgoing\nedges of a given node in the DIT. Each name corresponds to a child node of\nthe given node. Note that the list operation does not return any records; it\nmerely returns names. In other words, calling read with as input the name\n/C=NL/O=VU University /OU=Computer Science /CN=Main server\nwill return the record shown in Figure 5.29, whereas calling list will return\nthe names starand zephyr from the entries shown in Figure 5.29 as well as the\nnames of other hosts that have been registered in a similar way.\nImplementing an LDAP directory service proceeds in much the same way\nas implementing a naming service such as DNS, except that LDAP supports\nmore lookup operations as we will discuss shortly. When dealing with a large-\nscale directory, the DIT is usually partitioned and distributed across several\nservers, known as directory service agents (DSA ). Each part of a partitioned\nDIT thus corresponds to a zone in DNS. Likewise, each DSA behaves very\nmuch the same as a normal name server, except that it implements a number\nof typical directory services, such as advanced search operations.\nClients are represented by what are called directory user agents , or simply\nDUA . A DUA is similar to a name resolver in structured-naming services. A\nDUA exchanges information with a DSA according to a standardized access\nprotocol.\nWhat makes an LDAP implementation different from a DNS implementa-\ntion are the facilities for searching through a DIB. In particular, facilities are\nprovided to search for a directory entry given a set of criteria that attributes\nof the searched entries should meet. For example, suppose that we want a list\nof all main servers at VU University. Using the notation de\ufb01ned in Howes\n[1997], such a list can be returned using a search operation like\nsearch(\u2018\u2018(C=NL)(O=VU University)(OU=*)(CN=Main server)\u2019\u2019)\nIn this example, we have speci\ufb01ed that the place to look for main servers\nis the organization named VU_University in country NL, but that we are not\ninterested in a particular organizational unit. However, each returned result\nshould have the CNattribute equal to Main _server .\nAs we already mentioned, searching in a directory service is generally an\nexpensive operation. For example, to \ufb01nd all main servers at VU University\nrequires searching all entries at each department and combining the results\nin a single answer. In other words, we will generally need to access several\nleaf nodes of a DIT in order to get an answer. In practice, this also means that\nseveral DSAs need to be accessed. In contrast, naming services can often be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "288 CHAPTER 5. NAMING\nimplemented in such a way that a lookup operation requires accessing only a\nsingle leaf node.\nThis whole setup of LDAP can be taken one step further by allowing\nseveral trees to co-exist, while also being linked to each other. This ap-\nproach is followed in Microsoft\u2019s Active Directory leading to a forest of LDAP\ndomains [Allen and Lowe-Norris, 2003]. Obviously, searching in such an\norganization can be overwhelmingly complex. To circumvent some of the\nscalability problems, Active Directory usually assumes there is a global index\nserver (called a global catalog) that can be searched \ufb01rst. The index will\nindicate which LDAP domains need to be searched further.\nAlthough LDAP by itself already exploits hierarchy for scalability, it is\ncommon to combine LDAP with DNS. For example, every tree in LDAP needs\nto be accessible at the root (known in Active Directory as a domain controller).\nThe root is often known under a DNS name, which, in turn, can be found\nthrough an appropriate SRV record as we explained above.\nDecentralized implementations\nNotably with the advent of peer-to-peer systems, researchers have also been\nlooking for solutions for decentralized attribute-based naming systems. In\nparticular, peer-to-peer systems are often used to store \ufb01les. Initially, \ufb01les\ncould not be searched\u2014they could only be looked up by their key. However,\nhaving the possibility to search for a \ufb01le based on descriptors can be extremely\nconvenient, where each descriptor is nothing but an (attribute, value) pair.\nObviously, querying every node in a peer-to-peer system to see if it contains\na \ufb01le matching one or more of such pairs is infeasible. What we need is a\nmapping of (attribute, value) pairs to index servers , which, in turn, point to\n\ufb01les matching those pairs.\nUsing a distributed index\nLet us \ufb01rst look at the situation of building a (distributed) index. The basic\nidea is that a search query is formulated as a list of (attribute, value) pairs, just\nas in the case of our LDAP examples. The result should be a list of (references\nto) entities that match allpairs. In the case of a peer-to-peer system storing\n\ufb01les, a list of keys to relevant \ufb01les may be returned, after which the client can\nlook up each of those \ufb01les using the returned keys.\nA straightforward approach toward a distributed index is the following.\nAssume there are ddifferent attributes. In that case we can use a server for\neach of the dattributes, where a server for attribute Amaintains a set of ( E,val)\npairs for each entity Ethat has the value valfor attribute A. A search query\nsuch as\nsearch(\u2018\u2018(Country=NL)(Organization=VU University)\n(OrganizationalUnit=*)(CommonName=Main server)\u2019\u2019)\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n288 CHAPTER 5. NAMING\nimplemented in such a way that a lookup operation requires accessing only a\nsingle leaf node.\nThis whole setup of LDAP can be taken one step further by allowing\nseveral trees to co-exist, while also being linked to each other. This ap-\nproach is followed in Microsoft\u2019s Active Directory leading to a forest of LDAP\ndomains [Allen and Lowe-Norris, 2003]. Obviously, searching in such an\norganization can be overwhelmingly complex. To circumvent some of the\nscalability problems, Active Directory usually assumes there is a global index\nserver (called a global catalog) that can be searched \ufb01rst. The index will\nindicate which LDAP domains need to be searched further.\nAlthough LDAP by itself already exploits hierarchy for scalability, it is\ncommon to combine LDAP with DNS. For example, every tree in LDAP needs\nto be accessible at the root (known in Active Directory as a domain controller).\nThe root is often known under a DNS name, which, in turn, can be found\nthrough an appropriate SRV record as we explained above.\nDecentralized implementations\nNotably with the advent of peer-to-peer systems, researchers have also been\nlooking for solutions for decentralized attribute-based naming systems. In\nparticular, peer-to-peer systems are often used to store \ufb01les. Initially, \ufb01les\ncould not be searched\u2014they could only be looked up by their key. However,\nhaving the possibility to search for a \ufb01le based on descriptors can be extremely\nconvenient, where each descriptor is nothing but an (attribute, value) pair.\nObviously, querying every node in a peer-to-peer system to see if it contains\na \ufb01le matching one or more of such pairs is infeasible. What we need is a\nmapping of (attribute, value) pairs to index servers , which, in turn, point to\n\ufb01les matching those pairs.\nUsing a distributed index\nLet us \ufb01rst look at the situation of building a (distributed) index. The basic\nidea is that a search query is formulated as a list of (attribute, value) pairs, just\nas in the case of our LDAP examples. The result should be a list of (references\nto) entities that match allpairs. In the case of a peer-to-peer system storing\n\ufb01les, a list of keys to relevant \ufb01les may be returned, after which the client can\nlook up each of those \ufb01les using the returned keys.\nA straightforward approach toward a distributed index is the following.\nAssume there are ddifferent attributes. In that case we can use a server for\neach of the dattributes, where a server for attribute Amaintains a set of ( E,val)\npairs for each entity Ethat has the value valfor attribute A. A search query\nsuch as\nsearch(\u2018\u2018(Country=NL)(Organization=VU University)\n(OrganizationalUnit=*)(CommonName=Main server)\u2019\u2019)\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 289\nwould be sent to the servers for Country ,Organization , and CommonName ,\nrespectively, after which the client would need to see which entities occur in all\nthree sets as returned by the servers. To prevent that a server needs to maintain\na very large set of entities, the set for each server can be further partitioned\nand distributed across several subservers, each subserver associated with the\nsame attribute.\nMore precisely, if we have a set of attributes fa1,. . .,aNg, then for each\nattribute akwe associate a set Sk=fSk\n1,. . .,Sknkgofnkservers. Assuming that\nan attribute aktakes values from a set Rk, we construct a global mapping F\nsuch that\nF(ak,v) = Sk\njwith Sk\nj2Skand v2Rk\nIn this example, server Sk\njwould keep track of each key associated with a \ufb01le\nhaving ak=v. The beauty of this scheme is its simplicity. If L(ak,v)is the\nset of keys returned by server F(ak,v), then a query can be formulated as a\nlogical expression such as\n\u0000\nF(a1,v1)^F(a2,v2)\u0001_F(a3,v3)\nwhich can then be processed on the client side by constructing the set\n\u0000\nL(a1,v1)\\L(a2,v2)\u0001[L(a3,v3)\nUnfortunately, there are important drawbacks to this scheme. First, any\nquery involving kattributes requires contacting kindex servers, which may\nincur signi\ufb01cant communication costs. Furthermore, and related, is that the\nclient is required to process the sets returned by the index servers. Just\nimagine that each \ufb01le has two attributes \frstName and lastName , respectively,\nand that a client is looking for the \ufb01le owned by Pheriby Smith . Now, although\nPheriby may be quite unique for a \ufb01rst name, Smith de\ufb01nitely is not. However,\nour poor client will have to receive perhaps millions of keys of \ufb01les for which\nlastName =Smith , while there may actually be only a handful of \ufb01les for\nwhich \frstName =Pheriby . Thirdly, although this scheme does allow to leave\ncertain attributes unspeci\ufb01ed (by simply not mentioning them in the query),\nit does not easily support range queries, such as, price= [1000\u00002500].\nSpace-\ufb01lling curves\nA common approach to implementing decentralized attribute-based naming\nsystems is to use what are known as space-\ufb01lling curves . The basic idea is to\nmap the N-dimensional space covered by the Nattributesfa1,. . .,aNginto a\nsingle dimension, and then use, for example, a simple hashing technique to\ndistribute the resultant space among index servers. One of the key issues is to\nhave (attribute, value) pairs that are \u201cclose\u201d to each other be handled by the\nsame index server.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 289\nwould be sent to the servers for Country ,Organization , and CommonName ,\nrespectively, after which the client would need to see which entities occur in all\nthree sets as returned by the servers. To prevent that a server needs to maintain\na very large set of entities, the set for each server can be further partitioned\nand distributed across several subservers, each subserver associated with the\nsame attribute.\nMore precisely, if we have a set of attributes fa1,. . .,aNg, then for each\nattribute akwe associate a set Sk=fSk\n1,. . .,Sknkgofnkservers. Assuming that\nan attribute aktakes values from a set Rk, we construct a global mapping F\nsuch that\nF(ak,v) = Sk\njwith Sk\nj2Skand v2Rk\nIn this example, server Sk\njwould keep track of each key associated with a \ufb01le\nhaving ak=v. The beauty of this scheme is its simplicity. If L(ak,v)is the\nset of keys returned by server F(ak,v), then a query can be formulated as a\nlogical expression such as\n\u0000\nF(a1,v1)^F(a2,v2)\u0001_F(a3,v3)\nwhich can then be processed on the client side by constructing the set\n\u0000\nL(a1,v1)\\L(a2,v2)\u0001[L(a3,v3)\nUnfortunately, there are important drawbacks to this scheme. First, any\nquery involving kattributes requires contacting kindex servers, which may\nincur signi\ufb01cant communication costs. Furthermore, and related, is that the\nclient is required to process the sets returned by the index servers. Just\nimagine that each \ufb01le has two attributes \frstName and lastName , respectively,\nand that a client is looking for the \ufb01le owned by Pheriby Smith . Now, although\nPheriby may be quite unique for a \ufb01rst name, Smith de\ufb01nitely is not. However,\nour poor client will have to receive perhaps millions of keys of \ufb01les for which\nlastName =Smith , while there may actually be only a handful of \ufb01les for\nwhich \frstName =Pheriby . Thirdly, although this scheme does allow to leave\ncertain attributes unspeci\ufb01ed (by simply not mentioning them in the query),\nit does not easily support range queries, such as, price= [1000\u00002500].\nSpace-\ufb01lling curves\nA common approach to implementing decentralized attribute-based naming\nsystems is to use what are known as space-\ufb01lling curves . The basic idea is to\nmap the N-dimensional space covered by the Nattributesfa1,. . .,aNginto a\nsingle dimension, and then use, for example, a simple hashing technique to\ndistribute the resultant space among index servers. One of the key issues is to\nhave (attribute, value) pairs that are \u201cclose\u201d to each other be handled by the\nsame index server.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "290 CHAPTER 5. NAMING\nLet us make matters concrete and look into one popular case, namely\nHilbert space-\ufb01lling curves (see, for example, Lawder and King [2000]). These\nare easiest to explain by looking only at two dimensions, that is, considering\nonly two distinct attributes. The possible values that each attribute can have\ncorresponds to one axis in a two-dimensional space. Without loss of generality,\nwe assume that each attribute takes on values in the interval [0, 1). As a \ufb01rst\napproximation of the square, we divide it into four quadrants, as shown in\nFigure 5.30(a). All data values (x,y)with 0\u0014x,y<0.5are associated with\nindex 0. Values (x,y)with 0.5\u0014x,y<1.0 are associated with index 2.\n(a) (b)\nFigure 5.30: Reducing a two-dimensional space to a single dimension through\na Hilbert space-\ufb01lling curve of (a) order 1, and (b) order 4.\nWe can repeat this procedure recursively for each subsquare: divide it\ninto four smaller squares and connect the smaller squares through a single\nline. Using rotation and re\ufb02ection, we make sure that this line can be nicely\nconnected to the one in the previously neighboring larger subsquare (which\nhas also been divided into smaller squares). To illustrate, where Figure 5.30(a)\nshows a Hilbert curve of order 1, Figure 5.30(b) shows a curve of order 4 with\n256 indices. In general, a Hilbert curve of order kconnects 22ksubsquares,\nand thus has also 22kindices. There are various ways in which we can\nsystematically draw a curve through a two-dimensional space that has been\npartitioned into equally sized squares. Furthermore, the process can be easily\nexpanded to higher dimensions, as explained by Sagan [1994] and Bader\n[2013].\nAn important property of space-\ufb01lling curves is that they preserve locality:\ntwo indices that are close to each other on the curve correspond to two\npoints that are also close to each other in the multidimensional space. (Note\nthat the reverse is not always true: two points close to each other in the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n290 CHAPTER 5. NAMING\nLet us make matters concrete and look into one popular case, namely\nHilbert space-\ufb01lling curves (see, for example, Lawder and King [2000]). These\nare easiest to explain by looking only at two dimensions, that is, considering\nonly two distinct attributes. The possible values that each attribute can have\ncorresponds to one axis in a two-dimensional space. Without loss of generality,\nwe assume that each attribute takes on values in the interval [0, 1). As a \ufb01rst\napproximation of the square, we divide it into four quadrants, as shown in\nFigure 5.30(a). All data values (x,y)with 0\u0014x,y<0.5are associated with\nindex 0. Values (x,y)with 0.5\u0014x,y<1.0 are associated with index 2.\n(a) (b)\nFigure 5.30: Reducing a two-dimensional space to a single dimension through\na Hilbert space-\ufb01lling curve of (a) order 1, and (b) order 4.\nWe can repeat this procedure recursively for each subsquare: divide it\ninto four smaller squares and connect the smaller squares through a single\nline. Using rotation and re\ufb02ection, we make sure that this line can be nicely\nconnected to the one in the previously neighboring larger subsquare (which\nhas also been divided into smaller squares). To illustrate, where Figure 5.30(a)\nshows a Hilbert curve of order 1, Figure 5.30(b) shows a curve of order 4 with\n256 indices. In general, a Hilbert curve of order kconnects 22ksubsquares,\nand thus has also 22kindices. There are various ways in which we can\nsystematically draw a curve through a two-dimensional space that has been\npartitioned into equally sized squares. Furthermore, the process can be easily\nexpanded to higher dimensions, as explained by Sagan [1994] and Bader\n[2013].\nAn important property of space-\ufb01lling curves is that they preserve locality:\ntwo indices that are close to each other on the curve correspond to two\npoints that are also close to each other in the multidimensional space. (Note\nthat the reverse is not always true: two points close to each other in the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 291\nmultidimensional space need not necessarily lie close to each other on the\ncurve.)\nTo complete the story, several things need to be done. First, attribute\nvalues need to indexed. Assume that we are dealing with a total of Npossible\nattributesfa1,. . .,aNg, and that each entity assigns a value to each of these N\nattributes (possibly the equivalent of a \u201cdon\u2019t care\u201d value). To keep matters\nsimple, we assume that each attribute value is normalized to a value in the\ninterval [0, 1). Then clearly, an entity Ehaving the tuple of values (v1,. . .,vN)\nis associated with a real-valued coordinate in an N-dimensional space, in turn\nuniquely associated with an N-dimensional subsquare as we discussed for the\ntwo-dimensional case. The center of such a subsquare corresponds to an index\non the associated Hilbert space-\ufb01lling curve, and is now the index associated\nwith entity E. Of course, multiple entities whose associated coordinates fall in\nthe same subsquare will all have the same index. To avoid such collisions as\nmuch as possible, we need to use high-ordered space-\ufb01lling curves. Orders of\n32 or 64 are not uncommon.\nSecond, we also need to be able to search for entities. The principle of\nsearching for entities based on their attribute values should now be clear.\nSuppose we were looking for \ufb01les whose two attribute values a1and a2lie\nin intervals [v1\nl,v1u)and[v2\nl,v2u), respectively (with vi\nl<viu). Clearly, this\ndelineates a rectangular region through which the curve passes, and all \ufb01les\nindexed by those segments of the curve that intersect with that region match\nthe search criterion. We therefore need an operation that returns a series of\ncurve-related indices given a region (expressed in terms of subsquares) in\nthe associated N-dimensional space. Such an operation is clearly dependent\non which space-\ufb01lling curve has been used, but interestingly, need not be\ndependent on actual entities.\nFinally, we need to maintain (references to) the entities associated with\nindices. One approach, used in the Squid system [Schmidt and Parashar,\n2008], is to use a Chord ring. In Squid, the index space is chosen to be the\nsame as that of the Chord ring, that is, both use m-bit identi\ufb01ers. Then clearly,\nthe Chord node responsible for index iwill store (references to) the entities\nindexed by i.\nNote 5.14 (More information: Other examples)\nDecentralized implementations of attribute-based naming systems have received\na lot of attention. The ones based on space-\ufb01lling curves are relatively popular,\nbut several alternatives have been proposed as well. Let us take a look at some\nrepresentative examples to shed some light on the design space.\nAttribute-value trees Here is another example where (attribute, value ) pairs\nare supported by a DHT-based system. First, assume that queries consist of a\nconjunction of pairs as with LDAP , that is, a user speci\ufb01es a list of attributes,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 291\nmultidimensional space need not necessarily lie close to each other on the\ncurve.)\nTo complete the story, several things need to be done. First, attribute\nvalues need to indexed. Assume that we are dealing with a total of Npossible\nattributesfa1,. . .,aNg, and that each entity assigns a value to each of these N\nattributes (possibly the equivalent of a \u201cdon\u2019t care\u201d value). To keep matters\nsimple, we assume that each attribute value is normalized to a value in the\ninterval [0, 1). Then clearly, an entity Ehaving the tuple of values (v1,. . .,vN)\nis associated with a real-valued coordinate in an N-dimensional space, in turn\nuniquely associated with an N-dimensional subsquare as we discussed for the\ntwo-dimensional case. The center of such a subsquare corresponds to an index\non the associated Hilbert space-\ufb01lling curve, and is now the index associated\nwith entity E. Of course, multiple entities whose associated coordinates fall in\nthe same subsquare will all have the same index. To avoid such collisions as\nmuch as possible, we need to use high-ordered space-\ufb01lling curves. Orders of\n32 or 64 are not uncommon.\nSecond, we also need to be able to search for entities. The principle of\nsearching for entities based on their attribute values should now be clear.\nSuppose we were looking for \ufb01les whose two attribute values a1and a2lie\nin intervals [v1\nl,v1u)and[v2\nl,v2u), respectively (with vi\nl<viu). Clearly, this\ndelineates a rectangular region through which the curve passes, and all \ufb01les\nindexed by those segments of the curve that intersect with that region match\nthe search criterion. We therefore need an operation that returns a series of\ncurve-related indices given a region (expressed in terms of subsquares) in\nthe associated N-dimensional space. Such an operation is clearly dependent\non which space-\ufb01lling curve has been used, but interestingly, need not be\ndependent on actual entities.\nFinally, we need to maintain (references to) the entities associated with\nindices. One approach, used in the Squid system [Schmidt and Parashar,\n2008], is to use a Chord ring. In Squid, the index space is chosen to be the\nsame as that of the Chord ring, that is, both use m-bit identi\ufb01ers. Then clearly,\nthe Chord node responsible for index iwill store (references to) the entities\nindexed by i.\nNote 5.14 (More information: Other examples)\nDecentralized implementations of attribute-based naming systems have received\na lot of attention. The ones based on space-\ufb01lling curves are relatively popular,\nbut several alternatives have been proposed as well. Let us take a look at some\nrepresentative examples to shed some light on the design space.\nAttribute-value trees Here is another example where (attribute, value ) pairs\nare supported by a DHT-based system. First, assume that queries consist of a\nconjunction of pairs as with LDAP , that is, a user speci\ufb01es a list of attributes,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "292 CHAPTER 5. NAMING\nalong with the unique value he wants to see for every attribute. Such single-\nvalued queries are supported in the INS/Twine system [Balazinska et al., 2002].\nEach entity (referred to as a resource) is assumed to be described by means of\npossibly hierarchically organized attributes such as shown in Figure 5.31. Each\nsuch description is translated into an attribute-value tree (A VTree ) which is then\nused as the basis for an encoding that maps well onto a DHT-based system.\nFigure 5.31: (a) A general description of a resource. (b) Its representation\nas an AVTree.\nThe main issue is to transform the AVTrees again into a collection of keys\nthat can be looked up in a DHT system. In this case, every path originating in\nthe root is assigned a unique hash value, where a path description starts with\na link (representing an attribute), and ends either in a node (value), or another\nlink. Taking Figure 5.31 as our example, the following keys of all such paths are\nconsidered:\nKey Computed as\nh1: hash(type-book)\nh2: hash(type-book -author )\nh3: hash(type-book -author -Tolkien )\nh4: hash(type-book -title)\nh5: hash(type-book -title-LOTR )\nh6: hash(genre -fantasy )\nA node responsible for hash value hiwill keep (a reference to) the actual\nresource. In our example, this may lead to six nodes storing information on\nTolkien\u2019s Lord of the Rings. However, the bene\ufb01t of this redundancy is that it will\nallow supporting partial queries. For example, consider a query such as \u201cReturn\nbooks written by Tolkien.\u201d This query is translated into the AVTree shown in\nFigure 5.32 leading to computing the following three hashes:\nh1:hash(type-book)\nh2:hash(type-book -author )\nh3:hash(type-book -author -Tolkien )\nThese values will be sent to nodes that store information on Tolkien\u2019s books,\nand will at least return Lord of the Rings. Note that a hash such as h1is rather\ngeneral and will be generated often. These type of hashes can be \ufb01ltered out of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n292 CHAPTER 5. NAMING\nalong with the unique value he wants to see for every attribute. Such single-\nvalued queries are supported in the INS/Twine system [Balazinska et al., 2002].\nEach entity (referred to as a resource) is assumed to be described by means of\npossibly hierarchically organized attributes such as shown in Figure 5.31. Each\nsuch description is translated into an attribute-value tree (A VTree ) which is then\nused as the basis for an encoding that maps well onto a DHT-based system.\nFigure 5.31: (a) A general description of a resource. (b) Its representation\nas an AVTree.\nThe main issue is to transform the AVTrees again into a collection of keys\nthat can be looked up in a DHT system. In this case, every path originating in\nthe root is assigned a unique hash value, where a path description starts with\na link (representing an attribute), and ends either in a node (value), or another\nlink. Taking Figure 5.31 as our example, the following keys of all such paths are\nconsidered:\nKey Computed as\nh1: hash(type-book)\nh2: hash(type-book -author )\nh3: hash(type-book -author -Tolkien )\nh4: hash(type-book -title)\nh5: hash(type-book -title-LOTR )\nh6: hash(genre -fantasy )\nA node responsible for hash value hiwill keep (a reference to) the actual\nresource. In our example, this may lead to six nodes storing information on\nTolkien\u2019s Lord of the Rings. However, the bene\ufb01t of this redundancy is that it will\nallow supporting partial queries. For example, consider a query such as \u201cReturn\nbooks written by Tolkien.\u201d This query is translated into the AVTree shown in\nFigure 5.32 leading to computing the following three hashes:\nh1:hash(type-book)\nh2:hash(type-book -author )\nh3:hash(type-book -author -Tolkien )\nThese values will be sent to nodes that store information on Tolkien\u2019s books,\nand will at least return Lord of the Rings. Note that a hash such as h1is rather\ngeneral and will be generated often. These type of hashes can be \ufb01ltered out of\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.4. ATTRIBUTE-BASED NAMING 293\nthe system. Moreover, it is not dif\ufb01cult to see that only the most speci\ufb01c hashes\nneed to be evaluated.\nFigure 5.32: (a) The resource description of a query. (b) Its representation\nas an AVTree.\nSWORD: including range queries Let us take a look at queries that can contain\nrange speci\ufb01cations for attribute values. We discuss a solution adopted in the\nSWORD resource discovery system [Oppenheimer et al., 2005].\nIn SWORD, ( attribute, value ) pairs are \ufb01rst transformed into a key for a DHT.\nThese pairs always contain a single value; only queries may contain value ranges\nfor attributes. When computing the key (by means of a hash) the name of the\nattribute and its value are kept separate. Speci\ufb01c bits in the key will identify the\nattribute name, while others identify its value. In addition, the key will contain\na number of random bits to guarantee uniqueness among all keys that need to\nbe generated. In this way, the space of attributes is conveniently partitioned: if\nnbits are reserved to code attribute names, 2ndifferent server groups can be\nused, one group for each attribute name. Likewise, by using mbits to encode\nvalues, a further partitioning per server group can be applied to store speci\ufb01c\n(attribute, value ) pairs. DHTs are used only for distributing attribute names.\nFor each attribute name, the possible range of its value is partitioned into\nsubranges and a single server is assigned to each subrange. To explain, consider\na resource description with two attributes: a1taking values in the range [1..10]\nand a2taking values in the range [101...200 ]. Assume there are two servers for\na1:S11takes care of recording values of a1in[1..5], and S12for values in [6..10].\nLikewise, server S21records values for a2in range [101..150 ]and server S22\nfor values in [151..200 ]. Then, when an entity Ehas associated attribute values\n(a1=7,a2=175), server S12andserver S22will maintain a copy of, or a reference\ntoE.\nThe advantage of this scheme is that range queries can be easily supported.\nWhen a query is issued to return resources that have a2lying between 165 and\n189, the query can be forwarded to server S22who can then return the resources\nthat match the query range. The drawback, however, is that updates need to be\nsent to multiple servers. Moreover, it is not immediately clear how well the load is\nbalanced between the various servers. In particular, if certain range queries turn\nout to be very popular, speci\ufb01c servers will receive a high fraction of all queries.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.4. ATTRIBUTE-BASED NAMING 293\nthe system. Moreover, it is not dif\ufb01cult to see that only the most speci\ufb01c hashes\nneed to be evaluated.\nFigure 5.32: (a) The resource description of a query. (b) Its representation\nas an AVTree.\nSWORD: including range queries Let us take a look at queries that can contain\nrange speci\ufb01cations for attribute values. We discuss a solution adopted in the\nSWORD resource discovery system [Oppenheimer et al., 2005].\nIn SWORD, ( attribute, value ) pairs are \ufb01rst transformed into a key for a DHT.\nThese pairs always contain a single value; only queries may contain value ranges\nfor attributes. When computing the key (by means of a hash) the name of the\nattribute and its value are kept separate. Speci\ufb01c bits in the key will identify the\nattribute name, while others identify its value. In addition, the key will contain\na number of random bits to guarantee uniqueness among all keys that need to\nbe generated. In this way, the space of attributes is conveniently partitioned: if\nnbits are reserved to code attribute names, 2ndifferent server groups can be\nused, one group for each attribute name. Likewise, by using mbits to encode\nvalues, a further partitioning per server group can be applied to store speci\ufb01c\n(attribute, value ) pairs. DHTs are used only for distributing attribute names.\nFor each attribute name, the possible range of its value is partitioned into\nsubranges and a single server is assigned to each subrange. To explain, consider\na resource description with two attributes: a1taking values in the range [1..10]\nand a2taking values in the range [101...200 ]. Assume there are two servers for\na1:S11takes care of recording values of a1in[1..5], and S12for values in [6..10].\nLikewise, server S21records values for a2in range [101..150 ]and server S22\nfor values in [151..200 ]. Then, when an entity Ehas associated attribute values\n(a1=7,a2=175), server S12andserver S22will maintain a copy of, or a reference\ntoE.\nThe advantage of this scheme is that range queries can be easily supported.\nWhen a query is issued to return resources that have a2lying between 165 and\n189, the query can be forwarded to server S22who can then return the resources\nthat match the query range. The drawback, however, is that updates need to be\nsent to multiple servers. Moreover, it is not immediately clear how well the load is\nbalanced between the various servers. In particular, if certain range queries turn\nout to be very popular, speci\ufb01c servers will receive a high fraction of all queries.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "294 CHAPTER 5. NAMING\nSummarizing remarks There are indeed many different ways of supporting\nattribute-based naming systems in a decentralized fashion. The essence in all\ncases is to assign attributes to servers so that clients know where to direct their\nqueries, yet at the same time make sure that there is a balance in the load for the\nset of servers. In this light, supporting range queries requires special attention, if\nonly to decide, as in SWORD, which server is going to be responsible for which\nsubrange.\nIn practice, we see that when dealing with Nattributes many systems model\nthe collection of (attribute, value) pairs as an N-dimensional space in which each\nentity is represented by a unique point in that space. Conceptually, a search\naddresses a subspace and leads to identifying the servers responsible for that\nsubspace. In the simplest case, we assign each attribute to one server leading to\nO(N)servers. In this scheme, a query addressing kattributes needs to be sent\ntokservers, while the querying client needs to combine the results. We have\ndiscussed this case previously. The problem is to divide the ranges per attribute\namong subservers such that we have a reasonable balance of the workload. A\nsolution to this problem is discussed in [Bharambe et al., 2004].\nInstead of letting a client combine results, we can let servers collaborate. To\nthis end, the N-dimensional space is divided into subspaces by splitting each\ndimension dinto ndintervals. This splitting leads to a total of n1\u0002\u0001\u0001\u0001\u0002 nN\nsubspaces where each subspace is assigned to a separate server. Even with nd=2\nfor each dimension, we will face a total of O(2N)servers. Using space-\ufb01lling\ncurves, we can reduce the number of dimensions to one, and use a separate\ntechnique for deciding which N-dimensional subspace is served by which server.\nPractice indicates that load balancing may become an issue. An alternative\nsolution in which the number of dimensions is still reduced, but larger than one,\nwhile also maintaining load balancing, has been built into HyperDex [Escriva\net al., 2012]. The authors also address the problem of replication and consistency\nin case the naming system at the same time stores the entities which it indexes. In\nthat case, whenever a server indexes an entity E, it will have to be copied to the\nrespective server.\nWe \ufb01nally point to an attribute-based naming system that integrates naming\nwith resource selection service [Stratan et al., 2012]. Again, the attribute space is\nmodeled by an N-dimensional space in which each resource is associated with a\ncoordinate. In this situation, each resource maintains a link to a another resource,\nbut one that is responsible for a subspace of exponentially increasing size. The net\neffect is that each resource needs to have only a \ufb01xed number of neighbors, while\nrouting a query to the relevant subspace takes only a linear number of steps. The\norganization is akin to the use of \ufb01nger tables in Chord.\n5.5 Summary\nNames are used to refer to entities. Essentially, there are three types of names.\nAn address is the name of an access point associated with an entity, also\nsimply called the address of an entity. An identi\ufb01er is another type of name.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n294 CHAPTER 5. NAMING\nSummarizing remarks There are indeed many different ways of supporting\nattribute-based naming systems in a decentralized fashion. The essence in all\ncases is to assign attributes to servers so that clients know where to direct their\nqueries, yet at the same time make sure that there is a balance in the load for the\nset of servers. In this light, supporting range queries requires special attention, if\nonly to decide, as in SWORD, which server is going to be responsible for which\nsubrange.\nIn practice, we see that when dealing with Nattributes many systems model\nthe collection of (attribute, value) pairs as an N-dimensional space in which each\nentity is represented by a unique point in that space. Conceptually, a search\naddresses a subspace and leads to identifying the servers responsible for that\nsubspace. In the simplest case, we assign each attribute to one server leading to\nO(N)servers. In this scheme, a query addressing kattributes needs to be sent\ntokservers, while the querying client needs to combine the results. We have\ndiscussed this case previously. The problem is to divide the ranges per attribute\namong subservers such that we have a reasonable balance of the workload. A\nsolution to this problem is discussed in [Bharambe et al., 2004].\nInstead of letting a client combine results, we can let servers collaborate. To\nthis end, the N-dimensional space is divided into subspaces by splitting each\ndimension dinto ndintervals. This splitting leads to a total of n1\u0002\u0001\u0001\u0001\u0002 nN\nsubspaces where each subspace is assigned to a separate server. Even with nd=2\nfor each dimension, we will face a total of O(2N)servers. Using space-\ufb01lling\ncurves, we can reduce the number of dimensions to one, and use a separate\ntechnique for deciding which N-dimensional subspace is served by which server.\nPractice indicates that load balancing may become an issue. An alternative\nsolution in which the number of dimensions is still reduced, but larger than one,\nwhile also maintaining load balancing, has been built into HyperDex [Escriva\net al., 2012]. The authors also address the problem of replication and consistency\nin case the naming system at the same time stores the entities which it indexes. In\nthat case, whenever a server indexes an entity E, it will have to be copied to the\nrespective server.\nWe \ufb01nally point to an attribute-based naming system that integrates naming\nwith resource selection service [Stratan et al., 2012]. Again, the attribute space is\nmodeled by an N-dimensional space in which each resource is associated with a\ncoordinate. In this situation, each resource maintains a link to a another resource,\nbut one that is responsible for a subspace of exponentially increasing size. The net\neffect is that each resource needs to have only a \ufb01xed number of neighbors, while\nrouting a query to the relevant subspace takes only a linear number of steps. The\norganization is akin to the use of \ufb01nger tables in Chord.\n5.5 Summary\nNames are used to refer to entities. Essentially, there are three types of names.\nAn address is the name of an access point associated with an entity, also\nsimply called the address of an entity. An identi\ufb01er is another type of name.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "5.5. SUMMARY 295\nIt has three properties: each entity is referred to by exactly one identi\ufb01er, an\nidenti\ufb01er refers to only one entity, and is never assigned to another entity.\nFinally, human-friendly names are targeted to be used by humans and as such\nare represented as character strings. Given these types, we make a distinction\nbetween \ufb02at naming, structured naming, and attribute-based naming.\nSystems for \ufb02at naming essentially need to resolve an identi\ufb01er to the\naddress of its associated entity. This locating of an entity can be done in\ndifferent ways. The \ufb01rst approach is to use broadcasting or multicasting. The\nidenti\ufb01er of the entity is broadcast to every process in the distributed system.\nThe process offering an access point for the entity responds by providing an\naddress for that access point. Obviously, this approach has limited scalability.\nA second approach is to use forwarding pointers. Each time an entity\nmoves to a next location, it leaves behind a pointer telling where it will be\nnext. Locating the entity requires traversing the path of forwarding pointers.\nTo avoid large chains of pointers, it is important to reduce chains periodically\nA third approach is to allocate a home to an entity. Each time an entity\nmoves to another location, it informs its home where it is. Locating an entity\nproceeds by \ufb01rst asking its home for the current location.\nA fourth approach is to organize all nodes into a structured peer-to-peer\nsystem, and systematically assign nodes to entities taking their respective\nidenti\ufb01ers into account. By subsequently devising a routing algorithm by\nwhich lookup requests are moved toward the node responsible for a given\nentity, ef\ufb01cient and robust name resolution is possible.\nA \ufb01fth approach is to build a hierarchical search tree. The network is\ndivided into nonoverlapping domains. Domains can be grouped into higher-\nlevel (nonoverlapping) domains, and so on. There is a single top-level domain\nthat covers the entire network. Each domain at every level has an associated\ndirectory node. If an entity is located in a domain D, the directory node of the\nnext higher-level domain will have a pointer to D. A lowest-level directory\nnode stores the address of the entity. The top-level directory node knows\nabout all entities.\nStructured names are easily organized in a name space. A name space\ncan be represented by a naming graph in which a node represents a named\nentity and the label on an edge represents the name under which that entity\nis known. A node having multiple outgoing edges represents a collection of\nentities and is also known as a context node or directory. Large-scale naming\ngraphs are often organized as rooted acyclic directed graphs.\nNaming graphs are convenient to organize human-friendly names in a\nstructured way. An entity can be referred to by a path name. Name resolution\nis the process of traversing the naming graph by looking up the components\nof a path name, one at a time. A large-scale naming graph is implemented by\ndistributing its nodes across multiple name servers. When resolving a path\nname by traversing the naming graph, name resolution continues at the next\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n5.5. SUMMARY 295\nIt has three properties: each entity is referred to by exactly one identi\ufb01er, an\nidenti\ufb01er refers to only one entity, and is never assigned to another entity.\nFinally, human-friendly names are targeted to be used by humans and as such\nare represented as character strings. Given these types, we make a distinction\nbetween \ufb02at naming, structured naming, and attribute-based naming.\nSystems for \ufb02at naming essentially need to resolve an identi\ufb01er to the\naddress of its associated entity. This locating of an entity can be done in\ndifferent ways. The \ufb01rst approach is to use broadcasting or multicasting. The\nidenti\ufb01er of the entity is broadcast to every process in the distributed system.\nThe process offering an access point for the entity responds by providing an\naddress for that access point. Obviously, this approach has limited scalability.\nA second approach is to use forwarding pointers. Each time an entity\nmoves to a next location, it leaves behind a pointer telling where it will be\nnext. Locating the entity requires traversing the path of forwarding pointers.\nTo avoid large chains of pointers, it is important to reduce chains periodically\nA third approach is to allocate a home to an entity. Each time an entity\nmoves to another location, it informs its home where it is. Locating an entity\nproceeds by \ufb01rst asking its home for the current location.\nA fourth approach is to organize all nodes into a structured peer-to-peer\nsystem, and systematically assign nodes to entities taking their respective\nidenti\ufb01ers into account. By subsequently devising a routing algorithm by\nwhich lookup requests are moved toward the node responsible for a given\nentity, ef\ufb01cient and robust name resolution is possible.\nA \ufb01fth approach is to build a hierarchical search tree. The network is\ndivided into nonoverlapping domains. Domains can be grouped into higher-\nlevel (nonoverlapping) domains, and so on. There is a single top-level domain\nthat covers the entire network. Each domain at every level has an associated\ndirectory node. If an entity is located in a domain D, the directory node of the\nnext higher-level domain will have a pointer to D. A lowest-level directory\nnode stores the address of the entity. The top-level directory node knows\nabout all entities.\nStructured names are easily organized in a name space. A name space\ncan be represented by a naming graph in which a node represents a named\nentity and the label on an edge represents the name under which that entity\nis known. A node having multiple outgoing edges represents a collection of\nentities and is also known as a context node or directory. Large-scale naming\ngraphs are often organized as rooted acyclic directed graphs.\nNaming graphs are convenient to organize human-friendly names in a\nstructured way. An entity can be referred to by a path name. Name resolution\nis the process of traversing the naming graph by looking up the components\nof a path name, one at a time. A large-scale naming graph is implemented by\ndistributing its nodes across multiple name servers. When resolving a path\nname by traversing the naming graph, name resolution continues at the next\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "296 CHAPTER 5. NAMING\nname server as soon as a node is reached implemented by that server.\nMore problematic are attribute-based naming schemes in which entities are\ndescribed by a collection of ( attribute, value ) pairs. Queries are also formulated\nas such pairs, essentially requiring an exhaustive search through all descriptors.\nSuch a search is feasible only when the descriptors are stored in a single\ndatabase. However, alternative solutions have been devised by which the pairs\nare mapped onto DHT-based systems, essentially leading to a distribution of\nthe collection of entity descriptors. Using space-\ufb01lling curves, we can then\nmake different nodes responsible for different values of an attribute which\nhelps in effectively distributing the load among the nodes in the case of search\noperations.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n296 CHAPTER 5. NAMING\nname server as soon as a node is reached implemented by that server.\nMore problematic are attribute-based naming schemes in which entities are\ndescribed by a collection of ( attribute, value ) pairs. Queries are also formulated\nas such pairs, essentially requiring an exhaustive search through all descriptors.\nSuch a search is feasible only when the descriptors are stored in a single\ndatabase. However, alternative solutions have been devised by which the pairs\nare mapped onto DHT-based systems, essentially leading to a distribution of\nthe collection of entity descriptors. Using space-\ufb01lling curves, we can then\nmake different nodes responsible for different values of an attribute which\nhelps in effectively distributing the load among the nodes in the case of search\noperations.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 6\nCoordination\nIn the previous chapters, we have looked at processes and communication\nbetween processes. While communication is important, it is not the entire\nstory. Closely related is how processes cooperate and synchronize with one\nanother. Cooperation is partly supported by means of naming, which allows\nprocesses to at least share resources, or entities in general.\nIn this chapter, we mainly concentrate on how processes can synchronize\nand coordinate their actions. For example, it is important that multiple\nprocesses do not simultaneously access a shared resource, such as a \ufb01le, but\ninstead cooperate in granting each other temporary exclusive access. Another\nexample is that multiple processes may sometimes need to agree on the\nordering of events, such as whether message m1from process Pwas sent\nbefore or after message m2from process Q.\nSynchronization and coordination are two closely related phenomena. In\nprocess synchronization we make sure that one process waits for another to\ncomplete its operation. When dealing with data synchronization , the problem\nis to ensure that two sets of data are the same. When it comes to coordination ,\nthe goal is to manage the interactions and dependencies between activities\nin a distributed system [Malone and Crowston, 1994]. From this perspective,\none could state that coordination encapsulates synchronization.\nAs it turns out, coordination in distributed systems is often much more\ndif\ufb01cult compared to that in uniprocessor or multiprocessor systems. The\nproblems and solutions that are discussed in this chapter are, by their nature,\nrather general, and occur in many different situations in distributed systems.\nWe start with a discussion of the issue of synchronization based on actual\ntime, followed by synchronization in which only relative ordering matters\nrather than ordering in absolute time.\nIn many cases, it is important that a group of processes can appoint one\nprocess as a coordinator, which can be done by means of election algorithms.\nWe discuss various election algorithms in a separate section. Before that,\n297\nChapter 6\nCoordination\nIn the previous chapters, we have looked at processes and communication\nbetween processes. While communication is important, it is not the entire\nstory. Closely related is how processes cooperate and synchronize with one\nanother. Cooperation is partly supported by means of naming, which allows\nprocesses to at least share resources, or entities in general.\nIn this chapter, we mainly concentrate on how processes can synchronize\nand coordinate their actions. For example, it is important that multiple\nprocesses do not simultaneously access a shared resource, such as a \ufb01le, but\ninstead cooperate in granting each other temporary exclusive access. Another\nexample is that multiple processes may sometimes need to agree on the\nordering of events, such as whether message m1from process Pwas sent\nbefore or after message m2from process Q.\nSynchronization and coordination are two closely related phenomena. In\nprocess synchronization we make sure that one process waits for another to\ncomplete its operation. When dealing with data synchronization , the problem\nis to ensure that two sets of data are the same. When it comes to coordination ,\nthe goal is to manage the interactions and dependencies between activities\nin a distributed system [Malone and Crowston, 1994]. From this perspective,\none could state that coordination encapsulates synchronization.\nAs it turns out, coordination in distributed systems is often much more\ndif\ufb01cult compared to that in uniprocessor or multiprocessor systems. The\nproblems and solutions that are discussed in this chapter are, by their nature,\nrather general, and occur in many different situations in distributed systems.\nWe start with a discussion of the issue of synchronization based on actual\ntime, followed by synchronization in which only relative ordering matters\nrather than ordering in absolute time.\nIn many cases, it is important that a group of processes can appoint one\nprocess as a coordinator, which can be done by means of election algorithms.\nWe discuss various election algorithms in a separate section. Before that,\n297", "298 CHAPTER 6. COORDINATION\nwe look into a number of algorithms for coordinating mutual exclusion to a\nshared resource. As a special class of coordination problems, we also dive into\nlocation systems by which we place a process in a multidimensional plane.\nSuch placements come in handy when dealing with very large distributed\nsystems.\nWe already came across publish-subscribe systems, but have not yet dis-\ncussed in any detail how we actually match subscriptions to publications.\nThere are many ways to do this and we look at centralized as well as decen-\ntralized implementations.\nFinally, we consider three different gossip-based coordination problems:\naggregation, peer sampling, and overlay construction.\nDistributed algorithms come in all sorts and \ufb02avors and have been devel-\noped for very different types of distributed systems. Many examples (and\nfurther references) can be found in Andrews [2000], Cachin et al. [2011],\nand Fokkink [2013]. More formal approaches to a wealth of algorithms can\nbe found in text books from Attiya and Welch [2004], Lynch [1996], Santoro\n[2007], and Tel [2000].\n6.1 Clock synchronization\nIn a centralized system, time is unambiguous. When a process wants to know\nthe time, it simply makes a call to the operating system. If process Aasks for\nthe time, and then a little later process Basks for the time, the value that B\ngets will be higher than (or possibly equal to) the value Agot. It will certainly\nnot be lower. In a distributed system, achieving agreement on time is not\ntrivial.\nJust think, for a moment, about the implications of the lack of global time\non the Unix make program, as a simple example. Normally, in Unix large\nprograms are split up into multiple source \ufb01les, so that a change to one source\n\ufb01le requires only one \ufb01le to be recompiled, not all the \ufb01les. If a program\nconsists of 100 \ufb01les, not having to recompile everything because one \ufb01le has\nbeen changed greatly increases the speed at which programmers can work.\nThe way make normally works is simple. When the programmer has\n\ufb01nished changing all the source \ufb01les, he runs make , which examines the times\nat which all the source and object \ufb01les were last modi\ufb01ed. If the source \ufb01le\ninput.c has time 2151 and the corresponding object \ufb01le input.o has time\n2150, make knows that input.c has been changed since input.o was created,\nand thus input.c must be recompiled. On the other hand, if output.c has\ntime 2144 and output.o has time 2145, no compilation is needed. Thus make\ngoes through all the source \ufb01les to \ufb01nd out which ones need to be recompiled\nand calls the compiler to recompile them.\nNow imagine what could happen in a distributed system in which there\nwas no global agreement on time. Suppose that output.o has time 2144 as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n298 CHAPTER 6. COORDINATION\nwe look into a number of algorithms for coordinating mutual exclusion to a\nshared resource. As a special class of coordination problems, we also dive into\nlocation systems by which we place a process in a multidimensional plane.\nSuch placements come in handy when dealing with very large distributed\nsystems.\nWe already came across publish-subscribe systems, but have not yet dis-\ncussed in any detail how we actually match subscriptions to publications.\nThere are many ways to do this and we look at centralized as well as decen-\ntralized implementations.\nFinally, we consider three different gossip-based coordination problems:\naggregation, peer sampling, and overlay construction.\nDistributed algorithms come in all sorts and \ufb02avors and have been devel-\noped for very different types of distributed systems. Many examples (and\nfurther references) can be found in Andrews [2000], Cachin et al. [2011],\nand Fokkink [2013]. More formal approaches to a wealth of algorithms can\nbe found in text books from Attiya and Welch [2004], Lynch [1996], Santoro\n[2007], and Tel [2000].\n6.1 Clock synchronization\nIn a centralized system, time is unambiguous. When a process wants to know\nthe time, it simply makes a call to the operating system. If process Aasks for\nthe time, and then a little later process Basks for the time, the value that B\ngets will be higher than (or possibly equal to) the value Agot. It will certainly\nnot be lower. In a distributed system, achieving agreement on time is not\ntrivial.\nJust think, for a moment, about the implications of the lack of global time\non the Unix make program, as a simple example. Normally, in Unix large\nprograms are split up into multiple source \ufb01les, so that a change to one source\n\ufb01le requires only one \ufb01le to be recompiled, not all the \ufb01les. If a program\nconsists of 100 \ufb01les, not having to recompile everything because one \ufb01le has\nbeen changed greatly increases the speed at which programmers can work.\nThe way make normally works is simple. When the programmer has\n\ufb01nished changing all the source \ufb01les, he runs make , which examines the times\nat which all the source and object \ufb01les were last modi\ufb01ed. If the source \ufb01le\ninput.c has time 2151 and the corresponding object \ufb01le input.o has time\n2150, make knows that input.c has been changed since input.o was created,\nand thus input.c must be recompiled. On the other hand, if output.c has\ntime 2144 and output.o has time 2145, no compilation is needed. Thus make\ngoes through all the source \ufb01les to \ufb01nd out which ones need to be recompiled\nand calls the compiler to recompile them.\nNow imagine what could happen in a distributed system in which there\nwas no global agreement on time. Suppose that output.o has time 2144 as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 299\nabove, and shortly thereafter output.c is modi\ufb01ed but is assigned time 2143\nbecause the clock on its machine is slightly behind, as shown in Figure 6.1.\nMake will not call the compiler. The resulting executable binary program will\nthen contain a mixture of object \ufb01les from the old sources and the new sources.\nIt will probably crash and the programmer will go crazy trying to understand\nwhat is wrong with the code.\nFigure 6.1: When each machine has its own clock, an event that occurred after\nanother event may nevertheless be assigned an earlier time.\nThere are many more examples where an accurate account of time is\nneeded. The example above can easily be reformulated to \ufb01le timestamps in\ngeneral. In addition, think of application domains such as \ufb01nancial brokerage,\nsecurity auditing, and collaborative sensing, and it will become clear that\naccurate timing is important. Since time is so basic to the way people think\nand the effect of not having all the clocks synchronized can be so dramatic, it\nis \ufb01tting that we begin our study of synchronization with the simple question:\nIs it possible to synchronize all the clocks in a distributed system? The answer\nis surprisingly complicated.\nPhysical clocks\nNearly all computers have a circuit for keeping track of time. Despite the\nwidespread use of the word \u201cclock\u201d to refer to these devices, they are not\nactually clocks in the usual sense. Timer is perhaps a better word. A computer\ntimer is usually a precisely machined quartz crystal. When kept under tension,\nquartz crystals oscillate at a well-de\ufb01ned frequency that depends on the kind\nof crystal, how it is cut, and the amount of tension. Associated with each\ncrystal are two registers, a counter and a holding register . Each oscillation of\nthe crystal decrements the counter by one. When the counter gets to zero, an\ninterrupt is generated and the counter is reloaded from the holding register.\nIn this way, it is possible to program a timer to generate an interrupt 60 times\na second, or at any other desired frequency. Each interrupt is called one clock\ntick.\nWhen the system is booted, it usually asks the user to enter the date and\ntime, which is then converted to the number of ticks after some known starting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 299\nabove, and shortly thereafter output.c is modi\ufb01ed but is assigned time 2143\nbecause the clock on its machine is slightly behind, as shown in Figure 6.1.\nMake will not call the compiler. The resulting executable binary program will\nthen contain a mixture of object \ufb01les from the old sources and the new sources.\nIt will probably crash and the programmer will go crazy trying to understand\nwhat is wrong with the code.\nFigure 6.1: When each machine has its own clock, an event that occurred after\nanother event may nevertheless be assigned an earlier time.\nThere are many more examples where an accurate account of time is\nneeded. The example above can easily be reformulated to \ufb01le timestamps in\ngeneral. In addition, think of application domains such as \ufb01nancial brokerage,\nsecurity auditing, and collaborative sensing, and it will become clear that\naccurate timing is important. Since time is so basic to the way people think\nand the effect of not having all the clocks synchronized can be so dramatic, it\nis \ufb01tting that we begin our study of synchronization with the simple question:\nIs it possible to synchronize all the clocks in a distributed system? The answer\nis surprisingly complicated.\nPhysical clocks\nNearly all computers have a circuit for keeping track of time. Despite the\nwidespread use of the word \u201cclock\u201d to refer to these devices, they are not\nactually clocks in the usual sense. Timer is perhaps a better word. A computer\ntimer is usually a precisely machined quartz crystal. When kept under tension,\nquartz crystals oscillate at a well-de\ufb01ned frequency that depends on the kind\nof crystal, how it is cut, and the amount of tension. Associated with each\ncrystal are two registers, a counter and a holding register . Each oscillation of\nthe crystal decrements the counter by one. When the counter gets to zero, an\ninterrupt is generated and the counter is reloaded from the holding register.\nIn this way, it is possible to program a timer to generate an interrupt 60 times\na second, or at any other desired frequency. Each interrupt is called one clock\ntick.\nWhen the system is booted, it usually asks the user to enter the date and\ntime, which is then converted to the number of ticks after some known starting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "300 CHAPTER 6. COORDINATION\ndate and stored in memory. Most computers have a special battery-backed\nup CMOS RAM so that the date and time need not be entered on subsequent\nboots. At every clock tick, the interrupt service procedure adds one to the\ntime stored in memory. In this way, the (software) clock is kept up to date.\nWith a single computer and a single clock, it does not matter much if this\nclock is off by a small amount. Since all processes on the machine use the\nsame clock, they will still be internally consistent. For example, if the \ufb01le\ninput.c has time 2151 and \ufb01le input.o has time 2150, make will recompile the\nsource \ufb01le, even if the clock is off by 2 and the true times are 2153 and 2152,\nrespectively. All that really matters are the relative times.\nAs soon as multiple CPUs are introduced, each with its own clock, the sit-\nuation changes radically. Although the frequency at which a crystal oscillator\nruns is usually fairly stable, it is impossible to guarantee that the crystals in\ndifferent computers all run at exactly the same frequency. In practice, when\na system has ncomputers, all ncrystals will run at slightly different rates,\ncausing the (software) clocks gradually to get out of sync and give different\nvalues when read out. This difference in time values is called clock skew . As\na consequence of this clock skew, programs that expect the time associated\nwith a \ufb01le, object, process, or message to be correct and independent of the\nmachine on which it was generated (i.e., which clock it used) can fail, as we\nsaw in the make example above.\nIn some systems (e.g., real-time systems), the actual clock time is important.\nUnder these circumstances, external physical clocks are needed. For reasons of\nef\ufb01ciency and redundancy, multiple physical clocks are generally considered\ndesirable, which yields two problems: (1) how do we synchronize them with\nreal-world clocks, and (2) how do we synchronize the clocks with each other?\nNote 6.1 (More information: Determining real time)\nBefore answering these questions, let us digress slightly to see how time is\nactually measured. It is not nearly as easy as one might think, especially when\nhigh accuracy is required. Since the invention of mechanical clocks in the 17th\ncentury, time has been measured astronomically. Every day, the sun appears to\nrise on the eastern horizon, then climbs to a maximum height in the sky, and\n\ufb01nally sinks in the west. The event of the sun\u2019s reaching its highest apparent\npoint in the sky is called the transit of the sun . This event occurs at about noon\neach day. The interval between two consecutive transits of the sun is called the\nsolar day . Since there are 24 hours in a day, each containing 3600 seconds, the\nsolar second is de\ufb01ned as exactly 1/86400th of a solar day. The geometry of the\nmean solar day calculation is shown in Figure 6.2.\nIn the 1940s, it was established that the period of the earth\u2019s rotation is not\nconstant. The earth is slowing down due to tidal friction and atmospheric drag.\nBased on studies of growth patterns in ancient coral, geologists now believe that\n300 million years ago there were about 400 days per year. The length of the year\n(the time for one trip around the sun) is not thought to have changed; the day has\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n300 CHAPTER 6. COORDINATION\ndate and stored in memory. Most computers have a special battery-backed\nup CMOS RAM so that the date and time need not be entered on subsequent\nboots. At every clock tick, the interrupt service procedure adds one to the\ntime stored in memory. In this way, the (software) clock is kept up to date.\nWith a single computer and a single clock, it does not matter much if this\nclock is off by a small amount. Since all processes on the machine use the\nsame clock, they will still be internally consistent. For example, if the \ufb01le\ninput.c has time 2151 and \ufb01le input.o has time 2150, make will recompile the\nsource \ufb01le, even if the clock is off by 2 and the true times are 2153 and 2152,\nrespectively. All that really matters are the relative times.\nAs soon as multiple CPUs are introduced, each with its own clock, the sit-\nuation changes radically. Although the frequency at which a crystal oscillator\nruns is usually fairly stable, it is impossible to guarantee that the crystals in\ndifferent computers all run at exactly the same frequency. In practice, when\na system has ncomputers, all ncrystals will run at slightly different rates,\ncausing the (software) clocks gradually to get out of sync and give different\nvalues when read out. This difference in time values is called clock skew . As\na consequence of this clock skew, programs that expect the time associated\nwith a \ufb01le, object, process, or message to be correct and independent of the\nmachine on which it was generated (i.e., which clock it used) can fail, as we\nsaw in the make example above.\nIn some systems (e.g., real-time systems), the actual clock time is important.\nUnder these circumstances, external physical clocks are needed. For reasons of\nef\ufb01ciency and redundancy, multiple physical clocks are generally considered\ndesirable, which yields two problems: (1) how do we synchronize them with\nreal-world clocks, and (2) how do we synchronize the clocks with each other?\nNote 6.1 (More information: Determining real time)\nBefore answering these questions, let us digress slightly to see how time is\nactually measured. It is not nearly as easy as one might think, especially when\nhigh accuracy is required. Since the invention of mechanical clocks in the 17th\ncentury, time has been measured astronomically. Every day, the sun appears to\nrise on the eastern horizon, then climbs to a maximum height in the sky, and\n\ufb01nally sinks in the west. The event of the sun\u2019s reaching its highest apparent\npoint in the sky is called the transit of the sun . This event occurs at about noon\neach day. The interval between two consecutive transits of the sun is called the\nsolar day . Since there are 24 hours in a day, each containing 3600 seconds, the\nsolar second is de\ufb01ned as exactly 1/86400th of a solar day. The geometry of the\nmean solar day calculation is shown in Figure 6.2.\nIn the 1940s, it was established that the period of the earth\u2019s rotation is not\nconstant. The earth is slowing down due to tidal friction and atmospheric drag.\nBased on studies of growth patterns in ancient coral, geologists now believe that\n300 million years ago there were about 400 days per year. The length of the year\n(the time for one trip around the sun) is not thought to have changed; the day has\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 301\nsimply become longer. In addition to this long-term trend, short-term variations in\nthe length of the day also occur, probably caused by turbulence deep in the earth\u2019s\ncore of molten iron. These revelations lead astronomers to compute the length\nof the day by measuring a large number of days and taking the average before\ndividing by 86,400. The resulting quantity was called the mean solar second .\nFigure 6.2: Computation of the mean solar day.\nWith the invention of the atomic clock in 1948, it became possible to measure\ntime much more accurately, and independent of the wiggling and wobbling of the\nearth, by counting transitions of the cesium 133 atom. The physicists took over\nthe job of timekeeping from the astronomers and de\ufb01ned the second to be the\ntime it takes the cesium 133 atom to make exactly 9,192,631,770 transitions. The\nchoice of 9,192,631,770 was made to make the atomic second equal to the mean\nsolar second in the year of its introduction. Currently, several laboratories around\nthe world have cesium 133 clocks. Periodically, each laboratory tells the Bureau\nInternational de l\u2019Heure (BIH) in Paris how many times its clock has ticked. The\nBIH averages these to produce International Atomic Time , which is abbreviated\nTAI. Thus TAI is just the mean number of ticks of the cesium 133 clocks since\nmidnight on Jan. 1, 1958 (the beginning of time) divided by 9,192,631,770.\nAlthough TAI is highly stable and available to anyone who wants to go to the\ntrouble of buying a cesium clock, there is a serious problem with it; 86,400 TAI\nseconds is now about 3 msec less than a mean solar day (because the mean solar\nday is getting longer all the time). Using TAI for keeping time would mean that\nover the course of the years, noon would get earlier and earlier, until it would\neventually occur in the wee hours of the morning. People might notice this and\nwe could have the same kind of situation as occurred in 1582 when Pope Gregory\nXIII decreed that 10 days be omitted from the calendar. This event caused riots in\nthe streets because landlords demanded a full month\u2019s rent and bankers a full\nmonth\u2019s interest, while employers refused to pay workers for the 10 days they did\nnot work, to mention only a few of the con\ufb02icts. The Protestant countries, as a\nmatter of principle, refused to have anything to do with papal decrees and did\nnot accept the Gregorian calendar for 170 years.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 301\nsimply become longer. In addition to this long-term trend, short-term variations in\nthe length of the day also occur, probably caused by turbulence deep in the earth\u2019s\ncore of molten iron. These revelations lead astronomers to compute the length\nof the day by measuring a large number of days and taking the average before\ndividing by 86,400. The resulting quantity was called the mean solar second .\nFigure 6.2: Computation of the mean solar day.\nWith the invention of the atomic clock in 1948, it became possible to measure\ntime much more accurately, and independent of the wiggling and wobbling of the\nearth, by counting transitions of the cesium 133 atom. The physicists took over\nthe job of timekeeping from the astronomers and de\ufb01ned the second to be the\ntime it takes the cesium 133 atom to make exactly 9,192,631,770 transitions. The\nchoice of 9,192,631,770 was made to make the atomic second equal to the mean\nsolar second in the year of its introduction. Currently, several laboratories around\nthe world have cesium 133 clocks. Periodically, each laboratory tells the Bureau\nInternational de l\u2019Heure (BIH) in Paris how many times its clock has ticked. The\nBIH averages these to produce International Atomic Time , which is abbreviated\nTAI. Thus TAI is just the mean number of ticks of the cesium 133 clocks since\nmidnight on Jan. 1, 1958 (the beginning of time) divided by 9,192,631,770.\nAlthough TAI is highly stable and available to anyone who wants to go to the\ntrouble of buying a cesium clock, there is a serious problem with it; 86,400 TAI\nseconds is now about 3 msec less than a mean solar day (because the mean solar\nday is getting longer all the time). Using TAI for keeping time would mean that\nover the course of the years, noon would get earlier and earlier, until it would\neventually occur in the wee hours of the morning. People might notice this and\nwe could have the same kind of situation as occurred in 1582 when Pope Gregory\nXIII decreed that 10 days be omitted from the calendar. This event caused riots in\nthe streets because landlords demanded a full month\u2019s rent and bankers a full\nmonth\u2019s interest, while employers refused to pay workers for the 10 days they did\nnot work, to mention only a few of the con\ufb02icts. The Protestant countries, as a\nmatter of principle, refused to have anything to do with papal decrees and did\nnot accept the Gregorian calendar for 170 years.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "302 CHAPTER 6. COORDINATION\nBIH solves the problem by introducing leap seconds whenever the discrepancy\nbetween TAI and solar time grows to 800 msec. The use of leap seconds is\nillustrated in Figure 6.3. This correction gives rise to a time system based on\nconstant TAI seconds but which stays in phase with the apparent motion of the\nsun. This time system is known as UTC.\nFigure 6.3: TAI seconds are of constant length, unlike solar seconds. Leap\nseconds are introduced when necessary to keep in phase with the sun.\nMost electric power companies synchronize the timing of their 60-Hz or 50-Hz\nclocks to UTC, so when BIH announces a leap second, the power companies raise\ntheir frequency to 61 Hz or 51 Hz for 60 or 50 sec, to advance all the clocks in their\ndistribution area. Since 1 sec is a noticeable interval for a computer, an operating\nsystem that needs to keep accurate time over a period of years must have special\nsoftware to account for leap seconds as they are announced (unless they use the\npower line for time, which is usually too crude). The total number of leap seconds\nintroduced into UTC so far is about 30.\nThe basis for keeping global time is a called Universal Coordinated Time ,\nbut is abbreviated as UTC . UTC is the basis of all modern civil timekeeping\nand is a worldwide standard. To provide UTC to people who need precise\ntime, some 40 shortwave radio stations around the world broadcast a short\npulse at the start of each UTC second. The accuracy of these stations is about\n\u00061 msec, but due to random atmospheric \ufb02uctuations that can affect the\nlength of the signal path, in practice the accuracy is no better than \u000610 msec.\nSeveral earth satellites also offer a UTC service. The Geostationary Envi-\nronment Operational Satellite can provide UTC accurately to 0.5 msec, and\nsome other satellites do even better. By combining receptions from several\nsatellites, ground time servers can be built offering an accuracy of 50 nsec.\nUTC receivers are commercially available and many computers are equipped\nwith one.\nClock synchronization algorithms\nIf one machine has a UTC receiver, the goal becomes keeping all the other\nmachines synchronized to it. If no machines have UTC receivers, each machine\nkeeps track of its own time, and the goal is to keep all the machines together\nas well as possible. Many algorithms have been proposed for doing this\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n302 CHAPTER 6. COORDINATION\nBIH solves the problem by introducing leap seconds whenever the discrepancy\nbetween TAI and solar time grows to 800 msec. The use of leap seconds is\nillustrated in Figure 6.3. This correction gives rise to a time system based on\nconstant TAI seconds but which stays in phase with the apparent motion of the\nsun. This time system is known as UTC.\nFigure 6.3: TAI seconds are of constant length, unlike solar seconds. Leap\nseconds are introduced when necessary to keep in phase with the sun.\nMost electric power companies synchronize the timing of their 60-Hz or 50-Hz\nclocks to UTC, so when BIH announces a leap second, the power companies raise\ntheir frequency to 61 Hz or 51 Hz for 60 or 50 sec, to advance all the clocks in their\ndistribution area. Since 1 sec is a noticeable interval for a computer, an operating\nsystem that needs to keep accurate time over a period of years must have special\nsoftware to account for leap seconds as they are announced (unless they use the\npower line for time, which is usually too crude). The total number of leap seconds\nintroduced into UTC so far is about 30.\nThe basis for keeping global time is a called Universal Coordinated Time ,\nbut is abbreviated as UTC . UTC is the basis of all modern civil timekeeping\nand is a worldwide standard. To provide UTC to people who need precise\ntime, some 40 shortwave radio stations around the world broadcast a short\npulse at the start of each UTC second. The accuracy of these stations is about\n\u00061 msec, but due to random atmospheric \ufb02uctuations that can affect the\nlength of the signal path, in practice the accuracy is no better than \u000610 msec.\nSeveral earth satellites also offer a UTC service. The Geostationary Envi-\nronment Operational Satellite can provide UTC accurately to 0.5 msec, and\nsome other satellites do even better. By combining receptions from several\nsatellites, ground time servers can be built offering an accuracy of 50 nsec.\nUTC receivers are commercially available and many computers are equipped\nwith one.\nClock synchronization algorithms\nIf one machine has a UTC receiver, the goal becomes keeping all the other\nmachines synchronized to it. If no machines have UTC receivers, each machine\nkeeps track of its own time, and the goal is to keep all the machines together\nas well as possible. Many algorithms have been proposed for doing this\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 303\nsynchronization. Surveys are provided by Ramanathan et al. [1990], Horauer\n[2004], and Shin et al. [2011].\nAll clocks are based on some harmonic oscillator: an object that resonates\nat a certain frequency and from which we can subsequently derive time.\nAtomic clocks are based on the transitions of the cesium 133 atom, which is\nnot only very high, but also very constant. Hardware clocks in most computers\nuse a crystal oscillator based on quartz, which is also capable of producing\na very high, stable frequency, although not as stable as that of atomic clocks.\nA software clock in a computer is derived from that computer\u2019s hardware\nclock. In particular, the hardware clock is assumed to cause an interrupt f\ntimes per second. When this timer goes off, the interrupt handler adds 1 to a\ncounter that keeps track of the number of ticks (interrupts) since some agreed-\nupon time in the past. This counter acts as a software clock C, resonating at\nfrequency F.\nWhen the UTC time is t, denote by Cp(t)the value of the software clock\non machine p. The goal of clock synchronization algorithms is to keep the\ndeviation between the respective clocks of any two machines in a distributed\nsystem, within a speci\ufb01ed bound, known as the precision p:\n8t,8p,q:jCp(t)\u0000Cq(t)j\u0014p\nNote that precision refers to the deviation of clocks only between machines\nthat are part of a distributed system. When considering an external reference\npoint, like UTC, we speak of accuracy , aiming to keep it bound to a value a:\n8t,8p:jCp(t)\u0000tj\u0014a\nThe whole idea of clock synchronization is that we keep clocks precise , referred\nto as internal synchronization oraccurate , known as external synchroniza-\ntion. A set of clocks that are accurate within bound a, will be precise within\nbound p=2a. However, being precise does not allow us to conclude anything\nabout the accuracy of clocks.\nIn a perfect world, we would have Cp(t) = tfor all pand all t, and\nthus a=p=0. Unfortunately, hardware clocks, and thus also software\nclocks, are subject to clock drift : because their frequency is not perfect and\naffected by external sources such as temperature, clocks on different machines\nwill gradually start showing different values for time. This is known as\ntheclock drift rate : the difference per unit of time from a perfect reference\nclock. A typical quartz-based hardware clock has a clock drift rate of some\n10\u00006seconds per second, or approximately 31.5 seconds per year. Computer\nhardware clocks exist that have much lower drift rates.\nThe speci\ufb01cations of a hardware clock include its maximum clock drift\nrate r. IfF(t)denotes the actual oscillator frequency of the hardware clock at\ntime tand Fits ideal (constant) frequency, then a hardware clock is living up\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 303\nsynchronization. Surveys are provided by Ramanathan et al. [1990], Horauer\n[2004], and Shin et al. [2011].\nAll clocks are based on some harmonic oscillator: an object that resonates\nat a certain frequency and from which we can subsequently derive time.\nAtomic clocks are based on the transitions of the cesium 133 atom, which is\nnot only very high, but also very constant. Hardware clocks in most computers\nuse a crystal oscillator based on quartz, which is also capable of producing\na very high, stable frequency, although not as stable as that of atomic clocks.\nA software clock in a computer is derived from that computer\u2019s hardware\nclock. In particular, the hardware clock is assumed to cause an interrupt f\ntimes per second. When this timer goes off, the interrupt handler adds 1 to a\ncounter that keeps track of the number of ticks (interrupts) since some agreed-\nupon time in the past. This counter acts as a software clock C, resonating at\nfrequency F.\nWhen the UTC time is t, denote by Cp(t)the value of the software clock\non machine p. The goal of clock synchronization algorithms is to keep the\ndeviation between the respective clocks of any two machines in a distributed\nsystem, within a speci\ufb01ed bound, known as the precision p:\n8t,8p,q:jCp(t)\u0000Cq(t)j\u0014p\nNote that precision refers to the deviation of clocks only between machines\nthat are part of a distributed system. When considering an external reference\npoint, like UTC, we speak of accuracy , aiming to keep it bound to a value a:\n8t,8p:jCp(t)\u0000tj\u0014a\nThe whole idea of clock synchronization is that we keep clocks precise , referred\nto as internal synchronization oraccurate , known as external synchroniza-\ntion. A set of clocks that are accurate within bound a, will be precise within\nbound p=2a. However, being precise does not allow us to conclude anything\nabout the accuracy of clocks.\nIn a perfect world, we would have Cp(t) = tfor all pand all t, and\nthus a=p=0. Unfortunately, hardware clocks, and thus also software\nclocks, are subject to clock drift : because their frequency is not perfect and\naffected by external sources such as temperature, clocks on different machines\nwill gradually start showing different values for time. This is known as\ntheclock drift rate : the difference per unit of time from a perfect reference\nclock. A typical quartz-based hardware clock has a clock drift rate of some\n10\u00006seconds per second, or approximately 31.5 seconds per year. Computer\nhardware clocks exist that have much lower drift rates.\nThe speci\ufb01cations of a hardware clock include its maximum clock drift\nrate r. IfF(t)denotes the actual oscillator frequency of the hardware clock at\ntime tand Fits ideal (constant) frequency, then a hardware clock is living up\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "304 CHAPTER 6. COORDINATION\nto its speci\ufb01cations if\n8t:(1\u0000r)\u0014F(t)\nF\u0014(1+r)\nBy using hardware interrupts we are directly coupling a software clock to the\nhardware clock, and thus also its clock drift rate. In particular, we have that\nCp(t) =1\nFZt\n0F(t)dt, and thus:dCp(t)\ndt=F(t)\nF\nwhich brings us to our ultimate goal, namely keeping the software clock drift\nrate also bounded to r:\n8t: 1\u0000r\u0014dCp(t)\ndt\u00141+r\nSlow, perfect, and fast clocks are shown in Figure 6.4.\nFigure 6.4: The relation between clock time and UTC when clocks tick at\ndifferent rates.\nIf two clocks are drifting from UTC in the opposite direction, at a time\nDtafter they were synchronized, they may be as much as 2r\u0001Dtapart. If\nthe system designers want to guarantee a precision p, that is, that no two\nclocks ever differ by more than pseconds, clocks must be resynchronized\n(in software) at least every p/(2r)seconds. The various algorithms differ in\nprecisely how this resynchronization is done.\nNetwork Time Protocol\nA common approach in many protocols and originally proposed by Cristian\n[1989], is to let clients contact a time server. The latter can accurately provide\nthe current time, for example, because it is equipped with a UTC receiver\nor an accurate clock. The problem, of course, is that when contacting the\nserver, message delays will have outdated the reported time. The trick is to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n304 CHAPTER 6. COORDINATION\nto its speci\ufb01cations if\n8t:(1\u0000r)\u0014F(t)\nF\u0014(1+r)\nBy using hardware interrupts we are directly coupling a software clock to the\nhardware clock, and thus also its clock drift rate. In particular, we have that\nCp(t) =1\nFZt\n0F(t)dt, and thus:dCp(t)\ndt=F(t)\nF\nwhich brings us to our ultimate goal, namely keeping the software clock drift\nrate also bounded to r:\n8t: 1\u0000r\u0014dCp(t)\ndt\u00141+r\nSlow, perfect, and fast clocks are shown in Figure 6.4.\nFigure 6.4: The relation between clock time and UTC when clocks tick at\ndifferent rates.\nIf two clocks are drifting from UTC in the opposite direction, at a time\nDtafter they were synchronized, they may be as much as 2r\u0001Dtapart. If\nthe system designers want to guarantee a precision p, that is, that no two\nclocks ever differ by more than pseconds, clocks must be resynchronized\n(in software) at least every p/(2r)seconds. The various algorithms differ in\nprecisely how this resynchronization is done.\nNetwork Time Protocol\nA common approach in many protocols and originally proposed by Cristian\n[1989], is to let clients contact a time server. The latter can accurately provide\nthe current time, for example, because it is equipped with a UTC receiver\nor an accurate clock. The problem, of course, is that when contacting the\nserver, message delays will have outdated the reported time. The trick is to\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 305\nFigure 6.5: Getting the current time from a time server.\n\ufb01nd a good estimation for these delays. Consider the situation sketched in\nFigure 6.5.\nIn this case, Awill send a request to B, timestamped with value T1.B,\nin turn, will record the time of receipt T2(taken from its own local clock),\nand returns a response timestamped with value T3, and piggybacking the\npreviously recorded value T2. Finally, Arecords the time of the response\u2019s\narrival, T4. Let us assume that the propagation delays from AtoBis roughly\nthe same as BtoA, meaning that dTreq=T2\u0000T1\u0019T4\u0000T3=dTres. In that\ncase, Acan estimate its offset relative to Bas\nq=T3+(T2\u0000T1) + ( T4\u0000T3)\n2\u0000T4=(T2\u0000T1) + ( T3\u0000T4)\n2\nOf course, time is not allowed to run backward. If A\u2019s clock is fast, q<0,\nmeaning that Ashould, in principle, set its clock backward. This is not allowed\nas it could cause serious problems such as an object \ufb01le compiled just after\nthe clock change having a time earlier than the source which was modi\ufb01ed\njust before the clock change.\nSuch a change must be introduced gradually. One way is as follows.\nSuppose that the timer is set to generate 100 interrupts per second. Normally,\neach interrupt would add 10 msec to the time. When slowing down, the\ninterrupt routine adds only 9 msec each time until the correction has been\nmade. Similarly, the clock can be advanced gradually by adding 11 msec at\neach interrupt instead of jumping it forward all at once.\nIn the case of the network time protocol (NTP ), this protocol is set up\npairwise between servers. In other words, Bwill also probe Afor its current\ntime. The offset qis computed as given above, along with the estimation dfor\nthe delay:\nd=(T4\u0000T1)\u0000(T3\u0000T2)\n2\nEight pairs of ( q,d) values are buffered, \ufb01nally taking the minimal value\nfound for das the best estimation for the delay between the two servers, and\nsubsequently the associated value qas the most reliable estimation of the\noffset.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 305\nFigure 6.5: Getting the current time from a time server.\n\ufb01nd a good estimation for these delays. Consider the situation sketched in\nFigure 6.5.\nIn this case, Awill send a request to B, timestamped with value T1.B,\nin turn, will record the time of receipt T2(taken from its own local clock),\nand returns a response timestamped with value T3, and piggybacking the\npreviously recorded value T2. Finally, Arecords the time of the response\u2019s\narrival, T4. Let us assume that the propagation delays from AtoBis roughly\nthe same as BtoA, meaning that dTreq=T2\u0000T1\u0019T4\u0000T3=dTres. In that\ncase, Acan estimate its offset relative to Bas\nq=T3+(T2\u0000T1) + ( T4\u0000T3)\n2\u0000T4=(T2\u0000T1) + ( T3\u0000T4)\n2\nOf course, time is not allowed to run backward. If A\u2019s clock is fast, q<0,\nmeaning that Ashould, in principle, set its clock backward. This is not allowed\nas it could cause serious problems such as an object \ufb01le compiled just after\nthe clock change having a time earlier than the source which was modi\ufb01ed\njust before the clock change.\nSuch a change must be introduced gradually. One way is as follows.\nSuppose that the timer is set to generate 100 interrupts per second. Normally,\neach interrupt would add 10 msec to the time. When slowing down, the\ninterrupt routine adds only 9 msec each time until the correction has been\nmade. Similarly, the clock can be advanced gradually by adding 11 msec at\neach interrupt instead of jumping it forward all at once.\nIn the case of the network time protocol (NTP ), this protocol is set up\npairwise between servers. In other words, Bwill also probe Afor its current\ntime. The offset qis computed as given above, along with the estimation dfor\nthe delay:\nd=(T4\u0000T1)\u0000(T3\u0000T2)\n2\nEight pairs of ( q,d) values are buffered, \ufb01nally taking the minimal value\nfound for das the best estimation for the delay between the two servers, and\nsubsequently the associated value qas the most reliable estimation of the\noffset.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "306 CHAPTER 6. COORDINATION\nApplying NTP symmetrically should, in principle, also let Badjust its\nclock to that of A. However, if B\u2019s clock is known to be more accurate, then\nsuch an adjustment would be foolish. To solve this problem, NTP divides\nservers into strata. A server with a reference clock such as a UTC receiver or\nan atomic clock, is known to be a stratum-1 server (the clock itself is said to\noperate at stratum 0). When Acontacts B, it will adjust only its time if its own\nstratum level is higher than that of B. Moreover, after the synchronization, A\u2019s\nstratum level will become one higher than that of B. In other words, if Bis a\nstratum- kserver, then Awill become a stratum- (k+1)server if its original\nstratum level was already larger than k. Due to the symmetry of NTP , if A\u2019s\nstratum level was lower than that of B,Bwill adjust itself to A.\nThere are many important features about NTP , of which many relate to\nidentifying and masking errors, but also security attacks. NTP was originally\ndescribed in [Mills, 1992] and is known to achieve (worldwide) accuracy in\nthe range of 1\u201350 msec. A detailed description of NTP can be found in [Mills,\n2011].\nThe Berkeley algorithm\nIn many clock synchronization algorithms the time server is passive. Other\nmachines periodically ask it for the time. All it does is respond to their\nqueries. In Berkeley Unix exactly the opposite approach is taken [Gusella\nand Zatti, 1989]. Here the time server (actually, a time daemon) is active,\npolling every machine from time to time to ask what time it is there. Based\non the answers, it computes an average time and tells all the other machines\nto advance their clocks to the new time or slow their clocks down until some\nspeci\ufb01ed reduction has been achieved. This method is suitable for a system in\nwhich no machine has a UTC receiver. The time daemon\u2019s time must be set\nmanually by the operator periodically. The method is illustrated in Figure 6.6.\nIn Figure 6.6(a) at 3:00, the time daemon tells the other machines its time\nand asks for theirs. In Figure 6.6(b) they respond with how far ahead or\nbehind the time daemon they are. Armed with these numbers, the time\ndaemon computes the average and tells each machine how to adjust its clock\n[see Figure 6.6(c)].\nNote that for many purposes, it is suf\ufb01cient that all machines agree on\nthe same time. It is not essential that this time also agrees with the real time\nas announced on the radio every hour. If in our example of Figure 6.6 the\ntime daemon\u2019s clock would never be manually calibrated, no harm is done\nprovided none of the other nodes communicates with external computers.\nEveryone will just happily agree on a current time, without that value having\nany relation with reality. The Berkeley algorithm is thus typically an internal\nclock synchronization algorithm.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n306 CHAPTER 6. COORDINATION\nApplying NTP symmetrically should, in principle, also let Badjust its\nclock to that of A. However, if B\u2019s clock is known to be more accurate, then\nsuch an adjustment would be foolish. To solve this problem, NTP divides\nservers into strata. A server with a reference clock such as a UTC receiver or\nan atomic clock, is known to be a stratum-1 server (the clock itself is said to\noperate at stratum 0). When Acontacts B, it will adjust only its time if its own\nstratum level is higher than that of B. Moreover, after the synchronization, A\u2019s\nstratum level will become one higher than that of B. In other words, if Bis a\nstratum- kserver, then Awill become a stratum- (k+1)server if its original\nstratum level was already larger than k. Due to the symmetry of NTP , if A\u2019s\nstratum level was lower than that of B,Bwill adjust itself to A.\nThere are many important features about NTP , of which many relate to\nidentifying and masking errors, but also security attacks. NTP was originally\ndescribed in [Mills, 1992] and is known to achieve (worldwide) accuracy in\nthe range of 1\u201350 msec. A detailed description of NTP can be found in [Mills,\n2011].\nThe Berkeley algorithm\nIn many clock synchronization algorithms the time server is passive. Other\nmachines periodically ask it for the time. All it does is respond to their\nqueries. In Berkeley Unix exactly the opposite approach is taken [Gusella\nand Zatti, 1989]. Here the time server (actually, a time daemon) is active,\npolling every machine from time to time to ask what time it is there. Based\non the answers, it computes an average time and tells all the other machines\nto advance their clocks to the new time or slow their clocks down until some\nspeci\ufb01ed reduction has been achieved. This method is suitable for a system in\nwhich no machine has a UTC receiver. The time daemon\u2019s time must be set\nmanually by the operator periodically. The method is illustrated in Figure 6.6.\nIn Figure 6.6(a) at 3:00, the time daemon tells the other machines its time\nand asks for theirs. In Figure 6.6(b) they respond with how far ahead or\nbehind the time daemon they are. Armed with these numbers, the time\ndaemon computes the average and tells each machine how to adjust its clock\n[see Figure 6.6(c)].\nNote that for many purposes, it is suf\ufb01cient that all machines agree on\nthe same time. It is not essential that this time also agrees with the real time\nas announced on the radio every hour. If in our example of Figure 6.6 the\ntime daemon\u2019s clock would never be manually calibrated, no harm is done\nprovided none of the other nodes communicates with external computers.\nEveryone will just happily agree on a current time, without that value having\nany relation with reality. The Berkeley algorithm is thus typically an internal\nclock synchronization algorithm.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 307\n(a) (b) (c)\nFigure 6.6: (a) The time daemon asks all the other machines for their clock\nvalues. (b) The machines answer. (c) The time daemon tells everyone how to\nadjust their clock.\nClock synchronization in wireless networks\nAn important advantage of more traditional distributed systems is that we\ncan easily and ef\ufb01ciently deploy time servers. Moreover, most machines can\ncontact each other, allowing for a relatively simple dissemination of infor-\nmation. These assumptions are no longer valid in many wireless networks,\nnotably sensor networks. Nodes are resource constrained, and multihop\nrouting is expensive. In addition, it is often important to optimize algorithms\nfor energy consumption. These and other observations have led to the design\nof very different clock synchronization algorithms for wireless networks. In\nthe following, we consider one speci\ufb01c solution. Sivrikaya and Yener [2004]\nprovide a brief overview of other solutions. An extensive survey can be found\nin [Sundararaman et al., 2005].\nReference broadcast synchronization (RBS ) is a clock synchronization\nprotocol that is quite different from other proposals [Elson et al., 2002]. First,\nthe protocol does not assume that there is a single node with an accurate\naccount of the actual time available. Instead of aiming to provide all nodes\nUTC time, it aims at merely internally synchronizing the clocks, just as the\nBerkeley algorithm does. Second, the solutions we have discussed so far are\ndesigned to bring the sender and receiver into sync, essentially following a\ntwo-way protocol. RBS deviates from this pattern by letting only the receivers\nsynchronize, keeping the sender out of the loop.\nIn RBS, a sender broadcasts a reference message that will allow its receivers\nto adjust their clocks. A key observation is that in a sensor network the time\nto propagate a signal to other nodes is roughly constant, provided no multi-\nhop routing is assumed. Propagation time in this case is measured from\nthe moment that a message leaves the network interface of the sender. As\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 307\n(a) (b) (c)\nFigure 6.6: (a) The time daemon asks all the other machines for their clock\nvalues. (b) The machines answer. (c) The time daemon tells everyone how to\nadjust their clock.\nClock synchronization in wireless networks\nAn important advantage of more traditional distributed systems is that we\ncan easily and ef\ufb01ciently deploy time servers. Moreover, most machines can\ncontact each other, allowing for a relatively simple dissemination of infor-\nmation. These assumptions are no longer valid in many wireless networks,\nnotably sensor networks. Nodes are resource constrained, and multihop\nrouting is expensive. In addition, it is often important to optimize algorithms\nfor energy consumption. These and other observations have led to the design\nof very different clock synchronization algorithms for wireless networks. In\nthe following, we consider one speci\ufb01c solution. Sivrikaya and Yener [2004]\nprovide a brief overview of other solutions. An extensive survey can be found\nin [Sundararaman et al., 2005].\nReference broadcast synchronization (RBS ) is a clock synchronization\nprotocol that is quite different from other proposals [Elson et al., 2002]. First,\nthe protocol does not assume that there is a single node with an accurate\naccount of the actual time available. Instead of aiming to provide all nodes\nUTC time, it aims at merely internally synchronizing the clocks, just as the\nBerkeley algorithm does. Second, the solutions we have discussed so far are\ndesigned to bring the sender and receiver into sync, essentially following a\ntwo-way protocol. RBS deviates from this pattern by letting only the receivers\nsynchronize, keeping the sender out of the loop.\nIn RBS, a sender broadcasts a reference message that will allow its receivers\nto adjust their clocks. A key observation is that in a sensor network the time\nto propagate a signal to other nodes is roughly constant, provided no multi-\nhop routing is assumed. Propagation time in this case is measured from\nthe moment that a message leaves the network interface of the sender. As\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "308 CHAPTER 6. COORDINATION\na consequence, two important sources for variation in message transfer no\nlonger play a role in estimating delays: the time spent to construct a message,\nand the time spent to access the network. This principle is shown in Figure 6.7.\nFigure 6.7: The usual critical path and the one used in RBS in determining\nnetwork delays.\nNote that in protocols such as NTP , a timestamp is added to the message\nbefore it is passed on to the network interface. Furthermore, as wireless\nnetworks are based on a contention protocol, there is generally no saying how\nlong it will take before a message can actually be transmitted. These factors\nof nondeterminism are eliminated in RBS. What remains is the delivery time\nat the receiver, but this time varies considerably less than the network-access\ntime.\nThe idea underlying RBS is simple: when a node broadcasts a reference\nmessage m, each node psimply records the time Tp,mthat it received m. Note\nthat Tp,mis read from p\u2019s local clock. Ignoring clock skew, two nodes pand q\ncan exchange each other\u2019s delivery times in order to estimate their mutual,\nrelative offset:\nOffset [p,q] =\u00e5M\nk=1(Tp,k\u0000Tq,k)\nM\nwhere Mis the total number of reference messages sent. This information is\nimportant: node pwill know the value of q\u2019s clock relative to its own value.\nMoreover, if it simply stores these offsets, there is no need to adjust its own\nclock, which saves energy.\nUnfortunately, clocks can drift apart. The effect is that simply computing\nthe average offset as done above will not work: the last values sent are\nsimply less accurate than the \ufb01rst ones. Moreover, as time goes by, the offset\nwill presumably increase. Elson et al. [2002] use a very simple algorithm to\ncompensate for this: instead of computing an average they apply standard\nlinear regression to compute the offset as a function:\nOffset [p,q](t) =at+b\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n308 CHAPTER 6. COORDINATION\na consequence, two important sources for variation in message transfer no\nlonger play a role in estimating delays: the time spent to construct a message,\nand the time spent to access the network. This principle is shown in Figure 6.7.\nFigure 6.7: The usual critical path and the one used in RBS in determining\nnetwork delays.\nNote that in protocols such as NTP , a timestamp is added to the message\nbefore it is passed on to the network interface. Furthermore, as wireless\nnetworks are based on a contention protocol, there is generally no saying how\nlong it will take before a message can actually be transmitted. These factors\nof nondeterminism are eliminated in RBS. What remains is the delivery time\nat the receiver, but this time varies considerably less than the network-access\ntime.\nThe idea underlying RBS is simple: when a node broadcasts a reference\nmessage m, each node psimply records the time Tp,mthat it received m. Note\nthat Tp,mis read from p\u2019s local clock. Ignoring clock skew, two nodes pand q\ncan exchange each other\u2019s delivery times in order to estimate their mutual,\nrelative offset:\nOffset [p,q] =\u00e5M\nk=1(Tp,k\u0000Tq,k)\nM\nwhere Mis the total number of reference messages sent. This information is\nimportant: node pwill know the value of q\u2019s clock relative to its own value.\nMoreover, if it simply stores these offsets, there is no need to adjust its own\nclock, which saves energy.\nUnfortunately, clocks can drift apart. The effect is that simply computing\nthe average offset as done above will not work: the last values sent are\nsimply less accurate than the \ufb01rst ones. Moreover, as time goes by, the offset\nwill presumably increase. Elson et al. [2002] use a very simple algorithm to\ncompensate for this: instead of computing an average they apply standard\nlinear regression to compute the offset as a function:\nOffset [p,q](t) =at+b\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.1. CLOCK SYNCHRONIZATION 309\nThe constants aandbare computed from the pairs (Tp,k,Tq,k). This new form\nwill allow a much more accurate computation of q\u2019s current clock value by\nnode p, and vice versa.\nNote 6.2 (More information: How important is an accurate account of time?)\nSo why is time such a big deal for distributed systems? As we shall discuss in\nthe remainder of this chapter, reaching consensus on a global ordering of events\nis what we really want, and this can be achieved without any notion of global\nabsolute time. However, as will become clear, alternative methods for distributed\ncoordination do not come easy.\nAnd life would be much simpler if processes in a distributed system could\ntimestamp their events with in\ufb01nite precision. Although in\ufb01nite precision is\nasking too much, we can come practically close. Researchers at Google were\nconfronted with the fact that their customers would really like to make use of\na globally distributed database that supported transactions. Such a database\nwould need to serve massive numbers of clients, rendering the use of, for ex-\nample, a central transaction processing monitor as we discussed in Section 1.3,\ninfeasible. Instead, for their Spanner system, Google decided to implement a\ntrue-time service, called TrueTime [Corbett et al., 2013]. This service provides\nthree operations:\nOperation Result\nTT.now() A time interval [Tlwb,Tupb]with Tlwb<Tupb\nTT.after(t) True if timestamp thas de\ufb01nitely passed\nTT.before(t) True if timestamp thas de\ufb01nitely not arrived\nThe most important aspect is that Tlwband Tupbareguaranteed bounds. Of\ncourse, if e=Tupb\u0000Tlwbis large, say 1 hour, then implementing the service\nis relatively easy. Impressively enough, e=6ms. To achieve this accuracy, the\nTrueTime service makes use of time master machines of which there are several\nper data center. Time-slave daemons run on every machine in a data center\nand query multiple time masters, including ones from other data centers, very\nsimilar to what we described for NTP . Many time masters are equipped with\naccurate GPS receivers, while many others are independently equipped with\natomic clocks. The result is a collection of time sources with a high degree of\nmutual independence (which is important for reasons of fault tolerance). Using\na version of an algorithm developed by Marzullo and Owicki [1983], outliers\nare kept out of the computations. Meanwhile, the performance of TrueTime is\ncontinuously monitored and \u201cbad\u201d time machines are (manually) removed to give\nat least very high guarantees for the accuracy of the TrueTime service.\nWith a guaranteed accuracy of 6 milliseconds, building a transactional system\nbecomes much easier: transactions can actually be timestamped, even by different\nservers, with the restriction that timestamping may need to be delayed for etime\nunits. More precisely, in order to know for sure that a transaction has committed,\nreading the resulting data may impose a wait for eunits. This is achieved by\npessimistically assigning a timestamp to a transaction that writes data to the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.1. CLOCK SYNCHRONIZATION 309\nThe constants aandbare computed from the pairs (Tp,k,Tq,k). This new form\nwill allow a much more accurate computation of q\u2019s current clock value by\nnode p, and vice versa.\nNote 6.2 (More information: How important is an accurate account of time?)\nSo why is time such a big deal for distributed systems? As we shall discuss in\nthe remainder of this chapter, reaching consensus on a global ordering of events\nis what we really want, and this can be achieved without any notion of global\nabsolute time. However, as will become clear, alternative methods for distributed\ncoordination do not come easy.\nAnd life would be much simpler if processes in a distributed system could\ntimestamp their events with in\ufb01nite precision. Although in\ufb01nite precision is\nasking too much, we can come practically close. Researchers at Google were\nconfronted with the fact that their customers would really like to make use of\na globally distributed database that supported transactions. Such a database\nwould need to serve massive numbers of clients, rendering the use of, for ex-\nample, a central transaction processing monitor as we discussed in Section 1.3,\ninfeasible. Instead, for their Spanner system, Google decided to implement a\ntrue-time service, called TrueTime [Corbett et al., 2013]. This service provides\nthree operations:\nOperation Result\nTT.now() A time interval [Tlwb,Tupb]with Tlwb<Tupb\nTT.after(t) True if timestamp thas de\ufb01nitely passed\nTT.before(t) True if timestamp thas de\ufb01nitely not arrived\nThe most important aspect is that Tlwband Tupbareguaranteed bounds. Of\ncourse, if e=Tupb\u0000Tlwbis large, say 1 hour, then implementing the service\nis relatively easy. Impressively enough, e=6ms. To achieve this accuracy, the\nTrueTime service makes use of time master machines of which there are several\nper data center. Time-slave daemons run on every machine in a data center\nand query multiple time masters, including ones from other data centers, very\nsimilar to what we described for NTP . Many time masters are equipped with\naccurate GPS receivers, while many others are independently equipped with\natomic clocks. The result is a collection of time sources with a high degree of\nmutual independence (which is important for reasons of fault tolerance). Using\na version of an algorithm developed by Marzullo and Owicki [1983], outliers\nare kept out of the computations. Meanwhile, the performance of TrueTime is\ncontinuously monitored and \u201cbad\u201d time machines are (manually) removed to give\nat least very high guarantees for the accuracy of the TrueTime service.\nWith a guaranteed accuracy of 6 milliseconds, building a transactional system\nbecomes much easier: transactions can actually be timestamped, even by different\nservers, with the restriction that timestamping may need to be delayed for etime\nunits. More precisely, in order to know for sure that a transaction has committed,\nreading the resulting data may impose a wait for eunits. This is achieved by\npessimistically assigning a timestamp to a transaction that writes data to the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "310 CHAPTER 6. COORDINATION\nglobal database and making sure that clients never see any changes before the\nassigned timestamp (which is relatively easy to implement).\nThere are many details to this approach, which can be found in [Corbett\net al., 2013]. As we are still dealing with a time interval, taking more traditional\nordering mechanisms into account it is possible to improve results, as explained\nby Demirbas and Kulkarni [2013].\n6.2 Logical clocks\nClock synchronization is naturally related to time, although it may not be\nnecessary to have an accurate account of the real time: it may be suf\ufb01cient\nthat every node in a distributed systems agrees on acurrent time. We can\ngo one step further. For running make it is adequate that two nodes agree\nthat input .ois outdated by a new version of input .c, for example. In this case,\nkeeping track of each other\u2019s events (such as a producing a new version of\ninput .c) is what matters. For these algorithms, it is conventional to speak of\nthe clocks as logical clocks .\nIn a seminal paper, Lamport [1978] showed that although clock synchro-\nnization is possible, it need not be absolute. If two processes do not interact,\nit is not necessary that their clocks be synchronized because the lack of syn-\nchronization would not be observable and thus could not cause problems.\nFurthermore, he pointed out that what usually matters is not that all processes\nagree on exactly what time it is, but rather that they agree on the order in which\nevents occur . In the make example, what counts is whether input .cis older or\nnewer than input .o, not their respective absolute creation times.\nLamport\u2019s logical clocks\nTo synchronize logical clocks, Lamport de\ufb01ned a relation called happens-\nbefore . The expression a!bis read \u201cevent ahappens before event b\u201d and\nmeans that all processes agree that \ufb01rst event aoccurs, then afterward, event b\noccurs. The happens-before relation can be observed directly in two situations:\n1.Ifaand bare events in the same process, and aoccurs before b, then\na!bis true.\n2.Ifais the event of a message being sent by one process, and bis the\nevent of the message being received by another process, then a!bis\nalso true. A message cannot be received before it is sent, or even at the\nsame time it is sent, since it takes a \ufb01nite, nonzero amount of time to\narrive.\nHappens-before is a transitive relation, so if a!band b!c, then a!c.\nIf two events, xand y, happen in different processes that do not exchange\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n310 CHAPTER 6. COORDINATION\nglobal database and making sure that clients never see any changes before the\nassigned timestamp (which is relatively easy to implement).\nThere are many details to this approach, which can be found in [Corbett\net al., 2013]. As we are still dealing with a time interval, taking more traditional\nordering mechanisms into account it is possible to improve results, as explained\nby Demirbas and Kulkarni [2013].\n6.2 Logical clocks\nClock synchronization is naturally related to time, although it may not be\nnecessary to have an accurate account of the real time: it may be suf\ufb01cient\nthat every node in a distributed systems agrees on acurrent time. We can\ngo one step further. For running make it is adequate that two nodes agree\nthat input .ois outdated by a new version of input .c, for example. In this case,\nkeeping track of each other\u2019s events (such as a producing a new version of\ninput .c) is what matters. For these algorithms, it is conventional to speak of\nthe clocks as logical clocks .\nIn a seminal paper, Lamport [1978] showed that although clock synchro-\nnization is possible, it need not be absolute. If two processes do not interact,\nit is not necessary that their clocks be synchronized because the lack of syn-\nchronization would not be observable and thus could not cause problems.\nFurthermore, he pointed out that what usually matters is not that all processes\nagree on exactly what time it is, but rather that they agree on the order in which\nevents occur . In the make example, what counts is whether input .cis older or\nnewer than input .o, not their respective absolute creation times.\nLamport\u2019s logical clocks\nTo synchronize logical clocks, Lamport de\ufb01ned a relation called happens-\nbefore . The expression a!bis read \u201cevent ahappens before event b\u201d and\nmeans that all processes agree that \ufb01rst event aoccurs, then afterward, event b\noccurs. The happens-before relation can be observed directly in two situations:\n1.Ifaand bare events in the same process, and aoccurs before b, then\na!bis true.\n2.Ifais the event of a message being sent by one process, and bis the\nevent of the message being received by another process, then a!bis\nalso true. A message cannot be received before it is sent, or even at the\nsame time it is sent, since it takes a \ufb01nite, nonzero amount of time to\narrive.\nHappens-before is a transitive relation, so if a!band b!c, then a!c.\nIf two events, xand y, happen in different processes that do not exchange\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.2. LOGICAL CLOCKS 311\nmessages (not even indirectly via third parties), then x!yis not true, but\nneither is y!x. These events are said to be concurrent , which simply means\nthat nothing can be said (or need be said) about when the events happened or\nwhich event happened \ufb01rst.\nWhat we need is a way of measuring a notion of time such that for every\nevent, a, we can assign it a time value C(a)on which all processes agree.\nThese time values must have the property that if a!b, then C(a)<C(b).\nTo rephrase the conditions we stated earlier, if aand bare two events within\nthe same process and aoccurs before b, then C(a)<C(b). Similarly, if a\nis the sending of a message by one process and bis the reception of that\nmessage by another process, then C(a)and C(b)must be assigned in such a\nway that everyone agrees on the values of C(a)and C(b)with C(a)<C(b).\nIn addition, the clock time, C, must always go forward (increasing), never\nbackward (decreasing). Corrections to time can be made by adding a positive\nvalue, never by subtracting one.\nNow let us look at the algorithm Lamport proposed for assigning times\nto events. Consider the three processes depicted in Figure 6.8. The processes\nrun on different machines, each with its own clock. For the sake of argument,\nwe assume that a clock is implemented as a software counter: the counter\nis incremented by a speci\ufb01c value every Ttime units. However, the value\nby which a clock is incremented differs per process. The clock in process\nP1is incremented by 6 units, 8 units in process P2, and 10 units in process\nP3, respectively. (Below, we explain that Lamport clocks are, in fact, event\ncounters , which explains why their value may differ between processes.)\n(a) (b)\nFigure 6.8: (a) Three processes, each with its own (logical) clock. The clocks\nrun at different rates. (b) Lamport\u2019s algorithm corrects their values.\nAt time 6, process P1sends message m1to process P2. How long this\nmessage takes to arrive depends on whose clock you believe. In any event,\nthe clock in process P2reads 16 when it arrives. If the message carries the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.2. LOGICAL CLOCKS 311\nmessages (not even indirectly via third parties), then x!yis not true, but\nneither is y!x. These events are said to be concurrent , which simply means\nthat nothing can be said (or need be said) about when the events happened or\nwhich event happened \ufb01rst.\nWhat we need is a way of measuring a notion of time such that for every\nevent, a, we can assign it a time value C(a)on which all processes agree.\nThese time values must have the property that if a!b, then C(a)<C(b).\nTo rephrase the conditions we stated earlier, if aand bare two events within\nthe same process and aoccurs before b, then C(a)<C(b). Similarly, if a\nis the sending of a message by one process and bis the reception of that\nmessage by another process, then C(a)and C(b)must be assigned in such a\nway that everyone agrees on the values of C(a)and C(b)with C(a)<C(b).\nIn addition, the clock time, C, must always go forward (increasing), never\nbackward (decreasing). Corrections to time can be made by adding a positive\nvalue, never by subtracting one.\nNow let us look at the algorithm Lamport proposed for assigning times\nto events. Consider the three processes depicted in Figure 6.8. The processes\nrun on different machines, each with its own clock. For the sake of argument,\nwe assume that a clock is implemented as a software counter: the counter\nis incremented by a speci\ufb01c value every Ttime units. However, the value\nby which a clock is incremented differs per process. The clock in process\nP1is incremented by 6 units, 8 units in process P2, and 10 units in process\nP3, respectively. (Below, we explain that Lamport clocks are, in fact, event\ncounters , which explains why their value may differ between processes.)\n(a) (b)\nFigure 6.8: (a) Three processes, each with its own (logical) clock. The clocks\nrun at different rates. (b) Lamport\u2019s algorithm corrects their values.\nAt time 6, process P1sends message m1to process P2. How long this\nmessage takes to arrive depends on whose clock you believe. In any event,\nthe clock in process P2reads 16 when it arrives. If the message carries the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "312 CHAPTER 6. COORDINATION\nstarting time, 6, in it, process P2will conclude that it took 10 ticks to make the\njourney. This value is certainly possible. According to this reasoning, message\nm2from P2toP3takes 16 ticks, again a plausible value.\nNow consider message m3. It leaves process P3at 60 and arrives at P2at\n56. Similarly, message m4from P2toP1leaves at 64 and arrives at 54. These\nvalues are clearly impossible. It is this situation that must be prevented.\nLamport\u2019s solution follows directly from the happens-before relation. Since\nm3left at 60, it must arrive at 61 or later. Therefore, each message carries the\nsending time according to the sender\u2019s clock. When a message arrives and\nthe receiver\u2019s clock shows a value prior to the time the message was sent,\nthe receiver fast forwards its clock to be one more than the sending time. In\nFigure 6.8, we see that m3now arrives at 61. Similarly, m4arrives at 70.\nLet us formulate this procedure more precisely. At this point, it is impor-\ntant to distinguish three different layers of software, as we already encountered\nin Chapter 1: the network, a middleware layer, and an application layer, as\nshown in Figure 6.9 What follows is typically part of the middleware layer.\nFigure 6.9: The positioning of Lamport\u2019s logical clocks in distributed systems.\nTo implement Lamport\u2019s logical clocks, each process Pimaintains a local\ncounter Ci. These counters are updated according to the following steps [Ray-\nnal and Singhal, 1996]:\n1.Before executing an event (i.e., sending a message over the network,\ndelivering a message to an application, or some other internal event), Pi\nincrements Ci:Ci Ci+1.\n2.When process Pisends a message mto process Pj, it sets m\u2019s timestamp\nts(m)equal to Ciafter having executed the previous step.\n3.Upon the receipt of a message m, process Pjadjusts its own local counter\nasCj maxfCj,ts(m)gafter which it then executes the \ufb01rst step and\ndelivers the message to the application.\nIn some situations, an additional requirement is desirable: no two events ever\noccur at exactly the same time. To achieve this goal, we also use the unique\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n312 CHAPTER 6. COORDINATION\nstarting time, 6, in it, process P2will conclude that it took 10 ticks to make the\njourney. This value is certainly possible. According to this reasoning, message\nm2from P2toP3takes 16 ticks, again a plausible value.\nNow consider message m3. It leaves process P3at 60 and arrives at P2at\n56. Similarly, message m4from P2toP1leaves at 64 and arrives at 54. These\nvalues are clearly impossible. It is this situation that must be prevented.\nLamport\u2019s solution follows directly from the happens-before relation. Since\nm3left at 60, it must arrive at 61 or later. Therefore, each message carries the\nsending time according to the sender\u2019s clock. When a message arrives and\nthe receiver\u2019s clock shows a value prior to the time the message was sent,\nthe receiver fast forwards its clock to be one more than the sending time. In\nFigure 6.8, we see that m3now arrives at 61. Similarly, m4arrives at 70.\nLet us formulate this procedure more precisely. At this point, it is impor-\ntant to distinguish three different layers of software, as we already encountered\nin Chapter 1: the network, a middleware layer, and an application layer, as\nshown in Figure 6.9 What follows is typically part of the middleware layer.\nFigure 6.9: The positioning of Lamport\u2019s logical clocks in distributed systems.\nTo implement Lamport\u2019s logical clocks, each process Pimaintains a local\ncounter Ci. These counters are updated according to the following steps [Ray-\nnal and Singhal, 1996]:\n1.Before executing an event (i.e., sending a message over the network,\ndelivering a message to an application, or some other internal event), Pi\nincrements Ci:Ci Ci+1.\n2.When process Pisends a message mto process Pj, it sets m\u2019s timestamp\nts(m)equal to Ciafter having executed the previous step.\n3.Upon the receipt of a message m, process Pjadjusts its own local counter\nasCj maxfCj,ts(m)gafter which it then executes the \ufb01rst step and\ndelivers the message to the application.\nIn some situations, an additional requirement is desirable: no two events ever\noccur at exactly the same time. To achieve this goal, we also use the unique\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.2. LOGICAL CLOCKS 313\nprocess identi\ufb01er to break ties and use tuples instead of only the counter\u2019s\nvalues. For example, an event at time 40 at process Piwill be timestamped as\nh40,ii. If we also have an event h40,jiand i<j, thenh40,ii<h40,ji.\nNote that by assigning the event time C(a) Ci(a)ifahappened at\nprocess Piat time Ci(a), we have a distributed implementation of the global\ntime value we were initially seeking for; we have thus constructed a logical\nclock .\nExample: Total-ordered multicasting\nAs an application of Lamport\u2019s logical clocks, consider the situation in which\na database has been replicated across several sites. For example, to improve\nquery performance, a bank may place copies of an account database in two\ndifferent cities, say New York and San Francisco. A query is always forwarded\nto the nearest copy. The price for a fast response to a query is partly paid in\nhigher update costs, because each update operation must be carried out at\neach replica.\nIn fact, there is a more stringent requirement with respect to updates.\nAssume a customer in San Francisco wants to add $100 to his account, which\ncurrently contains $1,000. At the same time, a bank employee in New York\ninitiates an update by which the customer\u2019s account is to be increased with\n1 percent interest. Both updates should be carried out at both copies of the\ndatabase. However, due to communication delays in the underlying network,\nthe updates may arrive in the order as shown in Figure 6.10.\nFigure 6.10: Updating a replicated database and leaving it in an inconsistent\nstate.\nThe customer\u2019s update operation is performed in San Francisco before the\ninterest update. In contrast, the copy of the account in the New York replica is\n\ufb01rst updated with the 1 percent interest, and after that with the $100 deposit.\nConsequently, the San Francisco database will record a total amount of $1,111,\nwhereas the New York database records $1,110.\nThe problem that we are faced with is that the two update operations\nshould have been performed in the same order at each copy. Although it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.2. LOGICAL CLOCKS 313\nprocess identi\ufb01er to break ties and use tuples instead of only the counter\u2019s\nvalues. For example, an event at time 40 at process Piwill be timestamped as\nh40,ii. If we also have an event h40,jiand i<j, thenh40,ii<h40,ji.\nNote that by assigning the event time C(a) Ci(a)ifahappened at\nprocess Piat time Ci(a), we have a distributed implementation of the global\ntime value we were initially seeking for; we have thus constructed a logical\nclock .\nExample: Total-ordered multicasting\nAs an application of Lamport\u2019s logical clocks, consider the situation in which\na database has been replicated across several sites. For example, to improve\nquery performance, a bank may place copies of an account database in two\ndifferent cities, say New York and San Francisco. A query is always forwarded\nto the nearest copy. The price for a fast response to a query is partly paid in\nhigher update costs, because each update operation must be carried out at\neach replica.\nIn fact, there is a more stringent requirement with respect to updates.\nAssume a customer in San Francisco wants to add $100 to his account, which\ncurrently contains $1,000. At the same time, a bank employee in New York\ninitiates an update by which the customer\u2019s account is to be increased with\n1 percent interest. Both updates should be carried out at both copies of the\ndatabase. However, due to communication delays in the underlying network,\nthe updates may arrive in the order as shown in Figure 6.10.\nFigure 6.10: Updating a replicated database and leaving it in an inconsistent\nstate.\nThe customer\u2019s update operation is performed in San Francisco before the\ninterest update. In contrast, the copy of the account in the New York replica is\n\ufb01rst updated with the 1 percent interest, and after that with the $100 deposit.\nConsequently, the San Francisco database will record a total amount of $1,111,\nwhereas the New York database records $1,110.\nThe problem that we are faced with is that the two update operations\nshould have been performed in the same order at each copy. Although it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "314 CHAPTER 6. COORDINATION\nmakes a difference whether the deposit is processed before the interest update\nor the other way around, which order is followed is not important from a\nconsistency point of view. The important issue is that both copies should be\nexactly the same. In general, situations such as these require a total-ordered\nmulticast , that is, a multicast operation by which all messages are delivered\nin the same order to each receiver. Lamport\u2019s logical clocks can be used to\nimplement total-ordered multicasts in a completely distributed fashion.\nConsider a group of processes multicasting messages to each other. Each\nmessage is always timestamped with the current (logical) time of its sender.\nWhen a message is multicast, it is conceptually also sent to the sender. In\naddition, we assume that messages from the same sender are received in the\norder they were sent, and that no messages are lost.\nWhen a process receives a message, it is put into a local queue, ordered\naccording to its timestamp. The receiver multicasts an acknowledgment to the\nother processes. Note that if we follow Lamport\u2019s algorithm for adjusting local\nclocks, the timestamp of the received message is lower than the timestamp\nof the acknowledgment. The interesting aspect of this approach is that all\nprocesses will eventually have the same copy of the local queue (provided no\nmessages are removed).\nA process can deliver a queued message to the application it is running\nonly when that message is at the head of the queue and has been acknowl-\nedged by each other process. At that point, the message is removed from the\nqueue and handed over to the application; the associated acknowledgments\ncan simply be removed. Because each process has the same copy of the queue,\nall messages are delivered in the same order everywhere. In other words, we\nhave established total-ordered multicasting. We leave it as an exercise to the\nreader to \ufb01gure out that it is not strictly necessary that each multicast message\nhas been explicitly acknowledged. It is suf\ufb01cient that a process reacts to an\nincoming message either by returning an acknowledgment or sending its own\nmulticast message.\nTotal-ordered multicasting is an important vehicle for replicated services\nwhere the replicas are kept consistent by letting them execute the same\noperations in the same order everywhere. As the replicas essentially follow\nthe same transitions in the same \ufb01nite state machine, it is also known as state\nmachine replication [Schneider, 1990].\nNote 6.3 (Advanced: Using Lamport clocks to achieve mutual exclusion)\nTo further illustrate the usage of Lamport\u2019s clocks, let us see how we can use the\nprevious algorithm for total-ordered multicasting to establish access to what is\ncommonly known as a critical section : a section of code that can be executed\nby at most one process at a time. This algorithm is very similar to the one for\nmulticasting, as essentially all processes need to agree on the order by which\nprocesses are allowed to enter their critical section.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n314 CHAPTER 6. COORDINATION\nmakes a difference whether the deposit is processed before the interest update\nor the other way around, which order is followed is not important from a\nconsistency point of view. The important issue is that both copies should be\nexactly the same. In general, situations such as these require a total-ordered\nmulticast , that is, a multicast operation by which all messages are delivered\nin the same order to each receiver. Lamport\u2019s logical clocks can be used to\nimplement total-ordered multicasts in a completely distributed fashion.\nConsider a group of processes multicasting messages to each other. Each\nmessage is always timestamped with the current (logical) time of its sender.\nWhen a message is multicast, it is conceptually also sent to the sender. In\naddition, we assume that messages from the same sender are received in the\norder they were sent, and that no messages are lost.\nWhen a process receives a message, it is put into a local queue, ordered\naccording to its timestamp. The receiver multicasts an acknowledgment to the\nother processes. Note that if we follow Lamport\u2019s algorithm for adjusting local\nclocks, the timestamp of the received message is lower than the timestamp\nof the acknowledgment. The interesting aspect of this approach is that all\nprocesses will eventually have the same copy of the local queue (provided no\nmessages are removed).\nA process can deliver a queued message to the application it is running\nonly when that message is at the head of the queue and has been acknowl-\nedged by each other process. At that point, the message is removed from the\nqueue and handed over to the application; the associated acknowledgments\ncan simply be removed. Because each process has the same copy of the queue,\nall messages are delivered in the same order everywhere. In other words, we\nhave established total-ordered multicasting. We leave it as an exercise to the\nreader to \ufb01gure out that it is not strictly necessary that each multicast message\nhas been explicitly acknowledged. It is suf\ufb01cient that a process reacts to an\nincoming message either by returning an acknowledgment or sending its own\nmulticast message.\nTotal-ordered multicasting is an important vehicle for replicated services\nwhere the replicas are kept consistent by letting them execute the same\noperations in the same order everywhere. As the replicas essentially follow\nthe same transitions in the same \ufb01nite state machine, it is also known as state\nmachine replication [Schneider, 1990].\nNote 6.3 (Advanced: Using Lamport clocks to achieve mutual exclusion)\nTo further illustrate the usage of Lamport\u2019s clocks, let us see how we can use the\nprevious algorithm for total-ordered multicasting to establish access to what is\ncommonly known as a critical section : a section of code that can be executed\nby at most one process at a time. This algorithm is very similar to the one for\nmulticasting, as essentially all processes need to agree on the order by which\nprocesses are allowed to enter their critical section.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.2. LOGICAL CLOCKS 315\nFigure 6.11(a) shows the code that each process executes when requesting,\nreleasing, or allowing access to the critical section (again, leaving out details).\nEach process maintains a request queue as well as a logical clock. To enter the\ncritical section, a call to requestToEnter is made, which results in inserting an\nENTER message with timestamp (clock,procID) into the local queue and sending\nthat message to the other processes. The operation cleanupQ essentially sorts the\nqueue. We return to it shortly.\n1class Process:\n2 def__init__(self, chan):\n3 self.queue = [] # The request queue\n4 self.clock = 0 # The current logical clock\n5\n6 defrequestToEnter(self):\n7 self.clock = self.clock + 1 # Increment clock value\n8 self.queue.append((self.clock, self.procID, ENTER)) # Append request to q\n9 self.cleanupQ() # Sort the queue\n10 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,ENTER)) # Send request\n11\n12 defallowToEnter(self, requester):\n13 self.clock = self.clock + 1 # Increment clock value\n14 self.chan.sendTo([requester], (self.clock,self.procID,ALLOW)) # Permit other\n15\n16 defrelease(self):\n17 tmp = [r forrinself.queue[1:] ifr[2] == ENTER] # Remove all ALLOWs\n18 self.queue = tmp # and copy to new queue\n19 self.clock = self.clock + 1 # Increment clock value\n20 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,RELEASE)) # Release\n21\n22 defallowedToEnter(self):\n23 commProcs = set([req[1] forreqinself.queue[1:]]) # See who has sent a message\n24 return (self.queue[0][1]==self.procID and len (self.otherProcs)== len(commProcs))\nFigure 6.11: (a) Using Lamport\u2019s logical clocks for mutual exclusion.\nWhen a process Preceives an ENTER message from process Q, it can simply\nallow Qto enter its critical section, even if Pwants to do so as well. In that case,\nP\u2019s request will have a lower logical timestamp than the ALLOW message sent by P\ntoQ, meaning that P\u2019s request will have been inserted into Q\u2019s queue before P\u2019s\nALLOW message.\nFinally, when a process leaves its critical section, it calls release . It cleans up\nits local queue by removing all received ALLOW messages, leaving only the ENTER\nrequests from other processes. It then multicasts a RELEASE message.\nIn order to actually enter a critical section, a process will have to repeatedly call\nallowedToEnter and when returned False , will have to block on a next incoming\nmessage. The operation allowedToEnter does what is to be expected: it checks\nif the calling process\u2019s ENTER message is at the head of the queue, and sees if all\nother processes have sent a message as well. The latter is encoded through the set\ncommProcs , which contains the procID s of all processes having sent a message by\ninspecting all messages in the local queue from the second position and onwards.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.2. LOGICAL CLOCKS 315\nFigure 6.11(a) shows the code that each process executes when requesting,\nreleasing, or allowing access to the critical section (again, leaving out details).\nEach process maintains a request queue as well as a logical clock. To enter the\ncritical section, a call to requestToEnter is made, which results in inserting an\nENTER message with timestamp (clock,procID) into the local queue and sending\nthat message to the other processes. The operation cleanupQ essentially sorts the\nqueue. We return to it shortly.\n1class Process:\n2 def__init__(self, chan):\n3 self.queue = [] # The request queue\n4 self.clock = 0 # The current logical clock\n5\n6 defrequestToEnter(self):\n7 self.clock = self.clock + 1 # Increment clock value\n8 self.queue.append((self.clock, self.procID, ENTER)) # Append request to q\n9 self.cleanupQ() # Sort the queue\n10 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,ENTER)) # Send request\n11\n12 defallowToEnter(self, requester):\n13 self.clock = self.clock + 1 # Increment clock value\n14 self.chan.sendTo([requester], (self.clock,self.procID,ALLOW)) # Permit other\n15\n16 defrelease(self):\n17 tmp = [r forrinself.queue[1:] ifr[2] == ENTER] # Remove all ALLOWs\n18 self.queue = tmp # and copy to new queue\n19 self.clock = self.clock + 1 # Increment clock value\n20 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,RELEASE)) # Release\n21\n22 defallowedToEnter(self):\n23 commProcs = set([req[1] forreqinself.queue[1:]]) # See who has sent a message\n24 return (self.queue[0][1]==self.procID and len (self.otherProcs)== len(commProcs))\nFigure 6.11: (a) Using Lamport\u2019s logical clocks for mutual exclusion.\nWhen a process Preceives an ENTER message from process Q, it can simply\nallow Qto enter its critical section, even if Pwants to do so as well. In that case,\nP\u2019s request will have a lower logical timestamp than the ALLOW message sent by P\ntoQ, meaning that P\u2019s request will have been inserted into Q\u2019s queue before P\u2019s\nALLOW message.\nFinally, when a process leaves its critical section, it calls release . It cleans up\nits local queue by removing all received ALLOW messages, leaving only the ENTER\nrequests from other processes. It then multicasts a RELEASE message.\nIn order to actually enter a critical section, a process will have to repeatedly call\nallowedToEnter and when returned False , will have to block on a next incoming\nmessage. The operation allowedToEnter does what is to be expected: it checks\nif the calling process\u2019s ENTER message is at the head of the queue, and sees if all\nother processes have sent a message as well. The latter is encoded through the set\ncommProcs , which contains the procID s of all processes having sent a message by\ninspecting all messages in the local queue from the second position and onwards.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "316 CHAPTER 6. COORDINATION\n1 defreceive(self):\n2 msg = self.chan.recvFrom(self.otherProcs)[1] # Pick up any message\n3 self.clock = max(self.clock, msg[0]) # Adjust clock value...\n4 self.clock = self.clock + 1 # ...and increment\n5 ifmsg[2] == ENTER:\n6 self.queue.append(msg) # Append an ENTER request\n7 self.allowToEnter(msg[1]) # and unconditionally allow\n8 elif msg[2] == ALLOW:\n9 self.queue.append(msg) # Append an ALLOW\n10 elif msg[2] == RELEASE:\n11 del(self.queue[0]) # Just remove first message\n12 self.cleanupQ() # And sort and cleanup\nFigure 6.11: (b) Using Lamport\u2019s logical clocks for mutual exclusion:\nhandling incoming requests.\nWhat to do when a message is received is shown in Figure 6.11(b). First,\nthe local clock is adjusted according to the rules for Lamport\u2019s logical clocks\nexplained above. When receiving an ENTER orALLOW message, that message is\nsimply inserted into the queue. An entry request is always acknowledged, as we\njust explained. When a RELEASE message is received, the original ENTER request is\nremoved. Note that this request is at the head of the queue. After that, the queue\nis cleaned up again.\nAt this point, note that if we would clean up the queue by only sorting it,\nwe may get into trouble. Suppose that processes Pand Qwant to enter their\nrespective critical sections at roughly the same time, but that Pis allowed to go\n\ufb01rst based on logical-clock values. Pmay \ufb01nd Q\u2019s request in its queue, along\nwith ENTER orALLOW messages from other processes. If its own request is at the\nhead of its queue, Pwill proceed and enter its critical section. However, Qwill\nalso send an ALLOW message to Pas well, in addition to its original ENTER message.\nThat ALLOW message may arrive after Phad already entered its critical section, but\nbefore ENTER messages from other processes. When Qeventually enters, and leaves\nits critical section, Q\u2019sRELEASE message would result in removing Q\u2019s original\nENTER message, but not the ALLOW message it had previously sent to P. By now,\nthat message is at the head of P\u2019s queue, effectively blocking the entrance to the\ncritical section of other processes in P\u2019s queue. Cleaning up the queue thus also\ninvolves removing old ALLOW messages.\nVector clocks\nLamport\u2019s logical clocks lead to a situation where all events in a distributed\nsystem are totally ordered with the property that if event ahappened before\nevent b, then awill also be positioned in that ordering before b, that is,\nC(a)<C(b).\nHowever, with Lamport clocks, nothing can be said about the relationship\nbetween two events aand bby merely comparing their time values C(a)\nand C(b), respectively. In other words, if C(a)<C(b), then this does not\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n316 CHAPTER 6. COORDINATION\n1 defreceive(self):\n2 msg = self.chan.recvFrom(self.otherProcs)[1] # Pick up any message\n3 self.clock = max(self.clock, msg[0]) # Adjust clock value...\n4 self.clock = self.clock + 1 # ...and increment\n5 ifmsg[2] == ENTER:\n6 self.queue.append(msg) # Append an ENTER request\n7 self.allowToEnter(msg[1]) # and unconditionally allow\n8 elif msg[2] == ALLOW:\n9 self.queue.append(msg) # Append an ALLOW\n10 elif msg[2] == RELEASE:\n11 del(self.queue[0]) # Just remove first message\n12 self.cleanupQ() # And sort and cleanup\nFigure 6.11: (b) Using Lamport\u2019s logical clocks for mutual exclusion:\nhandling incoming requests.\nWhat to do when a message is received is shown in Figure 6.11(b). First,\nthe local clock is adjusted according to the rules for Lamport\u2019s logical clocks\nexplained above. When receiving an ENTER orALLOW message, that message is\nsimply inserted into the queue. An entry request is always acknowledged, as we\njust explained. When a RELEASE message is received, the original ENTER request is\nremoved. Note that this request is at the head of the queue. After that, the queue\nis cleaned up again.\nAt this point, note that if we would clean up the queue by only sorting it,\nwe may get into trouble. Suppose that processes Pand Qwant to enter their\nrespective critical sections at roughly the same time, but that Pis allowed to go\n\ufb01rst based on logical-clock values. Pmay \ufb01nd Q\u2019s request in its queue, along\nwith ENTER orALLOW messages from other processes. If its own request is at the\nhead of its queue, Pwill proceed and enter its critical section. However, Qwill\nalso send an ALLOW message to Pas well, in addition to its original ENTER message.\nThat ALLOW message may arrive after Phad already entered its critical section, but\nbefore ENTER messages from other processes. When Qeventually enters, and leaves\nits critical section, Q\u2019sRELEASE message would result in removing Q\u2019s original\nENTER message, but not the ALLOW message it had previously sent to P. By now,\nthat message is at the head of P\u2019s queue, effectively blocking the entrance to the\ncritical section of other processes in P\u2019s queue. Cleaning up the queue thus also\ninvolves removing old ALLOW messages.\nVector clocks\nLamport\u2019s logical clocks lead to a situation where all events in a distributed\nsystem are totally ordered with the property that if event ahappened before\nevent b, then awill also be positioned in that ordering before b, that is,\nC(a)<C(b).\nHowever, with Lamport clocks, nothing can be said about the relationship\nbetween two events aand bby merely comparing their time values C(a)\nand C(b), respectively. In other words, if C(a)<C(b), then this does not\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.2. LOGICAL CLOCKS 317\nnecessarily imply that aindeed happened before b. Something more is needed\nfor that.\nTo explain, consider the messages as sent by the three processes shown\nin Figure 6.12. Denote by Tsnd(mi)the logical time at which message miwas\nsent, and likewise, by Trcv(mi)the time of its receipt. By construction, we\nknow that for each message Tsnd(mi)<Trcv(mi). But what can we conclude\nin general from Trcv(mi)<Tsnd(mj)for different messages miand mj?\nFigure 6.12: Concurrent message transmission using logical clocks.\nIn the case for which mi=m1and mj=m3, we know that these values\ncorrespond to events that took place at process P2, meaning that m3was\nindeed sent after the receipt of message m1. This may indicate that the\nsending of message m3depended on what was received through message m1.\nAt the same time, we also know that Trcv(m1)<Tsnd(m2). However, as far as\nwe can tell from Figure 6.12, the sending of m2has nothing to do with the\nreceipt of m1.\nThe problem is that Lamport clocks do not capture causality . In practice,\ncausality is captured by means of vector clocks . To better understand where\nthese come from, we follow the explanation as given by Baquero and Preguica\n[2016]. In fact, tracking causality is simple if we assign each event a unique\nname such as the combination of a process ID and a locally incrementing\ncounter: pkis the kthevent that happened at process P. The problem then\nboils down to keeping track of causal histories . For example, if two local\nevents happened at process P, then the causal history H(p2)of event p2is\nfp1,p2g.\nNow assume that process Psends a message to process Q(which is an\nevent at Pand thus recorded as pkfrom some k), and that at the time of arrival\n(and event for Q), the most recent causal history of Qwasfq1g. To track\ncausality, Palso sends its most recent causal history (assume it was fp1,p2g,\nextended with p3expressing the sending of the message). Upon arrival, Q\nrecords the event ( q2), and merges the two causal histories into a new one:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.2. LOGICAL CLOCKS 317\nnecessarily imply that aindeed happened before b. Something more is needed\nfor that.\nTo explain, consider the messages as sent by the three processes shown\nin Figure 6.12. Denote by Tsnd(mi)the logical time at which message miwas\nsent, and likewise, by Trcv(mi)the time of its receipt. By construction, we\nknow that for each message Tsnd(mi)<Trcv(mi). But what can we conclude\nin general from Trcv(mi)<Tsnd(mj)for different messages miand mj?\nFigure 6.12: Concurrent message transmission using logical clocks.\nIn the case for which mi=m1and mj=m3, we know that these values\ncorrespond to events that took place at process P2, meaning that m3was\nindeed sent after the receipt of message m1. This may indicate that the\nsending of message m3depended on what was received through message m1.\nAt the same time, we also know that Trcv(m1)<Tsnd(m2). However, as far as\nwe can tell from Figure 6.12, the sending of m2has nothing to do with the\nreceipt of m1.\nThe problem is that Lamport clocks do not capture causality . In practice,\ncausality is captured by means of vector clocks . To better understand where\nthese come from, we follow the explanation as given by Baquero and Preguica\n[2016]. In fact, tracking causality is simple if we assign each event a unique\nname such as the combination of a process ID and a locally incrementing\ncounter: pkis the kthevent that happened at process P. The problem then\nboils down to keeping track of causal histories . For example, if two local\nevents happened at process P, then the causal history H(p2)of event p2is\nfp1,p2g.\nNow assume that process Psends a message to process Q(which is an\nevent at Pand thus recorded as pkfrom some k), and that at the time of arrival\n(and event for Q), the most recent causal history of Qwasfq1g. To track\ncausality, Palso sends its most recent causal history (assume it was fp1,p2g,\nextended with p3expressing the sending of the message). Upon arrival, Q\nrecords the event ( q2), and merges the two causal histories into a new one:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "318 CHAPTER 6. COORDINATION\nfp1,p2,p3,q1,q2g.\nChecking whether an event pcausally precedes an event qcan be done\nby checking whether H(p)\u001aH(q)(i.e., it should be a proper subset). In fact,\nwith our notation, it even suf\ufb01ces to check whether p2H(q), assuming that\nqis always the last local event in H(q).\nThe problem with causal histories, is that their representation is not very\nef\ufb01cient. However, there is no need to keep track of all successive events from\nthe same process: the last one will do. If we subsequently assign an index to\neach process, we can represent a causal history as a vector, in which the jth\nentry represents the number of events that happened at process Pj. Causality\ncan then be captured by means of vector clocks , which are constructed by\nletting each process Pimaintain a vector VCiwith the following two properties:\n1.VCi[i]is the number of events that have occurred so far at Pi. In other\nwords, VCi[i]is the local logical clock at process Pi.\n2.IfVCi[j] =kthen Piknows that kevents have occurred at Pj. It is thus\nPi\u2019s knowledge of the local time at Pj.\nThe \ufb01rst property is maintained by incrementing VCi[i]at the occurrence of\neach new event that happens at process Pi. The second property is maintained\nby piggybacking vectors along with messages that are sent. In particular, the\nfollowing steps are performed:\n1.Before executing an event (i.e., sending a message over the network,\ndelivering a message to an application, or some other internal event), Pi\nexecutes VCi[i] VCi[i] +1. This is equivalent to recording a new event\nthat happened at Pi.\n2.When process Pisends a message mtoPj, it sets m\u2019s (vector) timestamp\nts(m)equal to VCiafter having executed the previous step (i.e., it also\nrecords the sending of the message as an event that takes place at Pi).\n3.Upon the receipt of a message m, process Pjadjusts its own vector by\nsetting VCj[k] maxfVCj[k],ts(m)[k]gfor each k(which is equivalent\nto merging causal histories), after which it executes the \ufb01rst step (record-\ning the receipt of the message) and then delivers the message to the\napplication.\nNote that if an event ahas timestamp ts(a), then ts(a)[i]\u00001denotes the\nnumber of events processed at Pithat causally precede a. As a consequence,\nwhen Pjreceives a message mfrom Piwith timestamp ts(m), it knows about\nthe number of events that have occurred at Pithat causally preceded the\nsending of m. More important, however, is that Pjis also told how many\nevents at other processes have taken place, known to Pi, before Pisent message\nm. In other words, timestamp ts(m)tells the receiver how many events in\nother processes have preceded the sending of m, and on which mmay causally\ndepend.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n318 CHAPTER 6. COORDINATION\nfp1,p2,p3,q1,q2g.\nChecking whether an event pcausally precedes an event qcan be done\nby checking whether H(p)\u001aH(q)(i.e., it should be a proper subset). In fact,\nwith our notation, it even suf\ufb01ces to check whether p2H(q), assuming that\nqis always the last local event in H(q).\nThe problem with causal histories, is that their representation is not very\nef\ufb01cient. However, there is no need to keep track of all successive events from\nthe same process: the last one will do. If we subsequently assign an index to\neach process, we can represent a causal history as a vector, in which the jth\nentry represents the number of events that happened at process Pj. Causality\ncan then be captured by means of vector clocks , which are constructed by\nletting each process Pimaintain a vector VCiwith the following two properties:\n1.VCi[i]is the number of events that have occurred so far at Pi. In other\nwords, VCi[i]is the local logical clock at process Pi.\n2.IfVCi[j] =kthen Piknows that kevents have occurred at Pj. It is thus\nPi\u2019s knowledge of the local time at Pj.\nThe \ufb01rst property is maintained by incrementing VCi[i]at the occurrence of\neach new event that happens at process Pi. The second property is maintained\nby piggybacking vectors along with messages that are sent. In particular, the\nfollowing steps are performed:\n1.Before executing an event (i.e., sending a message over the network,\ndelivering a message to an application, or some other internal event), Pi\nexecutes VCi[i] VCi[i] +1. This is equivalent to recording a new event\nthat happened at Pi.\n2.When process Pisends a message mtoPj, it sets m\u2019s (vector) timestamp\nts(m)equal to VCiafter having executed the previous step (i.e., it also\nrecords the sending of the message as an event that takes place at Pi).\n3.Upon the receipt of a message m, process Pjadjusts its own vector by\nsetting VCj[k] maxfVCj[k],ts(m)[k]gfor each k(which is equivalent\nto merging causal histories), after which it executes the \ufb01rst step (record-\ning the receipt of the message) and then delivers the message to the\napplication.\nNote that if an event ahas timestamp ts(a), then ts(a)[i]\u00001denotes the\nnumber of events processed at Pithat causally precede a. As a consequence,\nwhen Pjreceives a message mfrom Piwith timestamp ts(m), it knows about\nthe number of events that have occurred at Pithat causally preceded the\nsending of m. More important, however, is that Pjis also told how many\nevents at other processes have taken place, known to Pi, before Pisent message\nm. In other words, timestamp ts(m)tells the receiver how many events in\nother processes have preceded the sending of m, and on which mmay causally\ndepend.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.2. LOGICAL CLOCKS 319\nTo see what this means, consider Figure 6.13 which shows three processes.\nIn Figure 6.13(a), P2sends a message m1at logical time VC2= (0, 1, 0 )to\nprocess P1. Message m1thus receives timestamp ts(m1) = ( 0, 1, 0 ). Upon\nits receipt, P1adjusts its logical time to VC1 (1, 1, 0 )and delivers it. Mes-\nsage m2is sent by P1toP3with timestamp ts(m2) = ( 2, 1, 0 ). Before P1\nsends another message, m3, an event happens at P1, eventually leading to\ntimestamping m3with value (4, 1, 0 ). After receiving m3, process P2sends\nmessage m4toP3, with timestamp ts(m4) = ( 4, 3, 0 ).\n(a)\n(b)\nFigure 6.13: Capturing potential causality when exchanging messages.\nNow consider the situation shown in Figure 6.13(b). Here, we have delayed\nsending message m2until after message m3has been sent, and after the\nevent had taken place. It is not dif\ufb01cult to see that ts(m2) = ( 4, 1, 0 ), while\nts(m4) = ( 2, 3, 0 ). Compared to Figure 6.13(a), we have the following situation:\nSituation ts(m2) ts(m4) ts(m2) ts(m2) Conclusion\n< >\nts(m4) ts(m4)\nFigure 6.13(a) (2, 1, 0 )(4, 3, 0 ) Y es No m2may causally precede m4\nFigure 6.13(b) (4, 1, 0 )(2, 3, 0 ) No No m2andm4may con\ufb02ict\nWe use the notation ts(a)<ts(b)if and only if for all k,ts(a)[k]\u0014ts(b)[k]\nand there is at least one index k0for which ts(a)[k0]<ts(b)[k0]. Thus, by using\nvector clocks, process P3can detect whether m4may be causally dependent\nonm2, or whether there may be a potential con\ufb02ict. Note, by the way, that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.2. LOGICAL CLOCKS 319\nTo see what this means, consider Figure 6.13 which shows three processes.\nIn Figure 6.13(a), P2sends a message m1at logical time VC2= (0, 1, 0 )to\nprocess P1. Message m1thus receives timestamp ts(m1) = ( 0, 1, 0 ). Upon\nits receipt, P1adjusts its logical time to VC1 (1, 1, 0 )and delivers it. Mes-\nsage m2is sent by P1toP3with timestamp ts(m2) = ( 2, 1, 0 ). Before P1\nsends another message, m3, an event happens at P1, eventually leading to\ntimestamping m3with value (4, 1, 0 ). After receiving m3, process P2sends\nmessage m4toP3, with timestamp ts(m4) = ( 4, 3, 0 ).\n(a)\n(b)\nFigure 6.13: Capturing potential causality when exchanging messages.\nNow consider the situation shown in Figure 6.13(b). Here, we have delayed\nsending message m2until after message m3has been sent, and after the\nevent had taken place. It is not dif\ufb01cult to see that ts(m2) = ( 4, 1, 0 ), while\nts(m4) = ( 2, 3, 0 ). Compared to Figure 6.13(a), we have the following situation:\nSituation ts(m2) ts(m4) ts(m2) ts(m2) Conclusion\n< >\nts(m4) ts(m4)\nFigure 6.13(a) (2, 1, 0 )(4, 3, 0 ) Y es No m2may causally precede m4\nFigure 6.13(b) (4, 1, 0 )(2, 3, 0 ) No No m2andm4may con\ufb02ict\nWe use the notation ts(a)<ts(b)if and only if for all k,ts(a)[k]\u0014ts(b)[k]\nand there is at least one index k0for which ts(a)[k0]<ts(b)[k0]. Thus, by using\nvector clocks, process P3can detect whether m4may be causally dependent\nonm2, or whether there may be a potential con\ufb02ict. Note, by the way, that\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "320 CHAPTER 6. COORDINATION\nwithout knowing the actual information contained in messages, it is not\npossible to state with certainty that there is indeed a causal relationship, or\nperhaps a con\ufb02ict.\nNote 6.4 (Advanced: Enforcing causal communication)\nUsing vector clocks, it is now possible to ensure that a message is delivered only\nif all messages that may have causally precede it have been received as well.\nTo enable such a scheme, we will assume that messages are multicast within a\ngroup of processes. Note that this causal-ordered multicasting is weaker than\ntotal-ordered multicasting. Speci\ufb01cally, if two messages are not in any way related\nto each other, we do not care in which order they are delivered to applications.\nThey may even be delivered in different order at different locations.\nFor enforcing causal message delivery, we assume that clocks are adjusted\nonly when sending and delivering messages (note, again, that messages are not\nadjusted when they are received by a process, but only when they are delivered\nto an application). In particular, upon sending a message, process Piwill only\nincrement VCi[i]by 1. When it delivers a message mwith timestamp ts(m), it\nonly adjusts VCi[k]to maxfVCi[k],ts(m)[k]gfor each k.\nNow suppose that Pjreceives a message mfrom Piwith (vector) timestamp\nts(m). The delivery of the message to the application layer will then be delayed\nuntil the following two conditions are met:\n1.ts(m)[i] =VCj[i] +1\n2.ts(m)[k]\u0014VCj[k]for all k6=i\nThe \ufb01rst condition states that mis the next message that Pjwas expecting from\nprocess Pi. The second condition states that Pjhas delivered all the messages that\nhave been delivered by Piwhen it sent message m. Note that there is no need for\nprocess Pjto delay the delivery of its own messages.\nFigure 6.14: Enforcing causal communication.\nAs an example, consider three processes P1,P2, and P3as shown in Figure 6.14.\nAt local time (1, 0, 0 ),P1sends message mto the other two processes. Note that\nts(m) = ( 1, 0, 0 ). Its receipt and subsequent delivery by P2, will bring the logical\nclock at P2to(1, 0, 0 ), effectively indicating that it has received one message from\nP1, has itself sent no message so far, and has not yet received a message from P3.\nP2then decides to send m\u0003, at updated time (1, 1, 0 ), which arrives at P3sooner\nthan m.\nWhen comparing the timestamp of mwith its current time, which is (0, 0, 0 ),\nP3concludes that it is still missing a message from P1which P2apparently had\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n320 CHAPTER 6. COORDINATION\nwithout knowing the actual information contained in messages, it is not\npossible to state with certainty that there is indeed a causal relationship, or\nperhaps a con\ufb02ict.\nNote 6.4 (Advanced: Enforcing causal communication)\nUsing vector clocks, it is now possible to ensure that a message is delivered only\nif all messages that may have causally precede it have been received as well.\nTo enable such a scheme, we will assume that messages are multicast within a\ngroup of processes. Note that this causal-ordered multicasting is weaker than\ntotal-ordered multicasting. Speci\ufb01cally, if two messages are not in any way related\nto each other, we do not care in which order they are delivered to applications.\nThey may even be delivered in different order at different locations.\nFor enforcing causal message delivery, we assume that clocks are adjusted\nonly when sending and delivering messages (note, again, that messages are not\nadjusted when they are received by a process, but only when they are delivered\nto an application). In particular, upon sending a message, process Piwill only\nincrement VCi[i]by 1. When it delivers a message mwith timestamp ts(m), it\nonly adjusts VCi[k]to maxfVCi[k],ts(m)[k]gfor each k.\nNow suppose that Pjreceives a message mfrom Piwith (vector) timestamp\nts(m). The delivery of the message to the application layer will then be delayed\nuntil the following two conditions are met:\n1.ts(m)[i] =VCj[i] +1\n2.ts(m)[k]\u0014VCj[k]for all k6=i\nThe \ufb01rst condition states that mis the next message that Pjwas expecting from\nprocess Pi. The second condition states that Pjhas delivered all the messages that\nhave been delivered by Piwhen it sent message m. Note that there is no need for\nprocess Pjto delay the delivery of its own messages.\nFigure 6.14: Enforcing causal communication.\nAs an example, consider three processes P1,P2, and P3as shown in Figure 6.14.\nAt local time (1, 0, 0 ),P1sends message mto the other two processes. Note that\nts(m) = ( 1, 0, 0 ). Its receipt and subsequent delivery by P2, will bring the logical\nclock at P2to(1, 0, 0 ), effectively indicating that it has received one message from\nP1, has itself sent no message so far, and has not yet received a message from P3.\nP2then decides to send m\u0003, at updated time (1, 1, 0 ), which arrives at P3sooner\nthan m.\nWhen comparing the timestamp of mwith its current time, which is (0, 0, 0 ),\nP3concludes that it is still missing a message from P1which P2apparently had\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.3. MUTUAL EXCLUSION 321\ndelivered before sending m\u0003.P3therefore decides to postpone the delivery of m\u0003\n(and will also not adjust its local, logical clock). Later, after mhas been received\nand delivered by P3, which brings its local clock to (1, 0, 0 ),P3can deliver message\nm\u0003and also update its clock.\nA note on ordered message delivery. Some middleware systems, notably ISIS\nand its successor Horus [Birman and van Renesse, 1994], provide support for\ntotal-ordered and causal-ordered (reliable) multicasting. There has been some\ncontroversy whether such support should be provided as part of the message-\ncommunication layer, or whether applications should handle ordering (see, e.g.,\nCheriton and Skeen [1993]; Birman [1994]). Matters have not been settled, but\nmore important is that the arguments still hold today.\nThere are two main problems with letting the middleware deal with message\nordering. First, because the middleware cannot tell what a message actually\ncontains, only potential causality is captured. For example, two messages from the\nsame sender that are completely independent will always be marked as causally\nrelated by the middleware layer. This approach is overly restrictive and may lead\nto ef\ufb01ciency problems.\nA second problem is that not all causality may be captured. Consider an\nelectronic bulletin board. Suppose Alice posts an article. If she then phones Bob\ntelling about what she just wrote, Bob may post another article as a reaction\nwithout having seen Alice\u2019s posting on the board. In other words, there is a\ncausality between Bob\u2019s posting and that of Alice due to external communication.\nThis causality is not captured by the bulletin board system.\nIn essence, ordering issues, like many other application-speci\ufb01c communi-\ncation issues, can be adequately solved by looking at the application for which\ncommunication is taking place. This is also known as the end-to-end argument in\nsystems design [Saltzer et al., 1984]. A drawback of having only application-level\nsolutions is that a developer is forced to concentrate on issues that do not immedi-\nately relate to the core functionality of the application. For example, ordering may\nnot be the most important problem when developing a messaging system such as\nan electronic bulletin board. In that case, having an underlying communication\nlayer handle ordering may turn out to be convenient. We will come across the\nend-to-end argument a number of times.\n6.3 Mutual exclusion\nFundamental to distributed systems is the concurrency and collaboration\namong multiple processes. In many cases, this also means that processes\nwill need to simultaneously access the same resources. To prevent that such\nconcurrent accesses corrupt the resource, or make it inconsistent, solutions are\nneeded to grant mutual exclusive access by processes. In this section, we take\na look at some important and representative distributed algorithms that have\nbeen proposed. Surveys of distributed algorithms for mutual exclusion are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.3. MUTUAL EXCLUSION 321\ndelivered before sending m\u0003.P3therefore decides to postpone the delivery of m\u0003\n(and will also not adjust its local, logical clock). Later, after mhas been received\nand delivered by P3, which brings its local clock to (1, 0, 0 ),P3can deliver message\nm\u0003and also update its clock.\nA note on ordered message delivery. Some middleware systems, notably ISIS\nand its successor Horus [Birman and van Renesse, 1994], provide support for\ntotal-ordered and causal-ordered (reliable) multicasting. There has been some\ncontroversy whether such support should be provided as part of the message-\ncommunication layer, or whether applications should handle ordering (see, e.g.,\nCheriton and Skeen [1993]; Birman [1994]). Matters have not been settled, but\nmore important is that the arguments still hold today.\nThere are two main problems with letting the middleware deal with message\nordering. First, because the middleware cannot tell what a message actually\ncontains, only potential causality is captured. For example, two messages from the\nsame sender that are completely independent will always be marked as causally\nrelated by the middleware layer. This approach is overly restrictive and may lead\nto ef\ufb01ciency problems.\nA second problem is that not all causality may be captured. Consider an\nelectronic bulletin board. Suppose Alice posts an article. If she then phones Bob\ntelling about what she just wrote, Bob may post another article as a reaction\nwithout having seen Alice\u2019s posting on the board. In other words, there is a\ncausality between Bob\u2019s posting and that of Alice due to external communication.\nThis causality is not captured by the bulletin board system.\nIn essence, ordering issues, like many other application-speci\ufb01c communi-\ncation issues, can be adequately solved by looking at the application for which\ncommunication is taking place. This is also known as the end-to-end argument in\nsystems design [Saltzer et al., 1984]. A drawback of having only application-level\nsolutions is that a developer is forced to concentrate on issues that do not immedi-\nately relate to the core functionality of the application. For example, ordering may\nnot be the most important problem when developing a messaging system such as\nan electronic bulletin board. In that case, having an underlying communication\nlayer handle ordering may turn out to be convenient. We will come across the\nend-to-end argument a number of times.\n6.3 Mutual exclusion\nFundamental to distributed systems is the concurrency and collaboration\namong multiple processes. In many cases, this also means that processes\nwill need to simultaneously access the same resources. To prevent that such\nconcurrent accesses corrupt the resource, or make it inconsistent, solutions are\nneeded to grant mutual exclusive access by processes. In this section, we take\na look at some important and representative distributed algorithms that have\nbeen proposed. Surveys of distributed algorithms for mutual exclusion are\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "322 CHAPTER 6. COORDINATION\nprovided by Saxena and Rai [2003] and Velazquez [1993]. Various algorithms\nare also presented in [Kshemkalyani and Singhal, 2008].\nOverview\nDistributed mutual exclusion algorithms can be classi\ufb01ed into two different\ncategories. In token-based solutions mutual exclusion is achieved by passing\na special message between the processes, known as a token . There is only one\ntoken available and who ever has that token is allowed to access the shared\nresource. When \ufb01nished, the token is passed on to a next process. If a process\nhaving the token is not interested in accessing the resource, it passes it on.\nToken-based solutions have a few important properties. First, depending\non how the processes are organized, they can fairly easily ensure that every\nprocess will get a chance at accessing the resource. In other words, they avoid\nstarvation . Second, deadlocks by which several processes are inde\ufb01nitely\nwaiting for each other to proceed, can easily be avoided, contributing to their\nsimplicity. The main drawback of token-based solutions is a rather serious\none: when the token is lost (e.g., because the process holding it crashed), an\nintricate distributed procedure needs to be started to ensure that a new token\nis created, but above all, that it is also the only token.\nAs an alternative, many distributed mutual exclusion algorithms follow\napermission-based approach . In this case, a process wanting to access the\nresource \ufb01rst requires the permission from other processes. There are many\ndifferent ways toward granting such a permission and in the sections that\nfollow we will consider a few of them.\nA centralized algorithm\nA straightforward way to achieve mutual exclusion in a distributed system is\nto simulate how it is done in a one-processor system. One process is elected\nas the coordinator. Whenever a process wants to access a shared resource, it\nsends a request message to the coordinator stating which resource it wants to\naccess and asking for permission. If no other process is currently accessing\nthat resource, the coordinator sends back a reply granting permission, as\nshown in Figure 6.15(a). When the reply arrives, the requester can go ahead.\nNow suppose that another process, P2in Figure 6.15(b) asks for permission\nto access the resource. The coordinator knows that a different process is\nalready at the resource, so it cannot grant permission. The exact method used\nto deny permission is system dependent. In Figure 6.15(b) the coordinator just\nrefrains from replying, thus blocking process P2, which is waiting for a reply.\nAlternatively, it could send a reply saying \u201cpermission denied.\u201d Either way, it\nqueues the request from P2for the time being and waits for more messages.\nWhen process P1is \ufb01nished with the resource, it sends a message to the\ncoordinator releasing its exclusive access, as shown in Figure 6.15(c). The\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n322 CHAPTER 6. COORDINATION\nprovided by Saxena and Rai [2003] and Velazquez [1993]. Various algorithms\nare also presented in [Kshemkalyani and Singhal, 2008].\nOverview\nDistributed mutual exclusion algorithms can be classi\ufb01ed into two different\ncategories. In token-based solutions mutual exclusion is achieved by passing\na special message between the processes, known as a token . There is only one\ntoken available and who ever has that token is allowed to access the shared\nresource. When \ufb01nished, the token is passed on to a next process. If a process\nhaving the token is not interested in accessing the resource, it passes it on.\nToken-based solutions have a few important properties. First, depending\non how the processes are organized, they can fairly easily ensure that every\nprocess will get a chance at accessing the resource. In other words, they avoid\nstarvation . Second, deadlocks by which several processes are inde\ufb01nitely\nwaiting for each other to proceed, can easily be avoided, contributing to their\nsimplicity. The main drawback of token-based solutions is a rather serious\none: when the token is lost (e.g., because the process holding it crashed), an\nintricate distributed procedure needs to be started to ensure that a new token\nis created, but above all, that it is also the only token.\nAs an alternative, many distributed mutual exclusion algorithms follow\napermission-based approach . In this case, a process wanting to access the\nresource \ufb01rst requires the permission from other processes. There are many\ndifferent ways toward granting such a permission and in the sections that\nfollow we will consider a few of them.\nA centralized algorithm\nA straightforward way to achieve mutual exclusion in a distributed system is\nto simulate how it is done in a one-processor system. One process is elected\nas the coordinator. Whenever a process wants to access a shared resource, it\nsends a request message to the coordinator stating which resource it wants to\naccess and asking for permission. If no other process is currently accessing\nthat resource, the coordinator sends back a reply granting permission, as\nshown in Figure 6.15(a). When the reply arrives, the requester can go ahead.\nNow suppose that another process, P2in Figure 6.15(b) asks for permission\nto access the resource. The coordinator knows that a different process is\nalready at the resource, so it cannot grant permission. The exact method used\nto deny permission is system dependent. In Figure 6.15(b) the coordinator just\nrefrains from replying, thus blocking process P2, which is waiting for a reply.\nAlternatively, it could send a reply saying \u201cpermission denied.\u201d Either way, it\nqueues the request from P2for the time being and waits for more messages.\nWhen process P1is \ufb01nished with the resource, it sends a message to the\ncoordinator releasing its exclusive access, as shown in Figure 6.15(c). The\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.3. MUTUAL EXCLUSION 323\n(a) (b) (c)\nFigure 6.15: (a) Process P1asks for permission to access a shared resource. Per-\nmission is granted. (b) Process P2asks permission to access the same resource,\nbut receives no reply. (c) When P1releases the resource, the coordinator\nreplies to P2.\ncoordinator takes the \ufb01rst item off the queue of deferred requests and sends\nthat process a grant message. If the process was still blocked (i.e., this is\nthe \ufb01rst message to it), it unblocks and accesses the resource. If an explicit\nmessage has already been sent denying permission, the process will have to\npoll for incoming traf\ufb01c or block later. Either way, when it sees the grant, it\ncan go ahead as well.\nIt is easy to see that the algorithm guarantees mutual exclusion: the\ncoordinator lets only one process at a time access the resource. It is also fair,\nsince requests are granted in the order in which they are received. No process\never waits forever (no starvation). The scheme is easy to implement, too, and\nrequires only three messages per use of resource (request, grant, release). Its\nsimplicity makes it an attractive solution for many practical situations.\nThe centralized approach also has shortcomings. The coordinator is a\nsingle point of failure, so if it crashes, the entire system may go down. If\nprocesses normally block after making a request, they cannot distinguish a\ndead coordinator from \u201cpermission denied\u201d since in both cases no message\ncomes back. In addition, in a large system, a single coordinator can become a\nperformance bottleneck. Nevertheless, the bene\ufb01ts coming from its simplicity\noutweigh in many cases the potential drawbacks. Moreover, distributed\nsolutions are not necessarily better, as we illustrate in Section 6.3.\nA distributed algorithm\nUsing Lamport\u2019s logical clocks, and inspired by Lamport\u2019s original solution\nfor distributed mutual exclusion (which we discussed in ) Note 6.3, Ricart and\nAgrawala [1981] provided the following algorithm. Their solution requires a\ntotal ordering of all events in the system. That is, for any pair of events, such\nas messages, it must be unambiguous which one actually happened \ufb01rst.\nThe algorithm works as follows. When a process wants to access a shared\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.3. MUTUAL EXCLUSION 323\n(a) (b) (c)\nFigure 6.15: (a) Process P1asks for permission to access a shared resource. Per-\nmission is granted. (b) Process P2asks permission to access the same resource,\nbut receives no reply. (c) When P1releases the resource, the coordinator\nreplies to P2.\ncoordinator takes the \ufb01rst item off the queue of deferred requests and sends\nthat process a grant message. If the process was still blocked (i.e., this is\nthe \ufb01rst message to it), it unblocks and accesses the resource. If an explicit\nmessage has already been sent denying permission, the process will have to\npoll for incoming traf\ufb01c or block later. Either way, when it sees the grant, it\ncan go ahead as well.\nIt is easy to see that the algorithm guarantees mutual exclusion: the\ncoordinator lets only one process at a time access the resource. It is also fair,\nsince requests are granted in the order in which they are received. No process\never waits forever (no starvation). The scheme is easy to implement, too, and\nrequires only three messages per use of resource (request, grant, release). Its\nsimplicity makes it an attractive solution for many practical situations.\nThe centralized approach also has shortcomings. The coordinator is a\nsingle point of failure, so if it crashes, the entire system may go down. If\nprocesses normally block after making a request, they cannot distinguish a\ndead coordinator from \u201cpermission denied\u201d since in both cases no message\ncomes back. In addition, in a large system, a single coordinator can become a\nperformance bottleneck. Nevertheless, the bene\ufb01ts coming from its simplicity\noutweigh in many cases the potential drawbacks. Moreover, distributed\nsolutions are not necessarily better, as we illustrate in Section 6.3.\nA distributed algorithm\nUsing Lamport\u2019s logical clocks, and inspired by Lamport\u2019s original solution\nfor distributed mutual exclusion (which we discussed in ) Note 6.3, Ricart and\nAgrawala [1981] provided the following algorithm. Their solution requires a\ntotal ordering of all events in the system. That is, for any pair of events, such\nas messages, it must be unambiguous which one actually happened \ufb01rst.\nThe algorithm works as follows. When a process wants to access a shared\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "324 CHAPTER 6. COORDINATION\nresource, it builds a message containing the name of the resource, its process\nnumber, and the current (logical) time. It then sends the message to all other\nprocesses, conceptually including itself. The sending of messages is assumed\nto be reliable; that is, no message is lost.\nWhen a process receives a request message from another process, the\naction it takes depends on its own state with respect to the resource named in\nthe message. Three different cases have to be clearly distinguished:\n\u2022If the receiver is not accessing the resource and does not want to access\nit, it sends back an OKmessage to the sender.\n\u2022If the receiver already has access to the resource, it simply does not reply.\nInstead, it queues the request.\n\u2022If the receiver wants to access the resource as well but has not yet done\nso, it compares the timestamp of the incoming message with the one\ncontained in the message that it has sent everyone. The lowest one wins.\nIf the incoming message has a lower timestamp, the receiver sends back\nanOKmessage. If its own message has a lower timestamp, the receiver\nqueues the incoming request and sends nothing.\nAfter sending out requests asking permission, a process sits back and waits\nuntil everyone else has given permission. As soon as all the permissions are\nin, it may go ahead. When it is \ufb01nished, it sends OKmessages to all processes\nin its queue and deletes them all from the queue. If there is no con\ufb02ict, it\nclearly works. However, suppose that two processes try to simultaneously\naccess the resource, as shown in Figure 6.16(a).\n(a) (b) (c)\nFigure 6.16: (a) Two processes want to access a shared resource at the same\nmoment. (b) P0has the lowest timestamp, so it wins. (c) When process P0is\ndone, it sends an OKalso, so P2can now go ahead.\nProcess P0sends everyone a request with timestamp 8, while at the same\ntime, process P2sends everyone a request with timestamp 12. P1is not\ninterested in the resource, so it sends OKto both senders. Processes P0and\nP2both see the con\ufb02ict and compare timestamps. P2sees that it has lost,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n324 CHAPTER 6. COORDINATION\nresource, it builds a message containing the name of the resource, its process\nnumber, and the current (logical) time. It then sends the message to all other\nprocesses, conceptually including itself. The sending of messages is assumed\nto be reliable; that is, no message is lost.\nWhen a process receives a request message from another process, the\naction it takes depends on its own state with respect to the resource named in\nthe message. Three different cases have to be clearly distinguished:\n\u2022If the receiver is not accessing the resource and does not want to access\nit, it sends back an OKmessage to the sender.\n\u2022If the receiver already has access to the resource, it simply does not reply.\nInstead, it queues the request.\n\u2022If the receiver wants to access the resource as well but has not yet done\nso, it compares the timestamp of the incoming message with the one\ncontained in the message that it has sent everyone. The lowest one wins.\nIf the incoming message has a lower timestamp, the receiver sends back\nanOKmessage. If its own message has a lower timestamp, the receiver\nqueues the incoming request and sends nothing.\nAfter sending out requests asking permission, a process sits back and waits\nuntil everyone else has given permission. As soon as all the permissions are\nin, it may go ahead. When it is \ufb01nished, it sends OKmessages to all processes\nin its queue and deletes them all from the queue. If there is no con\ufb02ict, it\nclearly works. However, suppose that two processes try to simultaneously\naccess the resource, as shown in Figure 6.16(a).\n(a) (b) (c)\nFigure 6.16: (a) Two processes want to access a shared resource at the same\nmoment. (b) P0has the lowest timestamp, so it wins. (c) When process P0is\ndone, it sends an OKalso, so P2can now go ahead.\nProcess P0sends everyone a request with timestamp 8, while at the same\ntime, process P2sends everyone a request with timestamp 12. P1is not\ninterested in the resource, so it sends OKto both senders. Processes P0and\nP2both see the con\ufb02ict and compare timestamps. P2sees that it has lost,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.3. MUTUAL EXCLUSION 325\nso it grants permission to P0by sending OK. Process P0now queues the\nrequest from P2for later processing and accesses the resource, as shown\nin Figure 6.16(b). When it is \ufb01nished, it removes the request from P2from\nits queue and sends an OKmessage to P2, allowing the latter to go ahead,\nas shown in Figure 6.16(c). The algorithm works because in the case of a\ncon\ufb02ict, the lowest timestamp wins and everyone agrees on the ordering of\nthe timestamps.\nWith this algorithm, mutual exclusion is guaranteed without deadlock or\nstarvation. If the total number of processes is N, then the number of messages\nthat a process needs to send and receive before it can enter its critical section\nis2\u0001(N\u00001):N\u00001request messages to all other processes, and subsequently\nN\u00001OKmessages, one from each other process.\nUnfortunately, this algorithm has Npoints of failure. If any process\ncrashes, it will fail to respond to requests. This silence will be interpreted\n(incorrectly) as denial of permission, thus blocking all subsequent attempts by\nall processes to enter any of their respective critical regions. The algorithm can\nbe patched up as follows. When a request comes in, the receiver always sends\na reply, either granting or denying permission. Whenever either a request or a\nreply is lost, the sender times out and keeps trying until either a reply comes\nback or the sender concludes that the destination is dead. After a request is\ndenied, the sender should block waiting for a subsequent OKmessage.\nAnother problem with this algorithm is that either a multicast commu-\nnication primitive must be used, or each process must maintain the group\nmembership list itself, including processes entering the group, leaving the\ngroup, and crashing. The method works best with small groups of processes\nthat never change their group memberships. Finally, note that allprocesses are\ninvolved in alldecisions concerning accessing the shared resource, which may\nimpose a burden on processes running on resource-constrained machines.\nVarious minor improvements are possible to this algorithm. For example,\ngetting permission from everyone is overkill. All that is needed is a method to\nprevent two processes from accessing the resource at the same time. The algo-\nrithm can be modi\ufb01ed to grant permission when it has collected permission\nfrom a simple majority of the other processes, rather than from all of them.\nA token-ring algorithm\nA completely different approach to deterministically achieving mutual ex-\nclusion in a distributed system is illustrated in Figure 6.17. In software, we\nconstruct an overlay network in the form of a logical ring in which each\nprocess is assigned a position in the ring. All that matters is that each process\nknows who is next in line after itself.\nWhen the ring is initialized, process P0is given a token . The token\ncirculates around the ring. Assuming there are Nprocesses, the token is\npassed from process Pkto process P(k+1)mod Nin point-to-point messages.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.3. MUTUAL EXCLUSION 325\nso it grants permission to P0by sending OK. Process P0now queues the\nrequest from P2for later processing and accesses the resource, as shown\nin Figure 6.16(b). When it is \ufb01nished, it removes the request from P2from\nits queue and sends an OKmessage to P2, allowing the latter to go ahead,\nas shown in Figure 6.16(c). The algorithm works because in the case of a\ncon\ufb02ict, the lowest timestamp wins and everyone agrees on the ordering of\nthe timestamps.\nWith this algorithm, mutual exclusion is guaranteed without deadlock or\nstarvation. If the total number of processes is N, then the number of messages\nthat a process needs to send and receive before it can enter its critical section\nis2\u0001(N\u00001):N\u00001request messages to all other processes, and subsequently\nN\u00001OKmessages, one from each other process.\nUnfortunately, this algorithm has Npoints of failure. If any process\ncrashes, it will fail to respond to requests. This silence will be interpreted\n(incorrectly) as denial of permission, thus blocking all subsequent attempts by\nall processes to enter any of their respective critical regions. The algorithm can\nbe patched up as follows. When a request comes in, the receiver always sends\na reply, either granting or denying permission. Whenever either a request or a\nreply is lost, the sender times out and keeps trying until either a reply comes\nback or the sender concludes that the destination is dead. After a request is\ndenied, the sender should block waiting for a subsequent OKmessage.\nAnother problem with this algorithm is that either a multicast commu-\nnication primitive must be used, or each process must maintain the group\nmembership list itself, including processes entering the group, leaving the\ngroup, and crashing. The method works best with small groups of processes\nthat never change their group memberships. Finally, note that allprocesses are\ninvolved in alldecisions concerning accessing the shared resource, which may\nimpose a burden on processes running on resource-constrained machines.\nVarious minor improvements are possible to this algorithm. For example,\ngetting permission from everyone is overkill. All that is needed is a method to\nprevent two processes from accessing the resource at the same time. The algo-\nrithm can be modi\ufb01ed to grant permission when it has collected permission\nfrom a simple majority of the other processes, rather than from all of them.\nA token-ring algorithm\nA completely different approach to deterministically achieving mutual ex-\nclusion in a distributed system is illustrated in Figure 6.17. In software, we\nconstruct an overlay network in the form of a logical ring in which each\nprocess is assigned a position in the ring. All that matters is that each process\nknows who is next in line after itself.\nWhen the ring is initialized, process P0is given a token . The token\ncirculates around the ring. Assuming there are Nprocesses, the token is\npassed from process Pkto process P(k+1)mod Nin point-to-point messages.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "326 CHAPTER 6. COORDINATION\nFigure 6.17: An overlay network constructed as a logical ring with a token\ncirculating between its members.\nWhen a process acquires the token from its neighbor, it checks to see if it\nneeds to access the shared resource. If so, the process goes ahead, does all the\nwork it needs to, and releases the resources. After it has \ufb01nished, it passes\nthe token along the ring. It is not permitted to immediately enter the resource\nagain using the same token.\nIf a process is handed the token by its neighbor and is not interested in the\nresource, it just passes the token along. As a consequence, when no processes\nneed the resource, the token just circulates around the ring.\nThe correctness of this algorithm is easy to see. Only one process has the\ntoken at any instant, so only one process can actually get to the resource. Since\nthe token circulates among the processes in a well-de\ufb01ned order, starvation\ncannot occur. Once a process decides it wants to have access to the resource,\nat worst it will have to wait for every other process to use the resource.\nThis algorithm has its own problems. If the token is ever lost, for example,\nbecause its holder crashes or due to a lost message containing the token, it\nmust be regenerated. In fact, detecting that it is lost may be dif\ufb01cult, since the\namount of time between successive appearances of the token on the network\nis unbounded. The fact that the token has not been spotted for an hour does\nnot mean that it has been lost; somebody may still be using it.\nThe algorithm also runs into trouble if a process crashes, but recovery is\nrelatively easy. If we require a process receiving the token to acknowledge\nreceipt, a dead process will be detected when its neighbor tries to give it the\ntoken and fails. At that point the dead process can be removed from the group,\nand the token holder can throw the token over the head of the dead process to\nthe next member down the line, or the one after that, if necessary. Of course,\ndoing so requires that everyone maintains the current ring con\ufb01guration.\nA decentralized algorithm\nLet us take a look at fully decentralized solution. Lin et al. [2004] propose to\nuse a voting algorithm. Each resource is assumed to be replicated Ntimes.\nEvery replica has its own coordinator for controlling the access by concurrent\nprocesses.\nHowever, whenever a process wants to access the resource, it will simply\nneed to get a majority vote from m>N/2coordinators. We assume that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n326 CHAPTER 6. COORDINATION\nFigure 6.17: An overlay network constructed as a logical ring with a token\ncirculating between its members.\nWhen a process acquires the token from its neighbor, it checks to see if it\nneeds to access the shared resource. If so, the process goes ahead, does all the\nwork it needs to, and releases the resources. After it has \ufb01nished, it passes\nthe token along the ring. It is not permitted to immediately enter the resource\nagain using the same token.\nIf a process is handed the token by its neighbor and is not interested in the\nresource, it just passes the token along. As a consequence, when no processes\nneed the resource, the token just circulates around the ring.\nThe correctness of this algorithm is easy to see. Only one process has the\ntoken at any instant, so only one process can actually get to the resource. Since\nthe token circulates among the processes in a well-de\ufb01ned order, starvation\ncannot occur. Once a process decides it wants to have access to the resource,\nat worst it will have to wait for every other process to use the resource.\nThis algorithm has its own problems. If the token is ever lost, for example,\nbecause its holder crashes or due to a lost message containing the token, it\nmust be regenerated. In fact, detecting that it is lost may be dif\ufb01cult, since the\namount of time between successive appearances of the token on the network\nis unbounded. The fact that the token has not been spotted for an hour does\nnot mean that it has been lost; somebody may still be using it.\nThe algorithm also runs into trouble if a process crashes, but recovery is\nrelatively easy. If we require a process receiving the token to acknowledge\nreceipt, a dead process will be detected when its neighbor tries to give it the\ntoken and fails. At that point the dead process can be removed from the group,\nand the token holder can throw the token over the head of the dead process to\nthe next member down the line, or the one after that, if necessary. Of course,\ndoing so requires that everyone maintains the current ring con\ufb01guration.\nA decentralized algorithm\nLet us take a look at fully decentralized solution. Lin et al. [2004] propose to\nuse a voting algorithm. Each resource is assumed to be replicated Ntimes.\nEvery replica has its own coordinator for controlling the access by concurrent\nprocesses.\nHowever, whenever a process wants to access the resource, it will simply\nneed to get a majority vote from m>N/2coordinators. We assume that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.3. MUTUAL EXCLUSION 327\nwhen a coordinator does not give permission to access a resource (which it\nwill do when it had granted permission to another process), it will tell the\nrequester.\nThe assumption is that when a coordinator crashes, it recovers quickly\nbut will have forgotten any vote it gave before it crashed. Another way of\nviewing this is that a coordinator resets itself at arbitrary moments. The risk\nthat we are taking is that a reset will make the coordinator forget that it had\npreviously granted permission to some process to access the resource. As a\nconsequence, it may incorrectly grant this permission again to another process\nafter its recovery.\nLetp=Dt/Tbe the probability that a coordinator resets during a time\ninterval Dt, while having a lifetime of T. The probability P[k]that kout of m\ncoordinators reset during the same interval is then\nP[k] =\u0012m\nk\u0013\npk(1\u0000p)m\u0000k\nIffcoordinators reset, then the correctness of the voting mechanism will be\nviolated when we have only a minority of nonfaulty coordinators, that is, when\nm\u0000f\u0014N/2, or, in other words, when f\u0015m\u0000N/2. The probability that\nsuch a violation occurs is then \u00e5N\nk=m\u0000N/2P[k]. To give an impression of what\nthis could mean, Figure 6.18 shows the probability of violating correctness for\ndifferent values of N,m, and p. Note that we compute pby considering the\nnumber of seconds per hour that a coordinator resets, and also taking this\nvalue to be the average time needed to access a resource. Our values for pare\nconsidered to be (very) conservative. The conclusion is that, in general, the\nprobability of violating correctness can be so low that it can be neglected in\ncomparison to other types of failure.\nN m p Violation\n8 53 sec/hour <10\u000015\n8 63 sec/hour <10\u000018\n16 93 sec/hour <10\u000027\n16 12 3 sec/hour <10\u000036\n32 17 3 sec/hour <10\u000052\n32 24 3 sec/hour <10\u000073N m p Violation\n8 530 sec/hour <10\u000010\n8 630 sec/hour <10\u000011\n16 930 sec/hour <10\u000018\n16 12 30 sec/hour <10\u000024\n32 17 30 sec/hour <10\u000035\n32 24 30 sec/hour <10\u000049\nFigure 6.18: Violation probabilities for various parameter values of decentral-\nized mutual exclusion.\nTo implement this scheme, we can use a system in which a resource is\nreplicated Ntimes. Assume that the resource is known under its unique\nname rname . We can then assume that the i-th replica is named rname iwhich\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.3. MUTUAL EXCLUSION 327\nwhen a coordinator does not give permission to access a resource (which it\nwill do when it had granted permission to another process), it will tell the\nrequester.\nThe assumption is that when a coordinator crashes, it recovers quickly\nbut will have forgotten any vote it gave before it crashed. Another way of\nviewing this is that a coordinator resets itself at arbitrary moments. The risk\nthat we are taking is that a reset will make the coordinator forget that it had\npreviously granted permission to some process to access the resource. As a\nconsequence, it may incorrectly grant this permission again to another process\nafter its recovery.\nLetp=Dt/Tbe the probability that a coordinator resets during a time\ninterval Dt, while having a lifetime of T. The probability P[k]that kout of m\ncoordinators reset during the same interval is then\nP[k] =\u0012m\nk\u0013\npk(1\u0000p)m\u0000k\nIffcoordinators reset, then the correctness of the voting mechanism will be\nviolated when we have only a minority of nonfaulty coordinators, that is, when\nm\u0000f\u0014N/2, or, in other words, when f\u0015m\u0000N/2. The probability that\nsuch a violation occurs is then \u00e5N\nk=m\u0000N/2P[k]. To give an impression of what\nthis could mean, Figure 6.18 shows the probability of violating correctness for\ndifferent values of N,m, and p. Note that we compute pby considering the\nnumber of seconds per hour that a coordinator resets, and also taking this\nvalue to be the average time needed to access a resource. Our values for pare\nconsidered to be (very) conservative. The conclusion is that, in general, the\nprobability of violating correctness can be so low that it can be neglected in\ncomparison to other types of failure.\nN m p Violation\n8 53 sec/hour <10\u000015\n8 63 sec/hour <10\u000018\n16 93 sec/hour <10\u000027\n16 12 3 sec/hour <10\u000036\n32 17 3 sec/hour <10\u000052\n32 24 3 sec/hour <10\u000073N m p Violation\n8 530 sec/hour <10\u000010\n8 630 sec/hour <10\u000011\n16 930 sec/hour <10\u000018\n16 12 30 sec/hour <10\u000024\n32 17 30 sec/hour <10\u000035\n32 24 30 sec/hour <10\u000049\nFigure 6.18: Violation probabilities for various parameter values of decentral-\nized mutual exclusion.\nTo implement this scheme, we can use a system in which a resource is\nreplicated Ntimes. Assume that the resource is known under its unique\nname rname . We can then assume that the i-th replica is named rname iwhich\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "328 CHAPTER 6. COORDINATION\nis then used to compute a unique key using a known hash function. As a\nconsequence, every process can generate the Nkeys given a resource\u2019s name,\nand subsequently look up each node responsible for a replica (and controlling\naccess to that replica) using some commonly used naming system.\nIf permission to access the resource is denied (i.e., a process gets less than\nmvotes), it is assumed that it will back off for some randomly chosen time,\nand make a next attempt later. The problem with this scheme is that if many\nnodes want to access the same resource, it turns out that the utilization rapidly\ndrops. In that case, there are so many nodes competing to get access that\neventually no one is able to get enough votes leaving the resource unused. A\nsolution to solve this problem can be found in [Lin et al., 2004].\nNote 6.5 (More information: A comparison of the mutual-exclusion algorithms)\nMessages per Delay before entry\nAlgorithm entry/exit (in message times)\nCentralized 3 2\nDistributed 3\u0001(N\u00001) 2\u0001(N\u00001)\nToken ring 1, . . . , \u00a5 0, . . . , N\u00001\nDecentralized 2\u0001m\u0001k+m,k=1, 2, . . . 2\u0001m\u0001k\nFigure 6.19: A comparison of four mutual exclusion algorithms.\nA brief comparison of the mutual exclusion algorithms we have looked at is\ninstructive. In Figure 6.19 we have listed the algorithms and two performance\nproperties: the number of messages required for a process to access and release a\nshared resource, and the delay before access can occur (assuming messages are\npassed sequentially over a network).\nIn the following, we assume only point-to-point messages (or, equivalently,\ncount a multicast to Nprocesses as Nmessages).\n\u2022The centralized algorithm is simplest and also most ef\ufb01cient. It requires\nonly three messages to enter and leave a critical region: a request, a grant\nto enter, and a release to exit.\n\u2022The distributed algorithm requires N\u00001request messages, one to each of\nthe other processes, and an additional N\u00001grant messages, for a total of\n2\u0001(N\u00001).\n\u2022With the token ring algorithm, the number is variable. If every process\nconstantly wants to enter a critical region, then each token pass will result\nin one entry and exit, for an average of one message per critical region\nentered. At the other extreme, the token may sometimes circulate for hours\nwithout anyone being interested in it. In this case, the number of messages\nper entry into a critical region is unbounded.\n\u2022In the decentralized case, we see that a request message needs to be sent to\nmcoordinators, followed by a response message. However, it is possible\nthat several attempts need to be made (for which we introduce the variable\nk). An exit requires sending a message to each of the mcoordinators.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n328 CHAPTER 6. COORDINATION\nis then used to compute a unique key using a known hash function. As a\nconsequence, every process can generate the Nkeys given a resource\u2019s name,\nand subsequently look up each node responsible for a replica (and controlling\naccess to that replica) using some commonly used naming system.\nIf permission to access the resource is denied (i.e., a process gets less than\nmvotes), it is assumed that it will back off for some randomly chosen time,\nand make a next attempt later. The problem with this scheme is that if many\nnodes want to access the same resource, it turns out that the utilization rapidly\ndrops. In that case, there are so many nodes competing to get access that\neventually no one is able to get enough votes leaving the resource unused. A\nsolution to solve this problem can be found in [Lin et al., 2004].\nNote 6.5 (More information: A comparison of the mutual-exclusion algorithms)\nMessages per Delay before entry\nAlgorithm entry/exit (in message times)\nCentralized 3 2\nDistributed 3\u0001(N\u00001) 2\u0001(N\u00001)\nToken ring 1, . . . , \u00a5 0, . . . , N\u00001\nDecentralized 2\u0001m\u0001k+m,k=1, 2, . . . 2\u0001m\u0001k\nFigure 6.19: A comparison of four mutual exclusion algorithms.\nA brief comparison of the mutual exclusion algorithms we have looked at is\ninstructive. In Figure 6.19 we have listed the algorithms and two performance\nproperties: the number of messages required for a process to access and release a\nshared resource, and the delay before access can occur (assuming messages are\npassed sequentially over a network).\nIn the following, we assume only point-to-point messages (or, equivalently,\ncount a multicast to Nprocesses as Nmessages).\n\u2022The centralized algorithm is simplest and also most ef\ufb01cient. It requires\nonly three messages to enter and leave a critical region: a request, a grant\nto enter, and a release to exit.\n\u2022The distributed algorithm requires N\u00001request messages, one to each of\nthe other processes, and an additional N\u00001grant messages, for a total of\n2\u0001(N\u00001).\n\u2022With the token ring algorithm, the number is variable. If every process\nconstantly wants to enter a critical region, then each token pass will result\nin one entry and exit, for an average of one message per critical region\nentered. At the other extreme, the token may sometimes circulate for hours\nwithout anyone being interested in it. In this case, the number of messages\nper entry into a critical region is unbounded.\n\u2022In the decentralized case, we see that a request message needs to be sent to\nmcoordinators, followed by a response message. However, it is possible\nthat several attempts need to be made (for which we introduce the variable\nk). An exit requires sending a message to each of the mcoordinators.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.4. ELECTION ALGORITHMS 329\nThe delay from the moment a process needs to enter a critical region until its\nactual entry also varies. For a worst-case analysis, we assume that messages are\nsent one after the other (i.e., there are never two or more messages in transit at\nthe same time), and that message transfer time is roughly the same everywhere.\nDelay can then be expressed in message transfer time units , or simply MTTU .\nUnder these assumptions, when the time using a resource is short, the dominant\nfactor in the delay is determined by the total number of messages sent through\nthe system before access can be granted. When resources are used for a long\nperiod of time, the dominant factor is waiting for everyone else to take their turn.\nIn Figure 6.19 we show the former case.\n\u2022It takes only two MTTUs to enter a critical region in the centralized case,\ncaused by a request message and the subsequent grant message sent by the\ncoordinator.\n\u2022The distributed algorithm requires sending N\u00001request messages, and\nreceiving another N\u00001 grant messages, adding up to 2 \u0001(N\u00001)MTTUs.\n\u2022For the token ring, the delay varies from 0 MTTU (in case the token had\njust arrived) to N\u00001 (for when the token had just departed).\n\u2022The decentralized case requires sending mmessages to coordinators, and\nanother mresponses, but a process may need to go through k\u00151attempts,\nadding up to 2\u0001m\u0001kMTTUs.\nVirtually all algorithms suffer badly in the event of crashes. Special measures\nand additional complexity must be introduced to avoid having a crash bring down\nthe entire system. It is somewhat ironic that distributed algorithms are generally\nmore sensitive to crashes than centralized ones. In this sense, it should not come\nas a surprise that, indeed, centralized mutual exclusion is widely applied: it\nis simple to understand the behavior, and relatively easy to increase the fault\ntolerance of the centralized server. However, centralized solutions may suffer\nfrom scalability problems.\n6.4 Election algorithms\nMany distributed algorithms require one process to act as coordinator, initiator,\nor otherwise perform some special role. In general, it does not matter which\nprocess takes on this special responsibility, but one of them has to do it. In\nthis section we will look at algorithms for electing a coordinator (using this as\na generic name for the special process).\nIf all processes are exactly the same, with no distinguishing characteristics,\nthere is no way to select one of them to be special. Consequently, we will\nassume that each process Phas a unique identi\ufb01er id(P). In general, elec-\ntion algorithms attempt to locate the process with the highest identi\ufb01er and\ndesignate it as coordinator. The algorithms differ in the way they locate the\ncoordinator.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.4. ELECTION ALGORITHMS 329\nThe delay from the moment a process needs to enter a critical region until its\nactual entry also varies. For a worst-case analysis, we assume that messages are\nsent one after the other (i.e., there are never two or more messages in transit at\nthe same time), and that message transfer time is roughly the same everywhere.\nDelay can then be expressed in message transfer time units , or simply MTTU .\nUnder these assumptions, when the time using a resource is short, the dominant\nfactor in the delay is determined by the total number of messages sent through\nthe system before access can be granted. When resources are used for a long\nperiod of time, the dominant factor is waiting for everyone else to take their turn.\nIn Figure 6.19 we show the former case.\n\u2022It takes only two MTTUs to enter a critical region in the centralized case,\ncaused by a request message and the subsequent grant message sent by the\ncoordinator.\n\u2022The distributed algorithm requires sending N\u00001request messages, and\nreceiving another N\u00001 grant messages, adding up to 2 \u0001(N\u00001)MTTUs.\n\u2022For the token ring, the delay varies from 0 MTTU (in case the token had\njust arrived) to N\u00001 (for when the token had just departed).\n\u2022The decentralized case requires sending mmessages to coordinators, and\nanother mresponses, but a process may need to go through k\u00151attempts,\nadding up to 2\u0001m\u0001kMTTUs.\nVirtually all algorithms suffer badly in the event of crashes. Special measures\nand additional complexity must be introduced to avoid having a crash bring down\nthe entire system. It is somewhat ironic that distributed algorithms are generally\nmore sensitive to crashes than centralized ones. In this sense, it should not come\nas a surprise that, indeed, centralized mutual exclusion is widely applied: it\nis simple to understand the behavior, and relatively easy to increase the fault\ntolerance of the centralized server. However, centralized solutions may suffer\nfrom scalability problems.\n6.4 Election algorithms\nMany distributed algorithms require one process to act as coordinator, initiator,\nor otherwise perform some special role. In general, it does not matter which\nprocess takes on this special responsibility, but one of them has to do it. In\nthis section we will look at algorithms for electing a coordinator (using this as\na generic name for the special process).\nIf all processes are exactly the same, with no distinguishing characteristics,\nthere is no way to select one of them to be special. Consequently, we will\nassume that each process Phas a unique identi\ufb01er id(P). In general, elec-\ntion algorithms attempt to locate the process with the highest identi\ufb01er and\ndesignate it as coordinator. The algorithms differ in the way they locate the\ncoordinator.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "330 CHAPTER 6. COORDINATION\nFurthermore, we also assume that every process knows the identi\ufb01er of\nevery other process. In other words, each process has complete knowledge of\nthe process group in which a coordinator must be elected. What the processes\ndo not know is which ones are currently up and which ones are currently\ndown. The goal of an election algorithm is to ensure that when an election\nstarts, it concludes with all processes agreeing on who the new coordinator is\nto be. There are many algorithms and variations, of which several important\nones are discussed in the text books by Tel [2000] and Lynch [1996].\nThe bully algorithm\nA well-known solution for electing a coordinator is the bully algorithm de-\nvised by Garcia-Molina [1982]. In the following, we consider Nprocesses\nfP0,. . .,PN\u00001gand let id(Pk) =k. When any process notices that the coordi-\nnator is no longer responding to requests, it initiates an election. A process,\nPk, holds an election as follows:\n1.Pksends an ELECTION message to all processes with higher identi\ufb01ers:\nPk+1,Pk+2, . . . , PN\u00001.\n2. If no one responds, Pkwins the election and becomes coordinator.\n3. If one of the higher-ups answers, it takes over and Pk\u2019s job is done.\nAt any moment, a process can get an ELECTION message from one of its\nlower-numbered colleagues. When such a message arrives, the receiver sends\nanOKmessage back to the sender to indicate that he is alive and will take\nover. The receiver then holds an election, unless it is already holding one.\nEventually, all processes give up but one, and that one is the new coordinator.\nIt announces its victory by sending all processes a message telling them that\nstarting immediately it is the new coordinator.\nIf a process that was previously down comes back up, it holds an election.\nIf it happens to be the highest-numbered process currently running, it will\nwin the election and take over the coordinator\u2019s job. Thus the biggest guy in\ntown always wins, hence the name \u201cbully algorithm.\u201d\nIn Figure 6.20 we see an example of how the bully algorithm works. The\ngroup consists of eight processes, with identi\ufb01ers numbered from 0 to 7.\nPreviously process P7was the coordinator, but it has just crashed. Process\nP4is the \ufb01rst one to notice this, so it sends ELECTION messages to all the\nprocesses higher than it, namely P5,P6, and P7, as shown in Figure 6.20(a).\nProcesses P5and P6both respond with OK, as shown in Figure 6.20(b). Upon\ngetting the \ufb01rst of these responses, P4knows that its job is over, knowing that\neither one of P5orP6will take over and become coordinator. Process P4just\nsits back and waits to see who the winner will be (although at this point it\ncan make a pretty good guess).\nIn Figure 6.20(c) both P5and P6hold elections, each one sending messages\nonly to those processes with identi\ufb01ers higher than itself. In Figure 6.20(d),\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n330 CHAPTER 6. COORDINATION\nFurthermore, we also assume that every process knows the identi\ufb01er of\nevery other process. In other words, each process has complete knowledge of\nthe process group in which a coordinator must be elected. What the processes\ndo not know is which ones are currently up and which ones are currently\ndown. The goal of an election algorithm is to ensure that when an election\nstarts, it concludes with all processes agreeing on who the new coordinator is\nto be. There are many algorithms and variations, of which several important\nones are discussed in the text books by Tel [2000] and Lynch [1996].\nThe bully algorithm\nA well-known solution for electing a coordinator is the bully algorithm de-\nvised by Garcia-Molina [1982]. In the following, we consider Nprocesses\nfP0,. . .,PN\u00001gand let id(Pk) =k. When any process notices that the coordi-\nnator is no longer responding to requests, it initiates an election. A process,\nPk, holds an election as follows:\n1.Pksends an ELECTION message to all processes with higher identi\ufb01ers:\nPk+1,Pk+2, . . . , PN\u00001.\n2. If no one responds, Pkwins the election and becomes coordinator.\n3. If one of the higher-ups answers, it takes over and Pk\u2019s job is done.\nAt any moment, a process can get an ELECTION message from one of its\nlower-numbered colleagues. When such a message arrives, the receiver sends\nanOKmessage back to the sender to indicate that he is alive and will take\nover. The receiver then holds an election, unless it is already holding one.\nEventually, all processes give up but one, and that one is the new coordinator.\nIt announces its victory by sending all processes a message telling them that\nstarting immediately it is the new coordinator.\nIf a process that was previously down comes back up, it holds an election.\nIf it happens to be the highest-numbered process currently running, it will\nwin the election and take over the coordinator\u2019s job. Thus the biggest guy in\ntown always wins, hence the name \u201cbully algorithm.\u201d\nIn Figure 6.20 we see an example of how the bully algorithm works. The\ngroup consists of eight processes, with identi\ufb01ers numbered from 0 to 7.\nPreviously process P7was the coordinator, but it has just crashed. Process\nP4is the \ufb01rst one to notice this, so it sends ELECTION messages to all the\nprocesses higher than it, namely P5,P6, and P7, as shown in Figure 6.20(a).\nProcesses P5and P6both respond with OK, as shown in Figure 6.20(b). Upon\ngetting the \ufb01rst of these responses, P4knows that its job is over, knowing that\neither one of P5orP6will take over and become coordinator. Process P4just\nsits back and waits to see who the winner will be (although at this point it\ncan make a pretty good guess).\nIn Figure 6.20(c) both P5and P6hold elections, each one sending messages\nonly to those processes with identi\ufb01ers higher than itself. In Figure 6.20(d),\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.4. ELECTION ALGORITHMS 331\n(a) (b)\n(c)\n(d) (e)\nFigure 6.20: The bully election algorithm. (a) Process 4 holds an election. (b)\nProcesses 5 and 6 respond, telling 4 to stop. (c) Now 5 and 6 each hold an\nelection. (d) Process 6 tells 5 to stop. (e) Process 6 wins and tells everyone.\nP6tells P5that it will take over. At this point P6knows that P7is dead and\nthat it ( P6) is the winner. If there is state information to be collected from disk\nor elsewhere to pick up where the old coordinator left off, P6must now do\nwhat is needed. When it is ready to take over, it announces the takeover by\nsending a COORDINATOR message to all running processes. When P4gets\nthis message, it can now continue with the operation it was trying to do when\nit discovered that P7was dead, but using P6as the coordinator this time. In\nthis way the failure of P7is handled and the work can continue.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.4. ELECTION ALGORITHMS 331\n(a) (b)\n(c)\n(d) (e)\nFigure 6.20: The bully election algorithm. (a) Process 4 holds an election. (b)\nProcesses 5 and 6 respond, telling 4 to stop. (c) Now 5 and 6 each hold an\nelection. (d) Process 6 tells 5 to stop. (e) Process 6 wins and tells everyone.\nP6tells P5that it will take over. At this point P6knows that P7is dead and\nthat it ( P6) is the winner. If there is state information to be collected from disk\nor elsewhere to pick up where the old coordinator left off, P6must now do\nwhat is needed. When it is ready to take over, it announces the takeover by\nsending a COORDINATOR message to all running processes. When P4gets\nthis message, it can now continue with the operation it was trying to do when\nit discovered that P7was dead, but using P6as the coordinator this time. In\nthis way the failure of P7is handled and the work can continue.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "332 CHAPTER 6. COORDINATION\nIf process P7is ever restarted, it will send all the others a COORDINATOR\nmessage and bully them into submission.\nA ring algorithm\nConsider the following election algorithm that is based on the use of a (logical)\nring. Unlike some ring algorithms, this one does not use a token. We assume\nthat each process knows who its successor is. When any process notices that\nthe coordinator is not functioning, it builds an ELECTION message containing\nits own process identi\ufb01er and sends the message to its successor. If the\nsuccessor is down, the sender skips over the successor and goes to the next\nmember along the ring, or the one after that, until a running process is located.\nAt each step along the way, the sender adds its own identi\ufb01er to the list in the\nmessage effectively making itself a candidate to be elected as coordinator.\nEventually, the message gets back to the process that started it all. That pro-\ncess recognizes this event when it receives an incoming message containing its\nown identi\ufb01er. At that point, the message type is changed to COORDINATOR\nand circulated once again, this time to inform everyone else who the coordi-\nnator is (the list member with the highest identi\ufb01er) and who the members of\nthe new ring are. When this message has circulated once, it is removed and\neveryone goes back to work.\nFigure 6.21: Election algorithm using a ring. The solid line shows the election\nmessages initiated by P6; the dashed one those by P3.\nIn Figure 6.21 we see what happens if two processes, P3and P6, discover\nsimultaneously that the previous coordinator, process P7, has crashed. Each\nof these builds an ELECTION message and each of them starts circulating its\nmessage, independent of the other one. Eventually, both messages will go all\nthe way around, and both P3and P6will convert them into COORDINATOR\nmessages, with exactly the same members and in the same order. When both\nhave gone around again, both will be removed. It does no harm to have extra\nmessages circulating; at worst it consumes a little bandwidth, but this is not\nconsidered wasteful.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n332 CHAPTER 6. COORDINATION\nIf process P7is ever restarted, it will send all the others a COORDINATOR\nmessage and bully them into submission.\nA ring algorithm\nConsider the following election algorithm that is based on the use of a (logical)\nring. Unlike some ring algorithms, this one does not use a token. We assume\nthat each process knows who its successor is. When any process notices that\nthe coordinator is not functioning, it builds an ELECTION message containing\nits own process identi\ufb01er and sends the message to its successor. If the\nsuccessor is down, the sender skips over the successor and goes to the next\nmember along the ring, or the one after that, until a running process is located.\nAt each step along the way, the sender adds its own identi\ufb01er to the list in the\nmessage effectively making itself a candidate to be elected as coordinator.\nEventually, the message gets back to the process that started it all. That pro-\ncess recognizes this event when it receives an incoming message containing its\nown identi\ufb01er. At that point, the message type is changed to COORDINATOR\nand circulated once again, this time to inform everyone else who the coordi-\nnator is (the list member with the highest identi\ufb01er) and who the members of\nthe new ring are. When this message has circulated once, it is removed and\neveryone goes back to work.\nFigure 6.21: Election algorithm using a ring. The solid line shows the election\nmessages initiated by P6; the dashed one those by P3.\nIn Figure 6.21 we see what happens if two processes, P3and P6, discover\nsimultaneously that the previous coordinator, process P7, has crashed. Each\nof these builds an ELECTION message and each of them starts circulating its\nmessage, independent of the other one. Eventually, both messages will go all\nthe way around, and both P3and P6will convert them into COORDINATOR\nmessages, with exactly the same members and in the same order. When both\nhave gone around again, both will be removed. It does no harm to have extra\nmessages circulating; at worst it consumes a little bandwidth, but this is not\nconsidered wasteful.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.4. ELECTION ALGORITHMS 333\nElections in wireless environments\nTraditional election algorithms are generally based on assumptions that are\nnot realistic in wireless environments. For example, they assume that message\npassing is reliable and that the topology of the network does not change.\nThese assumptions are false in most wireless environments, especially those\nfor mobile ad hoc networks.\nOnly few protocols for elections have been developed that work in ad hoc\nnetworks. Vasudevan et al. [2004] propose a solution that can handle failing\nnodes and partitioning networks. An important property of their solution is\nthat the bestleader can be elected rather than just a random one as was more\nor less the case in the previously discussed solutions. Their protocol works as\nfollows. To simplify our discussion, we concentrate only on ad hoc networks\nand ignore that nodes can move.\nConsider a wireless ad hoc network. To elect a leader, any node in the\nnetwork, called the source, can initiate an election by sending an ELECTION\nmessage to its immediate neighbors (i.e., the nodes in its range). When a\nnode receives an ELECTION for the \ufb01rst time, it designates the sender as its\nparent, and subsequently sends out an ELECTION message to all its immediate\nneighbors, except for the parent. When a node receives an ELECTION message\nfrom a node other than its parent, it merely acknowledges the receipt.\nWhen node Rhas designated node Qas its parent, it forwards the\nELECTION message to its immediate neighbors (excluding Q) and waits for\nacknowledgments to come in before acknowledging the ELECTION message\nfrom Q. This waiting has an important consequence. First, note that neighbors\nthat have already selected a parent will immediately respond to R. More\nspeci\ufb01cally, if all neighbors already have a parent, Ris a leaf node and will be\nable to report back to Qquickly. In doing so, it will also report information\nsuch as its battery lifetime and other resource capacities.\nThis information will later allow Qto compare R\u2019s capacities to that of\nother downstream nodes, and select the best eligible node for leadership. Of\ncourse, Qhad sent an ELECTION message only because its own parent Phad\ndone so as well. In turn, when Qeventually acknowledges the ELECTION\nmessage previously sent by P, it will pass the most eligible node to Pas well.\nIn this way, the source will eventually get to know which node is best to be\nselected as leader, after which it will broadcast this information to all other\nnodes.\nThis process is illustrated in Figure 6.22. Nodes have been labeled ato\nj, along with their capacity. Node ainitiates an election by broadcasting\nanELECTION message to nodes band j, as shown in Figure 6.22(b) After\nthat step, ELECTION messages are propagated to all nodes, ending with the\nsituation shown in Figure 6.22(e), where we have omitted the last broadcast\nby nodes fand i. From there on, each node reports to its parent the node\nwith the best capacity, as shown in Figure 6.22(f). For example, when node g\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.4. ELECTION ALGORITHMS 333\nElections in wireless environments\nTraditional election algorithms are generally based on assumptions that are\nnot realistic in wireless environments. For example, they assume that message\npassing is reliable and that the topology of the network does not change.\nThese assumptions are false in most wireless environments, especially those\nfor mobile ad hoc networks.\nOnly few protocols for elections have been developed that work in ad hoc\nnetworks. Vasudevan et al. [2004] propose a solution that can handle failing\nnodes and partitioning networks. An important property of their solution is\nthat the bestleader can be elected rather than just a random one as was more\nor less the case in the previously discussed solutions. Their protocol works as\nfollows. To simplify our discussion, we concentrate only on ad hoc networks\nand ignore that nodes can move.\nConsider a wireless ad hoc network. To elect a leader, any node in the\nnetwork, called the source, can initiate an election by sending an ELECTION\nmessage to its immediate neighbors (i.e., the nodes in its range). When a\nnode receives an ELECTION for the \ufb01rst time, it designates the sender as its\nparent, and subsequently sends out an ELECTION message to all its immediate\nneighbors, except for the parent. When a node receives an ELECTION message\nfrom a node other than its parent, it merely acknowledges the receipt.\nWhen node Rhas designated node Qas its parent, it forwards the\nELECTION message to its immediate neighbors (excluding Q) and waits for\nacknowledgments to come in before acknowledging the ELECTION message\nfrom Q. This waiting has an important consequence. First, note that neighbors\nthat have already selected a parent will immediately respond to R. More\nspeci\ufb01cally, if all neighbors already have a parent, Ris a leaf node and will be\nable to report back to Qquickly. In doing so, it will also report information\nsuch as its battery lifetime and other resource capacities.\nThis information will later allow Qto compare R\u2019s capacities to that of\nother downstream nodes, and select the best eligible node for leadership. Of\ncourse, Qhad sent an ELECTION message only because its own parent Phad\ndone so as well. In turn, when Qeventually acknowledges the ELECTION\nmessage previously sent by P, it will pass the most eligible node to Pas well.\nIn this way, the source will eventually get to know which node is best to be\nselected as leader, after which it will broadcast this information to all other\nnodes.\nThis process is illustrated in Figure 6.22. Nodes have been labeled ato\nj, along with their capacity. Node ainitiates an election by broadcasting\nanELECTION message to nodes band j, as shown in Figure 6.22(b) After\nthat step, ELECTION messages are propagated to all nodes, ending with the\nsituation shown in Figure 6.22(e), where we have omitted the last broadcast\nby nodes fand i. From there on, each node reports to its parent the node\nwith the best capacity, as shown in Figure 6.22(f). For example, when node g\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "334 CHAPTER 6. COORDINATION\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 6.22: Election algorithm in a wireless network, with node aas the\nsource. (a) Initial network. (b)\u2013(e) The build-tree phase (last broadcast step by\nnodes fand inot shown). (f) Reporting of best node to source.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n334 CHAPTER 6. COORDINATION\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 6.22: Election algorithm in a wireless network, with node aas the\nsource. (a) Initial network. (b)\u2013(e) The build-tree phase (last broadcast step by\nnodes fand inot shown). (f) Reporting of best node to source.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.4. ELECTION ALGORITHMS 335\nreceives the acknowledgments from its children eand h, it will notice that h\nis the best node, propagating [h,8]to its own parent, node b. In the end, the\nsource will note that his the best leader and will broadcast this information\nto all other nodes.\nWhen multiple elections are initiated, each node will decide to join only\none election. To this end, each source tags its ELECTION message with a\nunique identi\ufb01er. Nodes will participate only in the election with the highest\nidenti\ufb01er, stopping any running participation in other elections.\nWith some minor adjustments, the protocol can be shown to operate also\nwhen the network partitions, and when nodes join and leave. The details can\nbe found in Vasudevan et al. [2004].\nElections in large-scale systems\nMany leader-election algorithms apply to only relatively small distributed\nsystems. Moreover, algorithms often concentrate on the selection of only\na single node. There are situations when several nodes should actually be\nselected, such as in the case of super peers in peer-to-peer networks, which\nwe discussed in Section 2.3. In this section, we concentrate speci\ufb01cally on the\nproblem of selecting super peers.\nThe following requirements need to be met for super-peer selection (see\nalso [Lo et al., 2005]):\n1. Normal nodes should have low-latency access to super peers.\n2. Super peers should be evenly distributed across the overlay network.\n3.There should be a prede\ufb01ned portion of super peers relative to the total\nnumber of nodes in the overlay network.\n4. Each super peer should not need to serve more than a \ufb01xed number of\nnormal nodes.\nFortunately, these requirements are relatively easy to meet in most peer-to-\npeer systems, given the fact that the overlay network is either structured (as\nin DHT-based systems), or randomly unstructured (as, for example, can be\nrealized with gossip-based solutions). Let us take a look at solutions proposed\nby Lo et al. [2005].\nIn the case of DHT-based systems, the basic idea is to reserve a fraction\nof the identi\ufb01er space for super peers. In a DHT-based system, each node\nreceives a random and uniformly assigned m-bit identi\ufb01er. Now suppose we\nreserve the \ufb01rst (i.e., leftmost) kbits to identify super peers. For example, if\nwe need Nsuperpeers, then the \ufb01rst dlog2(N)ebits of any keycan be used to\nidentify these nodes.\nTo explain, assume we have a (small) Chord system with m=8and\nk=3. When looking up the node responsible for a speci\ufb01c key K, we can\n\ufb01rst decide to route the lookup request to the node responsible for the pattern\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.4. ELECTION ALGORITHMS 335\nreceives the acknowledgments from its children eand h, it will notice that h\nis the best node, propagating [h,8]to its own parent, node b. In the end, the\nsource will note that his the best leader and will broadcast this information\nto all other nodes.\nWhen multiple elections are initiated, each node will decide to join only\none election. To this end, each source tags its ELECTION message with a\nunique identi\ufb01er. Nodes will participate only in the election with the highest\nidenti\ufb01er, stopping any running participation in other elections.\nWith some minor adjustments, the protocol can be shown to operate also\nwhen the network partitions, and when nodes join and leave. The details can\nbe found in Vasudevan et al. [2004].\nElections in large-scale systems\nMany leader-election algorithms apply to only relatively small distributed\nsystems. Moreover, algorithms often concentrate on the selection of only\na single node. There are situations when several nodes should actually be\nselected, such as in the case of super peers in peer-to-peer networks, which\nwe discussed in Section 2.3. In this section, we concentrate speci\ufb01cally on the\nproblem of selecting super peers.\nThe following requirements need to be met for super-peer selection (see\nalso [Lo et al., 2005]):\n1. Normal nodes should have low-latency access to super peers.\n2. Super peers should be evenly distributed across the overlay network.\n3.There should be a prede\ufb01ned portion of super peers relative to the total\nnumber of nodes in the overlay network.\n4. Each super peer should not need to serve more than a \ufb01xed number of\nnormal nodes.\nFortunately, these requirements are relatively easy to meet in most peer-to-\npeer systems, given the fact that the overlay network is either structured (as\nin DHT-based systems), or randomly unstructured (as, for example, can be\nrealized with gossip-based solutions). Let us take a look at solutions proposed\nby Lo et al. [2005].\nIn the case of DHT-based systems, the basic idea is to reserve a fraction\nof the identi\ufb01er space for super peers. In a DHT-based system, each node\nreceives a random and uniformly assigned m-bit identi\ufb01er. Now suppose we\nreserve the \ufb01rst (i.e., leftmost) kbits to identify super peers. For example, if\nwe need Nsuperpeers, then the \ufb01rst dlog2(N)ebits of any keycan be used to\nidentify these nodes.\nTo explain, assume we have a (small) Chord system with m=8and\nk=3. When looking up the node responsible for a speci\ufb01c key K, we can\n\ufb01rst decide to route the lookup request to the node responsible for the pattern\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "336 CHAPTER 6. COORDINATION\nK^11100000 which is then treated as the superpeer.1Note that each node with\nidenti\ufb01er IDcan check whether it is a super peer by looking up ID^11100000\nto see if this request is routed to itself. Provided node identi\ufb01ers are uniformly\nassigned to nodes, it can be seen that with a total of Nnodes the number of\nsuper peers is, on average, equal to 2k\u0000mN.\nA completely different approach is based on positioning nodes in an m-\ndimensional geometric space. In this case, assume we need to place Nsuper\npeers evenly throughout the overlay. The basic idea is simple: a total of N\ntokens are spread across Nrandomly chosen nodes. No node can hold more\nthan one token. Each token represents a repelling force by which another\ntoken is inclined to move away. The net effect is that if all tokens exert the same\nrepulsion force, they will move away from each other and spread themselves\nevenly in the geometric space.\nThis approach requires that nodes holding a token learn about other\ntokens. To this end, we can use a gossiping protocol by which a token\u2019s force\nis disseminated throughout the network. If a node discovers that the total\nforces that are acting on it exceed a threshold, it will move the token in the\ndirection of the combined forces, as shown in Figure 6.23. When a token is\nheld by a node for a given amount of time, that node will promote itself to\nsuperpeer.\nFigure 6.23: Moving tokens in a two-dimensional space using repulsion forces.\n6.5 Location systems\nWhen looking at very large distributed systems that are dispersed across a\nwide-area network, it is often necessary to take proximity into account. Just\nimagine a distributed system organized as an overlay network in which two\nprocesses are neighbors in the overlay network, but are actually placed far\napart in the underlying network. If these two processes communicate a lot, it\nmay have been better to ensure that they are also physically placed in each\n1We use the binary operator \u201c ^\u201d to denote a bitwise and.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n336 CHAPTER 6. COORDINATION\nK^11100000 which is then treated as the superpeer.1Note that each node with\nidenti\ufb01er IDcan check whether it is a super peer by looking up ID^11100000\nto see if this request is routed to itself. Provided node identi\ufb01ers are uniformly\nassigned to nodes, it can be seen that with a total of Nnodes the number of\nsuper peers is, on average, equal to 2k\u0000mN.\nA completely different approach is based on positioning nodes in an m-\ndimensional geometric space. In this case, assume we need to place Nsuper\npeers evenly throughout the overlay. The basic idea is simple: a total of N\ntokens are spread across Nrandomly chosen nodes. No node can hold more\nthan one token. Each token represents a repelling force by which another\ntoken is inclined to move away. The net effect is that if all tokens exert the same\nrepulsion force, they will move away from each other and spread themselves\nevenly in the geometric space.\nThis approach requires that nodes holding a token learn about other\ntokens. To this end, we can use a gossiping protocol by which a token\u2019s force\nis disseminated throughout the network. If a node discovers that the total\nforces that are acting on it exceed a threshold, it will move the token in the\ndirection of the combined forces, as shown in Figure 6.23. When a token is\nheld by a node for a given amount of time, that node will promote itself to\nsuperpeer.\nFigure 6.23: Moving tokens in a two-dimensional space using repulsion forces.\n6.5 Location systems\nWhen looking at very large distributed systems that are dispersed across a\nwide-area network, it is often necessary to take proximity into account. Just\nimagine a distributed system organized as an overlay network in which two\nprocesses are neighbors in the overlay network, but are actually placed far\napart in the underlying network. If these two processes communicate a lot, it\nmay have been better to ensure that they are also physically placed in each\n1We use the binary operator \u201c ^\u201d to denote a bitwise and.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.5. LOCATION SYSTEMS 337\nother\u2019s proximity. In this section, we take a look at location-based techniques\nto coordinate the placement of processes and their communication.\nGPS: Global Positioning System\nLet us start by considering how to determine your geographical position\nanywhere on Earth. This positioning problem is by itself solved through\na highly speci\ufb01c, dedicated distributed system, namely GPS , which is an\nacronym for Global Positioning System . GPS is a satellite-based distributed\nsystem that was launched in 1978. Although it initially was used mainly for\nmilitary applications, it by now has found its way to many civilian applications,\nnotably for traf\ufb01c navigation. However, many more application domains exist.\nFor example, modern smartphones now allow owners to track each other\u2019s\nposition. This principle can easily be applied to tracking other things as well,\nincluding pets, children, cars, boats, and so on.\nGPS uses up to 72 satellites each circulating in an orbit at a height of\napproximately 20,000 km. Each satellite has up to four atomic clocks, which\nare regularly calibrated from special stations on Earth. A satellite continuously\nbroadcasts its position, and time stamps each message with its local time. This\nbroadcasting allows every receiver on Earth to accurately compute its own\nposition using, in principle, only four satellites. To explain, let us \ufb01rst assume\nthat all clocks, including the receiver\u2019s, are synchronized.\nIn order to compute a position, consider \ufb01rst the two-dimensional case,\nas shown in Figure 6.24, in which three satellites are drawn, along with the\ncircles representing points at the same distance from each respective satellite.\nWe see that the intersection of the three circles is a unique point.\nFigure 6.24: Computing a node\u2019s position in a two-dimensional space.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.5. LOCATION SYSTEMS 337\nother\u2019s proximity. In this section, we take a look at location-based techniques\nto coordinate the placement of processes and their communication.\nGPS: Global Positioning System\nLet us start by considering how to determine your geographical position\nanywhere on Earth. This positioning problem is by itself solved through\na highly speci\ufb01c, dedicated distributed system, namely GPS , which is an\nacronym for Global Positioning System . GPS is a satellite-based distributed\nsystem that was launched in 1978. Although it initially was used mainly for\nmilitary applications, it by now has found its way to many civilian applications,\nnotably for traf\ufb01c navigation. However, many more application domains exist.\nFor example, modern smartphones now allow owners to track each other\u2019s\nposition. This principle can easily be applied to tracking other things as well,\nincluding pets, children, cars, boats, and so on.\nGPS uses up to 72 satellites each circulating in an orbit at a height of\napproximately 20,000 km. Each satellite has up to four atomic clocks, which\nare regularly calibrated from special stations on Earth. A satellite continuously\nbroadcasts its position, and time stamps each message with its local time. This\nbroadcasting allows every receiver on Earth to accurately compute its own\nposition using, in principle, only four satellites. To explain, let us \ufb01rst assume\nthat all clocks, including the receiver\u2019s, are synchronized.\nIn order to compute a position, consider \ufb01rst the two-dimensional case,\nas shown in Figure 6.24, in which three satellites are drawn, along with the\ncircles representing points at the same distance from each respective satellite.\nWe see that the intersection of the three circles is a unique point.\nFigure 6.24: Computing a node\u2019s position in a two-dimensional space.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "338 CHAPTER 6. COORDINATION\nThis principle of intersecting circles can be expanded to three dimensions,\nmeaning that we need to know the distance to four satellites to determine the\nlongitude, latitude, and altitude of a receiver on Earth. This positioning is\nall fairly straightforward, but determining the distance to a satellite becomes\ncomplicated when we move from theory to practice. There are at least two\nimportant real-world facts that we need to take into account:\n1.It takes a while before data on a satellite\u2019s position reaches the receiver.\n2. The receiver\u2019s clock is generally not in sync with that of a satellite.\nAssume that the timestamp from a satellite is completely accurate. Let\nDrdenote the deviation of the receiver\u2019s clock from the actual time. When a\nmessage is received from satellite Siwith timestamp Ti, then the measured\ndelay Diby the receiver consists of two components: the actual delay, along\nwith its own deviation:\nDi= (Tnow\u0000Ti) +Dr\nwhere Tnowis the actual current time. As signals travel with the speed of light,\nc, the measured distance \u02dcdito satellite Siis equal to c\u0001Di. With\ndi=c\u0001(Tnow\u0000Ti)\nbeing the real distance between the receiver and satellite Si, the measured\ndistance can be rewritten to \u02dcdi=di+c\u0001Dr. The real distance is now computed\nas:\n\u02dcdi\u0000c\u0001Dr=q\n(xi\u0000xr)2+ (yi\u0000yr)2+ (zi\u0000zr)2\nwhere xi,yi, and zidenote the coordinates of satellite Si. What we see now is\na system of quadratic equations with four unknowns ( xr,yr,zr, and also Dr).\nWe thus need four reference points (i.e., satellites) to \ufb01nd a unique solution\nthat will also give us Dr. A GPS measurement will thus also give an account\nof the actual time.\nSo far, we have assumed that measurements are perfectly accurate. Of\ncourse, they are not. There are many sources of errors, starting with the\nfact that the atomic clocks in the satellites are not always in perfect sync, the\nposition of a satellite is not known precisely, the receiver\u2019s clock has a \ufb01nite\naccuracy, the signal propagation speed is not constant (as signals appear to\nslow down when entering, e.g., the ionosphere), and so on. On average, this\nleads to an error of some 5\u201310 meters. Special modulation techniques, as\nwell as special receivers, are needed to improve accuracy. Using so-called\ndifferential GPS , by which corrective information is sent through wide-area\nlinks, accuracy can be further improved. More information can be found in\n[LaMarca and de Lara, 2008], as well as an excellent overview by Zogg [2002].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n338 CHAPTER 6. COORDINATION\nThis principle of intersecting circles can be expanded to three dimensions,\nmeaning that we need to know the distance to four satellites to determine the\nlongitude, latitude, and altitude of a receiver on Earth. This positioning is\nall fairly straightforward, but determining the distance to a satellite becomes\ncomplicated when we move from theory to practice. There are at least two\nimportant real-world facts that we need to take into account:\n1.It takes a while before data on a satellite\u2019s position reaches the receiver.\n2. The receiver\u2019s clock is generally not in sync with that of a satellite.\nAssume that the timestamp from a satellite is completely accurate. Let\nDrdenote the deviation of the receiver\u2019s clock from the actual time. When a\nmessage is received from satellite Siwith timestamp Ti, then the measured\ndelay Diby the receiver consists of two components: the actual delay, along\nwith its own deviation:\nDi= (Tnow\u0000Ti) +Dr\nwhere Tnowis the actual current time. As signals travel with the speed of light,\nc, the measured distance \u02dcdito satellite Siis equal to c\u0001Di. With\ndi=c\u0001(Tnow\u0000Ti)\nbeing the real distance between the receiver and satellite Si, the measured\ndistance can be rewritten to \u02dcdi=di+c\u0001Dr. The real distance is now computed\nas:\n\u02dcdi\u0000c\u0001Dr=q\n(xi\u0000xr)2+ (yi\u0000yr)2+ (zi\u0000zr)2\nwhere xi,yi, and zidenote the coordinates of satellite Si. What we see now is\na system of quadratic equations with four unknowns ( xr,yr,zr, and also Dr).\nWe thus need four reference points (i.e., satellites) to \ufb01nd a unique solution\nthat will also give us Dr. A GPS measurement will thus also give an account\nof the actual time.\nSo far, we have assumed that measurements are perfectly accurate. Of\ncourse, they are not. There are many sources of errors, starting with the\nfact that the atomic clocks in the satellites are not always in perfect sync, the\nposition of a satellite is not known precisely, the receiver\u2019s clock has a \ufb01nite\naccuracy, the signal propagation speed is not constant (as signals appear to\nslow down when entering, e.g., the ionosphere), and so on. On average, this\nleads to an error of some 5\u201310 meters. Special modulation techniques, as\nwell as special receivers, are needed to improve accuracy. Using so-called\ndifferential GPS , by which corrective information is sent through wide-area\nlinks, accuracy can be further improved. More information can be found in\n[LaMarca and de Lara, 2008], as well as an excellent overview by Zogg [2002].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.5. LOCATION SYSTEMS 339\nWhen GPS is not an option\nA major drawback of GPS is that it can generally not be used indoors. For that\npurpose, other techniques are necessary. An increasingly popular technique\nis to make use of the numerous WiFi access points available. The basic idea\nis simple: if we have a database of known access points along with their\ncoordinates, and we can estimate our distance to an access point, then with\nonly three detected access points, we should be able to compute our position.\nOf course, it really is not that simple at all.\nA major problem is determining the coordinates of an access point. A\npopular approach is to do this through war driving : using a WiFi-enabled\ndevice along with a GPS receiver, someone drives or walks through an area\nand records observed access points. An access point can be identi\ufb01ed through\nits SSID or its MAC-level network address. An access point APshould\nbe detected at several locations before its coordinates can be estimated. A\nsimple method is to compute the centroid: assume we have detected AP\natNdifferent locations f~x1,~x2,. . .,~xNg, where each location ~xiconsists of\na (latitude, longitude )-pair as provided by the GPS receiver. We then simply\nestimate AP\u2019s location ~xAPas\n~xAP=\u00e5N\ni=1~xi\nN.\nAccuracy can be improved by taking the observed signal strength into account,\nand giving more weight to a location with relatively high observed signal\nstrength than to a location where only a weak signal was detected. In the end,\nwe obtain an estimation of the coordinates of the access point. The accuracy\nof this estimation is strongly in\ufb02uenced by:\n\u2022 the accuracy of each GPS detection point ~xi\n\u2022 the fact that an access point has a nonuniform transmission range\n\u2022 the number of sampled detection points N.\nStudies show that estimates of the coordinates of an access point may be\ntens of meters off from the actual location (see, e.g., Kim et al. [2006a] or\nTsui et al. [2010]). Moreover, access points come and go at a relatively high\nrate. Nevertheless, locating and positioning access points is widely popular,\nexempli\ufb01ed by the open-access Wigle database which is populated through\ncrowd sourcing.2\nLogical positioning of nodes\nInstead of trying to \ufb01nd the absolute location of a node in a distributed system,\nan alternative is to use a logical, proximity-based location. In geometric\n2Seewigle.net .\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.5. LOCATION SYSTEMS 339\nWhen GPS is not an option\nA major drawback of GPS is that it can generally not be used indoors. For that\npurpose, other techniques are necessary. An increasingly popular technique\nis to make use of the numerous WiFi access points available. The basic idea\nis simple: if we have a database of known access points along with their\ncoordinates, and we can estimate our distance to an access point, then with\nonly three detected access points, we should be able to compute our position.\nOf course, it really is not that simple at all.\nA major problem is determining the coordinates of an access point. A\npopular approach is to do this through war driving : using a WiFi-enabled\ndevice along with a GPS receiver, someone drives or walks through an area\nand records observed access points. An access point can be identi\ufb01ed through\nits SSID or its MAC-level network address. An access point APshould\nbe detected at several locations before its coordinates can be estimated. A\nsimple method is to compute the centroid: assume we have detected AP\natNdifferent locations f~x1,~x2,. . .,~xNg, where each location ~xiconsists of\na (latitude, longitude )-pair as provided by the GPS receiver. We then simply\nestimate AP\u2019s location ~xAPas\n~xAP=\u00e5N\ni=1~xi\nN.\nAccuracy can be improved by taking the observed signal strength into account,\nand giving more weight to a location with relatively high observed signal\nstrength than to a location where only a weak signal was detected. In the end,\nwe obtain an estimation of the coordinates of the access point. The accuracy\nof this estimation is strongly in\ufb02uenced by:\n\u2022 the accuracy of each GPS detection point ~xi\n\u2022 the fact that an access point has a nonuniform transmission range\n\u2022 the number of sampled detection points N.\nStudies show that estimates of the coordinates of an access point may be\ntens of meters off from the actual location (see, e.g., Kim et al. [2006a] or\nTsui et al. [2010]). Moreover, access points come and go at a relatively high\nrate. Nevertheless, locating and positioning access points is widely popular,\nexempli\ufb01ed by the open-access Wigle database which is populated through\ncrowd sourcing.2\nLogical positioning of nodes\nInstead of trying to \ufb01nd the absolute location of a node in a distributed system,\nan alternative is to use a logical, proximity-based location. In geometric\n2Seewigle.net .\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "340 CHAPTER 6. COORDINATION\noverlay networks each node is given a position in an m-dimensional geometric\nspace, such that the distance between two nodes in that space re\ufb02ects a real-\nworld performance metric. Computing such a position is the core business of a\nNetwork Coordinates System , or simply NCS , which are surveyed by Donnet\net al. [2010]. The simplest, and most applied example, is where distance\ncorresponds to internode latency. In other words, given two nodes Pand Q,\nthen the distance \u02c6d(P,Q)re\ufb02ects how long it would take for a message to\ntravel from PtoQand vice versa . We use the notation \u02c6dto denote distance in\na system where nodes have been assigned coordinates.\nThere are many applications of geometric overlay networks. Consider the\nsituation where a Web site at server Ohas been replicated to multiple servers\nS1,. . .,SNon the Internet. When a client Crequests a page from O, the latter\nmay decide to redirect that request to the server closest to C, that is, the one\nthat will give the best response time. If the geometric location of Cis known,\nas well as those of each replica server, Ocan then simply pick that server Si\nfor which \u02c6d(C,Si)is minimal. Note that such a selection requires only local\nprocessing at O. In other words, there is, for example, no need to sample all\nthe latencies between Cand each of the replica servers.\nAnother example is optimal replica placement. Consider again a Web\nsite that has gathered the positions of its clients. If the site were to replicate\nits content to Nservers, it can compute the Nbest positions where to place\nreplicas such that the average client-to-replica response time is minimal.\nPerforming such computations is almost trivially feasible if clients and servers\nhave geometric positions that re\ufb02ect internode latencies.\nAs a last example, consider position-based routing (see, e.g., [Popescu\net al., 2012] or [Bilal et al., 2013]). In such schemes, a message is forwarded\nto its destination using only positioning information. For example, a naive\nrouting algorithm to let each node forward a message to the neighbor closest\nto the destination. Although it can be easily shown that this speci\ufb01c algorithm\nneed not converge, it illustrates that only local information is used to take a\ndecision. There is no need to propagate link information or such to all nodes\nin the network, as is the case with conventional routing algorithms.\nCentralized positioning\nPositioning a node in an m-dimensional geometric space requires m+1\ndistance measures to nodes with known positions. Assuming that node P\nwants to compute its own position, it contacts three other nodes with known\npositions and measures its distance to each of them. Contacting only one node\nwould tell Pabout the circle it is located on; contacting only two nodes would\ntell it about the position of the intersection of two circles (which generally\nconsists of two points); a third node would subsequently allow Pto compute\nits actual location.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n340 CHAPTER 6. COORDINATION\noverlay networks each node is given a position in an m-dimensional geometric\nspace, such that the distance between two nodes in that space re\ufb02ects a real-\nworld performance metric. Computing such a position is the core business of a\nNetwork Coordinates System , or simply NCS , which are surveyed by Donnet\net al. [2010]. The simplest, and most applied example, is where distance\ncorresponds to internode latency. In other words, given two nodes Pand Q,\nthen the distance \u02c6d(P,Q)re\ufb02ects how long it would take for a message to\ntravel from PtoQand vice versa . We use the notation \u02c6dto denote distance in\na system where nodes have been assigned coordinates.\nThere are many applications of geometric overlay networks. Consider the\nsituation where a Web site at server Ohas been replicated to multiple servers\nS1,. . .,SNon the Internet. When a client Crequests a page from O, the latter\nmay decide to redirect that request to the server closest to C, that is, the one\nthat will give the best response time. If the geometric location of Cis known,\nas well as those of each replica server, Ocan then simply pick that server Si\nfor which \u02c6d(C,Si)is minimal. Note that such a selection requires only local\nprocessing at O. In other words, there is, for example, no need to sample all\nthe latencies between Cand each of the replica servers.\nAnother example is optimal replica placement. Consider again a Web\nsite that has gathered the positions of its clients. If the site were to replicate\nits content to Nservers, it can compute the Nbest positions where to place\nreplicas such that the average client-to-replica response time is minimal.\nPerforming such computations is almost trivially feasible if clients and servers\nhave geometric positions that re\ufb02ect internode latencies.\nAs a last example, consider position-based routing (see, e.g., [Popescu\net al., 2012] or [Bilal et al., 2013]). In such schemes, a message is forwarded\nto its destination using only positioning information. For example, a naive\nrouting algorithm to let each node forward a message to the neighbor closest\nto the destination. Although it can be easily shown that this speci\ufb01c algorithm\nneed not converge, it illustrates that only local information is used to take a\ndecision. There is no need to propagate link information or such to all nodes\nin the network, as is the case with conventional routing algorithms.\nCentralized positioning\nPositioning a node in an m-dimensional geometric space requires m+1\ndistance measures to nodes with known positions. Assuming that node P\nwants to compute its own position, it contacts three other nodes with known\npositions and measures its distance to each of them. Contacting only one node\nwould tell Pabout the circle it is located on; contacting only two nodes would\ntell it about the position of the intersection of two circles (which generally\nconsists of two points); a third node would subsequently allow Pto compute\nits actual location.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.5. LOCATION SYSTEMS 341\nNode Pcan compute its own coordinates (xP,yP)by solving the three\nquadratic equations with the two unknowns xPand yP:\n\u02dcdi=q\n(xi\u0000xP)2+ (yi\u0000yP)2 (i=1, 2, 3 )\nHere, we use \u02dcdto denote measured, or estimated distance. As said, \u02dcdigenerally\ncorresponds to measuring the latency between Pand the node at (xi,yi). This\nlatency can be estimated as being half the round-trip delay, but it should\nbe clear that its value will be different over time. The effect is a different\npositioning whenever Pwould want to recompute its position. Moreover, if\nother nodes would use P\u2019s current position to compute their own coordinates,\nthen it should be clear that the error in positioning Pwill affect the accuracy\nof the positioning of other nodes.\nIt should also be clear that measured distances between a set of nodes\nwill generally not even be consistent. For example, assume we are computing\ndistances in a one-dimensional space as shown in Figure 6.25. In this example,\nwe see that although Rmeasures its distance to Qas 2.0, and \u02dcd(P,Q)has been\nmeasured to be 1.0, when Rmeasures \u02dcd(P,R)it \ufb01nds 2.8, which is clearly\ninconsistent with the other two measurements.\nFigure 6.25: Inconsistent distance measurements in a one-dimensional space.\nFigure 6.25 also suggests how this situation can be improved. In our simple\nexample, we could solve the inconsistencies by merely computing positions\nin a two-dimensional space. This by itself, however, is not a general solution\nwhen dealing with many measurements. In fact, considering that Internet\nlatency measurements may violate the triangle inequality , it is generally\nimpossible to resolve inconsistencies completely. The triangle inequality states\nthat in a geometric space, for any arbitrary three nodes P,Q, and Rit must\nalways be true that\nd(P,R)\u0014d(P,Q) +d(Q,R).\nThere are various ways to approach these issues. One common approach,\nproposed by Ng and Zhang [2002], is to use Nspecial nodes L1,. . .,LN, known\naslandmarks . Landmarks measure their pairwise latencies \u02dcd(Li,Lj)and\nsubsequently let a central node compute the coordinates for each landmark.\nTo this end, the central node seeks to minimize the following aggregated error\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.5. LOCATION SYSTEMS 341\nNode Pcan compute its own coordinates (xP,yP)by solving the three\nquadratic equations with the two unknowns xPand yP:\n\u02dcdi=q\n(xi\u0000xP)2+ (yi\u0000yP)2 (i=1, 2, 3 )\nHere, we use \u02dcdto denote measured, or estimated distance. As said, \u02dcdigenerally\ncorresponds to measuring the latency between Pand the node at (xi,yi). This\nlatency can be estimated as being half the round-trip delay, but it should\nbe clear that its value will be different over time. The effect is a different\npositioning whenever Pwould want to recompute its position. Moreover, if\nother nodes would use P\u2019s current position to compute their own coordinates,\nthen it should be clear that the error in positioning Pwill affect the accuracy\nof the positioning of other nodes.\nIt should also be clear that measured distances between a set of nodes\nwill generally not even be consistent. For example, assume we are computing\ndistances in a one-dimensional space as shown in Figure 6.25. In this example,\nwe see that although Rmeasures its distance to Qas 2.0, and \u02dcd(P,Q)has been\nmeasured to be 1.0, when Rmeasures \u02dcd(P,R)it \ufb01nds 2.8, which is clearly\ninconsistent with the other two measurements.\nFigure 6.25: Inconsistent distance measurements in a one-dimensional space.\nFigure 6.25 also suggests how this situation can be improved. In our simple\nexample, we could solve the inconsistencies by merely computing positions\nin a two-dimensional space. This by itself, however, is not a general solution\nwhen dealing with many measurements. In fact, considering that Internet\nlatency measurements may violate the triangle inequality , it is generally\nimpossible to resolve inconsistencies completely. The triangle inequality states\nthat in a geometric space, for any arbitrary three nodes P,Q, and Rit must\nalways be true that\nd(P,R)\u0014d(P,Q) +d(Q,R).\nThere are various ways to approach these issues. One common approach,\nproposed by Ng and Zhang [2002], is to use Nspecial nodes L1,. . .,LN, known\naslandmarks . Landmarks measure their pairwise latencies \u02dcd(Li,Lj)and\nsubsequently let a central node compute the coordinates for each landmark.\nTo this end, the central node seeks to minimize the following aggregated error\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "342 CHAPTER 6. COORDINATION\nfunction:\nN\n\u00e5\ni=1N\n\u00e5\nj=i+1\u0012\u02dcd(Li,Lj)\u0000\u02c6d(Li,Lj)\n\u02dcd(Li,Lj)\u00132\nwhere, again, \u02c6d(Li,Lj)corresponds to the distance after nodes Liand Ljhave\nbeen positioned.\nThe hidden parameter in minimizing the aggregated error function is the\ndimension m. Obviously, we have that N>m, but nothing prevents us from\nchoosing a value for mthat is much smaller than N. In that case, a node P\nmeasures its distance to each of the Nlandmarks and computes its coordinates\nby minimizing\nN\n\u00e5\ni=1\u0012\u02dcd(Li,P)\u0000\u02c6d(Li,P)\n\u02dcd(Li,P)\u00132\nAs it turns out, with well-chosen landmarks, mcan be as small as 6 or 7, with\n\u02c6d(P,Q)being no more than a factor 2 different from the actual latency d(P,Q)\nfor arbitrary nodes Pand Q[Szymaniak et al., 2004; 2008].\nDecentralized positioning\nAnother way to tackle this problem is to view the collection of nodes as a huge\nsystem in which nodes are attached to each other through springs. In this\ncase,j\u02dcd(P,Q)\u0000\u02c6d(P,Q)jindicates to what extent nodes Pand Qare displaced\nrelative to the situation in which the system of springs would be at rest. By\nletting each node (slightly) change its position, it can be shown that the system\nwill eventually converge to an optimal organization in which the aggregated\nerror is minimal. This approach is followed in Vivaldi, described in [Dabek\net al., 2004a].\nIn a system with Nnodes P1,. . .,PN, Vivaldi aims at minimizing the\nfollowing aggregated error:\nN\n\u00e5\ni=1N\n\u00e5\nj=1j\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)j2\nwhere \u02dcd(Pi,Pj)is the measured distance (i.e., latency) between nodes Piand\nPj, and \u02c6d(Pi,Pj)the distance computed from the network coordinates of each\nnode. Let ~xidenote the coordinates of node Pi. In a situation that each node\nis placed in a geometric space, the force that node Piexerts on node Pjis\ncomputed as:\n~Fij=\u0000\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)\u0001\u0002u(~xi\u0000~xj)\nwith u(~xi\u0000~xj)denoting the unit vector in the direction of ~xi\u0000~xj. In other\nwords, if Fij>0, node Piwill push Pjaway from itself, and will otherwise\npull it toward itself. Node Pinow repeatedly executes the following steps:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n342 CHAPTER 6. COORDINATION\nfunction:\nN\n\u00e5\ni=1N\n\u00e5\nj=i+1\u0012\u02dcd(Li,Lj)\u0000\u02c6d(Li,Lj)\n\u02dcd(Li,Lj)\u00132\nwhere, again, \u02c6d(Li,Lj)corresponds to the distance after nodes Liand Ljhave\nbeen positioned.\nThe hidden parameter in minimizing the aggregated error function is the\ndimension m. Obviously, we have that N>m, but nothing prevents us from\nchoosing a value for mthat is much smaller than N. In that case, a node P\nmeasures its distance to each of the Nlandmarks and computes its coordinates\nby minimizing\nN\n\u00e5\ni=1\u0012\u02dcd(Li,P)\u0000\u02c6d(Li,P)\n\u02dcd(Li,P)\u00132\nAs it turns out, with well-chosen landmarks, mcan be as small as 6 or 7, with\n\u02c6d(P,Q)being no more than a factor 2 different from the actual latency d(P,Q)\nfor arbitrary nodes Pand Q[Szymaniak et al., 2004; 2008].\nDecentralized positioning\nAnother way to tackle this problem is to view the collection of nodes as a huge\nsystem in which nodes are attached to each other through springs. In this\ncase,j\u02dcd(P,Q)\u0000\u02c6d(P,Q)jindicates to what extent nodes Pand Qare displaced\nrelative to the situation in which the system of springs would be at rest. By\nletting each node (slightly) change its position, it can be shown that the system\nwill eventually converge to an optimal organization in which the aggregated\nerror is minimal. This approach is followed in Vivaldi, described in [Dabek\net al., 2004a].\nIn a system with Nnodes P1,. . .,PN, Vivaldi aims at minimizing the\nfollowing aggregated error:\nN\n\u00e5\ni=1N\n\u00e5\nj=1j\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)j2\nwhere \u02dcd(Pi,Pj)is the measured distance (i.e., latency) between nodes Piand\nPj, and \u02c6d(Pi,Pj)the distance computed from the network coordinates of each\nnode. Let ~xidenote the coordinates of node Pi. In a situation that each node\nis placed in a geometric space, the force that node Piexerts on node Pjis\ncomputed as:\n~Fij=\u0000\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)\u0001\u0002u(~xi\u0000~xj)\nwith u(~xi\u0000~xj)denoting the unit vector in the direction of ~xi\u0000~xj. In other\nwords, if Fij>0, node Piwill push Pjaway from itself, and will otherwise\npull it toward itself. Node Pinow repeatedly executes the following steps:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.6. DISTRIBUTED EVENT MATCHING 343\n1. Measure the latency \u02dcdijto node Pj, and also receive Pj\u2019s coordinates ~xj.\n2. Compute the error e=\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)\n3. Compute the direction ~u=u(~xi\u0000~xj).\n4. Compute the force vector Fij=e\u0001~u\n5. Adjust own position by moving along the force vector: ~xi ~xi+d\u0001~u.\nA crucial element is the choice of d: too large and the system will oscillate;\ntoo small and convergence to a stable situation will take a long time. The\ntrick is to have an adaptive value, which is large when the error is large as\nwell, but small when only small adjustments are needed. Details can be found\nin [Dabek et al., 2004a].\n6.6 Distributed event matching\nAs a \ufb01nal subject concerning the coordination among processes, we consider\ndistributed event matching. Event matching, or more precisely, noti\ufb01cation\n\ufb01ltering , is at the heart of publish-subscribe systems. The problem boils down\nto the following:\n\u2022A process speci\ufb01es through a subscription Sin which events it is inter-\nested.\n\u2022When a process publishes a noti\ufb01cation Non the occurrence of an event,\nthe system needs to see if Smatches N.\n\u2022In the case of a match, the system should send the noti\ufb01cation N, possibly\nincluding the data associated with the event that took place, to the\nsubscriber.\nAs a consequence, we need to facilitate at least two things: (1) matching\nsubscriptions against events, and (2) notifying a subscriber in the case of a\nmatch. The two can be separated, but this need not always be the case. In the\nfollowing, we assume the existence of a function match (S,N)which returns\ntrue when subscription Smatches the noti\ufb01cation N, and false otherwise.\nCentralized implementations\nA simple, naive implementation of event matching is to have a fully centralized\nserver that handles all subscriptions and noti\ufb01cations. In such a scheme, a\nsubscriber simply submits a subscription, which is subsequently stored. When\na publisher submits a noti\ufb01cation, that noti\ufb01cation is checked against each\nand every subscription, and when a match is found, the noti\ufb01cation is copied\nand forwarded to the associated subscriber.\nObviously, this is not a very scalable solution. Nevertheless, provided the\nmatching can be done ef\ufb01ciently and the server itself has enough processing\npower, the solution is feasible for many cases. For example, using a centralized\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.6. DISTRIBUTED EVENT MATCHING 343\n1. Measure the latency \u02dcdijto node Pj, and also receive Pj\u2019s coordinates ~xj.\n2. Compute the error e=\u02dcd(Pi,Pj)\u0000\u02c6d(Pi,Pj)\n3. Compute the direction ~u=u(~xi\u0000~xj).\n4. Compute the force vector Fij=e\u0001~u\n5. Adjust own position by moving along the force vector: ~xi ~xi+d\u0001~u.\nA crucial element is the choice of d: too large and the system will oscillate;\ntoo small and convergence to a stable situation will take a long time. The\ntrick is to have an adaptive value, which is large when the error is large as\nwell, but small when only small adjustments are needed. Details can be found\nin [Dabek et al., 2004a].\n6.6 Distributed event matching\nAs a \ufb01nal subject concerning the coordination among processes, we consider\ndistributed event matching. Event matching, or more precisely, noti\ufb01cation\n\ufb01ltering , is at the heart of publish-subscribe systems. The problem boils down\nto the following:\n\u2022A process speci\ufb01es through a subscription Sin which events it is inter-\nested.\n\u2022When a process publishes a noti\ufb01cation Non the occurrence of an event,\nthe system needs to see if Smatches N.\n\u2022In the case of a match, the system should send the noti\ufb01cation N, possibly\nincluding the data associated with the event that took place, to the\nsubscriber.\nAs a consequence, we need to facilitate at least two things: (1) matching\nsubscriptions against events, and (2) notifying a subscriber in the case of a\nmatch. The two can be separated, but this need not always be the case. In the\nfollowing, we assume the existence of a function match (S,N)which returns\ntrue when subscription Smatches the noti\ufb01cation N, and false otherwise.\nCentralized implementations\nA simple, naive implementation of event matching is to have a fully centralized\nserver that handles all subscriptions and noti\ufb01cations. In such a scheme, a\nsubscriber simply submits a subscription, which is subsequently stored. When\na publisher submits a noti\ufb01cation, that noti\ufb01cation is checked against each\nand every subscription, and when a match is found, the noti\ufb01cation is copied\nand forwarded to the associated subscriber.\nObviously, this is not a very scalable solution. Nevertheless, provided the\nmatching can be done ef\ufb01ciently and the server itself has enough processing\npower, the solution is feasible for many cases. For example, using a centralized\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "344 CHAPTER 6. COORDINATION\nserver is the canonical solution for implementing Linda tuple spaces or Java\nSpaces. Likewise, many publish-subscribe systems that run within a single\ndepartment or organization can be implemented through a central server.\nImportant in these cases, is that the matching function can be implemented\nef\ufb01ciently. In practice, this is often the case when dealing with topic-based\n\ufb01ltering: matching then resorts to checking for equality of attribute values.\nNote that a simple way to scale up a centralized implementation, is to de-\nterministically divide the work across multiple servers. A standard approach\nis to make use of two functions, as explained by Baldoni et al. [2009]:\n\u2022a function sub2node (S), which takes a subscription Sand maps it to a\nnonempty subset of servers\n\u2022a function not2node (N), which takes a noti\ufb01cation Nand maps it to a\nnonempty subset of servers.\nThe servers to which sub2node (S)is mapped are called the rendezvous nodes\nforS. Likewise, sub2node (N)are the rendezvous nodes for N. The only\nconstraint that needs to be satis\ufb01ed, is that for any subscription Sand matching\nnoti\ufb01cation N,sub2node (S)\\not2node (N)6=\u00c6. In other words, there must be\nat least one server that can handle the subscription when there is a matching\nnoti\ufb01cation. In practice, this constraint is satis\ufb01ed by topic-based publish-\nsubscribe systems by using a hashing function on the names of the topics.\nThe idea of having a central server can be extended by distributing the\nmatching across multiple servers and dividing the work. The servers, gener-\nally referred to as brokers , are organized into an overlay network. The issue\nthen becomes how to route noti\ufb01cations to the appropriate set of subscribers.\nFollowing Baldoni et al. [2009], we distinguish three different classes: (1) \ufb02ood-\ning, (2) selective routing, and (3) gossip-based dissemination. An extensive\nsurvey on combining peer-to-peer networks and publish-subscribe systems is\nprovided by Kermarrec and Trianta\ufb01llou [2013].\nA straightforward way to make sure that noti\ufb01cations reach their sub-\nscribers, is to deploy broadcasting. There are essentially two approaches. First,\nwe store each subscription at every broker, while publishing noti\ufb01cations only\na single broker. The latter will handle identifying the matching subscriptions\nand subsequently copy and forward the noti\ufb01cation. The alternative is to\nstore a subscription only at one broker while broadcasting noti\ufb01cations to all\nbrokers. In that case, matching is distributed across the brokers which may\nlead to a more balanced workload among the brokers.\nNote 6.6 (Example: TIB/Rendezvous)\nFlooding noti\ufb01cations is used in TIB/Rendezvous , of which the basic archi-\ntecture is shown in Figure 6.26 [TIBCO]. In this approach, a noti\ufb01cation is\na message tagged with a compound keyword describing its content, such as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n344 CHAPTER 6. COORDINATION\nserver is the canonical solution for implementing Linda tuple spaces or Java\nSpaces. Likewise, many publish-subscribe systems that run within a single\ndepartment or organization can be implemented through a central server.\nImportant in these cases, is that the matching function can be implemented\nef\ufb01ciently. In practice, this is often the case when dealing with topic-based\n\ufb01ltering: matching then resorts to checking for equality of attribute values.\nNote that a simple way to scale up a centralized implementation, is to de-\nterministically divide the work across multiple servers. A standard approach\nis to make use of two functions, as explained by Baldoni et al. [2009]:\n\u2022a function sub2node (S), which takes a subscription Sand maps it to a\nnonempty subset of servers\n\u2022a function not2node (N), which takes a noti\ufb01cation Nand maps it to a\nnonempty subset of servers.\nThe servers to which sub2node (S)is mapped are called the rendezvous nodes\nforS. Likewise, sub2node (N)are the rendezvous nodes for N. The only\nconstraint that needs to be satis\ufb01ed, is that for any subscription Sand matching\nnoti\ufb01cation N,sub2node (S)\\not2node (N)6=\u00c6. In other words, there must be\nat least one server that can handle the subscription when there is a matching\nnoti\ufb01cation. In practice, this constraint is satis\ufb01ed by topic-based publish-\nsubscribe systems by using a hashing function on the names of the topics.\nThe idea of having a central server can be extended by distributing the\nmatching across multiple servers and dividing the work. The servers, gener-\nally referred to as brokers , are organized into an overlay network. The issue\nthen becomes how to route noti\ufb01cations to the appropriate set of subscribers.\nFollowing Baldoni et al. [2009], we distinguish three different classes: (1) \ufb02ood-\ning, (2) selective routing, and (3) gossip-based dissemination. An extensive\nsurvey on combining peer-to-peer networks and publish-subscribe systems is\nprovided by Kermarrec and Trianta\ufb01llou [2013].\nA straightforward way to make sure that noti\ufb01cations reach their sub-\nscribers, is to deploy broadcasting. There are essentially two approaches. First,\nwe store each subscription at every broker, while publishing noti\ufb01cations only\na single broker. The latter will handle identifying the matching subscriptions\nand subsequently copy and forward the noti\ufb01cation. The alternative is to\nstore a subscription only at one broker while broadcasting noti\ufb01cations to all\nbrokers. In that case, matching is distributed across the brokers which may\nlead to a more balanced workload among the brokers.\nNote 6.6 (Example: TIB/Rendezvous)\nFlooding noti\ufb01cations is used in TIB/Rendezvous , of which the basic archi-\ntecture is shown in Figure 6.26 [TIBCO]. In this approach, a noti\ufb01cation is\na message tagged with a compound keyword describing its content, such as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.6. DISTRIBUTED EVENT MATCHING 345\nnews .comp .os.books . A subscriber provides (parts of) a keyword, or indicating the\nmessages it wants to receive, such as news .comp .\u0003.books . These keywords are\nsaid to indicate the subject of a message.\nFigure 6.26: The principle of a publish/subscribe system as implemented\nin TIB/Rendezvous.\nFundamental to its implementation is the use of broadcasting common in\nlocal-area networks, although it also uses more ef\ufb01cient communication facilities\nwhen possible. For example, if it is known exactly where a subscriber resides,\npoint-to-point messages will generally be used. Each host on such a network will\nrun a rendezvous daemon , which takes care that messages are sent and delivered\naccording to their subject. Whenever a message is published, it is multicast to\neach host on the network running a rendezvous daemon. Typically, multicasting\nis implemented using the facilities offered by the underlying network, such as\nIP-multicasting or hardware broadcasting.\nProcesses that subscribe to a subject pass their subscription to their local\ndaemon. The daemon constructs a table of ( process, subject ), entries and whenever\na message on subject Sarrives, the daemon simply checks in its table for local\nsubscribers, and forwards the message to each one. If there are no subscribers for\nS, the message is discarded immediately.\nWhen using multicasting as is done in TIB/Rendezvous, there is no reason\nwhy subscriptions cannot be elaborate and be more than string comparison as\nis currently the case. The crucial observation here is that because messages are\nforwarded to every node anyway, the potentially complex matching of published\ndata against subscriptions can be done entirely locally without further network\ncommunication. needed.\nWhen systems become big, \ufb02ooding is not the best way to go, if even\npossible. Instead, routing noti\ufb01cations across the overlay network of brokers\nmay be necessary. This is typically the way to go in information-centric\nnetworking , which makes use of name-based routing [Ahlgren et al., 2012;\nXylomenos et al., 2014]. Name-based routing is a special case of selective\nnoti\ufb01cation routing. Crucial in this setup is that brokers can take routing\ndecisions by considering the content of a noti\ufb01cation message. More precisely,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.6. DISTRIBUTED EVENT MATCHING 345\nnews .comp .os.books . A subscriber provides (parts of) a keyword, or indicating the\nmessages it wants to receive, such as news .comp .\u0003.books . These keywords are\nsaid to indicate the subject of a message.\nFigure 6.26: The principle of a publish/subscribe system as implemented\nin TIB/Rendezvous.\nFundamental to its implementation is the use of broadcasting common in\nlocal-area networks, although it also uses more ef\ufb01cient communication facilities\nwhen possible. For example, if it is known exactly where a subscriber resides,\npoint-to-point messages will generally be used. Each host on such a network will\nrun a rendezvous daemon , which takes care that messages are sent and delivered\naccording to their subject. Whenever a message is published, it is multicast to\neach host on the network running a rendezvous daemon. Typically, multicasting\nis implemented using the facilities offered by the underlying network, such as\nIP-multicasting or hardware broadcasting.\nProcesses that subscribe to a subject pass their subscription to their local\ndaemon. The daemon constructs a table of ( process, subject ), entries and whenever\na message on subject Sarrives, the daemon simply checks in its table for local\nsubscribers, and forwards the message to each one. If there are no subscribers for\nS, the message is discarded immediately.\nWhen using multicasting as is done in TIB/Rendezvous, there is no reason\nwhy subscriptions cannot be elaborate and be more than string comparison as\nis currently the case. The crucial observation here is that because messages are\nforwarded to every node anyway, the potentially complex matching of published\ndata against subscriptions can be done entirely locally without further network\ncommunication. needed.\nWhen systems become big, \ufb02ooding is not the best way to go, if even\npossible. Instead, routing noti\ufb01cations across the overlay network of brokers\nmay be necessary. This is typically the way to go in information-centric\nnetworking , which makes use of name-based routing [Ahlgren et al., 2012;\nXylomenos et al., 2014]. Name-based routing is a special case of selective\nnoti\ufb01cation routing. Crucial in this setup is that brokers can take routing\ndecisions by considering the content of a noti\ufb01cation message. More precisely,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "346 CHAPTER 6. COORDINATION\nit is assumed that each noti\ufb01cation carries enough information that can be\nused to cut-off routes for which it is known that they do not lead to its\nsubscribers.\nA practical approach toward selective routing is proposed by Carzaniga\net al. [2004]. Consider a publish-subscribe system consisting of Nbrokers to\nwhich clients (i.e., applications) can send subscriptions and retrieve noti\ufb01ca-\ntions.\nCarzaniga et al. [2004] propose a two-layered routing scheme in which\nthe lowest layer consists of a shared broadcast tree connecting the Nbrokers.\nThere are various ways for setting up such a tree, ranging from network-\nlevel multicast support to application-level multicast trees as we discussed\nin Chapter 4. Here, we also assume that such a tree has been set up with\ntheNbrokers as end nodes, along with a collection of intermediate nodes\nforming routers. Note that the distinction between a server and a router is\nonly a logical one: a single machine may host both kinds of processes.\nFigure 6.27: Naive content-based routing.\nEvery broker broadcasts its subscriptions to all other brokers. As a result,\nevery broker will be able to compile a list of ( subject, destination ) pairs. Then,\nwhenever a process publishes a noti\ufb01cation N, its associated broker prepends\nthe destination brokers to that message. When the message reaches a router,\nthe latter can use the list to decide on the paths that the message should\nfollow, as shown in Figure 6.27.\nWe can re\ufb01ne the capabilities of routers for deciding where to forward\nnoti\ufb01cations to. To that end, each broker broadcasts its subscription across\nthe network so that routers can compose routing \ufb01lters . For example, assume\nthat node 3 in Figure 6.27 subscribes to noti\ufb01cations for which an attribute a\nlies in the range [0, 3], but that node 4 wants messages with a2[2, 5]. In this\ncase, router R2will create a routing \ufb01lter as a table with an entry for each of\nits outgoing links (in this case three: one to node 3, one to node 4, and one\ntoward router R1), as shown in Figure 6.28.\nMore interesting is what happens at router R1. In this example, the\nsubscriptions from nodes 3 and 4 dictate that any noti\ufb01cation with alying in\nthe interval [0, 3][[2, 5] = [ 0, 5]should be forwarded along the path to router\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n346 CHAPTER 6. COORDINATION\nit is assumed that each noti\ufb01cation carries enough information that can be\nused to cut-off routes for which it is known that they do not lead to its\nsubscribers.\nA practical approach toward selective routing is proposed by Carzaniga\net al. [2004]. Consider a publish-subscribe system consisting of Nbrokers to\nwhich clients (i.e., applications) can send subscriptions and retrieve noti\ufb01ca-\ntions.\nCarzaniga et al. [2004] propose a two-layered routing scheme in which\nthe lowest layer consists of a shared broadcast tree connecting the Nbrokers.\nThere are various ways for setting up such a tree, ranging from network-\nlevel multicast support to application-level multicast trees as we discussed\nin Chapter 4. Here, we also assume that such a tree has been set up with\ntheNbrokers as end nodes, along with a collection of intermediate nodes\nforming routers. Note that the distinction between a server and a router is\nonly a logical one: a single machine may host both kinds of processes.\nFigure 6.27: Naive content-based routing.\nEvery broker broadcasts its subscriptions to all other brokers. As a result,\nevery broker will be able to compile a list of ( subject, destination ) pairs. Then,\nwhenever a process publishes a noti\ufb01cation N, its associated broker prepends\nthe destination brokers to that message. When the message reaches a router,\nthe latter can use the list to decide on the paths that the message should\nfollow, as shown in Figure 6.27.\nWe can re\ufb01ne the capabilities of routers for deciding where to forward\nnoti\ufb01cations to. To that end, each broker broadcasts its subscription across\nthe network so that routers can compose routing \ufb01lters . For example, assume\nthat node 3 in Figure 6.27 subscribes to noti\ufb01cations for which an attribute a\nlies in the range [0, 3], but that node 4 wants messages with a2[2, 5]. In this\ncase, router R2will create a routing \ufb01lter as a table with an entry for each of\nits outgoing links (in this case three: one to node 3, one to node 4, and one\ntoward router R1), as shown in Figure 6.28.\nMore interesting is what happens at router R1. In this example, the\nsubscriptions from nodes 3 and 4 dictate that any noti\ufb01cation with alying in\nthe interval [0, 3][[2, 5] = [ 0, 5]should be forwarded along the path to router\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.6. DISTRIBUTED EVENT MATCHING 347\nInterface Filter\nTo node 3 a2[0, 3]\nTo node 4 a2[2, 5]\nToward router R1(unspeci\ufb01ed)\nFigure 6.28: A partially \ufb01lled routing table.\nR2, and this is precisely the information that R1will store in its table. It is\nnot dif\ufb01cult to imagine that more intricate subscription compositions can be\nsupported.\nThis simple example also illustrates that whenever a node leaves the\nsystem, or when it is no longer interested in speci\ufb01c noti\ufb01cations, it should\ncancel its subscription and essentially broadcast this information to all routers.\nThis cancellation, in turn, may lead to adjusting various routing \ufb01lters. Late\nadjustments will at worst lead to unnecessary traf\ufb01c as noti\ufb01cations may be\nforwarded along paths for which there are no longer subscribers. Nevertheless,\ntimely adjustments are needed to keep performance at an acceptable level.\nThe last type of distributed event matching is based on gossiping. The\nbasic idea is that subscribers interested in the same noti\ufb01cations form their\nown overlay network (which is constructed through gossiping), so that once\na noti\ufb01cation is published, it merely needs to be routed to the appropriate\noverlay. For the latter, a random walk can be deployed. This approach is\nfollowing in TERA [Baldoni et al., 2007]. As an alternative, in PolderCast [Setty\net al., 2012] a publisher \ufb01rst joins the overlay of subscribers before \ufb02ooding its\nnoti\ufb01cation. The subscriber overlay is built per topic and constitutes a ring\nwith shortcuts to facilitate ef\ufb01cient dissemination of a noti\ufb01cation.\nNote 6.7 (Advanced: Gossiping for content-based event matching)\nA more sophisticated approach toward combining gossiping and event matching\nis followed in Sub-2-Sub [Voulgaris et al., 2006]. Consider a publish-subscribe\nsystem in which data items can be described by means of Nattributes a1,. . .,aN\nwhose value can be directly mapped to a \ufb02oating-point number. Such values\ninclude, for example, \ufb02oats, integers, enumerations, booleans, and strings. A\nsubscription Stakes the form of a tuple of ( attribute, value/range ) pairs, such as\nS=ha1!3.0,a4![0.0, 0.5 )i\nIn this example, Sspeci\ufb01es that a1should be equal to 3.0, and a4should lie in the\ninterval [0.0, 0.5 ). Other attributes are allowed to take on any value. For clarity,\nassume that every node ienters only one subscription Si.\nNote that each subscription Siactually speci\ufb01es a subset Siin the N-\ndimensional space of \ufb02oating-point numbers. Such a subset is also called a\nhyperspace. For the system as a whole, noti\ufb01cations that fall in the union S=[Si\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.6. DISTRIBUTED EVENT MATCHING 347\nInterface Filter\nTo node 3 a2[0, 3]\nTo node 4 a2[2, 5]\nToward router R1(unspeci\ufb01ed)\nFigure 6.28: A partially \ufb01lled routing table.\nR2, and this is precisely the information that R1will store in its table. It is\nnot dif\ufb01cult to imagine that more intricate subscription compositions can be\nsupported.\nThis simple example also illustrates that whenever a node leaves the\nsystem, or when it is no longer interested in speci\ufb01c noti\ufb01cations, it should\ncancel its subscription and essentially broadcast this information to all routers.\nThis cancellation, in turn, may lead to adjusting various routing \ufb01lters. Late\nadjustments will at worst lead to unnecessary traf\ufb01c as noti\ufb01cations may be\nforwarded along paths for which there are no longer subscribers. Nevertheless,\ntimely adjustments are needed to keep performance at an acceptable level.\nThe last type of distributed event matching is based on gossiping. The\nbasic idea is that subscribers interested in the same noti\ufb01cations form their\nown overlay network (which is constructed through gossiping), so that once\na noti\ufb01cation is published, it merely needs to be routed to the appropriate\noverlay. For the latter, a random walk can be deployed. This approach is\nfollowing in TERA [Baldoni et al., 2007]. As an alternative, in PolderCast [Setty\net al., 2012] a publisher \ufb01rst joins the overlay of subscribers before \ufb02ooding its\nnoti\ufb01cation. The subscriber overlay is built per topic and constitutes a ring\nwith shortcuts to facilitate ef\ufb01cient dissemination of a noti\ufb01cation.\nNote 6.7 (Advanced: Gossiping for content-based event matching)\nA more sophisticated approach toward combining gossiping and event matching\nis followed in Sub-2-Sub [Voulgaris et al., 2006]. Consider a publish-subscribe\nsystem in which data items can be described by means of Nattributes a1,. . .,aN\nwhose value can be directly mapped to a \ufb02oating-point number. Such values\ninclude, for example, \ufb02oats, integers, enumerations, booleans, and strings. A\nsubscription Stakes the form of a tuple of ( attribute, value/range ) pairs, such as\nS=ha1!3.0,a4![0.0, 0.5 )i\nIn this example, Sspeci\ufb01es that a1should be equal to 3.0, and a4should lie in the\ninterval [0.0, 0.5 ). Other attributes are allowed to take on any value. For clarity,\nassume that every node ienters only one subscription Si.\nNote that each subscription Siactually speci\ufb01es a subset Siin the N-\ndimensional space of \ufb02oating-point numbers. Such a subset is also called a\nhyperspace. For the system as a whole, noti\ufb01cations that fall in the union S=[Si\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "348 CHAPTER 6. COORDINATION\nof these hyperspaces are the only ones of interest. The whole idea is to auto-\nmatically partition Sinto Mdisjoint hyperspaces S1,. . .,SMsuch that each falls\ncompletely in one of the subscription hyperspaces Si, and together they cover all\nsubscriptions. More formally, we have that:\n(Sm\\Si6=\u00c6))(Sm\u0012Si)\nSub-2-Sub keeps Mminimal in the sense that there is no partitioning with fewer\nparts Sm. To this end, for each hyperspace Sm, it registers exactly those nodes i\nfor which Sm\u0012Si. In that case, when a noti\ufb01cation is published, the system need\nmerely \ufb01nd the Smto which the associated event belongs, from which point it\ncan forward the noti\ufb01cation to the appropriate nodes.\nTo this end, nodes regularly exchange subscriptions through gossiping. If\ntwo nodes iand jnotice that their respective subscriptions intersect, that is,\nSij\u0011Si\\Sj6=\u00c6they will record this fact and keep references to each other.\nIf they discover a third node kwith Sijk\u0011Sij\\Sk6=\u00c6, the three of them\nwill connect to each other so that a noti\ufb01cation Nfrom Sijkcan be ef\ufb01ciently\ndisseminated. Note that if Sij\u0000Sijk6=\u00c6, nodes iandjwill maintain their mutual\nreferences, but now associate it strictly with Sij\u0000Sijk.\nIn essence, what we are seeking is a means to cluster nodes into Mdifferent\ngroups, such that nodes iand jbelong to the same group if and only if their\nsubscriptions SiandSjintersect. Moreover, nodes in the same group should be\norganized into an overlay network that will allow ef\ufb01cient dissemination of a data\nitem in the hyperspace associated with that group. This situation for a single\nattribute is sketched in Figure 6.29.\nFigure 6.29: Grouping nodes for supporting range queries in a gossip-\nbased publish-subscribe system.\nHere, we see a total of seven nodes in which the horizontal line for node i\nindicates its range of interest for the value of the single attribute. Also shown is the\ngrouping of nodes into disjoint ranges of interests for values of the attribute. For\nexample, nodes 3,4,7, and 10will be grouped together representing the interval\n[16.5, 21.0 ]. Any data item with a value in this range should be disseminated to\nonly these four nodes.\nTo construct these groups, the nodes are organized into a gossip-based un-\nstructured network. Each node maintains a list of references to other neighbors\n(i.e., a partial view ), which it periodically exchanges with one of its neighbors.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n348 CHAPTER 6. COORDINATION\nof these hyperspaces are the only ones of interest. The whole idea is to auto-\nmatically partition Sinto Mdisjoint hyperspaces S1,. . .,SMsuch that each falls\ncompletely in one of the subscription hyperspaces Si, and together they cover all\nsubscriptions. More formally, we have that:\n(Sm\\Si6=\u00c6))(Sm\u0012Si)\nSub-2-Sub keeps Mminimal in the sense that there is no partitioning with fewer\nparts Sm. To this end, for each hyperspace Sm, it registers exactly those nodes i\nfor which Sm\u0012Si. In that case, when a noti\ufb01cation is published, the system need\nmerely \ufb01nd the Smto which the associated event belongs, from which point it\ncan forward the noti\ufb01cation to the appropriate nodes.\nTo this end, nodes regularly exchange subscriptions through gossiping. If\ntwo nodes iand jnotice that their respective subscriptions intersect, that is,\nSij\u0011Si\\Sj6=\u00c6they will record this fact and keep references to each other.\nIf they discover a third node kwith Sijk\u0011Sij\\Sk6=\u00c6, the three of them\nwill connect to each other so that a noti\ufb01cation Nfrom Sijkcan be ef\ufb01ciently\ndisseminated. Note that if Sij\u0000Sijk6=\u00c6, nodes iandjwill maintain their mutual\nreferences, but now associate it strictly with Sij\u0000Sijk.\nIn essence, what we are seeking is a means to cluster nodes into Mdifferent\ngroups, such that nodes iand jbelong to the same group if and only if their\nsubscriptions SiandSjintersect. Moreover, nodes in the same group should be\norganized into an overlay network that will allow ef\ufb01cient dissemination of a data\nitem in the hyperspace associated with that group. This situation for a single\nattribute is sketched in Figure 6.29.\nFigure 6.29: Grouping nodes for supporting range queries in a gossip-\nbased publish-subscribe system.\nHere, we see a total of seven nodes in which the horizontal line for node i\nindicates its range of interest for the value of the single attribute. Also shown is the\ngrouping of nodes into disjoint ranges of interests for values of the attribute. For\nexample, nodes 3,4,7, and 10will be grouped together representing the interval\n[16.5, 21.0 ]. Any data item with a value in this range should be disseminated to\nonly these four nodes.\nTo construct these groups, the nodes are organized into a gossip-based un-\nstructured network. Each node maintains a list of references to other neighbors\n(i.e., a partial view ), which it periodically exchanges with one of its neighbors.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.7. GOSSIP-BASED COORDINATION 349\nSuch an exchange will allow a node to learn about random other nodes in the sys-\ntem. Every node keeps track of the nodes it discovers with overlapping interests\n(i.e., with an intersecting subscription).\nAt a certain moment, every node iwill generally have references to other\nnodes with overlapping interests. As part of exchanging information with a node\nj, node iorders these nodes by their identi\ufb01ers and selects the one with the lowest\nidenti\ufb01er i1>j, such that its subscription overlaps with that of node j, that is,\nSj,i1\u0011Si1\\Sj6=\u00c6.\nThe next one to be selected is i2>i1such that its subscription also overlaps\nwith that of j, but only if it contains elements not yet covered by node i1. In\nother words, we should have that Sj,i1,i2\u0011(Si2\u0000Sj,i1)\\Sj6=\u00c6. This process is\nrepeated until all nodes that have an overlapping interest with node ihave been\ninspected, leading to an ordered list i1<i2<\u0001\u0001\u0001<in. Note that a node ikis in\nthis list because it covers a region Rof common interest to node iand jnot yet\njointly covered by nodes with a lower identi\ufb01er than ik. In effect, node ikis the\n\ufb01rst node that node jshould forward a noti\ufb01cation to that falls in this unique\nregion R. This procedure can be expanded to let node iconstruct a bidirectional\nring. Such a ring is also shown in Figure 6.29.\nWhenever a noti\ufb01cation Nis published, it is disseminated as quickly as\npossible to anynode that is interested in it. As it turns out, with the information\navailable at every node \ufb01nding a node iinterested in Nis simple. From there\non, node ineed simply forward Nalong the ring of subscribers for the particular\nrange that Nfalls into. To speed up dissemination, shortcuts are maintained for\neach ring as well.\n6.7 Gossip-based coordination\nAs a \ufb01nal topic in coordination, we take a look at a few important examples\nin which gossiping is deployed. In the following, we look at aggregation,\nlarge-scale peer sampling, and overlay construction, respectively.\nAggregation\nLet us take a look at some interesting applications of epidemic protocols. We\nalready mentioned spreading updates, which is perhaps the most widely-\ndeployed application. In the same light, gossiping can be used to discover\nnodes that have a few outgoing wide-area links, to subsequently apply direc-\ntional gossiping.\nAnother interesting application area is simply collecting, or actually aggre-\ngating information [Jelasity et al., 2005]. Consider the following information\nexchange. Every node Piinitially chooses an arbitrary number, say vi. When\nnode Picontacts node Pj, they each update their value as:\nvi,vj (vi+vj)/2\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.7. GOSSIP-BASED COORDINATION 349\nSuch an exchange will allow a node to learn about random other nodes in the sys-\ntem. Every node keeps track of the nodes it discovers with overlapping interests\n(i.e., with an intersecting subscription).\nAt a certain moment, every node iwill generally have references to other\nnodes with overlapping interests. As part of exchanging information with a node\nj, node iorders these nodes by their identi\ufb01ers and selects the one with the lowest\nidenti\ufb01er i1>j, such that its subscription overlaps with that of node j, that is,\nSj,i1\u0011Si1\\Sj6=\u00c6.\nThe next one to be selected is i2>i1such that its subscription also overlaps\nwith that of j, but only if it contains elements not yet covered by node i1. In\nother words, we should have that Sj,i1,i2\u0011(Si2\u0000Sj,i1)\\Sj6=\u00c6. This process is\nrepeated until all nodes that have an overlapping interest with node ihave been\ninspected, leading to an ordered list i1<i2<\u0001\u0001\u0001<in. Note that a node ikis in\nthis list because it covers a region Rof common interest to node iand jnot yet\njointly covered by nodes with a lower identi\ufb01er than ik. In effect, node ikis the\n\ufb01rst node that node jshould forward a noti\ufb01cation to that falls in this unique\nregion R. This procedure can be expanded to let node iconstruct a bidirectional\nring. Such a ring is also shown in Figure 6.29.\nWhenever a noti\ufb01cation Nis published, it is disseminated as quickly as\npossible to anynode that is interested in it. As it turns out, with the information\navailable at every node \ufb01nding a node iinterested in Nis simple. From there\non, node ineed simply forward Nalong the ring of subscribers for the particular\nrange that Nfalls into. To speed up dissemination, shortcuts are maintained for\neach ring as well.\n6.7 Gossip-based coordination\nAs a \ufb01nal topic in coordination, we take a look at a few important examples\nin which gossiping is deployed. In the following, we look at aggregation,\nlarge-scale peer sampling, and overlay construction, respectively.\nAggregation\nLet us take a look at some interesting applications of epidemic protocols. We\nalready mentioned spreading updates, which is perhaps the most widely-\ndeployed application. In the same light, gossiping can be used to discover\nnodes that have a few outgoing wide-area links, to subsequently apply direc-\ntional gossiping.\nAnother interesting application area is simply collecting, or actually aggre-\ngating information [Jelasity et al., 2005]. Consider the following information\nexchange. Every node Piinitially chooses an arbitrary number, say vi. When\nnode Picontacts node Pj, they each update their value as:\nvi,vj (vi+vj)/2\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "350 CHAPTER 6. COORDINATION\nObviously, after this exchange, both Piand Pjwill have the same value. In\nfact, it is not dif\ufb01cult to see that eventually all nodes will have the same\nvalue, namely the average of all initial values. Propagation speed is again\nexponential.\nWhat use does computing the average have? Consider the situation that\nall nodes Pihave set vito zero, except for P1who has set v1to 1:\nvi (\n1 if i=1\n0otherwise\nIf there are Nnodes, then eventually each node will compute the average,\nwhich is 1/N. As a consequence, every node Pican estimate the size of the\nsystem as being 1/ vi.\nComputing the average may prove to be dif\ufb01cult when nodes regularly join\nand leave the system. One practical solution to this problem is to introduce\nepochs. Assuming that node P1is stable, it simply starts a new epoch now\nand then. When node Pisees a new epoch for the \ufb01rst time, it resets its own\nvariable vito zero and starts computing the average again.\nOf course, other results can also be computed. For example, instead of\nhaving a \ufb01xed node such as P1start the computation of the average, we\ncan easily pick a random node as follows. Every node Piinitially sets vi\nto a random number from the same interval, say (0, 1], and also stores it\npermanently as mi. Upon an exchange between nodes Piand Pj, each change\ntheir value to:\nvi,vj maxfvi,vjg\nEach node Pifor which mi<viwill lose the competition for being the initiator\nin starting the computation of the average. In the end, there will be a single\nwinner. Of course, although it is easy to conclude that a node has lost, it is\nmuch more dif\ufb01cult to decide that it has won, as it remains uncertain whether\nall results have come in. The solution to this problem is to be optimistic: a\nnode always assumes it is the winner until proven otherwise. At that point,\nit simply resets the variable it is using for computing the average to zero.\nNote that by now, several different computations (in our example computing\na maximum and computing an average) may be executing simultaneously.\nA peer-sampling service\nAn important aspect in epidemic protocols is the ability of a node Pto choose\nanother node Qat random from all available nodes in the network. When\ngiving the matter some thought, we may actually have a serious problem:\nif the network consists of thousands of nodes, how can Pever pick one of\nthese nodes at random without having a complete overview of the network?\nFor smaller networks, one could often resort to a central service that had\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n350 CHAPTER 6. COORDINATION\nObviously, after this exchange, both Piand Pjwill have the same value. In\nfact, it is not dif\ufb01cult to see that eventually all nodes will have the same\nvalue, namely the average of all initial values. Propagation speed is again\nexponential.\nWhat use does computing the average have? Consider the situation that\nall nodes Pihave set vito zero, except for P1who has set v1to 1:\nvi (\n1 if i=1\n0otherwise\nIf there are Nnodes, then eventually each node will compute the average,\nwhich is 1/N. As a consequence, every node Pican estimate the size of the\nsystem as being 1/ vi.\nComputing the average may prove to be dif\ufb01cult when nodes regularly join\nand leave the system. One practical solution to this problem is to introduce\nepochs. Assuming that node P1is stable, it simply starts a new epoch now\nand then. When node Pisees a new epoch for the \ufb01rst time, it resets its own\nvariable vito zero and starts computing the average again.\nOf course, other results can also be computed. For example, instead of\nhaving a \ufb01xed node such as P1start the computation of the average, we\ncan easily pick a random node as follows. Every node Piinitially sets vi\nto a random number from the same interval, say (0, 1], and also stores it\npermanently as mi. Upon an exchange between nodes Piand Pj, each change\ntheir value to:\nvi,vj maxfvi,vjg\nEach node Pifor which mi<viwill lose the competition for being the initiator\nin starting the computation of the average. In the end, there will be a single\nwinner. Of course, although it is easy to conclude that a node has lost, it is\nmuch more dif\ufb01cult to decide that it has won, as it remains uncertain whether\nall results have come in. The solution to this problem is to be optimistic: a\nnode always assumes it is the winner until proven otherwise. At that point,\nit simply resets the variable it is using for computing the average to zero.\nNote that by now, several different computations (in our example computing\na maximum and computing an average) may be executing simultaneously.\nA peer-sampling service\nAn important aspect in epidemic protocols is the ability of a node Pto choose\nanother node Qat random from all available nodes in the network. When\ngiving the matter some thought, we may actually have a serious problem:\nif the network consists of thousands of nodes, how can Pever pick one of\nthese nodes at random without having a complete overview of the network?\nFor smaller networks, one could often resort to a central service that had\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.7. GOSSIP-BASED COORDINATION 351\nregistered every participating node. Obviously, this approach can never scale\nto large networks.\nA solution is to construct a fully decentralized peer-sampling service , or\nPSS for short. As it turns out, and somewhat counter-intuitive, a PSS can be\nbuilt using an epidemic protocol. As explored by Jelasity et al. [2007], each\nnode maintains a list of cneighbors, where, ideally, each of these neighbors\nrepresents a randomly chosen livenode from the current set of nodes. This\nlist of neighbors is also referred to as a partial view . There are many ways\nto construct such a partial view. In their solution, it is assumed that nodes\nregularly exchange entries from their partial view. Each entry identi\ufb01es\nanother node in the network, and has an associated age that indicates how old\nthe reference to that node is. Two threads are used, as shown in Figure 6.30.\n1selectPeer(&Q);\n2selectToSend(&bufs);\n3sendTo(Q, bufs);\n4\n5receiveFrom(Q, &bufr);\n6selectToKeep(p_view, bufr);\u0000!\n \u00001\n2\n3receiveFromAny(&P, &bufr);\n4selectToSend(&bufs);\n5sendTo(P, bufs);\n6selectToKeep(p_view, bufr);\n(a) (b)\nFigure 6.30: Communication between the (a) active and (b) passive thread in\na peer-sampling service.\nThe different selection operations are speci\ufb01ed as follows:\n\u2022selectPeer : Randomly select a neighbor from the local partial view\n\u2022selectToSend : Select some other entries from the partial view, and add\nto the list intended for the selected neighbor.\n\u2022selectToKeep : Add received entries to partial view, remove repeated\nitems, and shrink view to citems.\nThe active thread takes the initiative to communicate with another node. It\nselects that node from its current partial view. It continues by constructing a\nlist containing c/2+1entries, including an entry identifying itself. The other\nentries are taken from the current partial view. After sending the list to the\nselected neighbor, it waits for a response.\nThat neighbor, in the meantime, will also have constructed a list by means\nof the passive thread shown in Figure 6.30(b) whose activities strongly resem-\nble that of the active thread.\nThe crucial point is the construction of a new partial view. This view, for\ncontacting as well as for the contacted peer, will contain exactly centries, part\nof which will come from the received list. In essence, there are two ways to\nconstruct the new view. First, the two nodes may decide to discard the entries\nthat they had sent to each other. Effectively, this means that they will swap\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.7. GOSSIP-BASED COORDINATION 351\nregistered every participating node. Obviously, this approach can never scale\nto large networks.\nA solution is to construct a fully decentralized peer-sampling service , or\nPSS for short. As it turns out, and somewhat counter-intuitive, a PSS can be\nbuilt using an epidemic protocol. As explored by Jelasity et al. [2007], each\nnode maintains a list of cneighbors, where, ideally, each of these neighbors\nrepresents a randomly chosen livenode from the current set of nodes. This\nlist of neighbors is also referred to as a partial view . There are many ways\nto construct such a partial view. In their solution, it is assumed that nodes\nregularly exchange entries from their partial view. Each entry identi\ufb01es\nanother node in the network, and has an associated age that indicates how old\nthe reference to that node is. Two threads are used, as shown in Figure 6.30.\n1selectPeer(&Q);\n2selectToSend(&bufs);\n3sendTo(Q, bufs);\n4\n5receiveFrom(Q, &bufr);\n6selectToKeep(p_view, bufr);\u0000!\n \u00001\n2\n3receiveFromAny(&P, &bufr);\n4selectToSend(&bufs);\n5sendTo(P, bufs);\n6selectToKeep(p_view, bufr);\n(a) (b)\nFigure 6.30: Communication between the (a) active and (b) passive thread in\na peer-sampling service.\nThe different selection operations are speci\ufb01ed as follows:\n\u2022selectPeer : Randomly select a neighbor from the local partial view\n\u2022selectToSend : Select some other entries from the partial view, and add\nto the list intended for the selected neighbor.\n\u2022selectToKeep : Add received entries to partial view, remove repeated\nitems, and shrink view to citems.\nThe active thread takes the initiative to communicate with another node. It\nselects that node from its current partial view. It continues by constructing a\nlist containing c/2+1entries, including an entry identifying itself. The other\nentries are taken from the current partial view. After sending the list to the\nselected neighbor, it waits for a response.\nThat neighbor, in the meantime, will also have constructed a list by means\nof the passive thread shown in Figure 6.30(b) whose activities strongly resem-\nble that of the active thread.\nThe crucial point is the construction of a new partial view. This view, for\ncontacting as well as for the contacted peer, will contain exactly centries, part\nof which will come from the received list. In essence, there are two ways to\nconstruct the new view. First, the two nodes may decide to discard the entries\nthat they had sent to each other. Effectively, this means that they will swap\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "352 CHAPTER 6. COORDINATION\npart of their original views. The second approach is to discard as many old\nentries as possible (meaning, in practice, that after every gossiping round, the\nage of each entry in every partial view is incremented by one).\nAs it turns out, as long as peers regularly run the exchange algorithm\njust described, selecting a random peer from a thus dynamically changing\npartial view, is indistinguishable from randomly selecting a peer from the\nentire network. Of course, selecting a peer from a partial view should occur\nat approximately the same frequency as the refreshing of partial views. We\nhave thus constructed a fully decentralized gossip-based peer-sampling ser-\nvice. A simple, and often-used implementation of a peer-sampling service is\nCyclon [Voulgaris et al., 2005].\nGossip-based overlay construction\nAlthough it would seem that structured and unstructured peer-to-peer systems\nform strict independent classes, this need actually not be case (see also Castro\net al. [2005]). One key observation is that by carefully exchanging and selecting\nentries from partial views, it is possible to construct and maintain speci\ufb01c\ntopologies of overlay networks. This topology management is achieved by\nadopting a two-layered approach, as shown in Figure 6.31.\nFigure 6.31: A two-layered approach for constructing and maintaining speci\ufb01c\noverlay topologies using techniques from unstructured peer-to-peer systems.\nThe lowest layer constitutes an unstructured peer-to-peer system in which\nnodes periodically exchange entries of their partial views with the aim to\nprovide a peer-sampling service. Accuracy in this case refers to the fact that\nthe partial view should be \ufb01lled with entries referring to randomly selected\nlivenodes.\nThe lowest layer passes its partial view to the higher layer, where an\nadditional selection of entries takes place. This then leads to a second list\nof neighbors corresponding to the desired topology. Jelasity and Kermarrec\n[2006] propose to use a ranking function by which nodes are ordered according\nto some criterion relative to a given node. A simple ranking function is to\norder a set of nodes by increasing distance from a given node P. In that case,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n352 CHAPTER 6. COORDINATION\npart of their original views. The second approach is to discard as many old\nentries as possible (meaning, in practice, that after every gossiping round, the\nage of each entry in every partial view is incremented by one).\nAs it turns out, as long as peers regularly run the exchange algorithm\njust described, selecting a random peer from a thus dynamically changing\npartial view, is indistinguishable from randomly selecting a peer from the\nentire network. Of course, selecting a peer from a partial view should occur\nat approximately the same frequency as the refreshing of partial views. We\nhave thus constructed a fully decentralized gossip-based peer-sampling ser-\nvice. A simple, and often-used implementation of a peer-sampling service is\nCyclon [Voulgaris et al., 2005].\nGossip-based overlay construction\nAlthough it would seem that structured and unstructured peer-to-peer systems\nform strict independent classes, this need actually not be case (see also Castro\net al. [2005]). One key observation is that by carefully exchanging and selecting\nentries from partial views, it is possible to construct and maintain speci\ufb01c\ntopologies of overlay networks. This topology management is achieved by\nadopting a two-layered approach, as shown in Figure 6.31.\nFigure 6.31: A two-layered approach for constructing and maintaining speci\ufb01c\noverlay topologies using techniques from unstructured peer-to-peer systems.\nThe lowest layer constitutes an unstructured peer-to-peer system in which\nnodes periodically exchange entries of their partial views with the aim to\nprovide a peer-sampling service. Accuracy in this case refers to the fact that\nthe partial view should be \ufb01lled with entries referring to randomly selected\nlivenodes.\nThe lowest layer passes its partial view to the higher layer, where an\nadditional selection of entries takes place. This then leads to a second list\nof neighbors corresponding to the desired topology. Jelasity and Kermarrec\n[2006] propose to use a ranking function by which nodes are ordered according\nto some criterion relative to a given node. A simple ranking function is to\norder a set of nodes by increasing distance from a given node P. In that case,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "6.8. SUMMARY 353\nnode Pwill gradually build up a list of its nearest neighbors, provided the\nlowest layer continues to pass randomly selected nodes.\nAs an illustration, consider a logical grid of size N\u0002Nwith a node placed\non each point of the grid. Every node is required to maintain a list of c\nnearest neighbors, where the distance between a node at (a1,a2)and(b1,b2)\nis de\ufb01ned as d1+d2, with di=min(N\u0000jai\u0000bij,jai\u0000bij). If the lowest layer\nperiodically executes the protocol as outlined in Figure 6.30, the topology that\nwill evolve is a torus, shown in Figure 6.31.\nFigure 6.32: Generating a speci\ufb01c overlay network using a two-layered un-\nstructured peer-to-peer system (adapted with permission from [Jelasity and\nBabaoglu, 2006]).\n6.8 Summary\nStrongly related to communication between processes is the issue of how\nprocesses in distributed systems synchronize. Synchronization is all about\ndoing the right thing at the right time. A problem in distributed systems, and\ncomputer networks in general, is that there is no notion of a globally shared\nclock. In other words, processes on different machines have their own idea of\nwhat time it is.\nThere are various way to synchronize clocks in a distributed system, but\nall methods are essentially based on exchanging clock values, while taking\ninto account the time it takes to send and receive messages. Variations in\ncommunication delays and the way those variations are dealt with, largely\ndetermine the accuracy of clock synchronization algorithms.\nIn many cases, knowing the absolute time is not necessary. What counts is\nthat related events at different processes happen in the correct order. Lamport\nshowed that by introducing a notion of logical clocks, it is possible for a\ncollection of processes to reach global agreement on the correct ordering of\nevents. In essence, each event e, such as sending or receiving a message,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n6.8. SUMMARY 353\nnode Pwill gradually build up a list of its nearest neighbors, provided the\nlowest layer continues to pass randomly selected nodes.\nAs an illustration, consider a logical grid of size N\u0002Nwith a node placed\non each point of the grid. Every node is required to maintain a list of c\nnearest neighbors, where the distance between a node at (a1,a2)and(b1,b2)\nis de\ufb01ned as d1+d2, with di=min(N\u0000jai\u0000bij,jai\u0000bij). If the lowest layer\nperiodically executes the protocol as outlined in Figure 6.30, the topology that\nwill evolve is a torus, shown in Figure 6.31.\nFigure 6.32: Generating a speci\ufb01c overlay network using a two-layered un-\nstructured peer-to-peer system (adapted with permission from [Jelasity and\nBabaoglu, 2006]).\n6.8 Summary\nStrongly related to communication between processes is the issue of how\nprocesses in distributed systems synchronize. Synchronization is all about\ndoing the right thing at the right time. A problem in distributed systems, and\ncomputer networks in general, is that there is no notion of a globally shared\nclock. In other words, processes on different machines have their own idea of\nwhat time it is.\nThere are various way to synchronize clocks in a distributed system, but\nall methods are essentially based on exchanging clock values, while taking\ninto account the time it takes to send and receive messages. Variations in\ncommunication delays and the way those variations are dealt with, largely\ndetermine the accuracy of clock synchronization algorithms.\nIn many cases, knowing the absolute time is not necessary. What counts is\nthat related events at different processes happen in the correct order. Lamport\nshowed that by introducing a notion of logical clocks, it is possible for a\ncollection of processes to reach global agreement on the correct ordering of\nevents. In essence, each event e, such as sending or receiving a message,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "354 CHAPTER 6. COORDINATION\nis assigned a globally unique logical timestamp C(e)such that when event\nahappened before b,C(a)<C(b). Lamport timestamps can be extended\nto vector timestamps: if C(a)<C(b), we even know that event acausally\npreceded b.\nAn important class of synchronization algorithms is that of distributed\nmutual exclusion. These algorithms ensure that in a distributed collection\nof processes, at most one process at a time has access to a shared resource.\nDistributed mutual exclusion can easily be achieved if we make use of a\ncoordinator that keeps track of whose turn it is. Fully distributed algorithms\nalso exist, but have the drawback that they are generally more susceptible to\ncommunication and process failures.\nSynchronization between processes often requires that one process acts as\na coordinator. In those cases where the coordinator is not \ufb01xed, it is necessary\nthat processes in a distributed computation decide on who is going to be\nthat coordinator. Such a decision is taken by means of election algorithms.\nElection algorithms are primarily used in cases where the coordinator can\ncrash. However, they can also be applied for the selection of superpeers in\npeer-to-peer systems.\nRelated to these synchronization problems is positioning nodes in a ge-\nometric overlay. The basic idea is to assign each node coordinates from an\nm-dimensional space such that the geometric distance can be used as an\naccurate measure for the latency between two nodes. The method of assigning\ncoordinates strongly resembles the one applied in determining the location\nand time in GPS.\nParticularly challenging when it comes to coordination is distributed event\nmatching, which sits at the core of publish-subscribe systems. Relatively\nsimple is the case when we have a central implementations where matching\nsubscriptions against noti\ufb01cations can be done by essentially doing one-to-one\ncomparisons. However, as soon as we aim at distributing the load, we are\nfaced with the problem on deciding beforehand which node is responsible for\nwhich part of the subscriptions, without knowing what kind of noti\ufb01cations\nto expect. This is particularly problematic for content-based matching, which\nin the end, requires advanced \ufb01ltering techniques to route noti\ufb01cations to the\nproper subscribers.\nFinally, the most important aspect in gossip-based coordination is being\nable to select another peer randomly from an entire overlay. As it turns out,\nwe can implement such a peer-sampling service using gossiping, by ensuring\nthat the partial view is refreshed regularly and in a random way. Combining\npeer sampling with a selective replacement of entries in a partial view allows\nus to ef\ufb01ciently construct structured overlay networks.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n354 CHAPTER 6. COORDINATION\nis assigned a globally unique logical timestamp C(e)such that when event\nahappened before b,C(a)<C(b). Lamport timestamps can be extended\nto vector timestamps: if C(a)<C(b), we even know that event acausally\npreceded b.\nAn important class of synchronization algorithms is that of distributed\nmutual exclusion. These algorithms ensure that in a distributed collection\nof processes, at most one process at a time has access to a shared resource.\nDistributed mutual exclusion can easily be achieved if we make use of a\ncoordinator that keeps track of whose turn it is. Fully distributed algorithms\nalso exist, but have the drawback that they are generally more susceptible to\ncommunication and process failures.\nSynchronization between processes often requires that one process acts as\na coordinator. In those cases where the coordinator is not \ufb01xed, it is necessary\nthat processes in a distributed computation decide on who is going to be\nthat coordinator. Such a decision is taken by means of election algorithms.\nElection algorithms are primarily used in cases where the coordinator can\ncrash. However, they can also be applied for the selection of superpeers in\npeer-to-peer systems.\nRelated to these synchronization problems is positioning nodes in a ge-\nometric overlay. The basic idea is to assign each node coordinates from an\nm-dimensional space such that the geometric distance can be used as an\naccurate measure for the latency between two nodes. The method of assigning\ncoordinates strongly resembles the one applied in determining the location\nand time in GPS.\nParticularly challenging when it comes to coordination is distributed event\nmatching, which sits at the core of publish-subscribe systems. Relatively\nsimple is the case when we have a central implementations where matching\nsubscriptions against noti\ufb01cations can be done by essentially doing one-to-one\ncomparisons. However, as soon as we aim at distributing the load, we are\nfaced with the problem on deciding beforehand which node is responsible for\nwhich part of the subscriptions, without knowing what kind of noti\ufb01cations\nto expect. This is particularly problematic for content-based matching, which\nin the end, requires advanced \ufb01ltering techniques to route noti\ufb01cations to the\nproper subscribers.\nFinally, the most important aspect in gossip-based coordination is being\nable to select another peer randomly from an entire overlay. As it turns out,\nwe can implement such a peer-sampling service using gossiping, by ensuring\nthat the partial view is refreshed regularly and in a random way. Combining\npeer sampling with a selective replacement of entries in a partial view allows\nus to ef\ufb01ciently construct structured overlay networks.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 7\nConsistency and replication\nAn important issue in distributed systems is the replication of data. Data\nare generally replicated to enhance reliability or improve performance. One\nof the major problems is keeping replicas consistent. Informally, this means\nthat when one copy is updated we need to ensure that the other copies are\nupdated as well; otherwise the replicas will no longer be the same. In this\nchapter, we take a detailed look at what consistency of replicated data actually\nmeans and the various ways that consistency can be achieved.\nWe start with a general introduction discussing why replication is useful\nand how it relates to scalability. We then continue by focusing on what consis-\ntency actually means. An important class of what are known as consistency\nmodels assumes that multiple processes simultaneously access shared data.\nConsistency for these situations can be formulated with respect to what pro-\ncesses can expect when reading and updating the shared data, knowing that\nothers are accessing that data as well.\nConsistency models for shared data are often hard to implement ef\ufb01ciently\nin large-scale distributed systems. Moreover, in many cases simpler models\ncan be used, which are also often easier to implement. One speci\ufb01c class is\nformed by client-centric consistency models, which concentrate on consis-\ntency from the perspective of a single (possibly mobile) client. Client-centric\nconsistency models are discussed in a separate section.\nConsistency is only half of the story. We also need to consider how\nconsistency is actually implemented. There are essentially two, more or\nless independent, issues we need to consider. First of all, we start with\nconcentrating on managing replicas, which takes into account not only the\nplacement of replica servers, but also how content is distributed to these\nservers.\nThe second issue is how replicas are kept consistent. In most cases,\napplications require a strong form of consistency. Informally, this means that\nupdates are to be propagated more or less immediately between replicas.\nThere are various alternatives for implementing strong consistency, which are\n355\nChapter 7\nConsistency and replication\nAn important issue in distributed systems is the replication of data. Data\nare generally replicated to enhance reliability or improve performance. One\nof the major problems is keeping replicas consistent. Informally, this means\nthat when one copy is updated we need to ensure that the other copies are\nupdated as well; otherwise the replicas will no longer be the same. In this\nchapter, we take a detailed look at what consistency of replicated data actually\nmeans and the various ways that consistency can be achieved.\nWe start with a general introduction discussing why replication is useful\nand how it relates to scalability. We then continue by focusing on what consis-\ntency actually means. An important class of what are known as consistency\nmodels assumes that multiple processes simultaneously access shared data.\nConsistency for these situations can be formulated with respect to what pro-\ncesses can expect when reading and updating the shared data, knowing that\nothers are accessing that data as well.\nConsistency models for shared data are often hard to implement ef\ufb01ciently\nin large-scale distributed systems. Moreover, in many cases simpler models\ncan be used, which are also often easier to implement. One speci\ufb01c class is\nformed by client-centric consistency models, which concentrate on consis-\ntency from the perspective of a single (possibly mobile) client. Client-centric\nconsistency models are discussed in a separate section.\nConsistency is only half of the story. We also need to consider how\nconsistency is actually implemented. There are essentially two, more or\nless independent, issues we need to consider. First of all, we start with\nconcentrating on managing replicas, which takes into account not only the\nplacement of replica servers, but also how content is distributed to these\nservers.\nThe second issue is how replicas are kept consistent. In most cases,\napplications require a strong form of consistency. Informally, this means that\nupdates are to be propagated more or less immediately between replicas.\nThere are various alternatives for implementing strong consistency, which are\n355", "356 CHAPTER 7. CONSISTENCY AND REPLICATION\ndiscussed in a separate section. Also, attention is paid to caching protocols,\nwhich form a special case of consistency protocols.\nBeing arguably the largest distributed system, we pay separate attention\nto caching and replication in Web-based systems, notably looking at content\ndelivery networks as well as edge-server caching techniques.\n7.1 Introduction\nIn this section, we start with discussing the important reasons for wanting to\nreplicate data in the \ufb01rst place. We concentrate on replication as a technique\nfor achieving scalability, and motivate why reasoning about consistency is so\nimportant.\nReasons for replication\nThere are two primary reasons for replicating data. First, data are replicated\nto increase the reliability of a system. If a \ufb01le system has been replicated\nit may be possible to continue working after one replica crashes by simply\nswitching to one of the other replicas. Also, by maintaining multiple copies,\nit becomes possible to provide better protection against corrupted data. For\nexample, imagine there are three copies of a \ufb01le and every read and write\noperation is performed on each copy. We can safeguard ourselves against a\nsingle, failing write operation, by considering the value that is returned by at\nleast two copies as being the correct one.\nThe other reason for replicating data is performance. Replication for\nperformance is important when a distributed system needs to scale in terms of\nsize or in terms of the geographical area it covers. Scaling with respect to size\noccurs, for example, when an increasing number of processes needs to access\ndata that are managed by a single server. In that case, performance can be\nimproved by replicating the server and subsequently dividing the workload\namong the processes accessing the data.\nScaling with respect to a geographical area may also require replication.\nThe basic idea is that by placing a copy of data in proximity of the process\nusing them, the time to access the data decreases. As a consequence, the\nperformance as perceived by that process increases. This example also illus-\ntrates that the bene\ufb01ts of replication for performance may be hard to evaluate.\nAlthough a client process may perceive better performance, it may also be the\ncase that more network bandwidth is now consumed keeping all replicas up\nto date.\nIf replication helps to improve reliability and performance, who could be\nagainst it? Unfortunately, there is a price to be paid when data are replicated.\nThe problem with replication is that having multiple copies may lead to\nconsistency problems. Whenever a copy is modi\ufb01ed, that copy becomes\ndifferent from the rest. Consequently, modi\ufb01cations have to be carried out on\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n356 CHAPTER 7. CONSISTENCY AND REPLICATION\ndiscussed in a separate section. Also, attention is paid to caching protocols,\nwhich form a special case of consistency protocols.\nBeing arguably the largest distributed system, we pay separate attention\nto caching and replication in Web-based systems, notably looking at content\ndelivery networks as well as edge-server caching techniques.\n7.1 Introduction\nIn this section, we start with discussing the important reasons for wanting to\nreplicate data in the \ufb01rst place. We concentrate on replication as a technique\nfor achieving scalability, and motivate why reasoning about consistency is so\nimportant.\nReasons for replication\nThere are two primary reasons for replicating data. First, data are replicated\nto increase the reliability of a system. If a \ufb01le system has been replicated\nit may be possible to continue working after one replica crashes by simply\nswitching to one of the other replicas. Also, by maintaining multiple copies,\nit becomes possible to provide better protection against corrupted data. For\nexample, imagine there are three copies of a \ufb01le and every read and write\noperation is performed on each copy. We can safeguard ourselves against a\nsingle, failing write operation, by considering the value that is returned by at\nleast two copies as being the correct one.\nThe other reason for replicating data is performance. Replication for\nperformance is important when a distributed system needs to scale in terms of\nsize or in terms of the geographical area it covers. Scaling with respect to size\noccurs, for example, when an increasing number of processes needs to access\ndata that are managed by a single server. In that case, performance can be\nimproved by replicating the server and subsequently dividing the workload\namong the processes accessing the data.\nScaling with respect to a geographical area may also require replication.\nThe basic idea is that by placing a copy of data in proximity of the process\nusing them, the time to access the data decreases. As a consequence, the\nperformance as perceived by that process increases. This example also illus-\ntrates that the bene\ufb01ts of replication for performance may be hard to evaluate.\nAlthough a client process may perceive better performance, it may also be the\ncase that more network bandwidth is now consumed keeping all replicas up\nto date.\nIf replication helps to improve reliability and performance, who could be\nagainst it? Unfortunately, there is a price to be paid when data are replicated.\nThe problem with replication is that having multiple copies may lead to\nconsistency problems. Whenever a copy is modi\ufb01ed, that copy becomes\ndifferent from the rest. Consequently, modi\ufb01cations have to be carried out on\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.1. INTRODUCTION 357\nall copies to ensure consistency. Exactly when and how those modi\ufb01cations\nneed to be carried out determines the price of replication.\nTo understand the problem, consider improving access times to Web pages.\nIf no special measures are taken, fetching a page from a remote Web server\nmay sometimes even take seconds to complete. To improve performance,\nWeb browsers often locally store a copy of a previously fetched Web page\n(i.e., they cache a Web page). If a user requires that page again, the browser\nautomatically returns the local copy. The access time as perceived by the user\nis excellent. However, if the user always wants to have the latest version of\na page, he may be in for bad luck. The problem is that if the page has been\nmodi\ufb01ed in the meantime, modi\ufb01cations will not have been propagated to\ncached copies, making those copies out-of-date.\nOne solution to the problem of returning a stale copy to the user is to\nforbid the browser to keep local copies in the \ufb01rst place, effectively letting the\nserver be fully in charge of replication. However, this solution may still lead\nto poor access times if no replica is placed near the user. Another solution is\nto let the Web server invalidate or update each cached copy, but this requires\nthat the server keeps track of all caches and sending them messages. This,\nin turn, may degrade the overall performance of the server. We return to\nperformance versus scalability issues below.\nIn the following we will mainly concentrate on replication for performance.\nReplication for reliability is discussed in Chapter 8.\nReplication as scaling technique\nReplication and caching for performance are widely applied as scaling tech-\nniques. Scalability issues generally appear in the form of performance prob-\nlems. Placing copies of data close to the processes using them can improve\nperformance through reduction of access time and thus solve scalability prob-\nlems.\nA possible trade-off that needs to be made is that keeping copies up to date\nmay require more network bandwidth. Consider a process Pthat accesses\na local replica Ntimes per second, whereas the replica itself is updated M\ntimes per second. Assume that an update completely refreshes the previous\nversion of the local replica. If N\u001cM, that is, the access-to-update ratio is\nvery low, we have the situation where many updated versions of the local\nreplica will never be accessed by P, rendering the network communication\nfor those versions useless. In this case, it may have been better not to install\na local replica close to P, or to apply a different strategy for updating the\nreplica.\nA more serious problem, however, is that keeping multiple copies con-\nsistent may itself be subject to serious scalability problems. Intuitively, a\ncollection of copies is consistent when the copies are always the same. This\nmeans that a read operation performed at any copy will always return the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.1. INTRODUCTION 357\nall copies to ensure consistency. Exactly when and how those modi\ufb01cations\nneed to be carried out determines the price of replication.\nTo understand the problem, consider improving access times to Web pages.\nIf no special measures are taken, fetching a page from a remote Web server\nmay sometimes even take seconds to complete. To improve performance,\nWeb browsers often locally store a copy of a previously fetched Web page\n(i.e., they cache a Web page). If a user requires that page again, the browser\nautomatically returns the local copy. The access time as perceived by the user\nis excellent. However, if the user always wants to have the latest version of\na page, he may be in for bad luck. The problem is that if the page has been\nmodi\ufb01ed in the meantime, modi\ufb01cations will not have been propagated to\ncached copies, making those copies out-of-date.\nOne solution to the problem of returning a stale copy to the user is to\nforbid the browser to keep local copies in the \ufb01rst place, effectively letting the\nserver be fully in charge of replication. However, this solution may still lead\nto poor access times if no replica is placed near the user. Another solution is\nto let the Web server invalidate or update each cached copy, but this requires\nthat the server keeps track of all caches and sending them messages. This,\nin turn, may degrade the overall performance of the server. We return to\nperformance versus scalability issues below.\nIn the following we will mainly concentrate on replication for performance.\nReplication for reliability is discussed in Chapter 8.\nReplication as scaling technique\nReplication and caching for performance are widely applied as scaling tech-\nniques. Scalability issues generally appear in the form of performance prob-\nlems. Placing copies of data close to the processes using them can improve\nperformance through reduction of access time and thus solve scalability prob-\nlems.\nA possible trade-off that needs to be made is that keeping copies up to date\nmay require more network bandwidth. Consider a process Pthat accesses\na local replica Ntimes per second, whereas the replica itself is updated M\ntimes per second. Assume that an update completely refreshes the previous\nversion of the local replica. If N\u001cM, that is, the access-to-update ratio is\nvery low, we have the situation where many updated versions of the local\nreplica will never be accessed by P, rendering the network communication\nfor those versions useless. In this case, it may have been better not to install\na local replica close to P, or to apply a different strategy for updating the\nreplica.\nA more serious problem, however, is that keeping multiple copies con-\nsistent may itself be subject to serious scalability problems. Intuitively, a\ncollection of copies is consistent when the copies are always the same. This\nmeans that a read operation performed at any copy will always return the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "358 CHAPTER 7. CONSISTENCY AND REPLICATION\nsame result. Consequently, when an update operation is performed on one\ncopy, the update should be propagated to all copies before a subsequent\noperation takes place, no matter at which copy that operation is initiated or\nperformed.\nThis type of consistency is sometimes informally (and imprecisely) re-\nferred to as tight consistency as provided by what is also called synchronous\nreplication. (In Section 7.2, we will provide precise de\ufb01nitions of consistency\nand introduce a range of consistency models.) The key idea is that an update\nis performed at all copies as a single atomic operation, or transaction. Unfor-\ntunately, implementing atomicity involving a large number of replicas that\nmay be widely dispersed across a large-scale network is inherently dif\ufb01cult\nwhen operations are also required to complete quickly.\nDif\ufb01culties come from the fact that we need to synchronize all replicas. In\nessence, this means that all replicas \ufb01rst need to reach agreement on when\nexactly an update is to be performed locally. For example, replicas may need\nto decide on a global ordering of operations using Lamport timestamps, or\nlet a coordinator assign such an order. Global synchronization simply takes\na lot of communication time, especially when replicas are spread across a\nwide-area network.\nWe are now faced with a dilemma. On the one hand, scalability problems\ncan be alleviated by applying replication and caching, leading to improved per-\nformance. On the other hand, to keep all copies consistent generally requires\nglobal synchronization, which is inherently costly in terms of performance.\nThe cure may be worse than the disease.\nIn many cases, the only real solution is to relax the consistency constraints.\nIn other words, if we can relax the requirement that updates need to be\nexecuted as atomic operations, we may be able to avoid (instantaneous) global\nsynchronizations, and may thus gain performance. The price paid is that\ncopies may not always be the same everywhere. As it turns out, to what\nextent consistency can be relaxed depends highly on the access and update\npatterns of the replicated data, as well as on the purpose for which those data\nare used.\nThere are a range of consistency models and many different ways to\nimplement models through what are called distribution and consistency\nprotocols. Approaches to classifying consistency and replication can be found\nin [Gray et al., 1996; Wiesmann et al., 2000] and [Aguilera and Terry, 2016].\n7.2 Data-centric consistency models\nTraditionally, consistency has been discussed in the context of read and write\noperations on shared data, available by means of (distributed) shared memory,\na (distributed) shared database, or a (distributed) \ufb01le system. Here, we use\nthe broader term data store . A data store may be physically distributed across\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n358 CHAPTER 7. CONSISTENCY AND REPLICATION\nsame result. Consequently, when an update operation is performed on one\ncopy, the update should be propagated to all copies before a subsequent\noperation takes place, no matter at which copy that operation is initiated or\nperformed.\nThis type of consistency is sometimes informally (and imprecisely) re-\nferred to as tight consistency as provided by what is also called synchronous\nreplication. (In Section 7.2, we will provide precise de\ufb01nitions of consistency\nand introduce a range of consistency models.) The key idea is that an update\nis performed at all copies as a single atomic operation, or transaction. Unfor-\ntunately, implementing atomicity involving a large number of replicas that\nmay be widely dispersed across a large-scale network is inherently dif\ufb01cult\nwhen operations are also required to complete quickly.\nDif\ufb01culties come from the fact that we need to synchronize all replicas. In\nessence, this means that all replicas \ufb01rst need to reach agreement on when\nexactly an update is to be performed locally. For example, replicas may need\nto decide on a global ordering of operations using Lamport timestamps, or\nlet a coordinator assign such an order. Global synchronization simply takes\na lot of communication time, especially when replicas are spread across a\nwide-area network.\nWe are now faced with a dilemma. On the one hand, scalability problems\ncan be alleviated by applying replication and caching, leading to improved per-\nformance. On the other hand, to keep all copies consistent generally requires\nglobal synchronization, which is inherently costly in terms of performance.\nThe cure may be worse than the disease.\nIn many cases, the only real solution is to relax the consistency constraints.\nIn other words, if we can relax the requirement that updates need to be\nexecuted as atomic operations, we may be able to avoid (instantaneous) global\nsynchronizations, and may thus gain performance. The price paid is that\ncopies may not always be the same everywhere. As it turns out, to what\nextent consistency can be relaxed depends highly on the access and update\npatterns of the replicated data, as well as on the purpose for which those data\nare used.\nThere are a range of consistency models and many different ways to\nimplement models through what are called distribution and consistency\nprotocols. Approaches to classifying consistency and replication can be found\nin [Gray et al., 1996; Wiesmann et al., 2000] and [Aguilera and Terry, 2016].\n7.2 Data-centric consistency models\nTraditionally, consistency has been discussed in the context of read and write\noperations on shared data, available by means of (distributed) shared memory,\na (distributed) shared database, or a (distributed) \ufb01le system. Here, we use\nthe broader term data store . A data store may be physically distributed across\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 359\nmultiple machines. In particular, each process that can access data from the\nstore is assumed to have a local (or nearby) copy available of the entire store.\nWrite operations are propagated to the other copies, as shown in Figure 7.1. A\ndata operation is classi\ufb01ed as a write operation when it changes the data, and\nis otherwise classi\ufb01ed as a read operation.\nFigure 7.1: The general organization of a logical data store, physically dis-\ntributed and replicated across multiple processes.\nAconsistency model is essentially a contract between processes and the\ndata store. It says that if processes agree to obey certain rules, the store\npromises to work correctly. Normally, a process that performs a read operation\non a data item, expects the operation to return a value that shows the results\nof the last write operation on that data.\nIn the absence of a global clock, it is dif\ufb01cult to de\ufb01ne precisely which\nwrite operation is the last one. As an alternative, we need to provide other\nde\ufb01nitions, leading to a range of consistency models. Each model effectively\nrestricts the values that a read operation on a data item can return. As is to be\nexpected, the ones with major restrictions are easy to use, for example when\ndeveloping applications, whereas those with minor restrictions are generally\nconsidered to be dif\ufb01cult to use in practice. The trade-off is, of course, that the\neasy-to-use models do not perform nearly as well as the dif\ufb01cult ones. Such\nis life.\nContinuous consistency\nThere is no such thing as a best solution to replicating data. Replicating data\nposes consistency problems that cannot be solved ef\ufb01ciently in a general way.\nOnly if we loosen consistency can there be hope for attaining ef\ufb01cient solutions.\nUnfortunately, there are also no general rules for loosening consistency: exactly\nwhat can be tolerated is highly dependent on applications.\nThere are different ways for applications to specify what inconsistencies\nthey can tolerate. Yu and Vahdat [2002] take a general approach by distin-\nguishing three independent axes for de\ufb01ning inconsistencies: deviation in\nnumerical values between replicas, deviation in staleness between replicas,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 359\nmultiple machines. In particular, each process that can access data from the\nstore is assumed to have a local (or nearby) copy available of the entire store.\nWrite operations are propagated to the other copies, as shown in Figure 7.1. A\ndata operation is classi\ufb01ed as a write operation when it changes the data, and\nis otherwise classi\ufb01ed as a read operation.\nFigure 7.1: The general organization of a logical data store, physically dis-\ntributed and replicated across multiple processes.\nAconsistency model is essentially a contract between processes and the\ndata store. It says that if processes agree to obey certain rules, the store\npromises to work correctly. Normally, a process that performs a read operation\non a data item, expects the operation to return a value that shows the results\nof the last write operation on that data.\nIn the absence of a global clock, it is dif\ufb01cult to de\ufb01ne precisely which\nwrite operation is the last one. As an alternative, we need to provide other\nde\ufb01nitions, leading to a range of consistency models. Each model effectively\nrestricts the values that a read operation on a data item can return. As is to be\nexpected, the ones with major restrictions are easy to use, for example when\ndeveloping applications, whereas those with minor restrictions are generally\nconsidered to be dif\ufb01cult to use in practice. The trade-off is, of course, that the\neasy-to-use models do not perform nearly as well as the dif\ufb01cult ones. Such\nis life.\nContinuous consistency\nThere is no such thing as a best solution to replicating data. Replicating data\nposes consistency problems that cannot be solved ef\ufb01ciently in a general way.\nOnly if we loosen consistency can there be hope for attaining ef\ufb01cient solutions.\nUnfortunately, there are also no general rules for loosening consistency: exactly\nwhat can be tolerated is highly dependent on applications.\nThere are different ways for applications to specify what inconsistencies\nthey can tolerate. Yu and Vahdat [2002] take a general approach by distin-\nguishing three independent axes for de\ufb01ning inconsistencies: deviation in\nnumerical values between replicas, deviation in staleness between replicas,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "360 CHAPTER 7. CONSISTENCY AND REPLICATION\nand deviation with respect to the ordering of update operations. They refer to\nthese deviations as forming continuous consistency ranges.\nMeasuring inconsistency in terms of numerical deviations can be used\nby applications for which the data have numerical semantics. One obvious\nexample is the replication of records containing stock market prices. In this\ncase, an application may specify that two copies should not deviate more than\n$0.02, which would be an absolute numerical deviation . Alternatively, a relative\nnumerical deviation could be speci\ufb01ed, stating that two copies should differ by\nno more than, for example, 0.5%. In both cases, we would see that if a stock\ngoes up (and one of the replicas is immediately updated) without violating\nthe speci\ufb01ed numerical deviations, replicas would still be considered to be\nmutually consistent.\nNumerical deviation can also be understood in terms of the number of\nupdates that have been applied to a given replica, but have not yet been seen\nby others. For example, a Web cache may not have seen a batch of operations\ncarried out by a Web server. In this case, the associated deviation in the value\nis also referred to as its weight .\nStaleness deviations relate to the last time a replica was updated. For some\napplications, it can be tolerated that a replica provides old data as long as it is\nnottooold. For example, weather reports typically stay reasonably accurate\nover some time, say a few hours. In such cases, a main server may receive\ntimely updates, but may decide to propagate updates to the replicas only once\nin a while.\nFinally, there are classes of applications in which the ordering of updates\nare allowed to be different at the various replicas, as long as the differences\nremain bounded. One way of looking at these updates is that they are applied\ntentatively to a local copy, awaiting global agreement from all replicas. As\na consequence, some updates may need to be rolled back and applied in a\ndifferent order before becoming permanent. Intuitively, ordering deviations\nare much harder to grasp than the other two consistency metrics.\nThe notion of a conit\nTo de\ufb01ne inconsistencies, Yu and Vahdat introduce a consistency unit , ab-\nbreviated to conit . A conit speci\ufb01es the unit over which consistency is to\nbe measured. For example, in our stock-exchange example, a conit could\nbe de\ufb01ned as a record representing a single stock. Another example is an\nindividual weather report.\nTo give an example of a conit, and at the same time illustrate numerical\nand ordering deviations, consider the situation of keeping track of a \ufb02eet of\ncars. In particular, the \ufb02eet owner is interested in knowing how much he pays\non average for gas. To this end, whenever a driver tanks gasoline, he reports\nthe amount of gasoline that has been tanked (recorded as g), the price paid\n(recorded as p), and the total distance since the last time he tanked (recorded\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n360 CHAPTER 7. CONSISTENCY AND REPLICATION\nand deviation with respect to the ordering of update operations. They refer to\nthese deviations as forming continuous consistency ranges.\nMeasuring inconsistency in terms of numerical deviations can be used\nby applications for which the data have numerical semantics. One obvious\nexample is the replication of records containing stock market prices. In this\ncase, an application may specify that two copies should not deviate more than\n$0.02, which would be an absolute numerical deviation . Alternatively, a relative\nnumerical deviation could be speci\ufb01ed, stating that two copies should differ by\nno more than, for example, 0.5%. In both cases, we would see that if a stock\ngoes up (and one of the replicas is immediately updated) without violating\nthe speci\ufb01ed numerical deviations, replicas would still be considered to be\nmutually consistent.\nNumerical deviation can also be understood in terms of the number of\nupdates that have been applied to a given replica, but have not yet been seen\nby others. For example, a Web cache may not have seen a batch of operations\ncarried out by a Web server. In this case, the associated deviation in the value\nis also referred to as its weight .\nStaleness deviations relate to the last time a replica was updated. For some\napplications, it can be tolerated that a replica provides old data as long as it is\nnottooold. For example, weather reports typically stay reasonably accurate\nover some time, say a few hours. In such cases, a main server may receive\ntimely updates, but may decide to propagate updates to the replicas only once\nin a while.\nFinally, there are classes of applications in which the ordering of updates\nare allowed to be different at the various replicas, as long as the differences\nremain bounded. One way of looking at these updates is that they are applied\ntentatively to a local copy, awaiting global agreement from all replicas. As\na consequence, some updates may need to be rolled back and applied in a\ndifferent order before becoming permanent. Intuitively, ordering deviations\nare much harder to grasp than the other two consistency metrics.\nThe notion of a conit\nTo de\ufb01ne inconsistencies, Yu and Vahdat introduce a consistency unit , ab-\nbreviated to conit . A conit speci\ufb01es the unit over which consistency is to\nbe measured. For example, in our stock-exchange example, a conit could\nbe de\ufb01ned as a record representing a single stock. Another example is an\nindividual weather report.\nTo give an example of a conit, and at the same time illustrate numerical\nand ordering deviations, consider the situation of keeping track of a \ufb02eet of\ncars. In particular, the \ufb02eet owner is interested in knowing how much he pays\non average for gas. To this end, whenever a driver tanks gasoline, he reports\nthe amount of gasoline that has been tanked (recorded as g), the price paid\n(recorded as p), and the total distance since the last time he tanked (recorded\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 361\nby the variable d). Technically, the three variables g,p, and dform a conit.\nThis conit is replicated across two servers, as shown in Figure 7.2, and a driver\nregularly reports his gas usage to one of the servers by separately updating\neach variable (without further considering the car in question).\nThe task of the servers is to keep the conit \u201cconsistently\u201d replicated. To\nthis end, each replica server maintains a two-dimensional vector clock. We\nuse the notationhT,Rito express an operation that was carried out by replica\nRat (its) logical time T.\nFigure 7.2: An example of keeping track of consistency deviations.\nIn this example we see two replicas that operate on a conit containing the\ndata items g,p, and dfrom our example. All variables are assumed to have\nbeen initialized to 0. Replica Areceived the operation\nh5,Bi:g g+45\nfrom replica B. We have shaded this operation gray to indicate that Ahas\ncommitted this operation to its local store. In other words, it has been made\npermanent and cannot be rolled back. Replica Aalso has three tentative\nupdate operations listed: h8,Ai,h9,Ai, andh10,Ai, respectively. In terms of\ncontinuous consistency, the fact that Ahas three tentative operations pending\nto be committed is referred to as an order deviation of, in this case, value 3.\nAnalogously, with in total three operations of which two have been committed,\nBhas an order deviation of 1.\nFrom this example, we see that A\u2019s logical clock value is now 11. Because\nthe last operation from Bthat Ahad received had timestamp 5, the vector\nclock at Awill be (11, 5), where we assume the \ufb01rst component of the vector\nis used for Aand the second for B. Along the same lines, the logical clock at\nBis(0, 8).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 361\nby the variable d). Technically, the three variables g,p, and dform a conit.\nThis conit is replicated across two servers, as shown in Figure 7.2, and a driver\nregularly reports his gas usage to one of the servers by separately updating\neach variable (without further considering the car in question).\nThe task of the servers is to keep the conit \u201cconsistently\u201d replicated. To\nthis end, each replica server maintains a two-dimensional vector clock. We\nuse the notationhT,Rito express an operation that was carried out by replica\nRat (its) logical time T.\nFigure 7.2: An example of keeping track of consistency deviations.\nIn this example we see two replicas that operate on a conit containing the\ndata items g,p, and dfrom our example. All variables are assumed to have\nbeen initialized to 0. Replica Areceived the operation\nh5,Bi:g g+45\nfrom replica B. We have shaded this operation gray to indicate that Ahas\ncommitted this operation to its local store. In other words, it has been made\npermanent and cannot be rolled back. Replica Aalso has three tentative\nupdate operations listed: h8,Ai,h9,Ai, andh10,Ai, respectively. In terms of\ncontinuous consistency, the fact that Ahas three tentative operations pending\nto be committed is referred to as an order deviation of, in this case, value 3.\nAnalogously, with in total three operations of which two have been committed,\nBhas an order deviation of 1.\nFrom this example, we see that A\u2019s logical clock value is now 11. Because\nthe last operation from Bthat Ahad received had timestamp 5, the vector\nclock at Awill be (11, 5), where we assume the \ufb01rst component of the vector\nis used for Aand the second for B. Along the same lines, the logical clock at\nBis(0, 8).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "362 CHAPTER 7. CONSISTENCY AND REPLICATION\nThe numerical deviation at a replica Rconsists of two components: the\nnumber of operations at all other replicas that have not yet been seen by\nR, along with the sum of corresponding missed values (more sophisticated\nschemes are, of course, also possible). In our example, Ahas not yet seen\noperationsh6,Biandh7,Biwith a total value of 70 + 412 units, leading to a\nnumerical deviation of (2, 482 ). Likewise, Bis still missing the three tentative\noperations at A, with a total summed value of 686, bringing B\u2019s numerical\ndeviation to (3, 686 ).\nUsing these notions, it becomes possible to specify speci\ufb01c consistency\nschemes. For example, we may restrict order deviation by specifying an\nacceptable maximal value. Likewise, we may want two replicas to never nu-\nmerically deviate by more than 1000 units. Having such consistency schemes\ndoes require that a replica knows how much it is deviating from other replicas,\nimplying that we need separate communication to keep replicas informed.\nThe underlying assumption is that such communication is much less expen-\nsive than communication to keep replicas synchronized. Admittedly, it is\nquestionable if this assumption also holds for our example.\nNote 7.1 (Advanced: On the granularity of conits)\nThere is a trade-off between maintaining \ufb01ne-grained and coarse-grained conits.\nIf a conit represents a lot of data, such as a complete database, then updates are\naggregated for all the data in the conit. As a consequence, this may bring replicas\nsooner in an inconsistent state. For example, assume that in Figure 7.3 two replicas\nmay differ in no more than one outstanding update. In that case, when the data\nitems in Figure 7.3 have each been updated once at the \ufb01rst replica, the second\none will need to be updated as well. This is not the case when choosing a smaller\nconit, as shown in Figure 7.3 There, the replicas are still considered to be up to\ndate. This problem is particularly important when the data items contained in a\nconit are used completely independently, in which case they are said to falsely\nshare the conit.\n(a) (b)\nFigure 7.3: Choosing the appropriate granularity for a conit. (a) Two\nupdates lead to update propagation. (b) No update propagation is needed.\nUnfortunately, making conits very small is not a good idea, for the simple\nreason that the total number of conits that need to be managed grows as well.\nIn other words, there is an overhead related to managing the conits that needs\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n362 CHAPTER 7. CONSISTENCY AND REPLICATION\nThe numerical deviation at a replica Rconsists of two components: the\nnumber of operations at all other replicas that have not yet been seen by\nR, along with the sum of corresponding missed values (more sophisticated\nschemes are, of course, also possible). In our example, Ahas not yet seen\noperationsh6,Biandh7,Biwith a total value of 70 + 412 units, leading to a\nnumerical deviation of (2, 482 ). Likewise, Bis still missing the three tentative\noperations at A, with a total summed value of 686, bringing B\u2019s numerical\ndeviation to (3, 686 ).\nUsing these notions, it becomes possible to specify speci\ufb01c consistency\nschemes. For example, we may restrict order deviation by specifying an\nacceptable maximal value. Likewise, we may want two replicas to never nu-\nmerically deviate by more than 1000 units. Having such consistency schemes\ndoes require that a replica knows how much it is deviating from other replicas,\nimplying that we need separate communication to keep replicas informed.\nThe underlying assumption is that such communication is much less expen-\nsive than communication to keep replicas synchronized. Admittedly, it is\nquestionable if this assumption also holds for our example.\nNote 7.1 (Advanced: On the granularity of conits)\nThere is a trade-off between maintaining \ufb01ne-grained and coarse-grained conits.\nIf a conit represents a lot of data, such as a complete database, then updates are\naggregated for all the data in the conit. As a consequence, this may bring replicas\nsooner in an inconsistent state. For example, assume that in Figure 7.3 two replicas\nmay differ in no more than one outstanding update. In that case, when the data\nitems in Figure 7.3 have each been updated once at the \ufb01rst replica, the second\none will need to be updated as well. This is not the case when choosing a smaller\nconit, as shown in Figure 7.3 There, the replicas are still considered to be up to\ndate. This problem is particularly important when the data items contained in a\nconit are used completely independently, in which case they are said to falsely\nshare the conit.\n(a) (b)\nFigure 7.3: Choosing the appropriate granularity for a conit. (a) Two\nupdates lead to update propagation. (b) No update propagation is needed.\nUnfortunately, making conits very small is not a good idea, for the simple\nreason that the total number of conits that need to be managed grows as well.\nIn other words, there is an overhead related to managing the conits that needs\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 363\nto be taken into account. This overhead, in turn, may adversely affect overall\nperformance, which has to be taken into account.\nAlthough from a conceptual point of view conits form an attractive means\nfor capturing consistency requirements, there are two important issues that\nneed to be dealt with before they can be put to practical use. First, in order\nto enforce consistency we need to have protocols. Protocols for continuous\nconsistency are discussed later in this chapter.\nA second issue is that program developers must specify the consistency\nrequirements for their applications. Practice indicates that obtaining such\nrequirements may be extremely dif\ufb01cult. Programmers are generally not used\nto handling replication, let alone understanding what it means to provide\ndetailed information on consistency. Therefore, it is mandatory that there are\nsimple and easy-to-understand programming interfaces.\nNote 7.2 (Advanced: Programming conits)\nContinuous consistency can be implemented as a toolkit which appears to pro-\ngrammers as just another library that they link with their applications. A conit is\nsimply declared alongside an update of a data item. For example, the fragment of\npseudocode\nAffectsConit(ConitQ, 1, 1);\nappend message m to queue Q;\nstates that appending a message to queue Qbelongs to a conit named ConitQ .\nLikewise, operations may now also be declared as being dependent on conits:\nDependsOnConit(ConitQ, 4, 0, 60);\nread message m from head of queue Q;\nIn this case, the call to DependsOnConit() speci\ufb01es that the numerical deviation,\nordering deviation, and staleness should be limited to the values 4, 0, and 60\n(seconds), respectively. This can be interpreted as that there should be at most\n4 unseen update operations at other replicas, there should be no tentative local\nupdates, and the local copy of Qshould have been checked for staleness no more\nthan 60 seconds ago. If these requirements are not ful\ufb01lled, the underlying\nmiddleware will attempt to bring the local copy of Qto a state such that the read\noperation can be carried out.\nThe question, of course, is how does the system know that Qis associated\nwith ConitQ ? For practical reasons, we can avoid explicit declarations of conits\nand concentrate only on the grouping of operations. The data to be replicated is\ncollectively considered to belong together. By subsequently associating a write\noperation with a named conit, and likewise for a read operation, we tell the\nmiddleware layer when to start synchronizing the entire replica. Indeed, there may\nbe a considerable amount of false sharing in such a case. If false sharing needs\nto be avoided, we would have to introduce a separate programming construct to\nexplicitly declare conits.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 363\nto be taken into account. This overhead, in turn, may adversely affect overall\nperformance, which has to be taken into account.\nAlthough from a conceptual point of view conits form an attractive means\nfor capturing consistency requirements, there are two important issues that\nneed to be dealt with before they can be put to practical use. First, in order\nto enforce consistency we need to have protocols. Protocols for continuous\nconsistency are discussed later in this chapter.\nA second issue is that program developers must specify the consistency\nrequirements for their applications. Practice indicates that obtaining such\nrequirements may be extremely dif\ufb01cult. Programmers are generally not used\nto handling replication, let alone understanding what it means to provide\ndetailed information on consistency. Therefore, it is mandatory that there are\nsimple and easy-to-understand programming interfaces.\nNote 7.2 (Advanced: Programming conits)\nContinuous consistency can be implemented as a toolkit which appears to pro-\ngrammers as just another library that they link with their applications. A conit is\nsimply declared alongside an update of a data item. For example, the fragment of\npseudocode\nAffectsConit(ConitQ, 1, 1);\nappend message m to queue Q;\nstates that appending a message to queue Qbelongs to a conit named ConitQ .\nLikewise, operations may now also be declared as being dependent on conits:\nDependsOnConit(ConitQ, 4, 0, 60);\nread message m from head of queue Q;\nIn this case, the call to DependsOnConit() speci\ufb01es that the numerical deviation,\nordering deviation, and staleness should be limited to the values 4, 0, and 60\n(seconds), respectively. This can be interpreted as that there should be at most\n4 unseen update operations at other replicas, there should be no tentative local\nupdates, and the local copy of Qshould have been checked for staleness no more\nthan 60 seconds ago. If these requirements are not ful\ufb01lled, the underlying\nmiddleware will attempt to bring the local copy of Qto a state such that the read\noperation can be carried out.\nThe question, of course, is how does the system know that Qis associated\nwith ConitQ ? For practical reasons, we can avoid explicit declarations of conits\nand concentrate only on the grouping of operations. The data to be replicated is\ncollectively considered to belong together. By subsequently associating a write\noperation with a named conit, and likewise for a read operation, we tell the\nmiddleware layer when to start synchronizing the entire replica. Indeed, there may\nbe a considerable amount of false sharing in such a case. If false sharing needs\nto be avoided, we would have to introduce a separate programming construct to\nexplicitly declare conits.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "364 CHAPTER 7. CONSISTENCY AND REPLICATION\nConsistent ordering of operations\nThere is a huge body of work on data-centric consistency models from the\npast decades. An important class of models comes from the \ufb01eld of parallel\nprogramming. Confronted with the fact that in parallel and distributed\ncomputing multiple processes will need to share resources and access these\nresources simultaneously, researchers have sought to express the semantics\nof concurrent accesses when shared resources are replicated. The models\nthat we discuss here all deal with consistently ordering operations on shared,\nreplicated data.\nIn principle, the models augment those of continuous consistency in the\nsense that when tentative updates at replicas need to be committed, replicas\nwill need to reach agreement on a global, that is, consistent ordering of those\nupdates.\nSequential consistency\nIn the following, we will use a special notation in which we draw the opera-\ntions of a process along a time axis. The time axis is always drawn horizontally,\nwith time increasing from left to right. We use the notation Wi(x)ato denote\nthat process Piwrites value ato data item x. Similarly, Ri(x)brepresents the\nfact that process Pireads xand is returned the value b. We assume that each\ndata item has initial value NIL. When there is no confusion concerning which\nprocess is accessing data, we omit the index from the symbols Wand R.\nFigure 7.4: Behavior of two processes operating on the same data item. The\nhorizontal axis is time.\nAs an example, in Figure 7.4 P1does a write to a data item x, modifying\nits value to a. Note that, according to our system model the operation W1(x)a\nis \ufb01rst performed on a copy of the data store that is local to P1, and only then\nis it propagated to the other local copies. In our example, P2later reads the\nvalue NIL, and some time after that a(from its local copy of the store). What\nwe are seeing here is that it took some time to propagate the update of xto\nP2, which is perfectly acceptable.\nSequential consistency is an important data-centric consistency model,\nwhich was \ufb01rst de\ufb01ned by Lamport [1979] in the context of shared memory\nfor multiprocessor systems. A data store is said to be sequentially consistent\nwhen it satis\ufb01es the following condition:\nThe result of any execution is the same as if the (read and write) opera-\ntions by all processes on the data store were executed in some sequential\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n364 CHAPTER 7. CONSISTENCY AND REPLICATION\nConsistent ordering of operations\nThere is a huge body of work on data-centric consistency models from the\npast decades. An important class of models comes from the \ufb01eld of parallel\nprogramming. Confronted with the fact that in parallel and distributed\ncomputing multiple processes will need to share resources and access these\nresources simultaneously, researchers have sought to express the semantics\nof concurrent accesses when shared resources are replicated. The models\nthat we discuss here all deal with consistently ordering operations on shared,\nreplicated data.\nIn principle, the models augment those of continuous consistency in the\nsense that when tentative updates at replicas need to be committed, replicas\nwill need to reach agreement on a global, that is, consistent ordering of those\nupdates.\nSequential consistency\nIn the following, we will use a special notation in which we draw the opera-\ntions of a process along a time axis. The time axis is always drawn horizontally,\nwith time increasing from left to right. We use the notation Wi(x)ato denote\nthat process Piwrites value ato data item x. Similarly, Ri(x)brepresents the\nfact that process Pireads xand is returned the value b. We assume that each\ndata item has initial value NIL. When there is no confusion concerning which\nprocess is accessing data, we omit the index from the symbols Wand R.\nFigure 7.4: Behavior of two processes operating on the same data item. The\nhorizontal axis is time.\nAs an example, in Figure 7.4 P1does a write to a data item x, modifying\nits value to a. Note that, according to our system model the operation W1(x)a\nis \ufb01rst performed on a copy of the data store that is local to P1, and only then\nis it propagated to the other local copies. In our example, P2later reads the\nvalue NIL, and some time after that a(from its local copy of the store). What\nwe are seeing here is that it took some time to propagate the update of xto\nP2, which is perfectly acceptable.\nSequential consistency is an important data-centric consistency model,\nwhich was \ufb01rst de\ufb01ned by Lamport [1979] in the context of shared memory\nfor multiprocessor systems. A data store is said to be sequentially consistent\nwhen it satis\ufb01es the following condition:\nThe result of any execution is the same as if the (read and write) opera-\ntions by all processes on the data store were executed in some sequential\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 365\norder and the operations of each individual process appear in this se-\nquence in the order speci\ufb01ed by its program.\nWhat this de\ufb01nition means is that when processes run concurrently on\n(possibly) different machines, any valid interleaving of read and write op-\nerations is acceptable behavior, but all processes see the same interleaving of\noperations . Note that nothing is said about time; that is, there is no reference\nto the \u201cmost recent\u201d write operation on a data item. Also, a process \u201csees\u201d\nthe writes from all processes but only through its own reads.\nThat time does not play a role can be seen from Figure 7.5. Consider four\nprocesses operating on the same data item x. In Figure 7.5(a) process P1\ufb01rst\nperforms W1(x)aonx. Later (in absolute time), process P2also performs\na write operation W2(x)b, by setting the value of xtob. However, both\nprocesses P3and P4\ufb01rst read value b, and later value a. In other words,\nthe write operation W2(x)bof process P2appears to have taken place before\nW1(x)aofP1.\n(a) (b)\nFigure 7.5: (a) A sequentially consistent data store. (b) A data store that is not\nsequentially consistent.\nIn contrast, Figure 7.5(b) violates sequential consistency because not all\nprocesses see the same interleaving of write operations. In particular, to\nprocess P3, it appears as if the data item has \ufb01rst been changed to b, and later\ntoa. On the other hand, P4will conclude that the \ufb01nal value is b.\nProcess P1 Process P2 Process P3\nx 1; y 1; z 1;\nprint(y,z); print(x,z); print(x,y);\nFigure 7.6: Three concurrently executing processes.\nTo make the notion of sequential consistency more concrete, consider\nthree concurrently executing processes P1,P2, and P3, shown in Figure 7.6\n(taken from [Dubois et al., 1988]). The data items in this example are formed\nby the three integer variables x,y, and z, which are stored in a (possibly\ndistributed) shared sequentially consistent data store. We assume that each\nvariable is initialized to 0. In this example, an assignment corresponds to a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 365\norder and the operations of each individual process appear in this se-\nquence in the order speci\ufb01ed by its program.\nWhat this de\ufb01nition means is that when processes run concurrently on\n(possibly) different machines, any valid interleaving of read and write op-\nerations is acceptable behavior, but all processes see the same interleaving of\noperations . Note that nothing is said about time; that is, there is no reference\nto the \u201cmost recent\u201d write operation on a data item. Also, a process \u201csees\u201d\nthe writes from all processes but only through its own reads.\nThat time does not play a role can be seen from Figure 7.5. Consider four\nprocesses operating on the same data item x. In Figure 7.5(a) process P1\ufb01rst\nperforms W1(x)aonx. Later (in absolute time), process P2also performs\na write operation W2(x)b, by setting the value of xtob. However, both\nprocesses P3and P4\ufb01rst read value b, and later value a. In other words,\nthe write operation W2(x)bof process P2appears to have taken place before\nW1(x)aofP1.\n(a) (b)\nFigure 7.5: (a) A sequentially consistent data store. (b) A data store that is not\nsequentially consistent.\nIn contrast, Figure 7.5(b) violates sequential consistency because not all\nprocesses see the same interleaving of write operations. In particular, to\nprocess P3, it appears as if the data item has \ufb01rst been changed to b, and later\ntoa. On the other hand, P4will conclude that the \ufb01nal value is b.\nProcess P1 Process P2 Process P3\nx 1; y 1; z 1;\nprint(y,z); print(x,z); print(x,y);\nFigure 7.6: Three concurrently executing processes.\nTo make the notion of sequential consistency more concrete, consider\nthree concurrently executing processes P1,P2, and P3, shown in Figure 7.6\n(taken from [Dubois et al., 1988]). The data items in this example are formed\nby the three integer variables x,y, and z, which are stored in a (possibly\ndistributed) shared sequentially consistent data store. We assume that each\nvariable is initialized to 0. In this example, an assignment corresponds to a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "366 CHAPTER 7. CONSISTENCY AND REPLICATION\nwrite operation, whereas a print statement corresponds to a simultaneous read\noperation of its two arguments. All statements are assumed to be indivisible.\nVarious interleaved execution sequences are possible. With six indepen-\ndent statements, there are potentially 720 (6!) possible execution sequences,\nalthough some of these violate program order. Consider the 120 (5!) sequences\nthat begin with x 1. Half of these have print(x,z) before y 1and\nthus violate program order. Half also have print(x,y) before z 1and\nalso violate program order. Only 1/4of the 120 sequences, or 30, are valid.\nAnother 30 valid sequences are possible starting with y 1and another 30\ncan begin with z 1, for a total of 90 valid execution sequences. Four of\nthese are shown in Figure 7.7.\nExecution 1 Execution 2 Execution 3 Execution 4\nP1:x 1; P1:x 1; P2:y 1; P2:y 1;\nP1:print(y,z); P2:y 1; P3:z 1; P1:x 1;\nP2:y 1; P2:print(x,z); P3:print(x,y); P3:z 1;\nP2:print(x,z); P1:print(y,z); P2:print(x,z); P2:print(x,z);\nP3:z 1; P3:z 1; P1:x 1; P1:print(y,z);\nP3:print(x,y); P3:print(x,y); P1:print(y,z); P3:print(x,y);\nPrints: 001011 Prints: 101011 Prints: 010111 Prints: 111111\nSignature: 0 0 1 0 1 1 Signature: 1 0 1 0 1 1 Signature: 1 1 0 1 0 1 Signature: 1 1 1 1 1 1\n(a) (b) (c) (d)\nFigure 7.7: Four valid execution sequences for the processes of Figure 7.6. The\nvertical axis is time.\nIn Figure 7.7(a) the three processes are run in order, \ufb01rst P1, then P2,\nthen P3. The other three examples demonstrate different, but equally valid,\ninterleavings of the statements in time. Each of the three processes prints\ntwo variables. Since the only values each variable can take on are the initial\nvalue (0), or the assigned value (1), each process produces a 2-bit string. The\nnumbers after Prints are the actual outputs that appear on the output device.\nIf we concatenate the output of P1,P2, and P3in that order, we get a 6-bit\nstring that characterizes a particular interleaving of statements. This is the\nstring listed as the Signature in Figure 7.7. Below we will characterize each\nordering by its signature rather than by its printout.\nNot all 64 signature patterns are allowed. As a trivial example, 00 00 00\nis not permitted, because that would imply that the print statements ran\nbefore the assignment statements, violating the requirement that statements\nare executed in program order. A more subtle example is 00 10 01 . The \ufb01rst\ntwo bits, 00, mean that yand zwere both 0 when P1did its printing. This\nsituation occurs only when P1executes both statements before P2orP3starts.\nThe next two bits, 10, mean that P2must run after P1has started but before\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n366 CHAPTER 7. CONSISTENCY AND REPLICATION\nwrite operation, whereas a print statement corresponds to a simultaneous read\noperation of its two arguments. All statements are assumed to be indivisible.\nVarious interleaved execution sequences are possible. With six indepen-\ndent statements, there are potentially 720 (6!) possible execution sequences,\nalthough some of these violate program order. Consider the 120 (5!) sequences\nthat begin with x 1. Half of these have print(x,z) before y 1and\nthus violate program order. Half also have print(x,y) before z 1and\nalso violate program order. Only 1/4of the 120 sequences, or 30, are valid.\nAnother 30 valid sequences are possible starting with y 1and another 30\ncan begin with z 1, for a total of 90 valid execution sequences. Four of\nthese are shown in Figure 7.7.\nExecution 1 Execution 2 Execution 3 Execution 4\nP1:x 1; P1:x 1; P2:y 1; P2:y 1;\nP1:print(y,z); P2:y 1; P3:z 1; P1:x 1;\nP2:y 1; P2:print(x,z); P3:print(x,y); P3:z 1;\nP2:print(x,z); P1:print(y,z); P2:print(x,z); P2:print(x,z);\nP3:z 1; P3:z 1; P1:x 1; P1:print(y,z);\nP3:print(x,y); P3:print(x,y); P1:print(y,z); P3:print(x,y);\nPrints: 001011 Prints: 101011 Prints: 010111 Prints: 111111\nSignature: 0 0 1 0 1 1 Signature: 1 0 1 0 1 1 Signature: 1 1 0 1 0 1 Signature: 1 1 1 1 1 1\n(a) (b) (c) (d)\nFigure 7.7: Four valid execution sequences for the processes of Figure 7.6. The\nvertical axis is time.\nIn Figure 7.7(a) the three processes are run in order, \ufb01rst P1, then P2,\nthen P3. The other three examples demonstrate different, but equally valid,\ninterleavings of the statements in time. Each of the three processes prints\ntwo variables. Since the only values each variable can take on are the initial\nvalue (0), or the assigned value (1), each process produces a 2-bit string. The\nnumbers after Prints are the actual outputs that appear on the output device.\nIf we concatenate the output of P1,P2, and P3in that order, we get a 6-bit\nstring that characterizes a particular interleaving of statements. This is the\nstring listed as the Signature in Figure 7.7. Below we will characterize each\nordering by its signature rather than by its printout.\nNot all 64 signature patterns are allowed. As a trivial example, 00 00 00\nis not permitted, because that would imply that the print statements ran\nbefore the assignment statements, violating the requirement that statements\nare executed in program order. A more subtle example is 00 10 01 . The \ufb01rst\ntwo bits, 00, mean that yand zwere both 0 when P1did its printing. This\nsituation occurs only when P1executes both statements before P2orP3starts.\nThe next two bits, 10, mean that P2must run after P1has started but before\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 367\nP3has started. The last two bits, 01, mean that P3must complete before P1\nstarts, but we have already seen that P1must go \ufb01rst. Therefore, 00 10 01 is\nnot allowed.\nIn short, the 90 different valid statement orderings produce a variety of\ndifferent program results (less than 64, though) that are allowed under the\nassumption of sequential consistency. The contract between the processes and\nthe distributed shared data store is that the processes must accept all of these\nas valid results. In other words, the processes must accept the four results\nshown in Figure 7.7 and all the other valid results as proper answers, and\nmust work correctly if any of them occurs. A program that works for some of\nthese results and not for others violates the contract with the data store and is\nincorrect.\nNote 7.3 (Advanced: The importance and intricacies of sequential consistency)\nThere is no doubt that sequential consistency is an important model. In essence,\nof all consistency models that exist and have been developed, it is the easiest\none to understand when developing concurrent and parallel applications. This is\ndue to the fact that the model matches best our expectations when we let several\nprograms operate on shared data simultaneously. At the same time, implementing\nsequential consistency is far from trivial [Adve and Boehm, 2010]. To illustrate,\nconsider the example involving two variables xandy, shown in Figure 7.8.\nFigure 7.8: Both xand yare each handled in a sequentially consistent\nmanner, but taken together, sequential consistency is violated.\nIf we just consider the write and read operations on x, the fact that P1reads\nthe value ais perfectly consistent. The same holds for the operation R2(y)bby\nprocess P2. However, when taken together, there is no way that we can order the\nwrite operations on xandysuch that we can have R1(x)aandR2(y)b(note that\nwe need to keep the ordering as executed by each process):\nOrdering of operations Result\nW1(x)a;W1(y)a;W2(y)b;W2(x)bR1(x)bR2(y)b\nW1(x)a;W2(y)b;W1(y)a;W2(x)bR1(x)bR2(y)a\nW1(x)a;W2(y)b;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W1(x)a;W1(y)a;W2(x)bR1(x)bR2(y)a\nW2(y)b;W1(x)a;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W2(x)b;W1(x)a;W1(y)aR1(x)aR2(y)a\nIn terms of transactions, the operations carried out by P1andP2are not serializ-\nable . Our example shows that sequential consistency is not compositional : when\nhaving data items that are each kept sequentially consistent, their composition as\na set need not be so [Herlihy and Shavit, 2008]. The problem of noncompositional\nconsistency can be solved by assuming linearizability . This is best explained\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 367\nP3has started. The last two bits, 01, mean that P3must complete before P1\nstarts, but we have already seen that P1must go \ufb01rst. Therefore, 00 10 01 is\nnot allowed.\nIn short, the 90 different valid statement orderings produce a variety of\ndifferent program results (less than 64, though) that are allowed under the\nassumption of sequential consistency. The contract between the processes and\nthe distributed shared data store is that the processes must accept all of these\nas valid results. In other words, the processes must accept the four results\nshown in Figure 7.7 and all the other valid results as proper answers, and\nmust work correctly if any of them occurs. A program that works for some of\nthese results and not for others violates the contract with the data store and is\nincorrect.\nNote 7.3 (Advanced: The importance and intricacies of sequential consistency)\nThere is no doubt that sequential consistency is an important model. In essence,\nof all consistency models that exist and have been developed, it is the easiest\none to understand when developing concurrent and parallel applications. This is\ndue to the fact that the model matches best our expectations when we let several\nprograms operate on shared data simultaneously. At the same time, implementing\nsequential consistency is far from trivial [Adve and Boehm, 2010]. To illustrate,\nconsider the example involving two variables xandy, shown in Figure 7.8.\nFigure 7.8: Both xand yare each handled in a sequentially consistent\nmanner, but taken together, sequential consistency is violated.\nIf we just consider the write and read operations on x, the fact that P1reads\nthe value ais perfectly consistent. The same holds for the operation R2(y)bby\nprocess P2. However, when taken together, there is no way that we can order the\nwrite operations on xandysuch that we can have R1(x)aandR2(y)b(note that\nwe need to keep the ordering as executed by each process):\nOrdering of operations Result\nW1(x)a;W1(y)a;W2(y)b;W2(x)bR1(x)bR2(y)b\nW1(x)a;W2(y)b;W1(y)a;W2(x)bR1(x)bR2(y)a\nW1(x)a;W2(y)b;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W1(x)a;W1(y)a;W2(x)bR1(x)bR2(y)a\nW2(y)b;W1(x)a;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W2(x)b;W1(x)a;W1(y)aR1(x)aR2(y)a\nIn terms of transactions, the operations carried out by P1andP2are not serializ-\nable . Our example shows that sequential consistency is not compositional : when\nhaving data items that are each kept sequentially consistent, their composition as\na set need not be so [Herlihy and Shavit, 2008]. The problem of noncompositional\nconsistency can be solved by assuming linearizability . This is best explained\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "368 CHAPTER 7. CONSISTENCY AND REPLICATION\nby making a distinction between the start and completion of an operation, and\nassuming that it may take some time. Linearizability states that:\nEach operation should appear to take effect instantaneously at some moment\nbetween its start and completion.\nReturning to our example, Figure 7.9 shows the same set of write operations, but\nwe have now also indicated when they take place: the shaded area designates\nthe time the operation is being executed. Linearizability states that the effect of\nan operation should take place somewhere during the interval indicated by the\nshaded area. In principle, this means that at the time of completion of a write\noperation, the results should be propagated to the other data stores.\nFigure 7.9: An example of taking linearizable sequential consistency into\naccount, with only one possible outcome for xandy.\nWith that in mind, the possibilities for properly ordering becomes limited:\nOrdering of operations Result\nW1(x)a;W2(y)b;W1(y)a;W2(x)bR1(x)bR2(y)a\nW1(x)a;W2(y)b;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W1(x)a;W1(y)a;W2(x)bR1(x)bR2(y)a\nW2(y)b;W1(x)a;W2(x)b;W1(y)aR1(x)bR2(y)a\nIn particular, W2(y)bis completed before W1(y)astarts, so that ywill have the\nvalue a. Likewise, W1(x)acompletes before W2(x)bstarts, so that xwill have\nvalue b. It should not come as a surprise that implementing linearizability on\na many-core architecture may impose serious performance problems. Yet at the\nsame time, it eases programmability considerably, so a trade-off needs to be made.\nCausal consistency\nThe causal consistency model [Hutto and Ahamad, 1990] represents a weak-\nening of sequential consistency in that it makes a distinction between events\nthat are potentially causally related and those that are not. We already came\nacross causality when discussing vector timestamps in the previous chapter.\nIf event bis caused or in\ufb02uenced by an earlier event a, causality requires that\neveryone else \ufb01rst see a, then see b.\nConsider a simple interaction by means of a distributed shared database.\nSuppose that process P1writes a data item x. Then P2reads xand writes\ny. Here the reading of xand the writing of yare potentially causally related\nbecause the computation of ymay have depended on the value of xas read by\nP2(i.e., the value written by P1).\nOn the other hand, if two processes spontaneously and simultaneously\nwrite two different data items, these are not causally related. Operations that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n368 CHAPTER 7. CONSISTENCY AND REPLICATION\nby making a distinction between the start and completion of an operation, and\nassuming that it may take some time. Linearizability states that:\nEach operation should appear to take effect instantaneously at some moment\nbetween its start and completion.\nReturning to our example, Figure 7.9 shows the same set of write operations, but\nwe have now also indicated when they take place: the shaded area designates\nthe time the operation is being executed. Linearizability states that the effect of\nan operation should take place somewhere during the interval indicated by the\nshaded area. In principle, this means that at the time of completion of a write\noperation, the results should be propagated to the other data stores.\nFigure 7.9: An example of taking linearizable sequential consistency into\naccount, with only one possible outcome for xandy.\nWith that in mind, the possibilities for properly ordering becomes limited:\nOrdering of operations Result\nW1(x)a;W2(y)b;W1(y)a;W2(x)bR1(x)bR2(y)a\nW1(x)a;W2(y)b;W2(x)b;W1(y)aR1(x)bR2(y)a\nW2(y)b;W1(x)a;W1(y)a;W2(x)bR1(x)bR2(y)a\nW2(y)b;W1(x)a;W2(x)b;W1(y)aR1(x)bR2(y)a\nIn particular, W2(y)bis completed before W1(y)astarts, so that ywill have the\nvalue a. Likewise, W1(x)acompletes before W2(x)bstarts, so that xwill have\nvalue b. It should not come as a surprise that implementing linearizability on\na many-core architecture may impose serious performance problems. Yet at the\nsame time, it eases programmability considerably, so a trade-off needs to be made.\nCausal consistency\nThe causal consistency model [Hutto and Ahamad, 1990] represents a weak-\nening of sequential consistency in that it makes a distinction between events\nthat are potentially causally related and those that are not. We already came\nacross causality when discussing vector timestamps in the previous chapter.\nIf event bis caused or in\ufb02uenced by an earlier event a, causality requires that\neveryone else \ufb01rst see a, then see b.\nConsider a simple interaction by means of a distributed shared database.\nSuppose that process P1writes a data item x. Then P2reads xand writes\ny. Here the reading of xand the writing of yare potentially causally related\nbecause the computation of ymay have depended on the value of xas read by\nP2(i.e., the value written by P1).\nOn the other hand, if two processes spontaneously and simultaneously\nwrite two different data items, these are not causally related. Operations that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 369\nare not causally related are said to be concurrent .\nFor a data store to be considered causally consistent, it is necessary that\nthe store obeys the following condition:\nWrites that are potentially causally related must be seen by all processes\nin the same order. Concurrent writes may be seen in a different order on\ndifferent machines.\nAs an example of causal consistency, consider Figure 7.10. Here we have an\nevent sequence that is allowed with a causally consistent store, but which is\nforbidden with a sequentially consistent store or a strictly consistent store.\nThe thing to note is that the writes W2(x)band W1(x)care concurrent, so it is\nnot required that all processes see them in the same order.\nFigure 7.10: This sequence is allowed with a causally-consistent store, but not\nwith a sequentially consistent store.\nNow consider a second example. In Figure 7.11(a) we have W2(x)bpoten-\ntially depending on W1(x)abecause writing the value binto xmay be a result\nof a computation involving the previously read value by R2(x)a. The two\nwrites are causally related, so all processes must see them in the same order.\nTherefore, Figure 7.11(a) is incorrect. On the other hand, in Figure 7.11(b) the\nread has been removed, so W1(x)aand W2(x)bare now concurrent writes. A\ncausally consistent store does not require concurrent writes to be globally or-\ndered, so Figure 7.11(b) is correct. Note that Figure 7.11(b) re\ufb02ects a situation\nthat would not be acceptable for a sequentially consistent store.\nImplementing causal consistency requires keeping track of which processes\nhave seen which writes. There are many subtle issues to take into account. To\nillustrate, assume we replace W2(x)bin Figure 7.11(a) with W2(y)b, and like-\nwise R3(x)bwith R3(y)b, respectively. This situation is shown in Figure 7.12.\nLet us \ufb01rst look at operation R3(x). Process P3executes this operation\nafter R3(y)b. We know at this point for sure that W(x)ahappened before W(y)b.\nIn particular, W(x)a!R(x)a!W(y)b, meaning that if we are to preserve\ncausality, reading xafter reading bfrom ycan return only a. If the system\nwould return NILtoP3it would violate the preservation of causal relation-\nships.\nWhat about R4(y)? Could it return the initial value of y, namely NIL? The\nanswer is af\ufb01rmative: although we have the formal happened-before relationship\nW(x)a!W(y)b, without having read bfrom y, process P4can still justi\ufb01ably\nobserve that W(x)atook place independently from the initialization of y.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 369\nare not causally related are said to be concurrent .\nFor a data store to be considered causally consistent, it is necessary that\nthe store obeys the following condition:\nWrites that are potentially causally related must be seen by all processes\nin the same order. Concurrent writes may be seen in a different order on\ndifferent machines.\nAs an example of causal consistency, consider Figure 7.10. Here we have an\nevent sequence that is allowed with a causally consistent store, but which is\nforbidden with a sequentially consistent store or a strictly consistent store.\nThe thing to note is that the writes W2(x)band W1(x)care concurrent, so it is\nnot required that all processes see them in the same order.\nFigure 7.10: This sequence is allowed with a causally-consistent store, but not\nwith a sequentially consistent store.\nNow consider a second example. In Figure 7.11(a) we have W2(x)bpoten-\ntially depending on W1(x)abecause writing the value binto xmay be a result\nof a computation involving the previously read value by R2(x)a. The two\nwrites are causally related, so all processes must see them in the same order.\nTherefore, Figure 7.11(a) is incorrect. On the other hand, in Figure 7.11(b) the\nread has been removed, so W1(x)aand W2(x)bare now concurrent writes. A\ncausally consistent store does not require concurrent writes to be globally or-\ndered, so Figure 7.11(b) is correct. Note that Figure 7.11(b) re\ufb02ects a situation\nthat would not be acceptable for a sequentially consistent store.\nImplementing causal consistency requires keeping track of which processes\nhave seen which writes. There are many subtle issues to take into account. To\nillustrate, assume we replace W2(x)bin Figure 7.11(a) with W2(y)b, and like-\nwise R3(x)bwith R3(y)b, respectively. This situation is shown in Figure 7.12.\nLet us \ufb01rst look at operation R3(x). Process P3executes this operation\nafter R3(y)b. We know at this point for sure that W(x)ahappened before W(y)b.\nIn particular, W(x)a!R(x)a!W(y)b, meaning that if we are to preserve\ncausality, reading xafter reading bfrom ycan return only a. If the system\nwould return NILtoP3it would violate the preservation of causal relation-\nships.\nWhat about R4(y)? Could it return the initial value of y, namely NIL? The\nanswer is af\ufb01rmative: although we have the formal happened-before relationship\nW(x)a!W(y)b, without having read bfrom y, process P4can still justi\ufb01ably\nobserve that W(x)atook place independently from the initialization of y.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "370 CHAPTER 7. CONSISTENCY AND REPLICATION\n(a)\n(b)\nFigure 7.11: (a) A violation of a causally-consistent store. (b) A correct\nsequence of events in a causally-consistent store.\nFigure 7.12: A slight modi\ufb01cation of Figure 7.11(a). What should R3(x)or\nR4(y)return?\nImplementationwise, preserving causality introduces some interesting\nquestions. Consider, for example, the middleware underlying process P3\nfrom Figure 7.12. At the point that this middleware returns the value bfrom\nreading y, it must know about the relationship W(x)a!W(y)b. In other\nwords, when the most recent value of ywas propagated to P3\u2019s middleware,\nat the very least metadata on y\u2019s dependency should have been propagated\nas well. Alternatively, the propagation may have also been done together\nwith updating xatP3\u2019s node. By-and-large, the bottom line is that we need a\ndependency graph of which operation is dependent on which other operations.\nSuch a graph may be pruned at the moment that dependent data is also locally\nstored.\nGrouping operations\nMany consistency models are de\ufb01ned at the level of elementary read and write\noperations. This level of granularity is for historical reasons: these models\nhave initially been developed for shared-memory multiprocessor systems and\nwere actually implemented at the hardware level.\nThe \ufb01ne granularity of these consistency models in many cases does not\nmatch the granularity as provided by applications. What we see there is that\nconcurrency between programs sharing data is generally kept under control\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n370 CHAPTER 7. CONSISTENCY AND REPLICATION\n(a)\n(b)\nFigure 7.11: (a) A violation of a causally-consistent store. (b) A correct\nsequence of events in a causally-consistent store.\nFigure 7.12: A slight modi\ufb01cation of Figure 7.11(a). What should R3(x)or\nR4(y)return?\nImplementationwise, preserving causality introduces some interesting\nquestions. Consider, for example, the middleware underlying process P3\nfrom Figure 7.12. At the point that this middleware returns the value bfrom\nreading y, it must know about the relationship W(x)a!W(y)b. In other\nwords, when the most recent value of ywas propagated to P3\u2019s middleware,\nat the very least metadata on y\u2019s dependency should have been propagated\nas well. Alternatively, the propagation may have also been done together\nwith updating xatP3\u2019s node. By-and-large, the bottom line is that we need a\ndependency graph of which operation is dependent on which other operations.\nSuch a graph may be pruned at the moment that dependent data is also locally\nstored.\nGrouping operations\nMany consistency models are de\ufb01ned at the level of elementary read and write\noperations. This level of granularity is for historical reasons: these models\nhave initially been developed for shared-memory multiprocessor systems and\nwere actually implemented at the hardware level.\nThe \ufb01ne granularity of these consistency models in many cases does not\nmatch the granularity as provided by applications. What we see there is that\nconcurrency between programs sharing data is generally kept under control\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 371\nthrough synchronization mechanisms for mutual exclusion and transactions.\nEffectively, what happens is that at the program level read and write operations\nare bracketed by the pair of operations ENTER_CS and LEAVE_CS . A process that\nhas successfully executed ENTER_CS will be ensured that all the data in its local\nstore is up to date. At that point, it can safely execute a series of read and\nwrite operations on that store, and subsequently wrap things up by calling\nLEAVE_CS . Data and instructions between ENTER_CS and LEAVE_CS is denoted\nas a critical section .\nIn essence, what happens is that within a program the data that are\noperated on by a series of read and write operations are protected against\nconcurrent accesses that would lead to seeing something else than the result\nof executing the series as a whole. Put differently, the bracketing turns the\nseries of read and write operations into an atomically executed unit, thus\nraising the level of granularity.\nIn order to reach this point, we do need to have precise semantics concern-\ning the operations ENTER_CS and LEAVE_CS . These semantics can be formulated\nin terms of shared synchronization variables , or simply locks . A lock has\nshared data items associated with it, and each shared data item is associated\nwith at most one lock. In the case of course-grained synchronization, all\nshared data items would be associated to just a single lock. Fine-grained\nsynchronization is achieved when each shared data item has its own unique\nlock. Of course, these are just two extremes of associating shared data to a\nlock. When a process enters a critical section it should acquire the relevant\nlocks, and likewise when it leaves the critical section, it releases these locks.\nEach lock has a current owner, namely, the process that last acquired it. A\nprocess not currently owning a lock but wanting to acquire it has to send a\nmessage to the current owner asking for ownership and the current values of\nthe data associated with that lock. While having exclusive access to a lock, a\nprocess is allowed to perform read and write operations. It is also possible for\nseveral processes to simultaneously have nonexclusive access to a lock, meaning\nthat they can read, but not write, the associated data. Of course, nonexclusive\naccess can be granted if and only if there is no other process having exclusive\naccess.\nWe now demand that the following criteria are met [Bershad et al., 1993]:\n\u2022Acquiring a lock can succeed only when all updates to its associated\nshared data have completed.\n\u2022Exclusive access to a lock can succeed only if no other process has\nexclusive or nonexclusive access to that lock.\n\u2022Nonexclusive access to a lock is allowed only if any previous exclusive\naccess has been completed, including updates on the lock\u2019s associated\ndata.\nNote that we are effectively demanding that the usage of locks is linearized,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 371\nthrough synchronization mechanisms for mutual exclusion and transactions.\nEffectively, what happens is that at the program level read and write operations\nare bracketed by the pair of operations ENTER_CS and LEAVE_CS . A process that\nhas successfully executed ENTER_CS will be ensured that all the data in its local\nstore is up to date. At that point, it can safely execute a series of read and\nwrite operations on that store, and subsequently wrap things up by calling\nLEAVE_CS . Data and instructions between ENTER_CS and LEAVE_CS is denoted\nas a critical section .\nIn essence, what happens is that within a program the data that are\noperated on by a series of read and write operations are protected against\nconcurrent accesses that would lead to seeing something else than the result\nof executing the series as a whole. Put differently, the bracketing turns the\nseries of read and write operations into an atomically executed unit, thus\nraising the level of granularity.\nIn order to reach this point, we do need to have precise semantics concern-\ning the operations ENTER_CS and LEAVE_CS . These semantics can be formulated\nin terms of shared synchronization variables , or simply locks . A lock has\nshared data items associated with it, and each shared data item is associated\nwith at most one lock. In the case of course-grained synchronization, all\nshared data items would be associated to just a single lock. Fine-grained\nsynchronization is achieved when each shared data item has its own unique\nlock. Of course, these are just two extremes of associating shared data to a\nlock. When a process enters a critical section it should acquire the relevant\nlocks, and likewise when it leaves the critical section, it releases these locks.\nEach lock has a current owner, namely, the process that last acquired it. A\nprocess not currently owning a lock but wanting to acquire it has to send a\nmessage to the current owner asking for ownership and the current values of\nthe data associated with that lock. While having exclusive access to a lock, a\nprocess is allowed to perform read and write operations. It is also possible for\nseveral processes to simultaneously have nonexclusive access to a lock, meaning\nthat they can read, but not write, the associated data. Of course, nonexclusive\naccess can be granted if and only if there is no other process having exclusive\naccess.\nWe now demand that the following criteria are met [Bershad et al., 1993]:\n\u2022Acquiring a lock can succeed only when all updates to its associated\nshared data have completed.\n\u2022Exclusive access to a lock can succeed only if no other process has\nexclusive or nonexclusive access to that lock.\n\u2022Nonexclusive access to a lock is allowed only if any previous exclusive\naccess has been completed, including updates on the lock\u2019s associated\ndata.\nNote that we are effectively demanding that the usage of locks is linearized,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "372 CHAPTER 7. CONSISTENCY AND REPLICATION\nadhering to sequential consistency. Figure 7.13 shows an example of what\nis known as entry consistency . We associate a lock with each data item\nseparately. We use the notation L(x)as an abbreviation for acquiring the lock\nforx, that is, locking x. Likewise, U(x)stands for releasing the lock on x, or\nunlocking it. In this case, P1locks x, changes xonce, after which it locks y.\nProcess P2also acquires the lock for xbut not for y, so that it will read value\naforx, but may read NILfory. However, because process P3\ufb01rst acquires the\nlock for y, it will read the value bwhen ywas unlocked by P1. It is important\nto note here that each process has a copy of a variable, but that this copy\nneed not be instantly or automatically updated. When locking or unlocking a\nvariable, a process is explicitly telling the underlying distributed system that\nthe copies of that variable need to be synchronized. A simple read operation\nwithout locking may thus result in reading a local value that is effectively\nstale.\nFigure 7.13: A valid event sequence for entry consistency.\nOne of the programming problems with entry consistency is properly\nassociating data with locks. One straightforward approach is to explicitly tell\nthe middleware which data are going to be accessed, as is generally done\nby declaring which database tables will be affected by a transaction. In an\nobject-based approach, we could associate a unique lock with each declared\nobject, effectively serializing all invocations to such objects.\nConsistency versus coherence\nAt this point, it is useful to clarify the difference between two closely related\nconcepts. The models we have discussed so far all deal with the fact that a\nnumber of processes execute read and write operations on a set of data items.\nAconsistency model describes what can be expected with respect to that set\nwhen multiple processes concurrently operate on that data. The set is then\nsaid to be consistent if it adheres to the rules described by the model.\nWhere data consistency is concerned with a set of data items, coher-\nence models describe what can be expected to hold for only a single data\nitem [Cantin et al., 2005]. In this case, we assume that a data item is replicated;\nit is said to be coherent when the various copies abide to the rules as de\ufb01ned\nby its associated consistency model. A popular model is that of sequential\nconsistency, but now applied to only a single data item. In effect, it means\nthat in the case of concurrent writes, all processes will eventually see the same\norder of updates taking place.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n372 CHAPTER 7. CONSISTENCY AND REPLICATION\nadhering to sequential consistency. Figure 7.13 shows an example of what\nis known as entry consistency . We associate a lock with each data item\nseparately. We use the notation L(x)as an abbreviation for acquiring the lock\nforx, that is, locking x. Likewise, U(x)stands for releasing the lock on x, or\nunlocking it. In this case, P1locks x, changes xonce, after which it locks y.\nProcess P2also acquires the lock for xbut not for y, so that it will read value\naforx, but may read NILfory. However, because process P3\ufb01rst acquires the\nlock for y, it will read the value bwhen ywas unlocked by P1. It is important\nto note here that each process has a copy of a variable, but that this copy\nneed not be instantly or automatically updated. When locking or unlocking a\nvariable, a process is explicitly telling the underlying distributed system that\nthe copies of that variable need to be synchronized. A simple read operation\nwithout locking may thus result in reading a local value that is effectively\nstale.\nFigure 7.13: A valid event sequence for entry consistency.\nOne of the programming problems with entry consistency is properly\nassociating data with locks. One straightforward approach is to explicitly tell\nthe middleware which data are going to be accessed, as is generally done\nby declaring which database tables will be affected by a transaction. In an\nobject-based approach, we could associate a unique lock with each declared\nobject, effectively serializing all invocations to such objects.\nConsistency versus coherence\nAt this point, it is useful to clarify the difference between two closely related\nconcepts. The models we have discussed so far all deal with the fact that a\nnumber of processes execute read and write operations on a set of data items.\nAconsistency model describes what can be expected with respect to that set\nwhen multiple processes concurrently operate on that data. The set is then\nsaid to be consistent if it adheres to the rules described by the model.\nWhere data consistency is concerned with a set of data items, coher-\nence models describe what can be expected to hold for only a single data\nitem [Cantin et al., 2005]. In this case, we assume that a data item is replicated;\nit is said to be coherent when the various copies abide to the rules as de\ufb01ned\nby its associated consistency model. A popular model is that of sequential\nconsistency, but now applied to only a single data item. In effect, it means\nthat in the case of concurrent writes, all processes will eventually see the same\norder of updates taking place.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.2. DATA-CENTRIC CONSISTENCY MODELS 373\nEventual consistency\nTo what extent processes actually operate in a concurrent fashion, and to\nwhat extent consistency needs to be guaranteed, may vary. There are many\nexamples in which concurrency appears only in a restricted form. For exam-\nple, in many database systems, most processes hardly ever perform update\noperations; they mostly read data from the database. Only one, or very few\nprocesses perform update operations. The question then is how fast updates\nshould be made available to only-reading processes. In the advent of globally\noperating content delivery networks, developers often choose to propagate\nupdates slowly, implicitly assuming that most clients are always redirected to\nthe same replica and will therefore never experience inconsistencies.\nAnother example is the Web. In virtually all cases, Web pages are updated\nby a single authority, such as a webmaster or the actual owner of the page.\nThere are normally no write-write con\ufb02icts to resolve. On the other hand, to\nimprove ef\ufb01ciency, browsers and Web proxies are often con\ufb01gured to keep a\nfetched page in a local cache and to return that page upon the next request.\nAn important aspect of both types of Web caches is that they may return\nout-of-date Web pages. In other words, the cached page that is returned to\nthe requesting client is an older version compared to the one available at\nthe actual Web server. As it turns out, many users \ufb01nd this inconsistency\nacceptable (to a certain degree), as long as they have access only to the same\ncache. In effect, they remain unaware of the fact that an update had taken\nplace, just as in the previous case of content delivery networks.\nYet another example, is a worldwide naming system such as DNS. The DNS\nname space is partitioned into domains, where each domain is assigned to a\nnaming authority, which acts as owner of that domain. Only that authority is\nallowed to update its part of the name space. Consequently, con\ufb02icts resulting\nfrom two operations that both want to perform an update on the same data\n(i.e., write-write con\ufb02icts ), never occur. The only situation that needs to be\nhandled are read-write con\ufb02icts , in which one process wants to update a data\nitem while another is concurrently attempting to read that item. As it turns\nout, also in this case is it often acceptable to propagate an update in a lazy\nfashion, meaning that a reading process will see an update only after some\ntime has passed since the update took place.\nThese examples can be viewed as cases of (large scale) distributed and\nreplicated databases that tolerate a relatively high degree of inconsistency.\nThey have in common that if no updates take place for a long time, all replicas\nwill gradually become consistent, that is, have exactly the same data stored.\nThis form of consistency is called eventual consistency [Vogels, 2009].\nData stores that are eventually consistent thus have the property that in\nthe absence of write-write con\ufb02icts, all replicas will converge toward identical\ncopies of each other. Eventual consistency essentially requires only that\nupdates are guaranteed to propagate to all replicas. Write-write con\ufb02icts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.2. DATA-CENTRIC CONSISTENCY MODELS 373\nEventual consistency\nTo what extent processes actually operate in a concurrent fashion, and to\nwhat extent consistency needs to be guaranteed, may vary. There are many\nexamples in which concurrency appears only in a restricted form. For exam-\nple, in many database systems, most processes hardly ever perform update\noperations; they mostly read data from the database. Only one, or very few\nprocesses perform update operations. The question then is how fast updates\nshould be made available to only-reading processes. In the advent of globally\noperating content delivery networks, developers often choose to propagate\nupdates slowly, implicitly assuming that most clients are always redirected to\nthe same replica and will therefore never experience inconsistencies.\nAnother example is the Web. In virtually all cases, Web pages are updated\nby a single authority, such as a webmaster or the actual owner of the page.\nThere are normally no write-write con\ufb02icts to resolve. On the other hand, to\nimprove ef\ufb01ciency, browsers and Web proxies are often con\ufb01gured to keep a\nfetched page in a local cache and to return that page upon the next request.\nAn important aspect of both types of Web caches is that they may return\nout-of-date Web pages. In other words, the cached page that is returned to\nthe requesting client is an older version compared to the one available at\nthe actual Web server. As it turns out, many users \ufb01nd this inconsistency\nacceptable (to a certain degree), as long as they have access only to the same\ncache. In effect, they remain unaware of the fact that an update had taken\nplace, just as in the previous case of content delivery networks.\nYet another example, is a worldwide naming system such as DNS. The DNS\nname space is partitioned into domains, where each domain is assigned to a\nnaming authority, which acts as owner of that domain. Only that authority is\nallowed to update its part of the name space. Consequently, con\ufb02icts resulting\nfrom two operations that both want to perform an update on the same data\n(i.e., write-write con\ufb02icts ), never occur. The only situation that needs to be\nhandled are read-write con\ufb02icts , in which one process wants to update a data\nitem while another is concurrently attempting to read that item. As it turns\nout, also in this case is it often acceptable to propagate an update in a lazy\nfashion, meaning that a reading process will see an update only after some\ntime has passed since the update took place.\nThese examples can be viewed as cases of (large scale) distributed and\nreplicated databases that tolerate a relatively high degree of inconsistency.\nThey have in common that if no updates take place for a long time, all replicas\nwill gradually become consistent, that is, have exactly the same data stored.\nThis form of consistency is called eventual consistency [Vogels, 2009].\nData stores that are eventually consistent thus have the property that in\nthe absence of write-write con\ufb02icts, all replicas will converge toward identical\ncopies of each other. Eventual consistency essentially requires only that\nupdates are guaranteed to propagate to all replicas. Write-write con\ufb02icts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "374 CHAPTER 7. CONSISTENCY AND REPLICATION\nare often relatively easy to solve when assuming that only a small group of\nprocesses can perform updates. In practice, we often also see that in the case\nof con\ufb02icts, one speci\ufb01c write operation is (globally) declared as \u201cwinner,\u201d\noverwriting the effects of any other con\ufb02icting write operation. Eventual\nconsistency is therefore often cheap to implement.\nNote 7.4 (Advanced: Making eventual consistency stronger)\nEventual consistency is a relatively easy model to understand, but equally im-\nportant is the fact that it is also relatively easy to implement. Nevertheless, it\nis a weak-consistency model with its own peculiarities. Consider a calendar\nshared between Alice, Bob, and Chuck. A meeting Mhas two attributes: a pro-\nposed starting time and a set of people who have con\ufb01rmed their attendance.\nWhen Alice proposes to start meeting Mat time T, and assuming no one else\nhas con\ufb01rmed attendance, she executes the operation WA(M)[T,fAg]. When\nBob con\ufb01rms his attendance, he will have read the tuple [T,fAg]and update M\naccordingly: WB(M)[T,fA,Bg]. In our example two meetings M1andM2need to\nbe planned.\nAssume the sequence of events\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\nWB(M1)[T1,fA,Bg]!WB(M2)[T2,fBg].\nIn other words, Bob con\ufb01rms his attendance at M1and then immediately proposes\nto schedule M2atT2. Unfortunately, Chuck concurrently proposes to schedule M1\natT3when Bob con\ufb01rms he can attend M1atT1. Formally, using the symbol \u201c k\u201d\nto denote concurrent operations, we have,\nWB(M1)[T1,fA,Bg]kWC(M1)[T3,fCg]\nUsing our usual notation, these operations can be illustrated as shown in Fig-\nure 7.14.\nFigure 7.14: The situation of updating two meetings M1andM2.\nEventual consistency may lead to very different scenarios. There is a number\nof write-write con\ufb02icts, but in any case, eventually [T2,fBg]will be stored for\nmeeting M2, as the result of the associated write operation by Bob. For the value\nof meeting M1there are different options. In principle, we have three possible\noutcomes: [T1,fAg],[T1,fA,Bg], and [T3,fCg]. Assuming we can maintain some\nnotion of a global clock, it is not very likely that WA(M1)[T1,fAg]will prevail.\nHowever, the two write operations WB(M1)[T1,fA,Bg]andWC(M1)[T1,fCg]are\ntruly in con\ufb02ict. In practice, one of them will win, presumably through a decision\nby a central coordinator.\nResearchers have been seeking to combine eventual consistency with stricter\nguarantees on ordering. Bailis et al. [2013] propose to use a separate layer\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n374 CHAPTER 7. CONSISTENCY AND REPLICATION\nare often relatively easy to solve when assuming that only a small group of\nprocesses can perform updates. In practice, we often also see that in the case\nof con\ufb02icts, one speci\ufb01c write operation is (globally) declared as \u201cwinner,\u201d\noverwriting the effects of any other con\ufb02icting write operation. Eventual\nconsistency is therefore often cheap to implement.\nNote 7.4 (Advanced: Making eventual consistency stronger)\nEventual consistency is a relatively easy model to understand, but equally im-\nportant is the fact that it is also relatively easy to implement. Nevertheless, it\nis a weak-consistency model with its own peculiarities. Consider a calendar\nshared between Alice, Bob, and Chuck. A meeting Mhas two attributes: a pro-\nposed starting time and a set of people who have con\ufb01rmed their attendance.\nWhen Alice proposes to start meeting Mat time T, and assuming no one else\nhas con\ufb01rmed attendance, she executes the operation WA(M)[T,fAg]. When\nBob con\ufb01rms his attendance, he will have read the tuple [T,fAg]and update M\naccordingly: WB(M)[T,fA,Bg]. In our example two meetings M1andM2need to\nbe planned.\nAssume the sequence of events\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\nWB(M1)[T1,fA,Bg]!WB(M2)[T2,fBg].\nIn other words, Bob con\ufb01rms his attendance at M1and then immediately proposes\nto schedule M2atT2. Unfortunately, Chuck concurrently proposes to schedule M1\natT3when Bob con\ufb01rms he can attend M1atT1. Formally, using the symbol \u201c k\u201d\nto denote concurrent operations, we have,\nWB(M1)[T1,fA,Bg]kWC(M1)[T3,fCg]\nUsing our usual notation, these operations can be illustrated as shown in Fig-\nure 7.14.\nFigure 7.14: The situation of updating two meetings M1andM2.\nEventual consistency may lead to very different scenarios. There is a number\nof write-write con\ufb02icts, but in any case, eventually [T2,fBg]will be stored for\nmeeting M2, as the result of the associated write operation by Bob. For the value\nof meeting M1there are different options. In principle, we have three possible\noutcomes: [T1,fAg],[T1,fA,Bg], and [T3,fCg]. Assuming we can maintain some\nnotion of a global clock, it is not very likely that WA(M1)[T1,fAg]will prevail.\nHowever, the two write operations WB(M1)[T1,fA,Bg]andWC(M1)[T1,fCg]are\ntruly in con\ufb02ict. In practice, one of them will win, presumably through a decision\nby a central coordinator.\nResearchers have been seeking to combine eventual consistency with stricter\nguarantees on ordering. Bailis et al. [2013] propose to use a separate layer\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.3. CLIENT-CENTRIC CONSISTENCY MODELS 375\nthat operates on top of an eventually consistent, distributed store. This layer\nimplements causal consistency, of which it has been formerly proven that it is the\nbest attainable consistency in the presence of network partitioning [Mahajan et al.,\n2011]. In our example, we have only one chain of dependencies:\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\nWB(M1)[T1,fA,Bg]!WB(M2)[T2,fBg].\nAn important observation is that with causal consistency in place, once a pro-\ncess reads [T2,fBg]for meeting M2, obtaining the value for M1returns ei-\nther[T1,fA,Bg]or[T3,fCg], but certainly not [T1,fAg]. The reason is that\nW(M1)[T1,fA,Bg]immediately precedes W(M2)[T2,fBg], and at worse may\nhave been overwritten by W(M1)[T3,fCg]. Causal consistency rules out that the\nsystem could return [T1,fAg].\nHowever, eventual consistency may overwrite previously stored data items.\nIn doing so, dependencies may be lost. To make this point clear, it is impor-\ntant to realize that in practice an operation at best keeps track of the immedi-\nate preceding operation it depends on. As soon as Wc(M1)[T3,fCg]overwrites\nWB(M1)[T1,fA,Bg](and propagates to all replicas), we also break the chain of\ndependencies\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\u0001\u0001\u0001! WB(M2)[T2,fBg]\nwhich would normally prevent WA(M1)[T1,fAg]ever overtaking\nWB(M1)[T1,fA,Bg]and any operation depending on it. As a consequence,\nmaintaining causal consistency requires that we do maintain a history of\ndependencies, instead of just keeping track of immediately preceding operations.\n7.3 Client-centric consistency models\nData-centric consistency models aim at providing a systemwide consistent\nview on a data store. An important assumption is that concurrent processes\nmay be simultaneously updating the data store, and that it is necessary to\nprovide consistency in the face of such concurrency. For example, in the case\nof object-based entry consistency, the data store guarantees that when an\nobject is called, the calling process is provided with a copy of the object that\nre\ufb02ects all changes to the object that have been made so far, possibly by other\nprocesses. During the call, it is also guaranteed that no other process can\ninterfere, that is, mutual exclusive access is provided to the calling process.\nBeing able to handle concurrent operations on shared data while maintain-\ning strong consistency is fundamental to distributed systems. For performance\nreasons, strong consistency may possibly be guaranteed only when processes\nuse mechanisms such as transactions or synchronization variables. Along the\nsame lines, it may be impossible to guarantee strong consistency, and weaker\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.3. CLIENT-CENTRIC CONSISTENCY MODELS 375\nthat operates on top of an eventually consistent, distributed store. This layer\nimplements causal consistency, of which it has been formerly proven that it is the\nbest attainable consistency in the presence of network partitioning [Mahajan et al.,\n2011]. In our example, we have only one chain of dependencies:\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\nWB(M1)[T1,fA,Bg]!WB(M2)[T2,fBg].\nAn important observation is that with causal consistency in place, once a pro-\ncess reads [T2,fBg]for meeting M2, obtaining the value for M1returns ei-\nther[T1,fA,Bg]or[T3,fCg], but certainly not [T1,fAg]. The reason is that\nW(M1)[T1,fA,Bg]immediately precedes W(M2)[T2,fBg], and at worse may\nhave been overwritten by W(M1)[T3,fCg]. Causal consistency rules out that the\nsystem could return [T1,fAg].\nHowever, eventual consistency may overwrite previously stored data items.\nIn doing so, dependencies may be lost. To make this point clear, it is impor-\ntant to realize that in practice an operation at best keeps track of the immedi-\nate preceding operation it depends on. As soon as Wc(M1)[T3,fCg]overwrites\nWB(M1)[T1,fA,Bg](and propagates to all replicas), we also break the chain of\ndependencies\nWA(M1)[T1,fAg]!RB(M1)[T1,fAg]!\u0001\u0001\u0001! WB(M2)[T2,fBg]\nwhich would normally prevent WA(M1)[T1,fAg]ever overtaking\nWB(M1)[T1,fA,Bg]and any operation depending on it. As a consequence,\nmaintaining causal consistency requires that we do maintain a history of\ndependencies, instead of just keeping track of immediately preceding operations.\n7.3 Client-centric consistency models\nData-centric consistency models aim at providing a systemwide consistent\nview on a data store. An important assumption is that concurrent processes\nmay be simultaneously updating the data store, and that it is necessary to\nprovide consistency in the face of such concurrency. For example, in the case\nof object-based entry consistency, the data store guarantees that when an\nobject is called, the calling process is provided with a copy of the object that\nre\ufb02ects all changes to the object that have been made so far, possibly by other\nprocesses. During the call, it is also guaranteed that no other process can\ninterfere, that is, mutual exclusive access is provided to the calling process.\nBeing able to handle concurrent operations on shared data while maintain-\ning strong consistency is fundamental to distributed systems. For performance\nreasons, strong consistency may possibly be guaranteed only when processes\nuse mechanisms such as transactions or synchronization variables. Along the\nsame lines, it may be impossible to guarantee strong consistency, and weaker\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "376 CHAPTER 7. CONSISTENCY AND REPLICATION\nforms need to be accepted, such as causal consistency in combination with\neventual consistency.\nIn this section, we take a look at a special class of distributed data stores.\nThe data stores we consider are characterized by the lack of simultaneous\nupdates, or when such updates happen, it is assumed that they can be\nrelatively easily resolved. Most operations involve reading data. These data\nstores offer a weak consistency model, such as eventual consistency. By\nintroducing special client-centric consistency models, it turns out that many\ninconsistencies can be hidden in a relatively cheap way.\nFigure 7.15: The principle of a mobile user accessing different replicas of a\ndistributed database.\nEventually consistent data stores generally work \ufb01ne as long as clients\nalways access the same replica. However, problems arise when different\nreplicas are accessed over a short period of time. This is best illustrated\nby considering a mobile user accessing a distributed database, as shown in\nFigure 7.15.\nThe mobile user, say, Alice, accesses the database by connecting to one\nof the replicas in a transparent way. In other words, the application running\non Alice\u2019s mobile device is unaware on which replica it is actually operating.\nAssume Alice performs several update operations and then disconnects again.\nLater, she accesses the database again, possibly after moving to a different\nlocation or by using a different access device. At that point, she may be\nconnected to a different replica than before, as shown in Figure 7.15. However,\nif the updates performed previously have not yet been propagated, Alice\nwill notice inconsistent behavior. In particular, she would expect to see all\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n376 CHAPTER 7. CONSISTENCY AND REPLICATION\nforms need to be accepted, such as causal consistency in combination with\neventual consistency.\nIn this section, we take a look at a special class of distributed data stores.\nThe data stores we consider are characterized by the lack of simultaneous\nupdates, or when such updates happen, it is assumed that they can be\nrelatively easily resolved. Most operations involve reading data. These data\nstores offer a weak consistency model, such as eventual consistency. By\nintroducing special client-centric consistency models, it turns out that many\ninconsistencies can be hidden in a relatively cheap way.\nFigure 7.15: The principle of a mobile user accessing different replicas of a\ndistributed database.\nEventually consistent data stores generally work \ufb01ne as long as clients\nalways access the same replica. However, problems arise when different\nreplicas are accessed over a short period of time. This is best illustrated\nby considering a mobile user accessing a distributed database, as shown in\nFigure 7.15.\nThe mobile user, say, Alice, accesses the database by connecting to one\nof the replicas in a transparent way. In other words, the application running\non Alice\u2019s mobile device is unaware on which replica it is actually operating.\nAssume Alice performs several update operations and then disconnects again.\nLater, she accesses the database again, possibly after moving to a different\nlocation or by using a different access device. At that point, she may be\nconnected to a different replica than before, as shown in Figure 7.15. However,\nif the updates performed previously have not yet been propagated, Alice\nwill notice inconsistent behavior. In particular, she would expect to see all\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.3. CLIENT-CENTRIC CONSISTENCY MODELS 377\npreviously made changes, but instead, it appears as if nothing at all has\nhappened.\nThis example is typical for eventually consistent data stores and is caused\nby the fact that users may sometimes operate on different replicas while\nupdates have not been fully propagated. The problem can be alleviated by\nintroducing client-centric consistency . In essence, client-centric consistency\nprovides guarantees for a single client concerning the consistency of accesses\nto a data store by that client. No guarantees are given concerning concurrent\naccesses by different clients. If Bob modi\ufb01es data that is shared with Alice\nbut which is stored at a different location, we may easily create write-write\ncon\ufb02icts. Moreover, if neither Alice nor Bob access the same location for some\ntime, such con\ufb02icts may take a long time before they are discovered.\nClient-centric consistency models originate from the work on Bayou and,\nmore general, from mobile-data systems (see, for example, Terry et al. [1994],\nTerry et al. [1998], or Terry [2008]). Bayou is a database system developed\nfor mobile computing, where it is assumed that network connectivity is\nunreliable and subject to various performance problems. Wireless networks\nand networks that span large areas, such as the Internet, fall into this category.\nBayou essentially distinguishes four different consistency models. To ex-\nplain these models, we again consider a data store that is physically distributed\nacross multiple machines. When a process accesses the data store, it generally\nconnects to the locally (or nearest) available copy, although, in principle, any\ncopy will do just \ufb01ne. All read and write operations are performed on that\nlocal copy. Updates are eventually propagated to the other copies.\nClient-centric consistency models are described using the following nota-\ntions. Let xidenote the version of data item x. Version xiis the result of a series\nof write operations that took place since initialization, its write set WS(xi). By\nappending write operations to that series we obtain another version xjand say\nthat xjfollows from xi. We use the notation WS(xi;xj)to indicate that xjfollows\nfrom xi. If we do not know if xjfollows from xi, we use the notation WS(xijxj).\nMonotonic reads\nThe \ufb01rst client-centric consistency model is that of monotonic reads. A\n(distributed) data store is said to provide monotonic-read consistency if the\nfollowing condition holds:\nIf a process reads the value of a data item x, any successive read operation\nonxby that process will always return that same value or a more recent\nvalue.\nIn other words, monotonic-read consistency guarantees that once a process\nhas seen a value of x, it will never see an older version of x.\nAs an example where monotonic reads are useful, consider a distributed\ne-mail database. In such a database, each user\u2019s mailbox may be distributed\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.3. CLIENT-CENTRIC CONSISTENCY MODELS 377\npreviously made changes, but instead, it appears as if nothing at all has\nhappened.\nThis example is typical for eventually consistent data stores and is caused\nby the fact that users may sometimes operate on different replicas while\nupdates have not been fully propagated. The problem can be alleviated by\nintroducing client-centric consistency . In essence, client-centric consistency\nprovides guarantees for a single client concerning the consistency of accesses\nto a data store by that client. No guarantees are given concerning concurrent\naccesses by different clients. If Bob modi\ufb01es data that is shared with Alice\nbut which is stored at a different location, we may easily create write-write\ncon\ufb02icts. Moreover, if neither Alice nor Bob access the same location for some\ntime, such con\ufb02icts may take a long time before they are discovered.\nClient-centric consistency models originate from the work on Bayou and,\nmore general, from mobile-data systems (see, for example, Terry et al. [1994],\nTerry et al. [1998], or Terry [2008]). Bayou is a database system developed\nfor mobile computing, where it is assumed that network connectivity is\nunreliable and subject to various performance problems. Wireless networks\nand networks that span large areas, such as the Internet, fall into this category.\nBayou essentially distinguishes four different consistency models. To ex-\nplain these models, we again consider a data store that is physically distributed\nacross multiple machines. When a process accesses the data store, it generally\nconnects to the locally (or nearest) available copy, although, in principle, any\ncopy will do just \ufb01ne. All read and write operations are performed on that\nlocal copy. Updates are eventually propagated to the other copies.\nClient-centric consistency models are described using the following nota-\ntions. Let xidenote the version of data item x. Version xiis the result of a series\nof write operations that took place since initialization, its write set WS(xi). By\nappending write operations to that series we obtain another version xjand say\nthat xjfollows from xi. We use the notation WS(xi;xj)to indicate that xjfollows\nfrom xi. If we do not know if xjfollows from xi, we use the notation WS(xijxj).\nMonotonic reads\nThe \ufb01rst client-centric consistency model is that of monotonic reads. A\n(distributed) data store is said to provide monotonic-read consistency if the\nfollowing condition holds:\nIf a process reads the value of a data item x, any successive read operation\nonxby that process will always return that same value or a more recent\nvalue.\nIn other words, monotonic-read consistency guarantees that once a process\nhas seen a value of x, it will never see an older version of x.\nAs an example where monotonic reads are useful, consider a distributed\ne-mail database. In such a database, each user\u2019s mailbox may be distributed\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "378 CHAPTER 7. CONSISTENCY AND REPLICATION\nand replicated across multiple machines. Mail can be inserted in a mailbox\nat any location. However, updates are propagated in a lazy (i.e., on demand)\nfashion. Only when a copy needs certain data for consistency are those data\npropagated to that copy. Suppose a user reads his mail in San Francisco.\nAssume that only reading mail does not affect the mailbox, that is, messages\nare not removed, stored in subdirectories, or even tagged as having already\nbeen read, and so on. When the user later \ufb02ies to New York and opens his\nmailbox again, monotonic-read consistency guarantees that the messages that\nwere in the mailbox in San Francisco will also be in the mailbox when it is\nopened in New York.\nUsing a notation similar to that for data-centric consistency models,\nmonotonic-read consistency can be graphically represented as shown in Fig-\nure 7.16. Rather than showing processes along the vertical axis, we now show\nlocal data stores , in our example L1and L2. A write or read operation is in-\ndexed by the process that executed the operation, that is, W1(x)adenotes that\nprocess P1wrote value atox. As we are not interested in speci\ufb01c values\nof shared data items, but rather their versions, we use the notation W1(x2)\nto indicate that process P1produces version x2without knowing anything\nabout other versions. W2(x1;x2)indicates that process P2is responsible for\nproducing version x2that follows from x1. Likewise, W2(x1jx2)denotes that\nprocess P2producing version x2concurrently to version x1(and thus poten-\ntially introducing a write-write con\ufb02ict). R1(x2)simply means that P1reads\nversion x2.\n(a) (b)\nFigure 7.16: The read operations performed by a single process Pat two\ndifferent local copies of the same data store. (a) A monotonic-read consistent\ndata store. (b) A data store that does not provide monotonic reads.\nIn Figure 7.16(a) process P1\ufb01rst performs a write operation on xatL1,\nproducing version x1and later reads this version. At L2process P2\ufb01rst\nproduces version x2, following from x1. When process P1moves to L2and\nreads xagain, it \ufb01nds a more recent value, but one that at least took its\nprevious write into account.\nFigure 7.16(b) shows a situation in which monotonic-read consistency is\nviolated. After process P1has read x1atL1, it later performs the operation\nR1(x2)atL2. However, the preceding write operation W2(x1jx2)by process\nP2atL2is known to produce a version that does not follow from x1. As a\nconsequence, P1\u2019s read operation at L2is known not to include the effect of\nthe write operations when it performed R1(x1)at location L1.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n378 CHAPTER 7. CONSISTENCY AND REPLICATION\nand replicated across multiple machines. Mail can be inserted in a mailbox\nat any location. However, updates are propagated in a lazy (i.e., on demand)\nfashion. Only when a copy needs certain data for consistency are those data\npropagated to that copy. Suppose a user reads his mail in San Francisco.\nAssume that only reading mail does not affect the mailbox, that is, messages\nare not removed, stored in subdirectories, or even tagged as having already\nbeen read, and so on. When the user later \ufb02ies to New York and opens his\nmailbox again, monotonic-read consistency guarantees that the messages that\nwere in the mailbox in San Francisco will also be in the mailbox when it is\nopened in New York.\nUsing a notation similar to that for data-centric consistency models,\nmonotonic-read consistency can be graphically represented as shown in Fig-\nure 7.16. Rather than showing processes along the vertical axis, we now show\nlocal data stores , in our example L1and L2. A write or read operation is in-\ndexed by the process that executed the operation, that is, W1(x)adenotes that\nprocess P1wrote value atox. As we are not interested in speci\ufb01c values\nof shared data items, but rather their versions, we use the notation W1(x2)\nto indicate that process P1produces version x2without knowing anything\nabout other versions. W2(x1;x2)indicates that process P2is responsible for\nproducing version x2that follows from x1. Likewise, W2(x1jx2)denotes that\nprocess P2producing version x2concurrently to version x1(and thus poten-\ntially introducing a write-write con\ufb02ict). R1(x2)simply means that P1reads\nversion x2.\n(a) (b)\nFigure 7.16: The read operations performed by a single process Pat two\ndifferent local copies of the same data store. (a) A monotonic-read consistent\ndata store. (b) A data store that does not provide monotonic reads.\nIn Figure 7.16(a) process P1\ufb01rst performs a write operation on xatL1,\nproducing version x1and later reads this version. At L2process P2\ufb01rst\nproduces version x2, following from x1. When process P1moves to L2and\nreads xagain, it \ufb01nds a more recent value, but one that at least took its\nprevious write into account.\nFigure 7.16(b) shows a situation in which monotonic-read consistency is\nviolated. After process P1has read x1atL1, it later performs the operation\nR1(x2)atL2. However, the preceding write operation W2(x1jx2)by process\nP2atL2is known to produce a version that does not follow from x1. As a\nconsequence, P1\u2019s read operation at L2is known not to include the effect of\nthe write operations when it performed R1(x1)at location L1.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.3. CLIENT-CENTRIC CONSISTENCY MODELS 379\nMonotonic writes\nIn many situations, it is important that write operations are propagated in\nthe correct order to all copies of the data store. This property is expressed\nin monotonic-write consistency. In a monotonic-write consistent store, the\nfollowing condition holds:\nA write operation by a process on a data item xis completed before any\nsuccessive write operation on xby the same process.\nMore formally, if we have two successive operations Wk(xi)and Wk(xj)by\nprocess Pk, then, regardless where Wk(xj)takes place, we also have WS(xi;xj).\nThus, completing a write operation means that the copy on which a successive\noperation is performed re\ufb02ects the effect of a previous write operation by the\nsame process, no matter where that operation was initiated. In other words, a\nwrite operation on a copy of item xis performed only if that copy has been\nbrought up to date by means of any preceding write operation by that same\nprocess, which may have taken place on other copies of x. If need be, the new\nwrite must wait for old ones to \ufb01nish.\nNote that monotonic-write consistency resembles data-centric FIFO consis-\ntency. The essence of FIFO consistency is that write operations by the same\nprocess are performed in the correct order everywhere. This ordering con-\nstraint also applies to monotonic writes, except that we are now considering\nconsistency only for a single process instead of for a collection of concurrent\nprocesses.\nBringing a copy of xup to date need not be necessary when each write\noperation completely overwrites the present value of x. However, write\noperations are often performed on only part of the state of a data item.\nConsider, for example, a software library. In many cases, updating such a\nlibrary is done by replacing one or more functions, leading to a next version.\nWith monotonic-write consistency, guarantees are given that if an update is\nperformed on a copy of the library, all preceding updates will be performed\n\ufb01rst. The resulting library will then indeed become the most recent version\nand will include all updates that have led to previous versions of the library.\nMonotonic-write consistency is shown in Figure 7.17. In Figure 7.17(a)\nprocess P1performs a write operation on xatL1, presented as the operation\nW1(x1). Later, P1performs another write operation on x, but this time at\nL2, shown as W1(x2;x3). The version produced by P1atL2follows from an\nupdate by process P2, in turn based on version x1. The latter is expressed\nby the operation W2(x1;x2). To ensure monotonic-write consistency, it is\nnecessary that the previous write operation at L1has already been propagated\ntoL2, and possibly updated.\nIn contrast, Figure 7.17(b) shows a situation in which monotonic-write\nconsistency is not guaranteed. Compared to Figure 7.17(a) what is missing is\nthe propagation of x1toL2before another version of xis produced, expressed\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.3. CLIENT-CENTRIC CONSISTENCY MODELS 379\nMonotonic writes\nIn many situations, it is important that write operations are propagated in\nthe correct order to all copies of the data store. This property is expressed\nin monotonic-write consistency. In a monotonic-write consistent store, the\nfollowing condition holds:\nA write operation by a process on a data item xis completed before any\nsuccessive write operation on xby the same process.\nMore formally, if we have two successive operations Wk(xi)and Wk(xj)by\nprocess Pk, then, regardless where Wk(xj)takes place, we also have WS(xi;xj).\nThus, completing a write operation means that the copy on which a successive\noperation is performed re\ufb02ects the effect of a previous write operation by the\nsame process, no matter where that operation was initiated. In other words, a\nwrite operation on a copy of item xis performed only if that copy has been\nbrought up to date by means of any preceding write operation by that same\nprocess, which may have taken place on other copies of x. If need be, the new\nwrite must wait for old ones to \ufb01nish.\nNote that monotonic-write consistency resembles data-centric FIFO consis-\ntency. The essence of FIFO consistency is that write operations by the same\nprocess are performed in the correct order everywhere. This ordering con-\nstraint also applies to monotonic writes, except that we are now considering\nconsistency only for a single process instead of for a collection of concurrent\nprocesses.\nBringing a copy of xup to date need not be necessary when each write\noperation completely overwrites the present value of x. However, write\noperations are often performed on only part of the state of a data item.\nConsider, for example, a software library. In many cases, updating such a\nlibrary is done by replacing one or more functions, leading to a next version.\nWith monotonic-write consistency, guarantees are given that if an update is\nperformed on a copy of the library, all preceding updates will be performed\n\ufb01rst. The resulting library will then indeed become the most recent version\nand will include all updates that have led to previous versions of the library.\nMonotonic-write consistency is shown in Figure 7.17. In Figure 7.17(a)\nprocess P1performs a write operation on xatL1, presented as the operation\nW1(x1). Later, P1performs another write operation on x, but this time at\nL2, shown as W1(x2;x3). The version produced by P1atL2follows from an\nupdate by process P2, in turn based on version x1. The latter is expressed\nby the operation W2(x1;x2). To ensure monotonic-write consistency, it is\nnecessary that the previous write operation at L1has already been propagated\ntoL2, and possibly updated.\nIn contrast, Figure 7.17(b) shows a situation in which monotonic-write\nconsistency is not guaranteed. Compared to Figure 7.17(a) what is missing is\nthe propagation of x1toL2before another version of xis produced, expressed\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "380 CHAPTER 7. CONSISTENCY AND REPLICATION\n(a) (b)\n(c) (d)\nFigure 7.17: The write operations performed at two different local copies\nof the same data store. (a) A monotonic-write consistent data store. (b) A\ndata store that does not provide monotonic-write consistency. (c) Again, no\nconsistency as WS(x1jx2)and thus also WS(x1jx3). (d) Consistent as WS(x1;x3)\nalthough x1has apparently overwritten x2.\nby the operation W2(x1jx2). In this case, process P2produced a concurrent\nversion to x1, after which process P1simply produces version x3, but again\nconcurrently to x1. Only slightly more subtle, but still violating monotonic-\nwrite consistency, is the situation sketched in Figure 7.17(c). Process P1now\nproduces version x3which follows from x2. However, because x2does not\nincorporate the write operations that led to x1, that is, WS(x1jx2), we also\nhave WS(x1jx3).\nAn interesting case is shown in Figure 7.17(d). The operation W2(x1jx2)\nproduces version x2concurrently to x1. However, later process P1produces\nversion x3, but apparently based on the fact that version x1had become\navailable at L2. How and when x1was transferred to L2is left unspeci\ufb01ed,\nbut in any case a write-write con\ufb02ict was created with version x2and resolved\nin favor of x1. A consequence is that the situation shown in Figure 7.17(d)\nfollows the rules for monotonic-write consistency. Note, however, that any\nsubsequent write by process P2atL2(without having read version x1) will\nimmediately violate consistency again. How such a violation can be prevented\nis left as an exercise to the reader.\nNote that, by the de\ufb01nition of monotonic-write consistency, write opera-\ntions by the same process are performed in the same order as they are initiated.\nA somewhat weaker form of monotonic writes is one in which the effects of a\nwrite operation are seen only if all preceding writes have been carried out as\nwell, but perhaps not in the order in which they have been originally initiated.\nThis consistency is applicable in those cases in which write operations are\ncommutative, so that ordering is really not necessary. Details are found in\n[Terry et al., 1994].\nRead your writes\nA data store is said to provide read-your-writes consistency , if the following\ncondition holds:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n380 CHAPTER 7. CONSISTENCY AND REPLICATION\n(a) (b)\n(c) (d)\nFigure 7.17: The write operations performed at two different local copies\nof the same data store. (a) A monotonic-write consistent data store. (b) A\ndata store that does not provide monotonic-write consistency. (c) Again, no\nconsistency as WS(x1jx2)and thus also WS(x1jx3). (d) Consistent as WS(x1;x3)\nalthough x1has apparently overwritten x2.\nby the operation W2(x1jx2). In this case, process P2produced a concurrent\nversion to x1, after which process P1simply produces version x3, but again\nconcurrently to x1. Only slightly more subtle, but still violating monotonic-\nwrite consistency, is the situation sketched in Figure 7.17(c). Process P1now\nproduces version x3which follows from x2. However, because x2does not\nincorporate the write operations that led to x1, that is, WS(x1jx2), we also\nhave WS(x1jx3).\nAn interesting case is shown in Figure 7.17(d). The operation W2(x1jx2)\nproduces version x2concurrently to x1. However, later process P1produces\nversion x3, but apparently based on the fact that version x1had become\navailable at L2. How and when x1was transferred to L2is left unspeci\ufb01ed,\nbut in any case a write-write con\ufb02ict was created with version x2and resolved\nin favor of x1. A consequence is that the situation shown in Figure 7.17(d)\nfollows the rules for monotonic-write consistency. Note, however, that any\nsubsequent write by process P2atL2(without having read version x1) will\nimmediately violate consistency again. How such a violation can be prevented\nis left as an exercise to the reader.\nNote that, by the de\ufb01nition of monotonic-write consistency, write opera-\ntions by the same process are performed in the same order as they are initiated.\nA somewhat weaker form of monotonic writes is one in which the effects of a\nwrite operation are seen only if all preceding writes have been carried out as\nwell, but perhaps not in the order in which they have been originally initiated.\nThis consistency is applicable in those cases in which write operations are\ncommutative, so that ordering is really not necessary. Details are found in\n[Terry et al., 1994].\nRead your writes\nA data store is said to provide read-your-writes consistency , if the following\ncondition holds:\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.3. CLIENT-CENTRIC CONSISTENCY MODELS 381\nThe effect of a write operation by a process on data item xwill always be\nseen by a successive read operation on xby the same process.\nIn other words, a write operation is always completed before a successive read\noperation by the same process, no matter where that read operation takes\nplace.\nThe absence of read-your-writes consistency is sometimes experienced\nwhen updating Web documents and subsequently viewing the effects. Update\noperations frequently take place by means of a standard editor or word\nprocessor, perhaps embedded as part of a content management system, which\nthen saves the new version on a \ufb01le system that is shared by the Web server.\nThe user\u2019s Web browser accesses that same \ufb01le, possibly after requesting it\nfrom the local Web server. However, once the \ufb01le has been fetched, either\nthe server or the browser often caches a local copy for subsequent accesses.\nConsequently, when the Web page is updated, the user will not see the effects\nif the browser or the server returns the cached copy instead of the original \ufb01le.\nRead-your-writes consistency can guarantee that if the editor and browser are\nintegrated into a single program, the cache is invalidated when the page is\nupdated, so that the updated \ufb01le is fetched and displayed.\nSimilar effects occur when updating passwords. For example, to enter a\ndigital library on the Web, it is often necessary to have an account with an\naccompanying password. However, changing a password may take some time\nto come into effect, with the result that the library may be inaccessible to the\nuser for a few minutes. The delay can be caused because a separate server\nis used to manage passwords and it may take some time to subsequently\npropagate (encrypted) passwords to the various servers that constitute the\nlibrary.\nFigure 7.18(a) shows a data store that provides read-your-writes consis-\ntency. Note that Figure 7.18(a) is very similar to Figure 7.16(a), except that\nconsistency is now determined by the last write operation by process P1,\ninstead of its last read.\n(a) (b)\nFigure 7.18: (a) A data store that provides read-your-writes consistency. (b) A\ndata store that does not.\nIn Figure 7.18(a) process P1performed a write operation W1(x1)and\nlater a read operation at a different local copy. Read-your-writes consistency\nguarantees that the effects of the write operation can be seen by the succeeding\nread operation. This is expressed by W2(x1;x2), which states that a process P2\nproduced a new version of x, yet one based on x1. In contrast, in Figure 7.18(b)\nprocess P2produces a version concurrently to x1, expressed as W2(x1jx2).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.3. CLIENT-CENTRIC CONSISTENCY MODELS 381\nThe effect of a write operation by a process on data item xwill always be\nseen by a successive read operation on xby the same process.\nIn other words, a write operation is always completed before a successive read\noperation by the same process, no matter where that read operation takes\nplace.\nThe absence of read-your-writes consistency is sometimes experienced\nwhen updating Web documents and subsequently viewing the effects. Update\noperations frequently take place by means of a standard editor or word\nprocessor, perhaps embedded as part of a content management system, which\nthen saves the new version on a \ufb01le system that is shared by the Web server.\nThe user\u2019s Web browser accesses that same \ufb01le, possibly after requesting it\nfrom the local Web server. However, once the \ufb01le has been fetched, either\nthe server or the browser often caches a local copy for subsequent accesses.\nConsequently, when the Web page is updated, the user will not see the effects\nif the browser or the server returns the cached copy instead of the original \ufb01le.\nRead-your-writes consistency can guarantee that if the editor and browser are\nintegrated into a single program, the cache is invalidated when the page is\nupdated, so that the updated \ufb01le is fetched and displayed.\nSimilar effects occur when updating passwords. For example, to enter a\ndigital library on the Web, it is often necessary to have an account with an\naccompanying password. However, changing a password may take some time\nto come into effect, with the result that the library may be inaccessible to the\nuser for a few minutes. The delay can be caused because a separate server\nis used to manage passwords and it may take some time to subsequently\npropagate (encrypted) passwords to the various servers that constitute the\nlibrary.\nFigure 7.18(a) shows a data store that provides read-your-writes consis-\ntency. Note that Figure 7.18(a) is very similar to Figure 7.16(a), except that\nconsistency is now determined by the last write operation by process P1,\ninstead of its last read.\n(a) (b)\nFigure 7.18: (a) A data store that provides read-your-writes consistency. (b) A\ndata store that does not.\nIn Figure 7.18(a) process P1performed a write operation W1(x1)and\nlater a read operation at a different local copy. Read-your-writes consistency\nguarantees that the effects of the write operation can be seen by the succeeding\nread operation. This is expressed by W2(x1;x2), which states that a process P2\nproduced a new version of x, yet one based on x1. In contrast, in Figure 7.18(b)\nprocess P2produces a version concurrently to x1, expressed as W2(x1jx2).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "382 CHAPTER 7. CONSISTENCY AND REPLICATION\nThis means that the effects of the previous write operation by process P1have\nnot been propagated to L2at the time x2was produced. When P1reads x2, it\nwill not see the effects of its own write operation at L1.\nWrites follow reads\nThe last client-centric consistency model is one in which updates are propa-\ngated as the result of previous read operations. A data store is said to provide\nwrites-follow-reads consistency, if the following holds.\nA write operation by a process on a data item xfollowing a previous read\noperation on xby the same process is guaranteed to take place on the\nsame or a more recent value of xthat was read.\nIn other words, any successive write operation by a process on a data item\nxwill be performed on a copy of xthat is up to date with the value most\nrecently read by that process.\nWrites-follow-reads consistency can be used to guarantee that users of a\nnetwork newsgroup see a posting of a reaction to an article only after they\nhave seen the original article [Terry et al., 1994]. To understand the problem,\nassume that a user \ufb01rst reads an article A. Then, she reacts by posting a\nresponse B. By requiring writes-follow-reads consistency, Bwill be written\nto any copy of the newsgroup only after Ahas been written as well. Note\nthat users who only read articles need not require any speci\ufb01c client-centric\nconsistency model. The writes-follows-reads consistency assures that reactions\nto articles are stored at a local copy only if the original is stored there as well.\n(a) (b)\nFigure 7.19: (a) A writes-follow-reads consistent data store. (b) A data store\nthat does not provide writes-follow-reads consistency.\nThis consistency model is shown in Figure 7.19. In Figure 7.19(a), process\nP2reads version x1at local copy L1. This version of xwas previously pro-\nduced at L1by process P1through the operation W1(x1). That version was\nsubsequently propagated to L2, and used by another process P3to produce a\nnew version x2, expressed as W3(x1;x2). When process P2later updates its\nversion of xafter moving to L2, it is known that it will operate on a version that\nfollows from x1, expressed as W2(x2;x3). Because we also have W3(x1;x2), we\nknown that WS(x1;x3).\nThe situation shown in Figure 7.19(b) is different. Process P3produces a\nversion x2concurrently to that of x1. As a consequence, when P2updates x\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n382 CHAPTER 7. CONSISTENCY AND REPLICATION\nThis means that the effects of the previous write operation by process P1have\nnot been propagated to L2at the time x2was produced. When P1reads x2, it\nwill not see the effects of its own write operation at L1.\nWrites follow reads\nThe last client-centric consistency model is one in which updates are propa-\ngated as the result of previous read operations. A data store is said to provide\nwrites-follow-reads consistency, if the following holds.\nA write operation by a process on a data item xfollowing a previous read\noperation on xby the same process is guaranteed to take place on the\nsame or a more recent value of xthat was read.\nIn other words, any successive write operation by a process on a data item\nxwill be performed on a copy of xthat is up to date with the value most\nrecently read by that process.\nWrites-follow-reads consistency can be used to guarantee that users of a\nnetwork newsgroup see a posting of a reaction to an article only after they\nhave seen the original article [Terry et al., 1994]. To understand the problem,\nassume that a user \ufb01rst reads an article A. Then, she reacts by posting a\nresponse B. By requiring writes-follow-reads consistency, Bwill be written\nto any copy of the newsgroup only after Ahas been written as well. Note\nthat users who only read articles need not require any speci\ufb01c client-centric\nconsistency model. The writes-follows-reads consistency assures that reactions\nto articles are stored at a local copy only if the original is stored there as well.\n(a) (b)\nFigure 7.19: (a) A writes-follow-reads consistent data store. (b) A data store\nthat does not provide writes-follow-reads consistency.\nThis consistency model is shown in Figure 7.19. In Figure 7.19(a), process\nP2reads version x1at local copy L1. This version of xwas previously pro-\nduced at L1by process P1through the operation W1(x1). That version was\nsubsequently propagated to L2, and used by another process P3to produce a\nnew version x2, expressed as W3(x1;x2). When process P2later updates its\nversion of xafter moving to L2, it is known that it will operate on a version that\nfollows from x1, expressed as W2(x2;x3). Because we also have W3(x1;x2), we\nknown that WS(x1;x3).\nThe situation shown in Figure 7.19(b) is different. Process P3produces a\nversion x2concurrently to that of x1. As a consequence, when P2updates x\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 383\nafter reading x1, it will be updating a version it had not read before. Writes-\nfollow-reads consistency is then violated.\n7.4 Replica management\nA key issue for any distributed system that supports replication is to decide\nwhere, when, and by whom replicas should be placed, and subsequently\nwhich mechanisms to use for keeping the replicas consistent. The placement\nproblem itself should be split into two subproblems: that of placing replica\nservers , and that of placing content . The difference is a subtle one and the two\nissues are often not clearly separated. Replica-server placement is concerned\nwith \ufb01nding the best locations to place a server that can host (part of) a\ndata store. Content placement deals with \ufb01nding the best servers for placing\ncontent. Note that this often means that we are looking for the optimal\nplacement of only a single data item. Obviously, before content placement can\ntake place, replica servers will have to be placed \ufb01rst.\nFinding the best server location\nWhere perhaps over a decade ago one could be concerned about where to\nplace an individual server, matters have changed considerably with the advent\nof the many large-scale data centers located across the Internet. Likewise, con-\nnectivity continues to improve, making precisely locating servers less critical.\nNote 7.5 (Advanced: Replica-server placement)\nThe placement of replica servers is not an intensively studied problem for the\nsimple reason that it is often more of a management and commercial issue than\nan optimization problem. Nonetheless, analysis of client and network properties\nare useful to come to informed decisions.\nThere are various ways to compute the best placement of replica servers, but\nall boil down to an optimization problem in which the best Kout of Nlocations\nneed to be selected ( K<N). These problems are known to be computationally\ncomplex and can be solved only through heuristics. Qiu et al. [2001] take the\ndistance between clients and locations as their starting point. Distance can be\nmeasured in terms of latency or bandwidth. Their solution selects one server\nat a time such that the average distance between that server and its clients is\nminimal given that already kservers have been placed (meaning that there are\nN\u0000klocations left).\nAs an alternative, Radoslavov et al. [2001] propose to ignore the position of\nclients and only take the topology of the Internet as formed by the autonomous\nsystems. An autonomous system (AS) can best be viewed as a network in which\nthe nodes all run the same routing protocol and which is managed by a single\norganization. As of 2015, there were some 30,000 ASes. Radoslavov et al. \ufb01rst\nconsider the largest AS and place a server on the router with the largest number\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 383\nafter reading x1, it will be updating a version it had not read before. Writes-\nfollow-reads consistency is then violated.\n7.4 Replica management\nA key issue for any distributed system that supports replication is to decide\nwhere, when, and by whom replicas should be placed, and subsequently\nwhich mechanisms to use for keeping the replicas consistent. The placement\nproblem itself should be split into two subproblems: that of placing replica\nservers , and that of placing content . The difference is a subtle one and the two\nissues are often not clearly separated. Replica-server placement is concerned\nwith \ufb01nding the best locations to place a server that can host (part of) a\ndata store. Content placement deals with \ufb01nding the best servers for placing\ncontent. Note that this often means that we are looking for the optimal\nplacement of only a single data item. Obviously, before content placement can\ntake place, replica servers will have to be placed \ufb01rst.\nFinding the best server location\nWhere perhaps over a decade ago one could be concerned about where to\nplace an individual server, matters have changed considerably with the advent\nof the many large-scale data centers located across the Internet. Likewise, con-\nnectivity continues to improve, making precisely locating servers less critical.\nNote 7.5 (Advanced: Replica-server placement)\nThe placement of replica servers is not an intensively studied problem for the\nsimple reason that it is often more of a management and commercial issue than\nan optimization problem. Nonetheless, analysis of client and network properties\nare useful to come to informed decisions.\nThere are various ways to compute the best placement of replica servers, but\nall boil down to an optimization problem in which the best Kout of Nlocations\nneed to be selected ( K<N). These problems are known to be computationally\ncomplex and can be solved only through heuristics. Qiu et al. [2001] take the\ndistance between clients and locations as their starting point. Distance can be\nmeasured in terms of latency or bandwidth. Their solution selects one server\nat a time such that the average distance between that server and its clients is\nminimal given that already kservers have been placed (meaning that there are\nN\u0000klocations left).\nAs an alternative, Radoslavov et al. [2001] propose to ignore the position of\nclients and only take the topology of the Internet as formed by the autonomous\nsystems. An autonomous system (AS) can best be viewed as a network in which\nthe nodes all run the same routing protocol and which is managed by a single\norganization. As of 2015, there were some 30,000 ASes. Radoslavov et al. \ufb01rst\nconsider the largest AS and place a server on the router with the largest number\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "384 CHAPTER 7. CONSISTENCY AND REPLICATION\nof network interfaces (i.e., links). This algorithm is then repeated with the second\nlargest AS, and so on.\nAs it turns out, client-unaware server placement achieves similar results as\nclient-aware placement, under the assumption that clients are uniformly dis-\ntributed across the Internet (relative to the existing topology). To what extent this\nassumption is true is unclear. It has not been well studied.\nOne problem with these algorithms is that they are computationally expensive.\nFor example, both the previous algorithms have a complexity that is higher than\nO(N2), where Nis the number of locations to inspect. In practice, this means\nthat for even a few thousand locations, a computation may need to run for tens of\nminutes. This may be unacceptable.\nSzymaniak et al. [2006] have developed a method by which a region for placing\nreplicas can be quickly identi\ufb01ed. A region is identi\ufb01ed to be a collection of nodes\naccessing the same content, but for which the internode latency is low. The goal\nof the algorithm is \ufb01rst to select the most demanding regions\u2013that is, the one with\nthe most nodes\u2013and then to let one of the nodes in such a region act as replica\nserver.\nTo this end, nodes are assumed to be positioned in an m-dimensional geomet-\nric space, as we discussed in the previous chapter. The basic idea is to identify the\nKlargest clusters and assign a node from each cluster to host replicated content.\nTo identify these clusters, the entire space is partitioned into cells. The Kmost\ndense cells are then chosen for placing a replica server. A cell is nothing but an\nm-dimensional hypercube. For a two-dimensional space, this corresponds to a\nrectangle.\nFigure 7.20: Choosing a proper cell size for server placement.\nObviously, the cell size is important, as shown in Figure 7.20. If cells are\nchosen too large, then multiple clusters of nodes may be contained in the same\ncell. In that case, too few replica servers for those clusters would be chosen. On\nthe other hand, choosing small cells may lead to the situation that a single cluster\nis spread across a number of cells, leading to choosing too many replica servers.\nAs it turns out, an appropriate cell size can be computed as a simple function\nof the average distance between two nodes and the number of required replicas.\nWith this cell size, it can be shown that the algorithm performs as well as the\nclose-to-optimal one described by Qiu et al. [2001], but having a much lower\ncomplexity:O(N\u0002maxflog(N),Kg). To give an impression what this result\nmeans: experiments show that computing the 20 best replica locations for a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n384 CHAPTER 7. CONSISTENCY AND REPLICATION\nof network interfaces (i.e., links). This algorithm is then repeated with the second\nlargest AS, and so on.\nAs it turns out, client-unaware server placement achieves similar results as\nclient-aware placement, under the assumption that clients are uniformly dis-\ntributed across the Internet (relative to the existing topology). To what extent this\nassumption is true is unclear. It has not been well studied.\nOne problem with these algorithms is that they are computationally expensive.\nFor example, both the previous algorithms have a complexity that is higher than\nO(N2), where Nis the number of locations to inspect. In practice, this means\nthat for even a few thousand locations, a computation may need to run for tens of\nminutes. This may be unacceptable.\nSzymaniak et al. [2006] have developed a method by which a region for placing\nreplicas can be quickly identi\ufb01ed. A region is identi\ufb01ed to be a collection of nodes\naccessing the same content, but for which the internode latency is low. The goal\nof the algorithm is \ufb01rst to select the most demanding regions\u2013that is, the one with\nthe most nodes\u2013and then to let one of the nodes in such a region act as replica\nserver.\nTo this end, nodes are assumed to be positioned in an m-dimensional geomet-\nric space, as we discussed in the previous chapter. The basic idea is to identify the\nKlargest clusters and assign a node from each cluster to host replicated content.\nTo identify these clusters, the entire space is partitioned into cells. The Kmost\ndense cells are then chosen for placing a replica server. A cell is nothing but an\nm-dimensional hypercube. For a two-dimensional space, this corresponds to a\nrectangle.\nFigure 7.20: Choosing a proper cell size for server placement.\nObviously, the cell size is important, as shown in Figure 7.20. If cells are\nchosen too large, then multiple clusters of nodes may be contained in the same\ncell. In that case, too few replica servers for those clusters would be chosen. On\nthe other hand, choosing small cells may lead to the situation that a single cluster\nis spread across a number of cells, leading to choosing too many replica servers.\nAs it turns out, an appropriate cell size can be computed as a simple function\nof the average distance between two nodes and the number of required replicas.\nWith this cell size, it can be shown that the algorithm performs as well as the\nclose-to-optimal one described by Qiu et al. [2001], but having a much lower\ncomplexity:O(N\u0002maxflog(N),Kg). To give an impression what this result\nmeans: experiments show that computing the 20 best replica locations for a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 385\ncollection of 64,000 nodes is approximately 50,000 times faster. As a consequence,\nreplica-server placement can now be done in real time.\nContent replication and placement\nWhen it comes to content replication and placement, three different types of\nreplicas can be distinguished logically organized as shown in Figure 7.21.\nFigure 7.21: The logical organization of different kinds of copies of a data\nstore into three concentric rings.\nPermanent replicas\nPermanent replicas can be considered as the initial set of replicas that con-\nstitute a distributed data store. In many cases, the number of permanent\nreplicas is small. Consider, for example, a Web site. Distribution of a Web site\ngenerally comes in one of two forms. The \ufb01rst kind of distribution is one in\nwhich the \ufb01les that constitute a site are replicated across a limited number of\nservers at a single location. Whenever a request comes in, it is forwarded to\none of the servers, for instance, using a round-robin strategy.\nThe second form of distributed Web sites is what is called mirroring . In\nthis case, a Web site is copied to a limited number of servers, called mirror\nsites , which are geographically spread across the Internet. In most cases,\nclients simply choose one of the various mirror sites from a list offered to\nthem. Mirrored Web sites have in common with cluster-based Web sites that\nthere are only a few replicas, which are more or less statically con\ufb01gured.\nSimilar static organizations also appear with distributed databases [Kemme\net al., 2010; \u00d6zsu and Valduriez, 2011]. Again, the database can be distributed\nand replicated across a number of servers that together form a cluster of\nservers, often referred to as a shared-nothing architecture , emphasizing that\nneither disks nor main memory are shared by processors. Alternatively, a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 385\ncollection of 64,000 nodes is approximately 50,000 times faster. As a consequence,\nreplica-server placement can now be done in real time.\nContent replication and placement\nWhen it comes to content replication and placement, three different types of\nreplicas can be distinguished logically organized as shown in Figure 7.21.\nFigure 7.21: The logical organization of different kinds of copies of a data\nstore into three concentric rings.\nPermanent replicas\nPermanent replicas can be considered as the initial set of replicas that con-\nstitute a distributed data store. In many cases, the number of permanent\nreplicas is small. Consider, for example, a Web site. Distribution of a Web site\ngenerally comes in one of two forms. The \ufb01rst kind of distribution is one in\nwhich the \ufb01les that constitute a site are replicated across a limited number of\nservers at a single location. Whenever a request comes in, it is forwarded to\none of the servers, for instance, using a round-robin strategy.\nThe second form of distributed Web sites is what is called mirroring . In\nthis case, a Web site is copied to a limited number of servers, called mirror\nsites , which are geographically spread across the Internet. In most cases,\nclients simply choose one of the various mirror sites from a list offered to\nthem. Mirrored Web sites have in common with cluster-based Web sites that\nthere are only a few replicas, which are more or less statically con\ufb01gured.\nSimilar static organizations also appear with distributed databases [Kemme\net al., 2010; \u00d6zsu and Valduriez, 2011]. Again, the database can be distributed\nand replicated across a number of servers that together form a cluster of\nservers, often referred to as a shared-nothing architecture , emphasizing that\nneither disks nor main memory are shared by processors. Alternatively, a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "386 CHAPTER 7. CONSISTENCY AND REPLICATION\ndatabase is distributed and possibly replicated across a number of geograph-\nically dispersed sites. This architecture is generally deployed in federated\ndatabases [Sheth and Larson, 1990].\nServer-initiated replicas\nIn contrast to permanent replicas, server-initiated replicas are copies of a data\nstore that exist to enhance performance, and created at the initiative of the\n(owner of the) data store. Consider, for example, a Web server placed in\nNew York. Normally, this server can handle incoming requests quite easily,\nbut it may happen that over a couple of days a sudden burst of requests\ncome in from an unexpected location far from the server. In that case, it may\nbe worthwhile to install a number of temporary replicas in regions where\nrequests are coming from.\nNote 7.6 (Advanced: An example of dynamic Web-content placement)\nThe problem of dynamically placing replicas has since long been addressed in\nWeb hosting services. These services offer an often relatively static collection of\nservers spread across the Internet that can maintain and provide access to Web\n\ufb01les belonging to third parties. To provide optimal facilities such hosting services\ncan dynamically replicate \ufb01les to servers where those \ufb01les are needed to enhance\nperformance, that is, close to demanding (groups of) clients.\nGiven that the replica servers are already in place, deciding where to place\ncontent is not that dif\ufb01cult. An early case toward dynamic replication of \ufb01les in\nthe case of a Web hosting service is described by Rabinovich et al. [1999]. The\nalgorithm is designed to support Web pages for which reason it assumes that\nupdates are relatively rare compared to read requests. Using \ufb01les as the unit of\ndata, the algorithm works as follows.\nThe algorithm for dynamic replication takes two issues into account. First,\nreplication can take place to reduce the load on a server. Second, speci\ufb01c \ufb01les on\na server can be migrated or replicated to servers placed in the proximity of clients\nthat issue many requests for those \ufb01les. In the following, we concentrate only on\nthis second issue. We also leave out a number of details, which can be found in\n[Rabinovich et al., 1999].\nEach server keeps track of access counts per \ufb01le, and where access requests\ncome from. In particular, when a client Centers the service, it does so through\na server close to it. If client C1and client C2share the same closest server P, all\naccess requests for \ufb01le Fat server Qfrom C1andC2are jointly registered at Qas\na single access count cntQ(P,F). This situation is shown in Figure 7.22.\nWhen the number of requests for a speci\ufb01c \ufb01le Fat server Sdrops below a\ndeletion threshold del(S,F), that \ufb01le can be removed from S. As a consequence,\nthe number of replicas of that \ufb01le is reduced, possibly leading to higher work\nloads at other servers. Special measures are taken to ensure that at least one copy\nof each \ufb01le continues to exist.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n386 CHAPTER 7. CONSISTENCY AND REPLICATION\ndatabase is distributed and possibly replicated across a number of geograph-\nically dispersed sites. This architecture is generally deployed in federated\ndatabases [Sheth and Larson, 1990].\nServer-initiated replicas\nIn contrast to permanent replicas, server-initiated replicas are copies of a data\nstore that exist to enhance performance, and created at the initiative of the\n(owner of the) data store. Consider, for example, a Web server placed in\nNew York. Normally, this server can handle incoming requests quite easily,\nbut it may happen that over a couple of days a sudden burst of requests\ncome in from an unexpected location far from the server. In that case, it may\nbe worthwhile to install a number of temporary replicas in regions where\nrequests are coming from.\nNote 7.6 (Advanced: An example of dynamic Web-content placement)\nThe problem of dynamically placing replicas has since long been addressed in\nWeb hosting services. These services offer an often relatively static collection of\nservers spread across the Internet that can maintain and provide access to Web\n\ufb01les belonging to third parties. To provide optimal facilities such hosting services\ncan dynamically replicate \ufb01les to servers where those \ufb01les are needed to enhance\nperformance, that is, close to demanding (groups of) clients.\nGiven that the replica servers are already in place, deciding where to place\ncontent is not that dif\ufb01cult. An early case toward dynamic replication of \ufb01les in\nthe case of a Web hosting service is described by Rabinovich et al. [1999]. The\nalgorithm is designed to support Web pages for which reason it assumes that\nupdates are relatively rare compared to read requests. Using \ufb01les as the unit of\ndata, the algorithm works as follows.\nThe algorithm for dynamic replication takes two issues into account. First,\nreplication can take place to reduce the load on a server. Second, speci\ufb01c \ufb01les on\na server can be migrated or replicated to servers placed in the proximity of clients\nthat issue many requests for those \ufb01les. In the following, we concentrate only on\nthis second issue. We also leave out a number of details, which can be found in\n[Rabinovich et al., 1999].\nEach server keeps track of access counts per \ufb01le, and where access requests\ncome from. In particular, when a client Centers the service, it does so through\na server close to it. If client C1and client C2share the same closest server P, all\naccess requests for \ufb01le Fat server Qfrom C1andC2are jointly registered at Qas\na single access count cntQ(P,F). This situation is shown in Figure 7.22.\nWhen the number of requests for a speci\ufb01c \ufb01le Fat server Sdrops below a\ndeletion threshold del(S,F), that \ufb01le can be removed from S. As a consequence,\nthe number of replicas of that \ufb01le is reduced, possibly leading to higher work\nloads at other servers. Special measures are taken to ensure that at least one copy\nof each \ufb01le continues to exist.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 387\nFigure 7.22: Counting access requests from different clients.\nA replication threshold rep(S,F), which is always chosen higher than the\ndeletion threshold, indicates that the number of requests for a speci\ufb01c \ufb01le is so\nhigh that it may be worthwhile replicating it on another server. If the number of\nrequests lie somewhere between the deletion and replication threshold, the \ufb01le is\nallowed to be only migrated. In other words, in that case it is important to at least\nkeep the number of replicas for that \ufb01le the same.\nWhen a server Qdecides to reevaluate the placement of the \ufb01les it stores, it\nchecks the access count for each \ufb01le. If the total number of access requests for F\natQdrops below the deletion threshold del(Q,F), it will delete Funless it is the\nlast copy. Furthermore, if for some server P,cntQ(P,F)exceeds more than half of\nthe total requests for FatQ, server Pis requested to take over the copy of F. In\nother words, server Qwill attempt to migrate FtoP.\nMigration of \ufb01le Fto server Pmay not always succeed, for example, because\nPis already heavily loaded or is out of disk space. In that case, Qwill attempt to\nreplicate Fon other servers. Of course, replication can take place only if the total\nnumber of access requests for FatQexceeds the replication threshold rep(Q,F).\nServer Qchecks all other servers in the Web hosting service, starting with the one\nfarthest away. If, for some server R,cntQ(R,F)exceeds a certain fraction of all\nrequests for FatQ, an attempt is made to replicate FtoR.\nNote that as long as guarantees can be given that each data item is hosted\nby at least one server, it may suf\ufb01ce to use only server-initiated replication\nand not have any permanent replicas. However, permanent replicas are often\nuseful as a back-up facility, or to be used as the only replicas that are allowed\nto be changed to guarantee consistency. Server-initiated replicas are then used\nfor placing read-only copies close to clients.\nClient-initiated replicas\nAn important kind of replica is the one initiated by a client. Client-initiated\nreplicas are more commonly known as ( client )caches . In essence, a cache is a\nlocal storage facility that is used by a client to temporarily store a copy of the\ndata it has just requested. In principle, managing the cache is left entirely to\nthe client. The data store from where the data had been fetched has nothing to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 387\nFigure 7.22: Counting access requests from different clients.\nA replication threshold rep(S,F), which is always chosen higher than the\ndeletion threshold, indicates that the number of requests for a speci\ufb01c \ufb01le is so\nhigh that it may be worthwhile replicating it on another server. If the number of\nrequests lie somewhere between the deletion and replication threshold, the \ufb01le is\nallowed to be only migrated. In other words, in that case it is important to at least\nkeep the number of replicas for that \ufb01le the same.\nWhen a server Qdecides to reevaluate the placement of the \ufb01les it stores, it\nchecks the access count for each \ufb01le. If the total number of access requests for F\natQdrops below the deletion threshold del(Q,F), it will delete Funless it is the\nlast copy. Furthermore, if for some server P,cntQ(P,F)exceeds more than half of\nthe total requests for FatQ, server Pis requested to take over the copy of F. In\nother words, server Qwill attempt to migrate FtoP.\nMigration of \ufb01le Fto server Pmay not always succeed, for example, because\nPis already heavily loaded or is out of disk space. In that case, Qwill attempt to\nreplicate Fon other servers. Of course, replication can take place only if the total\nnumber of access requests for FatQexceeds the replication threshold rep(Q,F).\nServer Qchecks all other servers in the Web hosting service, starting with the one\nfarthest away. If, for some server R,cntQ(R,F)exceeds a certain fraction of all\nrequests for FatQ, an attempt is made to replicate FtoR.\nNote that as long as guarantees can be given that each data item is hosted\nby at least one server, it may suf\ufb01ce to use only server-initiated replication\nand not have any permanent replicas. However, permanent replicas are often\nuseful as a back-up facility, or to be used as the only replicas that are allowed\nto be changed to guarantee consistency. Server-initiated replicas are then used\nfor placing read-only copies close to clients.\nClient-initiated replicas\nAn important kind of replica is the one initiated by a client. Client-initiated\nreplicas are more commonly known as ( client )caches . In essence, a cache is a\nlocal storage facility that is used by a client to temporarily store a copy of the\ndata it has just requested. In principle, managing the cache is left entirely to\nthe client. The data store from where the data had been fetched has nothing to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "388 CHAPTER 7. CONSISTENCY AND REPLICATION\ndo with keeping cached data consistent. However, there are many occasions\nin which the client can rely on participation from the data store to inform it\nwhen cached data has become stale.\nClient caches are used only to improve access times to data. Normally,\nwhen a client wants access to some data, it connects to the nearest copy of the\ndata store from where it fetches the data it wants to read, or to where it stores\nthe data it had just modi\ufb01ed. When most operations involve only reading\ndata, performance can be improved by letting the client store requested data\nin a nearby cache. Such a cache could be located on the client\u2019s machine, or\non a separate machine in the same local-area network as the client. The next\ntime that same data needs to be read, the client can simply fetch it from this\nlocal cache. This scheme works \ufb01ne as long as the fetched data have not been\nmodi\ufb01ed in the meantime.\nData are generally kept in a cache for a limited amount of time, for example,\nto prevent extremely stale data from being used, or simply to make room\nfor other data. Whenever requested data can be fetched from the local cache,\nacache hit is said to have occurred. To improve the number of cache hits,\ncaches can be shared between clients. The underlying assumption is that a\ndata request from client C1may also be useful for a request from another\nnearby client C2.\nWhether this assumption is correct depends very much on the type of data\nstore. For example, in traditional \ufb01le systems, data \ufb01les are rarely shared at all\n(see, e.g., Muntz and Honeyman [1992] and Blaze [1993]) rendering a shared\ncache useless. Likewise, it turns out that using Web caches to share data\nhas been losing ground, partly also because of the improvement in network\nand server performance. Instead, server-initiated replication schemes are\nbecoming more effective.\nPlacement of client caches is relatively simple: a cache is normally placed\non the same machine as its client, or otherwise on a machine shared by clients\non the same local-area network. However, in some cases, extra levels of\ncaching are introduced by system administrators by placing a shared cache\nbetween a number of departments or organizations, or even placing a shared\ncache for an entire region such as a province or country.\nYet another approach is to place (cache) servers at speci\ufb01c points in a\nwide-area network and let a client locate the nearest server. When the server is\nlocated, it can be requested to hold copies of the data the client was previously\nfetching from somewhere else [Noble et al., 1999].\nContent distribution\nReplica management also deals with propagation of (updated) content to the\nrelevant replica servers. There are various trade-offs to make.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n388 CHAPTER 7. CONSISTENCY AND REPLICATION\ndo with keeping cached data consistent. However, there are many occasions\nin which the client can rely on participation from the data store to inform it\nwhen cached data has become stale.\nClient caches are used only to improve access times to data. Normally,\nwhen a client wants access to some data, it connects to the nearest copy of the\ndata store from where it fetches the data it wants to read, or to where it stores\nthe data it had just modi\ufb01ed. When most operations involve only reading\ndata, performance can be improved by letting the client store requested data\nin a nearby cache. Such a cache could be located on the client\u2019s machine, or\non a separate machine in the same local-area network as the client. The next\ntime that same data needs to be read, the client can simply fetch it from this\nlocal cache. This scheme works \ufb01ne as long as the fetched data have not been\nmodi\ufb01ed in the meantime.\nData are generally kept in a cache for a limited amount of time, for example,\nto prevent extremely stale data from being used, or simply to make room\nfor other data. Whenever requested data can be fetched from the local cache,\nacache hit is said to have occurred. To improve the number of cache hits,\ncaches can be shared between clients. The underlying assumption is that a\ndata request from client C1may also be useful for a request from another\nnearby client C2.\nWhether this assumption is correct depends very much on the type of data\nstore. For example, in traditional \ufb01le systems, data \ufb01les are rarely shared at all\n(see, e.g., Muntz and Honeyman [1992] and Blaze [1993]) rendering a shared\ncache useless. Likewise, it turns out that using Web caches to share data\nhas been losing ground, partly also because of the improvement in network\nand server performance. Instead, server-initiated replication schemes are\nbecoming more effective.\nPlacement of client caches is relatively simple: a cache is normally placed\non the same machine as its client, or otherwise on a machine shared by clients\non the same local-area network. However, in some cases, extra levels of\ncaching are introduced by system administrators by placing a shared cache\nbetween a number of departments or organizations, or even placing a shared\ncache for an entire region such as a province or country.\nYet another approach is to place (cache) servers at speci\ufb01c points in a\nwide-area network and let a client locate the nearest server. When the server is\nlocated, it can be requested to hold copies of the data the client was previously\nfetching from somewhere else [Noble et al., 1999].\nContent distribution\nReplica management also deals with propagation of (updated) content to the\nrelevant replica servers. There are various trade-offs to make.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 389\nState versus operations\nAn important design issue concerns what is actually to be propagated. Basi-\ncally, there are three possibilities:\n\u2022 Propagate only a noti\ufb01cation of an update.\n\u2022 Transfer data from one copy to another.\n\u2022 Propagate the update operation to other copies.\nPropagating a noti\ufb01cation is what invalidation protocols do. In an invali-\ndation protocol, other copies are informed that an update has taken place\nand that the data they contain are no longer valid. The invalidation may\nspecify which part of the data store has been updated, so that only part of\na copy is actually invalidated. The important issue is that no more than a\nnoti\ufb01cation is propagated. Whenever an operation on an invalidated copy is\nrequested, that copy generally needs to be updated \ufb01rst, depending on the\nspeci\ufb01c consistency model that is to be supported.\nThe main advantage of invalidation protocols is that they use little network\nbandwidth. The only information that needs to be transferred is a speci\ufb01cation\nof which data are no longer valid. Such protocols generally work best when\nthere are many update operations compared to read operations, that is, the\nread-to-write ratio is relatively small.\nConsider, for example, a data store in which updates are propagated by\nsending the modi\ufb01ed data to all replicas. If the size of the modi\ufb01ed data is\nlarge, and updates occur frequently compared to read operations, we may\nhave the situation that two updates occur after one another without any read\noperation being performed between them. Consequently, propagation of the\n\ufb01rst update to all replicas is effectively useless, as it will be overwritten by\nthe second update. Instead, sending a noti\ufb01cation that the data have been\nmodi\ufb01ed would have been more ef\ufb01cient.\nTransferring the modi\ufb01ed data among replicas is the second alternative,\nand is useful when the read-to-write ratio is relatively high. In that case, the\nprobability that an update will be effective in the sense that the modi\ufb01ed data\nwill be read before the next update takes place is high. Instead of propagating\nmodi\ufb01ed data, it is also possible to log the changes and transfer only those\nlogs to save bandwidth. In addition, transfers are often aggregated in the\nsense that multiple modi\ufb01cations are packed into a single message, thus\nsaving communication overhead.\nThe third approach is not to transfer any data modi\ufb01cations at all, but\nto tell each replica which update operation it should perform (and sending\nonly the parameter values that those operations need). This approach, also\nreferred to as active replication , assumes that each replica is represented\nby a process capable of \u201cactively\u201d keeping its associated data up to date by\nperforming operations [Schneider, 1990]. The main bene\ufb01t of active replication\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 389\nState versus operations\nAn important design issue concerns what is actually to be propagated. Basi-\ncally, there are three possibilities:\n\u2022 Propagate only a noti\ufb01cation of an update.\n\u2022 Transfer data from one copy to another.\n\u2022 Propagate the update operation to other copies.\nPropagating a noti\ufb01cation is what invalidation protocols do. In an invali-\ndation protocol, other copies are informed that an update has taken place\nand that the data they contain are no longer valid. The invalidation may\nspecify which part of the data store has been updated, so that only part of\na copy is actually invalidated. The important issue is that no more than a\nnoti\ufb01cation is propagated. Whenever an operation on an invalidated copy is\nrequested, that copy generally needs to be updated \ufb01rst, depending on the\nspeci\ufb01c consistency model that is to be supported.\nThe main advantage of invalidation protocols is that they use little network\nbandwidth. The only information that needs to be transferred is a speci\ufb01cation\nof which data are no longer valid. Such protocols generally work best when\nthere are many update operations compared to read operations, that is, the\nread-to-write ratio is relatively small.\nConsider, for example, a data store in which updates are propagated by\nsending the modi\ufb01ed data to all replicas. If the size of the modi\ufb01ed data is\nlarge, and updates occur frequently compared to read operations, we may\nhave the situation that two updates occur after one another without any read\noperation being performed between them. Consequently, propagation of the\n\ufb01rst update to all replicas is effectively useless, as it will be overwritten by\nthe second update. Instead, sending a noti\ufb01cation that the data have been\nmodi\ufb01ed would have been more ef\ufb01cient.\nTransferring the modi\ufb01ed data among replicas is the second alternative,\nand is useful when the read-to-write ratio is relatively high. In that case, the\nprobability that an update will be effective in the sense that the modi\ufb01ed data\nwill be read before the next update takes place is high. Instead of propagating\nmodi\ufb01ed data, it is also possible to log the changes and transfer only those\nlogs to save bandwidth. In addition, transfers are often aggregated in the\nsense that multiple modi\ufb01cations are packed into a single message, thus\nsaving communication overhead.\nThe third approach is not to transfer any data modi\ufb01cations at all, but\nto tell each replica which update operation it should perform (and sending\nonly the parameter values that those operations need). This approach, also\nreferred to as active replication , assumes that each replica is represented\nby a process capable of \u201cactively\u201d keeping its associated data up to date by\nperforming operations [Schneider, 1990]. The main bene\ufb01t of active replication\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "390 CHAPTER 7. CONSISTENCY AND REPLICATION\nis that updates can often be propagated at minimal bandwidth costs, provided\nthe size of the parameters associated with an operation are relatively small.\nMoreover, the operations can be of arbitrary complexity, which may allow\nfurther improvements in keeping replicas consistent. On the other hand, more\nprocessing power may be required by each replica, especially in those cases\nwhen operations are relatively complex.\nPull versus push protocols\nAnother design issue is whether updates are pulled or pushed. In a push-\nbased approach , also referred to as server-based protocols , updates are prop-\nagated to other replicas without those replicas even asking for the updates.\nPush-based approaches are often used between permanent and server-initiated\nreplicas, but can also be used to push updates to client caches. Server-based\nprotocols are generally applied when strong consistency is required.\nThis need for strong consistency is related to the fact that permanent and\nserver-initiated replicas, as well as large shared caches, are often shared by\nmany clients, which, in turn, mainly perform read operations. Consequently,\nthe read-to-update ratio at each replica is relatively high. In these cases, push-\nbased protocols are ef\ufb01cient in the sense that every pushed update can be\nexpected to be of use for at least one, but perhaps more readers. In addition,\npush-based protocols make consistent data immediately available when asked\nfor.\nIn contrast, in a pull-based approach , a server or client requests another\nserver to send it any updates it has at that moment. Pull-based protocols, also\ncalled client-based protocols , are often used by client caches. For example, a\ncommon strategy applied to Web caches is \ufb01rst to check whether cached data\nitems are still up to date. When a cache receives a request for items that are\nstill locally available, the cache checks with the original Web server whether\nthose data items have been modi\ufb01ed since they were cached. In the case of\na modi\ufb01cation, the modi\ufb01ed data are \ufb01rst transferred to the cache, and then\nreturned to the requesting client. If no modi\ufb01cations took place, the cached\ndata are returned. In other words, the client polls the server to see whether\nan update is needed.\nA pull-based approach is ef\ufb01cient when the read-to-update ratio is rela-\ntively low. This is often the case with (nonshared) client caches, which have\nonly one client. However, even when a cache is shared by many clients, a\npull-based approach may also prove to be ef\ufb01cient when the cached data items\nare rarely shared. The main drawback of a pull-based strategy in comparison\nto a push-based approach is that the response time increases in the case of a\ncache miss.\nWhen comparing push-based and pull-based solutions, there are a number\nof trade-offs to be made, as shown in Figure 7.23. For simplicity, consider\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n390 CHAPTER 7. CONSISTENCY AND REPLICATION\nis that updates can often be propagated at minimal bandwidth costs, provided\nthe size of the parameters associated with an operation are relatively small.\nMoreover, the operations can be of arbitrary complexity, which may allow\nfurther improvements in keeping replicas consistent. On the other hand, more\nprocessing power may be required by each replica, especially in those cases\nwhen operations are relatively complex.\nPull versus push protocols\nAnother design issue is whether updates are pulled or pushed. In a push-\nbased approach , also referred to as server-based protocols , updates are prop-\nagated to other replicas without those replicas even asking for the updates.\nPush-based approaches are often used between permanent and server-initiated\nreplicas, but can also be used to push updates to client caches. Server-based\nprotocols are generally applied when strong consistency is required.\nThis need for strong consistency is related to the fact that permanent and\nserver-initiated replicas, as well as large shared caches, are often shared by\nmany clients, which, in turn, mainly perform read operations. Consequently,\nthe read-to-update ratio at each replica is relatively high. In these cases, push-\nbased protocols are ef\ufb01cient in the sense that every pushed update can be\nexpected to be of use for at least one, but perhaps more readers. In addition,\npush-based protocols make consistent data immediately available when asked\nfor.\nIn contrast, in a pull-based approach , a server or client requests another\nserver to send it any updates it has at that moment. Pull-based protocols, also\ncalled client-based protocols , are often used by client caches. For example, a\ncommon strategy applied to Web caches is \ufb01rst to check whether cached data\nitems are still up to date. When a cache receives a request for items that are\nstill locally available, the cache checks with the original Web server whether\nthose data items have been modi\ufb01ed since they were cached. In the case of\na modi\ufb01cation, the modi\ufb01ed data are \ufb01rst transferred to the cache, and then\nreturned to the requesting client. If no modi\ufb01cations took place, the cached\ndata are returned. In other words, the client polls the server to see whether\nan update is needed.\nA pull-based approach is ef\ufb01cient when the read-to-update ratio is rela-\ntively low. This is often the case with (nonshared) client caches, which have\nonly one client. However, even when a cache is shared by many clients, a\npull-based approach may also prove to be ef\ufb01cient when the cached data items\nare rarely shared. The main drawback of a pull-based strategy in comparison\nto a push-based approach is that the response time increases in the case of a\ncache miss.\nWhen comparing push-based and pull-based solutions, there are a number\nof trade-offs to be made, as shown in Figure 7.23. For simplicity, consider\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 391\na client-server system consisting of a single, nondistributed server, and a\nnumber of client processes, each having their own cache.\nIssue Push-based Pull-based\nState at server List of client replicas and caches None\nMessages sent Update (and possibly fetch update\nlater)Poll and update\nResponse time\nat clientImmediate (or fetch-update time) Fetch-update\ntime\nFigure 7.23: A comparison between push-based and pull-based protocols in\nthe case of multiple-client, single-server systems.\nAn important issue is that in push-based protocols, the server needs to\nkeep track of all client caches. Apart from the fact that stateful servers are\noften less fault tolerant, keeping track of all client caches may introduce a\nconsiderable overhead at the server. For example, in a push-based approach, a\nWeb server may easily need to keep track of tens of thousands of client caches.\nEach time a Web page is updated, the server will need to go through its list\nof client caches holding a copy of that page, and subsequently propagate the\nupdate. Worse yet, if a client purges a page due to lack of space, it has to\ninform the server, leading to even more communication.\nThe messages that need to be sent between a client and the server also\ndiffer. In a push-based approach, the only communication is that the server\nsends updates to each client. When updates are actually only invalidations,\nadditional communication is needed by a client to fetch the modi\ufb01ed data. In\na pull-based approach, a client will have to poll the server, and, if necessary,\nfetch the modi\ufb01ed data.\nFinally, the response time at the client is also different. When a server\npushes modi\ufb01ed data to the client caches, it is clear that the response time at\nthe client side is zero. When invalidations are pushed, the response time is\nthe same as in the pull-based approach, and is determined by the time it takes\nto fetch the modi\ufb01ed data from the server.\nThese trade-offs have lead to a hybrid form of update propagation based\non leases. In the case of replica management, a lease is a promise by the\nserver that it will push updates to the client for a speci\ufb01ed time. When a\nlease expires, the client is forced to poll the server for updates and pull in the\nmodi\ufb01ed data if necessary. An alternative is that a client requests a new lease\nfor pushing updates when the previous lease expires.\nLeases, originally introduced by Gray and Cheriton [1989], provide a\nconvenient mechanism for dynamically switching between a push-based\nand pull-based strategy. Consider the following lease system that allows\nthe expiration time to be dynamically adapted depending on different lease\ncriteria, described in [Duvvuri et al., 2003]. We distinguish the following three\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 391\na client-server system consisting of a single, nondistributed server, and a\nnumber of client processes, each having their own cache.\nIssue Push-based Pull-based\nState at server List of client replicas and caches None\nMessages sent Update (and possibly fetch update\nlater)Poll and update\nResponse time\nat clientImmediate (or fetch-update time) Fetch-update\ntime\nFigure 7.23: A comparison between push-based and pull-based protocols in\nthe case of multiple-client, single-server systems.\nAn important issue is that in push-based protocols, the server needs to\nkeep track of all client caches. Apart from the fact that stateful servers are\noften less fault tolerant, keeping track of all client caches may introduce a\nconsiderable overhead at the server. For example, in a push-based approach, a\nWeb server may easily need to keep track of tens of thousands of client caches.\nEach time a Web page is updated, the server will need to go through its list\nof client caches holding a copy of that page, and subsequently propagate the\nupdate. Worse yet, if a client purges a page due to lack of space, it has to\ninform the server, leading to even more communication.\nThe messages that need to be sent between a client and the server also\ndiffer. In a push-based approach, the only communication is that the server\nsends updates to each client. When updates are actually only invalidations,\nadditional communication is needed by a client to fetch the modi\ufb01ed data. In\na pull-based approach, a client will have to poll the server, and, if necessary,\nfetch the modi\ufb01ed data.\nFinally, the response time at the client is also different. When a server\npushes modi\ufb01ed data to the client caches, it is clear that the response time at\nthe client side is zero. When invalidations are pushed, the response time is\nthe same as in the pull-based approach, and is determined by the time it takes\nto fetch the modi\ufb01ed data from the server.\nThese trade-offs have lead to a hybrid form of update propagation based\non leases. In the case of replica management, a lease is a promise by the\nserver that it will push updates to the client for a speci\ufb01ed time. When a\nlease expires, the client is forced to poll the server for updates and pull in the\nmodi\ufb01ed data if necessary. An alternative is that a client requests a new lease\nfor pushing updates when the previous lease expires.\nLeases, originally introduced by Gray and Cheriton [1989], provide a\nconvenient mechanism for dynamically switching between a push-based\nand pull-based strategy. Consider the following lease system that allows\nthe expiration time to be dynamically adapted depending on different lease\ncriteria, described in [Duvvuri et al., 2003]. We distinguish the following three\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "392 CHAPTER 7. CONSISTENCY AND REPLICATION\ntypes of leases. (Note that in all cases, updates are pushed by the server as\nlong as the lease has not expired.)\nFirst, age-based leases are given out on data items depending on the last\ntime the item was modi\ufb01ed. The underlying assumption is that data that\nhave not been modi\ufb01ed for a long time can be expected to remain unmodi\ufb01ed\nfor some time yet to come. This assumption has shown to be reasonable\nin the case of, for example, Web-based data and regular \ufb01les. By granting\nlong-lasting leases to data items that are expected to remain unmodi\ufb01ed, the\nnumber of update messages can be strongly reduced compared to the case\nwhere all leases have the same expiration time.\nAnother lease criterion is how often a speci\ufb01c client requests its cached\ncopy to be updated. With renewal-frequency-based leases , a server will hand\nout a long-lasting lease to a client whose cache often needs to be refreshed.\nOn the other hand, a client that asks only occasionally for a speci\ufb01c data item\nwill be handed a short-term lease for that item. The effect of this strategy is\nthat the server essentially keeps track only of those clients where its data are\npopular; moreover, those clients are offered a high degree of consistency.\nThe last criterion is that of state-space overhead at the server. When\nthe server realizes that it is gradually becoming overloaded, it lowers the\nexpiration time of new leases it hands out to clients. The effect of this state-\nbased lease strategy is that the server needs to keep track of fewer clients as\nleases expire more quickly. In other words, the server dynamically switches\nto a more stateless mode of operation, thereby expecting to of\ufb02oad itself so\nthat it can handle requests more ef\ufb01ciently. The obvious drawback is that it\nmay need to do more work when the read-to-update ratio is high.\nUnicasting versus multicasting\nRelated to pushing or pulling updates is deciding whether unicasting or\nmulticasting should be used. In unicast communication, when a server that is\npart of the data store sends its update to Nother servers, it does so by sending\nNseparate messages, one to each server. With multicasting, the underlying\nnetwork takes care of sending a message ef\ufb01ciently to multiple receivers.\nIn many cases, it is cheaper to use available multicasting facilities. An\nextreme situation is when all replicas are located in the same local-area\nnetwork and that hardware broadcasting is available. In that case, broadcasting\nor multicasting a message is no more expensive than a single point-to-point\nmessage. Unicasting updates would then be less ef\ufb01cient.\nMulticasting can often be ef\ufb01ciently combined with a push-based approach\nto propagating updates. When the two are carefully integrated, a server that\ndecides to push its updates to a number of other servers simply uses a single\nmulticast group to send its updates. In contrast, with a pull-based approach, it\nis generally only a single client or server that requests its copy to be updated.\nIn that case, unicasting may be the most ef\ufb01cient solution.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n392 CHAPTER 7. CONSISTENCY AND REPLICATION\ntypes of leases. (Note that in all cases, updates are pushed by the server as\nlong as the lease has not expired.)\nFirst, age-based leases are given out on data items depending on the last\ntime the item was modi\ufb01ed. The underlying assumption is that data that\nhave not been modi\ufb01ed for a long time can be expected to remain unmodi\ufb01ed\nfor some time yet to come. This assumption has shown to be reasonable\nin the case of, for example, Web-based data and regular \ufb01les. By granting\nlong-lasting leases to data items that are expected to remain unmodi\ufb01ed, the\nnumber of update messages can be strongly reduced compared to the case\nwhere all leases have the same expiration time.\nAnother lease criterion is how often a speci\ufb01c client requests its cached\ncopy to be updated. With renewal-frequency-based leases , a server will hand\nout a long-lasting lease to a client whose cache often needs to be refreshed.\nOn the other hand, a client that asks only occasionally for a speci\ufb01c data item\nwill be handed a short-term lease for that item. The effect of this strategy is\nthat the server essentially keeps track only of those clients where its data are\npopular; moreover, those clients are offered a high degree of consistency.\nThe last criterion is that of state-space overhead at the server. When\nthe server realizes that it is gradually becoming overloaded, it lowers the\nexpiration time of new leases it hands out to clients. The effect of this state-\nbased lease strategy is that the server needs to keep track of fewer clients as\nleases expire more quickly. In other words, the server dynamically switches\nto a more stateless mode of operation, thereby expecting to of\ufb02oad itself so\nthat it can handle requests more ef\ufb01ciently. The obvious drawback is that it\nmay need to do more work when the read-to-update ratio is high.\nUnicasting versus multicasting\nRelated to pushing or pulling updates is deciding whether unicasting or\nmulticasting should be used. In unicast communication, when a server that is\npart of the data store sends its update to Nother servers, it does so by sending\nNseparate messages, one to each server. With multicasting, the underlying\nnetwork takes care of sending a message ef\ufb01ciently to multiple receivers.\nIn many cases, it is cheaper to use available multicasting facilities. An\nextreme situation is when all replicas are located in the same local-area\nnetwork and that hardware broadcasting is available. In that case, broadcasting\nor multicasting a message is no more expensive than a single point-to-point\nmessage. Unicasting updates would then be less ef\ufb01cient.\nMulticasting can often be ef\ufb01ciently combined with a push-based approach\nto propagating updates. When the two are carefully integrated, a server that\ndecides to push its updates to a number of other servers simply uses a single\nmulticast group to send its updates. In contrast, with a pull-based approach, it\nis generally only a single client or server that requests its copy to be updated.\nIn that case, unicasting may be the most ef\ufb01cient solution.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 393\nManaging replicated objects\nAs we mentioned, data-centric consistency for distributed objects comes\nnaturally in the form of entry consistency. Recall that in this case, the goal\nis to group operations on shared data using synchronization variables (e.g.,\nin the form of locks). As objects naturally combine data and the operations\non that data, locking objects during an invocation serializes access and keeps\nthem consistent.\nAlthough conceptually associating a lock with an object is simple, it does\nnot necessarily provide a proper solution when an object is replicated. There\nare two issues that need to be solved for implementing entry consistency. The\n\ufb01rst one is that we need a means to prevent concurrent execution of multiple\ninvocations on the same object. In other words, when any method of an\nobject is being executed, no other methods may be executed. This requirement\nensures that access to the internal data of an object is indeed serialized. Simply\nusing local locking mechanisms will ensure this serialization.\nThe second issue is that in the case of a replicated object, we need to ensure\nthat all changes to the replicated state of the object are the same. In other\nwords, we need to make sure that no two independent method invocations\ntake place on different replicas at the same time. This requirement implies\nthat we need to order invocations such that each replica sees all invocations\nin the same order. We describe a few general solutions in Section 7.5.\nIn many cases, designing replicated objects is done by \ufb01rst designing a\nsingle object, possibly protecting it against concurrent access through local\nlocking, and subsequently replicating it. The role of middleware is to ensure\nthat if a client invokes a replicated object, the invocation is passed to the\nreplicas and handed to the their respective object servers in the same order\neverywhere. However, we also need to ensure that all threads in those servers\nprocess those requests in the correct order as well. The problem is sketched in\nFigure 7.24.\nMultithreaded (object) servers simply pick up an incoming request, pass\nit on to an available thread, and wait for the next request to come in. The\nserver\u2019s thread scheduler subsequently allocates the CPU to runnable threads.\nOf course, if the middleware has done its best to provide a total ordering\nfor request delivery, the thread schedulers should operate in a deterministic\nfashion in order not to mix the ordering of method invocations on the same\nobject. In other words, If threads T1\n1and T2\n1from Figure 7.24 handle the\nsame incoming (replicated) invocation request, they should both be scheduled\nbefore T1\n2and T2\n2, respectively.\nOf course, simply scheduling allthreads deterministically is not necessary.\nIn principle, if we already have total-ordered request delivery, we need only\nto ensure that all requests for the same replicated object are handled in the\norder they were delivered. Such an approach would allow invocations for\ndifferent objects to be processed concurrently, and without further restrictions\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 393\nManaging replicated objects\nAs we mentioned, data-centric consistency for distributed objects comes\nnaturally in the form of entry consistency. Recall that in this case, the goal\nis to group operations on shared data using synchronization variables (e.g.,\nin the form of locks). As objects naturally combine data and the operations\non that data, locking objects during an invocation serializes access and keeps\nthem consistent.\nAlthough conceptually associating a lock with an object is simple, it does\nnot necessarily provide a proper solution when an object is replicated. There\nare two issues that need to be solved for implementing entry consistency. The\n\ufb01rst one is that we need a means to prevent concurrent execution of multiple\ninvocations on the same object. In other words, when any method of an\nobject is being executed, no other methods may be executed. This requirement\nensures that access to the internal data of an object is indeed serialized. Simply\nusing local locking mechanisms will ensure this serialization.\nThe second issue is that in the case of a replicated object, we need to ensure\nthat all changes to the replicated state of the object are the same. In other\nwords, we need to make sure that no two independent method invocations\ntake place on different replicas at the same time. This requirement implies\nthat we need to order invocations such that each replica sees all invocations\nin the same order. We describe a few general solutions in Section 7.5.\nIn many cases, designing replicated objects is done by \ufb01rst designing a\nsingle object, possibly protecting it against concurrent access through local\nlocking, and subsequently replicating it. The role of middleware is to ensure\nthat if a client invokes a replicated object, the invocation is passed to the\nreplicas and handed to the their respective object servers in the same order\neverywhere. However, we also need to ensure that all threads in those servers\nprocess those requests in the correct order as well. The problem is sketched in\nFigure 7.24.\nMultithreaded (object) servers simply pick up an incoming request, pass\nit on to an available thread, and wait for the next request to come in. The\nserver\u2019s thread scheduler subsequently allocates the CPU to runnable threads.\nOf course, if the middleware has done its best to provide a total ordering\nfor request delivery, the thread schedulers should operate in a deterministic\nfashion in order not to mix the ordering of method invocations on the same\nobject. In other words, If threads T1\n1and T2\n1from Figure 7.24 handle the\nsame incoming (replicated) invocation request, they should both be scheduled\nbefore T1\n2and T2\n2, respectively.\nOf course, simply scheduling allthreads deterministically is not necessary.\nIn principle, if we already have total-ordered request delivery, we need only\nto ensure that all requests for the same replicated object are handled in the\norder they were delivered. Such an approach would allow invocations for\ndifferent objects to be processed concurrently, and without further restrictions\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "394 CHAPTER 7. CONSISTENCY AND REPLICATION\nFigure 7.24: Deterministic thread scheduling for replicated object servers.\nfrom the thread scheduler. Unfortunately, only few systems exist that support\nsuch concurrency.\nOne approach, described by Basile et al. [2002], ensures that threads\nsharing the same (local) lock are scheduled in the same order on every replica.\nAt the basics lies a primary-based scheme in which one of the replica servers\ntakes the lead in determining, for a speci\ufb01c lock, which thread goes \ufb01rst.\nAn improvement that avoids frequent communication between servers is\ndescribed in [Basile et al., 2003]. Note that threads that do not share a lock\ncan thus operate concurrently on each server.\nOne drawback of this scheme is that it operates at the level of the under-\nlying operating system, meaning that every lock needs to be managed. By\nproviding application-level information, a huge improvement in performance\ncan be made by identifying only those locks that are needed for serializing\naccess to replicated objects (see Taiani et al. [2005]).\nNote 7.7 (Advanced: Replicated invocations)\nAnother problem that needs to be solved is that of replicated invocations. Consider\nan object Acalling another object Bas shown in Figure 7.25. Object Bis assumed\nto call yet another object C. IfBis replicated, each replica of Bwill, in principle,\ncallCindependently. The problem is that Cis now called multiple times instead\nof only once. If the called method on Cresults in the transfer of $100,000, then\nclearly, someone is going to complain sooner or later.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n394 CHAPTER 7. CONSISTENCY AND REPLICATION\nFigure 7.24: Deterministic thread scheduling for replicated object servers.\nfrom the thread scheduler. Unfortunately, only few systems exist that support\nsuch concurrency.\nOne approach, described by Basile et al. [2002], ensures that threads\nsharing the same (local) lock are scheduled in the same order on every replica.\nAt the basics lies a primary-based scheme in which one of the replica servers\ntakes the lead in determining, for a speci\ufb01c lock, which thread goes \ufb01rst.\nAn improvement that avoids frequent communication between servers is\ndescribed in [Basile et al., 2003]. Note that threads that do not share a lock\ncan thus operate concurrently on each server.\nOne drawback of this scheme is that it operates at the level of the under-\nlying operating system, meaning that every lock needs to be managed. By\nproviding application-level information, a huge improvement in performance\ncan be made by identifying only those locks that are needed for serializing\naccess to replicated objects (see Taiani et al. [2005]).\nNote 7.7 (Advanced: Replicated invocations)\nAnother problem that needs to be solved is that of replicated invocations. Consider\nan object Acalling another object Bas shown in Figure 7.25. Object Bis assumed\nto call yet another object C. IfBis replicated, each replica of Bwill, in principle,\ncallCindependently. The problem is that Cis now called multiple times instead\nof only once. If the called method on Cresults in the transfer of $100,000, then\nclearly, someone is going to complain sooner or later.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.4. REPLICA MANAGEMENT 395\nFigure 7.25: The problem of replicated method invocations.\nFigure 7.26: (a) Forwarding an invocation request from a replicated object\nto another replicated object. (b) Returning a reply from one replicated\nobject to another.\nThere are not many general-purpose solutions to solve the problem of repli-\ncated invocations. One solution is to simply forbid it [Maassen et al., 2001],\nwhich makes sense when performance is at stake. However, when replicating for\nfault tolerance, the following solution proposed by Mazouni et al. [1995] may be\ndeployed. Their solution is independent of the replication policy, that is, the exact\ndetails of how replicas are kept consistent. The essence is to provide a replication-\naware communication layer on top of which (replicated) objects execute. When\na replicated object Binvokes another replicated object C, the invocation request\nis \ufb01rst assigned the same, unique identi\ufb01er by each replica of B. At that point, a\ncoordinator of the replicas of Bforwards its request to all the replicas of object C,\nwhile the other replicas of Bhold back their copy of the invocation request, as\nshown in Figure 7.26. The result is that only a single request is forwarded to each\nreplica of C.\nThe same mechanism is used to ensure that only a single reply message is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.4. REPLICA MANAGEMENT 395\nFigure 7.25: The problem of replicated method invocations.\nFigure 7.26: (a) Forwarding an invocation request from a replicated object\nto another replicated object. (b) Returning a reply from one replicated\nobject to another.\nThere are not many general-purpose solutions to solve the problem of repli-\ncated invocations. One solution is to simply forbid it [Maassen et al., 2001],\nwhich makes sense when performance is at stake. However, when replicating for\nfault tolerance, the following solution proposed by Mazouni et al. [1995] may be\ndeployed. Their solution is independent of the replication policy, that is, the exact\ndetails of how replicas are kept consistent. The essence is to provide a replication-\naware communication layer on top of which (replicated) objects execute. When\na replicated object Binvokes another replicated object C, the invocation request\nis \ufb01rst assigned the same, unique identi\ufb01er by each replica of B. At that point, a\ncoordinator of the replicas of Bforwards its request to all the replicas of object C,\nwhile the other replicas of Bhold back their copy of the invocation request, as\nshown in Figure 7.26. The result is that only a single request is forwarded to each\nreplica of C.\nThe same mechanism is used to ensure that only a single reply message is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "396 CHAPTER 7. CONSISTENCY AND REPLICATION\nreturned to the replicas of B. This situation is shown in Figure 7.26. A coordinator\nof the replicas of Cnotices it is dealing with a replicated reply message that has\nbeen generated by each replica of C. However, only the coordinator forwards that\nreply to the replicas of object B, while the other replicas of Chold back their copy\nof the reply message.\nWhen a replica of Breceives a reply message for an invocation request it had\neither forwarded to Cor held back because it was not the coordinator, the reply is\nthen handed to the actual object.\nIn essence, the scheme just described is based on using multicast communica-\ntion, but in preventing that the same message is multicast by different replicas.\nAs such, it is essentially a sender-based scheme. An alternative solution is to let a\nreceiving replica detect multiple copies of incoming messages belonging to the\nsame invocation, and to pass only one copy to its associated object. Details of this\nscheme are left as an exercise.\n7.5 Consistency protocols\nWe now concentrate on the actual implementation of consistency models\nby taking a look at several consistency protocols. A consistency protocol\ndescribes an implementation of a speci\ufb01c consistency model. We follow the\norganization of our discussion on consistency models by \ufb01rst taking a look at\ndata-centric models, followed by protocols for client-centric models.\nContinuous consistency\nAs part of their work on continuous consistency, Yu and Vahdat [2000] have\ndeveloped a number of protocols to tackle the three forms of consistency. In\nthe following, we brie\ufb02y consider a number of solutions, omitting details for\nclarity.\nBounding numerical deviation\nWe \ufb01rst concentrate on one solution for keeping the numerical deviation\nwithin bounds. Again, our purpose is not to go into all the details for each\nprotocol, but rather to give the general idea. Details for bounding numerical\ndeviation can be found in [Yu and Vahdat, 2000].\nWe concentrate on writes to a single data item x. Each write W(x)has an\nassociated value that represents the numerical value by which xis updated,\ndenoted as val(W(x)), or simply val(W). For simplicity, we assume that\nval(W)>0. Each write Wis initially submitted to one out of the Navailable\nreplica servers, in which case that server becomes the write\u2019s origin , denoted\nasorigin (W). If we consider the system at a speci\ufb01c moment in time we will\nsee several submitted writes that still need to be propagated to all servers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n396 CHAPTER 7. CONSISTENCY AND REPLICATION\nreturned to the replicas of B. This situation is shown in Figure 7.26. A coordinator\nof the replicas of Cnotices it is dealing with a replicated reply message that has\nbeen generated by each replica of C. However, only the coordinator forwards that\nreply to the replicas of object B, while the other replicas of Chold back their copy\nof the reply message.\nWhen a replica of Breceives a reply message for an invocation request it had\neither forwarded to Cor held back because it was not the coordinator, the reply is\nthen handed to the actual object.\nIn essence, the scheme just described is based on using multicast communica-\ntion, but in preventing that the same message is multicast by different replicas.\nAs such, it is essentially a sender-based scheme. An alternative solution is to let a\nreceiving replica detect multiple copies of incoming messages belonging to the\nsame invocation, and to pass only one copy to its associated object. Details of this\nscheme are left as an exercise.\n7.5 Consistency protocols\nWe now concentrate on the actual implementation of consistency models\nby taking a look at several consistency protocols. A consistency protocol\ndescribes an implementation of a speci\ufb01c consistency model. We follow the\norganization of our discussion on consistency models by \ufb01rst taking a look at\ndata-centric models, followed by protocols for client-centric models.\nContinuous consistency\nAs part of their work on continuous consistency, Yu and Vahdat [2000] have\ndeveloped a number of protocols to tackle the three forms of consistency. In\nthe following, we brie\ufb02y consider a number of solutions, omitting details for\nclarity.\nBounding numerical deviation\nWe \ufb01rst concentrate on one solution for keeping the numerical deviation\nwithin bounds. Again, our purpose is not to go into all the details for each\nprotocol, but rather to give the general idea. Details for bounding numerical\ndeviation can be found in [Yu and Vahdat, 2000].\nWe concentrate on writes to a single data item x. Each write W(x)has an\nassociated value that represents the numerical value by which xis updated,\ndenoted as val(W(x)), or simply val(W). For simplicity, we assume that\nval(W)>0. Each write Wis initially submitted to one out of the Navailable\nreplica servers, in which case that server becomes the write\u2019s origin , denoted\nasorigin (W). If we consider the system at a speci\ufb01c moment in time we will\nsee several submitted writes that still need to be propagated to all servers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 397\nTo this end, each server Siwill keep track of a log Liof writes that it has\nperformed on its own local copy of x.\nLetTW[i,j]be the effect of performing the writes executed by server Si\nthat originated from server Sj:\nTW[i,j] =\u00e5fval(W)jorigin (W) = Sjand W2Lig\nNote that TW[i,i]represents the aggregated writes submitted to Si. Our\ngoal is for any time t, to let the current value viofxat server Sideviate\nwithin bounds from the actual value vofx. This actual value is completely\ndetermined by all submitted writes. That is, if v0is the initial value of x, then\nv=v0+N\n\u00e5\nk=1TW[k,k]\nand\nvi=v0+N\n\u00e5\nk=1TW[i,k]\nNote that vi\u0014v. Let us concentrate only on absolute deviations. In particular,\nfor every server Si, we associate an upperbound disuch that we need to\nenforce:\nv\u0000vi\u0014di\nWrites submitted to a server Siwill need to be propagated to all other servers.\nThere are different ways in which this can be done, but typically an epidemic\nprotocol will allow rapid dissemination of updates. In any case, when a server\nSipropagates a write originating from SjtoSk, the latter will be able to learn\nabout the value TW[i,j]at the time the write was sent. In other words, Skcan\nmaintain a view TW k[i,j]of what it believes Siwill have as value for TW[i,j].\nObviously,\n0\u0014TW k[i,j]\u0014TW[i,j]\u0014TW[j,j]\nThe whole idea is that when server Sknotices that Sihas not been staying in\nthe right pace with the updates that have been submitted to Sk, it forwards\nwrites from its log to Si. This forwarding effectively advances the view TW k[i,k]\nthat Skhas of TW[i,k], making the deviation TW[i,k]\u0000TW k[i,k]smaller. In\nparticular, Skadvances its view on TW[i,k]when an application submits a\nnew write that would increase TW[k,k]\u0000TW k[i,k]beyond di/(N\u00001). We\nleave it as an exercise to the reader to show that advancement always ensures\nthat v\u0000vi\u0014di.\nBounding staleness deviations\nThere are many ways to keep the staleness of replicas within speci\ufb01ed bounds.\nOne simple approach is to let server Skkeep a real-time vector clock RVC k\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 397\nTo this end, each server Siwill keep track of a log Liof writes that it has\nperformed on its own local copy of x.\nLetTW[i,j]be the effect of performing the writes executed by server Si\nthat originated from server Sj:\nTW[i,j] =\u00e5fval(W)jorigin (W) = Sjand W2Lig\nNote that TW[i,i]represents the aggregated writes submitted to Si. Our\ngoal is for any time t, to let the current value viofxat server Sideviate\nwithin bounds from the actual value vofx. This actual value is completely\ndetermined by all submitted writes. That is, if v0is the initial value of x, then\nv=v0+N\n\u00e5\nk=1TW[k,k]\nand\nvi=v0+N\n\u00e5\nk=1TW[i,k]\nNote that vi\u0014v. Let us concentrate only on absolute deviations. In particular,\nfor every server Si, we associate an upperbound disuch that we need to\nenforce:\nv\u0000vi\u0014di\nWrites submitted to a server Siwill need to be propagated to all other servers.\nThere are different ways in which this can be done, but typically an epidemic\nprotocol will allow rapid dissemination of updates. In any case, when a server\nSipropagates a write originating from SjtoSk, the latter will be able to learn\nabout the value TW[i,j]at the time the write was sent. In other words, Skcan\nmaintain a view TW k[i,j]of what it believes Siwill have as value for TW[i,j].\nObviously,\n0\u0014TW k[i,j]\u0014TW[i,j]\u0014TW[j,j]\nThe whole idea is that when server Sknotices that Sihas not been staying in\nthe right pace with the updates that have been submitted to Sk, it forwards\nwrites from its log to Si. This forwarding effectively advances the view TW k[i,k]\nthat Skhas of TW[i,k], making the deviation TW[i,k]\u0000TW k[i,k]smaller. In\nparticular, Skadvances its view on TW[i,k]when an application submits a\nnew write that would increase TW[k,k]\u0000TW k[i,k]beyond di/(N\u00001). We\nleave it as an exercise to the reader to show that advancement always ensures\nthat v\u0000vi\u0014di.\nBounding staleness deviations\nThere are many ways to keep the staleness of replicas within speci\ufb01ed bounds.\nOne simple approach is to let server Skkeep a real-time vector clock RVC k\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "398 CHAPTER 7. CONSISTENCY AND REPLICATION\nwhere RVC k[i] =timeans that Skhas seen all writes that have been submitted\ntoSiup to time ti. In this case, we assume that each submitted write is\ntimestamped by its origin server, and that tidenotes the time local to Si.\nIf the clocks between the replica servers are loosely synchronized, then an\nacceptable protocol for bounding staleness would be the following. Whenever\nserver Sknotes that tk\u0000RVC k[i]is about to exceed a speci\ufb01ed limit, it simply\nstarts pulling in writes that originated from Siwith a timestamp later than\nRVC k[i].\nNote that in this case a replica server is responsible for keeping its copy\nofxup to date regarding writes that have been issued elsewhere. In contrast,\nwhen maintaining numerical bounds, we followed a push approach by letting\nan origin server keep replicas up to date by forwarding writes. The problem\nwith pushing writes in the case of staleness is that no guarantees can be given\nfor consistency when it is unknown in advance what the maximal propagation\ntime will be. This situation is somewhat improved by pulling in updates, as\nmultiple servers can help to keep a server\u2019s copy of xfresh (i.e., up to date).\nBounding ordering deviations\nRecall that ordering deviations in continuous consistency are caused by the\nfact that a replica server tentatively applies updates that have been submitted\nto it. As a result, each server will have a local queue of tentative writes for\nwhich the actual order in which they are to be applied to the local copy of x\nstill needs to be determined. The ordering deviation is bounded by specifying\nthe maximal length of the queue of tentative writes.\nAs a consequence, detecting when ordering consistency needs to be en-\nforced is simple: when the length of this local queue exceeds a speci\ufb01ed\nmaximal length. At that point, a server will no longer accept any newly\nsubmitted writes, but will instead attempt to commit tentative writes by ne-\ngotiating with other servers in which order its writes should be executed. In\nother words, we need to enforce a globally consistent ordering of tentative\nwrites.\nPrimary-based protocols\nIn practice, we see that distributed applications generally follow consistency\nmodels that are relatively easy to understand. These models include those for\nbounding staleness deviations, and to a lesser extent also those for bounding\nnumerical deviations. When it comes to models that handle consistent order-\ning of operations, sequential consistency, notably those in which operations\ncan be grouped through locking or transactions are popular.\nAs soon as consistency models become slightly dif\ufb01cult to understand for\napplication developers, we see that they are ignored even if performance could\nbe improved. The bottom line is that if the semantics of a consistency model\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n398 CHAPTER 7. CONSISTENCY AND REPLICATION\nwhere RVC k[i] =timeans that Skhas seen all writes that have been submitted\ntoSiup to time ti. In this case, we assume that each submitted write is\ntimestamped by its origin server, and that tidenotes the time local to Si.\nIf the clocks between the replica servers are loosely synchronized, then an\nacceptable protocol for bounding staleness would be the following. Whenever\nserver Sknotes that tk\u0000RVC k[i]is about to exceed a speci\ufb01ed limit, it simply\nstarts pulling in writes that originated from Siwith a timestamp later than\nRVC k[i].\nNote that in this case a replica server is responsible for keeping its copy\nofxup to date regarding writes that have been issued elsewhere. In contrast,\nwhen maintaining numerical bounds, we followed a push approach by letting\nan origin server keep replicas up to date by forwarding writes. The problem\nwith pushing writes in the case of staleness is that no guarantees can be given\nfor consistency when it is unknown in advance what the maximal propagation\ntime will be. This situation is somewhat improved by pulling in updates, as\nmultiple servers can help to keep a server\u2019s copy of xfresh (i.e., up to date).\nBounding ordering deviations\nRecall that ordering deviations in continuous consistency are caused by the\nfact that a replica server tentatively applies updates that have been submitted\nto it. As a result, each server will have a local queue of tentative writes for\nwhich the actual order in which they are to be applied to the local copy of x\nstill needs to be determined. The ordering deviation is bounded by specifying\nthe maximal length of the queue of tentative writes.\nAs a consequence, detecting when ordering consistency needs to be en-\nforced is simple: when the length of this local queue exceeds a speci\ufb01ed\nmaximal length. At that point, a server will no longer accept any newly\nsubmitted writes, but will instead attempt to commit tentative writes by ne-\ngotiating with other servers in which order its writes should be executed. In\nother words, we need to enforce a globally consistent ordering of tentative\nwrites.\nPrimary-based protocols\nIn practice, we see that distributed applications generally follow consistency\nmodels that are relatively easy to understand. These models include those for\nbounding staleness deviations, and to a lesser extent also those for bounding\nnumerical deviations. When it comes to models that handle consistent order-\ning of operations, sequential consistency, notably those in which operations\ncan be grouped through locking or transactions are popular.\nAs soon as consistency models become slightly dif\ufb01cult to understand for\napplication developers, we see that they are ignored even if performance could\nbe improved. The bottom line is that if the semantics of a consistency model\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 399\nare not intuitively clear, application developers will have a hard time building\ncorrect applications. Simplicity is appreciated (and perhaps justi\ufb01ably so).\nIn the case of sequential consistency, it turns out that primary-based\nprotocols prevail. In these protocols, each data item xin the data store has\nan associated primary, which is responsible for coordinating write operations\nonx. A distinction can be made as to whether the primary is \ufb01xed at a\nremote server or if write operations can be carried out locally after moving\nthe primary to the process where the write operation is initiated.\nRemote-write protocols\nThe simplest primary-based protocol that supports replication is the one in\nwhich all write operations need to be forwarded to a \ufb01xed single server.\nRead operations can be carried out locally. Such schemes are also known\nasprimary-backup protocols [Budhijara et al., 1993]. A primary-backup\nprotocol works as shown in Figure 7.27. A process wanting to perform a write\noperation on data item x, forwards that operation to the primary server for\nx. The primary performs the update on its local copy of x, and subsequently\nforwards the update to the backup servers. Each backup server performs\nthe update as well, and sends an acknowledgment back to the primary (not\nshown). When all backups have updated their local copy, the primary sends\nan acknowledgment back to the initial process.\nFigure 7.27: The principle of a primary-backup protocol.\nA potential performance problem with this scheme is that it may take a\nrelatively long time before the process that initiated the update is allowed to\ncontinue. In effect, an update is implemented as a blocking operation. An\nalternative is to use a nonblocking approach. As soon as the primary has\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 399\nare not intuitively clear, application developers will have a hard time building\ncorrect applications. Simplicity is appreciated (and perhaps justi\ufb01ably so).\nIn the case of sequential consistency, it turns out that primary-based\nprotocols prevail. In these protocols, each data item xin the data store has\nan associated primary, which is responsible for coordinating write operations\nonx. A distinction can be made as to whether the primary is \ufb01xed at a\nremote server or if write operations can be carried out locally after moving\nthe primary to the process where the write operation is initiated.\nRemote-write protocols\nThe simplest primary-based protocol that supports replication is the one in\nwhich all write operations need to be forwarded to a \ufb01xed single server.\nRead operations can be carried out locally. Such schemes are also known\nasprimary-backup protocols [Budhijara et al., 1993]. A primary-backup\nprotocol works as shown in Figure 7.27. A process wanting to perform a write\noperation on data item x, forwards that operation to the primary server for\nx. The primary performs the update on its local copy of x, and subsequently\nforwards the update to the backup servers. Each backup server performs\nthe update as well, and sends an acknowledgment back to the primary (not\nshown). When all backups have updated their local copy, the primary sends\nan acknowledgment back to the initial process.\nFigure 7.27: The principle of a primary-backup protocol.\nA potential performance problem with this scheme is that it may take a\nrelatively long time before the process that initiated the update is allowed to\ncontinue. In effect, an update is implemented as a blocking operation. An\nalternative is to use a nonblocking approach. As soon as the primary has\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "400 CHAPTER 7. CONSISTENCY AND REPLICATION\nupdated its local copy of x, it returns an acknowledgment. After that, it tells\nthe backup servers to perform the update as well. Nonblocking primary-\nbackup protocols are discussed in [Budhiraja and Marzullo, 1992].\nThe main problem with nonblocking primary-backup protocols has to do\nwith fault tolerance. In a blocking scheme, the client process knows for sure\nthat the update operation is backed up by several other servers. This is not\nthe case with a nonblocking solution. The advantage, of course, is that write\noperations may speed up considerably.\nPrimary-backup protocols provide a straightforward implementation of\nsequential consistency, as the primary can order all incoming writes in a\nglobally unique time order. Evidently, all processes see all write operations\nin the same order, no matter which backup server they use to perform read\noperations. Also, with blocking protocols, processes will always see the effects\nof their most recent write operation (note that this cannot be guaranteed with\na nonblocking protocol without taking special measures).\nLocal-write protocols\nA variant of primary-backup protocols is one in which the primary copy\nmigrates between processes that wish to perform a write operation. As before,\nwhenever a process wants to update data item x, it locates the primary copy of\nx, and subsequently moves it to its own location, as shown in Figure 7.28. The\nmain advantage of this approach is that multiple, successive write operations\ncan be carried out locally, while reading processes can still access their local\ncopy. However, such an improvement can be achieved only if a nonblocking\nprotocol is followed by which updates are propagated to the replicas after the\nprimary has \ufb01nished with locally performing the updates.\nThis primary-backup local-write protocol can also be applied to mobile\ncomputers that are able to operate in disconnected mode. Before disconnect-\ning, the mobile computer becomes the primary server for each data item it\nexpects to update. While being disconnected, all update operations are carried\nout locally, while other processes can still perform read operations (but no\nupdates). Later, when connecting again, updates are propagated from the\nprimary to the backups, bringing the data store in a consistent state again.\nAs a last variant of this scheme, nonblocking local-write primary-based\nprotocols are also used for distributed \ufb01le systems in general. In this case, there\nmay be a \ufb01xed central server through which normally all write operations\ntake place, as in the case of remote-write primary backup. However, the server\ntemporarily allows one of the replicas to perform a series of local updates,\nas this may considerably speed up performance. When the replica server is\ndone, the updates are propagated to the central server, from where they are\nthen distributed to the other replica servers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n400 CHAPTER 7. CONSISTENCY AND REPLICATION\nupdated its local copy of x, it returns an acknowledgment. After that, it tells\nthe backup servers to perform the update as well. Nonblocking primary-\nbackup protocols are discussed in [Budhiraja and Marzullo, 1992].\nThe main problem with nonblocking primary-backup protocols has to do\nwith fault tolerance. In a blocking scheme, the client process knows for sure\nthat the update operation is backed up by several other servers. This is not\nthe case with a nonblocking solution. The advantage, of course, is that write\noperations may speed up considerably.\nPrimary-backup protocols provide a straightforward implementation of\nsequential consistency, as the primary can order all incoming writes in a\nglobally unique time order. Evidently, all processes see all write operations\nin the same order, no matter which backup server they use to perform read\noperations. Also, with blocking protocols, processes will always see the effects\nof their most recent write operation (note that this cannot be guaranteed with\na nonblocking protocol without taking special measures).\nLocal-write protocols\nA variant of primary-backup protocols is one in which the primary copy\nmigrates between processes that wish to perform a write operation. As before,\nwhenever a process wants to update data item x, it locates the primary copy of\nx, and subsequently moves it to its own location, as shown in Figure 7.28. The\nmain advantage of this approach is that multiple, successive write operations\ncan be carried out locally, while reading processes can still access their local\ncopy. However, such an improvement can be achieved only if a nonblocking\nprotocol is followed by which updates are propagated to the replicas after the\nprimary has \ufb01nished with locally performing the updates.\nThis primary-backup local-write protocol can also be applied to mobile\ncomputers that are able to operate in disconnected mode. Before disconnect-\ning, the mobile computer becomes the primary server for each data item it\nexpects to update. While being disconnected, all update operations are carried\nout locally, while other processes can still perform read operations (but no\nupdates). Later, when connecting again, updates are propagated from the\nprimary to the backups, bringing the data store in a consistent state again.\nAs a last variant of this scheme, nonblocking local-write primary-based\nprotocols are also used for distributed \ufb01le systems in general. In this case, there\nmay be a \ufb01xed central server through which normally all write operations\ntake place, as in the case of remote-write primary backup. However, the server\ntemporarily allows one of the replicas to perform a series of local updates,\nas this may considerably speed up performance. When the replica server is\ndone, the updates are propagated to the central server, from where they are\nthen distributed to the other replica servers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 401\nFigure 7.28: Primary-backup protocol in which the primary migrates to the\nprocess wanting to perform an update.\nReplicated-write protocols\nIn replicated-write protocols, write operations can be carried out at multiple\nreplicas instead of only one, as in the case of primary-based replicas. A\ndistinction can be made between active replication, in which an operation is\nforwarded to all replicas, and consistency protocols based on majority voting.\nActive replication\nIn active replication, each replica has an associated process that carries out\nupdate operations. In contrast to other protocols, updates are generally\npropagated by means of the write operation that causes the update. In other\nwords, the operation is sent to each replica. However, it is also possible to\nsend the update.\nOne problem with active replication is that operations need to be carried\nout in the same order everywhere. Consequently, what is needed is a total-\nordered multicast mechanism. A practical approach to accomplish total\nordering is by means of a central coordinator, also called a sequencer . One\napproach is to \ufb01rst forward each operation to the sequencer, which assigns it\na unique sequence number and subsequently forwards the operation to all\nreplicas. Operations are carried out in the order of their sequence number.\nNote 7.8 (Advanced: Achieving scalability)\nNote that using a sequencer may easily introduce scalability problems. In fact, if\ntotal-ordered multicasting is needed, a combination of symmetric multicasting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 401\nFigure 7.28: Primary-backup protocol in which the primary migrates to the\nprocess wanting to perform an update.\nReplicated-write protocols\nIn replicated-write protocols, write operations can be carried out at multiple\nreplicas instead of only one, as in the case of primary-based replicas. A\ndistinction can be made between active replication, in which an operation is\nforwarded to all replicas, and consistency protocols based on majority voting.\nActive replication\nIn active replication, each replica has an associated process that carries out\nupdate operations. In contrast to other protocols, updates are generally\npropagated by means of the write operation that causes the update. In other\nwords, the operation is sent to each replica. However, it is also possible to\nsend the update.\nOne problem with active replication is that operations need to be carried\nout in the same order everywhere. Consequently, what is needed is a total-\nordered multicast mechanism. A practical approach to accomplish total\nordering is by means of a central coordinator, also called a sequencer . One\napproach is to \ufb01rst forward each operation to the sequencer, which assigns it\na unique sequence number and subsequently forwards the operation to all\nreplicas. Operations are carried out in the order of their sequence number.\nNote 7.8 (Advanced: Achieving scalability)\nNote that using a sequencer may easily introduce scalability problems. In fact, if\ntotal-ordered multicasting is needed, a combination of symmetric multicasting\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "402 CHAPTER 7. CONSISTENCY AND REPLICATION\nusing Lamport timestamps [Lamport, 1978] and sequencers may be necessary.\nSuch a solution is described by Rodrigues et al. [1996]. The essence of that\nsolution is to have multiple sequencers multicast update operations to each other\nand order the updates using Lamport\u2019s total-ordering mechanism, as described\nin Section 6.2. Nonsequencing processes are grouped such that each group\nuses a single sequencer. Any nonsequencing process sends update requests to\nits sequencer and waits until it receives an acknowledgment that its request\nhas been processed (i.e., multicast to the other sequencers in a total-ordered\nfashion). Obviously, there is a trade-off between the number of processes that act\nas sequencer and those that do not, as well as the choice of processes to act as\nsequencer. As it turns out, this trade-off depends very much on the application\nand, in particular, the relative update rate at each process.\nQuorum-based protocols\nA different approach to supporting replicated writes is to use voting , as\noriginally proposed by Thomas [1979] and generalized by Gifford [1979].\nThe basic idea is to require clients to request and acquire the permission of\nmultiple servers before either reading or writing a replicated data item.\nAs a simple example of how the algorithm works, consider a distributed\n\ufb01le system and suppose that a \ufb01le is replicated on Nservers. We could make\na rule stating that to update a \ufb01le, a client must \ufb01rst contact at least half the\nservers plus one (a majority) and get them to agree to do the update. Once\nthey have agreed, the \ufb01le is changed and a new version number is associated\nwith the new \ufb01le. The version number is used to identify the version of the\n\ufb01le and is the same for all the newly updated \ufb01les.\nTo read a replicated \ufb01le, a client must also contact at least half the servers\nplus one and ask them to send the version numbers associated with the \ufb01le.\nIf all the version numbers are the same, this must be the most recent version\nbecause an attempt to update only the remaining servers would fail because\nthere are not enough of them.\nFor example, if there are \ufb01ve servers and a client determines that three of\nthem have version 8, it is impossible that the other two have version 9. After\nall, any successful update from version 8 to version 9 requires getting three\nservers to agree to it, not just two.\nWhen quorum-based replication was originally introduced, a somewhat\nmore general scheme was proposed. In it, to read a \ufb01le of which Nreplicas\nexist, a client needs to assemble a read quorum , an arbitrary collection of any\nNRservers, or more. Similarly, to modify a \ufb01le, a write quorum of at least\nNWservers is required. The values of NRand NWare subject to the following\ntwo constraints:\n1.NR+NW>N\n2.NW>N/2\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n402 CHAPTER 7. CONSISTENCY AND REPLICATION\nusing Lamport timestamps [Lamport, 1978] and sequencers may be necessary.\nSuch a solution is described by Rodrigues et al. [1996]. The essence of that\nsolution is to have multiple sequencers multicast update operations to each other\nand order the updates using Lamport\u2019s total-ordering mechanism, as described\nin Section 6.2. Nonsequencing processes are grouped such that each group\nuses a single sequencer. Any nonsequencing process sends update requests to\nits sequencer and waits until it receives an acknowledgment that its request\nhas been processed (i.e., multicast to the other sequencers in a total-ordered\nfashion). Obviously, there is a trade-off between the number of processes that act\nas sequencer and those that do not, as well as the choice of processes to act as\nsequencer. As it turns out, this trade-off depends very much on the application\nand, in particular, the relative update rate at each process.\nQuorum-based protocols\nA different approach to supporting replicated writes is to use voting , as\noriginally proposed by Thomas [1979] and generalized by Gifford [1979].\nThe basic idea is to require clients to request and acquire the permission of\nmultiple servers before either reading or writing a replicated data item.\nAs a simple example of how the algorithm works, consider a distributed\n\ufb01le system and suppose that a \ufb01le is replicated on Nservers. We could make\na rule stating that to update a \ufb01le, a client must \ufb01rst contact at least half the\nservers plus one (a majority) and get them to agree to do the update. Once\nthey have agreed, the \ufb01le is changed and a new version number is associated\nwith the new \ufb01le. The version number is used to identify the version of the\n\ufb01le and is the same for all the newly updated \ufb01les.\nTo read a replicated \ufb01le, a client must also contact at least half the servers\nplus one and ask them to send the version numbers associated with the \ufb01le.\nIf all the version numbers are the same, this must be the most recent version\nbecause an attempt to update only the remaining servers would fail because\nthere are not enough of them.\nFor example, if there are \ufb01ve servers and a client determines that three of\nthem have version 8, it is impossible that the other two have version 9. After\nall, any successful update from version 8 to version 9 requires getting three\nservers to agree to it, not just two.\nWhen quorum-based replication was originally introduced, a somewhat\nmore general scheme was proposed. In it, to read a \ufb01le of which Nreplicas\nexist, a client needs to assemble a read quorum , an arbitrary collection of any\nNRservers, or more. Similarly, to modify a \ufb01le, a write quorum of at least\nNWservers is required. The values of NRand NWare subject to the following\ntwo constraints:\n1.NR+NW>N\n2.NW>N/2\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 403\nThe \ufb01rst constraint is used to prevent read-write con\ufb02icts, whereas the\nsecond prevents write-write con\ufb02icts. Only after the appropriate number of\nservers has agreed to participate can a \ufb01le be read or written.\nTo see how this algorithm works, consider Figure 7.29(a) which has NR=3\nand NW=10. Imagine that the most recent write quorum consisted of the\n10 servers Cthrough L. All of these get the new version and the new version\nnumber. Any subsequent read quorum of three servers will have to contain at\nleast one member of this set. When the client looks at the version numbers, it\nwill know which is most recent and take that one.\n(a) (b) (c)\nFigure 7.29: Three examples of the voting algorithm. The gray areas denote a\nread quorum; the white ones a write quorum. Servers in the intersection are\ndenoted in boldface. (a) A correct choice of read and write set. (b) A choice\nthat may lead to write-write con\ufb02icts. (c) A correct choice, known as ROWA\n(read one, write all).\nIn Figure 7.29 we see two more examples. In Figure 7.29(b) a write-write\ncon\ufb02ict may occur because NW\u0014N/2. In particular, if one client chooses\nfA,B,C,E,F,Ggas its write set and another client chooses fD,H,I,J,K,Lgas\nits write set, then clearly we will run into trouble as the two updates will both\nbe accepted without detecting that they actually con\ufb02ict.\nThe situation shown in Figure 7.29(c) is especially interesting because it\nsets NRto one, making it possible to read a replicated \ufb01le by \ufb01nding any\ncopy and using it. The price paid for this good read performance, however,\nis that write updates need to acquire all copies. This scheme is generally\nreferred to as Read-One, Write-All , (ROWA ). There are several variations of\nquorum-based replication protocols. Jalote [1994] provides a good overview.\nCache-coherence protocols\nCaches form a special case of replication, in the sense that they are generally\ncontrolled by clients instead of servers. However, cache-coherence protocols,\nwhich ensure that a cache is consistent with the server-initiated replicas are,\nin principle, not very different from the consistency protocols discussed so far.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 403\nThe \ufb01rst constraint is used to prevent read-write con\ufb02icts, whereas the\nsecond prevents write-write con\ufb02icts. Only after the appropriate number of\nservers has agreed to participate can a \ufb01le be read or written.\nTo see how this algorithm works, consider Figure 7.29(a) which has NR=3\nand NW=10. Imagine that the most recent write quorum consisted of the\n10 servers Cthrough L. All of these get the new version and the new version\nnumber. Any subsequent read quorum of three servers will have to contain at\nleast one member of this set. When the client looks at the version numbers, it\nwill know which is most recent and take that one.\n(a) (b) (c)\nFigure 7.29: Three examples of the voting algorithm. The gray areas denote a\nread quorum; the white ones a write quorum. Servers in the intersection are\ndenoted in boldface. (a) A correct choice of read and write set. (b) A choice\nthat may lead to write-write con\ufb02icts. (c) A correct choice, known as ROWA\n(read one, write all).\nIn Figure 7.29 we see two more examples. In Figure 7.29(b) a write-write\ncon\ufb02ict may occur because NW\u0014N/2. In particular, if one client chooses\nfA,B,C,E,F,Ggas its write set and another client chooses fD,H,I,J,K,Lgas\nits write set, then clearly we will run into trouble as the two updates will both\nbe accepted without detecting that they actually con\ufb02ict.\nThe situation shown in Figure 7.29(c) is especially interesting because it\nsets NRto one, making it possible to read a replicated \ufb01le by \ufb01nding any\ncopy and using it. The price paid for this good read performance, however,\nis that write updates need to acquire all copies. This scheme is generally\nreferred to as Read-One, Write-All , (ROWA ). There are several variations of\nquorum-based replication protocols. Jalote [1994] provides a good overview.\nCache-coherence protocols\nCaches form a special case of replication, in the sense that they are generally\ncontrolled by clients instead of servers. However, cache-coherence protocols,\nwhich ensure that a cache is consistent with the server-initiated replicas are,\nin principle, not very different from the consistency protocols discussed so far.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "404 CHAPTER 7. CONSISTENCY AND REPLICATION\nThere has been much research in the design and implementation of caches,\nespecially in the context of shared-memory multiprocessor systems. Many\nsolutions are based on support from the underlying hardware, for example,\nby assuming that snooping or ef\ufb01cient broadcasting can be done. In the\ncontext of middleware-based distributed systems that are built on top of\ngeneral-purpose operating systems, software-based solutions to caches are\nmore interesting. In this case, two separate criteria are often maintained to\nclassify caching protocols (see also Min and Baer [1992], Lilja [1993], or Tartalja\nand Milutinovic [1997]).\nFirst, caching solutions may differ in their coherence detection strategy ,\nthat is, when inconsistencies are actually detected. In static solutions, a com-\npiler is assumed to perform the necessary analysis prior to execution, and to\ndetermine which data may actually lead to inconsistencies because they may\nbe cached. The compiler simply inserts instructions that avoid inconsistencies.\nDynamic solutions are typically applied in the distributed systems studied\nin this book. In these solutions, inconsistencies are detected at runtime. For\nexample, a check is made with the server to see whether the cached data have\nbeen modi\ufb01ed since they were cached.\nIn the case of distributed databases, dynamic detection-based protocols\ncan be further classi\ufb01ed by considering exactly when during a transaction\nthe detection is done. Franklin et al. [1997] distinguish the following three\ncases. First, when during a transaction a cached data item is accessed, the\nclient needs to verify whether that data item is still consistent with the version\nstored at the (possibly replicated) server. The transaction cannot proceed to\nuse the cached version until its consistency has been de\ufb01nitively validated.\nA second, optimistic, approach is to let the transaction proceed while\nveri\ufb01cation is taking place. In this case, it is assumed that the cached data\nwere up to date when the transaction started. If that assumption later proves\nto be false, the transaction will have to abort.\nThe third approach is to verify whether the cached data are up to date only\nwhen the transaction commits. In effect, the transaction just starts operating\non the cached data and hopes for the best. After all the work has been done,\naccessed data are veri\ufb01ed for consistency. When stale data were used, the\ntransaction is aborted.\nAnother design issue for cache-coherence protocols is the coherence en-\nforcement strategy , which determines how caches are kept consistent with the\ncopies stored at servers. The simplest solution is to disallow shared data to be\ncached at all. Instead, shared data are kept only at the servers, which maintain\nconsistency using one of the primary-based or replication-write protocols\ndiscussed above. Clients are allowed to cache only private data. Obviously,\nthis solution can offer only limited performance improvements.\nWhen shared data can be cached, there are two approaches to enforce\ncache coherence. The \ufb01rst is to let a server send an invalidation to all caches\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n404 CHAPTER 7. CONSISTENCY AND REPLICATION\nThere has been much research in the design and implementation of caches,\nespecially in the context of shared-memory multiprocessor systems. Many\nsolutions are based on support from the underlying hardware, for example,\nby assuming that snooping or ef\ufb01cient broadcasting can be done. In the\ncontext of middleware-based distributed systems that are built on top of\ngeneral-purpose operating systems, software-based solutions to caches are\nmore interesting. In this case, two separate criteria are often maintained to\nclassify caching protocols (see also Min and Baer [1992], Lilja [1993], or Tartalja\nand Milutinovic [1997]).\nFirst, caching solutions may differ in their coherence detection strategy ,\nthat is, when inconsistencies are actually detected. In static solutions, a com-\npiler is assumed to perform the necessary analysis prior to execution, and to\ndetermine which data may actually lead to inconsistencies because they may\nbe cached. The compiler simply inserts instructions that avoid inconsistencies.\nDynamic solutions are typically applied in the distributed systems studied\nin this book. In these solutions, inconsistencies are detected at runtime. For\nexample, a check is made with the server to see whether the cached data have\nbeen modi\ufb01ed since they were cached.\nIn the case of distributed databases, dynamic detection-based protocols\ncan be further classi\ufb01ed by considering exactly when during a transaction\nthe detection is done. Franklin et al. [1997] distinguish the following three\ncases. First, when during a transaction a cached data item is accessed, the\nclient needs to verify whether that data item is still consistent with the version\nstored at the (possibly replicated) server. The transaction cannot proceed to\nuse the cached version until its consistency has been de\ufb01nitively validated.\nA second, optimistic, approach is to let the transaction proceed while\nveri\ufb01cation is taking place. In this case, it is assumed that the cached data\nwere up to date when the transaction started. If that assumption later proves\nto be false, the transaction will have to abort.\nThe third approach is to verify whether the cached data are up to date only\nwhen the transaction commits. In effect, the transaction just starts operating\non the cached data and hopes for the best. After all the work has been done,\naccessed data are veri\ufb01ed for consistency. When stale data were used, the\ntransaction is aborted.\nAnother design issue for cache-coherence protocols is the coherence en-\nforcement strategy , which determines how caches are kept consistent with the\ncopies stored at servers. The simplest solution is to disallow shared data to be\ncached at all. Instead, shared data are kept only at the servers, which maintain\nconsistency using one of the primary-based or replication-write protocols\ndiscussed above. Clients are allowed to cache only private data. Obviously,\nthis solution can offer only limited performance improvements.\nWhen shared data can be cached, there are two approaches to enforce\ncache coherence. The \ufb01rst is to let a server send an invalidation to all caches\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 405\nwhenever a data item is modi\ufb01ed. The second is to simply propagate the\nupdate. Most caching systems use one of these two schemes. Dynamically\nchoosing between sending invalidations or updates is sometimes supported\nin client-server databases.\nFinally, we also need to consider what happens when a process modi\ufb01es\ncached data. When read-only caches are used, update operations can be\nperformed only by servers, which subsequently follow some distribution\nprotocol to ensure that updates are propagated to caches. In many cases, a\npull-based approach is followed. In this case, a client detects that its cache is\nstale, and requests a server for an update.\nAn alternative approach is to allow clients to directly modify the cached\ndata, and forward the update to the servers. This approach is followed\ninwrite-through caches , which are often used in distributed \ufb01le systems.\nIn effect, write-through caching is similar to a primary-based local-write\nprotocol in which the client\u2019s cache has become a temporary primary. To\nguarantee (sequential) consistency, it is necessary that the client has been\ngranted exclusive write permissions, or otherwise write-write con\ufb02icts may\noccur.\nWrite-through caches potentially offer improved performance over other\nschemes as all operations can be carried out locally. Further improvements can\nbe made if we delay the propagation of updates by allowing multiple writes\nto take place before informing the servers. This leads to what is known as a\nwrite-back cache , which is, again, mainly applied in distributed \ufb01le systems.\nNote 7.9 (Example: Client-side caching in NFS)\nAs a practical example, consider the general caching model in NFS as shown in\nFigure 7.30. Each client can have a memory cache that contains data previously\nread from the server. In addition, there may also be a disk cache that is added as\nan extension to the memory cache, using the same consistency parameters.\nFigure 7.30: Client-side caching in NFS.\nTypically, clients cache \ufb01le data, attributes, \ufb01le handles, and directories. Dif-\nferent strategies exist to handle consistency of the cached data, cached attributes,\nand so on. Let us \ufb01rst take a look at caching \ufb01le data.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 405\nwhenever a data item is modi\ufb01ed. The second is to simply propagate the\nupdate. Most caching systems use one of these two schemes. Dynamically\nchoosing between sending invalidations or updates is sometimes supported\nin client-server databases.\nFinally, we also need to consider what happens when a process modi\ufb01es\ncached data. When read-only caches are used, update operations can be\nperformed only by servers, which subsequently follow some distribution\nprotocol to ensure that updates are propagated to caches. In many cases, a\npull-based approach is followed. In this case, a client detects that its cache is\nstale, and requests a server for an update.\nAn alternative approach is to allow clients to directly modify the cached\ndata, and forward the update to the servers. This approach is followed\ninwrite-through caches , which are often used in distributed \ufb01le systems.\nIn effect, write-through caching is similar to a primary-based local-write\nprotocol in which the client\u2019s cache has become a temporary primary. To\nguarantee (sequential) consistency, it is necessary that the client has been\ngranted exclusive write permissions, or otherwise write-write con\ufb02icts may\noccur.\nWrite-through caches potentially offer improved performance over other\nschemes as all operations can be carried out locally. Further improvements can\nbe made if we delay the propagation of updates by allowing multiple writes\nto take place before informing the servers. This leads to what is known as a\nwrite-back cache , which is, again, mainly applied in distributed \ufb01le systems.\nNote 7.9 (Example: Client-side caching in NFS)\nAs a practical example, consider the general caching model in NFS as shown in\nFigure 7.30. Each client can have a memory cache that contains data previously\nread from the server. In addition, there may also be a disk cache that is added as\nan extension to the memory cache, using the same consistency parameters.\nFigure 7.30: Client-side caching in NFS.\nTypically, clients cache \ufb01le data, attributes, \ufb01le handles, and directories. Dif-\nferent strategies exist to handle consistency of the cached data, cached attributes,\nand so on. Let us \ufb01rst take a look at caching \ufb01le data.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "406 CHAPTER 7. CONSISTENCY AND REPLICATION\nNFSv4 supports two different approaches for caching \ufb01le data. The simplest\napproach is when a client opens a \ufb01le and caches the data it obtains from the\nserver as the result of various read operations. In addition, write operations can\nbe carried out in the cache as well. When the client closes the \ufb01le, NFS requires\nthat if modi\ufb01cations have taken place, the cached data must be \ufb02ushed back\nto the server. This approach corresponds to implementing session semantics as\ndiscussed earlier.\nOnce (part of) a \ufb01le has been cached, a client can keep its data in the cache\neven after closing the \ufb01le. Also, several clients on the same machine can share a\nsingle cache. NFS requires that whenever a client opens a previously closed \ufb01le\nthat has been (partly) cached, the client must immediately revalidate the cached\ndata. Revalidation takes place by checking when the \ufb01le was last modi\ufb01ed and\ninvalidating the cache in case it contains stale data.\nIn NFSv4 a server may delegate some of its rights to a client when a \ufb01le\nis opened. Open delegation takes place when the client machine is allowed to\nlocally handle open andclose operations from other clients on the same machine.\nNormally, the server is in charge of checking whether opening a \ufb01le should\nsucceed or not, for example, because share reservations need to be taken into\naccount. With open delegation, the client machine is sometimes allowed to make\nsuch decisions, avoiding the need to contact the server.\nFor example, if a server has delegated the opening of a \ufb01le to a client that\nrequested write permissions, \ufb01le locking requests from other clients on the same\nmachine can also be handled locally. The server will still handle locking requests\nfrom clients on other machines, by simply denying those clients access to the \ufb01le.\nNote that this scheme does not work in the case of delegating a \ufb01le to a client that\nrequested only read permissions. In that case, whenever another local client wants\nto have write permissions, it will have to contact the server; it is not possible to\nhandle the request locally.\nAn important consequence of delegating a \ufb01le to a client is that the server\nneeds to be able to recall the delegation, for example, when another client on a\ndifferent machine needs to obtain access rights to the \ufb01le. Recalling a delegation\nrequires that the server can do a callback to the client, as illustrated in Figure 7.31.\nFigure 7.31: Using the NFSv4 callback mechanism to recall \ufb01le delegation.\nA callback is implemented in NFS using its underlying RPC mechanisms.\nNote, however, that callbacks require that the server keeps track of clients to\nwhich it has delegated a \ufb01le. Here, we see another example where an NFS server\ncan no longer be implemented in a stateless manner. Note, however, that the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n406 CHAPTER 7. CONSISTENCY AND REPLICATION\nNFSv4 supports two different approaches for caching \ufb01le data. The simplest\napproach is when a client opens a \ufb01le and caches the data it obtains from the\nserver as the result of various read operations. In addition, write operations can\nbe carried out in the cache as well. When the client closes the \ufb01le, NFS requires\nthat if modi\ufb01cations have taken place, the cached data must be \ufb02ushed back\nto the server. This approach corresponds to implementing session semantics as\ndiscussed earlier.\nOnce (part of) a \ufb01le has been cached, a client can keep its data in the cache\neven after closing the \ufb01le. Also, several clients on the same machine can share a\nsingle cache. NFS requires that whenever a client opens a previously closed \ufb01le\nthat has been (partly) cached, the client must immediately revalidate the cached\ndata. Revalidation takes place by checking when the \ufb01le was last modi\ufb01ed and\ninvalidating the cache in case it contains stale data.\nIn NFSv4 a server may delegate some of its rights to a client when a \ufb01le\nis opened. Open delegation takes place when the client machine is allowed to\nlocally handle open andclose operations from other clients on the same machine.\nNormally, the server is in charge of checking whether opening a \ufb01le should\nsucceed or not, for example, because share reservations need to be taken into\naccount. With open delegation, the client machine is sometimes allowed to make\nsuch decisions, avoiding the need to contact the server.\nFor example, if a server has delegated the opening of a \ufb01le to a client that\nrequested write permissions, \ufb01le locking requests from other clients on the same\nmachine can also be handled locally. The server will still handle locking requests\nfrom clients on other machines, by simply denying those clients access to the \ufb01le.\nNote that this scheme does not work in the case of delegating a \ufb01le to a client that\nrequested only read permissions. In that case, whenever another local client wants\nto have write permissions, it will have to contact the server; it is not possible to\nhandle the request locally.\nAn important consequence of delegating a \ufb01le to a client is that the server\nneeds to be able to recall the delegation, for example, when another client on a\ndifferent machine needs to obtain access rights to the \ufb01le. Recalling a delegation\nrequires that the server can do a callback to the client, as illustrated in Figure 7.31.\nFigure 7.31: Using the NFSv4 callback mechanism to recall \ufb01le delegation.\nA callback is implemented in NFS using its underlying RPC mechanisms.\nNote, however, that callbacks require that the server keeps track of clients to\nwhich it has delegated a \ufb01le. Here, we see another example where an NFS server\ncan no longer be implemented in a stateless manner. Note, however, that the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.5. CONSISTENCY PROTOCOLS 407\ncombination of delegation and stateful servers may lead to various problems in\nthe presence of client and server failures. For example, what should a server do\nwhen it had delegated a \ufb01le to a now unresponsive client?\nClients can also cache attribute values, but are largely left on their own when\nit comes to keeping cached values consistent. In particular, attribute values of the\nsame \ufb01le cached by two different clients may be different unless the clients keep\nthese attributes mutually consistent. Modi\ufb01cations to an attribute value should\nbe immediately forwarded to the server, thus following a write-through cache\ncoherence policy.\nA similar approach is followed for caching \ufb01le handles (or rather, the name-\nto-\ufb01le handle mapping) and directories. To mitigate the effects of inconsistencies,\nNFS uses leases on cached attributes, \ufb01le handles, and directories. After some\ntime has elapsed, cache entries are thus automatically invalidated and revalidation\nis needed before they are used again.\nImplementing client-centric consistency\nFor our last topic on consistency protocols, let us draw our attention to imple-\nmenting client-centric consistency. Implementing client-centric consistency is\nrelatively straightforward if performance issues are ignored.\nIn a naive implementation of client-centric consistency, each write opera-\ntion Wis assigned a globally unique identi\ufb01er. Such an identi\ufb01er is assigned\nby the server to which the write had been submitted. We refer to this server\nas the origin ofW. Then, for each client, we keep track of two sets of writes.\nThe read set for a client consists of the writes relevant for the read operations\nperformed by a client. Likewise, the write set consists of the (identi\ufb01ers of\nthe) writes performed by the client.\nMonotonic-read consistency is implemented as follows. When a client\nperforms a read operation at a server, that server is handed the client\u2019s read\nset to check whether all the identi\ufb01ed writes have taken place locally. If not,\nit contacts the other servers to ensure that it is brought up to date before\ncarrying out the read operation. Alternatively, the read operation is forwarded\nto a server where the write operations have already taken place. After the\nread operation is performed, the write operations that have taken place at the\nselected server and which are relevant for the read operation are added to the\nclient\u2019s read set.\nNote that it should be possible to determine exactly where the write\noperations identi\ufb01ed in the read set have taken place. For example, the write\nidenti\ufb01er could include the identi\ufb01er of the server to which the operation was\nsubmitted. That server is required to, for example, log the write operation so\nthat it can be replayed at another server. In addition, write operations should\nbe performed in the order they were submitted. Ordering can be achieved by\nletting the client generate a globally unique sequence number that is included\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.5. CONSISTENCY PROTOCOLS 407\ncombination of delegation and stateful servers may lead to various problems in\nthe presence of client and server failures. For example, what should a server do\nwhen it had delegated a \ufb01le to a now unresponsive client?\nClients can also cache attribute values, but are largely left on their own when\nit comes to keeping cached values consistent. In particular, attribute values of the\nsame \ufb01le cached by two different clients may be different unless the clients keep\nthese attributes mutually consistent. Modi\ufb01cations to an attribute value should\nbe immediately forwarded to the server, thus following a write-through cache\ncoherence policy.\nA similar approach is followed for caching \ufb01le handles (or rather, the name-\nto-\ufb01le handle mapping) and directories. To mitigate the effects of inconsistencies,\nNFS uses leases on cached attributes, \ufb01le handles, and directories. After some\ntime has elapsed, cache entries are thus automatically invalidated and revalidation\nis needed before they are used again.\nImplementing client-centric consistency\nFor our last topic on consistency protocols, let us draw our attention to imple-\nmenting client-centric consistency. Implementing client-centric consistency is\nrelatively straightforward if performance issues are ignored.\nIn a naive implementation of client-centric consistency, each write opera-\ntion Wis assigned a globally unique identi\ufb01er. Such an identi\ufb01er is assigned\nby the server to which the write had been submitted. We refer to this server\nas the origin ofW. Then, for each client, we keep track of two sets of writes.\nThe read set for a client consists of the writes relevant for the read operations\nperformed by a client. Likewise, the write set consists of the (identi\ufb01ers of\nthe) writes performed by the client.\nMonotonic-read consistency is implemented as follows. When a client\nperforms a read operation at a server, that server is handed the client\u2019s read\nset to check whether all the identi\ufb01ed writes have taken place locally. If not,\nit contacts the other servers to ensure that it is brought up to date before\ncarrying out the read operation. Alternatively, the read operation is forwarded\nto a server where the write operations have already taken place. After the\nread operation is performed, the write operations that have taken place at the\nselected server and which are relevant for the read operation are added to the\nclient\u2019s read set.\nNote that it should be possible to determine exactly where the write\noperations identi\ufb01ed in the read set have taken place. For example, the write\nidenti\ufb01er could include the identi\ufb01er of the server to which the operation was\nsubmitted. That server is required to, for example, log the write operation so\nthat it can be replayed at another server. In addition, write operations should\nbe performed in the order they were submitted. Ordering can be achieved by\nletting the client generate a globally unique sequence number that is included\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "408 CHAPTER 7. CONSISTENCY AND REPLICATION\nin the write identi\ufb01er. If each data item can be modi\ufb01ed only by its owner,\nthe latter can supply the sequence number.\nMonotonic-write consistency is implemented analogous to monotonic\nreads. Whenever a client initiates a new write operation at a server, the\nserver is handed over the client\u2019s write set. (Again, the size of the set may be\nprohibitively large in the face of performance requirements. An alternative\nsolution is discussed below.) It then ensures that the identi\ufb01ed write opera-\ntions are performed \ufb01rst and in the correct order. After performing the new\noperation, that operation\u2019s write identi\ufb01er is added to the write set. Note that\nbringing the current server up to date with the client\u2019s write set may introduce\na considerable increase in the client\u2019s response time since the client then waits\nfor the operation to fully complete.\nLikewise, read-your-writes consistency requires that the server where the\nread operation is performed has seen all the write operations in the client\u2019s\nwrite set. The writes can simply be fetched from other servers before the\nread operation is performed, although this may lead to a poor response\ntime. Alternatively, the client-side software can search for a server where\nthe identi\ufb01ed write operations in the client\u2019s write set have already been\nperformed.\nFinally, writes-follow-reads consistency can be implemented by \ufb01rst bring-\ning the selected server up to date with the write operations in the client\u2019s read\nset, and then later adding the identi\ufb01er of the write operation to the write set,\nalong with the identi\ufb01ers in the read set (which have now become relevant\nfor the write operation just performed).\nNote 7.10 (Advanced: Improving ef\ufb01ciency)\nIt is easy to see that the read set and write set associated with each client can\nbecome very large. To keep these sets manageable, a client\u2019s read and write\noperations are grouped into sessions. A session is typically associated with an\napplication: it is opened when the application starts and is closed when it exits.\nHowever, sessions may also be associated with applications that are temporarily\nexited, such as user agents for e-mail. Whenever a client closes a session, the sets\nare simply cleared. Of course, if a client opens a session that it never closes, the\nassociated read and write sets can still become very large.\nThe main problem with a naive implementation lies in the representation of\nthe read and write sets. Each set consists of a number of identi\ufb01ers for write\noperations. Whenever a client forwards a read or write request to a server, a set\nof identi\ufb01ers is handed to the server as well to see whether all write operations\nrelevant to the request have been carried out by that server.\nThis information can be more ef\ufb01ciently represented by means of vector\ntimestamps as follows. First, whenever a server accepts a new write operation W,\nit assigns that operation a globally unique identi\ufb01er along with a timestamp ts(W).\nA subsequent write operation submitted to that server is assigned a higher-valued\ntimestamp. Each server Simaintains a vector timestamp WVC i, where WVC i[j]is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n408 CHAPTER 7. CONSISTENCY AND REPLICATION\nin the write identi\ufb01er. If each data item can be modi\ufb01ed only by its owner,\nthe latter can supply the sequence number.\nMonotonic-write consistency is implemented analogous to monotonic\nreads. Whenever a client initiates a new write operation at a server, the\nserver is handed over the client\u2019s write set. (Again, the size of the set may be\nprohibitively large in the face of performance requirements. An alternative\nsolution is discussed below.) It then ensures that the identi\ufb01ed write opera-\ntions are performed \ufb01rst and in the correct order. After performing the new\noperation, that operation\u2019s write identi\ufb01er is added to the write set. Note that\nbringing the current server up to date with the client\u2019s write set may introduce\na considerable increase in the client\u2019s response time since the client then waits\nfor the operation to fully complete.\nLikewise, read-your-writes consistency requires that the server where the\nread operation is performed has seen all the write operations in the client\u2019s\nwrite set. The writes can simply be fetched from other servers before the\nread operation is performed, although this may lead to a poor response\ntime. Alternatively, the client-side software can search for a server where\nthe identi\ufb01ed write operations in the client\u2019s write set have already been\nperformed.\nFinally, writes-follow-reads consistency can be implemented by \ufb01rst bring-\ning the selected server up to date with the write operations in the client\u2019s read\nset, and then later adding the identi\ufb01er of the write operation to the write set,\nalong with the identi\ufb01ers in the read set (which have now become relevant\nfor the write operation just performed).\nNote 7.10 (Advanced: Improving ef\ufb01ciency)\nIt is easy to see that the read set and write set associated with each client can\nbecome very large. To keep these sets manageable, a client\u2019s read and write\noperations are grouped into sessions. A session is typically associated with an\napplication: it is opened when the application starts and is closed when it exits.\nHowever, sessions may also be associated with applications that are temporarily\nexited, such as user agents for e-mail. Whenever a client closes a session, the sets\nare simply cleared. Of course, if a client opens a session that it never closes, the\nassociated read and write sets can still become very large.\nThe main problem with a naive implementation lies in the representation of\nthe read and write sets. Each set consists of a number of identi\ufb01ers for write\noperations. Whenever a client forwards a read or write request to a server, a set\nof identi\ufb01ers is handed to the server as well to see whether all write operations\nrelevant to the request have been carried out by that server.\nThis information can be more ef\ufb01ciently represented by means of vector\ntimestamps as follows. First, whenever a server accepts a new write operation W,\nit assigns that operation a globally unique identi\ufb01er along with a timestamp ts(W).\nA subsequent write operation submitted to that server is assigned a higher-valued\ntimestamp. Each server Simaintains a vector timestamp WVC i, where WVC i[j]is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 409\nequal to the timestamp of the most recent write operation originating from Sjthat\nhas been processed by Si.\nFor clarity, assume that for each server, writes from Sjare processed in the\norder that they were submitted. Whenever a client issues a request to perform\na read or write operation Oat a speci\ufb01c server, that server returns its current\ntimestamp along with the results of O. Read and write sets are subsequently\nrepresented by vector timestamps. More speci\ufb01cally, for each session A, we\nconstruct a vector timestamp SVC Awith SVC A[i]set equal to the maximum\ntimestamp of all write operations in Athat originate from server Si:\nSVC A[j] =maxfts(W)jW2Aand origin (W) =Sjg\nIn other words, the timestamp of a session always represents the latest write\noperations that have been seen by the applications that are being executed as part\nof that session. The compactness is obtained by representing all observed write\noperations originating from the same server through a single timestamp.\nAs an example, suppose a client, as part of session A, logs in at server Si. To\nthat end, it passes SVC AtoSi. Assume that SVC A[j]>WVC i[j]. What this means\nis that Sihas not yet seen all the writes originating from Sjthat the client has\nseen. Depending on the required consistency, server Simay now have to fetch\nthese writes before being able to consistently report back to the client. Once the\noperation has been performed, server Siwill return its current timestamp WVC i.\nAt that point, SVC Ais adjusted to:\nSVC A[j] maxfSVC A[j],WVC i[j]g\nAgain, we see how vector timestamps can provide an elegant and compact way of\nrepresenting history in a distributed system.\n7.6 Example: Caching and replication in the Web\nThe Web is arguably the largest distributed system ever built. Originating from\na relatively simple client-server architecture, it is now a sophisticated system\nconsisting of many techniques to ensure stringent performance and availability\nrequirements. These requirements have led to numerous proposals for caching\nand replicating Web content. Where the original schemes (which are still\nlargely deployed) have been targeted toward supporting static content, much\neffort has also been put into supporting dynamic content, that is, supporting\ndocuments that are generated on-the-spot as the result of a request, as well as\nthose containing scripts and such. An overview of traditional Web caching\nand replication is provided by Rabinovich and Spastscheck [2002].\nClient-side caching in the Web generally occurs at two places. In the \ufb01rst\nplace, most browsers are equipped with a relatively simple caching facility.\nWhenever a document is fetched it is stored in the browser\u2019s cache from where\nit is loaded the next time. In the second place, a client\u2019s site often runs a Web\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 409\nequal to the timestamp of the most recent write operation originating from Sjthat\nhas been processed by Si.\nFor clarity, assume that for each server, writes from Sjare processed in the\norder that they were submitted. Whenever a client issues a request to perform\na read or write operation Oat a speci\ufb01c server, that server returns its current\ntimestamp along with the results of O. Read and write sets are subsequently\nrepresented by vector timestamps. More speci\ufb01cally, for each session A, we\nconstruct a vector timestamp SVC Awith SVC A[i]set equal to the maximum\ntimestamp of all write operations in Athat originate from server Si:\nSVC A[j] =maxfts(W)jW2Aand origin (W) =Sjg\nIn other words, the timestamp of a session always represents the latest write\noperations that have been seen by the applications that are being executed as part\nof that session. The compactness is obtained by representing all observed write\noperations originating from the same server through a single timestamp.\nAs an example, suppose a client, as part of session A, logs in at server Si. To\nthat end, it passes SVC AtoSi. Assume that SVC A[j]>WVC i[j]. What this means\nis that Sihas not yet seen all the writes originating from Sjthat the client has\nseen. Depending on the required consistency, server Simay now have to fetch\nthese writes before being able to consistently report back to the client. Once the\noperation has been performed, server Siwill return its current timestamp WVC i.\nAt that point, SVC Ais adjusted to:\nSVC A[j] maxfSVC A[j],WVC i[j]g\nAgain, we see how vector timestamps can provide an elegant and compact way of\nrepresenting history in a distributed system.\n7.6 Example: Caching and replication in the Web\nThe Web is arguably the largest distributed system ever built. Originating from\na relatively simple client-server architecture, it is now a sophisticated system\nconsisting of many techniques to ensure stringent performance and availability\nrequirements. These requirements have led to numerous proposals for caching\nand replicating Web content. Where the original schemes (which are still\nlargely deployed) have been targeted toward supporting static content, much\neffort has also been put into supporting dynamic content, that is, supporting\ndocuments that are generated on-the-spot as the result of a request, as well as\nthose containing scripts and such. An overview of traditional Web caching\nand replication is provided by Rabinovich and Spastscheck [2002].\nClient-side caching in the Web generally occurs at two places. In the \ufb01rst\nplace, most browsers are equipped with a relatively simple caching facility.\nWhenever a document is fetched it is stored in the browser\u2019s cache from where\nit is loaded the next time. In the second place, a client\u2019s site often runs a Web\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "410 CHAPTER 7. CONSISTENCY AND REPLICATION\nproxy. A Web proxy accepts requests from local clients and passes these to\nWeb servers. When a response comes in, the result is passed to the client. The\nadvantage of this approach is that the proxy can cache the result and return\nthat result to another client, if necessary. In other words, a Web proxy can\nimplement a shared cache. With so many documents being generated on the\n\ufb02y, the server generally provides the document in pieces instructing the client\nto cache only those parts that are not likely to change when the document is\nrequested a next time.\nIn addition to caching at browsers and proxies, ISPs generally also place\ncaches in their networks. Such schemes are mainly used to reduce network\ntraf\ufb01c (which is good for the ISP) and to improve performance (which is\ngood for end users). However, with multiple caches along the request path\nfrom client to server, there is a risk of increased latencies when caches do not\ncontain the requested information.\nNote 7.11 (Advanced: Cooperative caching)\nAs an alternative to building hierarchical caches, one can also organize caches\nfor cooperative deployment as shown in Figure 7.32. In cooperative caching or\ndistributed caching , whenever a cache miss occurs at a Web proxy, the proxy\n\ufb01rst checks a number of neighboring proxies to see if one of them contains the\nrequested document. If such a check fails, the proxy forwards the request to the\nWeb server responsible for the document. In more traditional settings, this scheme\nis primarily deployed with Web caches belonging to the same organization or\ninstitution.\nFigure 7.32: The principle of cooperative caching.\nA study by Wolman et al. [1999] shows that cooperative caching may be effec-\ntive for only relatively small groups of clients (in the order of tens of thousands\nof users). However, such groups can also be serviced by using a single proxy\ncache, which is much cheaper in terms of communication and resource usage.\nHowever, in a study from a decade later, Wendell and Freedman [2011] show\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n410 CHAPTER 7. CONSISTENCY AND REPLICATION\nproxy. A Web proxy accepts requests from local clients and passes these to\nWeb servers. When a response comes in, the result is passed to the client. The\nadvantage of this approach is that the proxy can cache the result and return\nthat result to another client, if necessary. In other words, a Web proxy can\nimplement a shared cache. With so many documents being generated on the\n\ufb02y, the server generally provides the document in pieces instructing the client\nto cache only those parts that are not likely to change when the document is\nrequested a next time.\nIn addition to caching at browsers and proxies, ISPs generally also place\ncaches in their networks. Such schemes are mainly used to reduce network\ntraf\ufb01c (which is good for the ISP) and to improve performance (which is\ngood for end users). However, with multiple caches along the request path\nfrom client to server, there is a risk of increased latencies when caches do not\ncontain the requested information.\nNote 7.11 (Advanced: Cooperative caching)\nAs an alternative to building hierarchical caches, one can also organize caches\nfor cooperative deployment as shown in Figure 7.32. In cooperative caching or\ndistributed caching , whenever a cache miss occurs at a Web proxy, the proxy\n\ufb01rst checks a number of neighboring proxies to see if one of them contains the\nrequested document. If such a check fails, the proxy forwards the request to the\nWeb server responsible for the document. In more traditional settings, this scheme\nis primarily deployed with Web caches belonging to the same organization or\ninstitution.\nFigure 7.32: The principle of cooperative caching.\nA study by Wolman et al. [1999] shows that cooperative caching may be effec-\ntive for only relatively small groups of clients (in the order of tens of thousands\nof users). However, such groups can also be serviced by using a single proxy\ncache, which is much cheaper in terms of communication and resource usage.\nHowever, in a study from a decade later, Wendell and Freedman [2011] show\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 411\nthat in a highly decentralized system, cooperative caching actually turned out be\nhighly effective. These studies do not necessarily contradict each other: in both\ncases, the conclusion is that the effect of cooperative caching depends highly on\nthe demands from clients.\nA comparison between hierarchical and cooperative caching by Rodriguez\net al. [2001] makes clear that there are various trade-offs to make. For example,\nbecause cooperative caches are generally connected through high-speed links,\nthe transmission time needed to fetch a document is much lower than for a\nhierarchical cache. Also, as is to be expected, storage requirements are less strict\nfor cooperative caches than hierarchical ones.\nDifferent cache-consistency protocols have been deployed in the Web.\nTo guarantee that a document returned from the cache is consistent, some\nWeb proxies \ufb01rst send a conditional HTTP getrequest to the server with an\nadditional If-Modi\fed -Since request header, specifying the last modi\ufb01cation\ntime associated with the cached document. Only if the document has been\nchanged since that time, will the server return the entire document. Otherwise,\nthe Web proxy can simply return its cached version to the requesting local\nclient, which corresponds to a pull-based protocol.\nUnfortunately, this strategy requires that the proxy contacts a server for\neach request. To improve performance at the cost of weaker consistency,\nthe widely-used Squid Web proxy [Wessels, 2004] assigns an expiration time\nTexpire that depends on how long ago the document was last modi\ufb01ed when\nit is cached. In particular, if Tlast_modi\ufb01ed is the last modi\ufb01cation time of a\ndocument (as recorded by its owner), and Tcached is the time it was cached,\nthen\nTexpire=a(Tcached\u0000Tlast_modi\ufb01ed ) +Tcached\nwith a=0.2(this value has been derived from practical experience). Until\nTexpire, the document is considered valid and the proxy will not contact the\nserver. After the expiration time, the proxy requests the server to send a fresh\ncopy, unless it had not been modi\ufb01ed. We note that Squid also allows the\nexpiration time to be bounded by a minimum and a maximum time.\nAs an alternative to a pull-based protocol is that the server noti\ufb01es proxies\nthat a document has been modi\ufb01ed by sending an invalidation. The problem\nwith this approach for Web proxies is that the server may need to keep track\nof a large number of proxies, inevitably leading to a scalability problem.\nHowever, by combining leases and invalidations, the state to be maintained\nat the server can be kept within acceptable bounds. Note that this state is\nlargely dictated by the expiration times set for leases: the lower, the less\ncaches a server needs to keep track of. Nevertheless, invalidation protocols\nfor Web proxy caches are hardly ever applied. A comparison of Web caching\nconsistency policies can be found in [Cao and Ozsu, 2002]. Their conclusion\nis that letting the server send invalidations can outperform any other method\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 411\nthat in a highly decentralized system, cooperative caching actually turned out be\nhighly effective. These studies do not necessarily contradict each other: in both\ncases, the conclusion is that the effect of cooperative caching depends highly on\nthe demands from clients.\nA comparison between hierarchical and cooperative caching by Rodriguez\net al. [2001] makes clear that there are various trade-offs to make. For example,\nbecause cooperative caches are generally connected through high-speed links,\nthe transmission time needed to fetch a document is much lower than for a\nhierarchical cache. Also, as is to be expected, storage requirements are less strict\nfor cooperative caches than hierarchical ones.\nDifferent cache-consistency protocols have been deployed in the Web.\nTo guarantee that a document returned from the cache is consistent, some\nWeb proxies \ufb01rst send a conditional HTTP getrequest to the server with an\nadditional If-Modi\fed -Since request header, specifying the last modi\ufb01cation\ntime associated with the cached document. Only if the document has been\nchanged since that time, will the server return the entire document. Otherwise,\nthe Web proxy can simply return its cached version to the requesting local\nclient, which corresponds to a pull-based protocol.\nUnfortunately, this strategy requires that the proxy contacts a server for\neach request. To improve performance at the cost of weaker consistency,\nthe widely-used Squid Web proxy [Wessels, 2004] assigns an expiration time\nTexpire that depends on how long ago the document was last modi\ufb01ed when\nit is cached. In particular, if Tlast_modi\ufb01ed is the last modi\ufb01cation time of a\ndocument (as recorded by its owner), and Tcached is the time it was cached,\nthen\nTexpire=a(Tcached\u0000Tlast_modi\ufb01ed ) +Tcached\nwith a=0.2(this value has been derived from practical experience). Until\nTexpire, the document is considered valid and the proxy will not contact the\nserver. After the expiration time, the proxy requests the server to send a fresh\ncopy, unless it had not been modi\ufb01ed. We note that Squid also allows the\nexpiration time to be bounded by a minimum and a maximum time.\nAs an alternative to a pull-based protocol is that the server noti\ufb01es proxies\nthat a document has been modi\ufb01ed by sending an invalidation. The problem\nwith this approach for Web proxies is that the server may need to keep track\nof a large number of proxies, inevitably leading to a scalability problem.\nHowever, by combining leases and invalidations, the state to be maintained\nat the server can be kept within acceptable bounds. Note that this state is\nlargely dictated by the expiration times set for leases: the lower, the less\ncaches a server needs to keep track of. Nevertheless, invalidation protocols\nfor Web proxy caches are hardly ever applied. A comparison of Web caching\nconsistency policies can be found in [Cao and Ozsu, 2002]. Their conclusion\nis that letting the server send invalidations can outperform any other method\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "412 CHAPTER 7. CONSISTENCY AND REPLICATION\nin terms of bandwidth and perceived client latency, while maintaining cached\ndocuments consistent with those at the origin server.\nFinally, we should also mention that much research has been conducted to\n\ufb01nd out what the best cache replacement strategies are. Numerous proposals\nexist, but by-and-large, simple replacement strategies such as evicting the\nleast recently used object work well enough. An in-depth survey of replace-\nment strategies is presented by Podling and Boszormenyi [2003]; Ali et al.\n[2011] provide a more recent overview which also includes Web prefetching\ntechniques.\nAs the importance of the Web continues to increase as a vehicle for organi-\nzations to present themselves and to directly interact with end users, we see\na shift between maintaining the content of a Web site and making sure that\nthe site is easily and continuously accessible. This distinction has paved the\nway for content delivery networks (CDN ). The main idea underlying these\nCDNs is that they act as a Web hosting service, providing an infrastructure\nfor distributing and replicating the Web documents of multiple sites across\nthe Internet. The size of the infrastructure can be impressive. For example,\nas of 2016, Akamai is reported to have over 200,000 servers spread across 120\ncountries.\nThe sheer size of a CDN requires that hosted documents are automatically\ndistributed and replicated. In most cases, a large-scale CDN is organized\nalong the lines of a feedback-control loop, as shown in Figure 7.33 and which\nis described extensively in [Sivasubramanian et al., 2004b].\nFigure 7.33: The general organization of a CDN as a feedback-control system.\nThere are essentially three different kinds of aspects related to replication\nin Web hosting systems: metric estimation, adaptation triggering, and taking\nappropriate measures. The latter can be subdivided into replica placement de-\ncisions, consistency enforcement, and client-request routing. In the following,\nwe brie\ufb02y pay attention to each these.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n412 CHAPTER 7. CONSISTENCY AND REPLICATION\nin terms of bandwidth and perceived client latency, while maintaining cached\ndocuments consistent with those at the origin server.\nFinally, we should also mention that much research has been conducted to\n\ufb01nd out what the best cache replacement strategies are. Numerous proposals\nexist, but by-and-large, simple replacement strategies such as evicting the\nleast recently used object work well enough. An in-depth survey of replace-\nment strategies is presented by Podling and Boszormenyi [2003]; Ali et al.\n[2011] provide a more recent overview which also includes Web prefetching\ntechniques.\nAs the importance of the Web continues to increase as a vehicle for organi-\nzations to present themselves and to directly interact with end users, we see\na shift between maintaining the content of a Web site and making sure that\nthe site is easily and continuously accessible. This distinction has paved the\nway for content delivery networks (CDN ). The main idea underlying these\nCDNs is that they act as a Web hosting service, providing an infrastructure\nfor distributing and replicating the Web documents of multiple sites across\nthe Internet. The size of the infrastructure can be impressive. For example,\nas of 2016, Akamai is reported to have over 200,000 servers spread across 120\ncountries.\nThe sheer size of a CDN requires that hosted documents are automatically\ndistributed and replicated. In most cases, a large-scale CDN is organized\nalong the lines of a feedback-control loop, as shown in Figure 7.33 and which\nis described extensively in [Sivasubramanian et al., 2004b].\nFigure 7.33: The general organization of a CDN as a feedback-control system.\nThere are essentially three different kinds of aspects related to replication\nin Web hosting systems: metric estimation, adaptation triggering, and taking\nappropriate measures. The latter can be subdivided into replica placement de-\ncisions, consistency enforcement, and client-request routing. In the following,\nwe brie\ufb02y pay attention to each these.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 413\nAn interesting aspect of CDNs is that they need to make a trade-off between\nmany aspects when it comes to hosting replicated content. For example, access\ntimes for a document may be optimal if a document is massively replicated,\nbut at the same time this incurs a \ufb01nancial cost, as well as a cost in terms of\nbandwidth usage for disseminating updates. By and large, there are many\nproposals for estimating how well a CDN is performing. These proposals can\nbe grouped into several classes.\nFirst, there are latency metrics , by which the time is measured for an action,\nfor example, fetching a document, to take place. Trivial as this may seem,\nestimating latencies becomes dif\ufb01cult when, for example, a process deciding\non the placement of replicas needs to know the delay between a client and\nsome remote server. Typically, an algorithm globally positioning nodes as\ndiscussed in Chapter 6 will need to be deployed.\nInstead of estimating latency, it may be more important to measure the\navailable bandwidth between two nodes. This information is particularly\nimportant when large documents need to be transferred, as in that case the\nresponsiveness of the system is largely dictated by the time that a document\ncan be transferred. There are various tools for measuring available bandwidth,\nbut in all cases it turns out that accurate measurements can be dif\ufb01cult to\nattain (see also Strauss et al. [2003], Shriram and Kaur [2007], Chaudhari and\nBiradar [2015], and Atxutegi et al. [2016]).\nAnother class consists of spatial metrics which mainly consist of measuring\nthe distance between nodes in terms of the number of network-level routing\nhops, or hops between autonomous systems. Again, determining the number\nof hops between two arbitrary nodes can be very dif\ufb01cult, and may also not\neven correlate with latency [Huffaker et al., 2002]. Moreover, simply looking\nat routing tables is not going to work when low-level techniques such as\nmulti-protocol label switching (MPLS ) are deployed. MPLS circumvents\nnetwork-level routing by using virtual-circuit techniques to immediately and\nef\ufb01ciently forward packets to their destination (see also Guichard et al. [2005]).\nPackets may thus follow completely different routes than advertised in the\ntables of network-level routers.\nA third class is formed by network usage metrics which most often entails\nconsumed bandwidth. Computing consumed bandwidth in terms of the\nnumber of bytes to transfer is generally easy. However, to do this correctly,\nwe need to take into account how often the document is read, how often it is\nupdated, and how often it is replicated.\nConsistency metrics tell us to what extent a replica is deviating from its mas-\nter copy. We already discussed extensively how consistency can be measured\nin the context of continuous consistency [Yu and Vahdat, 2002].\nFinally, \ufb01nancial metrics form another class for measuring how well a CDN\nis doing. Although not technical at all, considering that most CDN operate\non a commercial basis, it is clear that in many cases \ufb01nancial metrics will be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 413\nAn interesting aspect of CDNs is that they need to make a trade-off between\nmany aspects when it comes to hosting replicated content. For example, access\ntimes for a document may be optimal if a document is massively replicated,\nbut at the same time this incurs a \ufb01nancial cost, as well as a cost in terms of\nbandwidth usage for disseminating updates. By and large, there are many\nproposals for estimating how well a CDN is performing. These proposals can\nbe grouped into several classes.\nFirst, there are latency metrics , by which the time is measured for an action,\nfor example, fetching a document, to take place. Trivial as this may seem,\nestimating latencies becomes dif\ufb01cult when, for example, a process deciding\non the placement of replicas needs to know the delay between a client and\nsome remote server. Typically, an algorithm globally positioning nodes as\ndiscussed in Chapter 6 will need to be deployed.\nInstead of estimating latency, it may be more important to measure the\navailable bandwidth between two nodes. This information is particularly\nimportant when large documents need to be transferred, as in that case the\nresponsiveness of the system is largely dictated by the time that a document\ncan be transferred. There are various tools for measuring available bandwidth,\nbut in all cases it turns out that accurate measurements can be dif\ufb01cult to\nattain (see also Strauss et al. [2003], Shriram and Kaur [2007], Chaudhari and\nBiradar [2015], and Atxutegi et al. [2016]).\nAnother class consists of spatial metrics which mainly consist of measuring\nthe distance between nodes in terms of the number of network-level routing\nhops, or hops between autonomous systems. Again, determining the number\nof hops between two arbitrary nodes can be very dif\ufb01cult, and may also not\neven correlate with latency [Huffaker et al., 2002]. Moreover, simply looking\nat routing tables is not going to work when low-level techniques such as\nmulti-protocol label switching (MPLS ) are deployed. MPLS circumvents\nnetwork-level routing by using virtual-circuit techniques to immediately and\nef\ufb01ciently forward packets to their destination (see also Guichard et al. [2005]).\nPackets may thus follow completely different routes than advertised in the\ntables of network-level routers.\nA third class is formed by network usage metrics which most often entails\nconsumed bandwidth. Computing consumed bandwidth in terms of the\nnumber of bytes to transfer is generally easy. However, to do this correctly,\nwe need to take into account how often the document is read, how often it is\nupdated, and how often it is replicated.\nConsistency metrics tell us to what extent a replica is deviating from its mas-\nter copy. We already discussed extensively how consistency can be measured\nin the context of continuous consistency [Yu and Vahdat, 2002].\nFinally, \ufb01nancial metrics form another class for measuring how well a CDN\nis doing. Although not technical at all, considering that most CDN operate\non a commercial basis, it is clear that in many cases \ufb01nancial metrics will be\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "414 CHAPTER 7. CONSISTENCY AND REPLICATION\ndecisive. Moreover, the \ufb01nancial metrics are closely related to the actual infra-\nstructure of the Internet. For example, most commercial CDNs place servers\nat the edge of the Internet, meaning that they hire capacity from ISPs directly\nservicing end users. At this point, business models become intertwined with\ntechnological issues, an area that is not at all well understood. There is only\nfew material available on the relation between \ufb01nancial performance and\ntechnological issues [Janiga et al., 2001].\nFrom these examples it should become clear that simply measuring the\nperformance of a CDN, or even estimating its performance may by itself be\nan extremely complex task. In practice, for commercial CDNs the issue that\nreally counts is whether they can meet the service-level agreements that have\nbeen made with customers. These agreements are often formulated simply in\nterms of how quickly customers are to be serviced. It is then up to the CDN\nto make sure that these agreements are met.\nAnother question that needs to be addressed is when and how adaptations\nare to be triggered. A simple model is to periodically estimate metrics and\nsubsequently take measures as needed. This approach is often seen in practice.\nSpecial processes located at the servers collect information and periodically\ncheck for changes.\nNote 7.12 (Advanced: Flash crowds)\nA major drawback of periodic evaluation is that sudden changes may be missed.\nOne type of sudden change that has received considerable attention is that of\n\ufb02ash crowds. A \ufb02ash crowd is a legitimate sudden burst in requests for a speci\ufb01c\nWeb document. In many cases, these type of bursts can bring down an entire\nservice, in turn causing a cascade of service outages.\nHandling \ufb02ash crowds can be dif\ufb01cult. One solution is to massively replicate a\nWeb site and as soon as request rates start to rapidly increase, requests should be\nredirected to the replicas to of\ufb02oad the master copy. This type of overprovisioning\nis obviously not the way to go. Instead, what is needed is to dynamically create\nreplicas when demand goes up, and start redirecting requests when the going\ngets tough. With cloud computing now properly in place, combined with the fact\nthat cloning virtual machines is relatively simple, reacting to sudden changes in\nrequest demands is practically feasible. Also, as we shall discuss below, using\ntechniques for content delivery networks have proven to work well.\nHowever, even better than reacting would be to predict \ufb02ash crowds, yet this\nis actually very dif\ufb01cult if not practically impossible. Figure 7.34 shows access\ntraces for three different Web sites that suffered from a \ufb02ash crowd.\nAs a point of reference, Figure 7.34(a) shows regular access traces spanning\ntwo days. There are also some very strong peaks, but otherwise there is nothing\nshocking going on. In contrast, Figure 7.34(b) shows a two-day trace with four\nsudden \ufb02ash crowds. There is still some regularity, which may be discovered after\na while so that measures can be taken. However, the damage may be been done\nbefore reaching that point.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n414 CHAPTER 7. CONSISTENCY AND REPLICATION\ndecisive. Moreover, the \ufb01nancial metrics are closely related to the actual infra-\nstructure of the Internet. For example, most commercial CDNs place servers\nat the edge of the Internet, meaning that they hire capacity from ISPs directly\nservicing end users. At this point, business models become intertwined with\ntechnological issues, an area that is not at all well understood. There is only\nfew material available on the relation between \ufb01nancial performance and\ntechnological issues [Janiga et al., 2001].\nFrom these examples it should become clear that simply measuring the\nperformance of a CDN, or even estimating its performance may by itself be\nan extremely complex task. In practice, for commercial CDNs the issue that\nreally counts is whether they can meet the service-level agreements that have\nbeen made with customers. These agreements are often formulated simply in\nterms of how quickly customers are to be serviced. It is then up to the CDN\nto make sure that these agreements are met.\nAnother question that needs to be addressed is when and how adaptations\nare to be triggered. A simple model is to periodically estimate metrics and\nsubsequently take measures as needed. This approach is often seen in practice.\nSpecial processes located at the servers collect information and periodically\ncheck for changes.\nNote 7.12 (Advanced: Flash crowds)\nA major drawback of periodic evaluation is that sudden changes may be missed.\nOne type of sudden change that has received considerable attention is that of\n\ufb02ash crowds. A \ufb02ash crowd is a legitimate sudden burst in requests for a speci\ufb01c\nWeb document. In many cases, these type of bursts can bring down an entire\nservice, in turn causing a cascade of service outages.\nHandling \ufb02ash crowds can be dif\ufb01cult. One solution is to massively replicate a\nWeb site and as soon as request rates start to rapidly increase, requests should be\nredirected to the replicas to of\ufb02oad the master copy. This type of overprovisioning\nis obviously not the way to go. Instead, what is needed is to dynamically create\nreplicas when demand goes up, and start redirecting requests when the going\ngets tough. With cloud computing now properly in place, combined with the fact\nthat cloning virtual machines is relatively simple, reacting to sudden changes in\nrequest demands is practically feasible. Also, as we shall discuss below, using\ntechniques for content delivery networks have proven to work well.\nHowever, even better than reacting would be to predict \ufb02ash crowds, yet this\nis actually very dif\ufb01cult if not practically impossible. Figure 7.34 shows access\ntraces for three different Web sites that suffered from a \ufb02ash crowd.\nAs a point of reference, Figure 7.34(a) shows regular access traces spanning\ntwo days. There are also some very strong peaks, but otherwise there is nothing\nshocking going on. In contrast, Figure 7.34(b) shows a two-day trace with four\nsudden \ufb02ash crowds. There is still some regularity, which may be discovered after\na while so that measures can be taken. However, the damage may be been done\nbefore reaching that point.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 415\nFigure 7.34: One normal and three different access patterns re\ufb02ecting\n\ufb02ash-crowd behavior. Adapted from Baryshnikov et al. [2005].\nFigure 7.34(c) shows a trace spanning six days with at least two \ufb02ash crowds.\nIn this case, any predictor is going to have a serious problem, as it turns out that\nboth increases in request rate are almost instantaneously. Finally, Figure 7.34(d)\nshows a situation in which the \ufb01rst peak should probably cause no adaptations,\nbut the second obviously should. This situation turns out to be the type of\nbehavior that can be dealt with quite well through runtime analysis.\nAs mentioned, there are essentially only three (related) measures that\ncan be taken to change the behavior of a Web hosting service: changing the\nplacement of replicas, changing consistency enforcement, and deciding on\nhow and when to redirect client requests. We already discussed the \ufb01rst two\nmeasures extensively. Client-request redirection deserves some more attention.\nBefore we discuss some of the trade-offs, let us \ufb01rst consider how consistency\nand replication are dealt with in a practical setting by considering the Akamai\nsituation [Dilley et al., 2002; Nygren et al., 2010].\nThe basic idea is that each Web document consists of a main HTML (or\nXML) page in which several other documents such as images, video, and\naudio have been embedded. To display the entire document, it is necessary\nthat the embedded documents are fetched by the user\u2019s browser as well.\nThe assumption is that these embedded documents rarely change, for which\nreason it makes sense to cache or replicate them.\nEach embedded document is normally referenced through a URL. However,\nin Akamai\u2019s CDN, such a URL is modi\ufb01ed such that it refers to a virtual\nghost , which is a reference to an actual server in the CDN. The URL also\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 415\nFigure 7.34: One normal and three different access patterns re\ufb02ecting\n\ufb02ash-crowd behavior. Adapted from Baryshnikov et al. [2005].\nFigure 7.34(c) shows a trace spanning six days with at least two \ufb02ash crowds.\nIn this case, any predictor is going to have a serious problem, as it turns out that\nboth increases in request rate are almost instantaneously. Finally, Figure 7.34(d)\nshows a situation in which the \ufb01rst peak should probably cause no adaptations,\nbut the second obviously should. This situation turns out to be the type of\nbehavior that can be dealt with quite well through runtime analysis.\nAs mentioned, there are essentially only three (related) measures that\ncan be taken to change the behavior of a Web hosting service: changing the\nplacement of replicas, changing consistency enforcement, and deciding on\nhow and when to redirect client requests. We already discussed the \ufb01rst two\nmeasures extensively. Client-request redirection deserves some more attention.\nBefore we discuss some of the trade-offs, let us \ufb01rst consider how consistency\nand replication are dealt with in a practical setting by considering the Akamai\nsituation [Dilley et al., 2002; Nygren et al., 2010].\nThe basic idea is that each Web document consists of a main HTML (or\nXML) page in which several other documents such as images, video, and\naudio have been embedded. To display the entire document, it is necessary\nthat the embedded documents are fetched by the user\u2019s browser as well.\nThe assumption is that these embedded documents rarely change, for which\nreason it makes sense to cache or replicate them.\nEach embedded document is normally referenced through a URL. However,\nin Akamai\u2019s CDN, such a URL is modi\ufb01ed such that it refers to a virtual\nghost , which is a reference to an actual server in the CDN. The URL also\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "416 CHAPTER 7. CONSISTENCY AND REPLICATION\ncontains the host name of the origin server for reasons we explain next. The\nmodi\ufb01ed URL is resolved as follows, as is also shown in Figure 7.35.\nFigure 7.35: The principal working of the Akamai CDN.\nThe name of the virtual ghost includes a DNS name such as ghosting .com,\nwhich is resolved by the regular DNS naming system to a CDN DNS server\n(the result of step 3). Each such DNS server keeps track of servers close\nto the client. To this end, any of the proximity metrics we have discussed\npreviously could be used. In effect, the CDN DNS servers redirect the client\nto a replica server best for that client (step 4), which could mean the closest\none, the least-loaded one, or a combination of several such metrics (the actual\nredirection policy is proprietary).\nFinally, the client forwards the request for the embedded document to\nthe selected CDN server. If this server does not yet have the document, it\nfetches it from the original Web server (shown as step 6), caches it locally,\nand subsequently passes it to the client. If the document was already in the\nCDN server\u2019s cache, it can be returned forthwith. Note that in order to fetch\nthe embedded document, the replica server must be able to send a request\nto the origin server, for which reason its host name is also contained in the\nembedded document\u2019s URL.\nAn interesting aspect of this scheme is the simplicity by which consistency\nof documents can be enforced. Clearly, whenever a main document is changed,\na client will always be able to fetch it from the origin server. In the case of\nembedded documents, a different approach needs to be followed as these\ndocuments are, in principle, fetched from a nearby replica server. To this end,\na URL for an embedded document not only refers to a special host name that\neventually leads to a CDN DNS server, but also contains a unique identi\ufb01er\nthat is changed every time the embedded document changes. In effect, this\nidenti\ufb01er changes the name of the embedded document. As a consequence,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n416 CHAPTER 7. CONSISTENCY AND REPLICATION\ncontains the host name of the origin server for reasons we explain next. The\nmodi\ufb01ed URL is resolved as follows, as is also shown in Figure 7.35.\nFigure 7.35: The principal working of the Akamai CDN.\nThe name of the virtual ghost includes a DNS name such as ghosting .com,\nwhich is resolved by the regular DNS naming system to a CDN DNS server\n(the result of step 3). Each such DNS server keeps track of servers close\nto the client. To this end, any of the proximity metrics we have discussed\npreviously could be used. In effect, the CDN DNS servers redirect the client\nto a replica server best for that client (step 4), which could mean the closest\none, the least-loaded one, or a combination of several such metrics (the actual\nredirection policy is proprietary).\nFinally, the client forwards the request for the embedded document to\nthe selected CDN server. If this server does not yet have the document, it\nfetches it from the original Web server (shown as step 6), caches it locally,\nand subsequently passes it to the client. If the document was already in the\nCDN server\u2019s cache, it can be returned forthwith. Note that in order to fetch\nthe embedded document, the replica server must be able to send a request\nto the origin server, for which reason its host name is also contained in the\nembedded document\u2019s URL.\nAn interesting aspect of this scheme is the simplicity by which consistency\nof documents can be enforced. Clearly, whenever a main document is changed,\na client will always be able to fetch it from the origin server. In the case of\nembedded documents, a different approach needs to be followed as these\ndocuments are, in principle, fetched from a nearby replica server. To this end,\na URL for an embedded document not only refers to a special host name that\neventually leads to a CDN DNS server, but also contains a unique identi\ufb01er\nthat is changed every time the embedded document changes. In effect, this\nidenti\ufb01er changes the name of the embedded document. As a consequence,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 417\nwhen the client is redirected to a speci\ufb01c CDN server, that server will not \ufb01nd\nthe named document in its cache and will thus fetch it from the origin server.\nThe old document will eventually be evicted from the server\u2019s cache as it is\nno longer referenced.\nThis example already shows the importance of client-request redirection.\nIn principle, by properly redirecting clients, a CDN can stay in control when\nit comes to client-perceived performance, but also taking into account global\nsystem performance by, for example, avoiding that requests are sent to heavily\nloaded servers. These so-called adaptive redirection policies can be applied\nwhen information on the system\u2019s current behavior is provided to the pro-\ncesses that take redirection decisions. This brings us partly back to the metric\nestimation techniques discussed previously.\nBesides the different policies, an important issue is whether request redi-\nrection is transparent to the client or not. In essence, there are only three\nredirection techniques: TCP handoff, DNS redirection, and HTTP redirection.\nWe already discussed TCP handoff. This technique is applicable only for\nserver clusters and does not scale to wide-area networks.\nDNS redirection is a transparent mechanism by which the client can be\nkept completely unaware of where documents are located. Akamai\u2019s two-level\nredirection is one example of this technique. We can also directly deploy DNS\nto return one of several addresses as we discussed before. Note, however, that\nDNS redirection can be applied only to an entire site: the name of individual\ndocuments does not \ufb01t into the DNS name space.\nHTTP redirection, \ufb01nally, is a nontransparent mechanism. When a client\nrequests a speci\ufb01c document, it may be given an alternative URL as part\nof an HTTP response message to which it is then redirected. An important\nobservation is that this URL is visible to the client\u2019s browser. In fact, the\nuser may decide to bookmark the referral URL, potentially rendering the\nredirection policy useless.\nUp to this point we have mainly concentrated on caching and replicating\nstatic Web content. In practice, we see that the Web is increasingly offering\nmore dynamically generated content, but that it is also expanding toward\noffering services that can be called by remote applications. Also in these situa-\ntions we see that caching and replication can help considerably in improving\nthe overall performance, although the methods to reach such improvements\nare more subtle than what we discussed so far (see also Conti et al. [2005]).\nWhen considering improving performance of Web applications through\ncaching and replication, matters are complicated by the fact that several so-\nlutions can be deployed, with no single one standing out as the best. Let us\nconsider the edge-server situation as sketched in Figure 7.36 (see also Sivasub-\nramanian et al. [2007]). In this case, we assume a CDN in which each hosted\nsite has an origin server that acts as the authoritative site for all read and\nupdate operations. An edge server is used to handle client requests, and has\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 417\nwhen the client is redirected to a speci\ufb01c CDN server, that server will not \ufb01nd\nthe named document in its cache and will thus fetch it from the origin server.\nThe old document will eventually be evicted from the server\u2019s cache as it is\nno longer referenced.\nThis example already shows the importance of client-request redirection.\nIn principle, by properly redirecting clients, a CDN can stay in control when\nit comes to client-perceived performance, but also taking into account global\nsystem performance by, for example, avoiding that requests are sent to heavily\nloaded servers. These so-called adaptive redirection policies can be applied\nwhen information on the system\u2019s current behavior is provided to the pro-\ncesses that take redirection decisions. This brings us partly back to the metric\nestimation techniques discussed previously.\nBesides the different policies, an important issue is whether request redi-\nrection is transparent to the client or not. In essence, there are only three\nredirection techniques: TCP handoff, DNS redirection, and HTTP redirection.\nWe already discussed TCP handoff. This technique is applicable only for\nserver clusters and does not scale to wide-area networks.\nDNS redirection is a transparent mechanism by which the client can be\nkept completely unaware of where documents are located. Akamai\u2019s two-level\nredirection is one example of this technique. We can also directly deploy DNS\nto return one of several addresses as we discussed before. Note, however, that\nDNS redirection can be applied only to an entire site: the name of individual\ndocuments does not \ufb01t into the DNS name space.\nHTTP redirection, \ufb01nally, is a nontransparent mechanism. When a client\nrequests a speci\ufb01c document, it may be given an alternative URL as part\nof an HTTP response message to which it is then redirected. An important\nobservation is that this URL is visible to the client\u2019s browser. In fact, the\nuser may decide to bookmark the referral URL, potentially rendering the\nredirection policy useless.\nUp to this point we have mainly concentrated on caching and replicating\nstatic Web content. In practice, we see that the Web is increasingly offering\nmore dynamically generated content, but that it is also expanding toward\noffering services that can be called by remote applications. Also in these situa-\ntions we see that caching and replication can help considerably in improving\nthe overall performance, although the methods to reach such improvements\nare more subtle than what we discussed so far (see also Conti et al. [2005]).\nWhen considering improving performance of Web applications through\ncaching and replication, matters are complicated by the fact that several so-\nlutions can be deployed, with no single one standing out as the best. Let us\nconsider the edge-server situation as sketched in Figure 7.36 (see also Sivasub-\nramanian et al. [2007]). In this case, we assume a CDN in which each hosted\nsite has an origin server that acts as the authoritative site for all read and\nupdate operations. An edge server is used to handle client requests, and has\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "418 CHAPTER 7. CONSISTENCY AND REPLICATION\nFigure 7.36: Alternatives for caching and replication with Web applications.\nthe ability to store (partial) information as also kept at an origin server.\nRecall that in an edge-server architecture, Web clients request data through\nan edge server, which, in turn, gets its information from the origin server\nassociated with the speci\ufb01c Web site referred to by the client. As also shown in\nFigure 7.36 we assume that the origin server consists of a database from which\nresponses are dynamically created. Although we have shown only a single\nWeb server, it is common to organize each server according to a multitiered\narchitecture as we discussed before. An edge server can now be roughly\norganized along the following lines.\nFirst, to improve performance, we can decide to apply full replication of\nthe data stored at the origin server. This scheme works well whenever the\nupdate ratio is low and when queries require an extensive database search.\nAs mentioned above, we assume that all updates are carried out at the origin\nserver, which takes responsibility for keeping the replicas and the edge servers\nin a consistent state. Read operations can thus take place at the edge servers.\nHere we see that replicating for performance will fail when the update ratio\nis high, as each update will incur communication over a wide-area network\nto bring the replicas into a consistent state. As shown by Sivasubramanian\net al. [2004a], the read/update ratio is the determining factor to what extent\nthe origin database in a wide-area setting should be replicated.\nAnother case for full replication is when queries are generally complex. In\nthe case of a relational database, this means that a query requires that multiple\ntables need to be searched and processed, as is generally the case with a join\noperation. Opposed to complex queries are simple ones that generally require\naccess to only a single table in order to produce a response. In the latter case,\npartial replication by which only a subset of the data is stored at the edge\nserver may suf\ufb01ce.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n418 CHAPTER 7. CONSISTENCY AND REPLICATION\nFigure 7.36: Alternatives for caching and replication with Web applications.\nthe ability to store (partial) information as also kept at an origin server.\nRecall that in an edge-server architecture, Web clients request data through\nan edge server, which, in turn, gets its information from the origin server\nassociated with the speci\ufb01c Web site referred to by the client. As also shown in\nFigure 7.36 we assume that the origin server consists of a database from which\nresponses are dynamically created. Although we have shown only a single\nWeb server, it is common to organize each server according to a multitiered\narchitecture as we discussed before. An edge server can now be roughly\norganized along the following lines.\nFirst, to improve performance, we can decide to apply full replication of\nthe data stored at the origin server. This scheme works well whenever the\nupdate ratio is low and when queries require an extensive database search.\nAs mentioned above, we assume that all updates are carried out at the origin\nserver, which takes responsibility for keeping the replicas and the edge servers\nin a consistent state. Read operations can thus take place at the edge servers.\nHere we see that replicating for performance will fail when the update ratio\nis high, as each update will incur communication over a wide-area network\nto bring the replicas into a consistent state. As shown by Sivasubramanian\net al. [2004a], the read/update ratio is the determining factor to what extent\nthe origin database in a wide-area setting should be replicated.\nAnother case for full replication is when queries are generally complex. In\nthe case of a relational database, this means that a query requires that multiple\ntables need to be searched and processed, as is generally the case with a join\noperation. Opposed to complex queries are simple ones that generally require\naccess to only a single table in order to produce a response. In the latter case,\npartial replication by which only a subset of the data is stored at the edge\nserver may suf\ufb01ce.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 419\nAn alternative to partial replication is to make use of content-aware caches .\nThe basic idea in this case is that an edge server maintains a local database\nthat is now tailored to the type of queries that can be handled at the origin\nserver. To explain, in a full-\ufb02edged database system a query will operate\non a database in which the data has been organized into tables such that,\nfor example, redundancy is minimized. Such databases are also said to be\nnormalized .\nIn such databases, any query that adheres to the data schema can, in\nprinciple, be processed, although perhaps at considerable costs. With content-\naware caches, an edge server maintains a database that is organized according\nto the structure of queries. What this means is that queries are assumed to\nadhere to a limited number of templates, effectively meaning that the different\nkinds of queries that can be processed is restricted. In these cases, whenever\na query is received, the edge server matches the query against the available\ntemplates, and subsequently looks in its local database to compose a response,\nif possible. If the requested data is not available, the query is forwarded to\nthe origin server after which the response is cached before returning it to the\nclient.\nIn effect, what the edge server is doing is checking whether a query can be\nanswered with the data that is stored locally. This is also referred to as a query\ncontainment check . Note that such data was stored locally as responses to\npreviously issued queries. This approach works best when queries tend to be\nrepeated.\nPart of the complexity of content-aware caching comes from the fact\nthat the data at the edge server needs to be kept consistent. To this end,\nthe origin server needs to know which records are associated with which\ntemplates, so that any update of a record, or any update of a table, can be\nproperly addressed by, for example, sending an invalidation message to the\nappropriate edge servers. Another source of complexity comes from the fact\nthat queries still need to be processed at edge servers. In other words, there is\nnonnegligible computational power needed to handle queries. Considering\nthat databases often form a performance bottleneck in Web servers, alternative\nsolutions may be needed. Finally, caching results from queries that span\nmultiple tables (i.e., when queries are complex) such that a query containment\ncheck can be carried out effectively is not trivial. The reason is that the\norganization of the results may be very different from the organization of the\ntables on which the query operated.\nThese observations lead us to a third solution, namely content-blind\ncaching . The idea of content-blind caching is extremely simple: when a client\nsubmits a query to an edge server, the server \ufb01rst computes a unique hash\nvalue for that query. Using this hash value, it subsequently looks in its cache\nwhether it has processed this query before. If not, the query is forwarded\nto the origin and the result is cached before returning it to the client. If the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.6. EXAMPLE: CACHING AND REPLICATION IN THE WEB 419\nAn alternative to partial replication is to make use of content-aware caches .\nThe basic idea in this case is that an edge server maintains a local database\nthat is now tailored to the type of queries that can be handled at the origin\nserver. To explain, in a full-\ufb02edged database system a query will operate\non a database in which the data has been organized into tables such that,\nfor example, redundancy is minimized. Such databases are also said to be\nnormalized .\nIn such databases, any query that adheres to the data schema can, in\nprinciple, be processed, although perhaps at considerable costs. With content-\naware caches, an edge server maintains a database that is organized according\nto the structure of queries. What this means is that queries are assumed to\nadhere to a limited number of templates, effectively meaning that the different\nkinds of queries that can be processed is restricted. In these cases, whenever\na query is received, the edge server matches the query against the available\ntemplates, and subsequently looks in its local database to compose a response,\nif possible. If the requested data is not available, the query is forwarded to\nthe origin server after which the response is cached before returning it to the\nclient.\nIn effect, what the edge server is doing is checking whether a query can be\nanswered with the data that is stored locally. This is also referred to as a query\ncontainment check . Note that such data was stored locally as responses to\npreviously issued queries. This approach works best when queries tend to be\nrepeated.\nPart of the complexity of content-aware caching comes from the fact\nthat the data at the edge server needs to be kept consistent. To this end,\nthe origin server needs to know which records are associated with which\ntemplates, so that any update of a record, or any update of a table, can be\nproperly addressed by, for example, sending an invalidation message to the\nappropriate edge servers. Another source of complexity comes from the fact\nthat queries still need to be processed at edge servers. In other words, there is\nnonnegligible computational power needed to handle queries. Considering\nthat databases often form a performance bottleneck in Web servers, alternative\nsolutions may be needed. Finally, caching results from queries that span\nmultiple tables (i.e., when queries are complex) such that a query containment\ncheck can be carried out effectively is not trivial. The reason is that the\norganization of the results may be very different from the organization of the\ntables on which the query operated.\nThese observations lead us to a third solution, namely content-blind\ncaching . The idea of content-blind caching is extremely simple: when a client\nsubmits a query to an edge server, the server \ufb01rst computes a unique hash\nvalue for that query. Using this hash value, it subsequently looks in its cache\nwhether it has processed this query before. If not, the query is forwarded\nto the origin and the result is cached before returning it to the client. If the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "420 CHAPTER 7. CONSISTENCY AND REPLICATION\nquery had been processed before, the previously cached result is returned to\nthe client.\nThe main advantage of this scheme is the reduced computational effort\nthat is required from an edge server in comparison to the database approaches\ndescribed above. However, content-blind caching can be wasteful in terms of\nstorage as the caches may contain much more redundant data in comparison\nto content-aware caching or database replication. Note that such redundancy\nalso complicates the process of keeping the cache up to date as the origin\nserver may need to keep an accurate account of which updates can potentially\naffect cached query results. These problems can be alleviated when assuming\nthat queries can match only a limited set of prede\ufb01ned templates as we\ndiscussed above.\n7.7 Summary\nThere are primarily two reasons for replicating data: improving the reliability\nof a distributed system and improving performance. Replication introduces\na consistency problem: whenever a replica is updated, that replica becomes\ndifferent from the others. To keep replicas consistent, we need to propagate\nupdates in such a way that temporary inconsistencies are not noticed. Unfor-\ntunately, doing so may severely degrade performance, especially in large-scale\ndistributed systems.\nThe only solution to this problem is to relax consistency somewhat. Dif-\nferent consistency models exist. For continuous consistency, the goal is to\nset bounds to numerical deviation between replicas, staleness deviation, and\ndeviations in the ordering of operations.\nNumerical deviation refers to the value by which replicas may be different.\nThis type of deviation is highly application dependent, but can, for example,\nbe used in replication of stocks. Staleness deviation refers to the time by\nwhich a replica is still considered to be consistent, despite that updates may\nhave taken place some time ago. Staleness deviation is often used for Web\ncaches. Finally, ordering deviation refers to the maximum number of tentative\nwrites that may be outstanding at any server without having synchronized\nwith the other replica servers.\nConsistent ordering of operations has since long formed the basis for\nmany consistency models. Many variations exist, but only a few seem to\nprevail among application developers. Sequential consistency essentially\nprovides the semantics that programmers expect in concurrent programming:\nall write operations are seen by everyone in the same order. Less used,\nbut still relevant, is causal consistency, which re\ufb02ects that operations that\nare potentially dependent on each other are carried out in the order of that\ndependency.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n420 CHAPTER 7. CONSISTENCY AND REPLICATION\nquery had been processed before, the previously cached result is returned to\nthe client.\nThe main advantage of this scheme is the reduced computational effort\nthat is required from an edge server in comparison to the database approaches\ndescribed above. However, content-blind caching can be wasteful in terms of\nstorage as the caches may contain much more redundant data in comparison\nto content-aware caching or database replication. Note that such redundancy\nalso complicates the process of keeping the cache up to date as the origin\nserver may need to keep an accurate account of which updates can potentially\naffect cached query results. These problems can be alleviated when assuming\nthat queries can match only a limited set of prede\ufb01ned templates as we\ndiscussed above.\n7.7 Summary\nThere are primarily two reasons for replicating data: improving the reliability\nof a distributed system and improving performance. Replication introduces\na consistency problem: whenever a replica is updated, that replica becomes\ndifferent from the others. To keep replicas consistent, we need to propagate\nupdates in such a way that temporary inconsistencies are not noticed. Unfor-\ntunately, doing so may severely degrade performance, especially in large-scale\ndistributed systems.\nThe only solution to this problem is to relax consistency somewhat. Dif-\nferent consistency models exist. For continuous consistency, the goal is to\nset bounds to numerical deviation between replicas, staleness deviation, and\ndeviations in the ordering of operations.\nNumerical deviation refers to the value by which replicas may be different.\nThis type of deviation is highly application dependent, but can, for example,\nbe used in replication of stocks. Staleness deviation refers to the time by\nwhich a replica is still considered to be consistent, despite that updates may\nhave taken place some time ago. Staleness deviation is often used for Web\ncaches. Finally, ordering deviation refers to the maximum number of tentative\nwrites that may be outstanding at any server without having synchronized\nwith the other replica servers.\nConsistent ordering of operations has since long formed the basis for\nmany consistency models. Many variations exist, but only a few seem to\nprevail among application developers. Sequential consistency essentially\nprovides the semantics that programmers expect in concurrent programming:\nall write operations are seen by everyone in the same order. Less used,\nbut still relevant, is causal consistency, which re\ufb02ects that operations that\nare potentially dependent on each other are carried out in the order of that\ndependency.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "7.7. SUMMARY 421\nWeaker consistency models consider series of read and write operations.\nIn particular, they assume that each series is appropriately \u201cbracketed\u201d by ac-\ncompanying operations on synchronization variables, such as locks. Although\nthis requires explicit effort from programmers, these models are generally\neasier to implement in an ef\ufb01cient way than, for example, pure sequential\nconsistency.\nAs opposed to these data-centric models, researchers in the \ufb01eld of dis-\ntributed databases for mobile users have de\ufb01ned a number of client-centric\nconsistency models. Such models do not consider the fact that data may be\nshared by several users, but instead, concentrate on the consistency that an\nindividual client should be offered. The underlying assumption is that a client\nconnects to different replicas in the course of time, but that such differences\nshould be made transparent. In essence, client-centric consistency models\nensure that whenever a client connects to a new replica, that replica is brought\nup to date with the data that had been manipulated by that client before, and\nwhich may possibly reside at other replica sites.\nTo propagate updates, different techniques can be applied. A distinction\nneeds to be made concerning what is exactly propagated, to where updates are\npropagated, and by whom propagation is initiated. We can decide to propagate\nnoti\ufb01cations, operations, or state. Likewise, not every replica always needs to\nbe updated immediately. Which replica is updated at which time depends\non the distribution protocol. Finally, a choice can be made whether updates\nare pushed to other replicas, or that a replica pulls in updates from another\nreplica.\nConsistency protocols describe speci\ufb01c implementations of consistency\nmodels. With respect to sequential consistency and its variants, a distinction\ncan be made between primary-based protocols and replicated-write protocols.\nIn primary-based protocols, all update operations are forwarded to a primary\ncopy that subsequently ensures the update is properly ordered and forwarded.\nIn replicated-write protocols, an update is forwarded to several replicas at the\nsame time. In that case, correctly ordering operations often becomes more\ndif\ufb01cult.\nWe pay separate attention to caching and replication in the Web and,\nrelated, content delivery networks. As it turns out, using existing servers and\nservices, much of the techniques discussed before can be readily implemented\nusing appropriate redirection techniques. Particularly challenging is caching\ncontent when databases are involved, as in those cases, much of what a\nWeb server returns is dynamically generated. However, even in those cases,\nby carefully administrating what has already been cached at the edge, it is\npossible to invent highly ef\ufb01cient and effective caching schemes.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n7.7. SUMMARY 421\nWeaker consistency models consider series of read and write operations.\nIn particular, they assume that each series is appropriately \u201cbracketed\u201d by ac-\ncompanying operations on synchronization variables, such as locks. Although\nthis requires explicit effort from programmers, these models are generally\neasier to implement in an ef\ufb01cient way than, for example, pure sequential\nconsistency.\nAs opposed to these data-centric models, researchers in the \ufb01eld of dis-\ntributed databases for mobile users have de\ufb01ned a number of client-centric\nconsistency models. Such models do not consider the fact that data may be\nshared by several users, but instead, concentrate on the consistency that an\nindividual client should be offered. The underlying assumption is that a client\nconnects to different replicas in the course of time, but that such differences\nshould be made transparent. In essence, client-centric consistency models\nensure that whenever a client connects to a new replica, that replica is brought\nup to date with the data that had been manipulated by that client before, and\nwhich may possibly reside at other replica sites.\nTo propagate updates, different techniques can be applied. A distinction\nneeds to be made concerning what is exactly propagated, to where updates are\npropagated, and by whom propagation is initiated. We can decide to propagate\nnoti\ufb01cations, operations, or state. Likewise, not every replica always needs to\nbe updated immediately. Which replica is updated at which time depends\non the distribution protocol. Finally, a choice can be made whether updates\nare pushed to other replicas, or that a replica pulls in updates from another\nreplica.\nConsistency protocols describe speci\ufb01c implementations of consistency\nmodels. With respect to sequential consistency and its variants, a distinction\ncan be made between primary-based protocols and replicated-write protocols.\nIn primary-based protocols, all update operations are forwarded to a primary\ncopy that subsequently ensures the update is properly ordered and forwarded.\nIn replicated-write protocols, an update is forwarded to several replicas at the\nsame time. In that case, correctly ordering operations often becomes more\ndif\ufb01cult.\nWe pay separate attention to caching and replication in the Web and,\nrelated, content delivery networks. As it turns out, using existing servers and\nservices, much of the techniques discussed before can be readily implemented\nusing appropriate redirection techniques. Particularly challenging is caching\ncontent when databases are involved, as in those cases, much of what a\nWeb server returns is dynamically generated. However, even in those cases,\nby carefully administrating what has already been cached at the edge, it is\npossible to invent highly ef\ufb01cient and effective caching schemes.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "", "Chapter 8\nFault tolerance\nA characteristic feature of distributed systems that distinguishes them from\nsingle-machine systems is the notion of partial failure: part of the system is\nfailing while the remaining part continues to operate, and seemingly correctly.\nAn important goal in distributed-systems design is to construct the system\nin such a way that it can automatically recover from partial failures without\nseriously affecting the overall performance. In particular, whenever a failure\noccurs, the system should continue to operate in an acceptable way while\nrepairs are being made. In other words, a distributed system is expected to be\nfault tolerant.\nIn this chapter, we take a closer look at techniques to achieve fault toler-\nance. After providing some general background, we will \ufb01rst look at process\nresilience through process groups. In this case, multiple identical processes\ncooperate providing the appearance of a single logical process to ensure that\none or more of them can fail without a client noticing. A speci\ufb01cally dif\ufb01cult\npoint in process groups is reaching consensus among the group members\non which a client-requested operation is to perform. By now, Paxos is a\ncommonly adopted, yet relatively intricate algorithm, which we explain by\nbuilding it from the ground up. Likewise, we carefully examine the cases in\nwhich consensus can be reached, and under which circumstances.\nAchieving fault tolerance and reliable communication are strongly related.\nNext to reliable client-server communication we pay attention to reliable\ngroup communication and notably atomic multicasting. In the latter case, a\nmessage is delivered to all nonfaulty processes in a group, or to none at all.\nHaving atomic multicasting makes development of fault-tolerant solutions\nmuch easier.\nAtomicity is a property that is important in many applications. In this\nchapter, we pay attention to what are known as distributed commit protocols\nby which a group of processes are conducted to either jointly commit their\nlocal work, or collectively abort and return to a previous system state.\n423\nChapter 8\nFault tolerance\nA characteristic feature of distributed systems that distinguishes them from\nsingle-machine systems is the notion of partial failure: part of the system is\nfailing while the remaining part continues to operate, and seemingly correctly.\nAn important goal in distributed-systems design is to construct the system\nin such a way that it can automatically recover from partial failures without\nseriously affecting the overall performance. In particular, whenever a failure\noccurs, the system should continue to operate in an acceptable way while\nrepairs are being made. In other words, a distributed system is expected to be\nfault tolerant.\nIn this chapter, we take a closer look at techniques to achieve fault toler-\nance. After providing some general background, we will \ufb01rst look at process\nresilience through process groups. In this case, multiple identical processes\ncooperate providing the appearance of a single logical process to ensure that\none or more of them can fail without a client noticing. A speci\ufb01cally dif\ufb01cult\npoint in process groups is reaching consensus among the group members\non which a client-requested operation is to perform. By now, Paxos is a\ncommonly adopted, yet relatively intricate algorithm, which we explain by\nbuilding it from the ground up. Likewise, we carefully examine the cases in\nwhich consensus can be reached, and under which circumstances.\nAchieving fault tolerance and reliable communication are strongly related.\nNext to reliable client-server communication we pay attention to reliable\ngroup communication and notably atomic multicasting. In the latter case, a\nmessage is delivered to all nonfaulty processes in a group, or to none at all.\nHaving atomic multicasting makes development of fault-tolerant solutions\nmuch easier.\nAtomicity is a property that is important in many applications. In this\nchapter, we pay attention to what are known as distributed commit protocols\nby which a group of processes are conducted to either jointly commit their\nlocal work, or collectively abort and return to a previous system state.\n423", "424 CHAPTER 8. FAULT TOLERANCE\nFinally, we will examine how to recover from a failure. In particular, we\nconsider when and how the state of a distributed system should be saved to\nallow recovery to that state later on.\n8.1 Introduction to fault tolerance\nFault tolerance has been subject to much research in computer science. In\nthis section, we start with presenting the basic concepts related to processing\nfailures, followed by a discussion of failure models. The key technique for\nhandling failures is redundancy, which is also discussed. For more general\ninformation on fault tolerance in distributed systems, see, for example [Jalote,\n1994; Shooman, 2002] or [Koren and Krishna, 2007].\nBasic concepts\nTo understand the role of fault tolerance in distributed systems we \ufb01rst need to\ntake a closer look at what it actually means for a distributed system to tolerate\nfaults. Being fault tolerant is strongly related to what are called dependable\nsystems . Dependability is a term that covers a number of useful requirements\nfor distributed systems including the following [Kopetz and Verissimo, 1993]:\n\u2022 Availability\n\u2022 Reliability\n\u2022 Safety\n\u2022 Maintainability\nAvailability is de\ufb01ned as the property that a system is ready to be used\nimmediately. In general, it refers to the probability that the system is operating\ncorrectly at any given moment and is available to perform its functions on\nbehalf of its users. In other words, a highly available system is one that will\nmost likely be working at a given instant in time.\nReliability refers to the property that a system can run continuously\nwithout failure. In contrast to availability, reliability is de\ufb01ned in terms of a\ntime interval instead of an instant in time. A highly reliable system is one\nthat will most likely continue to work without interruption during a relatively\nlong period of time. This is a subtle but important difference when compared\nto availability. If a system goes down on average for one, seemingly random\nmillisecond every hour, it has an availability of more than 99.9999 percent,\nbut is still unreliable. Similarly, a system that never crashes but is shut down\nfor two speci\ufb01c weeks every August has high reliability but only 96 percent\navailability. The two are not the same.\nSafety refers to the situation that when a system temporarily fails to\noperate correctly, no catastrophic event happens. For example, many process-\ncontrol systems, such as those used for controlling nuclear power plants or\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n424 CHAPTER 8. FAULT TOLERANCE\nFinally, we will examine how to recover from a failure. In particular, we\nconsider when and how the state of a distributed system should be saved to\nallow recovery to that state later on.\n8.1 Introduction to fault tolerance\nFault tolerance has been subject to much research in computer science. In\nthis section, we start with presenting the basic concepts related to processing\nfailures, followed by a discussion of failure models. The key technique for\nhandling failures is redundancy, which is also discussed. For more general\ninformation on fault tolerance in distributed systems, see, for example [Jalote,\n1994; Shooman, 2002] or [Koren and Krishna, 2007].\nBasic concepts\nTo understand the role of fault tolerance in distributed systems we \ufb01rst need to\ntake a closer look at what it actually means for a distributed system to tolerate\nfaults. Being fault tolerant is strongly related to what are called dependable\nsystems . Dependability is a term that covers a number of useful requirements\nfor distributed systems including the following [Kopetz and Verissimo, 1993]:\n\u2022 Availability\n\u2022 Reliability\n\u2022 Safety\n\u2022 Maintainability\nAvailability is de\ufb01ned as the property that a system is ready to be used\nimmediately. In general, it refers to the probability that the system is operating\ncorrectly at any given moment and is available to perform its functions on\nbehalf of its users. In other words, a highly available system is one that will\nmost likely be working at a given instant in time.\nReliability refers to the property that a system can run continuously\nwithout failure. In contrast to availability, reliability is de\ufb01ned in terms of a\ntime interval instead of an instant in time. A highly reliable system is one\nthat will most likely continue to work without interruption during a relatively\nlong period of time. This is a subtle but important difference when compared\nto availability. If a system goes down on average for one, seemingly random\nmillisecond every hour, it has an availability of more than 99.9999 percent,\nbut is still unreliable. Similarly, a system that never crashes but is shut down\nfor two speci\ufb01c weeks every August has high reliability but only 96 percent\navailability. The two are not the same.\nSafety refers to the situation that when a system temporarily fails to\noperate correctly, no catastrophic event happens. For example, many process-\ncontrol systems, such as those used for controlling nuclear power plants or\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.1. INTRODUCTION TO FAULT TOLERANCE 425\nsending people into space, are required to provide a high degree of safety. If\nsuch control systems temporarily fail for only a very brief moment, the effects\ncould be disastrous. Many examples from the past (and probably many more\nyet to come) show how hard it is to build safe systems.\nFinally, maintainability refers to how easily a failed system can be re-\npaired. A highly maintainable system may also show a high degree of\navailability, especially if failures can be detected and repaired automatically.\nHowever, as we shall see later in this chapter, automatically recovering from\nfailures is easier said than done.\nNote 8.1 (More information: Traditional metrics)\nWe can be a bit more precise when it comes to describing availability and reliability.\nFormally, the availability A(t)of a component in the time interval [0,t)is de\ufb01ned\nas the average fraction of time that the component has been functioning correctly\nduring that interval. The long-term availability Aof a component is de\ufb01ned as\nA(\u00a5).\nLikewise, the reliability R(t)of a component in the time interval [0,t)is\nformally de\ufb01ned as the conditional probability that it has been functioning cor-\nrectly during that interval given that it was functioning correctly at time T=0.\nFollowing Pradhan [1996], to establish R(t)we consider a system of Nidentical\ncomponents. Let N0(t)denote the number of correctly operating components at\ntime tand N1(t)the number of failed components. Then, clearly,\nR(t) =N0(t)\nN=1\u0000N1(t)\nN=N0(t)\nN0(t) +N1(t)\nThe rate at which components are failing can be expressed as the derivative\ndN1(t)/dt. Dividing this by the number of correctly operating components at\ntime tgives us the failure rate function z(t):\nz(t) =1\nN0(t)dN1(t)\ndt\nFrom\ndR(t)\ndt=\u00001\nNdN1(t)\ndt\nit follows that\nz(t) =1\nN0(t)dN1(t)\ndt=\u0000N\nN0(t)dR(t)\ndt=\u00001\nR(t)dR(t)\ndt\nIf we make the simplifying assumption that a component does not age (and thus\nessentially has no wear-out phase), its failure rate will be constant, i.e., z(t) =z,\nimplying that\ndR(t)\ndt=\u0000zR(t)\nBecause R(0) =1, we obtain\nR(t) =e\u0000zt\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.1. INTRODUCTION TO FAULT TOLERANCE 425\nsending people into space, are required to provide a high degree of safety. If\nsuch control systems temporarily fail for only a very brief moment, the effects\ncould be disastrous. Many examples from the past (and probably many more\nyet to come) show how hard it is to build safe systems.\nFinally, maintainability refers to how easily a failed system can be re-\npaired. A highly maintainable system may also show a high degree of\navailability, especially if failures can be detected and repaired automatically.\nHowever, as we shall see later in this chapter, automatically recovering from\nfailures is easier said than done.\nNote 8.1 (More information: Traditional metrics)\nWe can be a bit more precise when it comes to describing availability and reliability.\nFormally, the availability A(t)of a component in the time interval [0,t)is de\ufb01ned\nas the average fraction of time that the component has been functioning correctly\nduring that interval. The long-term availability Aof a component is de\ufb01ned as\nA(\u00a5).\nLikewise, the reliability R(t)of a component in the time interval [0,t)is\nformally de\ufb01ned as the conditional probability that it has been functioning cor-\nrectly during that interval given that it was functioning correctly at time T=0.\nFollowing Pradhan [1996], to establish R(t)we consider a system of Nidentical\ncomponents. Let N0(t)denote the number of correctly operating components at\ntime tand N1(t)the number of failed components. Then, clearly,\nR(t) =N0(t)\nN=1\u0000N1(t)\nN=N0(t)\nN0(t) +N1(t)\nThe rate at which components are failing can be expressed as the derivative\ndN1(t)/dt. Dividing this by the number of correctly operating components at\ntime tgives us the failure rate function z(t):\nz(t) =1\nN0(t)dN1(t)\ndt\nFrom\ndR(t)\ndt=\u00001\nNdN1(t)\ndt\nit follows that\nz(t) =1\nN0(t)dN1(t)\ndt=\u0000N\nN0(t)dR(t)\ndt=\u00001\nR(t)dR(t)\ndt\nIf we make the simplifying assumption that a component does not age (and thus\nessentially has no wear-out phase), its failure rate will be constant, i.e., z(t) =z,\nimplying that\ndR(t)\ndt=\u0000zR(t)\nBecause R(0) =1, we obtain\nR(t) =e\u0000zt\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "426 CHAPTER 8. FAULT TOLERANCE\nIn other words, if we ignore aging of a component, we see that a constant failure\nrate leads to a reliability following an exponential distribution, having the form\nshown in Figure 8.1.\nFigure 8.1: The reliability of a component having a constant failure rate.\nTraditionally, fault-tolerance has been related to the following three metrics:\n\u2022Mean Time To Failure (MTTF ): The average time until a component fails.\n\u2022Mean Time To Repair (MTTR ): The average time needed to repair a com-\nponent.\n\u2022Mean Time Between Failures (MTBF ): Simply MTTF +MTTR .\nNote that\nA=MTTF\nMTBF=MTTF\nMTTF +MTTR\nAlso, these metrics make sense only if we have an accurate notion of what a failure\nactually is. As we will encounter later, identifying the occurrence of a failure may\nactually not be so obvious.\nOften, dependable systems are also required to provide a high degree of\nsecurity, especially when it comes to issues such as integrity. We will discuss\nsecurity in the next chapter.\nA system is said to failwhen it cannot meet its promises. In particular,\nif a distributed system is designed to provide its users with a number of\nservices, the system has failed when one or more of those services cannot be\n(completely) provided. An error is a part of a system\u2019s state that may lead\nto a failure. For example, when transmitting packets across a network, it is\nto be expected that some packets have been damaged when they arrive at\nthe receiver. Damaged in this context means that the receiver may incorrectly\nsense a bit value (e.g., reading a 1 instead of a 0), or may even be unable to\ndetect that something has arrived.\nThe cause of an error is called a fault . Clearly, \ufb01nding out what caused an\nerror is important. For example, a wrong or bad transmission medium may\neasily cause packets to be damaged. In this case, it is relatively easy to remove\nthe fault. However, transmission errors may also be caused by bad weather\nconditions such as in wireless networks. Changing the weather to reduce or\nprevent errors is a bit trickier.\nAs another example, a crashed program is clearly a failure, which may\nhave happened because the program entered a branch of code containing\na programming bug (i.e., a programming error). The cause of that bug is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n426 CHAPTER 8. FAULT TOLERANCE\nIn other words, if we ignore aging of a component, we see that a constant failure\nrate leads to a reliability following an exponential distribution, having the form\nshown in Figure 8.1.\nFigure 8.1: The reliability of a component having a constant failure rate.\nTraditionally, fault-tolerance has been related to the following three metrics:\n\u2022Mean Time To Failure (MTTF ): The average time until a component fails.\n\u2022Mean Time To Repair (MTTR ): The average time needed to repair a com-\nponent.\n\u2022Mean Time Between Failures (MTBF ): Simply MTTF +MTTR .\nNote that\nA=MTTF\nMTBF=MTTF\nMTTF +MTTR\nAlso, these metrics make sense only if we have an accurate notion of what a failure\nactually is. As we will encounter later, identifying the occurrence of a failure may\nactually not be so obvious.\nOften, dependable systems are also required to provide a high degree of\nsecurity, especially when it comes to issues such as integrity. We will discuss\nsecurity in the next chapter.\nA system is said to failwhen it cannot meet its promises. In particular,\nif a distributed system is designed to provide its users with a number of\nservices, the system has failed when one or more of those services cannot be\n(completely) provided. An error is a part of a system\u2019s state that may lead\nto a failure. For example, when transmitting packets across a network, it is\nto be expected that some packets have been damaged when they arrive at\nthe receiver. Damaged in this context means that the receiver may incorrectly\nsense a bit value (e.g., reading a 1 instead of a 0), or may even be unable to\ndetect that something has arrived.\nThe cause of an error is called a fault . Clearly, \ufb01nding out what caused an\nerror is important. For example, a wrong or bad transmission medium may\neasily cause packets to be damaged. In this case, it is relatively easy to remove\nthe fault. However, transmission errors may also be caused by bad weather\nconditions such as in wireless networks. Changing the weather to reduce or\nprevent errors is a bit trickier.\nAs another example, a crashed program is clearly a failure, which may\nhave happened because the program entered a branch of code containing\na programming bug (i.e., a programming error). The cause of that bug is\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.1. INTRODUCTION TO FAULT TOLERANCE 427\ntypically a programmer. In other words, the programmer is the fault of the\nerror (programming bug), in turn leading to a failure (a crashed program).\nBuilding dependable systems closely relates to controlling faults. As\nexplained by Avizienis et al. [2004], a distinction can be made between pre-\nventing, tolerating, removing, and forecasting faults. For our purposes, the\nmost important issue is fault tolerance , meaning that a system can provide\nits services even in the presence of faults. For example, by applying error-\ncorrecting codes for transmitting packets, it is possible to tolerate, to a certain\nextent, relatively poor transmission lines and reducing the probability that an\nerror (a damaged packet) may lead to a failure.\nFaults are generally classi\ufb01ed as transient, intermittent, or permanent.\nTransient faults occur once and then disappear. If the operation is repeated,\nthe fault goes away. A bird \ufb02ying through the beam of a microwave transmitter\nmay cause lost bits on some network (not to mention a roasted bird). If the\ntransmission times out and is retried, it will probably work the second time.\nAnintermittent fault occurs, then vanishes of its own accord, then reap-\npears, and so on. A loose contact on a connector will often cause an inter-\nmittent fault. Intermittent faults cause a great deal of aggravation because\nthey are dif\ufb01cult to diagnose. Typically, when the fault doctor shows up, the\nsystem works \ufb01ne.\nApermanent fault is one that continues to exist until the faulty compo-\nnent is replaced. Burnt-out chips, software bugs, and disk-head crashes are\nexamples of permanent faults.\nFailure models\nA system that fails is not adequately providing the services it was designed for.\nIf we consider a distributed system as a collection of servers that communicate\nwith one another and with their clients, not adequately providing services\nmeans that servers, communication channels, or possibly both, are not doing\nwhat they are supposed to do. However, a malfunctioning server itself may\nnot always be the fault we are looking for. If such a server depends on other\nservers to adequately provide its services, the cause of an error may need to\nbe searched for somewhere else.\nSuch dependency relations appear in abundance in distributed systems. A\nfailing disk may make life dif\ufb01cult for a \ufb01le server that is designed to provide\na highly available \ufb01le system. If such a \ufb01le server is part of a distributed\ndatabase, the proper working of the entire database may be at stake, as only\npart of its data may be accessible.\nTo get a better grasp on how serious a failure actually is, several classi\ufb01ca-\ntion schemes have been developed. One such scheme is shown in Figure 8.2,\nand is based on schemes described by Cristian [1991] and Hadzilacos and\nToueg [1993].\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.1. INTRODUCTION TO FAULT TOLERANCE 427\ntypically a programmer. In other words, the programmer is the fault of the\nerror (programming bug), in turn leading to a failure (a crashed program).\nBuilding dependable systems closely relates to controlling faults. As\nexplained by Avizienis et al. [2004], a distinction can be made between pre-\nventing, tolerating, removing, and forecasting faults. For our purposes, the\nmost important issue is fault tolerance , meaning that a system can provide\nits services even in the presence of faults. For example, by applying error-\ncorrecting codes for transmitting packets, it is possible to tolerate, to a certain\nextent, relatively poor transmission lines and reducing the probability that an\nerror (a damaged packet) may lead to a failure.\nFaults are generally classi\ufb01ed as transient, intermittent, or permanent.\nTransient faults occur once and then disappear. If the operation is repeated,\nthe fault goes away. A bird \ufb02ying through the beam of a microwave transmitter\nmay cause lost bits on some network (not to mention a roasted bird). If the\ntransmission times out and is retried, it will probably work the second time.\nAnintermittent fault occurs, then vanishes of its own accord, then reap-\npears, and so on. A loose contact on a connector will often cause an inter-\nmittent fault. Intermittent faults cause a great deal of aggravation because\nthey are dif\ufb01cult to diagnose. Typically, when the fault doctor shows up, the\nsystem works \ufb01ne.\nApermanent fault is one that continues to exist until the faulty compo-\nnent is replaced. Burnt-out chips, software bugs, and disk-head crashes are\nexamples of permanent faults.\nFailure models\nA system that fails is not adequately providing the services it was designed for.\nIf we consider a distributed system as a collection of servers that communicate\nwith one another and with their clients, not adequately providing services\nmeans that servers, communication channels, or possibly both, are not doing\nwhat they are supposed to do. However, a malfunctioning server itself may\nnot always be the fault we are looking for. If such a server depends on other\nservers to adequately provide its services, the cause of an error may need to\nbe searched for somewhere else.\nSuch dependency relations appear in abundance in distributed systems. A\nfailing disk may make life dif\ufb01cult for a \ufb01le server that is designed to provide\na highly available \ufb01le system. If such a \ufb01le server is part of a distributed\ndatabase, the proper working of the entire database may be at stake, as only\npart of its data may be accessible.\nTo get a better grasp on how serious a failure actually is, several classi\ufb01ca-\ntion schemes have been developed. One such scheme is shown in Figure 8.2,\nand is based on schemes described by Cristian [1991] and Hadzilacos and\nToueg [1993].\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "428 CHAPTER 8. FAULT TOLERANCE\nType of failure Description of server\u2019s behavior\nCrash failure Halts, but is working correctly until it halts\nOmission failure Fails to respond to incoming requests\nReceive omission Fails to receive incoming messages\nSend omission Fails to send messages\nTiming failure Response lies outside a speci\ufb01ed time interval\nResponse failure Response is incorrect\nValue failure The value of the response is wrong\nState-transition failure Deviates from the correct \ufb02ow of control\nArbitrary failure May produce arbitrary responses at arbitrary times\nFigure 8.2: Different types of failures.\nAcrash failure occurs when a server prematurely halts, but was working\ncorrectly until it stopped. An important aspect of crash failures is that once\nthe server has halted, nothing is heard from it anymore. A typical example of\na crash failure is an operating system that comes to a grinding halt, and for\nwhich there is only one solution: reboot it. Many personal computer systems\nsuffer from crash failures so often that people have come to expect them to be\nnormal. Consequently, moving the reset button from the back of a cabinet to\nthe front was done for good reason. Perhaps one day it can be moved to the\nback again, or even removed altogether.\nAnomission failure occurs when a server fails to respond to a request.\nSeveral things might go wrong. In the case of a receive-omission failure ,\npossibly the server never got the request in the \ufb01rst place. Note that it may\nwell be the case that the connection between a client and a server has been\ncorrectly established, but that there was no thread listening to incoming\nrequests. Also, a receive-omission failure will generally not affect the current\nstate of the server, as the server is unaware of any message sent to it.\nLikewise, a send-omission failure happens when the server has done its\nwork, but somehow fails in sending a response. Such a failure may happen,\nfor example, when a send buffer over\ufb02ows while the server was not prepared\nfor such a situation. Note that, in contrast to a receive-omission failure, the\nserver may now be in a state re\ufb02ecting that it has just completed a service for\nthe client. As a consequence, if the sending of its response fails, the server has\nto be prepared for the client to reissue its previous request.\nOther types of omission failures not related to communication may be\ncaused by software errors such as in\ufb01nite loops or improper memory manage-\nment by which the server is said to \u201chang.\u201d\nAnother class of failures is related to timing. Timing failures occur when\nthe response lies outside a speci\ufb01ed real-time interval. For example, in the\ncase of streaming video\u2019s, providing data too soon may easily cause trouble\nfor a recipient if there is not enough buffer space to hold all the incoming\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n428 CHAPTER 8. FAULT TOLERANCE\nType of failure Description of server\u2019s behavior\nCrash failure Halts, but is working correctly until it halts\nOmission failure Fails to respond to incoming requests\nReceive omission Fails to receive incoming messages\nSend omission Fails to send messages\nTiming failure Response lies outside a speci\ufb01ed time interval\nResponse failure Response is incorrect\nValue failure The value of the response is wrong\nState-transition failure Deviates from the correct \ufb02ow of control\nArbitrary failure May produce arbitrary responses at arbitrary times\nFigure 8.2: Different types of failures.\nAcrash failure occurs when a server prematurely halts, but was working\ncorrectly until it stopped. An important aspect of crash failures is that once\nthe server has halted, nothing is heard from it anymore. A typical example of\na crash failure is an operating system that comes to a grinding halt, and for\nwhich there is only one solution: reboot it. Many personal computer systems\nsuffer from crash failures so often that people have come to expect them to be\nnormal. Consequently, moving the reset button from the back of a cabinet to\nthe front was done for good reason. Perhaps one day it can be moved to the\nback again, or even removed altogether.\nAnomission failure occurs when a server fails to respond to a request.\nSeveral things might go wrong. In the case of a receive-omission failure ,\npossibly the server never got the request in the \ufb01rst place. Note that it may\nwell be the case that the connection between a client and a server has been\ncorrectly established, but that there was no thread listening to incoming\nrequests. Also, a receive-omission failure will generally not affect the current\nstate of the server, as the server is unaware of any message sent to it.\nLikewise, a send-omission failure happens when the server has done its\nwork, but somehow fails in sending a response. Such a failure may happen,\nfor example, when a send buffer over\ufb02ows while the server was not prepared\nfor such a situation. Note that, in contrast to a receive-omission failure, the\nserver may now be in a state re\ufb02ecting that it has just completed a service for\nthe client. As a consequence, if the sending of its response fails, the server has\nto be prepared for the client to reissue its previous request.\nOther types of omission failures not related to communication may be\ncaused by software errors such as in\ufb01nite loops or improper memory manage-\nment by which the server is said to \u201chang.\u201d\nAnother class of failures is related to timing. Timing failures occur when\nthe response lies outside a speci\ufb01ed real-time interval. For example, in the\ncase of streaming video\u2019s, providing data too soon may easily cause trouble\nfor a recipient if there is not enough buffer space to hold all the incoming\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.1. INTRODUCTION TO FAULT TOLERANCE 429\ndata. More common, however, is that a server responds too late, in which case\naperformance failure is said to occur.\nA serious type of failure is a response failure , by which the server\u2019s\nresponse is simply incorrect. Two kinds of response failures may happen.\nIn the case of a value failure, a server simply provides the wrong reply to a\nrequest. For example, a search engine that systematically returns Web pages\nnot related to any of the search terms used, has failed.\nThe other type of response failure is known as a state-transition failure .\nThis kind of failure happens when the server reacts unexpectedly to an\nincoming request. For example, if a server receives a message it cannot\nrecognize, a state-transition failure happens if no measures have been taken\nto handle such messages. In particular, a faulty server may incorrectly take\ndefault actions it should never have initiated.\nThe most serious are arbitrary failures , also known as Byzantine failures .\nIn effect, when arbitrary failures occur, clients should be prepared for the\nworst. In particular, it may happen that a server is producing output it\nshould never have produced, but which cannot be detected as being incorrect.\nByzantine failures were \ufb01rst analyzed by Pease et al. [1980] and Lamport et al.\n[1982]. We return to such failures below.\nNote 8.2 (More information: Omission and commission failures)\nIt has become somewhat of a habit to associate the occurrence of Byzantine\nfailures with maliciously operating processes. The term \u201cByzantine\u201d refers to the\nByzantine Empire, a time (330\u20131453) and place (the Balkans and modern Turkey)\nin which endless conspiracies, intrigue, and untruthfulness were alleged to be\ncommon in ruling circles.\nHowever, it may not be possible to detect whether an act was actually benign or\nmalicious. Is a networked computer running a poorly engineered operating system\nthat adversely affects the performance of other computers acting maliciously? In\nthis sense, it is better to make the following distinction, which effectively excludes\njudgement:\n\u2022Anomission failure occurs when a component fails to take an action that\nit should have taken.\n\u2022Acommission failure occurs when a component takes an action that it\nshould not have taken.\nThis difference, introduced by Mohan et al. [1983], also illustrates that a separation\nbetween dependability and security may at times be pretty dif\ufb01cult to make.\nMany of the aforementioned cases deal with the situation that a process\nPno longer perceives any actions from another process Q. However, can P\nconclude that Qhas indeed come to a halt? To answer this question, we need\nto make a distinction between two types of distributed systems:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.1. INTRODUCTION TO FAULT TOLERANCE 429\ndata. More common, however, is that a server responds too late, in which case\naperformance failure is said to occur.\nA serious type of failure is a response failure , by which the server\u2019s\nresponse is simply incorrect. Two kinds of response failures may happen.\nIn the case of a value failure, a server simply provides the wrong reply to a\nrequest. For example, a search engine that systematically returns Web pages\nnot related to any of the search terms used, has failed.\nThe other type of response failure is known as a state-transition failure .\nThis kind of failure happens when the server reacts unexpectedly to an\nincoming request. For example, if a server receives a message it cannot\nrecognize, a state-transition failure happens if no measures have been taken\nto handle such messages. In particular, a faulty server may incorrectly take\ndefault actions it should never have initiated.\nThe most serious are arbitrary failures , also known as Byzantine failures .\nIn effect, when arbitrary failures occur, clients should be prepared for the\nworst. In particular, it may happen that a server is producing output it\nshould never have produced, but which cannot be detected as being incorrect.\nByzantine failures were \ufb01rst analyzed by Pease et al. [1980] and Lamport et al.\n[1982]. We return to such failures below.\nNote 8.2 (More information: Omission and commission failures)\nIt has become somewhat of a habit to associate the occurrence of Byzantine\nfailures with maliciously operating processes. The term \u201cByzantine\u201d refers to the\nByzantine Empire, a time (330\u20131453) and place (the Balkans and modern Turkey)\nin which endless conspiracies, intrigue, and untruthfulness were alleged to be\ncommon in ruling circles.\nHowever, it may not be possible to detect whether an act was actually benign or\nmalicious. Is a networked computer running a poorly engineered operating system\nthat adversely affects the performance of other computers acting maliciously? In\nthis sense, it is better to make the following distinction, which effectively excludes\njudgement:\n\u2022Anomission failure occurs when a component fails to take an action that\nit should have taken.\n\u2022Acommission failure occurs when a component takes an action that it\nshould not have taken.\nThis difference, introduced by Mohan et al. [1983], also illustrates that a separation\nbetween dependability and security may at times be pretty dif\ufb01cult to make.\nMany of the aforementioned cases deal with the situation that a process\nPno longer perceives any actions from another process Q. However, can P\nconclude that Qhas indeed come to a halt? To answer this question, we need\nto make a distinction between two types of distributed systems:\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "430 CHAPTER 8. FAULT TOLERANCE\n\u2022In an asynchronous system , no assumptions about process execution\nspeeds or message delivery times are made. The consequence is that\nwhen process Pno longer perceives any actions from Q, it cannot\nconclude that Qcrashed. Instead, it may just be slow or its messages\nmay have been lost.\n\u2022In asynchronous system , process execution speeds and message-delivery\ntimes are bounded. This also means that when Qshows no more activity\nwhen it is expected to do so, process Pcan rightfully conclude that Q\nhas crashed.\nUnfortunately, pure synchronous systems exist only in theory. On the other\nhand, simply stating that every distributed system is asynchronous also does\nnot do just to what we see in practice and we would be overly pessimistic in\ndesigning distributed systems under the assumption that they are necessarily\nasynchronous. Instead, it is more realistic to assume that a distributed system\nispartially synchronous : most of the time it behaves as a synchronous system,\nyet there is no bound on the time that it behaves in an asynchronous fashion.\nIn other words, asynchronous behavior is an exception, meaning that we can\nnormally use timeouts to conclude that a process has indeed crashed, but that\noccasionally such a conclusion is false. In practice, this means that we will\nhave to design fault-tolerant solutions that can withstand incorrectly detecting\nthat a process halted.\nIn this context, halting failures can be classi\ufb01ed as follows, from the least\nto the most severe (see also Cachin et al. [2011]). We let process Pattempt to\ndetect that process Qhas failed.\n\u2022Fail-stop failures refer to crash failures that can be reliably detected.\nThis may occur when assuming nonfaulty communication links and\nwhen the failure-detecting process Pcan place a worst-case delay on\nresponses from Q.\n\u2022Fail-noisy failures are like fail-stop failures, except that Pwill only\neventually come to the correct conclusion that Qhas crashed. This means\nthat there may be some a priori unknown time in which P\u2019s detections\nof the behavior of Qare unreliable.\n\u2022When dealing with fail-silent failures , we assume that communication\nlinks are nonfaulty, but that process Pcannot distinguish crash failures\nfrom omission failures.\n\u2022Fail-safe failures cover the case of dealing with arbitrary failures by\nprocess Q, yet these failures are benign: they cannot do any harm.\n\u2022Finally, when dealing with fail-arbitrary failures ,Qmay fail in any\npossible way; failures may be unobservable in addition to being harmful\nto the otherwise correct behavior of other processes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n430 CHAPTER 8. FAULT TOLERANCE\n\u2022In an asynchronous system , no assumptions about process execution\nspeeds or message delivery times are made. The consequence is that\nwhen process Pno longer perceives any actions from Q, it cannot\nconclude that Qcrashed. Instead, it may just be slow or its messages\nmay have been lost.\n\u2022In asynchronous system , process execution speeds and message-delivery\ntimes are bounded. This also means that when Qshows no more activity\nwhen it is expected to do so, process Pcan rightfully conclude that Q\nhas crashed.\nUnfortunately, pure synchronous systems exist only in theory. On the other\nhand, simply stating that every distributed system is asynchronous also does\nnot do just to what we see in practice and we would be overly pessimistic in\ndesigning distributed systems under the assumption that they are necessarily\nasynchronous. Instead, it is more realistic to assume that a distributed system\nispartially synchronous : most of the time it behaves as a synchronous system,\nyet there is no bound on the time that it behaves in an asynchronous fashion.\nIn other words, asynchronous behavior is an exception, meaning that we can\nnormally use timeouts to conclude that a process has indeed crashed, but that\noccasionally such a conclusion is false. In practice, this means that we will\nhave to design fault-tolerant solutions that can withstand incorrectly detecting\nthat a process halted.\nIn this context, halting failures can be classi\ufb01ed as follows, from the least\nto the most severe (see also Cachin et al. [2011]). We let process Pattempt to\ndetect that process Qhas failed.\n\u2022Fail-stop failures refer to crash failures that can be reliably detected.\nThis may occur when assuming nonfaulty communication links and\nwhen the failure-detecting process Pcan place a worst-case delay on\nresponses from Q.\n\u2022Fail-noisy failures are like fail-stop failures, except that Pwill only\neventually come to the correct conclusion that Qhas crashed. This means\nthat there may be some a priori unknown time in which P\u2019s detections\nof the behavior of Qare unreliable.\n\u2022When dealing with fail-silent failures , we assume that communication\nlinks are nonfaulty, but that process Pcannot distinguish crash failures\nfrom omission failures.\n\u2022Fail-safe failures cover the case of dealing with arbitrary failures by\nprocess Q, yet these failures are benign: they cannot do any harm.\n\u2022Finally, when dealing with fail-arbitrary failures ,Qmay fail in any\npossible way; failures may be unobservable in addition to being harmful\nto the otherwise correct behavior of other processes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.1. INTRODUCTION TO FAULT TOLERANCE 431\nClearly, having to deal with fail-arbitrary failures is the worst that can happen.\nAs we shall discuss shortly, we can design distributed systems in such a way\nthat they can even tolerate these types of failures.\nFailure masking by redundancy\nIf a system is to be fault tolerant, the best it can do is to try to hide the\noccurrence of failures from other processes. The key technique for masking\nfaults is to use redundancy. Three kinds are possible: information redundancy,\ntime redundancy, and physical redundancy (see also Johnson [1995]). With\ninformation redundancy , extra bits are added to allow recovery from garbled\nbits. For example, a Hamming code can be added to transmitted data to\nrecover from noise on the transmission line.\nWith time redundancy , an action is performed, and then, if need be, it is\nperformed again. Transactions use this approach. If a transaction aborts, it\ncan be redone with no harm. Another well-known example is retransmitting\na request to a server when lacking an expected response. Time redundancy is\nespecially helpful when the faults are transient or intermittent.\nWith physical redundancy , extra equipment or processes are added to\nmake it possible for the system as a whole to tolerate the loss or malfunction-\ning of some components. Physical redundancy can thus be done either in\nhardware or in software. For example, extra processes can be added to the\nsystem so that if a small number of them crash, the system can still function\ncorrectly. In other words, by replicating processes, a high degree of fault\ntolerance may be achieved. We return to this type of software redundancy\nlater in this chapter.\nNote 8.3 (More information: Triple modular redundancy)\nIt is illustrative to see how redundancy has been applied in the design of electronic\ndevices. Consider, for example, the circuit of Figure 8.3(a). Here signals pass\nthrough devices A,B, and C, in sequence. If one of them is faulty, the \ufb01nal result\nwill probably be incorrect.\nIn Figure 8.3(b), each device is replicated three times. Following each stage\nin the circuit is a triplicated voter. Each voter is a circuit that has three inputs\nand one output. If two or three of the inputs are the same, the output is equal to\nthat input. If all three inputs are different, the output is unde\ufb01ned. This kind of\ndesign is known as Triple Modular Redundancy (TMR ).\nSuppose that element A2fails. Each of the voters, V1,V2, and V3gets two\ngood (identical) inputs and one rogue input, and each of them outputs the correct\nvalue to the second stage. In essence, the effect of A2failing is completely masked,\nso that the inputs to B1,B2, and B3are exactly the same as they would have been\nhad no fault occurred.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.1. INTRODUCTION TO FAULT TOLERANCE 431\nClearly, having to deal with fail-arbitrary failures is the worst that can happen.\nAs we shall discuss shortly, we can design distributed systems in such a way\nthat they can even tolerate these types of failures.\nFailure masking by redundancy\nIf a system is to be fault tolerant, the best it can do is to try to hide the\noccurrence of failures from other processes. The key technique for masking\nfaults is to use redundancy. Three kinds are possible: information redundancy,\ntime redundancy, and physical redundancy (see also Johnson [1995]). With\ninformation redundancy , extra bits are added to allow recovery from garbled\nbits. For example, a Hamming code can be added to transmitted data to\nrecover from noise on the transmission line.\nWith time redundancy , an action is performed, and then, if need be, it is\nperformed again. Transactions use this approach. If a transaction aborts, it\ncan be redone with no harm. Another well-known example is retransmitting\na request to a server when lacking an expected response. Time redundancy is\nespecially helpful when the faults are transient or intermittent.\nWith physical redundancy , extra equipment or processes are added to\nmake it possible for the system as a whole to tolerate the loss or malfunction-\ning of some components. Physical redundancy can thus be done either in\nhardware or in software. For example, extra processes can be added to the\nsystem so that if a small number of them crash, the system can still function\ncorrectly. In other words, by replicating processes, a high degree of fault\ntolerance may be achieved. We return to this type of software redundancy\nlater in this chapter.\nNote 8.3 (More information: Triple modular redundancy)\nIt is illustrative to see how redundancy has been applied in the design of electronic\ndevices. Consider, for example, the circuit of Figure 8.3(a). Here signals pass\nthrough devices A,B, and C, in sequence. If one of them is faulty, the \ufb01nal result\nwill probably be incorrect.\nIn Figure 8.3(b), each device is replicated three times. Following each stage\nin the circuit is a triplicated voter. Each voter is a circuit that has three inputs\nand one output. If two or three of the inputs are the same, the output is equal to\nthat input. If all three inputs are different, the output is unde\ufb01ned. This kind of\ndesign is known as Triple Modular Redundancy (TMR ).\nSuppose that element A2fails. Each of the voters, V1,V2, and V3gets two\ngood (identical) inputs and one rogue input, and each of them outputs the correct\nvalue to the second stage. In essence, the effect of A2failing is completely masked,\nso that the inputs to B1,B2, and B3are exactly the same as they would have been\nhad no fault occurred.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "432 CHAPTER 8. FAULT TOLERANCE\n(a)\n(b)\nFigure 8.3: Triple modular redundancy.\nNow consider what happens if B3and C1are also faulty, in addition to A2.\nThese effects are also masked, so the three \ufb01nal outputs are still correct.\nAt \ufb01rst it may not be obvious why three voters are needed at each stage. After\nall, one voter could also detect and pass though the majority view. However, a\nvoter is also a component and can also be faulty. Suppose, for example, that voter\nV1malfunctions. The input to B1will then be wrong, but as long as everything\nelse works, B2andB3will produce the same output and V4,V5, and V6will all\nproduce the correct result into stage three. A fault in V1is effectively no different\nthan a fault in B1. In both cases B1produces incorrect output, but in both cases it\nis voted down later and the \ufb01nal result is still correct.\nAlthough not all fault-tolerant distributed systems use TMR, the technique is\nvery general, and should give a clear feeling for what a fault-tolerant system is, as\nopposed to a system whose individual components are highly reliable but whose\norganization cannot tolerate faults (i.e., operate correctly even in the presence of\nfaulty components). Of course, TMR can be applied recursively, for example, to\nmake a chip highly reliable by using TMR inside it, unknown to the designers\nwho use the chip, possibly in their own circuit containing multiple copies of the\nchips along with voters.\n8.2 Process resilience\nNow that the basic issues of fault tolerance have been discussed, let us\nconcentrate on how fault tolerance can actually be achieved in distributed\nsystems. The \ufb01rst topic we discuss is protection against process failures,\nwhich is achieved by replicating processes into groups. In the following pages,\nwe consider the general design issues of process groups and discuss what\na fault-tolerant group actually is. Also, we look at how to reach consensus\nwithin a process group when one or more of its members cannot be trusted to\ngive correct answers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n432 CHAPTER 8. FAULT TOLERANCE\n(a)\n(b)\nFigure 8.3: Triple modular redundancy.\nNow consider what happens if B3and C1are also faulty, in addition to A2.\nThese effects are also masked, so the three \ufb01nal outputs are still correct.\nAt \ufb01rst it may not be obvious why three voters are needed at each stage. After\nall, one voter could also detect and pass though the majority view. However, a\nvoter is also a component and can also be faulty. Suppose, for example, that voter\nV1malfunctions. The input to B1will then be wrong, but as long as everything\nelse works, B2andB3will produce the same output and V4,V5, and V6will all\nproduce the correct result into stage three. A fault in V1is effectively no different\nthan a fault in B1. In both cases B1produces incorrect output, but in both cases it\nis voted down later and the \ufb01nal result is still correct.\nAlthough not all fault-tolerant distributed systems use TMR, the technique is\nvery general, and should give a clear feeling for what a fault-tolerant system is, as\nopposed to a system whose individual components are highly reliable but whose\norganization cannot tolerate faults (i.e., operate correctly even in the presence of\nfaulty components). Of course, TMR can be applied recursively, for example, to\nmake a chip highly reliable by using TMR inside it, unknown to the designers\nwho use the chip, possibly in their own circuit containing multiple copies of the\nchips along with voters.\n8.2 Process resilience\nNow that the basic issues of fault tolerance have been discussed, let us\nconcentrate on how fault tolerance can actually be achieved in distributed\nsystems. The \ufb01rst topic we discuss is protection against process failures,\nwhich is achieved by replicating processes into groups. In the following pages,\nwe consider the general design issues of process groups and discuss what\na fault-tolerant group actually is. Also, we look at how to reach consensus\nwithin a process group when one or more of its members cannot be trusted to\ngive correct answers.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 433\nResilience by process groups\nThe key approach to tolerating a faulty process is to organize several identical\nprocesses into a group. The key property that all groups have is that when a\nmessage is sent to the group itself, all members of the group receive it. In this\nway, if one process in a group fails, hopefully some other process can take\nover for it [Guerraoui and Schiper, 1997].\nProcess groups may be dynamic. New groups can be created and old\ngroups can be destroyed. A process can join a group or leave one during\nsystem operation. A process can be a member of several groups at the same\ntime. Consequently, mechanisms are needed for managing groups and group\nmembership.\nThe purpose of introducing groups is to allow a process to deal with\ncollections of other processes as a single abstraction. Thus a process Pcan\nsend a message to a group Q=fQ1,. . .,QNgof servers without having to\nknow who they are, how many there are, or where they are, which may\nchange from one call to the next. To P, the group Qappears to be a single,\nlogical process.\nGroup organization\nAn important distinction between different groups has to do with their internal\nstructure. In some groups, all processes are equal. There is no distinctive\nleader and all decisions are made collectively. In other groups, some kind\nof hierarchy exists. For example, one process is the coordinator and all the\nothers are workers. In this model, when a request for work is generated, either\nby an external client or by one of the workers, it is sent to the coordinator.\nThe coordinator then decides which worker is best suited to carry it out, and\nforwards it there. More complex hierarchies are also possible, of course. These\ncommunication patterns are illustrated in Figure 8.4.\nEach of these organizations has its own advantages and disadvantages.\nThe \ufb02at group is symmetrical and has no single point of failure. If one of\nthe processes crashes, the group simply becomes smaller, but can otherwise\ncontinue. A disadvantage is that decision making is more complicated. For\nexample, to decide anything, a vote often has to be taken, incurring some\ndelay and overhead.\nThe hierarchical group has the opposite properties. Loss of the coordinator\nbrings the entire group to a grinding halt, but as long as it is running, it\ncan make decisions without bothering everyone else. In practice, when the\ncoordinator in a hierarchical group fails, its role will need to be taken over and\none of the workers is elected as new coordinator. We discussed leader-election\nalgorithms in Chapter 6.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 433\nResilience by process groups\nThe key approach to tolerating a faulty process is to organize several identical\nprocesses into a group. The key property that all groups have is that when a\nmessage is sent to the group itself, all members of the group receive it. In this\nway, if one process in a group fails, hopefully some other process can take\nover for it [Guerraoui and Schiper, 1997].\nProcess groups may be dynamic. New groups can be created and old\ngroups can be destroyed. A process can join a group or leave one during\nsystem operation. A process can be a member of several groups at the same\ntime. Consequently, mechanisms are needed for managing groups and group\nmembership.\nThe purpose of introducing groups is to allow a process to deal with\ncollections of other processes as a single abstraction. Thus a process Pcan\nsend a message to a group Q=fQ1,. . .,QNgof servers without having to\nknow who they are, how many there are, or where they are, which may\nchange from one call to the next. To P, the group Qappears to be a single,\nlogical process.\nGroup organization\nAn important distinction between different groups has to do with their internal\nstructure. In some groups, all processes are equal. There is no distinctive\nleader and all decisions are made collectively. In other groups, some kind\nof hierarchy exists. For example, one process is the coordinator and all the\nothers are workers. In this model, when a request for work is generated, either\nby an external client or by one of the workers, it is sent to the coordinator.\nThe coordinator then decides which worker is best suited to carry it out, and\nforwards it there. More complex hierarchies are also possible, of course. These\ncommunication patterns are illustrated in Figure 8.4.\nEach of these organizations has its own advantages and disadvantages.\nThe \ufb02at group is symmetrical and has no single point of failure. If one of\nthe processes crashes, the group simply becomes smaller, but can otherwise\ncontinue. A disadvantage is that decision making is more complicated. For\nexample, to decide anything, a vote often has to be taken, incurring some\ndelay and overhead.\nThe hierarchical group has the opposite properties. Loss of the coordinator\nbrings the entire group to a grinding halt, but as long as it is running, it\ncan make decisions without bothering everyone else. In practice, when the\ncoordinator in a hierarchical group fails, its role will need to be taken over and\none of the workers is elected as new coordinator. We discussed leader-election\nalgorithms in Chapter 6.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "434 CHAPTER 8. FAULT TOLERANCE\n(a) (b)\nFigure 8.4: Communication in a (a) \ufb02at group and in a (b) hierarchical group.\nMembership management\nWhen group communication is present, some method is needed for creating\nand deleting groups, as well as for allowing processes to join and leave groups.\nOne possible approach is to have a group server to which all these requests\ncan be sent. The group server can then maintain a complete database of all the\ngroups and their exact membership. This method is straightforward, ef\ufb01cient,\nand fairly easy to implement. Unfortunately, it shares a major disadvantage\nwith all centralized techniques: a single point of failure. If the group server\ncrashes, group management ceases to exist. Probably most or all groups will\nhave to be reconstructed from scratch, possibly terminating whatever work\nwas going on.\nThe opposite approach is to manage group membership in a distributed\nway. For example, if (reliable) multicasting is available, an outsider can send a\nmessage to all group members announcing its wish to join the group.\nIdeally, to leave a group, a member just sends a goodbye message to\neveryone. In the context of fault tolerance, assuming fail-stop failure semantics\nis generally not appropriate. The trouble is, there is no polite announcement\nthat a process crashes as there is when a process leaves voluntarily. The other\nmembers have to discover this experimentally by noticing that the crashed\nmember no longer responds to anything. Once it is certain that the crashed\nmember is really down (and not just slow), it can be removed from the group.\nAnother knotty issue is that leaving and joining have to be synchronous\nwith data messages being sent. In other words, starting at the instant that a\nprocess has joined a group, it must receive all messages sent to that group.\nSimilarly, as soon as a process has left a group, it must not receive any more\nmessages from the group, and the other members must not receive any more\nmessages from it. One way of making sure that a join or leave is integrated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n434 CHAPTER 8. FAULT TOLERANCE\n(a) (b)\nFigure 8.4: Communication in a (a) \ufb02at group and in a (b) hierarchical group.\nMembership management\nWhen group communication is present, some method is needed for creating\nand deleting groups, as well as for allowing processes to join and leave groups.\nOne possible approach is to have a group server to which all these requests\ncan be sent. The group server can then maintain a complete database of all the\ngroups and their exact membership. This method is straightforward, ef\ufb01cient,\nand fairly easy to implement. Unfortunately, it shares a major disadvantage\nwith all centralized techniques: a single point of failure. If the group server\ncrashes, group management ceases to exist. Probably most or all groups will\nhave to be reconstructed from scratch, possibly terminating whatever work\nwas going on.\nThe opposite approach is to manage group membership in a distributed\nway. For example, if (reliable) multicasting is available, an outsider can send a\nmessage to all group members announcing its wish to join the group.\nIdeally, to leave a group, a member just sends a goodbye message to\neveryone. In the context of fault tolerance, assuming fail-stop failure semantics\nis generally not appropriate. The trouble is, there is no polite announcement\nthat a process crashes as there is when a process leaves voluntarily. The other\nmembers have to discover this experimentally by noticing that the crashed\nmember no longer responds to anything. Once it is certain that the crashed\nmember is really down (and not just slow), it can be removed from the group.\nAnother knotty issue is that leaving and joining have to be synchronous\nwith data messages being sent. In other words, starting at the instant that a\nprocess has joined a group, it must receive all messages sent to that group.\nSimilarly, as soon as a process has left a group, it must not receive any more\nmessages from the group, and the other members must not receive any more\nmessages from it. One way of making sure that a join or leave is integrated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 435\ninto the message stream at the right place is to convert this operation into a\nsequence of messages sent to the whole group.\nOne \ufb01nal issue relating to group membership is what to do if so many\nprocesses go down that the group can no longer function at all. Some protocol\nis needed to rebuild the group. Invariably, some process will have to take\nthe initiative to start the ball rolling, but what happens if two or three try\nat the same time? The protocol must to be able to withstand this. Again,\ncoordination through, for example, a leader-election algorithm may be needed.\nFailure masking and replication\nProcess groups are part of the solution for building fault-tolerant systems.\nIn particular, having a group of identical processes allows us to mask one\nor more faulty processes in that group. In other words, we can replicate\nprocesses and organize them into a group to replace a single (vulnerable)\nprocess with a (fault tolerant) group. As discussed in the previous chapter,\nthere are two ways to approach such replication: by means of primary-based\nprotocols, or through replicated-write protocols.\nPrimary-based replication in the case of fault tolerance generally appears\nin the form of a primary-backup protocol. In this case, a group of processes is\norganized in a hierarchical fashion in which a primary coordinates all write\noperations. In practice, the primary is \ufb01xed, although its role can be taken\nover by one of the backups, if need be. In effect, when the primary crashes,\nthe backups execute some election algorithm to choose a new primary.\nReplicated-write protocols are used in the form of active replication, as\nwell as by means of quorum-based protocols. These solutions correspond\nto organizing a collection of identical processes into a \ufb02at group. The main\nadvantage is that such groups have no single point of failure at the cost of\ndistributed coordination.\nAn important issue with using process groups to tolerate faults is how\nmuch replication is needed. To simplify our discussion, let us consider\nonly replicated-write systems. A system is said to be k-fault tolerant if it\ncan survive faults in kcomponents and still meet its speci\ufb01cations. If the\ncomponents, say processes, fail silently, then having k+1of them is enough\nto provide k-fault tolerance. If kof them simply stop, then the answer from\nthe other one can be used.\nOn the other hand, if processes exhibit arbitrary failures, continuing to\nrun when faulty and sending out erroneous or random replies, a minimum\nof2k+1processes are needed to achieve k-fault tolerance. In the worst case,\nthekfailing processes could accidentally (or even intentionally) generate the\nsame reply. However, the remaining k+1will also produce the same answer,\nso the client or voter can just believe the majority.\nNow suppose that in a k-fault tolerant group a single process fails. The\ngroup as a whole is still living up to its speci\ufb01cations, namely that it can\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 435\ninto the message stream at the right place is to convert this operation into a\nsequence of messages sent to the whole group.\nOne \ufb01nal issue relating to group membership is what to do if so many\nprocesses go down that the group can no longer function at all. Some protocol\nis needed to rebuild the group. Invariably, some process will have to take\nthe initiative to start the ball rolling, but what happens if two or three try\nat the same time? The protocol must to be able to withstand this. Again,\ncoordination through, for example, a leader-election algorithm may be needed.\nFailure masking and replication\nProcess groups are part of the solution for building fault-tolerant systems.\nIn particular, having a group of identical processes allows us to mask one\nor more faulty processes in that group. In other words, we can replicate\nprocesses and organize them into a group to replace a single (vulnerable)\nprocess with a (fault tolerant) group. As discussed in the previous chapter,\nthere are two ways to approach such replication: by means of primary-based\nprotocols, or through replicated-write protocols.\nPrimary-based replication in the case of fault tolerance generally appears\nin the form of a primary-backup protocol. In this case, a group of processes is\norganized in a hierarchical fashion in which a primary coordinates all write\noperations. In practice, the primary is \ufb01xed, although its role can be taken\nover by one of the backups, if need be. In effect, when the primary crashes,\nthe backups execute some election algorithm to choose a new primary.\nReplicated-write protocols are used in the form of active replication, as\nwell as by means of quorum-based protocols. These solutions correspond\nto organizing a collection of identical processes into a \ufb02at group. The main\nadvantage is that such groups have no single point of failure at the cost of\ndistributed coordination.\nAn important issue with using process groups to tolerate faults is how\nmuch replication is needed. To simplify our discussion, let us consider\nonly replicated-write systems. A system is said to be k-fault tolerant if it\ncan survive faults in kcomponents and still meet its speci\ufb01cations. If the\ncomponents, say processes, fail silently, then having k+1of them is enough\nto provide k-fault tolerance. If kof them simply stop, then the answer from\nthe other one can be used.\nOn the other hand, if processes exhibit arbitrary failures, continuing to\nrun when faulty and sending out erroneous or random replies, a minimum\nof2k+1processes are needed to achieve k-fault tolerance. In the worst case,\nthekfailing processes could accidentally (or even intentionally) generate the\nsame reply. However, the remaining k+1will also produce the same answer,\nso the client or voter can just believe the majority.\nNow suppose that in a k-fault tolerant group a single process fails. The\ngroup as a whole is still living up to its speci\ufb01cations, namely that it can\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "436 CHAPTER 8. FAULT TOLERANCE\ntolerate the failure of up to kof its members (of which one has just failed). But\nwhat happens if more than kmembers fail? In that case all bets are off and\nwhatever the group does, its results, if any, cannot be trusted. Another way of\nlooking at this is that the process group, in its appearance of mimicking the\nbehavior of a single, robust process, has failed.\nConsensus in faulty systems with crash failures\nAs mentioned, in terms of clients and servers, we have adopted a model in\nwhich a potentially very large collection of clients now send commands to a\ngroup of processes that jointly behave as a single, highly robust process . To make\nthis work, we need to make an important assumption:\nIn a fault-tolerant process group, each nonfaulty process executes the\nsame commands, in the same order, as every other nonfaulty process.\nFormally, this means that the group members need to reach consensus on\nwhich command to execute. If failures cannot happen, reaching consensus\nis easy. For example, we can use Lamport\u2019s totally ordered multicasting as\ndescribed in Section 6.2. Or, to keep it simple, using a centralized sequencer\nthat hands out a sequence number to each command that needs to be executed\nwill do the job as well. Unfortunately, life is not without failures, and reaching\nconsensus among a group of processes under more realistic assumptions turns\nout to be tricky.\nTo illustrate the problem at hand, let us assume we have a group of\nprocesses P=fP1,. . .,Pngoperating under fail-stop failure semantics. In\nother words, we assume that crash failures can be reliably detected among\nthe group members. Typically a client contacts a group member requesting\nit to execute a command. Every group member maintains a list of proposed\ncommands: some which it received directly from clients; others which it\nreceived from its fellow group members. We can reach consensus using the\nfollowing approach, adopted from Cachin et al. [2011], and referred to as\n\ufb02ooding consensus .\nConceptually the algorithm operates in rounds. In each round, a process Pi\nsends its list of proposed commands it has seen so far to every other process\ninP. At the end of a round, each process merges all received proposed\ncommands into a new list, from which it then will deterministically select the\ncommand to execute, if possible. It is important to realize that the selection\nalgorithm is the same for all processes. In other words, if all process have\nexactly the same list, they will all select the same command to execute (and\nremove that command from their list).\nIt is not dif\ufb01cult to see that this approach works as long as processes do\nnot fail. Problems start when a process Pidetects, during round r, that, say\nprocess Pkhas crashed. To make this concrete, assume we have a process\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n436 CHAPTER 8. FAULT TOLERANCE\ntolerate the failure of up to kof its members (of which one has just failed). But\nwhat happens if more than kmembers fail? In that case all bets are off and\nwhatever the group does, its results, if any, cannot be trusted. Another way of\nlooking at this is that the process group, in its appearance of mimicking the\nbehavior of a single, robust process, has failed.\nConsensus in faulty systems with crash failures\nAs mentioned, in terms of clients and servers, we have adopted a model in\nwhich a potentially very large collection of clients now send commands to a\ngroup of processes that jointly behave as a single, highly robust process . To make\nthis work, we need to make an important assumption:\nIn a fault-tolerant process group, each nonfaulty process executes the\nsame commands, in the same order, as every other nonfaulty process.\nFormally, this means that the group members need to reach consensus on\nwhich command to execute. If failures cannot happen, reaching consensus\nis easy. For example, we can use Lamport\u2019s totally ordered multicasting as\ndescribed in Section 6.2. Or, to keep it simple, using a centralized sequencer\nthat hands out a sequence number to each command that needs to be executed\nwill do the job as well. Unfortunately, life is not without failures, and reaching\nconsensus among a group of processes under more realistic assumptions turns\nout to be tricky.\nTo illustrate the problem at hand, let us assume we have a group of\nprocesses P=fP1,. . .,Pngoperating under fail-stop failure semantics. In\nother words, we assume that crash failures can be reliably detected among\nthe group members. Typically a client contacts a group member requesting\nit to execute a command. Every group member maintains a list of proposed\ncommands: some which it received directly from clients; others which it\nreceived from its fellow group members. We can reach consensus using the\nfollowing approach, adopted from Cachin et al. [2011], and referred to as\n\ufb02ooding consensus .\nConceptually the algorithm operates in rounds. In each round, a process Pi\nsends its list of proposed commands it has seen so far to every other process\ninP. At the end of a round, each process merges all received proposed\ncommands into a new list, from which it then will deterministically select the\ncommand to execute, if possible. It is important to realize that the selection\nalgorithm is the same for all processes. In other words, if all process have\nexactly the same list, they will all select the same command to execute (and\nremove that command from their list).\nIt is not dif\ufb01cult to see that this approach works as long as processes do\nnot fail. Problems start when a process Pidetects, during round r, that, say\nprocess Pkhas crashed. To make this concrete, assume we have a process\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 437\ngroup of four processes fP1,. . .,P4gand that P1crashes during round r.\nAlso, assume that P2receives the list of proposed commands from P1before\nit crashes, but that P3and P4do not (in other words, P1crashes before it got\na chance to send its list to P3and P4). This situation is sketched in Figure 8.5.\nFigure 8.5: Reaching consensus through \ufb02ooding in the presence of crash\nfailures. Adopted from Cachin et al. [2011].\nAssuming that all processes knew who was group member at the beginning\nof round r,P2is ready to make a decision on which command to execute\nwhen it receives the respective lists of the other members: it has all commands\nproposed so far. Not so for P3and P4. For example, P3may detect that P1\ncrashed, but it does not know if either P2orP4had already received P1\u2019s\nlist. From P3\u2019s perspective, if there is another process that did receive P1\u2019s\nproposed commands, that process may then make a different decision than\nitself. As a consequence, the best that P3can do is postpone its decision\nuntil the next round. The same holds for P4in this example. A process will\ndecide to move to a next round when it has received a message from every\nnonfaulty process. This assumes that each process can reliably detect the\ncrashing of another process, for otherwise it would not be able to decide who\nthe nonfaulty processes are.\nBecause process P2received all commands, it can indeed make a decision\nand can subsequently broadcast that decision to the others. Then, during the\nnext round r+1, processes P3and P4will also be able to make a decision:\nthey will decide to execute the same command selected by P2.\nTo understand why this algorithm is correct, it is important to realize that\na process will move to a next round without having made a decision, only\nwhen it detects that another process has failed. In the end, this means that in\nthe worst case at most one nonfaulty process remains, and this process can\nsimply decide whatever proposed command to execute. Again, note that we\nare assuming reliable failure detection.\nBut then, what happens when the decision by process P2that it sent to\nP3was lost? In that case, P3can still not make a decision. Worse, we need\nto make sure that it makes the same decision as P2and P4. If P2did not\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 437\ngroup of four processes fP1,. . .,P4gand that P1crashes during round r.\nAlso, assume that P2receives the list of proposed commands from P1before\nit crashes, but that P3and P4do not (in other words, P1crashes before it got\na chance to send its list to P3and P4). This situation is sketched in Figure 8.5.\nFigure 8.5: Reaching consensus through \ufb02ooding in the presence of crash\nfailures. Adopted from Cachin et al. [2011].\nAssuming that all processes knew who was group member at the beginning\nof round r,P2is ready to make a decision on which command to execute\nwhen it receives the respective lists of the other members: it has all commands\nproposed so far. Not so for P3and P4. For example, P3may detect that P1\ncrashed, but it does not know if either P2orP4had already received P1\u2019s\nlist. From P3\u2019s perspective, if there is another process that did receive P1\u2019s\nproposed commands, that process may then make a different decision than\nitself. As a consequence, the best that P3can do is postpone its decision\nuntil the next round. The same holds for P4in this example. A process will\ndecide to move to a next round when it has received a message from every\nnonfaulty process. This assumes that each process can reliably detect the\ncrashing of another process, for otherwise it would not be able to decide who\nthe nonfaulty processes are.\nBecause process P2received all commands, it can indeed make a decision\nand can subsequently broadcast that decision to the others. Then, during the\nnext round r+1, processes P3and P4will also be able to make a decision:\nthey will decide to execute the same command selected by P2.\nTo understand why this algorithm is correct, it is important to realize that\na process will move to a next round without having made a decision, only\nwhen it detects that another process has failed. In the end, this means that in\nthe worst case at most one nonfaulty process remains, and this process can\nsimply decide whatever proposed command to execute. Again, note that we\nare assuming reliable failure detection.\nBut then, what happens when the decision by process P2that it sent to\nP3was lost? In that case, P3can still not make a decision. Worse, we need\nto make sure that it makes the same decision as P2and P4. If P2did not\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "438 CHAPTER 8. FAULT TOLERANCE\ncrash, we can assume that a retransmission of its decision will save the day.\nIfP2did crash, this will be also detected by P4who will then subsequently\nrebroadcast its decision. In the meantime, P3has moved to a next round,\nand after receiving the decision by P4, will terminate its execution of the\nalgorithm.\nExample: Paxos\nThe \ufb02ooding-based consensus algorithm is not very realistic if only for the\nfact that it relies on a fail-stop failure model. More realistic is to assume\na fail-noisy failure model in which a process will eventually reliably detect\nthat another process has crashed. In the following, we describe a simpli\ufb01ed\nversion of a widely adopted consensus algorithm, known as Paxos . It was\noriginally published in 1989 as a technical report by Leslie Lamport, but\nit took about a decade before someone decided that it may not be such a\nbad idea to disseminate it through a regular scienti\ufb01c channel [Lamport,\n1998]. The original publication is not easy to understand, exempli\ufb01ed by\nother publications that aim at explaining it [Lampson, 1996; Prisco et al., 1997;\nLamport, 2001; van Renesse and Altinbuken, 2015].\nEssential Paxos\nThe assumptions under which Paxos operates are rather weak:\n\u2022The distributed system is partially synchronous (in fact, it may even be\nasynchronous).\n\u2022Communication between processes may be unreliable, meaning that\nmessages may be lost, duplicated, or reordered.\n\u2022Messages that are corrupted can be detected as such (and thus subse-\nquently ignored).\n\u2022All operations are deterministic: once an execution is started, it is known\nexactly what it will do.\n\u2022Processes may exhibit crash failures, but not arbitrary failures, nor do\nprocesses collude.\nBy-and-large, these are realistic assumptions for many practical distributed\nsystems.\nWe roughly follow the explanation given by Lamport [2001] and Kirsch\nand Amir [2008]. The algorithm operates as a network of logical processes, of\nwhich there are different types. First, there are clients that request a speci\ufb01c\noperation to be executed. At the server side, each client is represented by a\nsingle proposer , which attempts to have a client\u2019s request accepted. Normally,\na single proposer has been designated as being the leader , and drives the\nprotocol toward reaching consensus.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n438 CHAPTER 8. FAULT TOLERANCE\ncrash, we can assume that a retransmission of its decision will save the day.\nIfP2did crash, this will be also detected by P4who will then subsequently\nrebroadcast its decision. In the meantime, P3has moved to a next round,\nand after receiving the decision by P4, will terminate its execution of the\nalgorithm.\nExample: Paxos\nThe \ufb02ooding-based consensus algorithm is not very realistic if only for the\nfact that it relies on a fail-stop failure model. More realistic is to assume\na fail-noisy failure model in which a process will eventually reliably detect\nthat another process has crashed. In the following, we describe a simpli\ufb01ed\nversion of a widely adopted consensus algorithm, known as Paxos . It was\noriginally published in 1989 as a technical report by Leslie Lamport, but\nit took about a decade before someone decided that it may not be such a\nbad idea to disseminate it through a regular scienti\ufb01c channel [Lamport,\n1998]. The original publication is not easy to understand, exempli\ufb01ed by\nother publications that aim at explaining it [Lampson, 1996; Prisco et al., 1997;\nLamport, 2001; van Renesse and Altinbuken, 2015].\nEssential Paxos\nThe assumptions under which Paxos operates are rather weak:\n\u2022The distributed system is partially synchronous (in fact, it may even be\nasynchronous).\n\u2022Communication between processes may be unreliable, meaning that\nmessages may be lost, duplicated, or reordered.\n\u2022Messages that are corrupted can be detected as such (and thus subse-\nquently ignored).\n\u2022All operations are deterministic: once an execution is started, it is known\nexactly what it will do.\n\u2022Processes may exhibit crash failures, but not arbitrary failures, nor do\nprocesses collude.\nBy-and-large, these are realistic assumptions for many practical distributed\nsystems.\nWe roughly follow the explanation given by Lamport [2001] and Kirsch\nand Amir [2008]. The algorithm operates as a network of logical processes, of\nwhich there are different types. First, there are clients that request a speci\ufb01c\noperation to be executed. At the server side, each client is represented by a\nsingle proposer , which attempts to have a client\u2019s request accepted. Normally,\na single proposer has been designated as being the leader , and drives the\nprotocol toward reaching consensus.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 439\nWhat we need to establish is that a proposed operation is accepted by an\nacceptor . If a majority of acceptors accepts the same proposal, the proposal\nis said to be chosen . However, what is chosen still needs to be learned . To this\nend, we will have a number of learner processes, each of which will execute a\nchosen proposal once it has been informed by a majority of acceptors.\nIt is important to note that a single proposer, acceptor, and learner form a\nsingle physical process, running on a single machine that the client commu-\nnicates with, as shown in Figure 8.6. We thus assume that if, for example, a\nproposer crashes, then the physical process that it is part of will have crashed.\nBy replicating this server we aim at obtaining fault tolerance in the presence\nof crash failures.\nFigure 8.6: The organization of Paxos into different logical processes.\nThe basic model is that the leading proposer receives requests from clients,\none at a time. A nonleading proposer forwards any client request to the leader.\nThe leading proposer sends its proposal to all acceptors, telling each to accept\nthe requested operation. Each acceptor will subsequently broadcast a learn\nmessage. If a learner receives the same learn message from a majority of\nacceptors, it knows that consensus has been reached on which operation to\nexecute, and will execute it.\nThere are at least two speci\ufb01c issues that need further attention. First, not\nonly do the servers need to reach consensus on which operation to execute,\nwe also need to make sure that each of them actually executes it. In other\nwords, how do we know for sure that a majority of the nonfaulty servers will\ncarry out the operation? There is essentially only one way out: have learn\nmessages be retransmitted. However, to make this work, an acceptor will have\nto log its decisions (in turn requiring a mechanism for purging logs). Because\nwe are assuming globally ordered proposal timestamps (as explained shortly),\nmissing messages can be easily detected, and also accepted operations will\nalways be executed in the same order by all learners.\nAs a general rule, the server hosting the leading proposer will also inform\nthe client when its requested operation has been executed. If another process\nhad taken over the lead, then it will also handle the response to the client.\nThis brings us to the second important issue: a failing leader. Life would\nbe easy if the failure of a leader would be reliably detected, after which a\nnew leader would be elected, and later, the recovering leader would instantly\nnotice that the world around it had changed. Unfortunately, life is not so easy.\nPaxos has been designed to tolerate proposers who still believe they are in the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 439\nWhat we need to establish is that a proposed operation is accepted by an\nacceptor . If a majority of acceptors accepts the same proposal, the proposal\nis said to be chosen . However, what is chosen still needs to be learned . To this\nend, we will have a number of learner processes, each of which will execute a\nchosen proposal once it has been informed by a majority of acceptors.\nIt is important to note that a single proposer, acceptor, and learner form a\nsingle physical process, running on a single machine that the client commu-\nnicates with, as shown in Figure 8.6. We thus assume that if, for example, a\nproposer crashes, then the physical process that it is part of will have crashed.\nBy replicating this server we aim at obtaining fault tolerance in the presence\nof crash failures.\nFigure 8.6: The organization of Paxos into different logical processes.\nThe basic model is that the leading proposer receives requests from clients,\none at a time. A nonleading proposer forwards any client request to the leader.\nThe leading proposer sends its proposal to all acceptors, telling each to accept\nthe requested operation. Each acceptor will subsequently broadcast a learn\nmessage. If a learner receives the same learn message from a majority of\nacceptors, it knows that consensus has been reached on which operation to\nexecute, and will execute it.\nThere are at least two speci\ufb01c issues that need further attention. First, not\nonly do the servers need to reach consensus on which operation to execute,\nwe also need to make sure that each of them actually executes it. In other\nwords, how do we know for sure that a majority of the nonfaulty servers will\ncarry out the operation? There is essentially only one way out: have learn\nmessages be retransmitted. However, to make this work, an acceptor will have\nto log its decisions (in turn requiring a mechanism for purging logs). Because\nwe are assuming globally ordered proposal timestamps (as explained shortly),\nmissing messages can be easily detected, and also accepted operations will\nalways be executed in the same order by all learners.\nAs a general rule, the server hosting the leading proposer will also inform\nthe client when its requested operation has been executed. If another process\nhad taken over the lead, then it will also handle the response to the client.\nThis brings us to the second important issue: a failing leader. Life would\nbe easy if the failure of a leader would be reliably detected, after which a\nnew leader would be elected, and later, the recovering leader would instantly\nnotice that the world around it had changed. Unfortunately, life is not so easy.\nPaxos has been designed to tolerate proposers who still believe they are in the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "440 CHAPTER 8. FAULT TOLERANCE\nlead. The effect is that proposals may be sent out concurrently by different\nproposers (each believing to be the leader). We therefore need to make sure\nthat these proposals can be distinguished from one another in order to ensure\nthat the acceptors handle only the proposals from the current leader.\nNote that relying on a leading proposer implies that any practical imple-\nmentation of Paxos will need to be accompanied by a leader-election algorithm.\nIn principle, that algorithm can operate independently from Paxos, but is\nnormally part of it.\nTo distinguish concurrent proposals from different proposers, each pro-\nposal phas a uniquely associated (logical) timestamp ts(p). How uniqueness\nis achieved is left to an implementation, but we will describe some of the\ndetails shortly. Let oper(p)denote the operation associated with proposal p.\nThe trick is to allow multiple proposals to be accepted, but that each of these\naccepted proposals has the same associated operation. This can be achieved\nby guaranteeing that if a proposal pis chosen, then any proposal with a higher\ntimestamp will also have the same associated operation. In other words, we\nrequire that\npis chosen)for all p0with ts(p0)>ts(p):oper(p0) =oper(p)\nOf course, for pto be chosen, it needs to be accepted. That means that we\ncan guarantee our requirement when guaranteeing that if pis chosen, then\nany higher-timestamped proposal accepted by any acceptor, has the same\nassociated operation as p. However, this is not suf\ufb01cient, for suppose that\nat a certain moment a proposer simply sends a new proposal p0, with the\nhighest timestamp so far, to an acceptor Athat had not received any proposal\nbefore. Note that this may indeed happen according to our assumptions\nconcerning message loss and multiple proposers each believing to be in the\nlead. In absence of any other proposals, Awill simply accept p0. To prevent\nthis situation from happening, we thus need to guarantee that\nIf proposal pis chosen, then any higher-timestamped proposal issued by\na proposer, has the same associated operation as p.\nWhen explaining the Paxos algorithm below, we will indeed see that a pro-\nposer may need to adopt an operation coming from acceptors in favor of its\nown. This will happen after a leading proposer had failed, but its proposed\noperation had already made it to a majority of the acceptors.\nThe processes collectively formally ensure safety , in the sense that only\nproposed operations will be learned, and that at most one operation will be\nlearned at a time. In general, a safety property asserts that nothing bad will\nhappen. Furthermore, Paxos ensures conditional liveness in the sense that if\nenough processes remain up-and-running, then a proposed operation will\neventually be learned (and thus executed). Liveness , which tells us that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n440 CHAPTER 8. FAULT TOLERANCE\nlead. The effect is that proposals may be sent out concurrently by different\nproposers (each believing to be the leader). We therefore need to make sure\nthat these proposals can be distinguished from one another in order to ensure\nthat the acceptors handle only the proposals from the current leader.\nNote that relying on a leading proposer implies that any practical imple-\nmentation of Paxos will need to be accompanied by a leader-election algorithm.\nIn principle, that algorithm can operate independently from Paxos, but is\nnormally part of it.\nTo distinguish concurrent proposals from different proposers, each pro-\nposal phas a uniquely associated (logical) timestamp ts(p). How uniqueness\nis achieved is left to an implementation, but we will describe some of the\ndetails shortly. Let oper(p)denote the operation associated with proposal p.\nThe trick is to allow multiple proposals to be accepted, but that each of these\naccepted proposals has the same associated operation. This can be achieved\nby guaranteeing that if a proposal pis chosen, then any proposal with a higher\ntimestamp will also have the same associated operation. In other words, we\nrequire that\npis chosen)for all p0with ts(p0)>ts(p):oper(p0) =oper(p)\nOf course, for pto be chosen, it needs to be accepted. That means that we\ncan guarantee our requirement when guaranteeing that if pis chosen, then\nany higher-timestamped proposal accepted by any acceptor, has the same\nassociated operation as p. However, this is not suf\ufb01cient, for suppose that\nat a certain moment a proposer simply sends a new proposal p0, with the\nhighest timestamp so far, to an acceptor Athat had not received any proposal\nbefore. Note that this may indeed happen according to our assumptions\nconcerning message loss and multiple proposers each believing to be in the\nlead. In absence of any other proposals, Awill simply accept p0. To prevent\nthis situation from happening, we thus need to guarantee that\nIf proposal pis chosen, then any higher-timestamped proposal issued by\na proposer, has the same associated operation as p.\nWhen explaining the Paxos algorithm below, we will indeed see that a pro-\nposer may need to adopt an operation coming from acceptors in favor of its\nown. This will happen after a leading proposer had failed, but its proposed\noperation had already made it to a majority of the acceptors.\nThe processes collectively formally ensure safety , in the sense that only\nproposed operations will be learned, and that at most one operation will be\nlearned at a time. In general, a safety property asserts that nothing bad will\nhappen. Furthermore, Paxos ensures conditional liveness in the sense that if\nenough processes remain up-and-running, then a proposed operation will\neventually be learned (and thus executed). Liveness , which tells us that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 441\neventually something good will happen, is not guaranteed in Paxos, unless\nsome adaptations are made. We return to liveness in Note 8.4.\nThere are now two phases, each in turn consisting of two subphases.\nDuring the \ufb01rst phase, the leading proposer interacts with acceptors to get\na requested operation accepted for execution. The \ufb01rst phase is needed to\nrule out any trouble caused by different proposers each believing they are\nthe leader. The best that can happen is that an individual acceptor promises\nto consider the proposer\u2019s operation and ignore other requests. The worst is\nthat the proposer was too late and that it will be asked to adopt some other\nproposer\u2019s request instead. Apparently, a leadership change had taken place\nand there may be former requests that need to handled \ufb01rst.\nIn the second phase, the acceptors will have informed proposers about the\npromises they have made. The leading proposer essentially takes up a slightly\ndifferent role by promoting a single operation to the one to be executed, and\nsubsequently telling the acceptors.\nPhase 1a (prepare): The goal of this phase is that a proposer Pwho believes it\nis leader and is proposing operation o, tries to get its proposal timestamp\nanchored , in the sense that any lower timestamp failed, or that ohad also\nbeen previously proposed (i.e., with some lower proposal timestamp).\nTo this end, Pbroadcasts its proposal to the acceptors. For the operation\no, the proposer selects a proposal number mhigher than any of its\npreviously selected numbers. This leads to a proposal timestamp t=\n(m,i)where iis the (numerical) process identi\ufb01er of P. Note that\n(m,i)<(n,j),(m<n)or(m=nand i<j)\nThis timestamp for a proposal pis an implementation of the previously\nmentioned timestamp ts(p). Proposer Psends prepare (t)to all accep-\ntors (but note that messages may be lost). In doing so, it is (1) asking the\nacceptors to promise not to accept any proposals with a lower proposal\ntimestamp, and (2) to inform it about an accepted proposal, if any, with\nthe highest timestamp less than t. If such a proposal exists, the proposer\nwill adopt it.\nPhase 1b (promise): An acceptor Amay receive multiple proposals. Assume\nit receives prepare (t)from P. There are three cases to consider:\n(1)tis the highest proposal timestamp received from any proposer so\nfar. In that case, Awill return promise (t)toPstating that Awill\nignore any future proposals with a lower timestamp.\n(2)Iftis the highest timestamp so far, but another proposal (t0,o0)had\nalready been accepted, Aalso returns (t0,o0)toP. This will allow P\nto decide on the \ufb01nal operation that needs to be accepted.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 441\neventually something good will happen, is not guaranteed in Paxos, unless\nsome adaptations are made. We return to liveness in Note 8.4.\nThere are now two phases, each in turn consisting of two subphases.\nDuring the \ufb01rst phase, the leading proposer interacts with acceptors to get\na requested operation accepted for execution. The \ufb01rst phase is needed to\nrule out any trouble caused by different proposers each believing they are\nthe leader. The best that can happen is that an individual acceptor promises\nto consider the proposer\u2019s operation and ignore other requests. The worst is\nthat the proposer was too late and that it will be asked to adopt some other\nproposer\u2019s request instead. Apparently, a leadership change had taken place\nand there may be former requests that need to handled \ufb01rst.\nIn the second phase, the acceptors will have informed proposers about the\npromises they have made. The leading proposer essentially takes up a slightly\ndifferent role by promoting a single operation to the one to be executed, and\nsubsequently telling the acceptors.\nPhase 1a (prepare): The goal of this phase is that a proposer Pwho believes it\nis leader and is proposing operation o, tries to get its proposal timestamp\nanchored , in the sense that any lower timestamp failed, or that ohad also\nbeen previously proposed (i.e., with some lower proposal timestamp).\nTo this end, Pbroadcasts its proposal to the acceptors. For the operation\no, the proposer selects a proposal number mhigher than any of its\npreviously selected numbers. This leads to a proposal timestamp t=\n(m,i)where iis the (numerical) process identi\ufb01er of P. Note that\n(m,i)<(n,j),(m<n)or(m=nand i<j)\nThis timestamp for a proposal pis an implementation of the previously\nmentioned timestamp ts(p). Proposer Psends prepare (t)to all accep-\ntors (but note that messages may be lost). In doing so, it is (1) asking the\nacceptors to promise not to accept any proposals with a lower proposal\ntimestamp, and (2) to inform it about an accepted proposal, if any, with\nthe highest timestamp less than t. If such a proposal exists, the proposer\nwill adopt it.\nPhase 1b (promise): An acceptor Amay receive multiple proposals. Assume\nit receives prepare (t)from P. There are three cases to consider:\n(1)tis the highest proposal timestamp received from any proposer so\nfar. In that case, Awill return promise (t)toPstating that Awill\nignore any future proposals with a lower timestamp.\n(2)Iftis the highest timestamp so far, but another proposal (t0,o0)had\nalready been accepted, Aalso returns (t0,o0)toP. This will allow P\nto decide on the \ufb01nal operation that needs to be accepted.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "442 CHAPTER 8. FAULT TOLERANCE\n(3)In all other cases, do nothing: there is apparently another proposal\nwith a higher timestamp that is being processed.\nOnce the \ufb01rst phase has been completed, the leading proposer Pknows what\nthe acceptors have promised. Essentially, the leading proposer knows that all\nacceptors have agreed on the same operation. This will put Pinto a position\nto tell the acceptors that they can go ahead. This is needed, because although\nthe leading proposer knows on which operation consensus has been reached,\nthis consensus is not known to the others. Again, we assume that Preceived\na response from a majority of acceptors (whose respective responses may be\ndifferent).\nPhase 2a (accept): There are two cases to consider:\n(1)IfPdoes not receive any accepted operation from any of the ac-\nceptors, it will forward its own proposal for acceptance by sending\naccept (t,o)to all acceptors.\n(2)Otherwise, it was informed about another operation o0, which it will\nadopt and forward for acceptance. It does so by sending accept (t,\no0), where tisP\u2019s proposal timestamp and o0is the operation with\nproposal timestamp highest among all accepted operations that\nwere returned by the acceptors in Phase 1b.\nPhase 2b (learn): Finally, if an acceptor receives accept (t,o0), but did not\npreviously send a promise with a higher proposal timestamp, it will ac-\ncept operation o0, and tell all learners to execute o0by sending learn (o0).\nAt that point, the acceptor can forget about o0. A learner Lreceiving\nlearn (o0)from a majority of acceptors, will execute the operation o0.\nWe now also know that a majority of learners share the same idea on\nwhich operation to execute.\nIt is important to realize that this description of Paxos indeed captures only its\nessence: using a leading proposer to drive the acceptors toward the execution\nof the same operation. When it comes to practical implementations, much\nmore needs to be done (and more than we are willing to describe here). An\nexcellent description of what it means to realize Paxos has been written by\nKirsch and Amir [2008]. Another write-up on its practical implications can be\nfound in [Chandra et al., 2007].\nUnderstanding Paxos\nTo properly understand Paxos, but also many other consensus algorithms, it\nis useful to see how its design could have evolved. We say \u201ccould have,\u201d as\nthe evolution of the algorithm has never been documented. The following\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n442 CHAPTER 8. FAULT TOLERANCE\n(3)In all other cases, do nothing: there is apparently another proposal\nwith a higher timestamp that is being processed.\nOnce the \ufb01rst phase has been completed, the leading proposer Pknows what\nthe acceptors have promised. Essentially, the leading proposer knows that all\nacceptors have agreed on the same operation. This will put Pinto a position\nto tell the acceptors that they can go ahead. This is needed, because although\nthe leading proposer knows on which operation consensus has been reached,\nthis consensus is not known to the others. Again, we assume that Preceived\na response from a majority of acceptors (whose respective responses may be\ndifferent).\nPhase 2a (accept): There are two cases to consider:\n(1)IfPdoes not receive any accepted operation from any of the ac-\nceptors, it will forward its own proposal for acceptance by sending\naccept (t,o)to all acceptors.\n(2)Otherwise, it was informed about another operation o0, which it will\nadopt and forward for acceptance. It does so by sending accept (t,\no0), where tisP\u2019s proposal timestamp and o0is the operation with\nproposal timestamp highest among all accepted operations that\nwere returned by the acceptors in Phase 1b.\nPhase 2b (learn): Finally, if an acceptor receives accept (t,o0), but did not\npreviously send a promise with a higher proposal timestamp, it will ac-\ncept operation o0, and tell all learners to execute o0by sending learn (o0).\nAt that point, the acceptor can forget about o0. A learner Lreceiving\nlearn (o0)from a majority of acceptors, will execute the operation o0.\nWe now also know that a majority of learners share the same idea on\nwhich operation to execute.\nIt is important to realize that this description of Paxos indeed captures only its\nessence: using a leading proposer to drive the acceptors toward the execution\nof the same operation. When it comes to practical implementations, much\nmore needs to be done (and more than we are willing to describe here). An\nexcellent description of what it means to realize Paxos has been written by\nKirsch and Amir [2008]. Another write-up on its practical implications can be\nfound in [Chandra et al., 2007].\nUnderstanding Paxos\nTo properly understand Paxos, but also many other consensus algorithms, it\nis useful to see how its design could have evolved. We say \u201ccould have,\u201d as\nthe evolution of the algorithm has never been documented. The following\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 443\ndescription is largely based on work described by Meling and Jehl [2013]1. As\nour starting point, we consider a server that we wish to make more robust. By\nnow, we know that this can be achieved through replication and making sure\nthat all commands submitted by clients are executed by all servers in the same\norder. The simplest situation is to add one server, thus creating a group of two\nprocesses, say S1and S2. Also, to make sure that all commands are executed\nin the same order, we appoint one process to be a sequencer, which increments\nand associates a unique timestamp with every submitted command. Servers\nare required to execute commands according to their timestamp. In Paxos,\nsuch a server is referred to as the leader . We can also consider it to be a\nprimary server , with the other acting as a backup server .\nWe assume that a client broadcasts its requested command to all servers.\nIf a server notices it is missing a command, it can rely on the other server\nto forward it when necessary. We will not describe how this happens, but\nsilently assume that all commands are stored at the servers and that we merely\nneed to make sure that the servers agree on which command to execute next.\nAs a consequence, all remaining communication between servers consists of\ncontrol messages. To make this point clear, consider the situation sketched in\nFigure 8.7. (In what follows, we use subscripts to designate processes, and\nsuperscripts to designate operations and states.)\nFigure 8.7: Two clients communicating with a 2-server process group.\nIn this example, server S1is the leader and as such will hand out time-\nstamps to submitted requests. Client C1has submitted command o1while C2\nsubmitted o2.S1instructs S2to execute operation o2with timestamp 1, and\nlater operation o1with timestamp 2. After processing a command, a server\nwill return the result to the associated client. We designate this using the\nnotationhsj\nii, where iis the index of the reporting server, and jthe state it\nwas in, expressed as the sequence of operations it has carried out. In our\nexample, client C1will thus see the result hs21\nki, meaning that each server has\nexecuted o1after executing o2.\n1Special credits go to Hein Meling for helping us better understand what Paxos is all about.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 443\ndescription is largely based on work described by Meling and Jehl [2013]1. As\nour starting point, we consider a server that we wish to make more robust. By\nnow, we know that this can be achieved through replication and making sure\nthat all commands submitted by clients are executed by all servers in the same\norder. The simplest situation is to add one server, thus creating a group of two\nprocesses, say S1and S2. Also, to make sure that all commands are executed\nin the same order, we appoint one process to be a sequencer, which increments\nand associates a unique timestamp with every submitted command. Servers\nare required to execute commands according to their timestamp. In Paxos,\nsuch a server is referred to as the leader . We can also consider it to be a\nprimary server , with the other acting as a backup server .\nWe assume that a client broadcasts its requested command to all servers.\nIf a server notices it is missing a command, it can rely on the other server\nto forward it when necessary. We will not describe how this happens, but\nsilently assume that all commands are stored at the servers and that we merely\nneed to make sure that the servers agree on which command to execute next.\nAs a consequence, all remaining communication between servers consists of\ncontrol messages. To make this point clear, consider the situation sketched in\nFigure 8.7. (In what follows, we use subscripts to designate processes, and\nsuperscripts to designate operations and states.)\nFigure 8.7: Two clients communicating with a 2-server process group.\nIn this example, server S1is the leader and as such will hand out time-\nstamps to submitted requests. Client C1has submitted command o1while C2\nsubmitted o2.S1instructs S2to execute operation o2with timestamp 1, and\nlater operation o1with timestamp 2. After processing a command, a server\nwill return the result to the associated client. We designate this using the\nnotationhsj\nii, where iis the index of the reporting server, and jthe state it\nwas in, expressed as the sequence of operations it has carried out. In our\nexample, client C1will thus see the result hs21\nki, meaning that each server has\nexecuted o1after executing o2.\n1Special credits go to Hein Meling for helping us better understand what Paxos is all about.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "444 CHAPTER 8. FAULT TOLERANCE\nIn Paxos, when a leader associates a timestamp with an operation, it does\nso by sending an accept message to the other server(s). As we assume that\nmessages may be lost, a server accepting an operation odoes so by telling\nthe leader it has learned the operation by returning a learn (o)message.\nWhen the leader does not notice that operation ohas been learned, it simply\nretransmits an accept (o,t)message, with tbeing the original timestamp. Note\nthat in our description we are skipping the phase of coming to agreement on\nthe operation to be carried out: we assume the leader has decided and now\nneeds to reach consensus on executing that operation.\nCompensating for a lost message is relatively easy, but what happens when\nalso a server crashes? Let us \ufb01rst assume that a crash can be reliably detected.\nConsider the situation sketched in Figure 8.8(a). The issue, of course, is that\nserver S2will never have learned (about) operation o1. This situation can be\nprevented by demanding that a server may execute an operation only if it\nknows that the other server has learned the operation as well, as illustrated in\nFigure 8.8(b).\n(a)\n(b)\nFigure 8.8: (a) What may happen when the leader crashes in combination\nwith a lost accept, and (b) the solution, namely demanding that the other\nserver has learned the operation as well before executing it.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n444 CHAPTER 8. FAULT TOLERANCE\nIn Paxos, when a leader associates a timestamp with an operation, it does\nso by sending an accept message to the other server(s). As we assume that\nmessages may be lost, a server accepting an operation odoes so by telling\nthe leader it has learned the operation by returning a learn (o)message.\nWhen the leader does not notice that operation ohas been learned, it simply\nretransmits an accept (o,t)message, with tbeing the original timestamp. Note\nthat in our description we are skipping the phase of coming to agreement on\nthe operation to be carried out: we assume the leader has decided and now\nneeds to reach consensus on executing that operation.\nCompensating for a lost message is relatively easy, but what happens when\nalso a server crashes? Let us \ufb01rst assume that a crash can be reliably detected.\nConsider the situation sketched in Figure 8.8(a). The issue, of course, is that\nserver S2will never have learned (about) operation o1. This situation can be\nprevented by demanding that a server may execute an operation only if it\nknows that the other server has learned the operation as well, as illustrated in\nFigure 8.8(b).\n(a)\n(b)\nFigure 8.8: (a) What may happen when the leader crashes in combination\nwith a lost accept, and (b) the solution, namely demanding that the other\nserver has learned the operation as well before executing it.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 445\nIt is not dif\ufb01cult to see that with a larger process group, we can get into the\nsame situation as in Figure 8.8(a). Simply consider a group of three servers\nfS1,S2,S3gwith S1being the leader. If its accept (o1,t)message to S3is lost,\nyet it knows that S2has learned o1, then it should still not execute o1until\nit has also received a learn (o1)message from S3. This situation is shown in\nFigure 8.9.\nFigure 8.9: The situation when dealing with three servers of which two crash.\nNow imagine what would happen if learn (o1)returned by S2would not\nmake it to S1. Of course, S1would not execute o1, but otherwise we would\nstill be in trouble: S2will execute o1while S3would take over leadership and\nexecute o2without ever knowing that o1had already been processed. In other\nwords, also S2must wait with the execution of o1until it knows that S3has\nlearned that operation. This brings us to the following:\nIn Paxos, a server Scannot execute an operation ountil it has received\nalearn (o)from all other nonfaulty servers.\nUp to this point we have assumed that a process can reliably detect that\nanother process has crashed. In practice, this is not the case. As we will\ndiscuss more extensively shortly, a standard approach toward failure detection\nis to set a timeout on expected messages. For example, each server is required\nto send a message declaring it is still alive, and at the same time the other\nservers set timeouts on the expected receipt of such messages. If a timeout\nexpires, the sender is suspected to have failed. In a partially synchronous or\nfully asynchronous system, there is essentially no other solution. However,\nthe consequence is that a failure may be falsely detected, as the delivery of\nsuch \u201cI\u2019m alive\u201d messages may have simply been delayed or lost.\nLet us assume that Paxos has realized a failure detection mechanism, but\nthat the two servers falsely conclude that the other has failed, as shown in\nFigure 8.10. The problem is clear: each may now independently decide to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 445\nIt is not dif\ufb01cult to see that with a larger process group, we can get into the\nsame situation as in Figure 8.8(a). Simply consider a group of three servers\nfS1,S2,S3gwith S1being the leader. If its accept (o1,t)message to S3is lost,\nyet it knows that S2has learned o1, then it should still not execute o1until\nit has also received a learn (o1)message from S3. This situation is shown in\nFigure 8.9.\nFigure 8.9: The situation when dealing with three servers of which two crash.\nNow imagine what would happen if learn (o1)returned by S2would not\nmake it to S1. Of course, S1would not execute o1, but otherwise we would\nstill be in trouble: S2will execute o1while S3would take over leadership and\nexecute o2without ever knowing that o1had already been processed. In other\nwords, also S2must wait with the execution of o1until it knows that S3has\nlearned that operation. This brings us to the following:\nIn Paxos, a server Scannot execute an operation ountil it has received\nalearn (o)from all other nonfaulty servers.\nUp to this point we have assumed that a process can reliably detect that\nanother process has crashed. In practice, this is not the case. As we will\ndiscuss more extensively shortly, a standard approach toward failure detection\nis to set a timeout on expected messages. For example, each server is required\nto send a message declaring it is still alive, and at the same time the other\nservers set timeouts on the expected receipt of such messages. If a timeout\nexpires, the sender is suspected to have failed. In a partially synchronous or\nfully asynchronous system, there is essentially no other solution. However,\nthe consequence is that a failure may be falsely detected, as the delivery of\nsuch \u201cI\u2019m alive\u201d messages may have simply been delayed or lost.\nLet us assume that Paxos has realized a failure detection mechanism, but\nthat the two servers falsely conclude that the other has failed, as shown in\nFigure 8.10. The problem is clear: each may now independently decide to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "446 CHAPTER 8. FAULT TOLERANCE\nexecute their operation of choice, leading to divergent behavior. It is at this\npoint that we need to introduce an extra server, and demand that a server\ncan execute an operation only if it is certain that a majority will execute that\noperation. Note, that in the three-server case, execution of operation oby\nserver Scan take place as soon as Shas received at least one (other) learn (o)\nmessage. Together with the sender of that message, Swill form a majority.\nFigure 8.10: The situation in the case of false failure detections.\nWe have now come to a point where it should be clear that Paxos requires\nat least three replicated servers fS1,S2,S3gin order to operate correctly. Let\nus concentrate on the situation that one of these servers crashes. We make the\nfollowing assumptions.\n\u2022 Initially, S1is the leader.\n\u2022A server can reliably detect it has missed a message. The latter can\nbe realized, for example, through timestamps in the form of strictly\nincreasing sequence numbers. Whenever a server notices it has missed\na message, it can then simply request a retransmission and catch up\nbefore continuing.\n\u2022When a new leader needs to be elected, the remaining servers follow a\nstrictly deterministic algorithm. For example, we can safely assume that\nifS1crashes, then S2will become leader. Likewise, if S2crashes, S3will\ntake over, and so on.\n\u2022Clients may receive duplicate responses, but besides being required to\nrecognize duplicates, form no further part of the Paxos protocol. In\nother words, a client cannot be asked to help the servers to resolve a\nsituation.\nUnder these circumstances, it is fairly easy to see that no matter when one of\nS2orS3crashes, Paxos will behave correctly. Of course, we are still demanding\nthat execution of an operation can take place only if a server knows that a\nmajority will execute that operation.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n446 CHAPTER 8. FAULT TOLERANCE\nexecute their operation of choice, leading to divergent behavior. It is at this\npoint that we need to introduce an extra server, and demand that a server\ncan execute an operation only if it is certain that a majority will execute that\noperation. Note, that in the three-server case, execution of operation oby\nserver Scan take place as soon as Shas received at least one (other) learn (o)\nmessage. Together with the sender of that message, Swill form a majority.\nFigure 8.10: The situation in the case of false failure detections.\nWe have now come to a point where it should be clear that Paxos requires\nat least three replicated servers fS1,S2,S3gin order to operate correctly. Let\nus concentrate on the situation that one of these servers crashes. We make the\nfollowing assumptions.\n\u2022 Initially, S1is the leader.\n\u2022A server can reliably detect it has missed a message. The latter can\nbe realized, for example, through timestamps in the form of strictly\nincreasing sequence numbers. Whenever a server notices it has missed\na message, it can then simply request a retransmission and catch up\nbefore continuing.\n\u2022When a new leader needs to be elected, the remaining servers follow a\nstrictly deterministic algorithm. For example, we can safely assume that\nifS1crashes, then S2will become leader. Likewise, if S2crashes, S3will\ntake over, and so on.\n\u2022Clients may receive duplicate responses, but besides being required to\nrecognize duplicates, form no further part of the Paxos protocol. In\nother words, a client cannot be asked to help the servers to resolve a\nsituation.\nUnder these circumstances, it is fairly easy to see that no matter when one of\nS2orS3crashes, Paxos will behave correctly. Of course, we are still demanding\nthat execution of an operation can take place only if a server knows that a\nmajority will execute that operation.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 447\nSuppose now that S1crashes after the execution of operation o1. The\nworst that can happen in this case is that S3is completely ignorant of the\nsituation until the new leader, S2tells it to accept operation o2. Note that\nthis is announced through an accept (o2, 2)message such that the timestamp\nt=2will alert S3that it missed a previous accept message. S3will tell so to\nS2, who can then retransmit accept (o1, 1), allowing S3to catch up.\nLikewise, if S2missed accept (o1, 1), but did detect that S1crashed, it will\neventually either send accept (o1, 1)oraccept (o2, 1)(i.e., in both cases using\ntimestamp t=1, which was previously used by S1). Again, S3has enough\ninformation to get S2on the right track again. If S2had sent accept (o1, 1),S3\ncan simply tell S2that it already learned o1. In the other case, when S2sends\naccept (o2, 1),S3will inform S2that it apparently missed operation o1. We\nconclude that when S1crashes after executing an operation, Paxos behaves\ncorrectly.\nSo what can happen if S1crashes immediately after having sent accept (o1,\n1)to the other two servers? Suppose again that S3is completely ignorant of\nthe situation because messages are lost, until S2has taken over leadership\nand announces that o2should be accepted. Like before, S3can tell S2that\nit (i.e., S3) missed operation o1, so that S2can help S3to catch up. If S2\nmisses messages, but does detect that S1crashed, then as soon as it takes over\nleadership and proposes an operation, it will be using a stale timestamp. This\nwill trigger S3to inform S2that it missed operation o1, which saves the day.\nAgain, Paxos is seen to behave correctly.\nFigure 8.11: Why incorporating the ID of the current leader is needed.\nProblems may arise with false detections of crashes. Consider the sit-\nuation sketched in Figure 8.11. We see that the accept messages from S1\nare considerably delayed and that S2falsely detects S1having crashed. S2\ntakes over leadership and sends accept (o2, 1), i.e., with a timestamp t=1.\nHowever, when \ufb01nally accept (o1, 1)arrives, S3cannot do anything: this is\nnot a message it is expecting. Fortunately, if it knows who the current leader\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 447\nSuppose now that S1crashes after the execution of operation o1. The\nworst that can happen in this case is that S3is completely ignorant of the\nsituation until the new leader, S2tells it to accept operation o2. Note that\nthis is announced through an accept (o2, 2)message such that the timestamp\nt=2will alert S3that it missed a previous accept message. S3will tell so to\nS2, who can then retransmit accept (o1, 1), allowing S3to catch up.\nLikewise, if S2missed accept (o1, 1), but did detect that S1crashed, it will\neventually either send accept (o1, 1)oraccept (o2, 1)(i.e., in both cases using\ntimestamp t=1, which was previously used by S1). Again, S3has enough\ninformation to get S2on the right track again. If S2had sent accept (o1, 1),S3\ncan simply tell S2that it already learned o1. In the other case, when S2sends\naccept (o2, 1),S3will inform S2that it apparently missed operation o1. We\nconclude that when S1crashes after executing an operation, Paxos behaves\ncorrectly.\nSo what can happen if S1crashes immediately after having sent accept (o1,\n1)to the other two servers? Suppose again that S3is completely ignorant of\nthe situation because messages are lost, until S2has taken over leadership\nand announces that o2should be accepted. Like before, S3can tell S2that\nit (i.e., S3) missed operation o1, so that S2can help S3to catch up. If S2\nmisses messages, but does detect that S1crashed, then as soon as it takes over\nleadership and proposes an operation, it will be using a stale timestamp. This\nwill trigger S3to inform S2that it missed operation o1, which saves the day.\nAgain, Paxos is seen to behave correctly.\nFigure 8.11: Why incorporating the ID of the current leader is needed.\nProblems may arise with false detections of crashes. Consider the sit-\nuation sketched in Figure 8.11. We see that the accept messages from S1\nare considerably delayed and that S2falsely detects S1having crashed. S2\ntakes over leadership and sends accept (o2, 1), i.e., with a timestamp t=1.\nHowever, when \ufb01nally accept (o1, 1)arrives, S3cannot do anything: this is\nnot a message it is expecting. Fortunately, if it knows who the current leader\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "448 CHAPTER 8. FAULT TOLERANCE\nis, in combination with a deterministic leader election, it could safely reject\naccept (o1, 1), knowing that by now S2has taken over. We conclude that the\nleader should include its ID in an accept message.\nWe have covered almost all cases and have thus far shown that Paxos\nbehaves correctly. Unfortunately, although being correct, the algorithm can\nstill come to a grinding halt. Consider the situation illustrated in Figure 8.12.\nWhat we are seeing here is that, because the learn messages returned by S3are\nlost, neither S1norS2will ever be able to know what S3actually executed: did\nit learn (and execute) accept (o1, 1)before or after learning accept (o2, 1), or\nperhaps it learned neither operation? A solution to this problem is discussed\nin Note 8.4.\nFigure 8.12: When Paxos can make no further progress.\nNote 8.4 (Advanced: Making progress in Paxos)\nUp to this point we have discussed the development of Paxos such that safety is\nensured. Safety essentially means that nothing bad will happen, or, put differently,\nthat the behavior of the algorithm is correct. To also ensure that eventually\nsomething good will happen, generally referred to as liveness of an algorithm, we\nneed to do a bit more. In particular, we need to get out of the situation sketched\nin Figure 8.12.\nThe real problem with this situation is that the servers have no consensus on\nwho the leader is. Once S2decides it should take over leadership, it needs to\nensure that any outstanding operations initiated by S1have been properly dealt\nwith. In other words, it needs to ensure that its own leadership is not hindered\nby operations that have not yet been completed by all nonfaulty processes. If\nleadership is taken over too quickly and a new operation is proposed, a previous\noperation that has been executed by at least one server may not get a chance to be\nexecuted by all servers \ufb01rst.\nTo this end, Paxos enforces an explicit leadership takeover, and this is where\nthe role of proposers come from. When a server crashes, the next one in line\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n448 CHAPTER 8. FAULT TOLERANCE\nis, in combination with a deterministic leader election, it could safely reject\naccept (o1, 1), knowing that by now S2has taken over. We conclude that the\nleader should include its ID in an accept message.\nWe have covered almost all cases and have thus far shown that Paxos\nbehaves correctly. Unfortunately, although being correct, the algorithm can\nstill come to a grinding halt. Consider the situation illustrated in Figure 8.12.\nWhat we are seeing here is that, because the learn messages returned by S3are\nlost, neither S1norS2will ever be able to know what S3actually executed: did\nit learn (and execute) accept (o1, 1)before or after learning accept (o2, 1), or\nperhaps it learned neither operation? A solution to this problem is discussed\nin Note 8.4.\nFigure 8.12: When Paxos can make no further progress.\nNote 8.4 (Advanced: Making progress in Paxos)\nUp to this point we have discussed the development of Paxos such that safety is\nensured. Safety essentially means that nothing bad will happen, or, put differently,\nthat the behavior of the algorithm is correct. To also ensure that eventually\nsomething good will happen, generally referred to as liveness of an algorithm, we\nneed to do a bit more. In particular, we need to get out of the situation sketched\nin Figure 8.12.\nThe real problem with this situation is that the servers have no consensus on\nwho the leader is. Once S2decides it should take over leadership, it needs to\nensure that any outstanding operations initiated by S1have been properly dealt\nwith. In other words, it needs to ensure that its own leadership is not hindered\nby operations that have not yet been completed by all nonfaulty processes. If\nleadership is taken over too quickly and a new operation is proposed, a previous\noperation that has been executed by at least one server may not get a chance to be\nexecuted by all servers \ufb01rst.\nTo this end, Paxos enforces an explicit leadership takeover, and this is where\nthe role of proposers come from. When a server crashes, the next one in line\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 449\nwill need to take over (recall that Paxos assumes a deterministic leader-election\nalgorithm), but also ensure that any outstanding operations are dealt with. This ex-\nplicit takeover is implemented by broadcasting a proposal message: propose (Si),\nwhere Siis the next server to be leader. When server Sjreceives this message,\nit replies with a promise (oj,tj)message, containing the most recently executed\noperation ojand its corresponding timestamp tj. Note that Siis particularly\ninterested in the most recent operation o\u0003executed by a majority of servers. By\n\u201cadopting\u201d this operation from the (apparently crashed) server S\u0003that had origi-\nnally proposed its acceptance, Sican effectively complete what S\u0003could not due\nto its failure.\nThere are two obvious optimizations to this procedure. The \ufb01rst one is not\nthat servers return the most recently executed operation, but the most recently\nlearned operation that is still waiting to be executed, if any. Furthermore, because\nit may be that the collection of servers have no more pending operations, Sican\nalso suggest a next operation oiwhen initially proposing to take over leadership,\ngiving rise to a propose (Si,oi)message. In essence, this is the situation we\ndescribed earlier, yet now it should be clear where the idea of proposals actually\ncomes from.\nWhen Sireceives a majority of promises for operation o\u0003, and the highest\nreturned timestamp is t\u0003, it broadcasts accept (Si,o\u0003,t\u0003), which is essentially a\nretransmission of the last operation proposed before Sitook over leadership. If no\nsuch o\u0003exists, Siwill propose to accept its own operation oi.\nConsensus in faulty systems with arbitrary failures\nSo far, we assumed that replicas were only subject to crash failures, in which\ncase a process group needs to consist of 2k+1servers to survive kcrashed\nmembers. An important assumption in these cases is that a process does\nnot collude with another process, or, more speci\ufb01cally, is consistent in its\nmessages to others. The situations shown in Figure 8.13 should not happen.\nIn the \ufb01rst case, we see that process P2is forwarding a different value or\noperation than it is supposed to. Referring to Paxos, this could mean that a\nprimary tells the backups that not operation ohad been accepted, but instead\npropagates a different operation o0. In the second case, P1is telling different\nthings to different processes, such as having a leader sending operation oto\nsome backups, and at the same time operation o0to others. Again, we note\nthat this need not be malicious actions, but simply omission or commission\nfailures.\nIn this section, we take a look at reaching consensus in a fault-tolerant\nprocess group in which kmembers can fail assuming arbitrary failures. In\nparticular, we will show that we need at least 3k+1members to reach\nconsensus under these failure assumptions.\nConsider a process group consisting of nmembers, of which one has been\ndesignated to be the primary ,P, and the remaining n\u00001to be the backups\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 449\nwill need to take over (recall that Paxos assumes a deterministic leader-election\nalgorithm), but also ensure that any outstanding operations are dealt with. This ex-\nplicit takeover is implemented by broadcasting a proposal message: propose (Si),\nwhere Siis the next server to be leader. When server Sjreceives this message,\nit replies with a promise (oj,tj)message, containing the most recently executed\noperation ojand its corresponding timestamp tj. Note that Siis particularly\ninterested in the most recent operation o\u0003executed by a majority of servers. By\n\u201cadopting\u201d this operation from the (apparently crashed) server S\u0003that had origi-\nnally proposed its acceptance, Sican effectively complete what S\u0003could not due\nto its failure.\nThere are two obvious optimizations to this procedure. The \ufb01rst one is not\nthat servers return the most recently executed operation, but the most recently\nlearned operation that is still waiting to be executed, if any. Furthermore, because\nit may be that the collection of servers have no more pending operations, Sican\nalso suggest a next operation oiwhen initially proposing to take over leadership,\ngiving rise to a propose (Si,oi)message. In essence, this is the situation we\ndescribed earlier, yet now it should be clear where the idea of proposals actually\ncomes from.\nWhen Sireceives a majority of promises for operation o\u0003, and the highest\nreturned timestamp is t\u0003, it broadcasts accept (Si,o\u0003,t\u0003), which is essentially a\nretransmission of the last operation proposed before Sitook over leadership. If no\nsuch o\u0003exists, Siwill propose to accept its own operation oi.\nConsensus in faulty systems with arbitrary failures\nSo far, we assumed that replicas were only subject to crash failures, in which\ncase a process group needs to consist of 2k+1servers to survive kcrashed\nmembers. An important assumption in these cases is that a process does\nnot collude with another process, or, more speci\ufb01cally, is consistent in its\nmessages to others. The situations shown in Figure 8.13 should not happen.\nIn the \ufb01rst case, we see that process P2is forwarding a different value or\noperation than it is supposed to. Referring to Paxos, this could mean that a\nprimary tells the backups that not operation ohad been accepted, but instead\npropagates a different operation o0. In the second case, P1is telling different\nthings to different processes, such as having a leader sending operation oto\nsome backups, and at the same time operation o0to others. Again, we note\nthat this need not be malicious actions, but simply omission or commission\nfailures.\nIn this section, we take a look at reaching consensus in a fault-tolerant\nprocess group in which kmembers can fail assuming arbitrary failures. In\nparticular, we will show that we need at least 3k+1members to reach\nconsensus under these failure assumptions.\nConsider a process group consisting of nmembers, of which one has been\ndesignated to be the primary ,P, and the remaining n\u00001to be the backups\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "450 CHAPTER 8. FAULT TOLERANCE\n(a) (b)\nFigure 8.13: A process in a replicated group acting inconsistently: (a) not\nforwarding properly, and (b) telling different things to different processes.\nB1, . . . , Bn\u00001. We make the following assumptions:\n\u2022A client sends a value v2fT,Fgto the primary, where vstands for\neither true orfalse.\n\u2022 Messages may be lost, but this can be detected.\n\u2022Messages cannot be corrupted without that being detected (and thus\nsubsequently ignored).\n\u2022 A receiver of a message can reliably detect its sender.\nIn order to achieve what is known as Byzantine agreement , we need to satisfy\nthe following two requirements:\nBA1: Every nonfaulty backup process stores the same value.\nBA2: If the primary is nonfaulty then every nonfaulty backup process stores\nexactly what the primary had sent.\nNote that if the primary is faulty, BA1 tells us that the backups may store\nthe same, but different (and thus wrong) value than the one initially sent by\nthe client. Furthermore, it should be clear that if the primary is not faulty,\nsatisfying BA2 implies that BA1 is also satis\ufb01ed.\nWhy having 3k processes is not enough\nTo see why having only 3kprocesses is not enough to reach consensus, let\nus consider the situation in which we want to tolerate the failure of a single\nprocess, that is, k=1. Consider Figure 8.14, which is essentially an extension\nof Figure 8.13.\nIn Figure 8.14(a), we see that the faulty primary Pis sending two different\nvalues to the backups B1and B2, respectively. In order to reach consensus,\nboth backup processes forward the received value to the other, leading to\na second round of message exchanges. At that point, B1and B2each have\nreceived the set of values fT,Fg, from which it is impossible to draw a\nconclusion.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n450 CHAPTER 8. FAULT TOLERANCE\n(a) (b)\nFigure 8.13: A process in a replicated group acting inconsistently: (a) not\nforwarding properly, and (b) telling different things to different processes.\nB1, . . . , Bn\u00001. We make the following assumptions:\n\u2022A client sends a value v2fT,Fgto the primary, where vstands for\neither true orfalse.\n\u2022 Messages may be lost, but this can be detected.\n\u2022Messages cannot be corrupted without that being detected (and thus\nsubsequently ignored).\n\u2022 A receiver of a message can reliably detect its sender.\nIn order to achieve what is known as Byzantine agreement , we need to satisfy\nthe following two requirements:\nBA1: Every nonfaulty backup process stores the same value.\nBA2: If the primary is nonfaulty then every nonfaulty backup process stores\nexactly what the primary had sent.\nNote that if the primary is faulty, BA1 tells us that the backups may store\nthe same, but different (and thus wrong) value than the one initially sent by\nthe client. Furthermore, it should be clear that if the primary is not faulty,\nsatisfying BA2 implies that BA1 is also satis\ufb01ed.\nWhy having 3k processes is not enough\nTo see why having only 3kprocesses is not enough to reach consensus, let\nus consider the situation in which we want to tolerate the failure of a single\nprocess, that is, k=1. Consider Figure 8.14, which is essentially an extension\nof Figure 8.13.\nIn Figure 8.14(a), we see that the faulty primary Pis sending two different\nvalues to the backups B1and B2, respectively. In order to reach consensus,\nboth backup processes forward the received value to the other, leading to\na second round of message exchanges. At that point, B1and B2each have\nreceived the set of values fT,Fg, from which it is impossible to draw a\nconclusion.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 451\n(a) (b)\nFigure 8.14: Impossibility to reach consensus with 3 processes and trying to\ntolerate a single arbitrarily failing process.\nLikewise, we cannot reach consensus when wrong values are forwarded.\nIn Figure 8.14(b), the primary Pand backup B2operate correctly, but B1is\nnot. Instead of forwarding the value Tto process B2, it sends the incorrect\nvalue F. The result is that B2will now have seen the set of values fT,Fgfrom\nwhich it cannot draw any conclusions. In other words, Pand B2cannot reach\nconsensus. More speci\ufb01cally, B2is not able to decide what to store, so that we\ncannot satisfy requirement BA2.\nNote 8.5 (Advanced: The case where k>1and n\u00143k)\nGeneralizing this situation for other values of kis not that dif\ufb01cult. As explained\nby Kshemkalyani and Singhal [2008], we can use a simple reduction scheme.\nAssume that there is a solution for the case where k\u00151and n\u00143k. Partition the\nnprocesses Q1,. . .,Qninto three disjoint sets S1,S2, and S3, together containing\nall processes. Moreover, let each set Skhave less or equal than n/3members.\nFormally, this means that\n\u2022S1\\S2=S1\\S3=S2\\S3=\u00c6\n\u2022S1[S2[S3=fQ1, . . . , Qng\n\u2022 for each Si,jSij\u0014n/3\nNow consider a situation in which three processes Q\u0003\n1,Q\u0003\n2, and Q\u0003\n3simulate the\nactions that take place in and between the processes of S1,S2, and S3, respectively.\nIn other words, if a process in S1sends a message to another process in S2, then\nQ\u0003\n1will send a same message to Q2. The same holds for process communication\nwithin a group. Assume that Q\u0003\n1is faulty, yet Q\u0003\n2and Q\u0003\n3are not. All processes\nsimulated by Q\u0003\n1are now assumed to be faulty, and will thus lead to incorrect\nmessages being sent to Q\u0003\n2and Q\u0003\n3, respectively. Not so for Q\u0003\n2(and Q\u0003\n3): all\nmessages coming from processes in S2(and S3, respectively) are assumed to be\ncorrect. Because n\u00143kand for each set Siwe have thatjSij\u0014n/3, at most n/3\nof the simulated processes Q1,. . .,Qnare faulty. In other words, we are satisfying\nthe condition for which we assumed that there would be a general solution.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 451\n(a) (b)\nFigure 8.14: Impossibility to reach consensus with 3 processes and trying to\ntolerate a single arbitrarily failing process.\nLikewise, we cannot reach consensus when wrong values are forwarded.\nIn Figure 8.14(b), the primary Pand backup B2operate correctly, but B1is\nnot. Instead of forwarding the value Tto process B2, it sends the incorrect\nvalue F. The result is that B2will now have seen the set of values fT,Fgfrom\nwhich it cannot draw any conclusions. In other words, Pand B2cannot reach\nconsensus. More speci\ufb01cally, B2is not able to decide what to store, so that we\ncannot satisfy requirement BA2.\nNote 8.5 (Advanced: The case where k>1and n\u00143k)\nGeneralizing this situation for other values of kis not that dif\ufb01cult. As explained\nby Kshemkalyani and Singhal [2008], we can use a simple reduction scheme.\nAssume that there is a solution for the case where k\u00151and n\u00143k. Partition the\nnprocesses Q1,. . .,Qninto three disjoint sets S1,S2, and S3, together containing\nall processes. Moreover, let each set Skhave less or equal than n/3members.\nFormally, this means that\n\u2022S1\\S2=S1\\S3=S2\\S3=\u00c6\n\u2022S1[S2[S3=fQ1, . . . , Qng\n\u2022 for each Si,jSij\u0014n/3\nNow consider a situation in which three processes Q\u0003\n1,Q\u0003\n2, and Q\u0003\n3simulate the\nactions that take place in and between the processes of S1,S2, and S3, respectively.\nIn other words, if a process in S1sends a message to another process in S2, then\nQ\u0003\n1will send a same message to Q2. The same holds for process communication\nwithin a group. Assume that Q\u0003\n1is faulty, yet Q\u0003\n2and Q\u0003\n3are not. All processes\nsimulated by Q\u0003\n1are now assumed to be faulty, and will thus lead to incorrect\nmessages being sent to Q\u0003\n2and Q\u0003\n3, respectively. Not so for Q\u0003\n2(and Q\u0003\n3): all\nmessages coming from processes in S2(and S3, respectively) are assumed to be\ncorrect. Because n\u00143kand for each set Siwe have thatjSij\u0014n/3, at most n/3\nof the simulated processes Q1,. . .,Qnare faulty. In other words, we are satisfying\nthe condition for which we assumed that there would be a general solution.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "452 CHAPTER 8. FAULT TOLERANCE\n(a)\n(b)\nFigure 8.15: Reaching consensus with four processes of which one may fail\narbitrarily.\nWe can now come to a contradiction, for if there would exist a solution for\nthe general case, then the processes Q\u0003\n1,Q\u0003\n2, and Q\u0003\n3could simulate this solution,\nwhich would then also be a solution for the special case that n=3and k=1. Yet\nwe just proved that this cannot be so, leading to a contradiction. We conclude that\nour assumption that there is a general solution for k\u00151 and n\u00143kis false.\nWhy having 3k +1 processes is enough\nLet us now focus on the case in which we have a group of 3k+1processes.\nOur goal is to show that we can establish a solution in which kgroup members\nmay suffer from fail-arbitrary failures, yet the remaining nonfaulty processes\nwill still reach consensus. Again, we \ufb01rst concentrate on the case n=4,k=1.\nConsider Figure 8.15, which shows a situation with one primary Pand three\nbackup processes B1,B2, and B3.\nIn Figure 8.15(a) we have sketched the situation in which the primary Pis\nfaulty and is providing inconsistent information to its backups. In our solution,\nthe processes will forward what they receive to the others. During the \ufb01rst\nround, Psends TtoB1,FtoB2, and TtoB3, respectively. Each of the backups\nthen sends what they have to the others. With only the primary failing, this\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n452 CHAPTER 8. FAULT TOLERANCE\n(a)\n(b)\nFigure 8.15: Reaching consensus with four processes of which one may fail\narbitrarily.\nWe can now come to a contradiction, for if there would exist a solution for\nthe general case, then the processes Q\u0003\n1,Q\u0003\n2, and Q\u0003\n3could simulate this solution,\nwhich would then also be a solution for the special case that n=3and k=1. Yet\nwe just proved that this cannot be so, leading to a contradiction. We conclude that\nour assumption that there is a general solution for k\u00151 and n\u00143kis false.\nWhy having 3k +1 processes is enough\nLet us now focus on the case in which we have a group of 3k+1processes.\nOur goal is to show that we can establish a solution in which kgroup members\nmay suffer from fail-arbitrary failures, yet the remaining nonfaulty processes\nwill still reach consensus. Again, we \ufb01rst concentrate on the case n=4,k=1.\nConsider Figure 8.15, which shows a situation with one primary Pand three\nbackup processes B1,B2, and B3.\nIn Figure 8.15(a) we have sketched the situation in which the primary Pis\nfaulty and is providing inconsistent information to its backups. In our solution,\nthe processes will forward what they receive to the others. During the \ufb01rst\nround, Psends TtoB1,FtoB2, and TtoB3, respectively. Each of the backups\nthen sends what they have to the others. With only the primary failing, this\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 453\nmeans that after two rounds, each of the backups will have received the set of\nvaluesfT,T,Fg, meaning that they can reach consensus on the value T.\nWhen we consider the case that one of the backups fails, we get the\nsituation sketched in Figure 8.15(b). Assume that the (nonfaulty) primary\nsends Tto all the backups, yet B2is faulty. Where B1and B3will send out T\nto the other backups in a second round, the worst that B2may do is send out\nF, as shown in the \ufb01gure. Despite this failure, B1and B3will come to the same\nconclusion, namely that Phad sent out T, thereby meeting our requirement\nBA2 as stated before.\nNote 8.6 (Advanced: The case where k>1and n=3k+1)\nAs a sketch toward a general solution, consider the more intricate case in which\nn=7and k=2. We let the primary Psend out a value v0. Using a similar\nnotation as found in [Kshemkalyani and Singhal, 2008], we proceed as follows.\nNote that we effectively use the index 0 to denote the primary P(think of Pbeing\nequal to a special backup process B0).\n1.We let Psend v0to the six backups. Backup Bistores the received value\nasvi,0hi. This notation indicates that the value was received by process Bi,\nthat it was sent by P=B0, and that the value of v0was directly sent to Bi\nand not through another process (using the notation hi). To make matters\nconcrete, consider B4, which will store v0inv4,0hi.\n2.Each backup Bi, in turn, will send vi,0hito every one of the other \ufb01ve\nbackups, which is stored by Bjasvj,ih0i. This notation indicates that the\nvalue is stored at Bj, was sent by Bi, but that it originated from P=B0\n(through the notation h0i). Concentrating on B4, it eventually stores v4,5h0i,\nwhich is the value v5,0hisent by B5toB4.\n3.Suppose that Binow has the value vi,jh0i. Again, it will send out this value\nto all processes except P=B0,Bj, and Bi(i.e., itself). If Bkreceives vi,jh0i\nfrom Bi, it stores this received value in vk,ihj, 0i. Indeed, by then v0will\nhave traveled the path P!Bj!Bi!Bk. For example, B2may send out\nv2,1h0itoB4, who will store it as v4,2h1, 0i. Note at this point, that B4can\nsend out thisvalue only to processes B3,B5, and B6. There is no use in\nsending it out to other processes.\n4.Continuing this line of thought, assume that Bihas value vi,jhk, 0i, which\nit sends out to the three remaining processes not equal to P=B0,Bk,Bj,\nand Bi(itself). Returning to B4, eventually, B4will also receive a similar\nmessage from, say, B2, leading perhaps to a value v4,2h3, 1, 0i. That value\ncan be sent only to B5andB6, after which only a single round is left.\nOnce all these messages have been sent, each backup can start moving out of the\nrecursion again. Let us look at process B4again. From the last round, it will have\nstored a total of \ufb01ve messages:\n\u2022v4,1h6, 5, 3, 2, 0i\n\u2022v4,2h6, 5, 3, 1, 0i\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 453\nmeans that after two rounds, each of the backups will have received the set of\nvaluesfT,T,Fg, meaning that they can reach consensus on the value T.\nWhen we consider the case that one of the backups fails, we get the\nsituation sketched in Figure 8.15(b). Assume that the (nonfaulty) primary\nsends Tto all the backups, yet B2is faulty. Where B1and B3will send out T\nto the other backups in a second round, the worst that B2may do is send out\nF, as shown in the \ufb01gure. Despite this failure, B1and B3will come to the same\nconclusion, namely that Phad sent out T, thereby meeting our requirement\nBA2 as stated before.\nNote 8.6 (Advanced: The case where k>1and n=3k+1)\nAs a sketch toward a general solution, consider the more intricate case in which\nn=7and k=2. We let the primary Psend out a value v0. Using a similar\nnotation as found in [Kshemkalyani and Singhal, 2008], we proceed as follows.\nNote that we effectively use the index 0 to denote the primary P(think of Pbeing\nequal to a special backup process B0).\n1.We let Psend v0to the six backups. Backup Bistores the received value\nasvi,0hi. This notation indicates that the value was received by process Bi,\nthat it was sent by P=B0, and that the value of v0was directly sent to Bi\nand not through another process (using the notation hi). To make matters\nconcrete, consider B4, which will store v0inv4,0hi.\n2.Each backup Bi, in turn, will send vi,0hito every one of the other \ufb01ve\nbackups, which is stored by Bjasvj,ih0i. This notation indicates that the\nvalue is stored at Bj, was sent by Bi, but that it originated from P=B0\n(through the notation h0i). Concentrating on B4, it eventually stores v4,5h0i,\nwhich is the value v5,0hisent by B5toB4.\n3.Suppose that Binow has the value vi,jh0i. Again, it will send out this value\nto all processes except P=B0,Bj, and Bi(i.e., itself). If Bkreceives vi,jh0i\nfrom Bi, it stores this received value in vk,ihj, 0i. Indeed, by then v0will\nhave traveled the path P!Bj!Bi!Bk. For example, B2may send out\nv2,1h0itoB4, who will store it as v4,2h1, 0i. Note at this point, that B4can\nsend out thisvalue only to processes B3,B5, and B6. There is no use in\nsending it out to other processes.\n4.Continuing this line of thought, assume that Bihas value vi,jhk, 0i, which\nit sends out to the three remaining processes not equal to P=B0,Bk,Bj,\nand Bi(itself). Returning to B4, eventually, B4will also receive a similar\nmessage from, say, B2, leading perhaps to a value v4,2h3, 1, 0i. That value\ncan be sent only to B5andB6, after which only a single round is left.\nOnce all these messages have been sent, each backup can start moving out of the\nrecursion again. Let us look at process B4again. From the last round, it will have\nstored a total of \ufb01ve messages:\n\u2022v4,1h6, 5, 3, 2, 0i\n\u2022v4,2h6, 5, 3, 1, 0i\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "454 CHAPTER 8. FAULT TOLERANCE\n\u2022v4,3h6, 5, 2, 1, 0i\n\u2022v4,5h6, 3, 2, 1, 0i\n\u2022v4,6h5, 3, 2, 1, 0i\nWith these \ufb01ve values, it can start computing estimates ofv0, that is, a value that\nit believes v0should be. To this end, we assume that each (nonfaulty) process\nexecutes the same procedure majority ()that selects a unique value from a given\nset of inputs. In practice, this will be the majority among the input set. If there is\nno majority, a default value is chosen. To give a few examples:\nw4,1h5, 3, 2, 0i  majority (v4,1h5, 3, 2, 0i,v4,1h6, 5, 3, 2, 0i)\nw4,1h6, 3, 2, 0i  majority (v4,1h6, 3, 2, 0i,v4,2h6, 5, 3, 1, 0i)\nw4,5h3, 2, 1, 0i  majority (v4,5h3, 2, 1, 0i,v4,5h6, 3, 2, 1, 0i)\nw4,5h6, 3, 2, 0i  majority (v4,5h6, 3, 2, 0i,v4,5h6, 3, 2, 1, 0i)\nw4,6h3, 2, 1, 0i  majority (v4,6h3, 2, 1, 0i,v4,6h5, 3, 2, 1, 0i)\nw4,6h5, 3, 2, 0i  majority (v4,6h5, 3, 2, 0i,v4,1h6, 5, 3, 2, 0i)\nIn turn, allowing it to compute estimates like:\nw4,1h3, 2, 0i  majority (v4,1h3, 2, 0i,w4,5h3, 2, 1, 0i,w4,6h3, 2, 1, 0i\nw4,5h3, 2, 0i  majority (v4,5h3, 2, 0i,w4,1h5, 3, 2, 0i,w4,6h5, 3, 2, 0i\nw4,6h3, 2, 0i  majority (v4,6h3, 2, 0i,w4,1h6, 3, 2, 0i,w4,5h6, 3, 2, 0i\nAnd from there, for example\nw4,3h2, 0i  majority (v4,3h2, 0i,w4,1h3, 2, 0i,w4,5h3, 2, 0i,w4,6h3, 2, 0i)\nThis process continues until, eventually, B4will be able to execute\nw4,0hi majority (v4,0hi,w4,1h0i,w4,2h0i,w4,3h0i,w4,5h0i,w4,6h0i)\nand reach the \ufb01nal outcome.\nLet us now see why this scheme actually works. We denote by BAP(n,k)\nthe above sketched protocol to reach consensus. BAP(n,k) starts by having the\nprimary send out its value v0to the n\u00001backups. In the case the primary\noperates correctly, each of the backups will indeed receive v0. If the primary is\nnonfaulty, some backups receive v0while others receive v0(i.e., the opposite of\nv0).\nBecause we assume a backup Bicannot know whether or not the primary is\nworking correctly, it will have to check with the other backups. We therefore let\nBirun the protocol again, but in this case with value vi,0hiand with a smaller\nprocess group, namely fB1,. . .,Bi\u00001,Bi+1,. . .,Bng. In other words, Biexecutes\nBAP(n-1,k-1) with a total of n\u00002other processes. Note that at this point there\naren\u00001 instances of BAP(n-1,k-1) being executed in parallel.\nIn the end, we see that these executions result in each backup Bitaking the\nmajority of n\u00001 values:\n\u2022 One value comes from the primary: vi,0hi\n\u2022n\u00002values come from the other backups, in particular, Biis dealing with\nthe values vi,1hi, . . . , vi,i\u00001hi,vi,i+1hi, . . . , vi,n\u00001hi.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n454 CHAPTER 8. FAULT TOLERANCE\n\u2022v4,3h6, 5, 2, 1, 0i\n\u2022v4,5h6, 3, 2, 1, 0i\n\u2022v4,6h5, 3, 2, 1, 0i\nWith these \ufb01ve values, it can start computing estimates ofv0, that is, a value that\nit believes v0should be. To this end, we assume that each (nonfaulty) process\nexecutes the same procedure majority ()that selects a unique value from a given\nset of inputs. In practice, this will be the majority among the input set. If there is\nno majority, a default value is chosen. To give a few examples:\nw4,1h5, 3, 2, 0i  majority (v4,1h5, 3, 2, 0i,v4,1h6, 5, 3, 2, 0i)\nw4,1h6, 3, 2, 0i  majority (v4,1h6, 3, 2, 0i,v4,2h6, 5, 3, 1, 0i)\nw4,5h3, 2, 1, 0i  majority (v4,5h3, 2, 1, 0i,v4,5h6, 3, 2, 1, 0i)\nw4,5h6, 3, 2, 0i  majority (v4,5h6, 3, 2, 0i,v4,5h6, 3, 2, 1, 0i)\nw4,6h3, 2, 1, 0i  majority (v4,6h3, 2, 1, 0i,v4,6h5, 3, 2, 1, 0i)\nw4,6h5, 3, 2, 0i  majority (v4,6h5, 3, 2, 0i,v4,1h6, 5, 3, 2, 0i)\nIn turn, allowing it to compute estimates like:\nw4,1h3, 2, 0i  majority (v4,1h3, 2, 0i,w4,5h3, 2, 1, 0i,w4,6h3, 2, 1, 0i\nw4,5h3, 2, 0i  majority (v4,5h3, 2, 0i,w4,1h5, 3, 2, 0i,w4,6h5, 3, 2, 0i\nw4,6h3, 2, 0i  majority (v4,6h3, 2, 0i,w4,1h6, 3, 2, 0i,w4,5h6, 3, 2, 0i\nAnd from there, for example\nw4,3h2, 0i  majority (v4,3h2, 0i,w4,1h3, 2, 0i,w4,5h3, 2, 0i,w4,6h3, 2, 0i)\nThis process continues until, eventually, B4will be able to execute\nw4,0hi majority (v4,0hi,w4,1h0i,w4,2h0i,w4,3h0i,w4,5h0i,w4,6h0i)\nand reach the \ufb01nal outcome.\nLet us now see why this scheme actually works. We denote by BAP(n,k)\nthe above sketched protocol to reach consensus. BAP(n,k) starts by having the\nprimary send out its value v0to the n\u00001backups. In the case the primary\noperates correctly, each of the backups will indeed receive v0. If the primary is\nnonfaulty, some backups receive v0while others receive v0(i.e., the opposite of\nv0).\nBecause we assume a backup Bicannot know whether or not the primary is\nworking correctly, it will have to check with the other backups. We therefore let\nBirun the protocol again, but in this case with value vi,0hiand with a smaller\nprocess group, namely fB1,. . .,Bi\u00001,Bi+1,. . .,Bng. In other words, Biexecutes\nBAP(n-1,k-1) with a total of n\u00002other processes. Note that at this point there\naren\u00001 instances of BAP(n-1,k-1) being executed in parallel.\nIn the end, we see that these executions result in each backup Bitaking the\nmajority of n\u00001 values:\n\u2022 One value comes from the primary: vi,0hi\n\u2022n\u00002values come from the other backups, in particular, Biis dealing with\nthe values vi,1hi, . . . , vi,i\u00001hi,vi,i+1hi, . . . , vi,n\u00001hi.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 455\nHowever, because Bicannot trust a received value vi,jhi, it will have to check that\nvalue with the other n\u00002backups: B1,. . .,Bi\u00001,Bi+1,. . .,Bj\u00001,Bj+1,. . .,Bn\u00001.\nThis leads to the execution of BAP(n-2,k-2) , of which a total of n\u00002instances\nwill be running in parallel. This story continues, and eventually, a backup process\nwill need to run BAP(n-k,0) , which simply returns the value sent by the primary,\nafter which we can move up the recursion as described above.\nNote 8.7 (Advanced: Correctness of the Byzantine agreement protocol)\nWith the general scheme given in Note 8.6, it is not that dif\ufb01cult to see why the\nprotocol is correct. Following Koren and Krishna [2007], we use induction on kto\nprove that BAP(n,k) meets the requirements BA1 and BA2 for n\u00153k+1and for\nallk\u00150.\nFirst, consider the case k=0. In other words, we assume that there are no\nfaulty processes. In that case, whatever the primary sends to the backups, that\nvalue will be consistently propagated throughout the system, and no other value\nwill ever pop up. In other words, for any n,BAP(n,0) is correct. Now consider\nthe case k>0.\nFirst, consider the case that the primary is operating correctly. Without loss of\ngenerality, we can assume the primary sends out T. All the backups receive the\nsame value, namely T. Each backup will then run BAP(n-1,k-1) . By induction,\nwe know that each of these instances will be executed correctly. This means that\nfor any nonfaulty backup B, all the other nonfaulty backups will store the value\nthat was sent by B, namely T. Each nonfaulty backup receives, in total, n\u00001\nvalues, of which n\u00002come from other backups. Of those n\u00002, at most kvalues\nmay be wrong (i.e., F). With k\u0014(n\u00001)/3, this means that every nonfaulty\nbackup receives at least 1+ (n\u00002)\u0000(n\u00001)/3= (2n\u00002)/3values T. Because\n(2n\u00002)/3>n/3for all n>2, this means that every nonfaulty backup can take\na correct majority vote on the total number of received values, thus satisfying\nrequirement BA2.\nLet us now consider the case that the primary is faulty, meaning that at most\nk\u00001backups may operate incorrectly as well. The primary is assumed to send\nout any value it likes. There are a total of n\u00001backups, of which at most k\u00001\nare faulty. Each backup runs BAP(n-1,k-1) and by, induction, each one of these\ninstances is executed correctly. In particular, for every nonfaulty backup B, all\nthe other nonfaulty backups will vote for the value sent by B. This means that\nall nonfaulty backups will have the same vector of n\u00002results from their fellow\nbackups. Any difference between two nonfaulty backups can be caused only by\nthe fact that the primary sent something else to each of them. As a result, when\napplying majority ()to those complete vectors, the result for each backup will be\nthe same, so that requirement BA1 is met.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 455\nHowever, because Bicannot trust a received value vi,jhi, it will have to check that\nvalue with the other n\u00002backups: B1,. . .,Bi\u00001,Bi+1,. . .,Bj\u00001,Bj+1,. . .,Bn\u00001.\nThis leads to the execution of BAP(n-2,k-2) , of which a total of n\u00002instances\nwill be running in parallel. This story continues, and eventually, a backup process\nwill need to run BAP(n-k,0) , which simply returns the value sent by the primary,\nafter which we can move up the recursion as described above.\nNote 8.7 (Advanced: Correctness of the Byzantine agreement protocol)\nWith the general scheme given in Note 8.6, it is not that dif\ufb01cult to see why the\nprotocol is correct. Following Koren and Krishna [2007], we use induction on kto\nprove that BAP(n,k) meets the requirements BA1 and BA2 for n\u00153k+1and for\nallk\u00150.\nFirst, consider the case k=0. In other words, we assume that there are no\nfaulty processes. In that case, whatever the primary sends to the backups, that\nvalue will be consistently propagated throughout the system, and no other value\nwill ever pop up. In other words, for any n,BAP(n,0) is correct. Now consider\nthe case k>0.\nFirst, consider the case that the primary is operating correctly. Without loss of\ngenerality, we can assume the primary sends out T. All the backups receive the\nsame value, namely T. Each backup will then run BAP(n-1,k-1) . By induction,\nwe know that each of these instances will be executed correctly. This means that\nfor any nonfaulty backup B, all the other nonfaulty backups will store the value\nthat was sent by B, namely T. Each nonfaulty backup receives, in total, n\u00001\nvalues, of which n\u00002come from other backups. Of those n\u00002, at most kvalues\nmay be wrong (i.e., F). With k\u0014(n\u00001)/3, this means that every nonfaulty\nbackup receives at least 1+ (n\u00002)\u0000(n\u00001)/3= (2n\u00002)/3values T. Because\n(2n\u00002)/3>n/3for all n>2, this means that every nonfaulty backup can take\na correct majority vote on the total number of received values, thus satisfying\nrequirement BA2.\nLet us now consider the case that the primary is faulty, meaning that at most\nk\u00001backups may operate incorrectly as well. The primary is assumed to send\nout any value it likes. There are a total of n\u00001backups, of which at most k\u00001\nare faulty. Each backup runs BAP(n-1,k-1) and by, induction, each one of these\ninstances is executed correctly. In particular, for every nonfaulty backup B, all\nthe other nonfaulty backups will vote for the value sent by B. This means that\nall nonfaulty backups will have the same vector of n\u00002results from their fellow\nbackups. Any difference between two nonfaulty backups can be caused only by\nthe fact that the primary sent something else to each of them. As a result, when\napplying majority ()to those complete vectors, the result for each backup will be\nthe same, so that requirement BA1 is met.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "456 CHAPTER 8. FAULT TOLERANCE\nExample: Practical Byzantine Fault Tolerance\nByzantine fault tolerance was for long more or less an exotic topic, partly be-\ncause it turned out that combining safety, liveness, andpractical performance\nwas dif\ufb01cult to achieve. It was around 2000 that Barbara Liskov and her\nstudent at that time, Miguel Castro, managed to come up with a practical im-\nplementation of a protocol for replicating servers that could handle arbitrary\nfailures. Let us brie\ufb02y take a look at their solution, which has been coined\nPractical Byzantine Fault Tolerance , or simply PBFT [Castro and Liskov,\n2002].\nLike Paxos, PBFT makes only a few assumptions about its environment. It\nmakes no assumptions about the behavior of replica servers: a faulty server\nis assumed to exhibit arbitrary behavior. Likewise, messages may be lost,\ndelayed, and received out of order. However, a message\u2019s sender is assumed\nto be identi\ufb01able (which is achieved by having messages signed, as we will\ndiscuss in Chapter 9). Under these assumptions, and as long as no more than\nkservers fail, it can be proven that PBFT is safe, meaning that a client will\nalways receive a correct answer. If we can additionally assume synchrony,\nmeaning that message delays and response times are bounded, it also provides\nliveness . In practice, this means that PBFT assumes a partially synchronous\nmodel, in which unbounded delays are an exception, for example caused by\nan attack.\nTo understand the algorithm, let us take a step back and partly review\nwhat we have discussed so far on establishing a k-fault-tolerant process group.\nAn essential issue is that such a group behaves as a single, central server. As\na consequence, under the assumption of having only crash failures, when a\nclient sends a request, it should expect k+1identical answers. If a server had\ncrashed, fewer responses would be returned, but they would be the same.\nThe \ufb01rst problem that we need to solve is that concurrent requests are all\nhandled in the same order. To this end, PBFT adopts a primary-backup model\nwith a total of 3k+1replica servers. To keep matters simple, let us assume for\nnow that the primary is nonfaulty. In that case, a client Csends a request to\nexecute operation oto the primary (denoted as Pin Figure 8.16). The primary\nhas a notion of the current collection of nonfaulty replica servers, expressed\nin terms of a view v, which is simply a number. The primary assigns a\ntimestamp ttoo, which is then incremented to be used for a subsequent\nrequest. The primary subsequently sends a (signed) pre-prepare message\npre-prepare (t,v,o)to the backups.\nA (nonfaulty) backup will accept to pre-prepare if it is in vand has never\naccepted an operation with timestamp tinvbefore. Each backup that accepts\ntopre-prepare sends an (again signed) message prepare (t,v,o)to the others,\nincluding the primary. A key observation is that when a nonfaulty replica\nserver Shas received 2kmessages prepare (t,v,o)that all match the pre-prepare\nmessage Sitself received by the primary (i.e., all have the same value for t,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n456 CHAPTER 8. FAULT TOLERANCE\nExample: Practical Byzantine Fault Tolerance\nByzantine fault tolerance was for long more or less an exotic topic, partly be-\ncause it turned out that combining safety, liveness, andpractical performance\nwas dif\ufb01cult to achieve. It was around 2000 that Barbara Liskov and her\nstudent at that time, Miguel Castro, managed to come up with a practical im-\nplementation of a protocol for replicating servers that could handle arbitrary\nfailures. Let us brie\ufb02y take a look at their solution, which has been coined\nPractical Byzantine Fault Tolerance , or simply PBFT [Castro and Liskov,\n2002].\nLike Paxos, PBFT makes only a few assumptions about its environment. It\nmakes no assumptions about the behavior of replica servers: a faulty server\nis assumed to exhibit arbitrary behavior. Likewise, messages may be lost,\ndelayed, and received out of order. However, a message\u2019s sender is assumed\nto be identi\ufb01able (which is achieved by having messages signed, as we will\ndiscuss in Chapter 9). Under these assumptions, and as long as no more than\nkservers fail, it can be proven that PBFT is safe, meaning that a client will\nalways receive a correct answer. If we can additionally assume synchrony,\nmeaning that message delays and response times are bounded, it also provides\nliveness . In practice, this means that PBFT assumes a partially synchronous\nmodel, in which unbounded delays are an exception, for example caused by\nan attack.\nTo understand the algorithm, let us take a step back and partly review\nwhat we have discussed so far on establishing a k-fault-tolerant process group.\nAn essential issue is that such a group behaves as a single, central server. As\na consequence, under the assumption of having only crash failures, when a\nclient sends a request, it should expect k+1identical answers. If a server had\ncrashed, fewer responses would be returned, but they would be the same.\nThe \ufb01rst problem that we need to solve is that concurrent requests are all\nhandled in the same order. To this end, PBFT adopts a primary-backup model\nwith a total of 3k+1replica servers. To keep matters simple, let us assume for\nnow that the primary is nonfaulty. In that case, a client Csends a request to\nexecute operation oto the primary (denoted as Pin Figure 8.16). The primary\nhas a notion of the current collection of nonfaulty replica servers, expressed\nin terms of a view v, which is simply a number. The primary assigns a\ntimestamp ttoo, which is then incremented to be used for a subsequent\nrequest. The primary subsequently sends a (signed) pre-prepare message\npre-prepare (t,v,o)to the backups.\nA (nonfaulty) backup will accept to pre-prepare if it is in vand has never\naccepted an operation with timestamp tinvbefore. Each backup that accepts\ntopre-prepare sends an (again signed) message prepare (t,v,o)to the others,\nincluding the primary. A key observation is that when a nonfaulty replica\nserver Shas received 2kmessages prepare (t,v,o)that all match the pre-prepare\nmessage Sitself received by the primary (i.e., all have the same value for t,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 457\nFigure 8.16: The different phases in PBFT. Cis the client, Pis the primary,\nand B1,B2,B3are the backups. We assume that B2is faulty.\nv, and o, respectively), there is consensus among the nonfaulty servers on\nthe order of which operation goes \ufb01rst. To see why, let a prepare certi\ufb01cate\nPC(t,v,o)denote a certi\ufb01cate that is based on such a set of 2k+1messages.\nLetPC(t,v,o0)be another prepare certi\ufb01cate with the same values for tand v\nrespectively, but with a different operation o0. Because each prepare certi\ufb01cate\nis based on 2k+1values from a total of 3k+1replica servers, the intersection\nof two certi\ufb01cates will necessarily be based on messages from a subset of at\nleast k+1servers. Of this subset, we know that there is at least one nonfaulty\nserver, which will have sent the same prepare message. Hence, o=o0.\nAfter a replica server has a prepare certi\ufb01cate, it commits to the operation\nby broadcasting commit (t,v,o)to the other members in v. Each server S,\nin turn, collects 2kof such commit messages from other servers, leading to\ncommit certi\ufb01cate to execute operation o. At that point, it executes oand\nsends a response to the client. Again, with its own message and the 2kother\nmessages, Sknows that there is consensus among the nonfaulty servers on\nwhich operation to actually execute now. The client collects all the results and\ntakes as the answer the response that is returned by at least k+1replicas, of\nwhich it knows that there is at least one nonfaulty replica server contributing\nto that answer.\nSo far so good. However, we also need to deal with the situation that\nthe primary fails. If a backup detects that the primary fails, it broadcasts\naview-change message for view v+1. What we wish to establish is that a\nrequest that was still being processed at the time the primary failed, will\neventually get executed once and only once by all nonfaulty servers. To this end,\nwe \ufb01rst need to ensure that there are no two commit certi\ufb01cates with the same\ntimestamp that have different associated operations, regardless the view that\neach of them is associated with. This situation can be prevented by having\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 457\nFigure 8.16: The different phases in PBFT. Cis the client, Pis the primary,\nand B1,B2,B3are the backups. We assume that B2is faulty.\nv, and o, respectively), there is consensus among the nonfaulty servers on\nthe order of which operation goes \ufb01rst. To see why, let a prepare certi\ufb01cate\nPC(t,v,o)denote a certi\ufb01cate that is based on such a set of 2k+1messages.\nLetPC(t,v,o0)be another prepare certi\ufb01cate with the same values for tand v\nrespectively, but with a different operation o0. Because each prepare certi\ufb01cate\nis based on 2k+1values from a total of 3k+1replica servers, the intersection\nof two certi\ufb01cates will necessarily be based on messages from a subset of at\nleast k+1servers. Of this subset, we know that there is at least one nonfaulty\nserver, which will have sent the same prepare message. Hence, o=o0.\nAfter a replica server has a prepare certi\ufb01cate, it commits to the operation\nby broadcasting commit (t,v,o)to the other members in v. Each server S,\nin turn, collects 2kof such commit messages from other servers, leading to\ncommit certi\ufb01cate to execute operation o. At that point, it executes oand\nsends a response to the client. Again, with its own message and the 2kother\nmessages, Sknows that there is consensus among the nonfaulty servers on\nwhich operation to actually execute now. The client collects all the results and\ntakes as the answer the response that is returned by at least k+1replicas, of\nwhich it knows that there is at least one nonfaulty replica server contributing\nto that answer.\nSo far so good. However, we also need to deal with the situation that\nthe primary fails. If a backup detects that the primary fails, it broadcasts\naview-change message for view v+1. What we wish to establish is that a\nrequest that was still being processed at the time the primary failed, will\neventually get executed once and only once by all nonfaulty servers. To this end,\nwe \ufb01rst need to ensure that there are no two commit certi\ufb01cates with the same\ntimestamp that have different associated operations, regardless the view that\neach of them is associated with. This situation can be prevented by having\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "458 CHAPTER 8. FAULT TOLERANCE\na quorum of 2k+1commit certi\ufb01cates just as before, but this time based on\nprepare certi\ufb01cates. In other words, we want to regenerate commit certi\ufb01cates,\nbut now for the new view, and only to make sure that a nonfaulty server is\nnot missing any operation. In this respect, note that we may be generating a\ncerti\ufb01cate for an operation that a server Shad already executed (which can be\nobserved by looking at timestamps), but that certi\ufb01cate will be ignored by S\nas long as it keeps an account of its execution history.\nA backup server will broadcast a (signed) message view -change (v+1,P),\nwith Pbeing the set of its prepare certi\ufb01cates. (Note that we ignore garbage\ncollecting issues.) PBFT includes a deterministic function primary (w)known\nto all backups that returns who the next primary will be given a view w. This\nbackup will wait until it has a total of 2k+1view-change messages, leading\nto a view-change certi\ufb01cate X of prepare certi\ufb01cates. The new primary then\nbroadcasts new -view(v+1,X,O), where Oconsists of pre-prepare messages\nsuch that:\n\u2022pre-prepare (t,v+1,o)2Oif the prepare certi\ufb01cate PC(t,v0,o)2X\nsuch that there is no prepare certi\ufb01cate PC(t,v00,o0)with v00>v0,\nor\n\u2022pre-prepare (t,v+1,none)2Oif there is no prepare certi\ufb01cate\nPC(t,v0,o0)2X.\nWhat happens is that any outstanding, pre-prepared operation from a previous\nview is moved to the new view, but considering only the most recent view\nthat led to the installment of the current new view. Simplifying matters a bit,\neach backup will check Oand Xto make sure that all operations are indeed\nauthentic and broadcast prepare messages for all pre-prepare messages in O.\nWe have skipped many elements of PBFT that deal with its correctness\nand above all its ef\ufb01ciency. For example, we did not touch upon garbage\ncollecting logs or ef\ufb01cient ways of authenticating messages. Such details\ncan be found in [Castro and Liskov, 2002]. A description of a wrapper\nthat will allow the incorporation of Byzantine fault tolerance with legacy\napplications is described in [Castro et al., 2003]. Notably the performance\nof Byzantine fault tolerance has been subject to much research, leading to\nmany new protocols (see, for example, Zyzzyva [Kotla et al., 2009] and\nAbstract [Guerraoui et al., 2010]), yet even these new proposals often rely on\nthe original PBFT implementation. That there is still room for improvement\nwhen actually using PBFT for developing robust applications is discussed by\nChondros et al. [2012]. For example, PBFT assumes static membership (i.e.,\nclients and servers are known to each other in advance), but also assumes that\na replica server\u2019s memory acts as a stable, persistent storage. These and other\nshortcomings along with the inherent complexity of Byzantine fault tolerance\nhave formed a hurdle for its widespread usage.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n458 CHAPTER 8. FAULT TOLERANCE\na quorum of 2k+1commit certi\ufb01cates just as before, but this time based on\nprepare certi\ufb01cates. In other words, we want to regenerate commit certi\ufb01cates,\nbut now for the new view, and only to make sure that a nonfaulty server is\nnot missing any operation. In this respect, note that we may be generating a\ncerti\ufb01cate for an operation that a server Shad already executed (which can be\nobserved by looking at timestamps), but that certi\ufb01cate will be ignored by S\nas long as it keeps an account of its execution history.\nA backup server will broadcast a (signed) message view -change (v+1,P),\nwith Pbeing the set of its prepare certi\ufb01cates. (Note that we ignore garbage\ncollecting issues.) PBFT includes a deterministic function primary (w)known\nto all backups that returns who the next primary will be given a view w. This\nbackup will wait until it has a total of 2k+1view-change messages, leading\nto a view-change certi\ufb01cate X of prepare certi\ufb01cates. The new primary then\nbroadcasts new -view(v+1,X,O), where Oconsists of pre-prepare messages\nsuch that:\n\u2022pre-prepare (t,v+1,o)2Oif the prepare certi\ufb01cate PC(t,v0,o)2X\nsuch that there is no prepare certi\ufb01cate PC(t,v00,o0)with v00>v0,\nor\n\u2022pre-prepare (t,v+1,none)2Oif there is no prepare certi\ufb01cate\nPC(t,v0,o0)2X.\nWhat happens is that any outstanding, pre-prepared operation from a previous\nview is moved to the new view, but considering only the most recent view\nthat led to the installment of the current new view. Simplifying matters a bit,\neach backup will check Oand Xto make sure that all operations are indeed\nauthentic and broadcast prepare messages for all pre-prepare messages in O.\nWe have skipped many elements of PBFT that deal with its correctness\nand above all its ef\ufb01ciency. For example, we did not touch upon garbage\ncollecting logs or ef\ufb01cient ways of authenticating messages. Such details\ncan be found in [Castro and Liskov, 2002]. A description of a wrapper\nthat will allow the incorporation of Byzantine fault tolerance with legacy\napplications is described in [Castro et al., 2003]. Notably the performance\nof Byzantine fault tolerance has been subject to much research, leading to\nmany new protocols (see, for example, Zyzzyva [Kotla et al., 2009] and\nAbstract [Guerraoui et al., 2010]), yet even these new proposals often rely on\nthe original PBFT implementation. That there is still room for improvement\nwhen actually using PBFT for developing robust applications is discussed by\nChondros et al. [2012]. For example, PBFT assumes static membership (i.e.,\nclients and servers are known to each other in advance), but also assumes that\na replica server\u2019s memory acts as a stable, persistent storage. These and other\nshortcomings along with the inherent complexity of Byzantine fault tolerance\nhave formed a hurdle for its widespread usage.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 459\nSome limitations on realizing fault tolerance\nOrganizing replicated processes into a group helps to increase fault tolerance.\nHowever, what should have become clear by now is that there is a price to\npay, namely a potential loss of performance. In the solutions discussed so far,\nprocesses in a fault-tolerant group may need to exchange numerous messages\nbefore reaching a decision. The Byzantine agreement protocol is an excellent\nillustration of how tightly coupled processes may be. The question that comes\nto mind is whether realizing speci\ufb01c forms of fault tolerance, like being able\nto withstand arbitrary failures, is always possible.\nOn reaching consensus\nAs we mentioned, if a client can base its decisions through a voting mechanism,\nwe can tolerate that kout of 2k+1processes are lying about their result. The\nassumption we are making, however, is that processes do not team up to\nproduce a wrong result. In general, matters become more intricate if we\ndemand that a process group reaches consensus, which is needed in many\ncases. There are three requirements for reaching consensus [Fischer et al.,\n1985]:\n\u2022 Processes produce the same output value\n\u2022 Every output value must be valid\n\u2022 Every process must eventually provide output\nSome examples where reaching consensus is necessary include electing a\ncoordinator, deciding whether or not to commit a transaction, and dividing\nup tasks among workers. When the communication and processes are all\nperfect, reaching consensus is often straightforward, but when they are not,\nproblems arise.\nThe general goal of distributed consensus algorithms is to have all the\nnonfaulty processes reach consensus on some issue, and to establish that\nconsensus within a \ufb01nite number of steps. The problem is complicated by the\nfact that different assumptions about the underlying system require different\nsolutions, assuming solutions even exist. Turek and Shasha [1992] distinguish\nthe following cases:\n1.Synchronous versus asynchronous systems. Rephrasing our description\nsomewhat, a system is synchronous if and only if the processes are\nknown to operate in a lock-step mode. Formally, this means that there\nshould be some constant c\u00151, such that if any process has taken c+1\nsteps, every other process has taken at least 1 step.\n2.Communication delay is bounded or not. Delay is bounded if and\nonly if we know that every message is delivered with a globally and\npredetermined maximum time.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 459\nSome limitations on realizing fault tolerance\nOrganizing replicated processes into a group helps to increase fault tolerance.\nHowever, what should have become clear by now is that there is a price to\npay, namely a potential loss of performance. In the solutions discussed so far,\nprocesses in a fault-tolerant group may need to exchange numerous messages\nbefore reaching a decision. The Byzantine agreement protocol is an excellent\nillustration of how tightly coupled processes may be. The question that comes\nto mind is whether realizing speci\ufb01c forms of fault tolerance, like being able\nto withstand arbitrary failures, is always possible.\nOn reaching consensus\nAs we mentioned, if a client can base its decisions through a voting mechanism,\nwe can tolerate that kout of 2k+1processes are lying about their result. The\nassumption we are making, however, is that processes do not team up to\nproduce a wrong result. In general, matters become more intricate if we\ndemand that a process group reaches consensus, which is needed in many\ncases. There are three requirements for reaching consensus [Fischer et al.,\n1985]:\n\u2022 Processes produce the same output value\n\u2022 Every output value must be valid\n\u2022 Every process must eventually provide output\nSome examples where reaching consensus is necessary include electing a\ncoordinator, deciding whether or not to commit a transaction, and dividing\nup tasks among workers. When the communication and processes are all\nperfect, reaching consensus is often straightforward, but when they are not,\nproblems arise.\nThe general goal of distributed consensus algorithms is to have all the\nnonfaulty processes reach consensus on some issue, and to establish that\nconsensus within a \ufb01nite number of steps. The problem is complicated by the\nfact that different assumptions about the underlying system require different\nsolutions, assuming solutions even exist. Turek and Shasha [1992] distinguish\nthe following cases:\n1.Synchronous versus asynchronous systems. Rephrasing our description\nsomewhat, a system is synchronous if and only if the processes are\nknown to operate in a lock-step mode. Formally, this means that there\nshould be some constant c\u00151, such that if any process has taken c+1\nsteps, every other process has taken at least 1 step.\n2.Communication delay is bounded or not. Delay is bounded if and\nonly if we know that every message is delivered with a globally and\npredetermined maximum time.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "460 CHAPTER 8. FAULT TOLERANCE\n3.Message delivery is ordered (in real time) or not. In other words, we\ndistinguish the situation where messages from the different senders are\ndelivered in the order that they were sent in real global time, from the\nsituation in which we do not have such guarantees.\n4. Message transmission is done through unicasting or multicasting.\nAs it turns out, reaching consensus is possible only for the situations\nshown in Figure 8.17. In all other cases, it can be shown that no solution exists.\nNote that most distributed systems in practice assume that processes behave\nasynchronously, message transmission is unicast, and communication delays\nare unbounded. As a consequence, we need to make use of ordered (reliable)\nmessage delivery, such as provided by TCP . And again, in practical situations\nwe assume synchronous behavior to be the default, but take into account that\nthere may be unbounded delays as well. Figure 8.17 illustrates the nontrivial\nnature of distributed consensus when processes may fail.\nFigure 8.17: Circumstances under which distributed consensus can be reached.\nReaching consensus may not be possible. Fischer et al. [1985] proved that\nif messages cannot be guaranteed to be delivered within a known, \ufb01nite time,\nno consensus is possible if even one process is faulty (albeit if that one process\nfails silently). The problem with such systems is that arbitrarily slow processes\nare indistinguishable from crashed ones (i.e., you cannot tell the dead from\nthe living). These and other theoretical results are surveyed by Barborak et al.\n[1993] and Turek and Shasha [1992].\nConsistency, availability, and partitioning\nStrongly related to the conditions under which consensus can (not) be reached,\nis when consistency can be reached. Consistency in this case means that\nwhen we have a process group to which a client is sending requests, that\nthe responses returned to that client are correct. We are dealing with a\nsafety property : a property that asserts that nothing bad will happen. For\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n460 CHAPTER 8. FAULT TOLERANCE\n3.Message delivery is ordered (in real time) or not. In other words, we\ndistinguish the situation where messages from the different senders are\ndelivered in the order that they were sent in real global time, from the\nsituation in which we do not have such guarantees.\n4. Message transmission is done through unicasting or multicasting.\nAs it turns out, reaching consensus is possible only for the situations\nshown in Figure 8.17. In all other cases, it can be shown that no solution exists.\nNote that most distributed systems in practice assume that processes behave\nasynchronously, message transmission is unicast, and communication delays\nare unbounded. As a consequence, we need to make use of ordered (reliable)\nmessage delivery, such as provided by TCP . And again, in practical situations\nwe assume synchronous behavior to be the default, but take into account that\nthere may be unbounded delays as well. Figure 8.17 illustrates the nontrivial\nnature of distributed consensus when processes may fail.\nFigure 8.17: Circumstances under which distributed consensus can be reached.\nReaching consensus may not be possible. Fischer et al. [1985] proved that\nif messages cannot be guaranteed to be delivered within a known, \ufb01nite time,\nno consensus is possible if even one process is faulty (albeit if that one process\nfails silently). The problem with such systems is that arbitrarily slow processes\nare indistinguishable from crashed ones (i.e., you cannot tell the dead from\nthe living). These and other theoretical results are surveyed by Barborak et al.\n[1993] and Turek and Shasha [1992].\nConsistency, availability, and partitioning\nStrongly related to the conditions under which consensus can (not) be reached,\nis when consistency can be reached. Consistency in this case means that\nwhen we have a process group to which a client is sending requests, that\nthe responses returned to that client are correct. We are dealing with a\nsafety property : a property that asserts that nothing bad will happen. For\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 461\nour purposes, the type of operations we consider are those that seem to be\nexecuted in a clearly de\ufb01ned order by a single, centralized server. By now,\nwe know better: these operations are executed by a process group in order to\nwithstand the failures of kgroup members.\nWe introduced process groups to improve fault tolerance, and, more specif-\nically, to improve availability. Availability is typically a liveness property :\neventually, something good will happen. In terms of our process groups,\nwe aim to eventually get a (correct) response to every request issued by a\nclient. Being consistent in responses while also being highly available is not\nan unreasonable requirement for services that are part of a distributed system.\nUnfortunately, we may be asking too much.\nIn practical situations, our underlying assumption that the processes in\na group can indeed communicate with each other may be false. Messages\nmay be lost; a group may be partitioned due to a faulty network. In 2000,\nEric Brewer posed an important theorem which was later proven to be correct\nby Gilbert and Lynch [2002]:\nCAP Theorem : Any networked system providing shared data can pro-\nvide only two of the following three properties:\n\u2022C: consistency, by which a shared and replicated data item appears\nas a single, up-to-date copy\n\u2022A: availability, by which updates will always be eventually exe-\ncuted\n\u2022P: Tolerant to the partitioning of process group (e.g., because of a\nfailing network).\nIn other words, in a network subject to communication failures, it is im-\npossible to realize an atomic read/write shared memory that guarantees\na response to every request [Gilbert and Lynch, 2012].\nThis has now become known as the CAP theorem , \ufb01rst published as [Fox\nand Brewer, 1999]. As explained by Brewer [2012], one way of understanding\nthe theorem is to think of two processes unable to communicate because of a\nfailing network. Allowing one process to accept updates leads to inconsistency,\nso that we can only have properties fC,Pg. If the illusion of consistency is to\nbe provided while the two processes cannot communicate, then one of the\ntwo processes will have to pretend to be unavailable, implying having only\nfA,Pg. However, only if the two processes can communicate, is it possible to\nmaintain both consistency and high availability, meaning that we have only\nfC,Ag, but no longer property P.\nNote also the relationship with reaching consensus; in fact, where con-\nsensus requires proving that processes produce the same output, providing\nconsistency is weaker. This also means that if achieving CAP is impossible,\nthan so is consensus.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 461\nour purposes, the type of operations we consider are those that seem to be\nexecuted in a clearly de\ufb01ned order by a single, centralized server. By now,\nwe know better: these operations are executed by a process group in order to\nwithstand the failures of kgroup members.\nWe introduced process groups to improve fault tolerance, and, more specif-\nically, to improve availability. Availability is typically a liveness property :\neventually, something good will happen. In terms of our process groups,\nwe aim to eventually get a (correct) response to every request issued by a\nclient. Being consistent in responses while also being highly available is not\nan unreasonable requirement for services that are part of a distributed system.\nUnfortunately, we may be asking too much.\nIn practical situations, our underlying assumption that the processes in\na group can indeed communicate with each other may be false. Messages\nmay be lost; a group may be partitioned due to a faulty network. In 2000,\nEric Brewer posed an important theorem which was later proven to be correct\nby Gilbert and Lynch [2002]:\nCAP Theorem : Any networked system providing shared data can pro-\nvide only two of the following three properties:\n\u2022C: consistency, by which a shared and replicated data item appears\nas a single, up-to-date copy\n\u2022A: availability, by which updates will always be eventually exe-\ncuted\n\u2022P: Tolerant to the partitioning of process group (e.g., because of a\nfailing network).\nIn other words, in a network subject to communication failures, it is im-\npossible to realize an atomic read/write shared memory that guarantees\na response to every request [Gilbert and Lynch, 2012].\nThis has now become known as the CAP theorem , \ufb01rst published as [Fox\nand Brewer, 1999]. As explained by Brewer [2012], one way of understanding\nthe theorem is to think of two processes unable to communicate because of a\nfailing network. Allowing one process to accept updates leads to inconsistency,\nso that we can only have properties fC,Pg. If the illusion of consistency is to\nbe provided while the two processes cannot communicate, then one of the\ntwo processes will have to pretend to be unavailable, implying having only\nfA,Pg. However, only if the two processes can communicate, is it possible to\nmaintain both consistency and high availability, meaning that we have only\nfC,Ag, but no longer property P.\nNote also the relationship with reaching consensus; in fact, where con-\nsensus requires proving that processes produce the same output, providing\nconsistency is weaker. This also means that if achieving CAP is impossible,\nthan so is consensus.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "462 CHAPTER 8. FAULT TOLERANCE\nThe CAP theorem is all about reaching a trade-off between safety and live-\nness, based on the observation that obtaining both in an inherently unreliable\nsystem cannot be achieved. Practical distributed systems areinherently unreli-\nable. What Brewer and his colleagues observed is that in practical distributed\nsystems, one simply has to make a choice to proceed despite the fact that\nanother process cannot be reached. In other words, we need to do something\nwhen a partition manifests itself through high latency.\nThe bottom line when it seems that partitioning is taking place, is to\nproceed (tolerating partitions in favor of either consistency or availability),\nwhile simultaneously starting a recovery procedure that can mitigate the\neffects of potential inconsistencies. Exactly deciding on how to proceed is\napplication dependent: in many cases having duplicate keys in a database\ncan easily be \ufb01xed (implying that we should tolerate an inconsistency), while\nduplicate transfers of large sums of money may not (meaning that we should\ndecide to tolerate lower availability). One can argue that the CAP theorem\nessentially moves designers of distributed systems from theoretical solutions\nto engineering solutions. The interested reader is referred to [Brewer, 2012] to\nsee how such a move can be made.\nFailure detection\nIt may have become clear from our discussions so far that in order to properly\nmask failures, we generally need to detect them as well. Failure detection is\none of the cornerstones of fault tolerance in distributed systems. What it all\nboils down to is that for a group of processes, nonfaulty members should be\nable to decide who is still a member, and who is not. In other words, we need\nto be able to detect when a member has failed.\nWhen it comes to detecting process failures, there are essentially only two\nmechanisms. Either processes actively send \u201care you alive?\u201d messages to each\nother (for which they obviously expect an answer), or passively wait until\nmessages come in from different processes. The latter approach makes sense\nonly when it can be guaranteed that there is enough communication.\nThere is a huge body of theoretical work on failure detectors. What it all\nboils down to is that a timeout mechanism is used to check whether a process\nhas failed. If a process Pprobes another process Qto see if has failed, Pis\nsaid to suspect Qto have crashed if Qhas not responded within some time.\nNote 8.8 (More information: On perfect failure detectors)\nIt should be clear that in a synchronous distributed system, a suspected crash\ncorresponds to a known crash. In practice, however, we will be dealing with\npartially synchronous systems. In that case, it makes more sense to assume\neventually perfect failure detectors . In this case, a process Pwill suspect another\nprocess Qto have crashed after ttime units have elapsed and still Qdid not\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n462 CHAPTER 8. FAULT TOLERANCE\nThe CAP theorem is all about reaching a trade-off between safety and live-\nness, based on the observation that obtaining both in an inherently unreliable\nsystem cannot be achieved. Practical distributed systems areinherently unreli-\nable. What Brewer and his colleagues observed is that in practical distributed\nsystems, one simply has to make a choice to proceed despite the fact that\nanother process cannot be reached. In other words, we need to do something\nwhen a partition manifests itself through high latency.\nThe bottom line when it seems that partitioning is taking place, is to\nproceed (tolerating partitions in favor of either consistency or availability),\nwhile simultaneously starting a recovery procedure that can mitigate the\neffects of potential inconsistencies. Exactly deciding on how to proceed is\napplication dependent: in many cases having duplicate keys in a database\ncan easily be \ufb01xed (implying that we should tolerate an inconsistency), while\nduplicate transfers of large sums of money may not (meaning that we should\ndecide to tolerate lower availability). One can argue that the CAP theorem\nessentially moves designers of distributed systems from theoretical solutions\nto engineering solutions. The interested reader is referred to [Brewer, 2012] to\nsee how such a move can be made.\nFailure detection\nIt may have become clear from our discussions so far that in order to properly\nmask failures, we generally need to detect them as well. Failure detection is\none of the cornerstones of fault tolerance in distributed systems. What it all\nboils down to is that for a group of processes, nonfaulty members should be\nable to decide who is still a member, and who is not. In other words, we need\nto be able to detect when a member has failed.\nWhen it comes to detecting process failures, there are essentially only two\nmechanisms. Either processes actively send \u201care you alive?\u201d messages to each\nother (for which they obviously expect an answer), or passively wait until\nmessages come in from different processes. The latter approach makes sense\nonly when it can be guaranteed that there is enough communication.\nThere is a huge body of theoretical work on failure detectors. What it all\nboils down to is that a timeout mechanism is used to check whether a process\nhas failed. If a process Pprobes another process Qto see if has failed, Pis\nsaid to suspect Qto have crashed if Qhas not responded within some time.\nNote 8.8 (More information: On perfect failure detectors)\nIt should be clear that in a synchronous distributed system, a suspected crash\ncorresponds to a known crash. In practice, however, we will be dealing with\npartially synchronous systems. In that case, it makes more sense to assume\neventually perfect failure detectors . In this case, a process Pwill suspect another\nprocess Qto have crashed after ttime units have elapsed and still Qdid not\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.2. PROCESS RESILIENCE 463\nrespond to P\u2019s probe. However, if Qlater does send a message that is (also)\nreceived by P,Pwill (1) stop suspecting Q, and (2) increase the timeout value t.\nNote that if Qdoes crash (and does not recover), Pwill continue to suspect Q.\nIn real settings, there are problems with using probes and timeouts. For\nexample, due to unreliable networks, simply stating that a process has failed\nbecause it does not return an answer to a probe message may be wrong. In\nother words, it is quite easy to generate false positives. If a false positive has\nthe effect that a perfectly healthy process is removed from a membership list,\nthen clearly we are doing something wrong. Another serious problem is that\ntimeouts are just plain crude. As noticed by Birman [2012], there is hardly\nany work on building proper failure detection subsystems that take more into\naccount than only the lack of a reply to a single message. This statement is\neven more evident when looking at industry-deployed distributed systems.\nThere are various issues that need to be taken into account when designing\na failure detection subsystem (see also Zhuang et al. [2005]). For example, fail-\nure detection can take place through gossiping in which each node regularly\nannounces to its neighbors that it is still up and running. As we mentioned,\nan alternative is to let nodes actively probe each other.\nFailure detection can also be done as a side-effect of regularly exchanging\ninformation with neighbors, as is the case with gossip-based information\ndissemination (which we discussed in Chapter 4). This approach is essentially\nalso adopted in Obduro [Vogels, 2003]: processes periodically gossip their\nservice availability. This information is gradually disseminated through the\nnetwork. Eventually, every process will know about every other process, but\nmore importantly, will have enough information locally available to decide\nwhether a process has failed or not. A member for which the availability\ninformation is old, will presumably have failed.\nAnother important issue is that a failure detection subsystem should\nideally be able to distinguish network failures from node failures. One way of\ndealing with this problem is not to let a single node decide whether one of its\nneighbors has crashed. Instead, when noticing a timeout on a probe message,\na node requests other neighbors to see whether they can reach the presumed\nfailing node. Of course, positive information can also be shared: if a node is\nstill alive, that information can be forwarded to other interested parties (who\nmay be detecting a link failure to the suspected node).\nThis brings us to another key issue: when a member failure is detected,\nhow should other nonfaulty processes be informed? One simple, and some-\nwhat radical approach is the following. In FUSE [Dunagan et al., 2004],\nprocesses can be joined in a group that spans a wide-area network. The group\nmembers create a spanning tree that is used for monitoring member failures.\nMembers send ping messages to their neighbors. When a neighbor does not\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.2. PROCESS RESILIENCE 463\nrespond to P\u2019s probe. However, if Qlater does send a message that is (also)\nreceived by P,Pwill (1) stop suspecting Q, and (2) increase the timeout value t.\nNote that if Qdoes crash (and does not recover), Pwill continue to suspect Q.\nIn real settings, there are problems with using probes and timeouts. For\nexample, due to unreliable networks, simply stating that a process has failed\nbecause it does not return an answer to a probe message may be wrong. In\nother words, it is quite easy to generate false positives. If a false positive has\nthe effect that a perfectly healthy process is removed from a membership list,\nthen clearly we are doing something wrong. Another serious problem is that\ntimeouts are just plain crude. As noticed by Birman [2012], there is hardly\nany work on building proper failure detection subsystems that take more into\naccount than only the lack of a reply to a single message. This statement is\neven more evident when looking at industry-deployed distributed systems.\nThere are various issues that need to be taken into account when designing\na failure detection subsystem (see also Zhuang et al. [2005]). For example, fail-\nure detection can take place through gossiping in which each node regularly\nannounces to its neighbors that it is still up and running. As we mentioned,\nan alternative is to let nodes actively probe each other.\nFailure detection can also be done as a side-effect of regularly exchanging\ninformation with neighbors, as is the case with gossip-based information\ndissemination (which we discussed in Chapter 4). This approach is essentially\nalso adopted in Obduro [Vogels, 2003]: processes periodically gossip their\nservice availability. This information is gradually disseminated through the\nnetwork. Eventually, every process will know about every other process, but\nmore importantly, will have enough information locally available to decide\nwhether a process has failed or not. A member for which the availability\ninformation is old, will presumably have failed.\nAnother important issue is that a failure detection subsystem should\nideally be able to distinguish network failures from node failures. One way of\ndealing with this problem is not to let a single node decide whether one of its\nneighbors has crashed. Instead, when noticing a timeout on a probe message,\na node requests other neighbors to see whether they can reach the presumed\nfailing node. Of course, positive information can also be shared: if a node is\nstill alive, that information can be forwarded to other interested parties (who\nmay be detecting a link failure to the suspected node).\nThis brings us to another key issue: when a member failure is detected,\nhow should other nonfaulty processes be informed? One simple, and some-\nwhat radical approach is the following. In FUSE [Dunagan et al., 2004],\nprocesses can be joined in a group that spans a wide-area network. The group\nmembers create a spanning tree that is used for monitoring member failures.\nMembers send ping messages to their neighbors. When a neighbor does not\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "464 CHAPTER 8. FAULT TOLERANCE\nrespond, the pinging node immediately switches to a state in which it will\nalso no longer respond to pings from other nodes. By recursion, it is seen that\na single node failure is rapidly promoted to a group failure noti\ufb01cation.\n8.3 Reliable client-server communication\nIn many cases, fault tolerance in distributed systems concentrates on faulty\nprocesses. However, we also need to consider communication failures. Most\nof the failure models discussed previously apply equally well to communi-\ncation channels. In particular, a communication channel may exhibit crash,\nomission, timing, and arbitrary failures. In practice, when building reliable\ncommunication channels, the focus is on masking crash and omission failures.\nArbitrary failures may occur in the form of duplicate messages, resulting from\nthe fact that in a computer network messages may be buffered for a relatively\nlong time, and are reinjected into the network after the original sender has\nalready issued a retransmission (see, for example, Tanenbaum and Wetherall\n[2010]).\nPoint-to-point communication\nIn many distributed systems, reliable point-to-point communication is es-\ntablished by making use of a reliable transport protocol, such as TCP . TCP\nmasks omission failures, which occur in the form of lost messages, by using\nacknowledgments and retransmissions. Such failures are completely hidden\nfrom a TCP client.\nHowever, crash failures of connections are not masked. A crash failure\nmay occur when (for whatever reason) a TCP connection is abruptly broken so\nthat no more messages can be transmitted through the channel. In most cases,\nthe client is informed that the channel has crashed by raising an exception.\nThe only way to mask such failures is to let the distributed system attempt\nto automatically set up a new connection, by simply resending a connection\nrequest. The underlying assumption is that the other side is still, or again,\nresponsive to such requests.\nRPC semantics in the presence of failures\nLet us now take a closer look at client-server communication when using\nhigh-level communication facilities such as Remote Procedure Calls (RPCs).\nThe goal of RPC is to hide communication by making remote procedure calls\nlook just like local ones. With a few exceptions, so far we have come fairly\nclose. Indeed, as long as both client and server are functioning perfectly, RPC\ndoes its job well. The problem comes about when errors occur. It is then that\nthe differences between local and remote calls are not always easy to mask.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n464 CHAPTER 8. FAULT TOLERANCE\nrespond, the pinging node immediately switches to a state in which it will\nalso no longer respond to pings from other nodes. By recursion, it is seen that\na single node failure is rapidly promoted to a group failure noti\ufb01cation.\n8.3 Reliable client-server communication\nIn many cases, fault tolerance in distributed systems concentrates on faulty\nprocesses. However, we also need to consider communication failures. Most\nof the failure models discussed previously apply equally well to communi-\ncation channels. In particular, a communication channel may exhibit crash,\nomission, timing, and arbitrary failures. In practice, when building reliable\ncommunication channels, the focus is on masking crash and omission failures.\nArbitrary failures may occur in the form of duplicate messages, resulting from\nthe fact that in a computer network messages may be buffered for a relatively\nlong time, and are reinjected into the network after the original sender has\nalready issued a retransmission (see, for example, Tanenbaum and Wetherall\n[2010]).\nPoint-to-point communication\nIn many distributed systems, reliable point-to-point communication is es-\ntablished by making use of a reliable transport protocol, such as TCP . TCP\nmasks omission failures, which occur in the form of lost messages, by using\nacknowledgments and retransmissions. Such failures are completely hidden\nfrom a TCP client.\nHowever, crash failures of connections are not masked. A crash failure\nmay occur when (for whatever reason) a TCP connection is abruptly broken so\nthat no more messages can be transmitted through the channel. In most cases,\nthe client is informed that the channel has crashed by raising an exception.\nThe only way to mask such failures is to let the distributed system attempt\nto automatically set up a new connection, by simply resending a connection\nrequest. The underlying assumption is that the other side is still, or again,\nresponsive to such requests.\nRPC semantics in the presence of failures\nLet us now take a closer look at client-server communication when using\nhigh-level communication facilities such as Remote Procedure Calls (RPCs).\nThe goal of RPC is to hide communication by making remote procedure calls\nlook just like local ones. With a few exceptions, so far we have come fairly\nclose. Indeed, as long as both client and server are functioning perfectly, RPC\ndoes its job well. The problem comes about when errors occur. It is then that\nthe differences between local and remote calls are not always easy to mask.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.3. RELIABLE CLIENT-SERVER COMMUNICATION 465\nTo structure our discussion, let us distinguish between \ufb01ve different classes\nof failures that can occur in RPC systems, as follows:\n1. The client is unable to locate the server.\n2. The request message from the client to the server is lost.\n3. The server crashes after receiving a request.\n4. The reply message from the server to the client is lost.\n5. The client crashes after sending a request.\nEach of these categories poses different problems and requires different solu-\ntions.\nClient cannot locate the server\nTo start with, it can happen that the client cannot locate a suitable server. All\nservers might be down, for example. Alternatively, suppose that the client is\ncompiled using a particular version of the client stub, and the binary is not\nused for a considerable period of time. In the meantime, the server evolves\nand a new version of the interface is installed; new stubs are generated and\nput into use. When the client is eventually run, the binder will be unable\nto match it up with a server and will report failure. While this mechanism\nis used to protect the client from accidentally trying to talk to a server that\nmay not agree with it in terms of what parameters are required or what it is\nsupposed to do, the problem remains of how should this failure be dealt with.\nOne possible solution is to have the error raise an exception . In some\nlanguages, (e.g., Java), programmers can write special procedures that are\ninvoked upon speci\ufb01c errors, such as division by zero. In C, signal handlers\ncan be used for this purpose. In other words, we could de\ufb01ne a new signal\ntype SIGNOSERVER , and allow it to be handled in the same way as other\nsignals.\nThis approach, too, has drawbacks. To start with, not every language has\nexceptions or signals. Another point is that having to write an exception\nor signal handler destroys the transparency we have been trying to achieve.\nSuppose that you are a programmer and your boss tells you to write the append\nprocedure. You smile and tell her it will be written, tested, and documented\nin \ufb01ve minutes. Then she mentions that you also have to write an exception\nhandler as well, just in case the procedure is not there today. At this point it\nis pretty hard to maintain the illusion that remote procedures are no different\nfrom local ones, since writing an exception handler for \u201cCannot locate server\u201d\nwould be a rather unusual request in a nondistributed system. So much for\ntransparency.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.3. RELIABLE CLIENT-SERVER COMMUNICATION 465\nTo structure our discussion, let us distinguish between \ufb01ve different classes\nof failures that can occur in RPC systems, as follows:\n1. The client is unable to locate the server.\n2. The request message from the client to the server is lost.\n3. The server crashes after receiving a request.\n4. The reply message from the server to the client is lost.\n5. The client crashes after sending a request.\nEach of these categories poses different problems and requires different solu-\ntions.\nClient cannot locate the server\nTo start with, it can happen that the client cannot locate a suitable server. All\nservers might be down, for example. Alternatively, suppose that the client is\ncompiled using a particular version of the client stub, and the binary is not\nused for a considerable period of time. In the meantime, the server evolves\nand a new version of the interface is installed; new stubs are generated and\nput into use. When the client is eventually run, the binder will be unable\nto match it up with a server and will report failure. While this mechanism\nis used to protect the client from accidentally trying to talk to a server that\nmay not agree with it in terms of what parameters are required or what it is\nsupposed to do, the problem remains of how should this failure be dealt with.\nOne possible solution is to have the error raise an exception . In some\nlanguages, (e.g., Java), programmers can write special procedures that are\ninvoked upon speci\ufb01c errors, such as division by zero. In C, signal handlers\ncan be used for this purpose. In other words, we could de\ufb01ne a new signal\ntype SIGNOSERVER , and allow it to be handled in the same way as other\nsignals.\nThis approach, too, has drawbacks. To start with, not every language has\nexceptions or signals. Another point is that having to write an exception\nor signal handler destroys the transparency we have been trying to achieve.\nSuppose that you are a programmer and your boss tells you to write the append\nprocedure. You smile and tell her it will be written, tested, and documented\nin \ufb01ve minutes. Then she mentions that you also have to write an exception\nhandler as well, just in case the procedure is not there today. At this point it\nis pretty hard to maintain the illusion that remote procedures are no different\nfrom local ones, since writing an exception handler for \u201cCannot locate server\u201d\nwould be a rather unusual request in a nondistributed system. So much for\ntransparency.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "466 CHAPTER 8. FAULT TOLERANCE\n(a) (b) (c)\nFigure 8.18: A server in client-server communication. (a) The normal case.\n(b) Crash after execution. (c) Crash before execution.\nLost request messages\nThe second item on the list is dealing with lost request messages. This is\nthe easiest one to deal with: just have the operating system or client stub\nstart a timer when sending the request. If the timer expires before a reply\nor acknowledgment comes back, the message is sent again. If the message\nwas truly lost, the server will not be able to tell the difference between the\nretransmission and the original, and everything will work \ufb01ne. Unless, of\ncourse, so many request messages are lost that the client gives up and falsely\nconcludes that the server is down, in which case we are back to \u201cCannot\nlocate server.\u201d If the request was not lost, the only thing we need to do is let\nthe server be able to detect it is dealing with a retransmission. Unfortunately,\ndoing so is not so simple, as we explain when discussing lost replies.\nServer crashes\nThe next failure on the list is a server crash. The normal sequence of events\nat a server is shown in Figure 8.18(a). A request arrives, is carried out, and a\nreply is sent. Now consider Figure 8.18(b). A request arrives and is carried\nout, just as before, but the server crashes before it can send the reply. Finally,\nlook at Figure 8.18(c). Again a request arrives, but this time the server crashes\nbefore it can even be carried out. And, of course, no reply is sent back.\nThe annoying part of Figure 8.18 is that the correct treatment differs for (b)\nand (c). In (b) the system has to report failure back to the client (e.g., raise an\nexception), whereas in (c) it can just retransmit the request. The problem is\nthat the client\u2019s operating system cannot tell which is which. All it knows is\nthat its timer has expired.\nThree schools of thought exist on what to do here [Spector, 1982]. One\nphilosophy is to wait until the server reboots (or let the client\u2019s middleware\ntransparently rebind to a new server) and try the operation again. The idea is\nto keep trying until a reply has been received, then give it to the client. This\ntechnique is called at-least-once semantics and guarantees that the RPC has\nbeen carried out at least one time, but possibly more.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n466 CHAPTER 8. FAULT TOLERANCE\n(a) (b) (c)\nFigure 8.18: A server in client-server communication. (a) The normal case.\n(b) Crash after execution. (c) Crash before execution.\nLost request messages\nThe second item on the list is dealing with lost request messages. This is\nthe easiest one to deal with: just have the operating system or client stub\nstart a timer when sending the request. If the timer expires before a reply\nor acknowledgment comes back, the message is sent again. If the message\nwas truly lost, the server will not be able to tell the difference between the\nretransmission and the original, and everything will work \ufb01ne. Unless, of\ncourse, so many request messages are lost that the client gives up and falsely\nconcludes that the server is down, in which case we are back to \u201cCannot\nlocate server.\u201d If the request was not lost, the only thing we need to do is let\nthe server be able to detect it is dealing with a retransmission. Unfortunately,\ndoing so is not so simple, as we explain when discussing lost replies.\nServer crashes\nThe next failure on the list is a server crash. The normal sequence of events\nat a server is shown in Figure 8.18(a). A request arrives, is carried out, and a\nreply is sent. Now consider Figure 8.18(b). A request arrives and is carried\nout, just as before, but the server crashes before it can send the reply. Finally,\nlook at Figure 8.18(c). Again a request arrives, but this time the server crashes\nbefore it can even be carried out. And, of course, no reply is sent back.\nThe annoying part of Figure 8.18 is that the correct treatment differs for (b)\nand (c). In (b) the system has to report failure back to the client (e.g., raise an\nexception), whereas in (c) it can just retransmit the request. The problem is\nthat the client\u2019s operating system cannot tell which is which. All it knows is\nthat its timer has expired.\nThree schools of thought exist on what to do here [Spector, 1982]. One\nphilosophy is to wait until the server reboots (or let the client\u2019s middleware\ntransparently rebind to a new server) and try the operation again. The idea is\nto keep trying until a reply has been received, then give it to the client. This\ntechnique is called at-least-once semantics and guarantees that the RPC has\nbeen carried out at least one time, but possibly more.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.3. RELIABLE CLIENT-SERVER COMMUNICATION 467\nThe second philosophy gives up immediately and reports back failure.\nThis approach is called at-most-once semantics and guarantees that the RPC\nhas been carried out at most one time, but possibly not at all.\nThe third philosophy is to guarantee nothing. When a server crashes, the\nclient gets no help and no promises about what happened. The RPC may\nhave been carried out anywhere from zero to a large number of times. The\nmain virtue of this scheme is that it is easy to implement.\nNone of these are terribly attractive. What one would like is exactly-once\nsemantics , but in general, there is no way to arrange this. Imagine that\nthe remote operation consists of processing a document such as producing\na number of PDF \ufb01les from L ATEX and other sources. The server sends a\ncompletion message to the client when the document has been completely\nprocessed. Also assume that when a client issues a request, it receives an\nacknowledgment that the request has been delivered to the server. There are\ntwo strategies the server can follow. It can either send a completion message\njust before it actually tells the document processor to do its work, or after the\ndocument has been processed.\nAssume that the server crashes and subsequently recovers. It announces\nto all clients that it has just crashed but is now up and running again. The\nproblem is that the client does not know whether its request to process a\ndocument will actually have been carried out.\nThere are four strategies the client can follow. First, the client can decide\ntonever reissue a request, at the risk that the document will not be processed.\nSecond, it can decide to always reissue a request, but this may lead to the\ndocument being processed twice (which may easily incur a signi\ufb01cant amount\nof work when dealing with intricate documents). Third, it can decide to\nreissue a request only if it did not yet receive an acknowledgment that its\nrequest had been delivered to the server. In that case, the client is counting\non the fact that the server crashed before the request could be delivered.\nThe fourth and last strategy is to reissue a request only if it has received an\nacknowledgment for the request. With two strategies for the server, and four\nfor the client, there are a total of eight combinations to consider. Unfortunately,\nas it turns out, no combination is satisfactory: it can be shown that for any\ncombination either the request is lost forever, or carried out twice.\nNote 8.9 (Advanced: Why fully transparent server recovery is impossible)\nTo explain the situation of server recovery, note that there are three events that can\nhappen at the server: send the completion message (M), complete the processing\nof the document (P), and crash (C). Note that crashing during the processing of a\ndocument is considered the same as crashing before its completion. These events\ncan occur in six different orderings:\n1.M!P!C: A crash occurs after sending the completion message and\nprocessing the document.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.3. RELIABLE CLIENT-SERVER COMMUNICATION 467\nThe second philosophy gives up immediately and reports back failure.\nThis approach is called at-most-once semantics and guarantees that the RPC\nhas been carried out at most one time, but possibly not at all.\nThe third philosophy is to guarantee nothing. When a server crashes, the\nclient gets no help and no promises about what happened. The RPC may\nhave been carried out anywhere from zero to a large number of times. The\nmain virtue of this scheme is that it is easy to implement.\nNone of these are terribly attractive. What one would like is exactly-once\nsemantics , but in general, there is no way to arrange this. Imagine that\nthe remote operation consists of processing a document such as producing\na number of PDF \ufb01les from L ATEX and other sources. The server sends a\ncompletion message to the client when the document has been completely\nprocessed. Also assume that when a client issues a request, it receives an\nacknowledgment that the request has been delivered to the server. There are\ntwo strategies the server can follow. It can either send a completion message\njust before it actually tells the document processor to do its work, or after the\ndocument has been processed.\nAssume that the server crashes and subsequently recovers. It announces\nto all clients that it has just crashed but is now up and running again. The\nproblem is that the client does not know whether its request to process a\ndocument will actually have been carried out.\nThere are four strategies the client can follow. First, the client can decide\ntonever reissue a request, at the risk that the document will not be processed.\nSecond, it can decide to always reissue a request, but this may lead to the\ndocument being processed twice (which may easily incur a signi\ufb01cant amount\nof work when dealing with intricate documents). Third, it can decide to\nreissue a request only if it did not yet receive an acknowledgment that its\nrequest had been delivered to the server. In that case, the client is counting\non the fact that the server crashed before the request could be delivered.\nThe fourth and last strategy is to reissue a request only if it has received an\nacknowledgment for the request. With two strategies for the server, and four\nfor the client, there are a total of eight combinations to consider. Unfortunately,\nas it turns out, no combination is satisfactory: it can be shown that for any\ncombination either the request is lost forever, or carried out twice.\nNote 8.9 (Advanced: Why fully transparent server recovery is impossible)\nTo explain the situation of server recovery, note that there are three events that can\nhappen at the server: send the completion message (M), complete the processing\nof the document (P), and crash (C). Note that crashing during the processing of a\ndocument is considered the same as crashing before its completion. These events\ncan occur in six different orderings:\n1.M!P!C: A crash occurs after sending the completion message and\nprocessing the document.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "468 CHAPTER 8. FAULT TOLERANCE\n2.M!C!P: A crash happens after sending the completion message, but\nbefore the document could be (completely) processed.\n3.P!M!C: A crash occurs after sending the completion message and\nprocessing the document.\n4.P!C(!M): The document was processed, after which a crash occurs\nbefore the completion message could be sent.\n5.C(!P!M): A crash happens before the server could complete the\nprocessing of the document.\n6.C(!M!P): A crash happens before the server could even do anything.\nThe parentheses indicate an event that can no longer happen because the server\nalready crashed. Figure 8.19 shows all possible combinations. As can be readily\nveri\ufb01ed, there is no combination of client strategy and server strategy that will\nwork correctly under all possible event sequences. The bottom line is that the\nclient can never know whether the server crashed just before or after having the\ntext printed.\nReissue strategy\nAlways\nNever\nOnly when ACKed\nOnly when not ACKed\nClientStrategy M!P\nMPC MC(P) C(MP)\nDUP OK OK\nOK ZERO ZERO\nDUP OK ZERO\nOK ZERO OK\nServerStrategy P!M\nPMC PC(M) C(PM)\nDUP DUP OK\nOK OK ZERO\nDUP OK ZERO\nOK DUP OK\nServer\nOK = Document processed once\nDUP = Document processed twice\nZERO = Document not processed at all\nFigure 8.19: Different combinations of client and server strategies in the\npresence of server crashes. Events between brackets never take place\nbecause of a previous crash.\nIn short, the possibility of server crashes radically changes the nature of RPC\nand clearly distinguishes single-processor systems from distributed ones. In the\nformer case, a server crash also implies a client crash, so recovery is neither\npossible nor necessary. In the latter we can and should take action.\nLost reply messages\nLost replies can also be dif\ufb01cult to deal with. The obvious solution is just to\nrely on a timer again that has been set by the client\u2019s operating system. If no\nreply is forthcoming within a reasonable period, just send the request once\nmore. The trouble with this solution is that the client is not really sure why\nthere was no answer. Did the request or reply get lost, or is the server merely\nslow? It may make a difference.\nIn particular, some operations can safely be repeated as often as necessary\nwith no damage being done. A request such as asking for the \ufb01rst 1024\nbytes of a \ufb01le has no side effects and can be executed as often as necessary\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n468 CHAPTER 8. FAULT TOLERANCE\n2.M!C!P: A crash happens after sending the completion message, but\nbefore the document could be (completely) processed.\n3.P!M!C: A crash occurs after sending the completion message and\nprocessing the document.\n4.P!C(!M): The document was processed, after which a crash occurs\nbefore the completion message could be sent.\n5.C(!P!M): A crash happens before the server could complete the\nprocessing of the document.\n6.C(!M!P): A crash happens before the server could even do anything.\nThe parentheses indicate an event that can no longer happen because the server\nalready crashed. Figure 8.19 shows all possible combinations. As can be readily\nveri\ufb01ed, there is no combination of client strategy and server strategy that will\nwork correctly under all possible event sequences. The bottom line is that the\nclient can never know whether the server crashed just before or after having the\ntext printed.\nReissue strategy\nAlways\nNever\nOnly when ACKed\nOnly when not ACKed\nClientStrategy M!P\nMPC MC(P) C(MP)\nDUP OK OK\nOK ZERO ZERO\nDUP OK ZERO\nOK ZERO OK\nServerStrategy P!M\nPMC PC(M) C(PM)\nDUP DUP OK\nOK OK ZERO\nDUP OK ZERO\nOK DUP OK\nServer\nOK = Document processed once\nDUP = Document processed twice\nZERO = Document not processed at all\nFigure 8.19: Different combinations of client and server strategies in the\npresence of server crashes. Events between brackets never take place\nbecause of a previous crash.\nIn short, the possibility of server crashes radically changes the nature of RPC\nand clearly distinguishes single-processor systems from distributed ones. In the\nformer case, a server crash also implies a client crash, so recovery is neither\npossible nor necessary. In the latter we can and should take action.\nLost reply messages\nLost replies can also be dif\ufb01cult to deal with. The obvious solution is just to\nrely on a timer again that has been set by the client\u2019s operating system. If no\nreply is forthcoming within a reasonable period, just send the request once\nmore. The trouble with this solution is that the client is not really sure why\nthere was no answer. Did the request or reply get lost, or is the server merely\nslow? It may make a difference.\nIn particular, some operations can safely be repeated as often as necessary\nwith no damage being done. A request such as asking for the \ufb01rst 1024\nbytes of a \ufb01le has no side effects and can be executed as often as necessary\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.3. RELIABLE CLIENT-SERVER COMMUNICATION 469\nwithout any harm being done. A request that has this property is said to be\nidempotent .\nNow consider a request to a banking server asking to transfer money from\none account to another. If the request arrives and is carried out, but the reply\nis lost, the client will not know this and will retransmit the message. The\nbank server will interpret this request as a new one, and will carry it out too.\nTwice the amount of money will be transferred. Transferring money is not\nidempotent.\nOne way of solving this problem is to try to structure all the requests in\nan idempotent way. In practice, however, many requests (e.g., transferring\nmoney) are inherently nonidempotent, so something else is needed. Another\nmethod is to have the client assign each request a sequence number. By\nhaving the server keep track of the most recently received sequence number\nfrom each client that is using it, the server can tell the difference between\nan original request and a retransmission and can refuse to carry out any\nrequest a second time. However, the server will still have to send a response\nto the client. Note that this approach does require that the server maintains\nadministration on each client. Furthermore, it is not clear how long to maintain\nthis administration. An additional safeguard is to have a bit in the message\nheader that is used to distinguish initial requests from retransmissions (the\nidea being that it is always safe to perform an original request; retransmissions\nmay require more care).\nClient crashes\nThe \ufb01nal item on the list of failures is the client crash. What happens if a client\nsends a request to a server to do some work and crashes before the server\nreplies? At this point a computation is active and no parent is waiting for the\nresult. Such an unwanted computation is called an orphan (computation) .\nOrphan computations can cause a variety of problems that can interfere\nwith normal operation of the system. As a bare minimum, they waste pro-\ncessing power. They can also lock \ufb01les or otherwise tie up valuable resources.\nFinally, if the client reboots and does the RPC again, but the reply from the\norphan comes back immediately afterward, confusion can result.\nWhat can be done about orphans? Four solutions have been proposed [Nel-\nson, 1981]. First, before a client stub sends an RPC message, it makes a log\nentry telling what it is about to do. The log is kept on disk or some other\nmedium that survives crashes. After a reboot, the log is checked and the\norphan is explicitly killed off. This solution is called orphan extermination .\nThe disadvantage of this scheme is the horrendous expense of writing\na disk record for every RPC. Furthermore, it may not even work, since\norphans themselves may do RPCs, thus creating grandorphans or further\ndescendants that are dif\ufb01cult or impossible to locate. Finally, the network may\nbe partitioned, for example, due to a failed gateway, making it impossible\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.3. RELIABLE CLIENT-SERVER COMMUNICATION 469\nwithout any harm being done. A request that has this property is said to be\nidempotent .\nNow consider a request to a banking server asking to transfer money from\none account to another. If the request arrives and is carried out, but the reply\nis lost, the client will not know this and will retransmit the message. The\nbank server will interpret this request as a new one, and will carry it out too.\nTwice the amount of money will be transferred. Transferring money is not\nidempotent.\nOne way of solving this problem is to try to structure all the requests in\nan idempotent way. In practice, however, many requests (e.g., transferring\nmoney) are inherently nonidempotent, so something else is needed. Another\nmethod is to have the client assign each request a sequence number. By\nhaving the server keep track of the most recently received sequence number\nfrom each client that is using it, the server can tell the difference between\nan original request and a retransmission and can refuse to carry out any\nrequest a second time. However, the server will still have to send a response\nto the client. Note that this approach does require that the server maintains\nadministration on each client. Furthermore, it is not clear how long to maintain\nthis administration. An additional safeguard is to have a bit in the message\nheader that is used to distinguish initial requests from retransmissions (the\nidea being that it is always safe to perform an original request; retransmissions\nmay require more care).\nClient crashes\nThe \ufb01nal item on the list of failures is the client crash. What happens if a client\nsends a request to a server to do some work and crashes before the server\nreplies? At this point a computation is active and no parent is waiting for the\nresult. Such an unwanted computation is called an orphan (computation) .\nOrphan computations can cause a variety of problems that can interfere\nwith normal operation of the system. As a bare minimum, they waste pro-\ncessing power. They can also lock \ufb01les or otherwise tie up valuable resources.\nFinally, if the client reboots and does the RPC again, but the reply from the\norphan comes back immediately afterward, confusion can result.\nWhat can be done about orphans? Four solutions have been proposed [Nel-\nson, 1981]. First, before a client stub sends an RPC message, it makes a log\nentry telling what it is about to do. The log is kept on disk or some other\nmedium that survives crashes. After a reboot, the log is checked and the\norphan is explicitly killed off. This solution is called orphan extermination .\nThe disadvantage of this scheme is the horrendous expense of writing\na disk record for every RPC. Furthermore, it may not even work, since\norphans themselves may do RPCs, thus creating grandorphans or further\ndescendants that are dif\ufb01cult or impossible to locate. Finally, the network may\nbe partitioned, for example, due to a failed gateway, making it impossible\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "470 CHAPTER 8. FAULT TOLERANCE\nto kill them even if they can be located. All in all, this is not a promising\napproach.\nWith the second solution, called reincarnation , all these problems can\nbe solved without the need to write disk records. The way it works is to\ndivide time up into sequentially numbered epochs. When a client reboots,\nit broadcasts a message to all machines declaring the start of a new epoch.\nWhen such a broadcast comes in, all remote computations are killed. Of\ncourse, if the network is partitioned, some orphans may survive. Fortunately,\nhowever, when they report back, their replies will contain an obsolete epoch\nnumber, making them easy to detect.\nThe third solution is a variant on this idea, but somewhat less draconian.\nIt is called gentle reincarnation . When an epoch broadcast comes in, each\nmachine checks to see if it has any remote computations running locally, and\nif so, tries its best to locate their owners. Only if the owners cannot be located\nanywhere is the computation killed.\nIn the fourth solution, called expiration , each RPC is given a standard\namount of time, T, to do the job. If it cannot \ufb01nish, it must explicitly ask for\nanother quantum. Of course, this is quite a nuisance. On the other hand, if\nafter a crash the client waits a time Tbefore rebooting, all orphans are sure to\nbe gone. The problem to be solved here is choosing a reasonable value of Tin\nthe face of RPCs with wildly differing requirements.\nIn practice, all of these methods are crude and undesirable. Worse yet,\nkilling an orphan may have unforeseen consequences. For example, suppose\nthat an orphan has obtained locks on one or more \ufb01les or database records.\nIf the orphan is suddenly killed, these locks may remain forever. Also, an\norphan may have already made entries in various remote queues to start\nup other processes at some future time, so even killing the orphan may not\nremove all traces of it. Conceivably, it may even have started again, with\nunforeseen consequences. Orphan elimination is discussed in more detail\nby Panzieri and Shrivastava [1988].\n8.4 Reliable group communication\nConsidering how important process resilience by replication is, it is not sur-\nprising that reliable multicast services are important as well. Such services\nguarantee that messages are delivered to all members in a process group.\nUnfortunately, reliable multicasting turns out to be surprisingly tricky. In\nthis section, we take a closer look at the issues involved in reliably delivering\nmessages to a process group. Let us \ufb01rst de\ufb01ne what reliable group communi-\ncation actually is. Intuitively, it means that a message that is sent to a process\ngroup should be delivered to each member of that group. If we separate the\nlogic of handling messages from the core functionality of a group member,\nwe can conveniently make the distinction between receiving messages and\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n470 CHAPTER 8. FAULT TOLERANCE\nto kill them even if they can be located. All in all, this is not a promising\napproach.\nWith the second solution, called reincarnation , all these problems can\nbe solved without the need to write disk records. The way it works is to\ndivide time up into sequentially numbered epochs. When a client reboots,\nit broadcasts a message to all machines declaring the start of a new epoch.\nWhen such a broadcast comes in, all remote computations are killed. Of\ncourse, if the network is partitioned, some orphans may survive. Fortunately,\nhowever, when they report back, their replies will contain an obsolete epoch\nnumber, making them easy to detect.\nThe third solution is a variant on this idea, but somewhat less draconian.\nIt is called gentle reincarnation . When an epoch broadcast comes in, each\nmachine checks to see if it has any remote computations running locally, and\nif so, tries its best to locate their owners. Only if the owners cannot be located\nanywhere is the computation killed.\nIn the fourth solution, called expiration , each RPC is given a standard\namount of time, T, to do the job. If it cannot \ufb01nish, it must explicitly ask for\nanother quantum. Of course, this is quite a nuisance. On the other hand, if\nafter a crash the client waits a time Tbefore rebooting, all orphans are sure to\nbe gone. The problem to be solved here is choosing a reasonable value of Tin\nthe face of RPCs with wildly differing requirements.\nIn practice, all of these methods are crude and undesirable. Worse yet,\nkilling an orphan may have unforeseen consequences. For example, suppose\nthat an orphan has obtained locks on one or more \ufb01les or database records.\nIf the orphan is suddenly killed, these locks may remain forever. Also, an\norphan may have already made entries in various remote queues to start\nup other processes at some future time, so even killing the orphan may not\nremove all traces of it. Conceivably, it may even have started again, with\nunforeseen consequences. Orphan elimination is discussed in more detail\nby Panzieri and Shrivastava [1988].\n8.4 Reliable group communication\nConsidering how important process resilience by replication is, it is not sur-\nprising that reliable multicast services are important as well. Such services\nguarantee that messages are delivered to all members in a process group.\nUnfortunately, reliable multicasting turns out to be surprisingly tricky. In\nthis section, we take a closer look at the issues involved in reliably delivering\nmessages to a process group. Let us \ufb01rst de\ufb01ne what reliable group communi-\ncation actually is. Intuitively, it means that a message that is sent to a process\ngroup should be delivered to each member of that group. If we separate the\nlogic of handling messages from the core functionality of a group member,\nwe can conveniently make the distinction between receiving messages and\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 471\ndelivering messages, as illustrated in Figure 8.20. A message is received by\na message-handling component, which, in turn, delivers a message to the\ncomponent containing the core functionality of a group member. Informally,\na message that is received by process Pwill also be delivered by P.\nFigure 8.20: The distinction between receiving and delivering messages.\nAs an example, ensuring that messages from the same sender are delivered\nin the same order as they were sent, is typically taken care of by a message-\nhandling component. Likewise, providing reliable message-passing is a\nfeature that can and should be separated from the core functionality of a group\nmember, and is typically implemented by a message-handling component (if\nnot by the underlying operating system).\nWith this separation between receiving and delivering messages, we can be\nmore precise about what reliable group communication means. Let us make a\ndistinction between reliable communication in the presence of faulty processes,\nand reliable communication when processes are assumed to operate correctly.\nIn the \ufb01rst case, group communication is considered to be reliable when it can\nbe guaranteed that a message is received and subsequently delivered by all\nnonfaulty group members.\nThe tricky part here is that agreement should be reached on what the\ngroup actually looks like before a message can be delivered. If a sender\nintended to have a message delivered by each member of a group G, but that,\nfor whatever reason, at the time of delivery we actually have another group\nG06=G, we should ask ourselves if the message can be delivered or not.\nThe situation becomes simpler if we can ignore consensus on group mem-\nbership. In particular, let us \ufb01rst assume that a sending process has a list of\nintended recipients. In that case, it can simply deploy reliable transport-level\nprotocols such as TCP and, one by one, sends its message to each recipient. If\na receiving process fails, the message may be resent later when the process\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 471\ndelivering messages, as illustrated in Figure 8.20. A message is received by\na message-handling component, which, in turn, delivers a message to the\ncomponent containing the core functionality of a group member. Informally,\na message that is received by process Pwill also be delivered by P.\nFigure 8.20: The distinction between receiving and delivering messages.\nAs an example, ensuring that messages from the same sender are delivered\nin the same order as they were sent, is typically taken care of by a message-\nhandling component. Likewise, providing reliable message-passing is a\nfeature that can and should be separated from the core functionality of a group\nmember, and is typically implemented by a message-handling component (if\nnot by the underlying operating system).\nWith this separation between receiving and delivering messages, we can be\nmore precise about what reliable group communication means. Let us make a\ndistinction between reliable communication in the presence of faulty processes,\nand reliable communication when processes are assumed to operate correctly.\nIn the \ufb01rst case, group communication is considered to be reliable when it can\nbe guaranteed that a message is received and subsequently delivered by all\nnonfaulty group members.\nThe tricky part here is that agreement should be reached on what the\ngroup actually looks like before a message can be delivered. If a sender\nintended to have a message delivered by each member of a group G, but that,\nfor whatever reason, at the time of delivery we actually have another group\nG06=G, we should ask ourselves if the message can be delivered or not.\nThe situation becomes simpler if we can ignore consensus on group mem-\nbership. In particular, let us \ufb01rst assume that a sending process has a list of\nintended recipients. In that case, it can simply deploy reliable transport-level\nprotocols such as TCP and, one by one, sends its message to each recipient. If\na receiving process fails, the message may be resent later when the process\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "472 CHAPTER 8. FAULT TOLERANCE\nrecovers, or ignored altogether (for example, because the sender had left the\ngroup). In case a group member is expected to send a response, even if it\nis just an acknowledgement, communication can be speeded up by separat-\ning the sending of a request from receiving a response, as illustrated by the\nmessage sequence charts in Figure 8.21.\n(a)\n(b)\nFigure 8.21: (a) A sender sends out requests, but waits for a response before\nsending out the next one. (b) Requests are sent out in parallel, after which the\nsender waits for incoming responses.\nMost transport layers offer reliable point-to-point channels; they rarely\noffer reliable communication to a group of processes. The best they offer is to\nlet a process set up a point-to-point connection to each other process it wants\nto communicate with. When process groups are relatively small, this approach\nto establishing reliability is a straightforward and practical solution. On the\nother hand, we can often assume that the underlying communication system\ndoes offer unreliable multicasting, meaning that a multicast message may be\nlost part way and delivered by some, but not all, of the intended receivers.\nA simple solution to reach reliable group communication is shown in\nFigure 8.22. The sending process assigns a sequence number to each message\nit multicasts and stores the message locally in a history buffer. Assuming\nthe receivers are known to the sender, the sender simply keeps the message\nin its history buffer until each receiver has returned an acknowledgment.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n472 CHAPTER 8. FAULT TOLERANCE\nrecovers, or ignored altogether (for example, because the sender had left the\ngroup). In case a group member is expected to send a response, even if it\nis just an acknowledgement, communication can be speeded up by separat-\ning the sending of a request from receiving a response, as illustrated by the\nmessage sequence charts in Figure 8.21.\n(a)\n(b)\nFigure 8.21: (a) A sender sends out requests, but waits for a response before\nsending out the next one. (b) Requests are sent out in parallel, after which the\nsender waits for incoming responses.\nMost transport layers offer reliable point-to-point channels; they rarely\noffer reliable communication to a group of processes. The best they offer is to\nlet a process set up a point-to-point connection to each other process it wants\nto communicate with. When process groups are relatively small, this approach\nto establishing reliability is a straightforward and practical solution. On the\nother hand, we can often assume that the underlying communication system\ndoes offer unreliable multicasting, meaning that a multicast message may be\nlost part way and delivered by some, but not all, of the intended receivers.\nA simple solution to reach reliable group communication is shown in\nFigure 8.22. The sending process assigns a sequence number to each message\nit multicasts and stores the message locally in a history buffer. Assuming\nthe receivers are known to the sender, the sender simply keeps the message\nin its history buffer until each receiver has returned an acknowledgment.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 473\nA receiver can suspect it is missing a message mwith sequence number s\nwhen it has received messages with sequence numbers higher than s. In that\ncase, it returns a negative acknowledgement to the sender, requesting for a\nretransmission of m.\n(a)\n(b)\nFigure 8.22: A solution for reliable multicasting. (a) Message transmission.\n(b) Reporting feedback.\nThere are various design trade-offs to be made. For example, to reduce the\nnumber of messages returned to the sender, acknowledgments could possibly\nbe piggybacked with other messages. Also, retransmitting a message can be\ndone using point-to-point communication to each requesting process, or using\na single multicast message sent to all processes. General issues on reliable\nmulticasting are discussed by Popescu et al. [2007]. A survey and overview\nof reliable multicasting in the context of publish/subscribe systems, which is\nalso relevant here is [Esposito et al., 2013].\nNote 8.10 (Advanced: Scalability in reliable multicasting)\nThe main problem with the reliable multicast scheme just described is that it\ncannot support large numbers of receivers. If there are Nreceivers, the sender\nmust be prepared to accept at least Nacknowledgments. With many receivers,\nthe sender may be swamped with such feedback messages, which is also referred\nto as a feedback implosion . When replicating processes for fault tolerance, this\nsituation is not likely to occur as process groups are relatively small. When\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 473\nA receiver can suspect it is missing a message mwith sequence number s\nwhen it has received messages with sequence numbers higher than s. In that\ncase, it returns a negative acknowledgement to the sender, requesting for a\nretransmission of m.\n(a)\n(b)\nFigure 8.22: A solution for reliable multicasting. (a) Message transmission.\n(b) Reporting feedback.\nThere are various design trade-offs to be made. For example, to reduce the\nnumber of messages returned to the sender, acknowledgments could possibly\nbe piggybacked with other messages. Also, retransmitting a message can be\ndone using point-to-point communication to each requesting process, or using\na single multicast message sent to all processes. General issues on reliable\nmulticasting are discussed by Popescu et al. [2007]. A survey and overview\nof reliable multicasting in the context of publish/subscribe systems, which is\nalso relevant here is [Esposito et al., 2013].\nNote 8.10 (Advanced: Scalability in reliable multicasting)\nThe main problem with the reliable multicast scheme just described is that it\ncannot support large numbers of receivers. If there are Nreceivers, the sender\nmust be prepared to accept at least Nacknowledgments. With many receivers,\nthe sender may be swamped with such feedback messages, which is also referred\nto as a feedback implosion . When replicating processes for fault tolerance, this\nsituation is not likely to occur as process groups are relatively small. When\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "474 CHAPTER 8. FAULT TOLERANCE\nreplicating for performance, we have a different case. Moreover, we may then also\nneed to take into account that the receivers are spread across a wide-area network.\nOne solution to the problem of a feedback implosion is not to have receivers\nacknowledge the receipt of a message. Instead, a receiver returns a feedback\nmessage only to inform the sender it is missing a message. Returning only such\nnegative acknowledgments can be shown to generally scale better [Towsley et al.,\n1997], but no hard guarantees can be given that feedback implosions will never\nhappen.\nAnother problem with returning only negative acknowledgments is that\nthe sender will, in theory, be forced to keep a message in its history buffer\nforever. Because the sender can never know if a message has been correctly\ndelivered to all receivers, it should always be prepared for a receiver requesting\nthe retransmission of an old message. In practice, the sender will remove a\nmessage from its history buffer after some time has elapsed to prevent the buffer\nfrom over\ufb02owing. However, removing a message is done at the risk of a request\nfor a retransmission not being honored.\nSeveral proposals for scalable reliable multicasting exist. A comparison be-\ntween different schemes can be found in [Levine and Garcia-Luna-Aceves, 1998].\nWe now brie\ufb02y discuss two very different approaches that are representative of\nmany existing solutions.\nNonhierarchical feedback control The key issue to scalable solutions for reli-\nable multicasting is to reduce the number of feedback messages that are returned\nto the sender. A popular model that has been applied to several wide-area\napplications is feedback suppression . This scheme underlies the Scalable Reli-\nable Multicasting (SRM ) protocol developed by Floyd et al. [1997] and works as\nfollows.\nFirst, in SRM, receivers never acknowledge the successful delivery of a multi-\ncast message, but instead, report only when they are missing a message. How\nmessage loss is detected is left to the application. Only negative acknowledgments\nare returned as feedback. Whenever a receiver notices that it missed a message, it\nmulticasts its feedback to the rest of the group.\nMulticasting feedback allows another group member to suppress its own\nfeedback. Suppose several receivers missed message m. Each of them will need to\nreturn a negative acknowledgment to the sender, S, so that mcan be retransmitted.\nHowever, if we assume that retransmissions are always multicast to the entire\ngroup, it is suf\ufb01cient that only a single request for retransmission reaches S.\nFor this reason, a receiver Rthat did not receive message mschedules a\nfeedback message with some random delay. That is, the request for retransmission\nis not sent until some random time has elapsed. If, in the meantime, another\nrequest for retransmission for mreaches R,Rwill suppress its own feedback,\nknowing that mwill be retransmitted shortly. In this way, ideally, only a single\nfeedback message will reach S, which in turn subsequently retransmits m. This\nscheme is shown in Figure 8.23.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n474 CHAPTER 8. FAULT TOLERANCE\nreplicating for performance, we have a different case. Moreover, we may then also\nneed to take into account that the receivers are spread across a wide-area network.\nOne solution to the problem of a feedback implosion is not to have receivers\nacknowledge the receipt of a message. Instead, a receiver returns a feedback\nmessage only to inform the sender it is missing a message. Returning only such\nnegative acknowledgments can be shown to generally scale better [Towsley et al.,\n1997], but no hard guarantees can be given that feedback implosions will never\nhappen.\nAnother problem with returning only negative acknowledgments is that\nthe sender will, in theory, be forced to keep a message in its history buffer\nforever. Because the sender can never know if a message has been correctly\ndelivered to all receivers, it should always be prepared for a receiver requesting\nthe retransmission of an old message. In practice, the sender will remove a\nmessage from its history buffer after some time has elapsed to prevent the buffer\nfrom over\ufb02owing. However, removing a message is done at the risk of a request\nfor a retransmission not being honored.\nSeveral proposals for scalable reliable multicasting exist. A comparison be-\ntween different schemes can be found in [Levine and Garcia-Luna-Aceves, 1998].\nWe now brie\ufb02y discuss two very different approaches that are representative of\nmany existing solutions.\nNonhierarchical feedback control The key issue to scalable solutions for reli-\nable multicasting is to reduce the number of feedback messages that are returned\nto the sender. A popular model that has been applied to several wide-area\napplications is feedback suppression . This scheme underlies the Scalable Reli-\nable Multicasting (SRM ) protocol developed by Floyd et al. [1997] and works as\nfollows.\nFirst, in SRM, receivers never acknowledge the successful delivery of a multi-\ncast message, but instead, report only when they are missing a message. How\nmessage loss is detected is left to the application. Only negative acknowledgments\nare returned as feedback. Whenever a receiver notices that it missed a message, it\nmulticasts its feedback to the rest of the group.\nMulticasting feedback allows another group member to suppress its own\nfeedback. Suppose several receivers missed message m. Each of them will need to\nreturn a negative acknowledgment to the sender, S, so that mcan be retransmitted.\nHowever, if we assume that retransmissions are always multicast to the entire\ngroup, it is suf\ufb01cient that only a single request for retransmission reaches S.\nFor this reason, a receiver Rthat did not receive message mschedules a\nfeedback message with some random delay. That is, the request for retransmission\nis not sent until some random time has elapsed. If, in the meantime, another\nrequest for retransmission for mreaches R,Rwill suppress its own feedback,\nknowing that mwill be retransmitted shortly. In this way, ideally, only a single\nfeedback message will reach S, which in turn subsequently retransmits m. This\nscheme is shown in Figure 8.23.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 475\nFigure 8.23: Several receivers have scheduled a request for retransmission,\nbut the \ufb01rst retransmission request leads to the suppression of others.\nFeedback suppression has shown to scale reasonably well, and has been used\nas the underlying mechanism for a number of collaborative Internet applications,\nsuch as a shared whiteboard. However, the approach also introduces a number\nof serious problems. First, ensuring that only one request for retransmission is\nreturned to the sender requires a reasonably accurate scheduling of feedback\nmessages at each receiver. Otherwise, many receivers will still return their\nfeedback at the same time. Setting timers accordingly in a group of processes that\nis dispersed across a wide-area network is not that easy.\nAnother problem is that multicasting feedback also interrupts those processes\nto which the message has been successfully delivered. In other words, other\nreceivers are forced to receive and process messages that are useless to them. The\nonly solution to this problem is to let receivers that have not received message\nmjoin a separate multicast group for m, as explained by Kasera et al. [1997].\nUnfortunately, this solution requires that groups can be managed in a highly\nef\ufb01cient manner, which is hard to accomplish in a wide-area system. A better\napproach is therefore to let receivers that tend to miss the same messages team up\nand share the same multicast channel for feedback messages and retransmissions.\nDetails on this approach are found in [Liu et al., 1998].\nTo enhance the scalability of SRM, it is useful to let receivers assist in local\nrecovery. In particular, if a receiver to which message mhas been successfully\ndelivered, receives a request for retransmission, it can decide to multicast meven\nbefore the retransmission request reaches the original sender. Further details can\nbe found in [Floyd et al., 1997] and [Liu et al., 1998].\nHierarchical feedback control Feedback suppression as just described is basi-\ncally a nonhierarchical solution. However, achieving scalability for very large\ngroups of receivers requires that hierarchical approaches are adopted. A solution\nis shown in Figure 8.24. The group of receivers is partitioned into a number\nof subgroups, which are subsequently organized into a tree. Within each sub-\ngroup, any reliable multicasting scheme that works for small groups can be used.\nEach subgroup appoints a local coordinator, which represents that group in the\nmulticast tree. A link in the tree between two nodes corresponds to a reliable\nconnection between the coordinators of the respective subgroups.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 475\nFigure 8.23: Several receivers have scheduled a request for retransmission,\nbut the \ufb01rst retransmission request leads to the suppression of others.\nFeedback suppression has shown to scale reasonably well, and has been used\nas the underlying mechanism for a number of collaborative Internet applications,\nsuch as a shared whiteboard. However, the approach also introduces a number\nof serious problems. First, ensuring that only one request for retransmission is\nreturned to the sender requires a reasonably accurate scheduling of feedback\nmessages at each receiver. Otherwise, many receivers will still return their\nfeedback at the same time. Setting timers accordingly in a group of processes that\nis dispersed across a wide-area network is not that easy.\nAnother problem is that multicasting feedback also interrupts those processes\nto which the message has been successfully delivered. In other words, other\nreceivers are forced to receive and process messages that are useless to them. The\nonly solution to this problem is to let receivers that have not received message\nmjoin a separate multicast group for m, as explained by Kasera et al. [1997].\nUnfortunately, this solution requires that groups can be managed in a highly\nef\ufb01cient manner, which is hard to accomplish in a wide-area system. A better\napproach is therefore to let receivers that tend to miss the same messages team up\nand share the same multicast channel for feedback messages and retransmissions.\nDetails on this approach are found in [Liu et al., 1998].\nTo enhance the scalability of SRM, it is useful to let receivers assist in local\nrecovery. In particular, if a receiver to which message mhas been successfully\ndelivered, receives a request for retransmission, it can decide to multicast meven\nbefore the retransmission request reaches the original sender. Further details can\nbe found in [Floyd et al., 1997] and [Liu et al., 1998].\nHierarchical feedback control Feedback suppression as just described is basi-\ncally a nonhierarchical solution. However, achieving scalability for very large\ngroups of receivers requires that hierarchical approaches are adopted. A solution\nis shown in Figure 8.24. The group of receivers is partitioned into a number\nof subgroups, which are subsequently organized into a tree. Within each sub-\ngroup, any reliable multicasting scheme that works for small groups can be used.\nEach subgroup appoints a local coordinator, which represents that group in the\nmulticast tree. A link in the tree between two nodes corresponds to a reliable\nconnection between the coordinators of the respective subgroups.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "476 CHAPTER 8. FAULT TOLERANCE\nWhen a process Sin group Gwants to send a message, it simply uses the\nreliable multicast scheme for Gto reach all its members, including the group\u2019s\ncoordinator, say C.C, in turn, will forward the message to its neighboring\ncoordinators. As a general rule, a coordinator will forward an incoming message\nmto all its neighboring coordinators, except the one from which it received m.\nIn addition, a coordinator will reliably multicast the incoming message to all\nmembers of the subgroup it represents, and notably also handle retransmissions\nfor that group.\nIn an ACK-based scheme, if coordinator Cof group Gsends a message mto\ncoordinator C0of another, neighboring group G0, it will keep min its history buffer\nat least until C0has sent an acknowledgement. In a NACK-based scheme, only if\nG0detects it has missed m(and thus also all members of G0, and all coordinators\nto which G0would have forwarded m), it will send a nack message to C. It\nis thus seen that a single ack ornack message from a coordinator, aggregates\nmany feedback control messages from other processes, leading to a much more\nscalable reliable multicasting scheme. Scalability is further improved by letting a\ncoordinator handle the retransmissions to neighboring coordinators to which it\nhad forwarded a message.\nFigure 8.24: The essence of hierarchical reliable multicasting. Each local\ncoordinator forwards the message to its neighboring coordinators in the\ntree and later handles retransmission requests.\nNote that the nonhierarchical feedback control which we discussed before\ncan be used to improve the scalability of a single multicast group. Together\nwith hierarchical feedback control, we would combine relatively large reliable-\nmulticast subgroups into potentially large trees, thus being able to support reliable\nmulticasting for very large groups of processes.\nThe main problem with hierarchical solutions is the construction and manage-\nment of the tree: how are subgroups formed, which processes are appointed to\nbe coordinator, and how are the subgroups organized in a tree. In many cases, a\ntree needs to be constructed dynamically. Unfortunately, traditional network-level\nsolutions provide almost no adequate services for tree management. For this\nreason, application-level multicasting solutions as we discussed in Section 4.4\nhave gained popularity.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n476 CHAPTER 8. FAULT TOLERANCE\nWhen a process Sin group Gwants to send a message, it simply uses the\nreliable multicast scheme for Gto reach all its members, including the group\u2019s\ncoordinator, say C.C, in turn, will forward the message to its neighboring\ncoordinators. As a general rule, a coordinator will forward an incoming message\nmto all its neighboring coordinators, except the one from which it received m.\nIn addition, a coordinator will reliably multicast the incoming message to all\nmembers of the subgroup it represents, and notably also handle retransmissions\nfor that group.\nIn an ACK-based scheme, if coordinator Cof group Gsends a message mto\ncoordinator C0of another, neighboring group G0, it will keep min its history buffer\nat least until C0has sent an acknowledgement. In a NACK-based scheme, only if\nG0detects it has missed m(and thus also all members of G0, and all coordinators\nto which G0would have forwarded m), it will send a nack message to C. It\nis thus seen that a single ack ornack message from a coordinator, aggregates\nmany feedback control messages from other processes, leading to a much more\nscalable reliable multicasting scheme. Scalability is further improved by letting a\ncoordinator handle the retransmissions to neighboring coordinators to which it\nhad forwarded a message.\nFigure 8.24: The essence of hierarchical reliable multicasting. Each local\ncoordinator forwards the message to its neighboring coordinators in the\ntree and later handles retransmission requests.\nNote that the nonhierarchical feedback control which we discussed before\ncan be used to improve the scalability of a single multicast group. Together\nwith hierarchical feedback control, we would combine relatively large reliable-\nmulticast subgroups into potentially large trees, thus being able to support reliable\nmulticasting for very large groups of processes.\nThe main problem with hierarchical solutions is the construction and manage-\nment of the tree: how are subgroups formed, which processes are appointed to\nbe coordinator, and how are the subgroups organized in a tree. In many cases, a\ntree needs to be constructed dynamically. Unfortunately, traditional network-level\nsolutions provide almost no adequate services for tree management. For this\nreason, application-level multicasting solutions as we discussed in Section 4.4\nhave gained popularity.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 477\nGossip-based scalable reliable multicasting Finally, let us brie\ufb02y consider\ngossip-based multicasting schemes, in particular the following push-pull anti-\nentropy scheme that we discussed extensively in Section 4.4 .\nIn this scheme, a node Ppicks another node Qat random, and subsequently\nexchanges updates with Q. In other words, Ppushes updates that Qhas not seen\nbefore to Q, and pulls in any updates that Qhas, but which were missed by P.\nAfter the exchange, both processes have the same data. Clearly, this scheme is\nalready inherently robust, for if the communication between Pand Qfails for\nwhatever reason, Pwill simply pick some other node to exchange updates. The\nnet effect is that the speed by which an update propagates through the system\nslows down, but the reliability is affected only in extreme cases. Nevertheless,\nthis slowdown is considered important for some applications. In this light,\nthe comparison between traditional tree-based multicasting and gossip-based\nmulticasting for the purpose of aggregation as discussed by Nyers and Jelasity\n[2015] may be of interest.\nIn conclusion, building reliable multicast schemes that can scale to a large\nnumber of receivers spread across large networks, is a dif\ufb01cult problem. No single\nbest solution exists, and each solution introduces new problems.\nAtomic multicast\nLet us now return to the situation in which we need to achieve reliable\nmulticasting in the presence of process failures. In particular, what is often\nneeded in a distributed system is the guarantee that a message is delivered to\neither all group members or to none at all. This is also known as the atomic\nmulticast problem .\nTo see why atomicity is so important, consider a replicated database con-\nstructed as an application on top of a distributed system. The distributed\nsystem offers reliable multicasting facilities. In particular, it allows the con-\nstruction of process groups to which messages can be reliably sent. The\nreplicated database is therefore constructed as a group of processes, one pro-\ncess for each replica. Update operations are always multicast to all replicas\nand subsequently performed locally. We are thus assuming that an active-\nreplication protocol is being used.\nTo keep matters simple, assume a client contacts a replica Pand requests\nit to perform an update. The replica does so by multicasting the update to\nthe other group members. Unfortunately, before the multicast completes,\nPcrashes, leaving the rest of the group in a dif\ufb01cult position: some group\nmembers will have received the update request; others will not. If the members\nwho have received the request deliver it to the database, then obviously we will\nhave an inconsistent replicated database. Some replicas will have processed\nthe update, others will not. This situation needs to be avoided and we should\neither have that the update is delivered to all nonfaulty members, or to none\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 477\nGossip-based scalable reliable multicasting Finally, let us brie\ufb02y consider\ngossip-based multicasting schemes, in particular the following push-pull anti-\nentropy scheme that we discussed extensively in Section 4.4 .\nIn this scheme, a node Ppicks another node Qat random, and subsequently\nexchanges updates with Q. In other words, Ppushes updates that Qhas not seen\nbefore to Q, and pulls in any updates that Qhas, but which were missed by P.\nAfter the exchange, both processes have the same data. Clearly, this scheme is\nalready inherently robust, for if the communication between Pand Qfails for\nwhatever reason, Pwill simply pick some other node to exchange updates. The\nnet effect is that the speed by which an update propagates through the system\nslows down, but the reliability is affected only in extreme cases. Nevertheless,\nthis slowdown is considered important for some applications. In this light,\nthe comparison between traditional tree-based multicasting and gossip-based\nmulticasting for the purpose of aggregation as discussed by Nyers and Jelasity\n[2015] may be of interest.\nIn conclusion, building reliable multicast schemes that can scale to a large\nnumber of receivers spread across large networks, is a dif\ufb01cult problem. No single\nbest solution exists, and each solution introduces new problems.\nAtomic multicast\nLet us now return to the situation in which we need to achieve reliable\nmulticasting in the presence of process failures. In particular, what is often\nneeded in a distributed system is the guarantee that a message is delivered to\neither all group members or to none at all. This is also known as the atomic\nmulticast problem .\nTo see why atomicity is so important, consider a replicated database con-\nstructed as an application on top of a distributed system. The distributed\nsystem offers reliable multicasting facilities. In particular, it allows the con-\nstruction of process groups to which messages can be reliably sent. The\nreplicated database is therefore constructed as a group of processes, one pro-\ncess for each replica. Update operations are always multicast to all replicas\nand subsequently performed locally. We are thus assuming that an active-\nreplication protocol is being used.\nTo keep matters simple, assume a client contacts a replica Pand requests\nit to perform an update. The replica does so by multicasting the update to\nthe other group members. Unfortunately, before the multicast completes,\nPcrashes, leaving the rest of the group in a dif\ufb01cult position: some group\nmembers will have received the update request; others will not. If the members\nwho have received the request deliver it to the database, then obviously we will\nhave an inconsistent replicated database. Some replicas will have processed\nthe update, others will not. This situation needs to be avoided and we should\neither have that the update is delivered to all nonfaulty members, or to none\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "478 CHAPTER 8. FAULT TOLERANCE\nat all. The former case re\ufb02ects that Pcrashed after completing the multicast,\nwhile the latter represents Pcrashing before it even got a chance to request\nthe update.\nBoth these situations are \ufb01ne, and correspond to the case in which a client\ncommunicates with a single server that is allowed to crash. If a number\nof the group members would execute the update, while others would not,\ndistribution transparency is at stake, but even worse, the client would not\nknow what to make of the situation.\nVirtual synchrony\nReliable multicast in the presence of process failures can be accurately de\ufb01ned\nin terms of process groups and changes to group membership. As we did\nearlier, we make a distinction between receiving and delivering a message. In\nparticular, we again adopt a model in which the distributed system consists\nof message-handling components as was shown in Figure 8.20. A received\nmessage is locally buffered in this component until it can be delivered to the\napplication, which is logically placed as a group member at a higher layer.\nThe whole idea of atomic multicasting is that a multicast message mis\nuniquely associated with a list of processes that should deliver it. This delivery\nlist corresponds to a group view , namely, the view on the set of processes\ncontained in the group, which the sender had at the time message mwas\nmulticast. An important observation is that each process on that list has the\nsame view. In other words, they should all agree that mshould be delivered\nby each one of them and by no other process.\nNow suppose that message mis multicast at the time its sender, say P, has\ngroup view G. Furthermore, assume that while the multicast is taking place,\nanother process Qjoins or leaves the group. This change in group membership\nis naturally announced to all processes in G. Stated somewhat differently, a\nview change takes place by multicasting a message vcannouncing the joining\nor leaving of Q. We now have two multicast messages simultaneously in\ntransit: mand vc. What we need to guarantee is that mis either delivered by\nall processes in Gbefore any one executes the view change as speci\ufb01ed by\nvc, or mis not delivered at all. Note that this requirement is comparable to\ntotal-ordered multicasting, which we discussed in Chapter 6.\nA question that quickly comes to mind is that if mis not delivered by\nany process, how can we speak of a reliable multicast protocol? In principle,\nthere is only one case in which delivery of mis allowed to fail: when the\ngroup membership change is the result of the sender Pofmcrashing. In that\ncase, either all remaining (nonfaulty) members of Gshould deliver mbefore\nagreeing Pis no longer member of the group, or none should deliver m. As\nmentioned before, the latter corresponds to the situation that Pis considered\nto have crashed before it had a chance to send m.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n478 CHAPTER 8. FAULT TOLERANCE\nat all. The former case re\ufb02ects that Pcrashed after completing the multicast,\nwhile the latter represents Pcrashing before it even got a chance to request\nthe update.\nBoth these situations are \ufb01ne, and correspond to the case in which a client\ncommunicates with a single server that is allowed to crash. If a number\nof the group members would execute the update, while others would not,\ndistribution transparency is at stake, but even worse, the client would not\nknow what to make of the situation.\nVirtual synchrony\nReliable multicast in the presence of process failures can be accurately de\ufb01ned\nin terms of process groups and changes to group membership. As we did\nearlier, we make a distinction between receiving and delivering a message. In\nparticular, we again adopt a model in which the distributed system consists\nof message-handling components as was shown in Figure 8.20. A received\nmessage is locally buffered in this component until it can be delivered to the\napplication, which is logically placed as a group member at a higher layer.\nThe whole idea of atomic multicasting is that a multicast message mis\nuniquely associated with a list of processes that should deliver it. This delivery\nlist corresponds to a group view , namely, the view on the set of processes\ncontained in the group, which the sender had at the time message mwas\nmulticast. An important observation is that each process on that list has the\nsame view. In other words, they should all agree that mshould be delivered\nby each one of them and by no other process.\nNow suppose that message mis multicast at the time its sender, say P, has\ngroup view G. Furthermore, assume that while the multicast is taking place,\nanother process Qjoins or leaves the group. This change in group membership\nis naturally announced to all processes in G. Stated somewhat differently, a\nview change takes place by multicasting a message vcannouncing the joining\nor leaving of Q. We now have two multicast messages simultaneously in\ntransit: mand vc. What we need to guarantee is that mis either delivered by\nall processes in Gbefore any one executes the view change as speci\ufb01ed by\nvc, or mis not delivered at all. Note that this requirement is comparable to\ntotal-ordered multicasting, which we discussed in Chapter 6.\nA question that quickly comes to mind is that if mis not delivered by\nany process, how can we speak of a reliable multicast protocol? In principle,\nthere is only one case in which delivery of mis allowed to fail: when the\ngroup membership change is the result of the sender Pofmcrashing. In that\ncase, either all remaining (nonfaulty) members of Gshould deliver mbefore\nagreeing Pis no longer member of the group, or none should deliver m. As\nmentioned before, the latter corresponds to the situation that Pis considered\nto have crashed before it had a chance to send m.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 479\nFigure 8.25: The principle of virtual synchronous multicast.\nThis stronger form of reliable multicast guarantees that a message multicast\nto group view Gis delivered by each nonfaulty process in G. If the sender of\nthe message crashes during the multicast, the message is either delivered to\nall remaining processes, or ignored by each of them. Such a reliable multicast\nis said to be virtually synchronous [Birman and Joseph, 1987].\nTo illustrate these matters, consider the four processes shown in Figure 8.25.\nAt a certain point in time, we have a group consisting of S1,S2,S3, and\nS4. After some messages have been multicast, S3crashes. However, before\ncrashing, it succeeded in multicasting a message to processes S2and S4, but\nnot to S1. However, virtual synchrony in this case guarantees that the message\nis not delivered at all, effectively establishing the situation that the message\nwas never sent before S3crashed.\nAfter S3has been removed from the group, communication proceeds\nbetween the remaining group members. Later, when S3recovers, it can join\nthe group again, after its state has been brought up to date.\nThe principle of virtual synchrony comes from the fact that all multicasts\ntake place between view changes. Put somewhat differently, a view change\nacts as a barrier across which no multicast can pass. In a sense, it is comparable\nto the use of a synchronization variable in distributed data stores as discussed\nin the previous chapter. All multicasts that are in transit while a view change\ntakes place are completed before the view change comes into effect. The\nimplementation of virtual synchrony is not trivial as we discuss below.\nMessage Ordering\nVirtual synchrony allows an application developer to think about multicasts\nas taking place in epochs that are separated by group membership changes.\nHowever, nothing has yet been said concerning the ordering of multicasts. In\ngeneral, four different orderings are distinguished:\n1. Unordered multicasts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 479\nFigure 8.25: The principle of virtual synchronous multicast.\nThis stronger form of reliable multicast guarantees that a message multicast\nto group view Gis delivered by each nonfaulty process in G. If the sender of\nthe message crashes during the multicast, the message is either delivered to\nall remaining processes, or ignored by each of them. Such a reliable multicast\nis said to be virtually synchronous [Birman and Joseph, 1987].\nTo illustrate these matters, consider the four processes shown in Figure 8.25.\nAt a certain point in time, we have a group consisting of S1,S2,S3, and\nS4. After some messages have been multicast, S3crashes. However, before\ncrashing, it succeeded in multicasting a message to processes S2and S4, but\nnot to S1. However, virtual synchrony in this case guarantees that the message\nis not delivered at all, effectively establishing the situation that the message\nwas never sent before S3crashed.\nAfter S3has been removed from the group, communication proceeds\nbetween the remaining group members. Later, when S3recovers, it can join\nthe group again, after its state has been brought up to date.\nThe principle of virtual synchrony comes from the fact that all multicasts\ntake place between view changes. Put somewhat differently, a view change\nacts as a barrier across which no multicast can pass. In a sense, it is comparable\nto the use of a synchronization variable in distributed data stores as discussed\nin the previous chapter. All multicasts that are in transit while a view change\ntakes place are completed before the view change comes into effect. The\nimplementation of virtual synchrony is not trivial as we discuss below.\nMessage Ordering\nVirtual synchrony allows an application developer to think about multicasts\nas taking place in epochs that are separated by group membership changes.\nHowever, nothing has yet been said concerning the ordering of multicasts. In\ngeneral, four different orderings are distinguished:\n1. Unordered multicasts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "480 CHAPTER 8. FAULT TOLERANCE\n2. FIFO-ordered multicasts\n3. Causally ordered multicasts\n4. Totally ordered multicasts\nAreliable, unordered multicast is a virtually synchronous multicast in\nwhich no guarantees are given concerning the order in which received mes-\nsages are delivered by different processes. To explain, assume that reliable\nmulticasting is supported by a library providing a send and a receive primitive.\nThe receive operation blocks the caller until a message can be delivered.\nEvent order Process P 1Process P 2 Process P 3\n1 sends m1 receives m1receives m2\n2 sends m2 receives m2receives m1\nFigure 8.26: Three communicating processes in the same group. The ordering\nof events per process is shown along the vertical axis.\nNow suppose a sender P1multicasts two messages to a group while two\nother processes in that group are waiting for messages to arrive, as shown\nin Figure 8.26. Assuming that processes do not crash or leave the group\nduring these multicasts, it is possible that the message-handling component\natP2\ufb01rst receives message m1and then m2. Because there are no message-\nordering constraints, the messages may be delivered in the order that they are\nreceived. In contrast, the message-handling component at P3may \ufb01rst receive\nmessage m2followed by m1, and delivers these two in this same order to the\nhigher-level application of P3.\nIn the case of reliable FIFO-ordered multicasts , the message-handling\ncomponent layer is forced to deliver incoming messages from the same process\nin the same order as they have been sent. Consider the communication within\na group of four processes, as shown in Figure 8.27. With FIFO ordering,\nthe only thing that matters is that message m1is always delivered before\nm2, and, likewise, that message m3is always delivered before m4. This rule\nhas to be obeyed by all processes in the group. In other words, when the\ncommunication layer at P3receives m2\ufb01rst, it will wait with delivery to P3\nuntil it has received and delivered m1.\nHowever, there is no constraint regarding the delivery of messages sent\nby different processes. In other words, if process P2receives m1before m3, it\nmay deliver the two messages in that order. Meanwhile, process P3may have\nreceived m3before receiving m1. FIFO ordering states that P3may deliver m3\nbefore m1, although this delivery order is different from that of P2.\nFinally, reliable causally ordered multicast delivers messages so that\npotential causality between different messages is preserved. In other words, if\na message m1causally precedes another message m2, regardless of whether\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n480 CHAPTER 8. FAULT TOLERANCE\n2. FIFO-ordered multicasts\n3. Causally ordered multicasts\n4. Totally ordered multicasts\nAreliable, unordered multicast is a virtually synchronous multicast in\nwhich no guarantees are given concerning the order in which received mes-\nsages are delivered by different processes. To explain, assume that reliable\nmulticasting is supported by a library providing a send and a receive primitive.\nThe receive operation blocks the caller until a message can be delivered.\nEvent order Process P 1Process P 2 Process P 3\n1 sends m1 receives m1receives m2\n2 sends m2 receives m2receives m1\nFigure 8.26: Three communicating processes in the same group. The ordering\nof events per process is shown along the vertical axis.\nNow suppose a sender P1multicasts two messages to a group while two\nother processes in that group are waiting for messages to arrive, as shown\nin Figure 8.26. Assuming that processes do not crash or leave the group\nduring these multicasts, it is possible that the message-handling component\natP2\ufb01rst receives message m1and then m2. Because there are no message-\nordering constraints, the messages may be delivered in the order that they are\nreceived. In contrast, the message-handling component at P3may \ufb01rst receive\nmessage m2followed by m1, and delivers these two in this same order to the\nhigher-level application of P3.\nIn the case of reliable FIFO-ordered multicasts , the message-handling\ncomponent layer is forced to deliver incoming messages from the same process\nin the same order as they have been sent. Consider the communication within\na group of four processes, as shown in Figure 8.27. With FIFO ordering,\nthe only thing that matters is that message m1is always delivered before\nm2, and, likewise, that message m3is always delivered before m4. This rule\nhas to be obeyed by all processes in the group. In other words, when the\ncommunication layer at P3receives m2\ufb01rst, it will wait with delivery to P3\nuntil it has received and delivered m1.\nHowever, there is no constraint regarding the delivery of messages sent\nby different processes. In other words, if process P2receives m1before m3, it\nmay deliver the two messages in that order. Meanwhile, process P3may have\nreceived m3before receiving m1. FIFO ordering states that P3may deliver m3\nbefore m1, although this delivery order is different from that of P2.\nFinally, reliable causally ordered multicast delivers messages so that\npotential causality between different messages is preserved. In other words, if\na message m1causally precedes another message m2, regardless of whether\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.4. RELIABLE GROUP COMMUNICATION 481\nEvent order Process P 1Process P 2 Process P 3Process P 4\n1 sends m1 receives m1receives m3 sends m3\n2 sends m2 receives m3receives m1 sends m4\n3 receives m2receives m2\n4 receives m4receives m4\nFigure 8.27: Four processes in the same group with two different senders, and\na possible delivery order of messages under FIFO-ordered multicasting.\nthey were multicast by the same sender, then the communication layer at each\nreceiver will always deliver m2after it has received and delivered m1. Note\nthat causally ordered multicasts can be implemented using vector timestamps\nas discussed in Chapter 6.\nBesides these three orderings, there may be the additional constraint that\nmessage delivery is to be totally ordered as well. Total-ordered delivery\nmeans that regardless of whether message delivery is unordered, FIFO or-\ndered, or causally ordered, it is required additionally that when messages are\ndelivered, they are delivered in the same order to all group members.\nFor example, with the combination of FIFO and totally ordered multicast,\nprocesses P2and P3in Figure 8.27 may both \ufb01rst deliver message m3and\nthen message m1. However, if P2delivers m1before m3, while P3delivers m3\nbefore delivering m1, they would violate the total-ordering constraint. Note\nthat FIFO ordering should still be respected. In other words, m2should be\ndelivered after m1and, accordingly, m4should be delivered after m3.\nVirtually synchronous reliable multicasting offering total-ordered delivery\nof messages is called atomic multicasting . With the three different mes-\nsage ordering constraints discussed above, this leads to six forms of reliable\nmulticasting as shown in Figure 8.28 [Hadzilacos and Toueg, 1993].\nMulticast Basic message ordering TO delivery?\nReliable multicast None No\nFIFO multicast FIFO-ordered delivery No\nCausal multicast Causal-ordered delivery No\nAtomic multicast None Y es\nFIFO atomic multicast FIFO-ordered delivery Y es\nCausal atomic multicast Causal-ordered delivery Y es\nFigure 8.28: Six different versions of virtually synchronous reliable multicast-\ning and considering totally ordered delivery.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.4. RELIABLE GROUP COMMUNICATION 481\nEvent order Process P 1Process P 2 Process P 3Process P 4\n1 sends m1 receives m1receives m3 sends m3\n2 sends m2 receives m3receives m1 sends m4\n3 receives m2receives m2\n4 receives m4receives m4\nFigure 8.27: Four processes in the same group with two different senders, and\na possible delivery order of messages under FIFO-ordered multicasting.\nthey were multicast by the same sender, then the communication layer at each\nreceiver will always deliver m2after it has received and delivered m1. Note\nthat causally ordered multicasts can be implemented using vector timestamps\nas discussed in Chapter 6.\nBesides these three orderings, there may be the additional constraint that\nmessage delivery is to be totally ordered as well. Total-ordered delivery\nmeans that regardless of whether message delivery is unordered, FIFO or-\ndered, or causally ordered, it is required additionally that when messages are\ndelivered, they are delivered in the same order to all group members.\nFor example, with the combination of FIFO and totally ordered multicast,\nprocesses P2and P3in Figure 8.27 may both \ufb01rst deliver message m3and\nthen message m1. However, if P2delivers m1before m3, while P3delivers m3\nbefore delivering m1, they would violate the total-ordering constraint. Note\nthat FIFO ordering should still be respected. In other words, m2should be\ndelivered after m1and, accordingly, m4should be delivered after m3.\nVirtually synchronous reliable multicasting offering total-ordered delivery\nof messages is called atomic multicasting . With the three different mes-\nsage ordering constraints discussed above, this leads to six forms of reliable\nmulticasting as shown in Figure 8.28 [Hadzilacos and Toueg, 1993].\nMulticast Basic message ordering TO delivery?\nReliable multicast None No\nFIFO multicast FIFO-ordered delivery No\nCausal multicast Causal-ordered delivery No\nAtomic multicast None Y es\nFIFO atomic multicast FIFO-ordered delivery Y es\nCausal atomic multicast Causal-ordered delivery Y es\nFigure 8.28: Six different versions of virtually synchronous reliable multicast-\ning and considering totally ordered delivery.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "482 CHAPTER 8. FAULT TOLERANCE\nNote 8.11 (Advanced: Implementing virtual synchrony)\nLet us now consider a possible implementation of a virtually synchronous reliable\nmulticast. An example of such an implementation appears in Isis, a fault-tolerant\ndistributed system that has been in practical use in industry for several years. We\nwill focus on some of the implementation issues of this technique as described in\nBirman et al. [1991].\nReliable multicasting in Isis makes use of available reliable point-to-point com-\nmunication facilities of the underlying network, in particular, TCP . Multicasting a\nmessage mto a group of processes is implemented by reliably sending mto each\ngroup member. As a consequence, although each transmission is guaranteed to\nsucceed, there are no guarantees that allgroup members receive m. In particular,\nthe sender may fail before having transmitted mto each member.\nBesides reliable point-to-point communication, Isis also assumes that messages\nfrom the same source are received by a communication layer in the order they\nwere sent by that source. In practice, this requirement is solved by using TCP\nconnections for point-to-point communication.\nThe main problem that needs to be solved is to guarantee that all messages\nsent to view Gare delivered to all nonfaulty processes in Gbefore the next group\nmembership change takes place. The \ufb01rst issue that needs to be taken care of\nis making sure that each process in Ghas received all messages that were sent\ntoG. Note that because the sender of a message mtoGmay have failed before\ncompleting its multicast, there may indeed be processes in Gthat will never\nreceive m. Because the sender has crashed, these processes should get mfrom\nsomewhere else.\nThe solution to this problem is to let every process in Gkeep muntil it knows\nfor sure that all members in Ghave received it. If mhas been received by all\nmembers in G,mis said to be stable . Only stable messages are allowed to be\ndelivered. To ensure stability, it is suf\ufb01cient to select an arbitrary live process in\nGand request it to send mto all other processes in G.\nTo be more speci\ufb01c, assume the current view is Gi, but that it is necessary\nto install the next view Gi+1. Without loss of generality, we assume that Giand\nGi+1differ by at most one process. A process Pnotices the view change when\nit receives a view-change message. Such a message may come from the process\nwanting to join or leave the group, or from a process that had detected the failure\nof a process in Githat is now to be removed, as shown in Figure 8.29(a).\nWhen a process Preceives the view-change message for Gi+1, it \ufb01rst forwards\na copy of any unstable message from Giit still has to every process in Gi+1, and\nsubsequently marks it as being stable. Recall that Isis assumes point-to-point\ncommunication is reliable, so that forwarded messages are never lost. Such\nforwarding guarantees that all messages in Githat have been received by at least\none process are received by all nonfaulty processes in Gi. Note that it would also\nhave been suf\ufb01cient to elect a single coordinator to forward unstable messages.\nTo indicate that Pno longer has any unstable messages and that it is prepared\nto install Gi+1as soon as the other processes can do that as well, it multicasts a\n\ufb02ush message forGi+1, as shown in Figure 8.29(b). After Phas received a \ufb02ush\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n482 CHAPTER 8. FAULT TOLERANCE\nNote 8.11 (Advanced: Implementing virtual synchrony)\nLet us now consider a possible implementation of a virtually synchronous reliable\nmulticast. An example of such an implementation appears in Isis, a fault-tolerant\ndistributed system that has been in practical use in industry for several years. We\nwill focus on some of the implementation issues of this technique as described in\nBirman et al. [1991].\nReliable multicasting in Isis makes use of available reliable point-to-point com-\nmunication facilities of the underlying network, in particular, TCP . Multicasting a\nmessage mto a group of processes is implemented by reliably sending mto each\ngroup member. As a consequence, although each transmission is guaranteed to\nsucceed, there are no guarantees that allgroup members receive m. In particular,\nthe sender may fail before having transmitted mto each member.\nBesides reliable point-to-point communication, Isis also assumes that messages\nfrom the same source are received by a communication layer in the order they\nwere sent by that source. In practice, this requirement is solved by using TCP\nconnections for point-to-point communication.\nThe main problem that needs to be solved is to guarantee that all messages\nsent to view Gare delivered to all nonfaulty processes in Gbefore the next group\nmembership change takes place. The \ufb01rst issue that needs to be taken care of\nis making sure that each process in Ghas received all messages that were sent\ntoG. Note that because the sender of a message mtoGmay have failed before\ncompleting its multicast, there may indeed be processes in Gthat will never\nreceive m. Because the sender has crashed, these processes should get mfrom\nsomewhere else.\nThe solution to this problem is to let every process in Gkeep muntil it knows\nfor sure that all members in Ghave received it. If mhas been received by all\nmembers in G,mis said to be stable . Only stable messages are allowed to be\ndelivered. To ensure stability, it is suf\ufb01cient to select an arbitrary live process in\nGand request it to send mto all other processes in G.\nTo be more speci\ufb01c, assume the current view is Gi, but that it is necessary\nto install the next view Gi+1. Without loss of generality, we assume that Giand\nGi+1differ by at most one process. A process Pnotices the view change when\nit receives a view-change message. Such a message may come from the process\nwanting to join or leave the group, or from a process that had detected the failure\nof a process in Githat is now to be removed, as shown in Figure 8.29(a).\nWhen a process Preceives the view-change message for Gi+1, it \ufb01rst forwards\na copy of any unstable message from Giit still has to every process in Gi+1, and\nsubsequently marks it as being stable. Recall that Isis assumes point-to-point\ncommunication is reliable, so that forwarded messages are never lost. Such\nforwarding guarantees that all messages in Githat have been received by at least\none process are received by all nonfaulty processes in Gi. Note that it would also\nhave been suf\ufb01cient to elect a single coordinator to forward unstable messages.\nTo indicate that Pno longer has any unstable messages and that it is prepared\nto install Gi+1as soon as the other processes can do that as well, it multicasts a\n\ufb02ush message forGi+1, as shown in Figure 8.29(b). After Phas received a \ufb02ush\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.5. DISTRIBUTED COMMIT 483\nmessage for Gi+1from each other process, it can safely install the new view, as\nshown in Figure 8.29(c).\n(a) (b) (c)\nFigure 8.29: (a) Process 4 notices that process 7 has crashed and sends a\nview change. (b) Process 6 sends out all its unstable messages, followed by\na \ufb02ush message. (c) Process 6 installs the new view when it has received\na \ufb02ush message from everyone else.\nWhen a process Qreceives a message mwhile Qstill believes the current\nview is Gi, it delivers mtaking any additional message-ordering constraints into\naccount. If it had already received m, it considers the message to be a duplicate\nand discards it.\nBecause process Qwill eventually receive the view-change message for Gi+1,\nit will also \ufb01rst forward any of its unstable messages and subsequently wrap\nthings up by sending a \ufb02ush message for Gi+1. Note that due to the assumed\nFIFO-message ordering as provided by the underlying communication layer, a\n\ufb02ush message from a process is always received after the receipt of an unstable\nmessage from that same process.\nThe major \ufb02aw in the protocol described so far is that it cannot deal with\nprocess failures while a new view change is being announced. In particular, it\nassumes that until the new view Gi+1has been installed by each member in\nGi+1, no process in Gi+1will fail (which would lead to a next view Gi+2). This\nproblem is solved by announcing view changes for any view Gi+keven while\nprevious changes have not yet been installed by all processes. The details are\nrather intricate, yet the principle should be clear.\n8.5 Distributed commit\nThe atomic multicasting problem discussed in the previous section is an\nexample of a more general problem, known as distributed commit . The\ndistributed commit problem involves having an operation being performed\nby each member of a process group, or none at all. In the case of reliable\nmulticasting, the operation is the delivery of a message. With distributed\ntransactions, the operation may be the commit of a transaction at a single site\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.5. DISTRIBUTED COMMIT 483\nmessage for Gi+1from each other process, it can safely install the new view, as\nshown in Figure 8.29(c).\n(a) (b) (c)\nFigure 8.29: (a) Process 4 notices that process 7 has crashed and sends a\nview change. (b) Process 6 sends out all its unstable messages, followed by\na \ufb02ush message. (c) Process 6 installs the new view when it has received\na \ufb02ush message from everyone else.\nWhen a process Qreceives a message mwhile Qstill believes the current\nview is Gi, it delivers mtaking any additional message-ordering constraints into\naccount. If it had already received m, it considers the message to be a duplicate\nand discards it.\nBecause process Qwill eventually receive the view-change message for Gi+1,\nit will also \ufb01rst forward any of its unstable messages and subsequently wrap\nthings up by sending a \ufb02ush message for Gi+1. Note that due to the assumed\nFIFO-message ordering as provided by the underlying communication layer, a\n\ufb02ush message from a process is always received after the receipt of an unstable\nmessage from that same process.\nThe major \ufb02aw in the protocol described so far is that it cannot deal with\nprocess failures while a new view change is being announced. In particular, it\nassumes that until the new view Gi+1has been installed by each member in\nGi+1, no process in Gi+1will fail (which would lead to a next view Gi+2). This\nproblem is solved by announcing view changes for any view Gi+keven while\nprevious changes have not yet been installed by all processes. The details are\nrather intricate, yet the principle should be clear.\n8.5 Distributed commit\nThe atomic multicasting problem discussed in the previous section is an\nexample of a more general problem, known as distributed commit . The\ndistributed commit problem involves having an operation being performed\nby each member of a process group, or none at all. In the case of reliable\nmulticasting, the operation is the delivery of a message. With distributed\ntransactions, the operation may be the commit of a transaction at a single site\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "484 CHAPTER 8. FAULT TOLERANCE\nthat takes part in the transaction. Other examples of distributed commit, and\nhow it can be solved are discussed by Tanisch [2000].\nDistributed commit is often established by means of a coordinator. In a\nsimple scheme, this coordinator tells all other processes that are also involved,\ncalled participants, whether or not to (locally) perform the operation in\nquestion. This scheme is referred to as a one-phase commit protocol . It has\nthe obvious drawback that if one of the participants cannot actually perform\nthe operation, there is no way to tell the coordinator. For example, in the case\nof distributed transactions, a local commit may not be possible because this\nwould violate concurrency control constraints.\nIn practice, more sophisticated schemes are needed, the most common one\nbeing the two-phase commit protocol, which we discuss in detail below. The\nmain drawback of this protocol is that it cannot generally ef\ufb01ciently handle\nthe failure of the coordinator. To that end, a three-phase protocol has been\ndeveloped, which we discuss separately in Note 8.13.\nThe original two-phase commit protocol (2PC) is due to Gray [1978].\nWithout loss of generality, consider a distributed transaction involving the\nparticipation of a number of processes each running on a different machine.\nAssuming that no failures occur, the protocol consists of the following two\nphases, each consisting of two steps (see also Bernstein and Newcomer [2009]):\n1. The coordinator sends a vote -request message to all participants.\n2.When a participant receives a vote -request message, it returns either a\nvote -commit message to the coordinator telling the coordinator that it\nis prepared to locally commit its part of the transaction, or otherwise a\nvote -abort message.\n3.The coordinator collects all votes from the participants. If all participants\nhave voted to commit the transaction, then so will the coordinator. In that\ncase, it sends a global -commit message to all participants. However,\nif one participant had voted to abort the transaction, the coordinator\nwill also decide to abort the transaction and multicasts a global -abort\nmessage.\n4.Each participant that voted for a commit waits for the \ufb01nal reaction\nby the coordinator. If a participant receives a global -commit mes-\nsage, it locally commits the transaction. Otherwise, when receiving a\nglobal -abort message, the transaction is locally aborted as well.\nThe \ufb01rst phase is the voting phase, and consists of steps 1 and 2. The second\nphase is the decision phase, and consists of steps 3 and 4. These four steps\nare shown as \ufb01nite state diagrams in Figure 8.30.\nSeveral problems arise when this basic 2PC protocol is used in a system\nwhere failures occur. First, note that the coordinator as well as the participants\nhave states in which they block waiting for incoming messages. Consequently,\nthe protocol can easily fail when a process crashes for other processes may be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n484 CHAPTER 8. FAULT TOLERANCE\nthat takes part in the transaction. Other examples of distributed commit, and\nhow it can be solved are discussed by Tanisch [2000].\nDistributed commit is often established by means of a coordinator. In a\nsimple scheme, this coordinator tells all other processes that are also involved,\ncalled participants, whether or not to (locally) perform the operation in\nquestion. This scheme is referred to as a one-phase commit protocol . It has\nthe obvious drawback that if one of the participants cannot actually perform\nthe operation, there is no way to tell the coordinator. For example, in the case\nof distributed transactions, a local commit may not be possible because this\nwould violate concurrency control constraints.\nIn practice, more sophisticated schemes are needed, the most common one\nbeing the two-phase commit protocol, which we discuss in detail below. The\nmain drawback of this protocol is that it cannot generally ef\ufb01ciently handle\nthe failure of the coordinator. To that end, a three-phase protocol has been\ndeveloped, which we discuss separately in Note 8.13.\nThe original two-phase commit protocol (2PC) is due to Gray [1978].\nWithout loss of generality, consider a distributed transaction involving the\nparticipation of a number of processes each running on a different machine.\nAssuming that no failures occur, the protocol consists of the following two\nphases, each consisting of two steps (see also Bernstein and Newcomer [2009]):\n1. The coordinator sends a vote -request message to all participants.\n2.When a participant receives a vote -request message, it returns either a\nvote -commit message to the coordinator telling the coordinator that it\nis prepared to locally commit its part of the transaction, or otherwise a\nvote -abort message.\n3.The coordinator collects all votes from the participants. If all participants\nhave voted to commit the transaction, then so will the coordinator. In that\ncase, it sends a global -commit message to all participants. However,\nif one participant had voted to abort the transaction, the coordinator\nwill also decide to abort the transaction and multicasts a global -abort\nmessage.\n4.Each participant that voted for a commit waits for the \ufb01nal reaction\nby the coordinator. If a participant receives a global -commit mes-\nsage, it locally commits the transaction. Otherwise, when receiving a\nglobal -abort message, the transaction is locally aborted as well.\nThe \ufb01rst phase is the voting phase, and consists of steps 1 and 2. The second\nphase is the decision phase, and consists of steps 3 and 4. These four steps\nare shown as \ufb01nite state diagrams in Figure 8.30.\nSeveral problems arise when this basic 2PC protocol is used in a system\nwhere failures occur. First, note that the coordinator as well as the participants\nhave states in which they block waiting for incoming messages. Consequently,\nthe protocol can easily fail when a process crashes for other processes may be\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.5. DISTRIBUTED COMMIT 485\n(a) (b)\nFigure 8.30: (a) The \ufb01nite state machine for the coordinator in 2PC. (b) The\n\ufb01nite state machine for a participant.\ninde\ufb01nitely waiting for a message from that process. For this reason, timeout\nmechanisms are used. These mechanisms are explained in the following\npages.\nWhen taking a look at the \ufb01nite state machines in Figure 8.30, it can be seen\nthat there are a total of three states in which either a coordinator or participant\nis blocked waiting for an incoming message. First, a participant may be\nwaiting in its INIT state for a vote -request message from the coordinator.\nIf that message is not received after some time, the participant will simply\ndecide to locally abort the transaction, and thus send a vote -abort message\nto the coordinator.\nLikewise, the coordinator can be blocked in state WAIT , waiting for the\nvotes of each participant. If not all votes have been collected after a cer-\ntain period of time, the coordinator should vote for an abort as well, and\nsubsequently send global -abort to all participants.\nFinally, a participant can be blocked in state READY , waiting for the global\nvote as sent by the coordinator. If that message is not received within a given\ntime, the participant cannot simply decide to abort the transaction. Instead,\nit must \ufb01nd out which message the coordinator actually sent. The simplest\nsolution to this problem is to let each participant block until the coordinator\nrecovers again.\nA better solution is to let a participant Pcontact another participant Qto\nsee if it can decide from Q\u2019s current state what it should do. For example,\nsuppose that Qhad reached state COMMIT . This is possible only if the\ncoordinator had sent a global -commit message to Qjust before crashing.\nApparently, this message had not yet been sent to P. Consequently, Pmay\nnow also decide to locally commit. Likewise, if Qis in state ABORT ,Pcan\nsafely abort as well.\nNow suppose that Qis still in state INIT . This situation can occur when\nthe coordinator has sent vote -request to all participants, but this message\nhas reached P(which subsequently responded with a vote -commit message),\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.5. DISTRIBUTED COMMIT 485\n(a) (b)\nFigure 8.30: (a) The \ufb01nite state machine for the coordinator in 2PC. (b) The\n\ufb01nite state machine for a participant.\ninde\ufb01nitely waiting for a message from that process. For this reason, timeout\nmechanisms are used. These mechanisms are explained in the following\npages.\nWhen taking a look at the \ufb01nite state machines in Figure 8.30, it can be seen\nthat there are a total of three states in which either a coordinator or participant\nis blocked waiting for an incoming message. First, a participant may be\nwaiting in its INIT state for a vote -request message from the coordinator.\nIf that message is not received after some time, the participant will simply\ndecide to locally abort the transaction, and thus send a vote -abort message\nto the coordinator.\nLikewise, the coordinator can be blocked in state WAIT , waiting for the\nvotes of each participant. If not all votes have been collected after a cer-\ntain period of time, the coordinator should vote for an abort as well, and\nsubsequently send global -abort to all participants.\nFinally, a participant can be blocked in state READY , waiting for the global\nvote as sent by the coordinator. If that message is not received within a given\ntime, the participant cannot simply decide to abort the transaction. Instead,\nit must \ufb01nd out which message the coordinator actually sent. The simplest\nsolution to this problem is to let each participant block until the coordinator\nrecovers again.\nA better solution is to let a participant Pcontact another participant Qto\nsee if it can decide from Q\u2019s current state what it should do. For example,\nsuppose that Qhad reached state COMMIT . This is possible only if the\ncoordinator had sent a global -commit message to Qjust before crashing.\nApparently, this message had not yet been sent to P. Consequently, Pmay\nnow also decide to locally commit. Likewise, if Qis in state ABORT ,Pcan\nsafely abort as well.\nNow suppose that Qis still in state INIT . This situation can occur when\nthe coordinator has sent vote -request to all participants, but this message\nhas reached P(which subsequently responded with a vote -commit message),\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "486 CHAPTER 8. FAULT TOLERANCE\nState of Q Action by P\nCOMMIT Make transition to COMMIT\nABORT Make transition to ABORT\nINIT Make transition to ABORT\nREADY Contact another participant\nFigure 8.31: Actions taken by a participant Pwhen residing in state READY\nand having contacted another participant Q.\nbut has not reached Q. In other words, the coordinator had crashed while\nmulticasting vote -request . In this case, it is safe to abort the transaction:\nboth Pand Qcan make a transition to state ABORT .\nThe most dif\ufb01cult situation occurs when Qis also in state READY , waiting\nfor a response from the coordinator. In particular, if it turns out that all\nparticipants are in state READY , no decision can be taken. The problem\nis that although all participants are willing to commit, they still need the\ncoordinator\u2019s vote to reach the \ufb01nal decision. Consequently, the protocol\nblocks until the coordinator recovers.\nThe various options are summarized in Figure 8.31.\nTo ensure that a process can actually recover, it is necessary that it saves\nits state to persistent storage. For example, if a participant was in state INIT ,\nit can safely decide to locally abort the transaction when it recovers, and\nthen inform the coordinator. Likewise, when it had already taken a decision\nsuch as when it crashed while being in either state COMMIT orABORT , it\nis in order to recover to that state again, and retransmit its decision to the\ncoordinator.\nProblems arise when a participant crashed while residing in state READY .\nIn that case, when recovering, it cannot decide on its own what it should do\nnext, that is, commit or abort the transaction. Consequently, it is forced to\ncontact other participants to \ufb01nd what it should do, analogous to the situation\nwhen it times out while residing in state READY as described above.\nThe coordinator has only two critical states it needs to keep track of. When\nit starts the 2PC protocol, it should record that it is entering state WAIT so\nthat it can possibly retransmit the vote -request message to all participants\nafter recovering. Likewise, if it had come to a decision in the second phase, it\nis suf\ufb01cient if that decision has been recorded so that it can be retransmitted\nwhen recovering.\nIt may thus be possible that a participant will need to block until the\ncoordinator recovers. This situation occurs when all participants have received\nand processed the vote -request message from the coordinator, while in\nthe meantime, the coordinator crashed. In that case, participants cannot\ncooperatively decide on the \ufb01nal action to take. For this reason, 2PC is also\nreferred to as a blocking commit protocol .\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n486 CHAPTER 8. FAULT TOLERANCE\nState of Q Action by P\nCOMMIT Make transition to COMMIT\nABORT Make transition to ABORT\nINIT Make transition to ABORT\nREADY Contact another participant\nFigure 8.31: Actions taken by a participant Pwhen residing in state READY\nand having contacted another participant Q.\nbut has not reached Q. In other words, the coordinator had crashed while\nmulticasting vote -request . In this case, it is safe to abort the transaction:\nboth Pand Qcan make a transition to state ABORT .\nThe most dif\ufb01cult situation occurs when Qis also in state READY , waiting\nfor a response from the coordinator. In particular, if it turns out that all\nparticipants are in state READY , no decision can be taken. The problem\nis that although all participants are willing to commit, they still need the\ncoordinator\u2019s vote to reach the \ufb01nal decision. Consequently, the protocol\nblocks until the coordinator recovers.\nThe various options are summarized in Figure 8.31.\nTo ensure that a process can actually recover, it is necessary that it saves\nits state to persistent storage. For example, if a participant was in state INIT ,\nit can safely decide to locally abort the transaction when it recovers, and\nthen inform the coordinator. Likewise, when it had already taken a decision\nsuch as when it crashed while being in either state COMMIT orABORT , it\nis in order to recover to that state again, and retransmit its decision to the\ncoordinator.\nProblems arise when a participant crashed while residing in state READY .\nIn that case, when recovering, it cannot decide on its own what it should do\nnext, that is, commit or abort the transaction. Consequently, it is forced to\ncontact other participants to \ufb01nd what it should do, analogous to the situation\nwhen it times out while residing in state READY as described above.\nThe coordinator has only two critical states it needs to keep track of. When\nit starts the 2PC protocol, it should record that it is entering state WAIT so\nthat it can possibly retransmit the vote -request message to all participants\nafter recovering. Likewise, if it had come to a decision in the second phase, it\nis suf\ufb01cient if that decision has been recorded so that it can be retransmitted\nwhen recovering.\nIt may thus be possible that a participant will need to block until the\ncoordinator recovers. This situation occurs when all participants have received\nand processed the vote -request message from the coordinator, while in\nthe meantime, the coordinator crashed. In that case, participants cannot\ncooperatively decide on the \ufb01nal action to take. For this reason, 2PC is also\nreferred to as a blocking commit protocol .\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.5. DISTRIBUTED COMMIT 487\nThere are several solutions to avoid blocking. One solution is to use a\nmulticast primitive by which a receiver immediately multicasts a received\nmessage to all other processes [Babaoglu and Toueg, 1993]. It can be shown\nthat this approach allows a participant to reach a \ufb01nal decision, even if the\ncoordinator has not yet recovered. Another solution is to use three instead of\ntwo phases, as we discuss in Note 8.13.\nNote 8.12 (Advanced: 2PC outlined in Python)\nAn outline of the actions that are executed by the coordinator is given in Fig-\nure 8.32. The coordinator starts by multicasting a vote -request to all participants\nin order to collect their votes. It subsequently records that it is entering the WAIT\nstate, after which it waits for incoming votes from participants.\n1class Coordinator:\n2\n3 defrun(self):\n4 yetToReceive = list(participants)\n5 self.log.info(\u2019WAIT\u2019)\n6 self.chan.sendTo(participants, VOTE_REQUEST)\n7 while len (yetToReceive) > 0:\n8 msg = self.chan.recvFrom(participants, TIMEOUT)\n9 if(notmsg) or(msg[1] == VOTE_ABORT):\n10 self.log.info(\u2019ABORT\u2019)\n11 self.chan.sendTo(participants, GLOBAL_ABORT)\n12 return\n13 else: # msg[1] == VOTE_COMMIT\n14 yetToReceive.remove(msg[0])\n15 self.log.info(\u2019COMMIT\u2019)\n16 self.chan.sendTo(participants, GLOBAL_COMMIT)\nFigure 8.32: The steps taken by the coordinator in a 2PC protocol.\nIf not all votes have been collected but no more votes are received within\na given time interval prescribed in advance, the coordinator assumes that one\nor more participants have failed. Consequently, it should abort the transaction\nand multicasts a global -abort to the (remaining) participants. Likewise, if only\na single participant decides to abort the transaction, the coordinator will have\nto call off the transaction. If all participants vote to commit, global -commit is\n\ufb01rst logged and subsequently sent to all processes. Otherwise, the coordinator\nmulticasts a global -abort (after recording it in the local log).\nAfter receiving a vote request, the participant does its work. All bets are\noff if its work failed, but otherwise, it will vote for committing the transaction.\nIt records its decision in a local log and informs the coordinator by sending a\nvote -commit message. The participant must then wait for the global decision.\nAssuming this decision (which again should come from the coordinator) comes in\non time, it is simply written to the local log, after which it can be carried out (the\nlatter is not shown in the code).\nFigure 8.33 shows the steps taken by a participant. First, the process waits for\na vote request from the coordinator. If no message comes in, the transaction is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.5. DISTRIBUTED COMMIT 487\nThere are several solutions to avoid blocking. One solution is to use a\nmulticast primitive by which a receiver immediately multicasts a received\nmessage to all other processes [Babaoglu and Toueg, 1993]. It can be shown\nthat this approach allows a participant to reach a \ufb01nal decision, even if the\ncoordinator has not yet recovered. Another solution is to use three instead of\ntwo phases, as we discuss in Note 8.13.\nNote 8.12 (Advanced: 2PC outlined in Python)\nAn outline of the actions that are executed by the coordinator is given in Fig-\nure 8.32. The coordinator starts by multicasting a vote -request to all participants\nin order to collect their votes. It subsequently records that it is entering the WAIT\nstate, after which it waits for incoming votes from participants.\n1class Coordinator:\n2\n3 defrun(self):\n4 yetToReceive = list(participants)\n5 self.log.info(\u2019WAIT\u2019)\n6 self.chan.sendTo(participants, VOTE_REQUEST)\n7 while len (yetToReceive) > 0:\n8 msg = self.chan.recvFrom(participants, TIMEOUT)\n9 if(notmsg) or(msg[1] == VOTE_ABORT):\n10 self.log.info(\u2019ABORT\u2019)\n11 self.chan.sendTo(participants, GLOBAL_ABORT)\n12 return\n13 else: # msg[1] == VOTE_COMMIT\n14 yetToReceive.remove(msg[0])\n15 self.log.info(\u2019COMMIT\u2019)\n16 self.chan.sendTo(participants, GLOBAL_COMMIT)\nFigure 8.32: The steps taken by the coordinator in a 2PC protocol.\nIf not all votes have been collected but no more votes are received within\na given time interval prescribed in advance, the coordinator assumes that one\nor more participants have failed. Consequently, it should abort the transaction\nand multicasts a global -abort to the (remaining) participants. Likewise, if only\na single participant decides to abort the transaction, the coordinator will have\nto call off the transaction. If all participants vote to commit, global -commit is\n\ufb01rst logged and subsequently sent to all processes. Otherwise, the coordinator\nmulticasts a global -abort (after recording it in the local log).\nAfter receiving a vote request, the participant does its work. All bets are\noff if its work failed, but otherwise, it will vote for committing the transaction.\nIt records its decision in a local log and informs the coordinator by sending a\nvote -commit message. The participant must then wait for the global decision.\nAssuming this decision (which again should come from the coordinator) comes in\non time, it is simply written to the local log, after which it can be carried out (the\nlatter is not shown in the code).\nFigure 8.33 shows the steps taken by a participant. First, the process waits for\na vote request from the coordinator. If no message comes in, the transaction is\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "488 CHAPTER 8. FAULT TOLERANCE\nsimply aborted. Apparently, the coordinator had failed.\n1class Participant:\n2 defrun(self):\n3 self.log.info(\u2019INIT\u2019)\n4 msg = self.chan.recvFrom(coordinator, TIMEOUT)\n5 if(notmsg): # Crashed coordinator - give up entirely\n6 decision = LOCAL_ABORT\n7 else: # Coordinator will have sent VOTE_REQUEST\n8 decision = self.do_work()\n9 ifdecision == LOCAL_ABORT:\n10 self.chan.sendTo(coordinator, VOTE_ABORT)\n11 else: # Ready to commit, enter READY state\n12 self.log.info(\u2019READY\u2019)\n13 self.chan.sendTo(coordinator, VOTE_COMMIT)\n14 msg = self.chan.recvFrom(coordinator, TIMEOUT)\n15 if(notmsg): # Crashed coordinator - check the others\n16 self.chan.sendTo(all_participants, NEED_DECISION)\n17 while True:\n18 msg = self.chan.recvFromAny()\n19 ifmsg[1] in[GLOBAL_COMMIT, GLOBAL_ABORT, LOCAL_ABORT]:\n20 decision = msg[1]\n21 break\n22 else: # Coordinator came to a decision\n23 decision = msg[1]\n24 ifdecision == GLOBAL_COMMIT:\n25 self.log.info(\u2019COMMIT\u2019)\n26 else: # decision in [GLOBAL_ABORT, LOCAL_ABORT]:\n27 self.log.info(\u2019ABORT\u2019)\n28 while True: # Help any other participant when coordinator crashed\n29 msg = self.chan.recvFrom(all_participants)\n30 ifmsg[1] == NEED_DECISION:\n31 self.chan.sendTo([msg[0]], decision)\nFigure 8.33: The steps taken by a participant process in 2PC.\nHowever, if the participant times out while waiting for the coordinator\u2019s\ndecision to come in, it executes a termination protocol by \ufb01rst multicasting a\ndecision -request message to the other processes, after which it subsequently\nblocks while waiting for a response. When a decisive response comes in (possibly\nfrom the coordinator, which is assumed to eventually recover), the participant\nwrites the decision to its local log and handles accordingly. Any request from\nanother participant for the \ufb01nal decision is left unanswered as long as that decision\nis not known.\nWe keep every participant up-and-running after it decided to either commit\nor abort. It can then assist other participants in need of a decision after detecting\nthat the coordinator had crashed. To this end, a participant blocks on incoming\nmessages and returns its own decision when asked for. Note that we are actually\nproviding an implementation that supports partially synchronous behavior: we\nassume that timeouts can be applied as a mechanism to detect failures, but take\ninto account that we may have mistakenly concluded that a server crashed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n488 CHAPTER 8. FAULT TOLERANCE\nsimply aborted. Apparently, the coordinator had failed.\n1class Participant:\n2 defrun(self):\n3 self.log.info(\u2019INIT\u2019)\n4 msg = self.chan.recvFrom(coordinator, TIMEOUT)\n5 if(notmsg): # Crashed coordinator - give up entirely\n6 decision = LOCAL_ABORT\n7 else: # Coordinator will have sent VOTE_REQUEST\n8 decision = self.do_work()\n9 ifdecision == LOCAL_ABORT:\n10 self.chan.sendTo(coordinator, VOTE_ABORT)\n11 else: # Ready to commit, enter READY state\n12 self.log.info(\u2019READY\u2019)\n13 self.chan.sendTo(coordinator, VOTE_COMMIT)\n14 msg = self.chan.recvFrom(coordinator, TIMEOUT)\n15 if(notmsg): # Crashed coordinator - check the others\n16 self.chan.sendTo(all_participants, NEED_DECISION)\n17 while True:\n18 msg = self.chan.recvFromAny()\n19 ifmsg[1] in[GLOBAL_COMMIT, GLOBAL_ABORT, LOCAL_ABORT]:\n20 decision = msg[1]\n21 break\n22 else: # Coordinator came to a decision\n23 decision = msg[1]\n24 ifdecision == GLOBAL_COMMIT:\n25 self.log.info(\u2019COMMIT\u2019)\n26 else: # decision in [GLOBAL_ABORT, LOCAL_ABORT]:\n27 self.log.info(\u2019ABORT\u2019)\n28 while True: # Help any other participant when coordinator crashed\n29 msg = self.chan.recvFrom(all_participants)\n30 ifmsg[1] == NEED_DECISION:\n31 self.chan.sendTo([msg[0]], decision)\nFigure 8.33: The steps taken by a participant process in 2PC.\nHowever, if the participant times out while waiting for the coordinator\u2019s\ndecision to come in, it executes a termination protocol by \ufb01rst multicasting a\ndecision -request message to the other processes, after which it subsequently\nblocks while waiting for a response. When a decisive response comes in (possibly\nfrom the coordinator, which is assumed to eventually recover), the participant\nwrites the decision to its local log and handles accordingly. Any request from\nanother participant for the \ufb01nal decision is left unanswered as long as that decision\nis not known.\nWe keep every participant up-and-running after it decided to either commit\nor abort. It can then assist other participants in need of a decision after detecting\nthat the coordinator had crashed. To this end, a participant blocks on incoming\nmessages and returns its own decision when asked for. Note that we are actually\nproviding an implementation that supports partially synchronous behavior: we\nassume that timeouts can be applied as a mechanism to detect failures, but take\ninto account that we may have mistakenly concluded that a server crashed.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.5. DISTRIBUTED COMMIT 489\nNote 8.13 (Advanced: Three-phase commit)\nA problem with the two-phase commit protocol is that when the coordinator has\ncrashed, participants may not be able to reach a \ufb01nal decision. Consequently,\nparticipants may need to remain blocked until the coordinator recovers. Skeen\n[1981] developed a variant of 2PC, called the three-phase commit protocol (3PC),\nthat avoids blocking processes in the presence of fail-stop crashes. Although 3PC\nis widely referred to in the literature, it is not applied often in practice as the\nconditions under which 2PC blocks rarely occur. We discuss the protocol, as\nit provides further insight into solving fault-tolerance problems in distributed\nsystems.\nLike 2PC, 3PC is also formulated in terms of a coordinator and a number of\nparticipants. Their respective \ufb01nite state machines are shown in Figure 8.34. The\nessence of the protocol is that the states of the coordinator and each participant\nsatisfy the following two conditions:\n1.There is no single state from which it is possible to make a transition directly\nto either a COMMIT or an ABORT state.\n2.There is no state in which it is not possible to make a \ufb01nal decision, and\nfrom which a transition to a COMMIT state can be made.\nIt can be shown that these two conditions are necessary and suf\ufb01cient for a commit\nprotocol to be nonblocking [Skeen and Stonebraker, 1983].\n(a) (b)\nFigure 8.34: (a) The \ufb01nite state machine for the coordinator in 3PC. (b) The\n\ufb01nite state machine for a participant.\nThe coordinator in 3PC starts with sending a vote -request message to all\nparticipants, after which it waits for incoming responses. If any participant\nvotes to abort the transaction, the \ufb01nal decision will be to abort as well, so\nthe coordinator sends global -abort . However, when the transaction can be\ncommitted, a prepare -commit message is sent. Only after each participant has\nacknowledged it is now prepared to commit, will the coordinator send the \ufb01nal\nglobal -commit message by which the transaction is actually committed.\nAgain, there are only a few situations in which a process is blocked while\nwaiting for incoming messages. First, if a participant is waiting for a vote request\nfrom the coordinator while residing in state INIT , it will eventually make a\ntransition to state ABORT , thereby assuming that the coordinator has crashed.\nThis situation is identical to that in 2PC. Analogously, the coordinator may be in\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.5. DISTRIBUTED COMMIT 489\nNote 8.13 (Advanced: Three-phase commit)\nA problem with the two-phase commit protocol is that when the coordinator has\ncrashed, participants may not be able to reach a \ufb01nal decision. Consequently,\nparticipants may need to remain blocked until the coordinator recovers. Skeen\n[1981] developed a variant of 2PC, called the three-phase commit protocol (3PC),\nthat avoids blocking processes in the presence of fail-stop crashes. Although 3PC\nis widely referred to in the literature, it is not applied often in practice as the\nconditions under which 2PC blocks rarely occur. We discuss the protocol, as\nit provides further insight into solving fault-tolerance problems in distributed\nsystems.\nLike 2PC, 3PC is also formulated in terms of a coordinator and a number of\nparticipants. Their respective \ufb01nite state machines are shown in Figure 8.34. The\nessence of the protocol is that the states of the coordinator and each participant\nsatisfy the following two conditions:\n1.There is no single state from which it is possible to make a transition directly\nto either a COMMIT or an ABORT state.\n2.There is no state in which it is not possible to make a \ufb01nal decision, and\nfrom which a transition to a COMMIT state can be made.\nIt can be shown that these two conditions are necessary and suf\ufb01cient for a commit\nprotocol to be nonblocking [Skeen and Stonebraker, 1983].\n(a) (b)\nFigure 8.34: (a) The \ufb01nite state machine for the coordinator in 3PC. (b) The\n\ufb01nite state machine for a participant.\nThe coordinator in 3PC starts with sending a vote -request message to all\nparticipants, after which it waits for incoming responses. If any participant\nvotes to abort the transaction, the \ufb01nal decision will be to abort as well, so\nthe coordinator sends global -abort . However, when the transaction can be\ncommitted, a prepare -commit message is sent. Only after each participant has\nacknowledged it is now prepared to commit, will the coordinator send the \ufb01nal\nglobal -commit message by which the transaction is actually committed.\nAgain, there are only a few situations in which a process is blocked while\nwaiting for incoming messages. First, if a participant is waiting for a vote request\nfrom the coordinator while residing in state INIT , it will eventually make a\ntransition to state ABORT , thereby assuming that the coordinator has crashed.\nThis situation is identical to that in 2PC. Analogously, the coordinator may be in\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "490 CHAPTER 8. FAULT TOLERANCE\nstate WAIT , waiting for the votes from participants. On a timeout, the coordinator\nwill conclude that a participant crashed, and will thus abort the transaction by\nmulticasting a global -abort message.\nNow suppose the coordinator is blocked in state PRECOMMIT . On a timeout,\nit will conclude that one of the participants had crashed, but that participant\nis known to have voted for committing the transaction. Consequently, the co-\nordinator can instruct the operational participants to commit by multicasting a\nglobal -commit message. In addition, it relies on a recovery protocol for the\ncrashed participant to commit its part of the transaction when it comes up again.\nA participant Pmay block in the READY state or in the PRECOMMIT state.\nOn a timeout, Pcan conclude only that the coordinator has failed, so that it now\nneeds to \ufb01nd out what to do next. As in 2PC, if Pcontacts any other participant\nthat is in state COMMIT (orABORT ),Pshould move to that state as well. In\naddition, if all participants are in state PRECOMMIT , the transaction can be\ncommitted.\nAgain analogous to 2PC, if another participant Qis still in the INIT state,\nthe transaction can safely be aborted. It is important to note that Qcan be in\nstate INIT only if no other participant is in state PRECOMMIT . A participant\ncan reach PRECOMMIT only if the coordinator had reached state PRECOMMIT\nbefore crashing, and has thus received a vote to commit from each participant. In\nother words, no participant can reside in state INIT while another participant is in\nstate PRECOMMIT .\nIf each of the participants that Pcan contact is in state READY (and they\ntogether form a majority), the transaction should be aborted. The point to note\nis that another participant may have crashed and will later recover. However,\nneither P, nor any other of the operational participants knows what the state of\nthe crashed participant will be when it recovers. If the process recovers to state\nINIT , then deciding to abort the transaction is the only correct decision. At worst,\nthe process may recover to state PRECOMMIT , but in that case, it cannot do any\nharm to still abort the transaction.\nThis situation is the major difference with 2PC, where a crashed participant\ncould recover to a COMMIT state while all the others were still in state READY .\nIn that case, the remaining operational processes could not reach a \ufb01nal decision\nand would have to wait until the crashed process recovered. With 3PC, if any\noperational process is in its READY state, no crashed process will recover to a state\nother than INIT ,ABORT , orPRECOMMIT . For this reason, surviving processes\ncan always come to a \ufb01nal decision.\nFinally, if the processes that Pcan reach are in state PRECOMMIT (and they\nform a majority), then it is safe to commit the transaction. Again, it can be\nshown that in this case, all other processes will either be in state READY or at\nleast, will recover to state READY ,PRECOMMIT , orCOMMIT when they had\ncrashed. More details on 3PC can be found in [Bernstein et al., 1987] and [\u00d6zsu\nand Valduriez, 2011].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n490 CHAPTER 8. FAULT TOLERANCE\nstate WAIT , waiting for the votes from participants. On a timeout, the coordinator\nwill conclude that a participant crashed, and will thus abort the transaction by\nmulticasting a global -abort message.\nNow suppose the coordinator is blocked in state PRECOMMIT . On a timeout,\nit will conclude that one of the participants had crashed, but that participant\nis known to have voted for committing the transaction. Consequently, the co-\nordinator can instruct the operational participants to commit by multicasting a\nglobal -commit message. In addition, it relies on a recovery protocol for the\ncrashed participant to commit its part of the transaction when it comes up again.\nA participant Pmay block in the READY state or in the PRECOMMIT state.\nOn a timeout, Pcan conclude only that the coordinator has failed, so that it now\nneeds to \ufb01nd out what to do next. As in 2PC, if Pcontacts any other participant\nthat is in state COMMIT (orABORT ),Pshould move to that state as well. In\naddition, if all participants are in state PRECOMMIT , the transaction can be\ncommitted.\nAgain analogous to 2PC, if another participant Qis still in the INIT state,\nthe transaction can safely be aborted. It is important to note that Qcan be in\nstate INIT only if no other participant is in state PRECOMMIT . A participant\ncan reach PRECOMMIT only if the coordinator had reached state PRECOMMIT\nbefore crashing, and has thus received a vote to commit from each participant. In\nother words, no participant can reside in state INIT while another participant is in\nstate PRECOMMIT .\nIf each of the participants that Pcan contact is in state READY (and they\ntogether form a majority), the transaction should be aborted. The point to note\nis that another participant may have crashed and will later recover. However,\nneither P, nor any other of the operational participants knows what the state of\nthe crashed participant will be when it recovers. If the process recovers to state\nINIT , then deciding to abort the transaction is the only correct decision. At worst,\nthe process may recover to state PRECOMMIT , but in that case, it cannot do any\nharm to still abort the transaction.\nThis situation is the major difference with 2PC, where a crashed participant\ncould recover to a COMMIT state while all the others were still in state READY .\nIn that case, the remaining operational processes could not reach a \ufb01nal decision\nand would have to wait until the crashed process recovered. With 3PC, if any\noperational process is in its READY state, no crashed process will recover to a state\nother than INIT ,ABORT , orPRECOMMIT . For this reason, surviving processes\ncan always come to a \ufb01nal decision.\nFinally, if the processes that Pcan reach are in state PRECOMMIT (and they\nform a majority), then it is safe to commit the transaction. Again, it can be\nshown that in this case, all other processes will either be in state READY or at\nleast, will recover to state READY ,PRECOMMIT , orCOMMIT when they had\ncrashed. More details on 3PC can be found in [Bernstein et al., 1987] and [\u00d6zsu\nand Valduriez, 2011].\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.6. RECOVERY 491\n8.6 Recovery\nSo far, we have mainly concentrated on algorithms that allow us to tolerate\nfaults. However, once a failure has occurred, it is essential that the process\nwhere the failure happened can recover to a correct state. In what follows, we\n\ufb01rst concentrate on what it actually means to recover to a correct state, and\nsubsequently when and how the state of a distributed system can be recorded\nand recovered to, by means of checkpointing and message logging.\nIntroduction\nFundamental to fault tolerance is the recovery from an error. Recall that an\nerror is that part of a system that may lead to a failure. The whole idea of\nerror recovery is to replace an erroneous state with an error-free state. There\nare essentially two forms of error recovery.\nInbackward recovery , the main issue is to bring the system from its\npresent erroneous state back into a previously correct state. To do so, it will\nbe necessary to record the system\u2019s state from time to time, and to restore\nsuch a recorded state when things go wrong. Each time (part of) the system\u2019s\npresent state is recorded, a checkpoint is said to be made.\nAnother form of error recovery is forward recovery . In this case, when the\nsystem has entered an erroneous state, instead of moving back to a previous,\ncheckpointed state, an attempt is made to bring the system in a correct new\nstate from which it can continue to execute. The main problem with forward\nerror recovery mechanisms is that it has to be known in advance which errors\nmay occur. Only in that case is it possible to correct those errors and move to\na new state.\nThe distinction between backward and forward error recovery is easily\nexplained when considering the implementation of reliable communication.\nThe common approach to recover from a lost packet is to let the sender\nretransmit that packet. In effect, packet retransmission establishes that we\nattempt to go back to a previous, correct state, namely the one in which the\npacket that was lost is being sent. Reliable communication through packet\nretransmission is therefore an example of applying backward error recovery\ntechniques.\nAn alternative approach is to use a method known as erasure correction .\nIn this approach, a missing packet is constructed from other, successfully\ndelivered packets. For example, in an (n,k)-block erasure code, a set of k\nsource packets is encoded into a set of n encoded packets , such that anyset of\nkencoded packets is enough to reconstruct the original ksource packets.\nTypical values are k=16ork=32, and k<n\u00142k(see, for example, [Rizzo,\n1997]). If not enough packets have yet been delivered, the sender will have to\ncontinue transmitting packets until a previously lost packet can be constructed.\nErasure correction is a typical example of a forward error recovery approach.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.6. RECOVERY 491\n8.6 Recovery\nSo far, we have mainly concentrated on algorithms that allow us to tolerate\nfaults. However, once a failure has occurred, it is essential that the process\nwhere the failure happened can recover to a correct state. In what follows, we\n\ufb01rst concentrate on what it actually means to recover to a correct state, and\nsubsequently when and how the state of a distributed system can be recorded\nand recovered to, by means of checkpointing and message logging.\nIntroduction\nFundamental to fault tolerance is the recovery from an error. Recall that an\nerror is that part of a system that may lead to a failure. The whole idea of\nerror recovery is to replace an erroneous state with an error-free state. There\nare essentially two forms of error recovery.\nInbackward recovery , the main issue is to bring the system from its\npresent erroneous state back into a previously correct state. To do so, it will\nbe necessary to record the system\u2019s state from time to time, and to restore\nsuch a recorded state when things go wrong. Each time (part of) the system\u2019s\npresent state is recorded, a checkpoint is said to be made.\nAnother form of error recovery is forward recovery . In this case, when the\nsystem has entered an erroneous state, instead of moving back to a previous,\ncheckpointed state, an attempt is made to bring the system in a correct new\nstate from which it can continue to execute. The main problem with forward\nerror recovery mechanisms is that it has to be known in advance which errors\nmay occur. Only in that case is it possible to correct those errors and move to\na new state.\nThe distinction between backward and forward error recovery is easily\nexplained when considering the implementation of reliable communication.\nThe common approach to recover from a lost packet is to let the sender\nretransmit that packet. In effect, packet retransmission establishes that we\nattempt to go back to a previous, correct state, namely the one in which the\npacket that was lost is being sent. Reliable communication through packet\nretransmission is therefore an example of applying backward error recovery\ntechniques.\nAn alternative approach is to use a method known as erasure correction .\nIn this approach, a missing packet is constructed from other, successfully\ndelivered packets. For example, in an (n,k)-block erasure code, a set of k\nsource packets is encoded into a set of n encoded packets , such that anyset of\nkencoded packets is enough to reconstruct the original ksource packets.\nTypical values are k=16ork=32, and k<n\u00142k(see, for example, [Rizzo,\n1997]). If not enough packets have yet been delivered, the sender will have to\ncontinue transmitting packets until a previously lost packet can be constructed.\nErasure correction is a typical example of a forward error recovery approach.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "492 CHAPTER 8. FAULT TOLERANCE\nBy and large, backward error recovery techniques are widely applied as a\ngeneral mechanism for recovering from failures in distributed systems. The\nmajor bene\ufb01t of backward error recovery is that it is a generally applicable\nmethod independent of any speci\ufb01c system or process. In other words, it\ncan be integrated into (the middleware layer) of a distributed system as a\ngeneral-purpose service.\nHowever, backward error recovery also introduces some problems [Singhal\nand Shivaratri, 1994]. First, restoring a system or process to a previous state\nis generally a relatively costly operation in terms of performance. As will be\ndiscussed in succeeding sections, much work generally needs to be done to\nrecover from, for example, a process crash or site failure. A potential way out\nof this problem, is to devise very cheap mechanisms by which components\nare simply rebooted.\nSecond, because backward error recovery mechanisms are independent of\nthe distributed application for which they are actually used, no guarantees\ncan be given that once recovery has taken place, the same or similar failure\nwill not happen again. If such guarantees are needed, handling errors often\nrequires that the application gets into the loop of recovery. In other words,\nfull-\ufb02edged failure transparency can generally not be provided by backward\nerror recovery mechanisms.\nFinally, although backward error recovery requires checkpointing, some\nstates can simply never be rolled back to. For example, once a (possibly\nmalicious) person has taken the $1,000 that suddenly came rolling out of the\nincorrectly functioning automated teller machine, there is only a small chance\nthat money will be stuffed back in the machine. Likewise, recovering to a\nprevious state in most Unix systems after having enthusiastically typed\n/bin/rm -fr *\nbut from the wrong working directory, may turn a few people pale. Some\nthings are simply irreversible.\nCheckpointing allows the recovery to a previous correct state. However,\ntaking a checkpoint is often a costly operation and may have a severe perfor-\nmance penalty. As a consequence, many fault-tolerant distributed systems\ncombine checkpointing with message logging . In this case, after a check-\npoint has been taken, a process logs its messages before sending them off\n(called sender-based logging ). An alternative solution is to let the receiving\nprocess \ufb01rst log an incoming message before delivering it to the application\nit is executing. This scheme is also referred to as receiver-based logging .\nWhen a receiving process crashes, it is necessary to restore the most recently\ncheckpointed state, and from there on replay the messages that have been sent.\nConsequently, combining checkpoints with message logging makes it possible\nto restore a state that lies beyond the most recent checkpoint without the cost\nof checkpointing.\nIn a system where only checkpointing is used, processes will be restored\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n492 CHAPTER 8. FAULT TOLERANCE\nBy and large, backward error recovery techniques are widely applied as a\ngeneral mechanism for recovering from failures in distributed systems. The\nmajor bene\ufb01t of backward error recovery is that it is a generally applicable\nmethod independent of any speci\ufb01c system or process. In other words, it\ncan be integrated into (the middleware layer) of a distributed system as a\ngeneral-purpose service.\nHowever, backward error recovery also introduces some problems [Singhal\nand Shivaratri, 1994]. First, restoring a system or process to a previous state\nis generally a relatively costly operation in terms of performance. As will be\ndiscussed in succeeding sections, much work generally needs to be done to\nrecover from, for example, a process crash or site failure. A potential way out\nof this problem, is to devise very cheap mechanisms by which components\nare simply rebooted.\nSecond, because backward error recovery mechanisms are independent of\nthe distributed application for which they are actually used, no guarantees\ncan be given that once recovery has taken place, the same or similar failure\nwill not happen again. If such guarantees are needed, handling errors often\nrequires that the application gets into the loop of recovery. In other words,\nfull-\ufb02edged failure transparency can generally not be provided by backward\nerror recovery mechanisms.\nFinally, although backward error recovery requires checkpointing, some\nstates can simply never be rolled back to. For example, once a (possibly\nmalicious) person has taken the $1,000 that suddenly came rolling out of the\nincorrectly functioning automated teller machine, there is only a small chance\nthat money will be stuffed back in the machine. Likewise, recovering to a\nprevious state in most Unix systems after having enthusiastically typed\n/bin/rm -fr *\nbut from the wrong working directory, may turn a few people pale. Some\nthings are simply irreversible.\nCheckpointing allows the recovery to a previous correct state. However,\ntaking a checkpoint is often a costly operation and may have a severe perfor-\nmance penalty. As a consequence, many fault-tolerant distributed systems\ncombine checkpointing with message logging . In this case, after a check-\npoint has been taken, a process logs its messages before sending them off\n(called sender-based logging ). An alternative solution is to let the receiving\nprocess \ufb01rst log an incoming message before delivering it to the application\nit is executing. This scheme is also referred to as receiver-based logging .\nWhen a receiving process crashes, it is necessary to restore the most recently\ncheckpointed state, and from there on replay the messages that have been sent.\nConsequently, combining checkpoints with message logging makes it possible\nto restore a state that lies beyond the most recent checkpoint without the cost\nof checkpointing.\nIn a system where only checkpointing is used, processes will be restored\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.6. RECOVERY 493\nto a checkpointed state. From there on, their behavior may be different than it\nwas before the failure occurred. For example, because communication times\nare not deterministic, messages may now be delivered in a different order,\nin turn leading to different reactions by the receivers. However, if message\nlogging takes place, an actual replay of the events that happened since the\nlast checkpoint takes place. Such a replay makes it easier to interact with the\noutside world.\nFor example, consider the case that a failure occurred because a user\nprovided erroneous input. If only checkpointing is used, the system would\nhave to take a checkpoint before accepting the user\u2019s input in order to recover\nto exactly the same state. With message logging, an older checkpoint can\nbe used, after which a replay of events can take place up to the point that\nthe user should provide input. In practice, the combination of having fewer\ncheckpoints and message logging is more ef\ufb01cient than having to take many\ncheckpoints.\nElnozahy et al. [2002] provide a survey of checkpointing and logging in\ndistributed systems. Various algorithmic details can be found in Chow and\nJohnson [1997].\nCheckpointing\nIn a fault-tolerant distributed system, backward error recovery requires that\nthe system regularly saves its state2. In particular, we need to record a\nconsistent global state, also called a distributed snapshot . In a distributed\nsnapshot, if a process Phas recorded the receipt of a message, then there\nshould also be a process Qthat has recorded the sending of that message.\nAfter all, it must have come from somewhere.\nFigure 8.35: A recovery line.\nTo recover after a process or system failure requires that we construct a\nconsistent global state from local states as saved by each process. In particular,\nit is best to recover to the most recent distributed snapshot, also referred to\n2We assume that each process has access to a local, reliable storage.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.6. RECOVERY 493\nto a checkpointed state. From there on, their behavior may be different than it\nwas before the failure occurred. For example, because communication times\nare not deterministic, messages may now be delivered in a different order,\nin turn leading to different reactions by the receivers. However, if message\nlogging takes place, an actual replay of the events that happened since the\nlast checkpoint takes place. Such a replay makes it easier to interact with the\noutside world.\nFor example, consider the case that a failure occurred because a user\nprovided erroneous input. If only checkpointing is used, the system would\nhave to take a checkpoint before accepting the user\u2019s input in order to recover\nto exactly the same state. With message logging, an older checkpoint can\nbe used, after which a replay of events can take place up to the point that\nthe user should provide input. In practice, the combination of having fewer\ncheckpoints and message logging is more ef\ufb01cient than having to take many\ncheckpoints.\nElnozahy et al. [2002] provide a survey of checkpointing and logging in\ndistributed systems. Various algorithmic details can be found in Chow and\nJohnson [1997].\nCheckpointing\nIn a fault-tolerant distributed system, backward error recovery requires that\nthe system regularly saves its state2. In particular, we need to record a\nconsistent global state, also called a distributed snapshot . In a distributed\nsnapshot, if a process Phas recorded the receipt of a message, then there\nshould also be a process Qthat has recorded the sending of that message.\nAfter all, it must have come from somewhere.\nFigure 8.35: A recovery line.\nTo recover after a process or system failure requires that we construct a\nconsistent global state from local states as saved by each process. In particular,\nit is best to recover to the most recent distributed snapshot, also referred to\n2We assume that each process has access to a local, reliable storage.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "494 CHAPTER 8. FAULT TOLERANCE\nas a recovery line . In other words, a recovery line corresponds to the most\nrecent consistent collection of checkpoints, as shown in Figure 8.35.\nCoordinated checkpointing\nIncoordinated checkpointing all processes synchronize to jointly write their\nstate to local storage. The main advantage of coordinated checkpointing is\nthat the saved state is automatically globally consistent. A simple solution\nis to use a two-phase blocking protocol. A coordinator \ufb01rst multicasts a\ncheckpoint -request message to all processes. When a process receives\nsuch a message, it takes a local checkpoint, queues any subsequent message\nhanded to it by the application it is executing, and acknowledges to the\ncoordinator that it has taken a checkpoint. When the coordinator has received\nan acknowledgment from all processes, it multicasts a checkpoint -done\nmessage to allow the (blocked) processes to continue.\nIt is easy to see that this approach will also lead to a globally consistent\nstate, because no incoming message will ever be registered as part of a\ncheckpoint. The reason for this is that any message that follows a request for\ntaking a checkpoint is not considered to be part of the local checkpoint. At\nthe same time, outgoing messages (as handed to the checkpointing process by\nthe application it is running), are queued locally until the checkpoint -done\nmessage is received.\nAn improvement to this algorithm is to send a checkpoint request only to\nthose processes that depend on the recovery of the coordinator, and ignore the\nother processes. A process is dependent on the coordinator if it has received a\nmessage that is directly or indirectly causally related to a message that the\ncoordinator had sent since the last checkpoint. This leads to the notion of an\nincremental snapshot .\nTo take an incremental snapshot, the coordinator sends a checkpoint\nrequest only to those processes it had sent a message to since it last took a\ncheckpoint. When a process Preceives such a request, it forwards the request\nto all those processes to which Pitself had sent a message since the last\ncheckpoint, and so on. A process forwards the request only once. When all\nprocesses have been identi\ufb01ed, a second multicast is used to actually trigger\ncheckpointing and to let the processes continue where they had left off.\nIndependent checkpointing\nNow consider the case in which each process simply records its local state\nfrom time to time in an uncoordinated fashion. To discover a recovery line\nrequires that each process is rolled back to its most recently saved state. If\nthese local states jointly do not form a distributed snapshot, further rolling\nback is necessary. This process of a cascaded rollback may lead to what is\ncalled the domino effect and is shown in Figure 8.36.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n494 CHAPTER 8. FAULT TOLERANCE\nas a recovery line . In other words, a recovery line corresponds to the most\nrecent consistent collection of checkpoints, as shown in Figure 8.35.\nCoordinated checkpointing\nIncoordinated checkpointing all processes synchronize to jointly write their\nstate to local storage. The main advantage of coordinated checkpointing is\nthat the saved state is automatically globally consistent. A simple solution\nis to use a two-phase blocking protocol. A coordinator \ufb01rst multicasts a\ncheckpoint -request message to all processes. When a process receives\nsuch a message, it takes a local checkpoint, queues any subsequent message\nhanded to it by the application it is executing, and acknowledges to the\ncoordinator that it has taken a checkpoint. When the coordinator has received\nan acknowledgment from all processes, it multicasts a checkpoint -done\nmessage to allow the (blocked) processes to continue.\nIt is easy to see that this approach will also lead to a globally consistent\nstate, because no incoming message will ever be registered as part of a\ncheckpoint. The reason for this is that any message that follows a request for\ntaking a checkpoint is not considered to be part of the local checkpoint. At\nthe same time, outgoing messages (as handed to the checkpointing process by\nthe application it is running), are queued locally until the checkpoint -done\nmessage is received.\nAn improvement to this algorithm is to send a checkpoint request only to\nthose processes that depend on the recovery of the coordinator, and ignore the\nother processes. A process is dependent on the coordinator if it has received a\nmessage that is directly or indirectly causally related to a message that the\ncoordinator had sent since the last checkpoint. This leads to the notion of an\nincremental snapshot .\nTo take an incremental snapshot, the coordinator sends a checkpoint\nrequest only to those processes it had sent a message to since it last took a\ncheckpoint. When a process Preceives such a request, it forwards the request\nto all those processes to which Pitself had sent a message since the last\ncheckpoint, and so on. A process forwards the request only once. When all\nprocesses have been identi\ufb01ed, a second multicast is used to actually trigger\ncheckpointing and to let the processes continue where they had left off.\nIndependent checkpointing\nNow consider the case in which each process simply records its local state\nfrom time to time in an uncoordinated fashion. To discover a recovery line\nrequires that each process is rolled back to its most recently saved state. If\nthese local states jointly do not form a distributed snapshot, further rolling\nback is necessary. This process of a cascaded rollback may lead to what is\ncalled the domino effect and is shown in Figure 8.36.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.6. RECOVERY 495\nFigure 8.36: The domino effect.\nWhen process P2crashes, we need to restore its state to the most recently\nsaved checkpoint. As a consequence, process P1will also need to be rolled\nback. Unfortunately, the two most recently saved local states do not form a\nconsistent global state: the state saved by P2indicates the receipt of a message\nm, but no other process can be identi\ufb01ed as its sender. Consequently, P2needs\nto be rolled back to an earlier state.\nHowever, the next state to which P2is rolled back also cannot be used as\npart of a distributed snapshot. In this case, P1will have recorded the receipt\nof message m\u0003, but there is no recorded event of this message being sent. It is\ntherefore necessary to also roll P1back to a previous state. In this example, it\nturns out that the recovery line is actually the initial state of the system.\nAs processes take local checkpoints independent of each other, this method\nis also referred to as independent checkpointing . Its implementation requires\nthat dependencies are recorded in such a way that processes can jointly roll\nback to a consistent global state. To that end, let CPi(m)denote the m-th\ncheckpoint taken by process Pi. Also, let INT i(m)denote the interval between\ncheckpoints CPi(m\u00001)and CPi(m).\nWhen process Pisends a message in interval INT i(m), it piggybacks the\npair(i,m)to the receiving process. When process Pjreceives a message in in-\nterval INT j(n), along with the pair of indices (i,m), it records the dependency\nINT i(m)!INT j(n). Whenever Pjtakes checkpoint CPj(n), it additionally\nsaves this dependency to its local storage, along with the rest of the recovery\ninformation that is part of CPj(n).\nNow suppose that at a certain moment, process P1is required to roll back\nto checkpoint CPi(m\u00001). To ensure global consistency, we need to ensure\nthat all processes that have received messages from Piand that were sent in\ninterval INT i(m), are rolled back to a checkpointed state preceding the receipt\nof such messages. In particular, process Pjin our example, will need to be\nrolled back at least to checkpoint CPj(n\u00001). IfCPj(n\u00001)does not lead to a\nglobally consistent state, further rolling back may be necessary.\nCalculating the recovery line requires an analysis of the interval depen-\ndencies recorded by each process when a checkpoint was taken. Without\ngoing into any further details, it turns out that such calculations are fairly\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.6. RECOVERY 495\nFigure 8.36: The domino effect.\nWhen process P2crashes, we need to restore its state to the most recently\nsaved checkpoint. As a consequence, process P1will also need to be rolled\nback. Unfortunately, the two most recently saved local states do not form a\nconsistent global state: the state saved by P2indicates the receipt of a message\nm, but no other process can be identi\ufb01ed as its sender. Consequently, P2needs\nto be rolled back to an earlier state.\nHowever, the next state to which P2is rolled back also cannot be used as\npart of a distributed snapshot. In this case, P1will have recorded the receipt\nof message m\u0003, but there is no recorded event of this message being sent. It is\ntherefore necessary to also roll P1back to a previous state. In this example, it\nturns out that the recovery line is actually the initial state of the system.\nAs processes take local checkpoints independent of each other, this method\nis also referred to as independent checkpointing . Its implementation requires\nthat dependencies are recorded in such a way that processes can jointly roll\nback to a consistent global state. To that end, let CPi(m)denote the m-th\ncheckpoint taken by process Pi. Also, let INT i(m)denote the interval between\ncheckpoints CPi(m\u00001)and CPi(m).\nWhen process Pisends a message in interval INT i(m), it piggybacks the\npair(i,m)to the receiving process. When process Pjreceives a message in in-\nterval INT j(n), along with the pair of indices (i,m), it records the dependency\nINT i(m)!INT j(n). Whenever Pjtakes checkpoint CPj(n), it additionally\nsaves this dependency to its local storage, along with the rest of the recovery\ninformation that is part of CPj(n).\nNow suppose that at a certain moment, process P1is required to roll back\nto checkpoint CPi(m\u00001). To ensure global consistency, we need to ensure\nthat all processes that have received messages from Piand that were sent in\ninterval INT i(m), are rolled back to a checkpointed state preceding the receipt\nof such messages. In particular, process Pjin our example, will need to be\nrolled back at least to checkpoint CPj(n\u00001). IfCPj(n\u00001)does not lead to a\nglobally consistent state, further rolling back may be necessary.\nCalculating the recovery line requires an analysis of the interval depen-\ndencies recorded by each process when a checkpoint was taken. Without\ngoing into any further details, it turns out that such calculations are fairly\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "496 CHAPTER 8. FAULT TOLERANCE\ncomplex. In addition, as it turns out, it is often not the coordination between\nprocesses that is the dominating performance factor, but the overhead as the\nresult of having to save the state to local stable storage. Therefore, coordinated\ncheckpointing, which is much simpler than independent checkpointing, is\noften more popular, and will presumably stay so even when systems grow to\nmuch larger sizes [Elnozahy and Plank, 2004].\nMessage logging\nConsidering that checkpointing can be an expensive operation, techniques\nhave been sought to reduce the number of checkpoints, but still enable re-\ncovery. One such technique is logging messages. The basic idea underlying\nmessage logging is that if the transmission of messages can be replayed , we\ncan still reach a globally consistent state but without having to restore that\nstate from local storages. Instead, a checkpointed state is taken as a starting\npoint, and all messages that have been sent since are simply retransmitted\nand handled accordingly.\nThis approach works \ufb01ne under the assumption of what is called a piece-\nwise deterministic execution model . In such a model, the execution of each\nprocess is assumed to take place as a series of intervals in which events\ntake place. These events are the same as those discussed in the context of\nLamport\u2019s happened-before relationship in Chapter 6. For example, an event\nmay be the execution of an instruction, the sending of a message, and so on.\nEach interval in the piecewise deterministic model is assumed to start with\na nondeterministic event, such as the receipt of a message. However, from\nthat moment on, the execution of the process is completely deterministic. An\ninterval ends with the last event before a nondeterministic event occurs.\nIn effect, an interval can be replayed with a known result, that is, in a\ncompletely deterministic way, provided it is replayed starting with the same\nnondeterministic event as before. Consequently, if we record all nondeter-\nministic events in such a model, it becomes possible to completely replay the\nentire execution of a process in a deterministic way.\nConsidering that message logs are necessary to recover from a process\ncrash so that a globally consistent state is restored, it becomes important to\nknow precisely when messages are to be logged. Following the approach\ndescribed by Alvisi and Marzullo [1998], it turns out that many existing\nmessage-logging schemes can be easily characterized if we concentrate on\nhow they deal with orphan processes.\nAnorphan process is a process that survives the crash of another process,\nbut whose state is inconsistent with the crashed process after its recovery. As\nan example, consider the situation shown in Figure 8.37. Process Qreceives\nmessages m1and m2from process Pand R, respectively, and subsequently\nsends a message m3toR. However, in contrast to all other messages, message\nm2is not logged. If process Qcrashes and later recovers again, only the logged\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n496 CHAPTER 8. FAULT TOLERANCE\ncomplex. In addition, as it turns out, it is often not the coordination between\nprocesses that is the dominating performance factor, but the overhead as the\nresult of having to save the state to local stable storage. Therefore, coordinated\ncheckpointing, which is much simpler than independent checkpointing, is\noften more popular, and will presumably stay so even when systems grow to\nmuch larger sizes [Elnozahy and Plank, 2004].\nMessage logging\nConsidering that checkpointing can be an expensive operation, techniques\nhave been sought to reduce the number of checkpoints, but still enable re-\ncovery. One such technique is logging messages. The basic idea underlying\nmessage logging is that if the transmission of messages can be replayed , we\ncan still reach a globally consistent state but without having to restore that\nstate from local storages. Instead, a checkpointed state is taken as a starting\npoint, and all messages that have been sent since are simply retransmitted\nand handled accordingly.\nThis approach works \ufb01ne under the assumption of what is called a piece-\nwise deterministic execution model . In such a model, the execution of each\nprocess is assumed to take place as a series of intervals in which events\ntake place. These events are the same as those discussed in the context of\nLamport\u2019s happened-before relationship in Chapter 6. For example, an event\nmay be the execution of an instruction, the sending of a message, and so on.\nEach interval in the piecewise deterministic model is assumed to start with\na nondeterministic event, such as the receipt of a message. However, from\nthat moment on, the execution of the process is completely deterministic. An\ninterval ends with the last event before a nondeterministic event occurs.\nIn effect, an interval can be replayed with a known result, that is, in a\ncompletely deterministic way, provided it is replayed starting with the same\nnondeterministic event as before. Consequently, if we record all nondeter-\nministic events in such a model, it becomes possible to completely replay the\nentire execution of a process in a deterministic way.\nConsidering that message logs are necessary to recover from a process\ncrash so that a globally consistent state is restored, it becomes important to\nknow precisely when messages are to be logged. Following the approach\ndescribed by Alvisi and Marzullo [1998], it turns out that many existing\nmessage-logging schemes can be easily characterized if we concentrate on\nhow they deal with orphan processes.\nAnorphan process is a process that survives the crash of another process,\nbut whose state is inconsistent with the crashed process after its recovery. As\nan example, consider the situation shown in Figure 8.37. Process Qreceives\nmessages m1and m2from process Pand R, respectively, and subsequently\nsends a message m3toR. However, in contrast to all other messages, message\nm2is not logged. If process Qcrashes and later recovers again, only the logged\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.6. RECOVERY 497\nmessages required for the recovery of Qare replayed, in our example, m1.\nBecause m2was not logged, its transmission will not be replayed, meaning\nthat the transmission of m3may also not take place.\nFigure 8.37: Incorrect replay of messages after recovery, leading to an orphan\nprocess R.\nHowever, the situation after the recovery of Qis inconsistent with that\nbefore its recovery. In particular, Rholds a message ( m3) that was sent before\nthe crash, but whose receipt and delivery do not take place when replaying\nwhat had happened before the crash. Such inconsistencies should obviously\nbe avoided.\nNote 8.14 (Advanced: Characterizing message-logging schemes)\nTo characterize different message-logging schemes, we follow the approach de-\nscribed by Alvisi and Marzullo [1998]. Each message mis considered to have a\nheader that contains all information necessary to retransmit m, and to properly\nhandle it. For example, each header will identify the sender and the receiver, but\nalso a sequence number to recognize it as a duplicate. In addition, a delivery\nnumber may be added to decide when exactly it should be handed over to the\nreceiving application.\nA message is said to be stable if it can no longer be lost, for example, because\nit has been written to reliable, local storage. Stable messages can thus be used for\nrecovery by replaying their transmission.\nEach message mleads to a set DEP (m)of processes that depend on the\ndelivery of m. In particular, DEP (m)consists of those processes to which mhas\nbeen delivered. In addition, if another message m\u0003is causally dependent on\nthe delivery of m, and m\u0003has been delivered to a process Q, then Qwill also\nbe contained in DEP (m). Note that m\u0003is causally dependent on the delivery of\nm, if it was sent by the same process that previously delivered m, or which had\ndelivered another message that was causally dependent on the delivery of m.\nThe set COPY (m)consists of those processes that have a copy of m, but have\nnot (yet) reliably stored it. When a process Qdelivers message m, it also becomes\na member of COPY (m). Note that COPY (m)consists of those processes that could\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.6. RECOVERY 497\nmessages required for the recovery of Qare replayed, in our example, m1.\nBecause m2was not logged, its transmission will not be replayed, meaning\nthat the transmission of m3may also not take place.\nFigure 8.37: Incorrect replay of messages after recovery, leading to an orphan\nprocess R.\nHowever, the situation after the recovery of Qis inconsistent with that\nbefore its recovery. In particular, Rholds a message ( m3) that was sent before\nthe crash, but whose receipt and delivery do not take place when replaying\nwhat had happened before the crash. Such inconsistencies should obviously\nbe avoided.\nNote 8.14 (Advanced: Characterizing message-logging schemes)\nTo characterize different message-logging schemes, we follow the approach de-\nscribed by Alvisi and Marzullo [1998]. Each message mis considered to have a\nheader that contains all information necessary to retransmit m, and to properly\nhandle it. For example, each header will identify the sender and the receiver, but\nalso a sequence number to recognize it as a duplicate. In addition, a delivery\nnumber may be added to decide when exactly it should be handed over to the\nreceiving application.\nA message is said to be stable if it can no longer be lost, for example, because\nit has been written to reliable, local storage. Stable messages can thus be used for\nrecovery by replaying their transmission.\nEach message mleads to a set DEP (m)of processes that depend on the\ndelivery of m. In particular, DEP (m)consists of those processes to which mhas\nbeen delivered. In addition, if another message m\u0003is causally dependent on\nthe delivery of m, and m\u0003has been delivered to a process Q, then Qwill also\nbe contained in DEP (m). Note that m\u0003is causally dependent on the delivery of\nm, if it was sent by the same process that previously delivered m, or which had\ndelivered another message that was causally dependent on the delivery of m.\nThe set COPY (m)consists of those processes that have a copy of m, but have\nnot (yet) reliably stored it. When a process Qdelivers message m, it also becomes\na member of COPY (m). Note that COPY (m)consists of those processes that could\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "498 CHAPTER 8. FAULT TOLERANCE\nhand over a copy of mthat can be used to replay the transmission of m. If all\nthese processes crash, replaying the transmission of mis clearly not feasible.\nUsing these notations, it is now easy to de\ufb01ne precisely what an orphan\nprocess is. Let FAIL denote the collection of crashed processes, and assume Qis\none of the survivors. Qis an orphan process if there is a message m, such that\nQis contained in DEP (m), while at the same time every process in COPY (m)has\ncrashed. More formally:\nQis orphaned,9m:Q2DEP (m) and COPY (m)\u0012FAIL\nIn other words, an orphan process appears when it is dependent on m, but there\nis no way to replay m\u2019s transmission.\nTo avoid orphan processes, we thus need to ensure that if each process in\nCOPY (m)crashed, then no surviving process is left in DEP (m). In other words, all\nprocesses in DEP (m)should have crashed as well. This condition can be enforced\nif we can guarantee that whenever a process becomes a member of DEP (m), it also\nbecomes a member of COPY (m). In other words, whenever a process becomes\ndependent on the delivery of m, it will keep a copy of m.\nThere are essentially two approaches that can now be followed. The \ufb01rst\napproach is represented by what are called pessimistic logging protocols . These\nprotocols take care that for each nonstable message m, there is at most one process\ndependent on m. In other words, pessimistic logging protocols ensure that each\nnonstable message mis delivered to at most one process. Note that as soon as m\nis delivered to, say process P,Pbecomes a member of COPY (m).\nThe worst that can happen is that process Pcrashes without mever having\nbeen logged. With pessimistic logging, Pis not allowed to send any messages\nafter the delivery of mwithout \ufb01rst having ensured that mhas been written to\nreliable storage. Consequently, no other processes will ever become dependent on\nthe delivery of mtoP, without having the possibility of replaying the transmission\nofm. In this way, orphan processes are always avoided.\nIn contrast, in an optimistic logging protocol , the actual work is done after\na crash occurs. In particular, assume that for some message m, each process in\nCOPY (m)has crashed. In an optimistic approach, any orphan process in DEP (m)\nis rolled back to a state in which it no longer belongs to DEP (m). Clearly, optimistic\nlogging protocols need to keep track of dependencies, which complicates their\nimplementation.\nAs pointed out by Elnozahy et al. [2002], pessimistic logging is so much\nsimpler than optimistic approaches, that it is the preferred way of message\nlogging in practical distributed systems design.\nRecovery-oriented computing\nA related way of handling recovery is essentially to start over again. The\nunderlying principle toward this way of masking failures is that it may be\nmuch cheaper to optimize for recovery, then it is aiming for systems that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n498 CHAPTER 8. FAULT TOLERANCE\nhand over a copy of mthat can be used to replay the transmission of m. If all\nthese processes crash, replaying the transmission of mis clearly not feasible.\nUsing these notations, it is now easy to de\ufb01ne precisely what an orphan\nprocess is. Let FAIL denote the collection of crashed processes, and assume Qis\none of the survivors. Qis an orphan process if there is a message m, such that\nQis contained in DEP (m), while at the same time every process in COPY (m)has\ncrashed. More formally:\nQis orphaned,9m:Q2DEP (m) and COPY (m)\u0012FAIL\nIn other words, an orphan process appears when it is dependent on m, but there\nis no way to replay m\u2019s transmission.\nTo avoid orphan processes, we thus need to ensure that if each process in\nCOPY (m)crashed, then no surviving process is left in DEP (m). In other words, all\nprocesses in DEP (m)should have crashed as well. This condition can be enforced\nif we can guarantee that whenever a process becomes a member of DEP (m), it also\nbecomes a member of COPY (m). In other words, whenever a process becomes\ndependent on the delivery of m, it will keep a copy of m.\nThere are essentially two approaches that can now be followed. The \ufb01rst\napproach is represented by what are called pessimistic logging protocols . These\nprotocols take care that for each nonstable message m, there is at most one process\ndependent on m. In other words, pessimistic logging protocols ensure that each\nnonstable message mis delivered to at most one process. Note that as soon as m\nis delivered to, say process P,Pbecomes a member of COPY (m).\nThe worst that can happen is that process Pcrashes without mever having\nbeen logged. With pessimistic logging, Pis not allowed to send any messages\nafter the delivery of mwithout \ufb01rst having ensured that mhas been written to\nreliable storage. Consequently, no other processes will ever become dependent on\nthe delivery of mtoP, without having the possibility of replaying the transmission\nofm. In this way, orphan processes are always avoided.\nIn contrast, in an optimistic logging protocol , the actual work is done after\na crash occurs. In particular, assume that for some message m, each process in\nCOPY (m)has crashed. In an optimistic approach, any orphan process in DEP (m)\nis rolled back to a state in which it no longer belongs to DEP (m). Clearly, optimistic\nlogging protocols need to keep track of dependencies, which complicates their\nimplementation.\nAs pointed out by Elnozahy et al. [2002], pessimistic logging is so much\nsimpler than optimistic approaches, that it is the preferred way of message\nlogging in practical distributed systems design.\nRecovery-oriented computing\nA related way of handling recovery is essentially to start over again. The\nunderlying principle toward this way of masking failures is that it may be\nmuch cheaper to optimize for recovery, then it is aiming for systems that\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "8.7. SUMMARY 499\nare free from failures for a long time. This approach is also referred to as\nrecovery-oriented computing [Candea et al., 2004a].\nThere are different \ufb02avors of recovery-oriented computing. One \ufb02avor is\nto simply reboot (part of a system) and has been explored to restart Internet\nservers [Candea et al., 2004b; 2006]. In order to be able to reboot only a\npart of the system, it is crucial the fault is properly localized. At that point,\nrebooting simply means deleting all instances of the identi\ufb01ed components,\nalong with the threads operating on them, and (often) to just restart the\nassociated requests. Note that fault localization itself may be a nontrivial\nexercise [Steinder and Sethi, 2004].\nTo enable rebooting as a practical recovery technique requires that compo-\nnents are largely decoupled in the sense that there are few or no dependencies\nbetween different components. If there are strong dependencies, then fault\nlocalization and analysis may still require that a complete server needs to be\nrestarted at which point applying traditional recovery techniques as the ones\nwe just discussed may be more ef\ufb01cient.\nAnother \ufb02avor of recovery-oriented computing is to apply checkpointing\nand recovery techniques, but to continue execution in a changed environment.\nThe basic idea here is that many failures can be simply avoided if programs are\ngiven some more buffer space, memory is zeroed before allocated, changing\nthe ordering of message delivery (as long as this does not affect semantics),\nand so on [Qin et al., 2005]. The key idea is to tackle software failures\n(whereas many of the techniques discussed so far are aimed at, or are based\non hardware failures). Because software execution is highly deterministic,\nchanging an execution environment may save the day, but, of course, without\nrepairing anything.\n8.7 Summary\nFault tolerance is an important subject in distributed systems design. Fault\ntolerance is de\ufb01ned as the characteristic by which a system can mask the\noccurrence and recovery from failures. In other words, a system is fault\ntolerant if it can continue to operate in the presence of failures.\nSeveral types of failures exist. A crash failure occurs when a process\nsimply halts. An omission failure occurs when a process does not respond to\nincoming requests. When a process responds too soon or too late to a request,\nit is said to exhibit a timing failure. Responding to an incoming request, but in\nthe wrong way, is an example of a response failure. The most dif\ufb01cult failures\nto handle are those by which a process exhibits any kind of failure, called\narbitrary or Byzantine failures.\nRedundancy is the key technique needed to achieve fault tolerance. When\napplied to processes, the notion of process groups becomes important. A\nprocess group consists of a number of processes that closely cooperate to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n8.7. SUMMARY 499\nare free from failures for a long time. This approach is also referred to as\nrecovery-oriented computing [Candea et al., 2004a].\nThere are different \ufb02avors of recovery-oriented computing. One \ufb02avor is\nto simply reboot (part of a system) and has been explored to restart Internet\nservers [Candea et al., 2004b; 2006]. In order to be able to reboot only a\npart of the system, it is crucial the fault is properly localized. At that point,\nrebooting simply means deleting all instances of the identi\ufb01ed components,\nalong with the threads operating on them, and (often) to just restart the\nassociated requests. Note that fault localization itself may be a nontrivial\nexercise [Steinder and Sethi, 2004].\nTo enable rebooting as a practical recovery technique requires that compo-\nnents are largely decoupled in the sense that there are few or no dependencies\nbetween different components. If there are strong dependencies, then fault\nlocalization and analysis may still require that a complete server needs to be\nrestarted at which point applying traditional recovery techniques as the ones\nwe just discussed may be more ef\ufb01cient.\nAnother \ufb02avor of recovery-oriented computing is to apply checkpointing\nand recovery techniques, but to continue execution in a changed environment.\nThe basic idea here is that many failures can be simply avoided if programs are\ngiven some more buffer space, memory is zeroed before allocated, changing\nthe ordering of message delivery (as long as this does not affect semantics),\nand so on [Qin et al., 2005]. The key idea is to tackle software failures\n(whereas many of the techniques discussed so far are aimed at, or are based\non hardware failures). Because software execution is highly deterministic,\nchanging an execution environment may save the day, but, of course, without\nrepairing anything.\n8.7 Summary\nFault tolerance is an important subject in distributed systems design. Fault\ntolerance is de\ufb01ned as the characteristic by which a system can mask the\noccurrence and recovery from failures. In other words, a system is fault\ntolerant if it can continue to operate in the presence of failures.\nSeveral types of failures exist. A crash failure occurs when a process\nsimply halts. An omission failure occurs when a process does not respond to\nincoming requests. When a process responds too soon or too late to a request,\nit is said to exhibit a timing failure. Responding to an incoming request, but in\nthe wrong way, is an example of a response failure. The most dif\ufb01cult failures\nto handle are those by which a process exhibits any kind of failure, called\narbitrary or Byzantine failures.\nRedundancy is the key technique needed to achieve fault tolerance. When\napplied to processes, the notion of process groups becomes important. A\nprocess group consists of a number of processes that closely cooperate to\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "500 CHAPTER 8. FAULT TOLERANCE\nprovide a service. In fault-tolerant process groups, one or more processes\ncan fail without affecting the availability of the service the group implements.\nOften, it is necessary that communication within the group be highly reliable,\nand adheres to stringent ordering and atomicity properties in order to achieve\nfault tolerance.\nThe real problem is that members of a process group need to reach consen-\nsus in the presence of various failures. Paxos is by now a well-established and\nhighly robust consensus algorithm. By using 2k+1servers, it can establish\nk-fault tolerance. However, we need a total of 3k+1servers if it is needed to\ndeal with arbitrary failures.\nReliable group communication, also called reliable multicasting, comes\nin different forms. As long as groups are relatively small, it turns out that\nimplementing reliability is feasible. However, as soon as very large groups\nneed to be supported, scalability of reliable multicasting becomes problematic.\nThe key issue in achieving scalability is to reduce the number of feedback\nmessages by which receivers report the (un)successful receipt of a multicasted\nmessage.\nMatters become worse when atomicity is to be provided. In atomic mul-\nticast protocols, it is essential that each group member has the same view\nconcerning to which members a multicasted message has been delivered.\nAtomic multicasting can be precisely formulated in terms of a virtual syn-\nchronous execution model. In essence, this model introduces boundaries\nbetween which group membership does not change and which messages are\nreliably transmitted. A message can never cross a boundary.\nGroup membership changes are an example where each process needs to\nagree on the same list of members. Such agreement can be reached by means\nof a commit protocol, of which the two-phase commit protocol is the most\nwidely applied. In a two-phase commit protocol, a coordinator \ufb01rst checks\nwhether all processes agree to perform the same operation (i.e., whether\nthey all agree to commit), and in a second round, multicasts the outcome of\nthat poll. A three-phase commit protocol is used to handle the crash of the\ncoordinator without having to block all processes to reach agreement until\nthe coordinator recovers.\nRecovery in fault-tolerant systems is invariably achieved by checkpointing\nthe state of the system on a regular basis. Checkpointing is completely\ndistributed. Unfortunately, taking a checkpoint is an expensive operation.\nTo improve performance, many distributed systems combine checkpointing\nwith message logging. By logging the communication between processes,\nit becomes possible to replay the execution of the system after a crash has\noccurred.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n500 CHAPTER 8. FAULT TOLERANCE\nprovide a service. In fault-tolerant process groups, one or more processes\ncan fail without affecting the availability of the service the group implements.\nOften, it is necessary that communication within the group be highly reliable,\nand adheres to stringent ordering and atomicity properties in order to achieve\nfault tolerance.\nThe real problem is that members of a process group need to reach consen-\nsus in the presence of various failures. Paxos is by now a well-established and\nhighly robust consensus algorithm. By using 2k+1servers, it can establish\nk-fault tolerance. However, we need a total of 3k+1servers if it is needed to\ndeal with arbitrary failures.\nReliable group communication, also called reliable multicasting, comes\nin different forms. As long as groups are relatively small, it turns out that\nimplementing reliability is feasible. However, as soon as very large groups\nneed to be supported, scalability of reliable multicasting becomes problematic.\nThe key issue in achieving scalability is to reduce the number of feedback\nmessages by which receivers report the (un)successful receipt of a multicasted\nmessage.\nMatters become worse when atomicity is to be provided. In atomic mul-\nticast protocols, it is essential that each group member has the same view\nconcerning to which members a multicasted message has been delivered.\nAtomic multicasting can be precisely formulated in terms of a virtual syn-\nchronous execution model. In essence, this model introduces boundaries\nbetween which group membership does not change and which messages are\nreliably transmitted. A message can never cross a boundary.\nGroup membership changes are an example where each process needs to\nagree on the same list of members. Such agreement can be reached by means\nof a commit protocol, of which the two-phase commit protocol is the most\nwidely applied. In a two-phase commit protocol, a coordinator \ufb01rst checks\nwhether all processes agree to perform the same operation (i.e., whether\nthey all agree to commit), and in a second round, multicasts the outcome of\nthat poll. A three-phase commit protocol is used to handle the crash of the\ncoordinator without having to block all processes to reach agreement until\nthe coordinator recovers.\nRecovery in fault-tolerant systems is invariably achieved by checkpointing\nthe state of the system on a regular basis. Checkpointing is completely\ndistributed. Unfortunately, taking a checkpoint is an expensive operation.\nTo improve performance, many distributed systems combine checkpointing\nwith message logging. By logging the communication between processes,\nit becomes possible to replay the execution of the system after a crash has\noccurred.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "Chapter 9\nSecurity\nThe last principle of distributed systems that we discuss is security. Security\nis by no means the least important principle. However, one could argue that\nit is one of the most dif\ufb01cult principles, as security needs to be pervasive\nthroughout a system. A single design \ufb02aw with respect to security may render\nall security measures useless. In this chapter, we concentrate on the various\nmechanisms that are generally incorporated in distributed systems to support\nsecurity.\nWe start with introducing the basic issues of security. Building all kinds\nof security mechanisms into a system does not really make sense unless it is\nknown how those mechanisms are to be used, and against what. This requires\nthat we know about the security policy that is to be enforced. The notion of a\nsecurity policy, along with some general design issues for mechanisms that\nhelp enforce such policies, are discussed \ufb01rst. We also brie\ufb02y touch upon the\nnecessary cryptography.\nSecurity in distributed systems can roughly be divided into two parts.\nOne part concerns the communication between users or processes, possibly\nresiding on different machines. The principal mechanism for ensuring secure\ncommunication is that of a secure channel. Secure channels, and more speci\ufb01-\ncally, authentication, message integrity, and con\ufb01dentiality, are discussed in a\nseparate section.\nThe other part concerns authorization, which deals with ensuring that a\nprocess gets only those access rights to the resources in a distributed system\nit is entitled to. Authorization is covered in a separate section dealing with\naccess control. In addition to traditional access control mechanisms, we also\nfocus on access control when we have to deal with mobile code such as agents.\nWe also return to naming, and pay attention to the rather nasty problem\nof making sure that the name used to retrieve an object belongs to that object,\nbut also how to combine secure naming with human-friendly names.\nSecure channels and access control require mechanisms to distribute cryp-\n501\nChapter 9\nSecurity\nThe last principle of distributed systems that we discuss is security. Security\nis by no means the least important principle. However, one could argue that\nit is one of the most dif\ufb01cult principles, as security needs to be pervasive\nthroughout a system. A single design \ufb02aw with respect to security may render\nall security measures useless. In this chapter, we concentrate on the various\nmechanisms that are generally incorporated in distributed systems to support\nsecurity.\nWe start with introducing the basic issues of security. Building all kinds\nof security mechanisms into a system does not really make sense unless it is\nknown how those mechanisms are to be used, and against what. This requires\nthat we know about the security policy that is to be enforced. The notion of a\nsecurity policy, along with some general design issues for mechanisms that\nhelp enforce such policies, are discussed \ufb01rst. We also brie\ufb02y touch upon the\nnecessary cryptography.\nSecurity in distributed systems can roughly be divided into two parts.\nOne part concerns the communication between users or processes, possibly\nresiding on different machines. The principal mechanism for ensuring secure\ncommunication is that of a secure channel. Secure channels, and more speci\ufb01-\ncally, authentication, message integrity, and con\ufb01dentiality, are discussed in a\nseparate section.\nThe other part concerns authorization, which deals with ensuring that a\nprocess gets only those access rights to the resources in a distributed system\nit is entitled to. Authorization is covered in a separate section dealing with\naccess control. In addition to traditional access control mechanisms, we also\nfocus on access control when we have to deal with mobile code such as agents.\nWe also return to naming, and pay attention to the rather nasty problem\nof making sure that the name used to retrieve an object belongs to that object,\nbut also how to combine secure naming with human-friendly names.\nSecure channels and access control require mechanisms to distribute cryp-\n501", "502 CHAPTER 9. SECURITY\ntographic keys, but also mechanisms to add and remove users from a system.\nThese topics are covered by what is known as security management. In a\nseparate section, we discuss issues dealing with managing cryptographic keys,\nsecure group management, and handing out certi\ufb01cates that prove the owner\nis entitled to access speci\ufb01ed resources.\n9.1 Introduction to security\nWe start our description of security in distributed systems by taking a look\nat some general security issues. First, it is necessary to de\ufb01ne what a secure\nsystem is. We distinguish security policies from security mechanisms . Our\nsecond concern is to consider some general design issues for secure systems.\nFinally, we brie\ufb02y discuss some cryptographic algorithms, which play a key\nrole in the design of security protocols.\nSecurity threats, policies, and mechanisms\nSecurity in a computer system is strongly related to the notion of dependability.\nInformally, a dependable computer system is one that we justi\ufb01ably trust\nto deliver its services [Laprie, 1995]. Dependability includes availability,\nreliability, safety, and maintainability. However, if we are to put our trust in\na computer system, then con\ufb01dentiality and integrity should also be taken\ninto account. Con\ufb01dentiality refers to the property of a computer system\nwhereby its information is disclosed only to authorized parties. Integrity is\nthe characteristic that alterations to a system\u2019s assets can be made only in an\nauthorized way. In other words, improper alterations in a secure computer\nsystem should be detectable and recoverable. Major assets of any computer\nsystem are its hardware, software, and data.\nAnother way of looking at security in computer systems is that we attempt\nto protect the services and data it offers against security threats . There are\nfour types of security threats to consider [P\ufb02eeger, 2003]:\n1. Interception\n2. Interruption\n3. Modi\ufb01cation\n4. Fabrication\nThe concept of interception refers to the situation that an unauthorized party\nhas gained access to a service or data. A typical example of interception is\nwhere communication between two parties has been overheard by someone\nelse. Interception also happens when data are illegally copied, for example,\nafter breaking into a person\u2019s private directory in a \ufb01le system.\nAn example of interruption is when a \ufb01le is corrupted or lost. More\ngenerally interruption refers to the situation in which services or data become\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n502 CHAPTER 9. SECURITY\ntographic keys, but also mechanisms to add and remove users from a system.\nThese topics are covered by what is known as security management. In a\nseparate section, we discuss issues dealing with managing cryptographic keys,\nsecure group management, and handing out certi\ufb01cates that prove the owner\nis entitled to access speci\ufb01ed resources.\n9.1 Introduction to security\nWe start our description of security in distributed systems by taking a look\nat some general security issues. First, it is necessary to de\ufb01ne what a secure\nsystem is. We distinguish security policies from security mechanisms . Our\nsecond concern is to consider some general design issues for secure systems.\nFinally, we brie\ufb02y discuss some cryptographic algorithms, which play a key\nrole in the design of security protocols.\nSecurity threats, policies, and mechanisms\nSecurity in a computer system is strongly related to the notion of dependability.\nInformally, a dependable computer system is one that we justi\ufb01ably trust\nto deliver its services [Laprie, 1995]. Dependability includes availability,\nreliability, safety, and maintainability. However, if we are to put our trust in\na computer system, then con\ufb01dentiality and integrity should also be taken\ninto account. Con\ufb01dentiality refers to the property of a computer system\nwhereby its information is disclosed only to authorized parties. Integrity is\nthe characteristic that alterations to a system\u2019s assets can be made only in an\nauthorized way. In other words, improper alterations in a secure computer\nsystem should be detectable and recoverable. Major assets of any computer\nsystem are its hardware, software, and data.\nAnother way of looking at security in computer systems is that we attempt\nto protect the services and data it offers against security threats . There are\nfour types of security threats to consider [P\ufb02eeger, 2003]:\n1. Interception\n2. Interruption\n3. Modi\ufb01cation\n4. Fabrication\nThe concept of interception refers to the situation that an unauthorized party\nhas gained access to a service or data. A typical example of interception is\nwhere communication between two parties has been overheard by someone\nelse. Interception also happens when data are illegally copied, for example,\nafter breaking into a person\u2019s private directory in a \ufb01le system.\nAn example of interruption is when a \ufb01le is corrupted or lost. More\ngenerally interruption refers to the situation in which services or data become\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.1. INTRODUCTION TO SECURITY 503\nunavailable, unusable, destroyed, and so on. In this sense, denial of service\nattacks by which someone maliciously attempts to make a service inaccessible\nto other parties is a security threat that classi\ufb01es as interruption.\nModi\ufb01cations involve unauthorized changing of data or tampering with\na service so that it no longer adheres to its original speci\ufb01cations. Examples\nof modi\ufb01cations include intercepting and subsequently changing transmitted\ndata, tampering with database entries, and changing a program so that it\nsecretly logs the activities of its user.\nFabrication refers to the situation in which additional data or activity\nare generated that would normally not exist. For example, an intruder may\nattempt to add an entry into a password \ufb01le or database. Likewise, it is\nsometimes possible to break into a system by replaying previously sent\nmessages. We shall come across such examples later in this chapter.\nNote that interruption, modi\ufb01cation, and fabrication can each be seen as a\nform of data falsi\ufb01cation.\nSimply stating that a system should be able to protect itself against all\npossible security threats is not the way to actually build a secure system. What\nis \ufb01rst needed is a description of security requirements, that is, a security\npolicy. A security policy describes precisely which actions the entities in a\nsystem are allowed to take and which ones are prohibited. Entities include\nusers, services, data, machines, and so on. Once a security policy has been\nlaid down, it becomes possible to concentrate on the security mechanisms by\nwhich a policy can be enforced. Important security mechanisms are:\n1. Encryption\n2. Authentication\n3. Authorization\n4. Auditing\nEncryption is fundamental to computer security. Encryption transforms data\ninto something an attacker cannot understand. In other words, encryption\nprovides a means to implement data con\ufb01dentiality. In addition, encryption\nallows us to check whether data have been modi\ufb01ed. It thus also provides\nsupport for integrity checks.\nAuthentication is used to verify the claimed identity of a user, client, server,\nhost, or other entity. In the case of clients, the basic premise is that before a\nservice starts to perform any work on behalf of a client, the service must learn\nthe client\u2019s identity (unless the service is available to all). Typically, users\nare authenticated by means of passwords, but there are many other ways to\nauthenticate clients.\nAfter a client has been authenticated, it is necessary to check whether\nthat client is authorized to perform the action requested. Access to records\nin a medical database is a typical example. Depending on who accesses the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.1. INTRODUCTION TO SECURITY 503\nunavailable, unusable, destroyed, and so on. In this sense, denial of service\nattacks by which someone maliciously attempts to make a service inaccessible\nto other parties is a security threat that classi\ufb01es as interruption.\nModi\ufb01cations involve unauthorized changing of data or tampering with\na service so that it no longer adheres to its original speci\ufb01cations. Examples\nof modi\ufb01cations include intercepting and subsequently changing transmitted\ndata, tampering with database entries, and changing a program so that it\nsecretly logs the activities of its user.\nFabrication refers to the situation in which additional data or activity\nare generated that would normally not exist. For example, an intruder may\nattempt to add an entry into a password \ufb01le or database. Likewise, it is\nsometimes possible to break into a system by replaying previously sent\nmessages. We shall come across such examples later in this chapter.\nNote that interruption, modi\ufb01cation, and fabrication can each be seen as a\nform of data falsi\ufb01cation.\nSimply stating that a system should be able to protect itself against all\npossible security threats is not the way to actually build a secure system. What\nis \ufb01rst needed is a description of security requirements, that is, a security\npolicy. A security policy describes precisely which actions the entities in a\nsystem are allowed to take and which ones are prohibited. Entities include\nusers, services, data, machines, and so on. Once a security policy has been\nlaid down, it becomes possible to concentrate on the security mechanisms by\nwhich a policy can be enforced. Important security mechanisms are:\n1. Encryption\n2. Authentication\n3. Authorization\n4. Auditing\nEncryption is fundamental to computer security. Encryption transforms data\ninto something an attacker cannot understand. In other words, encryption\nprovides a means to implement data con\ufb01dentiality. In addition, encryption\nallows us to check whether data have been modi\ufb01ed. It thus also provides\nsupport for integrity checks.\nAuthentication is used to verify the claimed identity of a user, client, server,\nhost, or other entity. In the case of clients, the basic premise is that before a\nservice starts to perform any work on behalf of a client, the service must learn\nthe client\u2019s identity (unless the service is available to all). Typically, users\nare authenticated by means of passwords, but there are many other ways to\nauthenticate clients.\nAfter a client has been authenticated, it is necessary to check whether\nthat client is authorized to perform the action requested. Access to records\nin a medical database is a typical example. Depending on who accesses the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "504 CHAPTER 9. SECURITY\ndatabase, permission may be granted to read records, to modify certain \ufb01elds\nin a record, or to add or remove a record.\nAuditing tools are used to trace which clients accessed what, and in\nwhich way. Although auditing does not really provide any protection against\nsecurity threats, audit logs can be extremely useful for the analysis of a security\nbreach, and subsequently taking measures against intruders. For this reason,\nattackers are generally keen not to leave any traces that could eventually lead\nto exposing their identity. In this sense, logging accesses makes attacking\nsometimes a riskier business.\nDesign issues\nA distributed system, or any computer system for that matter, must provide\nsecurity services by which a wide range of security policies can be imple-\nmented. There are a number of important design issues that need to be taken\ninto account when implementing general-purpose security services. In the\nfollowing pages, we discuss three of these issues: focus of control, layering of\nsecurity mechanisms, and simplicity (see also Gollmann [2006]).\nFocus of control\nWhen considering the protection of a (possibly distributed) application, there\nare essentially three different approaches that can be followed, as shown in\nFigure 9.1. The \ufb01rst approach is to concentrate directly on the protection\nof the data that is associated with the application. By direct, we mean that\nirrespective of the various operations that can possibly be performed on a\ndata item, the primary concern is to ensure data integrity. Typically, this type\nof protection occurs in database systems in which various integrity constraints\ncan be formulated that are automatically checked each time a data item is\nmodi\ufb01ed (see, for example, [Doorn and Rivero, 2002]).\nThe second approach is to concentrate on protection by specifying exactly\nwhich operations may be invoked, and by whom, when certain data or\nresources are to be accessed. In this case, the focus of control is strongly\nrelated to access control mechanisms, which we discuss extensively later in\nthis chapter. For example, in an object-based system, it may be decided to\nspecify for each method that is made available to clients which clients are\npermitted to invoke that method. Alternatively, access control methods can be\napplied to an entire interface offered by an object, or to the entire object itself.\nThis approach thus allows for various granularities of access control.\nA third approach is to focus directly on users by taking measures by\nwhich only speci\ufb01c people have access to the application, irrespective of the\noperations they want to carry out. For example, a database in a bank may be\nprotected by denying access to anyone except the bank\u2019s upper management\nand people speci\ufb01cally authorized to access it. As another example, in many\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n504 CHAPTER 9. SECURITY\ndatabase, permission may be granted to read records, to modify certain \ufb01elds\nin a record, or to add or remove a record.\nAuditing tools are used to trace which clients accessed what, and in\nwhich way. Although auditing does not really provide any protection against\nsecurity threats, audit logs can be extremely useful for the analysis of a security\nbreach, and subsequently taking measures against intruders. For this reason,\nattackers are generally keen not to leave any traces that could eventually lead\nto exposing their identity. In this sense, logging accesses makes attacking\nsometimes a riskier business.\nDesign issues\nA distributed system, or any computer system for that matter, must provide\nsecurity services by which a wide range of security policies can be imple-\nmented. There are a number of important design issues that need to be taken\ninto account when implementing general-purpose security services. In the\nfollowing pages, we discuss three of these issues: focus of control, layering of\nsecurity mechanisms, and simplicity (see also Gollmann [2006]).\nFocus of control\nWhen considering the protection of a (possibly distributed) application, there\nare essentially three different approaches that can be followed, as shown in\nFigure 9.1. The \ufb01rst approach is to concentrate directly on the protection\nof the data that is associated with the application. By direct, we mean that\nirrespective of the various operations that can possibly be performed on a\ndata item, the primary concern is to ensure data integrity. Typically, this type\nof protection occurs in database systems in which various integrity constraints\ncan be formulated that are automatically checked each time a data item is\nmodi\ufb01ed (see, for example, [Doorn and Rivero, 2002]).\nThe second approach is to concentrate on protection by specifying exactly\nwhich operations may be invoked, and by whom, when certain data or\nresources are to be accessed. In this case, the focus of control is strongly\nrelated to access control mechanisms, which we discuss extensively later in\nthis chapter. For example, in an object-based system, it may be decided to\nspecify for each method that is made available to clients which clients are\npermitted to invoke that method. Alternatively, access control methods can be\napplied to an entire interface offered by an object, or to the entire object itself.\nThis approach thus allows for various granularities of access control.\nA third approach is to focus directly on users by taking measures by\nwhich only speci\ufb01c people have access to the application, irrespective of the\noperations they want to carry out. For example, a database in a bank may be\nprotected by denying access to anyone except the bank\u2019s upper management\nand people speci\ufb01cally authorized to access it. As another example, in many\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.1. INTRODUCTION TO SECURITY 505\n(a) (b)\n(c)\nFigure 9.1: Three approaches for protection against security threats. (a) Protec-\ntion against invalid operations (b) Protection against unauthorized invocations.\n(c) Protection against unauthorized users.\nuniversities, certain data and applications are restricted to be used by faculty\nand staff members only, whereas access by students is not allowed. In effect,\ncontrol is focused on de\ufb01ning roles that users have, and once a user\u2019s role\nhas been veri\ufb01ed, access to a resource is either granted or denied. As part\nof designing a secure system, it is thus necessary to de\ufb01ne roles that people\nmay have, and provide mechanisms to support role-based access control. We\nreturn to roles later in this chapter.\nLayering of security mechanisms\nAn important issue in designing secure systems is to decide at which level\nsecurity mechanisms should be placed. A level in this context is related to the\nlogical organization of a system into a number of layers. For example, com-\nputer networks are often organized into layers following some reference model,\nas we discussed in Chapter 4. In Chapter 1, we introduced the organization of\ndistributed systems consisting of separate layers for applications, middleware,\noperating system services, and the operating system kernel. Combining the\ntwo organizations leads roughly to what is shown in Figure 9.2.\nIn essence, Figure 9.2 separates general-purpose services from communi-\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.1. INTRODUCTION TO SECURITY 505\n(a) (b)\n(c)\nFigure 9.1: Three approaches for protection against security threats. (a) Protec-\ntion against invalid operations (b) Protection against unauthorized invocations.\n(c) Protection against unauthorized users.\nuniversities, certain data and applications are restricted to be used by faculty\nand staff members only, whereas access by students is not allowed. In effect,\ncontrol is focused on de\ufb01ning roles that users have, and once a user\u2019s role\nhas been veri\ufb01ed, access to a resource is either granted or denied. As part\nof designing a secure system, it is thus necessary to de\ufb01ne roles that people\nmay have, and provide mechanisms to support role-based access control. We\nreturn to roles later in this chapter.\nLayering of security mechanisms\nAn important issue in designing secure systems is to decide at which level\nsecurity mechanisms should be placed. A level in this context is related to the\nlogical organization of a system into a number of layers. For example, com-\nputer networks are often organized into layers following some reference model,\nas we discussed in Chapter 4. In Chapter 1, we introduced the organization of\ndistributed systems consisting of separate layers for applications, middleware,\noperating system services, and the operating system kernel. Combining the\ntwo organizations leads roughly to what is shown in Figure 9.2.\nIn essence, Figure 9.2 separates general-purpose services from communi-\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "506 CHAPTER 9. SECURITY\nFigure 9.2: The logical organization of a distributed system into several layers.\ncation services. This separation is important for understanding the layering\nof security in distributed systems and, in particular, the notion of trust. The\ndifference between trust and security is important. A system is either secure\nor it is not (taking various probabilistic measures into account), but whether a\nclient considers a system to be secure is a matter of trust. Security is technical;\ntrust is emotional [Bishop, 2003]. In which layer security mechanisms are\nplaced depends on the trust a client has in how secure the services are in a\nparticular layer.\nAs an example, consider an organization located at different sites that\nare connected through a low-level backbone connecting various local-area\nnetworks at possibly geographically dispersed sites, as shown in Figure 9.3.\nSuch connections can be con\ufb01gured using techniques like Multiprotocol\nLabel Switching (MPLS ) or techniques for Virtual Private Network s (VPN ).\nFigure 9.3: Several sites connected through a wide-area backbone service.\nSecurity can be provided by placing encryption devices at each backbone\nswitch, as also shown in Figure 9.3. These devices automatically encrypt and\ndecrypt packets that are sent between sites, but do not otherwise provide\nsecure communication between hosts at the same site. If Alice at site Asends\na message to Bob at site B, and she is worried about her message being\nintercepted, she must at least trust the encryption of intersite traf\ufb01c to work\nproperly. This means, for example, that she must trust the system administra-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n506 CHAPTER 9. SECURITY\nFigure 9.2: The logical organization of a distributed system into several layers.\ncation services. This separation is important for understanding the layering\nof security in distributed systems and, in particular, the notion of trust. The\ndifference between trust and security is important. A system is either secure\nor it is not (taking various probabilistic measures into account), but whether a\nclient considers a system to be secure is a matter of trust. Security is technical;\ntrust is emotional [Bishop, 2003]. In which layer security mechanisms are\nplaced depends on the trust a client has in how secure the services are in a\nparticular layer.\nAs an example, consider an organization located at different sites that\nare connected through a low-level backbone connecting various local-area\nnetworks at possibly geographically dispersed sites, as shown in Figure 9.3.\nSuch connections can be con\ufb01gured using techniques like Multiprotocol\nLabel Switching (MPLS ) or techniques for Virtual Private Network s (VPN ).\nFigure 9.3: Several sites connected through a wide-area backbone service.\nSecurity can be provided by placing encryption devices at each backbone\nswitch, as also shown in Figure 9.3. These devices automatically encrypt and\ndecrypt packets that are sent between sites, but do not otherwise provide\nsecure communication between hosts at the same site. If Alice at site Asends\na message to Bob at site B, and she is worried about her message being\nintercepted, she must at least trust the encryption of intersite traf\ufb01c to work\nproperly. This means, for example, that she must trust the system administra-\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.1. INTRODUCTION TO SECURITY 507\ntors at both sites to have taken the proper measures against tampering with\nthe devices.\nNow suppose that Alice does not trust the security of intersite traf\ufb01c.\nShe may then decide to take her own measures by using, for example, the\nTransport Layer Security (TLS) service, which can be used to securely send\nmessages across a TCP connection. The important thing to observe here is\nthat TLS allows Alice to set up a secure connection to Bob. All transport-level\nmessages will be encrypted\u2013and in our example at the link level as well, but\nthat is of no concern to Alice. In this case, Alice will have to put her trust into\nTLS. In other words, she believes that TLS is secure.\nIn distributed systems, security mechanisms are often placed in the mid-\ndleware layer. If Alice does not trust TLS, she may want to use a local secure\nRPC service. Again, she will have to trust this RPC service to do what it\npromises, such as not leaking information or properly authenticating clients\nand servers.\nSecurity services that are placed in the middleware layer of a distributed\nsystem can be trusted only if the services they rely on to be secure are indeed\nsecure. For example, if a secure RPC service is partly implemented by means\nof TLS, then trust in the RPC service depends on how much trust one has in\nTLS. If TLS is not trusted, then there can be no trust in the security of the RPC\nservice.\nDistribution of security mechanisms\nDependencies between services regarding trust lead to the notion of a Trusted\nComputing Base (TCB ). A TCB is the set of all security mechanisms in a\n(distributed) computer system that are needed to enforce a security policy,\nand that thus need to be trusted. The smaller the TCB, the better. If a\ndistributed system is built as middleware on an existing network operating\nsystem, its security may depend on the security of the underlying local\noperating systems. In other words, the TCB in a distributed system may\ninclude the local operating systems at various hosts.\nConsider a \ufb01le server in a distributed \ufb01le system. Such a server may need\nto rely on the various protection mechanisms offered by its local operating\nsystem. Such mechanisms include not only those for protecting \ufb01les against\naccesses by processes other than the \ufb01le server, but also mechanisms to protect\nthe \ufb01le server from being maliciously brought down.\nMiddleware-based distributed systems thus require trust in the existing\nlocal operating systems they depend on. If such trust does not exist, then part\nof the functionality of the local operating systems may need to be incorporated\ninto the distributed system itself. Consider a microkernel operating system,\nin which most operating-system services run as normal user processes. In\nthis case, the \ufb01le system, for instance, can be entirely replaced by one tailored\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.1. INTRODUCTION TO SECURITY 507\ntors at both sites to have taken the proper measures against tampering with\nthe devices.\nNow suppose that Alice does not trust the security of intersite traf\ufb01c.\nShe may then decide to take her own measures by using, for example, the\nTransport Layer Security (TLS) service, which can be used to securely send\nmessages across a TCP connection. The important thing to observe here is\nthat TLS allows Alice to set up a secure connection to Bob. All transport-level\nmessages will be encrypted\u2013and in our example at the link level as well, but\nthat is of no concern to Alice. In this case, Alice will have to put her trust into\nTLS. In other words, she believes that TLS is secure.\nIn distributed systems, security mechanisms are often placed in the mid-\ndleware layer. If Alice does not trust TLS, she may want to use a local secure\nRPC service. Again, she will have to trust this RPC service to do what it\npromises, such as not leaking information or properly authenticating clients\nand servers.\nSecurity services that are placed in the middleware layer of a distributed\nsystem can be trusted only if the services they rely on to be secure are indeed\nsecure. For example, if a secure RPC service is partly implemented by means\nof TLS, then trust in the RPC service depends on how much trust one has in\nTLS. If TLS is not trusted, then there can be no trust in the security of the RPC\nservice.\nDistribution of security mechanisms\nDependencies between services regarding trust lead to the notion of a Trusted\nComputing Base (TCB ). A TCB is the set of all security mechanisms in a\n(distributed) computer system that are needed to enforce a security policy,\nand that thus need to be trusted. The smaller the TCB, the better. If a\ndistributed system is built as middleware on an existing network operating\nsystem, its security may depend on the security of the underlying local\noperating systems. In other words, the TCB in a distributed system may\ninclude the local operating systems at various hosts.\nConsider a \ufb01le server in a distributed \ufb01le system. Such a server may need\nto rely on the various protection mechanisms offered by its local operating\nsystem. Such mechanisms include not only those for protecting \ufb01les against\naccesses by processes other than the \ufb01le server, but also mechanisms to protect\nthe \ufb01le server from being maliciously brought down.\nMiddleware-based distributed systems thus require trust in the existing\nlocal operating systems they depend on. If such trust does not exist, then part\nof the functionality of the local operating systems may need to be incorporated\ninto the distributed system itself. Consider a microkernel operating system,\nin which most operating-system services run as normal user processes. In\nthis case, the \ufb01le system, for instance, can be entirely replaced by one tailored\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "508 CHAPTER 9. SECURITY\nto the speci\ufb01c needs of a distributed system, including its various security\nmeasures.\nConsistent with this approach is to separate security services from other\ntypes of services by distributing services across different machines depending\non the level of security required. For example, for a secure distributed \ufb01le\nsystem, it may be possible to isolate the \ufb01le server from clients by placing\nthe server on a machine with a trusted operating system, possibly running\na dedicated secure \ufb01le system. Clients and their applications are placed on\nuntrusted machines.\nThis separation effectively reduces the TCB to a relatively small number\nof machines and software components. By subsequently protecting those\nmachines against security attacks from the outside, overall trust in the security\nof the distributed system can be increased.\nSimplicity\nAnother important design issue related to deciding in which layer to place\nsecurity mechanisms is that of simplicity. Designing a secure computer system\nis generally considered a dif\ufb01cult task. Consequently, if a system designer can\nuse a few, simple mechanisms that are easily understood and trusted to work,\nthe better it is.\nUnfortunately, simple mechanisms are not always suf\ufb01cient for implement-\ning security policies. Consider once again the situation in which Alice wants\nto send a message to Bob as discussed above. Link-level encryption is a simple\nand easy-to-understand mechanism to protect against interception of intersite\nmessage traf\ufb01c. However, much more is needed if Alice wants to be sure that\nonly Bob will receive her messages. In that case, user-level authentication\nservices are needed, and Alice may need to be aware of how such services\nwork in order to put her trust in it. User-level authentication may therefore\nrequire at least a notion of cryptographic keys and awareness of mechanisms\nsuch as certi\ufb01cates, despite the fact that many security services are highly\nautomated and hidden from users.\nIn other cases, the application itself is inherently complex and introducing\nsecurity only makes matters worse. An example application domain involving\ncomplex security protocols is that of digital payment systems. The complexity\nof digital payment protocols is often caused by the fact that multiple parties\nneed to communicate to make a payment. In these cases, it is important that\nthe underlying mechanisms that are used to implement the protocols are\nrelatively simple and easy to understand. Simplicity will contribute to the\ntrust that end users will put into the application and, more importantly, will\ncontribute to convincing the designers that the system has no security holes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n508 CHAPTER 9. SECURITY\nto the speci\ufb01c needs of a distributed system, including its various security\nmeasures.\nConsistent with this approach is to separate security services from other\ntypes of services by distributing services across different machines depending\non the level of security required. For example, for a secure distributed \ufb01le\nsystem, it may be possible to isolate the \ufb01le server from clients by placing\nthe server on a machine with a trusted operating system, possibly running\na dedicated secure \ufb01le system. Clients and their applications are placed on\nuntrusted machines.\nThis separation effectively reduces the TCB to a relatively small number\nof machines and software components. By subsequently protecting those\nmachines against security attacks from the outside, overall trust in the security\nof the distributed system can be increased.\nSimplicity\nAnother important design issue related to deciding in which layer to place\nsecurity mechanisms is that of simplicity. Designing a secure computer system\nis generally considered a dif\ufb01cult task. Consequently, if a system designer can\nuse a few, simple mechanisms that are easily understood and trusted to work,\nthe better it is.\nUnfortunately, simple mechanisms are not always suf\ufb01cient for implement-\ning security policies. Consider once again the situation in which Alice wants\nto send a message to Bob as discussed above. Link-level encryption is a simple\nand easy-to-understand mechanism to protect against interception of intersite\nmessage traf\ufb01c. However, much more is needed if Alice wants to be sure that\nonly Bob will receive her messages. In that case, user-level authentication\nservices are needed, and Alice may need to be aware of how such services\nwork in order to put her trust in it. User-level authentication may therefore\nrequire at least a notion of cryptographic keys and awareness of mechanisms\nsuch as certi\ufb01cates, despite the fact that many security services are highly\nautomated and hidden from users.\nIn other cases, the application itself is inherently complex and introducing\nsecurity only makes matters worse. An example application domain involving\ncomplex security protocols is that of digital payment systems. The complexity\nof digital payment protocols is often caused by the fact that multiple parties\nneed to communicate to make a payment. In these cases, it is important that\nthe underlying mechanisms that are used to implement the protocols are\nrelatively simple and easy to understand. Simplicity will contribute to the\ntrust that end users will put into the application and, more importantly, will\ncontribute to convincing the designers that the system has no security holes.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.1. INTRODUCTION TO SECURITY 509\nNote 9.1 (Advanced: Incorporating security into a development methodology)\nIf making a (distributed) system secure is so dif\ufb01cult, it may be wise not to just\nstate that security should be incorporated from the initial phases of development,\nbut actually support secure design in a methodological way. Several approaches\nexist to do just this, as surveyed by Uzunov et al. [2012]. The authors distinguish\ncode-based and model-based software development and life-cycle methodologies.\nIn the former, the main idea is enforce various security features without taking\na system\u2019s overall design or architecture into account. A simple example is always\nenforcing secure network communication, regardless whether such security is\nalways strictly necessary. Likewise, one may want to ensure that imported code\nis always run in a (secure) virtual machine., or that buffers can provably never\nover\ufb02ow. However, code-based methodologies also incorporate aspects such as\nthreat modeling and penetration testing.\nAlthough code-based methodologies appear to be more widely applied, model-\nbased methodologies are considered to be more superior. The essence is that\nmodel-based approaches take the entire design or architecture of a distributed\nsystem into account. Typically, they are integrated in the design process by\nenhancing modeling languages such as UML. The whole idea is that various\nsecurity features are made explicit in the early stages of requirements engineering\nand design. Unfortunately, this makes sense only if some guarantee can be given\nthat what is modeled is also (correctly) implemented. In practice, this turns\nout to be an obstacle, unless there are automated means to go from model to\nimplementation.\nIn the end, Uzunov et al. conclude that to successfully engineer security into\ndistributed systems there is still a long way to go. Their conclusion illustrates the\ndif\ufb01culty in systematically making systems secure. Such is life.\nCryptography\nFundamental to security in distributed systems is the use of cryptographic\ntechniques. The basic idea of applying these techniques is simple. Consider a\nsender Swanting to transmit message mto a receiver R. To protect the message\nagainst security threats, the sender \ufb01rst encrypts it into an unintelligible\nmessage m0, and subsequently sends m0toR.R, in turn, must decrypt the\nreceived message into its original form m.\nEncryption and decryption are accomplished by using cryptographic meth-\nods parameterized by keys, as shown in Figure 9.4. The original form of\nthe message that is sent is called the plaintext , shown as Pin Figure 9.4 the\nencrypted form is referred to as the ciphertext , illustrated as C.\nTo describe the various security protocols that are used in building security\nservices for distributed systems, it is useful to have a notation to relate plain-\ntext, ciphertext, and keys. Following the common notational conventions, we\nwill use C=EK(P)to denote that the ciphertext Cis obtained by encrypting\nthe plaintext Pusing key K. Likewise, P=DK(C)is used to express the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.1. INTRODUCTION TO SECURITY 509\nNote 9.1 (Advanced: Incorporating security into a development methodology)\nIf making a (distributed) system secure is so dif\ufb01cult, it may be wise not to just\nstate that security should be incorporated from the initial phases of development,\nbut actually support secure design in a methodological way. Several approaches\nexist to do just this, as surveyed by Uzunov et al. [2012]. The authors distinguish\ncode-based and model-based software development and life-cycle methodologies.\nIn the former, the main idea is enforce various security features without taking\na system\u2019s overall design or architecture into account. A simple example is always\nenforcing secure network communication, regardless whether such security is\nalways strictly necessary. Likewise, one may want to ensure that imported code\nis always run in a (secure) virtual machine., or that buffers can provably never\nover\ufb02ow. However, code-based methodologies also incorporate aspects such as\nthreat modeling and penetration testing.\nAlthough code-based methodologies appear to be more widely applied, model-\nbased methodologies are considered to be more superior. The essence is that\nmodel-based approaches take the entire design or architecture of a distributed\nsystem into account. Typically, they are integrated in the design process by\nenhancing modeling languages such as UML. The whole idea is that various\nsecurity features are made explicit in the early stages of requirements engineering\nand design. Unfortunately, this makes sense only if some guarantee can be given\nthat what is modeled is also (correctly) implemented. In practice, this turns\nout to be an obstacle, unless there are automated means to go from model to\nimplementation.\nIn the end, Uzunov et al. conclude that to successfully engineer security into\ndistributed systems there is still a long way to go. Their conclusion illustrates the\ndif\ufb01culty in systematically making systems secure. Such is life.\nCryptography\nFundamental to security in distributed systems is the use of cryptographic\ntechniques. The basic idea of applying these techniques is simple. Consider a\nsender Swanting to transmit message mto a receiver R. To protect the message\nagainst security threats, the sender \ufb01rst encrypts it into an unintelligible\nmessage m0, and subsequently sends m0toR.R, in turn, must decrypt the\nreceived message into its original form m.\nEncryption and decryption are accomplished by using cryptographic meth-\nods parameterized by keys, as shown in Figure 9.4. The original form of\nthe message that is sent is called the plaintext , shown as Pin Figure 9.4 the\nencrypted form is referred to as the ciphertext , illustrated as C.\nTo describe the various security protocols that are used in building security\nservices for distributed systems, it is useful to have a notation to relate plain-\ntext, ciphertext, and keys. Following the common notational conventions, we\nwill use C=EK(P)to denote that the ciphertext Cis obtained by encrypting\nthe plaintext Pusing key K. Likewise, P=DK(C)is used to express the\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "510 CHAPTER 9. SECURITY\nFigure 9.4: Intruders and eavesdroppers in communication.\ndecryption of the ciphertext Cusing key K, resulting in the plaintext P.\nReturning to our example shown in Figure 9.4 while transferring a message\nas ciphertext C, there are three different attacks that we need to protect against,\nand for which encryption helps. First, an intruder may intercept the message\nwithout either the sender or receiver being aware that eavesdropping is\nhappening. Of course, if the transmitted message has been encrypted in\nsuch a way that it cannot be easily decrypted without having the proper key,\ninterception is useless: the intruder will see only unintelligible data. (By the\nway, the fact alone that a message is being transmitted may sometimes be\nenough for an intruder to draw conclusions. For example, if during a world\ncrisis the amount of traf\ufb01c into the White House suddenly drops dramatically\nwhile the amount of traf\ufb01c going into a certain mountain in Colorado increases\nby the same amount, there may be useful information in knowing that.)\nThe second type of attack that needs to be dealt with is that of modifying\nthe message. Modifying plaintext is easy; modifying ciphertext that has been\nproperly encrypted is much more dif\ufb01cult because the intruder will \ufb01rst have\nto decrypt the message before he can meaningfully modify it. In addition,\nhe will also have to properly encrypt it again or otherwise the receiver may\nnotice that the message has been tampered with.\nThe third type of attack is when an intruder inserts encrypted messages\ninto the communication system, attempting to make Rbelieve these messages\ncame from S. Again, encryption can help protect against such attacks. Note\nthat if an intruder can modify messages, he can also insert messages.\nThere is a fundamental distinction between different cryptographic sys-\ntems, based on whether or not the encryption and decryption key are the\nsame. In a symmetric cryptosystem , the same key is used to encrypt and\ndecrypt a message:\nP=DK(EK(P))\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n510 CHAPTER 9. SECURITY\nFigure 9.4: Intruders and eavesdroppers in communication.\ndecryption of the ciphertext Cusing key K, resulting in the plaintext P.\nReturning to our example shown in Figure 9.4 while transferring a message\nas ciphertext C, there are three different attacks that we need to protect against,\nand for which encryption helps. First, an intruder may intercept the message\nwithout either the sender or receiver being aware that eavesdropping is\nhappening. Of course, if the transmitted message has been encrypted in\nsuch a way that it cannot be easily decrypted without having the proper key,\ninterception is useless: the intruder will see only unintelligible data. (By the\nway, the fact alone that a message is being transmitted may sometimes be\nenough for an intruder to draw conclusions. For example, if during a world\ncrisis the amount of traf\ufb01c into the White House suddenly drops dramatically\nwhile the amount of traf\ufb01c going into a certain mountain in Colorado increases\nby the same amount, there may be useful information in knowing that.)\nThe second type of attack that needs to be dealt with is that of modifying\nthe message. Modifying plaintext is easy; modifying ciphertext that has been\nproperly encrypted is much more dif\ufb01cult because the intruder will \ufb01rst have\nto decrypt the message before he can meaningfully modify it. In addition,\nhe will also have to properly encrypt it again or otherwise the receiver may\nnotice that the message has been tampered with.\nThe third type of attack is when an intruder inserts encrypted messages\ninto the communication system, attempting to make Rbelieve these messages\ncame from S. Again, encryption can help protect against such attacks. Note\nthat if an intruder can modify messages, he can also insert messages.\nThere is a fundamental distinction between different cryptographic sys-\ntems, based on whether or not the encryption and decryption key are the\nsame. In a symmetric cryptosystem , the same key is used to encrypt and\ndecrypt a message:\nP=DK(EK(P))\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.1. INTRODUCTION TO SECURITY 511\nSymmetric cryptosystems are also referred to as secret-key or shared-key\nsystems, because the sender and receiver are required to share the same key,\nand to ensure that protection works, this shared key must be kept secret; no\none else is allowed to see the key. We will use the notation KA,Bto denote a\nkey shared by Aand B.\nIn an asymmetric cryptosystem , the keys for encryption and decryption\nare different, but together form a unique pair. In other words, there is a\nseparate key KEfor encryption and one for decryption, KD, such that\nP=DKD(EKE(P))\nOne of the keys in an asymmetric cryptosystem is kept private; the other is\nmade public. For this reason, asymmetric cryptosystems are also referred to\naspublic-key systems . In what follows, we use the notation K+\nAto denote a\npublic key belonging to A, and K\u0000\nAas its corresponding private key.\nWhich one of the encryption or decryption keys that is actually made\npublic depends on how the keys are used. For example, if Alice wants to send\na con\ufb01dential message to Bob, she should use Bob\u2019s public key to encrypt\nthe message. Because Bob is the only one holding the associated and private\ndecryption key, he is also the only person that can decrypt the message.\nOn the other hand, suppose that Bob wants to know for sure that the\nmessage he just received actually came from Alice. In that case, Alice can\nkeep her encryption key private to encrypt the messages she sends. If Bob\ncan successfully decrypt a message using Alice\u2019s public key (and the plaintext\nin the message has enough information to make it meaningful to Bob), he\nknows that message must have come from Alice, because the decryption key\nis uniquely tied to the encryption key.\nOne \ufb01nal application of cryptography in distributed systems is the use of\nhash functions . A hash function Htakes a message mof arbitrary length as\ninput and produces a bit string hhaving a \ufb01xed length as output:\nh=H(m)\nA hash his somewhat comparable to the extra bits that are appended to\na message in communication systems to allow for error detection, such a\ncyclic-redundancy check (CRC).\nHash functions that are used in cryptographic systems have a number\nof essential properties. First, they are one-way functions , meaning that it is\ncomputationally infeasible to \ufb01nd the input mthat corresponds to a known\noutput h. On the other hand, computing hfrom mis easy. Second, they have\ntheweak collision resistance property, meaning that given an input mand its\nassociated output h=H(m), it is computationally infeasible to \ufb01nd another,\ndifferent input m06=m, such that H(m) =H(m0). Finally, cryptographic hash\nfunctions also have the strong collision resistance property, which means\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.1. INTRODUCTION TO SECURITY 511\nSymmetric cryptosystems are also referred to as secret-key or shared-key\nsystems, because the sender and receiver are required to share the same key,\nand to ensure that protection works, this shared key must be kept secret; no\none else is allowed to see the key. We will use the notation KA,Bto denote a\nkey shared by Aand B.\nIn an asymmetric cryptosystem , the keys for encryption and decryption\nare different, but together form a unique pair. In other words, there is a\nseparate key KEfor encryption and one for decryption, KD, such that\nP=DKD(EKE(P))\nOne of the keys in an asymmetric cryptosystem is kept private; the other is\nmade public. For this reason, asymmetric cryptosystems are also referred to\naspublic-key systems . In what follows, we use the notation K+\nAto denote a\npublic key belonging to A, and K\u0000\nAas its corresponding private key.\nWhich one of the encryption or decryption keys that is actually made\npublic depends on how the keys are used. For example, if Alice wants to send\na con\ufb01dential message to Bob, she should use Bob\u2019s public key to encrypt\nthe message. Because Bob is the only one holding the associated and private\ndecryption key, he is also the only person that can decrypt the message.\nOn the other hand, suppose that Bob wants to know for sure that the\nmessage he just received actually came from Alice. In that case, Alice can\nkeep her encryption key private to encrypt the messages she sends. If Bob\ncan successfully decrypt a message using Alice\u2019s public key (and the plaintext\nin the message has enough information to make it meaningful to Bob), he\nknows that message must have come from Alice, because the decryption key\nis uniquely tied to the encryption key.\nOne \ufb01nal application of cryptography in distributed systems is the use of\nhash functions . A hash function Htakes a message mof arbitrary length as\ninput and produces a bit string hhaving a \ufb01xed length as output:\nh=H(m)\nA hash his somewhat comparable to the extra bits that are appended to\na message in communication systems to allow for error detection, such a\ncyclic-redundancy check (CRC).\nHash functions that are used in cryptographic systems have a number\nof essential properties. First, they are one-way functions , meaning that it is\ncomputationally infeasible to \ufb01nd the input mthat corresponds to a known\noutput h. On the other hand, computing hfrom mis easy. Second, they have\ntheweak collision resistance property, meaning that given an input mand its\nassociated output h=H(m), it is computationally infeasible to \ufb01nd another,\ndifferent input m06=m, such that H(m) =H(m0). Finally, cryptographic hash\nfunctions also have the strong collision resistance property, which means\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "512 CHAPTER 9. SECURITY\nthat, when given only H, it is computationally infeasible to \ufb01nd any two\ndifferent input values mand m0, such that H(m) =H(m0).\nSimilar properties must apply to any encryption function Eand the keys\nthat are used. Furthermore, for any encryption function EK, it should be\ncomputationally infeasible to \ufb01nd the key Kwhen given the plaintext Pand\nassociated ciphertext C=EK(P). Likewise, analogous to collision resistance,\nwhen given a plaintext Pand a key K, it should be effectively impossible to\n\ufb01nd another key K0such that EK(P) =EK0(P).\nThe art and science of devising algorithms for cryptographic systems has\na long and fascinating history [Kahn, 1967], and building secure systems is\noften surprisingly dif\ufb01cult, or even impossible [Schneier, 2000]. It is beyond\nthe scope of this book to discuss any of these algorithms in detail. Information\non cryptographic algorithms can be found in [Ferguson et al., 2010], [Menezes\net al., 1996], and [Schneier, 1996]. Figure 9.5 summarizes the notation and\nabbreviations we use in the mathematical expressions to throughout this book.\nNotation Description\nKA,B Secret key shared by AandB\nK+\nAPublic key of A\nK\u0000\nAPrivate key of A\nK(d) Some data dencrypted with key K\nFigure 9.5: Notation used in this chapter.\n9.2 Secure channels\nThe client-server model is a convenient way to organize a distributed system.\nIn this model, servers may possibly be distributed and replicated, but also\nact as clients with respect to other servers. When considering security in\ndistributed systems, it is once again useful to think in terms of clients and\nservers. In particular, making a distributed system secure essentially boils\ndown to two predominant issues. The \ufb01rst issue is how to make the commu-\nnication between clients and servers secure. Secure communication requires\nauthentication of the communicating parties. In many cases it also requires\nensuring message integrity and possibly con\ufb01dentiality as well. As part of\nthis problem, we also need to consider protecting the communication within\na group of servers.\nThe second issue is that of authorization: once a server has accepted a\nrequest from a client, how can it \ufb01nd out whether that client is authorized\nto have that request carried out? Authorization is related to the problem of\ncontrolling access to resources.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n512 CHAPTER 9. SECURITY\nthat, when given only H, it is computationally infeasible to \ufb01nd any two\ndifferent input values mand m0, such that H(m) =H(m0).\nSimilar properties must apply to any encryption function Eand the keys\nthat are used. Furthermore, for any encryption function EK, it should be\ncomputationally infeasible to \ufb01nd the key Kwhen given the plaintext Pand\nassociated ciphertext C=EK(P). Likewise, analogous to collision resistance,\nwhen given a plaintext Pand a key K, it should be effectively impossible to\n\ufb01nd another key K0such that EK(P) =EK0(P).\nThe art and science of devising algorithms for cryptographic systems has\na long and fascinating history [Kahn, 1967], and building secure systems is\noften surprisingly dif\ufb01cult, or even impossible [Schneier, 2000]. It is beyond\nthe scope of this book to discuss any of these algorithms in detail. Information\non cryptographic algorithms can be found in [Ferguson et al., 2010], [Menezes\net al., 1996], and [Schneier, 1996]. Figure 9.5 summarizes the notation and\nabbreviations we use in the mathematical expressions to throughout this book.\nNotation Description\nKA,B Secret key shared by AandB\nK+\nAPublic key of A\nK\u0000\nAPrivate key of A\nK(d) Some data dencrypted with key K\nFigure 9.5: Notation used in this chapter.\n9.2 Secure channels\nThe client-server model is a convenient way to organize a distributed system.\nIn this model, servers may possibly be distributed and replicated, but also\nact as clients with respect to other servers. When considering security in\ndistributed systems, it is once again useful to think in terms of clients and\nservers. In particular, making a distributed system secure essentially boils\ndown to two predominant issues. The \ufb01rst issue is how to make the commu-\nnication between clients and servers secure. Secure communication requires\nauthentication of the communicating parties. In many cases it also requires\nensuring message integrity and possibly con\ufb01dentiality as well. As part of\nthis problem, we also need to consider protecting the communication within\na group of servers.\nThe second issue is that of authorization: once a server has accepted a\nrequest from a client, how can it \ufb01nd out whether that client is authorized\nto have that request carried out? Authorization is related to the problem of\ncontrolling access to resources.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 513\nThe issue of protecting communication between clients and servers, can be\nthought of in terms of setting up a secure channel between communicating\nparties [Voydock and Kent, 1983]. A secure channel protects senders and\nreceivers against interception, modi\ufb01cation, and fabrication of messages. It\ndoes not also necessarily protect against interruption. Protecting messages\nagainst interception is done by ensuring con\ufb01dentiality: the secure channel\nensures that its messages cannot be eavesdropped by intruders. Protecting\nagainst modi\ufb01cation and fabrication by intruders is done through protocols\nfor mutual authentication and message integrity. A detailed and formal\ndescription of the logic underlying authentication can be found in [Lampson\net al., 1992].\nAuthentication\nBefore going into the details of various authentication protocols, it is worth-\nwhile noting that authentication and message integrity cannot do without\neach other. Consider, for example, a distributed system that supports authen-\ntication of two communicating parties, but does not provide mechanisms to\nensure message integrity. In such a system, Bob may know for sure that Alice\nis the sender of a message m. However, if Bob cannot be given guarantees that\nmhas not been modi\ufb01ed during transmission, what use is it to him to know\nthat Alice sent (the original version of) m?\nLikewise, suppose that only message integrity is supported, but no mecha-\nnisms exist for authentication. When Bob receives a message stating that he\nhas just won $1,000,000 in the lottery, how happy can he be if he cannot verify\nthat the message was sent by the organizers of that lottery?\nConsequently, authentication and message integrity should go together. In\nmany protocols, the combination works roughly as follows. Again, assume\nthat Alice and Bob want to communicate, and that Alice takes the initiative in\nsetting up a channel. Alice starts by sending a message to Bob, or otherwise to\na trusted third party who will help set up the channel. Once the channel has\nbeen set up, Alice knows for sure that she is talking to Bob, and Bob knows\nfor sure he is talking to Alice, they can exchange messages.\nTo subsequently ensure integrity of the data messages that are exchanged\nafter authentication has taken place, it is common practice to use secret-key\ncryptography by means of session keys. A session key is a shared (secret) key\nthat is used to encrypt messages for integrity and possibly also con\ufb01dentiality.\nSuch a key is generally used only for as long as the channel exists. When the\nchannel is closed, its associated session key is destroyed.\nAuthentication based on a shared secret key\nLet us start by taking a look at an authentication protocol based on a secret\nkey that is already shared between Alice and Bob. How the two actually\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 513\nThe issue of protecting communication between clients and servers, can be\nthought of in terms of setting up a secure channel between communicating\nparties [Voydock and Kent, 1983]. A secure channel protects senders and\nreceivers against interception, modi\ufb01cation, and fabrication of messages. It\ndoes not also necessarily protect against interruption. Protecting messages\nagainst interception is done by ensuring con\ufb01dentiality: the secure channel\nensures that its messages cannot be eavesdropped by intruders. Protecting\nagainst modi\ufb01cation and fabrication by intruders is done through protocols\nfor mutual authentication and message integrity. A detailed and formal\ndescription of the logic underlying authentication can be found in [Lampson\net al., 1992].\nAuthentication\nBefore going into the details of various authentication protocols, it is worth-\nwhile noting that authentication and message integrity cannot do without\neach other. Consider, for example, a distributed system that supports authen-\ntication of two communicating parties, but does not provide mechanisms to\nensure message integrity. In such a system, Bob may know for sure that Alice\nis the sender of a message m. However, if Bob cannot be given guarantees that\nmhas not been modi\ufb01ed during transmission, what use is it to him to know\nthat Alice sent (the original version of) m?\nLikewise, suppose that only message integrity is supported, but no mecha-\nnisms exist for authentication. When Bob receives a message stating that he\nhas just won $1,000,000 in the lottery, how happy can he be if he cannot verify\nthat the message was sent by the organizers of that lottery?\nConsequently, authentication and message integrity should go together. In\nmany protocols, the combination works roughly as follows. Again, assume\nthat Alice and Bob want to communicate, and that Alice takes the initiative in\nsetting up a channel. Alice starts by sending a message to Bob, or otherwise to\na trusted third party who will help set up the channel. Once the channel has\nbeen set up, Alice knows for sure that she is talking to Bob, and Bob knows\nfor sure he is talking to Alice, they can exchange messages.\nTo subsequently ensure integrity of the data messages that are exchanged\nafter authentication has taken place, it is common practice to use secret-key\ncryptography by means of session keys. A session key is a shared (secret) key\nthat is used to encrypt messages for integrity and possibly also con\ufb01dentiality.\nSuch a key is generally used only for as long as the channel exists. When the\nchannel is closed, its associated session key is destroyed.\nAuthentication based on a shared secret key\nLet us start by taking a look at an authentication protocol based on a secret\nkey that is already shared between Alice and Bob. How the two actually\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "514 CHAPTER 9. SECURITY\nmanaged to obtain a shared key in a secure way is not important for now. In\nthe description of the protocol, Alice and Bob are abbreviated by Aand B,\nrespectively, and their shared key is denoted as KA,B. The protocol takes a\ncommon approach whereby one party challenges the other to a response that\ncan be correct only if the other knows the shared secret key. Such solutions\nare also known as challenge-response protocols .\nIn the case of authentication based on a shared secret key, the protocol\nproceeds as shown in Figure 9.6. First, Alice sends her identity to Bob (message\n1), indicating that she wants to set up a communication channel between the\ntwo. Bob subsequently sends a challenge RBto Alice, shown as message 2.\nSuch a challenge could take the form of a random number. Alice is required to\nencrypt the challenge with the secret key KA,Bthat she shares with Bob, and\nreturn the encrypted challenge to Bob. This response is shown as message 3\nin Figure 9.6 containing KA,B(RB).\nFigure 9.6: Authentication based on a shared secret key.\nWhen Bob receives the response KA,B(RB)to his challenge RB, he can\ndecrypt the message using the shared key again to see if it contains RB. If\nso, he then knows that Alice is on the other side, for who else could have\nencrypted RBwith KA,Bin the \ufb01rst place? In other words, Bob has now\nveri\ufb01ed that he is indeed talking to Alice. However, note that Alice has not\nyet veri\ufb01ed that it is indeed Bob on the other side of the channel. Therefore,\nshe sends a challenge RA(message 4), which Bob responds to by returning\nKA,B(RA), shown as message 5. When Alice decrypts it with KA,Band sees\nherRA, she knows she is talking to Bob.\nNote 9.2 (Advanced: On the design of security protocols)\nOne of the dif\ufb01cult issues in security is designing protocols that actually work.\nTo illustrate how easily things can go wrong, consider an \u201coptimization\u201d of the\nauthentication protocol in which the number of messages has been reduced from\n\ufb01ve to three, as shown in Figure 9.7. The basic idea is that if Alice eventually\nwants to challenge Bob anyway, she might as well send a challenge along with\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n514 CHAPTER 9. SECURITY\nmanaged to obtain a shared key in a secure way is not important for now. In\nthe description of the protocol, Alice and Bob are abbreviated by Aand B,\nrespectively, and their shared key is denoted as KA,B. The protocol takes a\ncommon approach whereby one party challenges the other to a response that\ncan be correct only if the other knows the shared secret key. Such solutions\nare also known as challenge-response protocols .\nIn the case of authentication based on a shared secret key, the protocol\nproceeds as shown in Figure 9.6. First, Alice sends her identity to Bob (message\n1), indicating that she wants to set up a communication channel between the\ntwo. Bob subsequently sends a challenge RBto Alice, shown as message 2.\nSuch a challenge could take the form of a random number. Alice is required to\nencrypt the challenge with the secret key KA,Bthat she shares with Bob, and\nreturn the encrypted challenge to Bob. This response is shown as message 3\nin Figure 9.6 containing KA,B(RB).\nFigure 9.6: Authentication based on a shared secret key.\nWhen Bob receives the response KA,B(RB)to his challenge RB, he can\ndecrypt the message using the shared key again to see if it contains RB. If\nso, he then knows that Alice is on the other side, for who else could have\nencrypted RBwith KA,Bin the \ufb01rst place? In other words, Bob has now\nveri\ufb01ed that he is indeed talking to Alice. However, note that Alice has not\nyet veri\ufb01ed that it is indeed Bob on the other side of the channel. Therefore,\nshe sends a challenge RA(message 4), which Bob responds to by returning\nKA,B(RA), shown as message 5. When Alice decrypts it with KA,Band sees\nherRA, she knows she is talking to Bob.\nNote 9.2 (Advanced: On the design of security protocols)\nOne of the dif\ufb01cult issues in security is designing protocols that actually work.\nTo illustrate how easily things can go wrong, consider an \u201coptimization\u201d of the\nauthentication protocol in which the number of messages has been reduced from\n\ufb01ve to three, as shown in Figure 9.7. The basic idea is that if Alice eventually\nwants to challenge Bob anyway, she might as well send a challenge along with\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 515\nher identity when setting up the channel. Likewise, Bob returns his response to\nthat challenge, along with his own challenge in a single message.\nUnfortunately, this protocol no longer works. It can easily be defeated by what\nis known as a re\ufb02ection attack . To explain how such an attack works, consider an\nintruder called Chuck, whom we denote as Cin our protocols. Chuck\u2019s goal is to\nset up a channel with Bob so that Bob believes he is talking to Alice. Chuck can\nestablish this if he responds correctly to a challenge sent by Bob, for instance, by\nreturning the encrypted version of a number that Bob sent. Without knowledge\nofKA,B, only Bob can do such an encryption, and this is precisely what Chuck\ntricks Bob into doing.\nFigure 9.7: Authentication based on a shared secret key, but using three\ninstead of \ufb01ve messages.\nThe attack is illustrated in Figure 9.8 Chuck starts out by sending a message\ncontaining Alice\u2019s identity A, along with a challenge RC. Bob returns his challenge\nRBand the response KA,B(RC)in a single message. At that point, Chuck would\nneed to prove he knows the secret key by returning KA,B(RB)to Bob. Unfortu-\nnately, he does not have KA,B. Instead, what he does is attempt to set up a second\nchannel to let Bob do the encryption for him.\nFigure 9.8: The re\ufb02ection attack.\nTherefore, Chuck sends Aand RBin a single message as before, but now\npretends that he wants a second channel. This is shown as message 3 in Figure 9.8\nBob, not recognizing that he, himself, had used RBbefore as a challenge, responds\nwith KA,B(RB)and another challenge RB2, shown as message 4. At that point,\nChuck has KA,B(RB)and \ufb01nishes setting up the \ufb01rst session by returning message\n5 containing the response KA,B(RB), which was originally requested from the\nchallenge sent in message 2.\nAs explained by Kaufman et al. [2003], one of the mistakes made during the\nadaptation of the original protocol was that the two parties in the new version of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 515\nher identity when setting up the channel. Likewise, Bob returns his response to\nthat challenge, along with his own challenge in a single message.\nUnfortunately, this protocol no longer works. It can easily be defeated by what\nis known as a re\ufb02ection attack . To explain how such an attack works, consider an\nintruder called Chuck, whom we denote as Cin our protocols. Chuck\u2019s goal is to\nset up a channel with Bob so that Bob believes he is talking to Alice. Chuck can\nestablish this if he responds correctly to a challenge sent by Bob, for instance, by\nreturning the encrypted version of a number that Bob sent. Without knowledge\nofKA,B, only Bob can do such an encryption, and this is precisely what Chuck\ntricks Bob into doing.\nFigure 9.7: Authentication based on a shared secret key, but using three\ninstead of \ufb01ve messages.\nThe attack is illustrated in Figure 9.8 Chuck starts out by sending a message\ncontaining Alice\u2019s identity A, along with a challenge RC. Bob returns his challenge\nRBand the response KA,B(RC)in a single message. At that point, Chuck would\nneed to prove he knows the secret key by returning KA,B(RB)to Bob. Unfortu-\nnately, he does not have KA,B. Instead, what he does is attempt to set up a second\nchannel to let Bob do the encryption for him.\nFigure 9.8: The re\ufb02ection attack.\nTherefore, Chuck sends Aand RBin a single message as before, but now\npretends that he wants a second channel. This is shown as message 3 in Figure 9.8\nBob, not recognizing that he, himself, had used RBbefore as a challenge, responds\nwith KA,B(RB)and another challenge RB2, shown as message 4. At that point,\nChuck has KA,B(RB)and \ufb01nishes setting up the \ufb01rst session by returning message\n5 containing the response KA,B(RB), which was originally requested from the\nchallenge sent in message 2.\nAs explained by Kaufman et al. [2003], one of the mistakes made during the\nadaptation of the original protocol was that the two parties in the new version of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "516 CHAPTER 9. SECURITY\nthe protocol were using the same challenge in two different runs of the protocol.\nA better design is to always use different challenges for the initiator and for the\nresponder. For example, if Alice always uses an odd number and Bob an even\nnumber, Bob would have recognized that something \ufb01shy was going on when\nreceiving RBin message 3 in Figure 9.8. (Unfortunately, this solution is subject to\nother attacks, notably the one known as the \u201cman-in-the-middle-attack,\u201d which is\nexplained in [Ferguson et al., 2010].) In general, letting the two parties setting up\na secure channel do a number of things identically is not a good idea.\nAnother principle that is violated in the adapted protocol is that Bob gave\naway valuable information in the form of the response KA,B(RC)without knowing\nfor sure to whom he was giving it. This principle was not violated in the original\nprotocol, in which Alice \ufb01rst needed to prove her identity, after which Bob was\nwilling to pass her encrypted information.\nThere are many principles that developers of cryptographic protocols have\ngradually come to learn over the years. One important lesson is that designing\nsecurity protocols that do what they are supposed to do is often much harder\nthan it looks. Also, tweaking an existing protocol to improve its performance,\ncan easily affect its correctness. More on design principles for protocols can\nbe found in [Abadi and Needham, 1996].\nAuthentication using a key distribution center\nOne of the problems with using a shared secret key for authentication is\nscalability. If a distributed system contains Nhosts, and each host is required\nto share a secret key with each of the other N\u00001hosts, the system as a whole\nneeds to manage N(N\u00001)/2keys, and each host has to manage N\u00001keys.\nFor large N, this will lead to problems. An alternative is to use a centralized\napproach by means of a Key Distribution Center (KDC ). This KDC shares\na secret key with each of the hosts, but no pair of hosts is required to have\na shared secret key as well. In other words, using a KDC requires that we\nmanage Nkeys instead of N(N\u00001)/2, which is clearly an improvement.\nIf Alice wants to set up a secure channel with Bob, she can do so with the\nhelp of a (trusted) KDC. The whole idea is that the KDC hands out a key to\nboth Alice and Bob that they can use for communication, shown in Figure 9.9.\nAlice \ufb01rst sends a message to the KDC, telling it that she wants to talk to\nBob. The KDC returns a message containing a shared secret key KA,Bthat\nshe can use. The message is encrypted with the secret key KA,KDC that Alice\nshares with the KDC. In addition, the KDC sends KA,Balso to Bob, but now\nencrypted with the secret key KB,KDC it shares with Bob.\nThe main drawback of this approach is that Alice may want to start setting\nup a secure channel with Bob even before Bob had received the shared key\nfrom the KDC. In addition, the KDC is required to get Bob into the loop by\npassing him the key. These problems can be circumvented if the KDC just\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n516 CHAPTER 9. SECURITY\nthe protocol were using the same challenge in two different runs of the protocol.\nA better design is to always use different challenges for the initiator and for the\nresponder. For example, if Alice always uses an odd number and Bob an even\nnumber, Bob would have recognized that something \ufb01shy was going on when\nreceiving RBin message 3 in Figure 9.8. (Unfortunately, this solution is subject to\nother attacks, notably the one known as the \u201cman-in-the-middle-attack,\u201d which is\nexplained in [Ferguson et al., 2010].) In general, letting the two parties setting up\na secure channel do a number of things identically is not a good idea.\nAnother principle that is violated in the adapted protocol is that Bob gave\naway valuable information in the form of the response KA,B(RC)without knowing\nfor sure to whom he was giving it. This principle was not violated in the original\nprotocol, in which Alice \ufb01rst needed to prove her identity, after which Bob was\nwilling to pass her encrypted information.\nThere are many principles that developers of cryptographic protocols have\ngradually come to learn over the years. One important lesson is that designing\nsecurity protocols that do what they are supposed to do is often much harder\nthan it looks. Also, tweaking an existing protocol to improve its performance,\ncan easily affect its correctness. More on design principles for protocols can\nbe found in [Abadi and Needham, 1996].\nAuthentication using a key distribution center\nOne of the problems with using a shared secret key for authentication is\nscalability. If a distributed system contains Nhosts, and each host is required\nto share a secret key with each of the other N\u00001hosts, the system as a whole\nneeds to manage N(N\u00001)/2keys, and each host has to manage N\u00001keys.\nFor large N, this will lead to problems. An alternative is to use a centralized\napproach by means of a Key Distribution Center (KDC ). This KDC shares\na secret key with each of the hosts, but no pair of hosts is required to have\na shared secret key as well. In other words, using a KDC requires that we\nmanage Nkeys instead of N(N\u00001)/2, which is clearly an improvement.\nIf Alice wants to set up a secure channel with Bob, she can do so with the\nhelp of a (trusted) KDC. The whole idea is that the KDC hands out a key to\nboth Alice and Bob that they can use for communication, shown in Figure 9.9.\nAlice \ufb01rst sends a message to the KDC, telling it that she wants to talk to\nBob. The KDC returns a message containing a shared secret key KA,Bthat\nshe can use. The message is encrypted with the secret key KA,KDC that Alice\nshares with the KDC. In addition, the KDC sends KA,Balso to Bob, but now\nencrypted with the secret key KB,KDC it shares with Bob.\nThe main drawback of this approach is that Alice may want to start setting\nup a secure channel with Bob even before Bob had received the shared key\nfrom the KDC. In addition, the KDC is required to get Bob into the loop by\npassing him the key. These problems can be circumvented if the KDC just\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 517\nFigure 9.9: The principle of using a Key Distribution Center (KDC).\npasses KB,KDC(KA,B)back to Alice, and lets her take care of connecting to Bob.\nThis leads to the protocol shown in Figure 9.10. The message KB,KDC(KA,B)\nis also known as a ticket . It is Alice\u2019s job to pass this ticket to Bob. Note\nthat Bob is still the only one who can make sensible use of the ticket, as he is\nthe only one besides the KDC who knows how to decrypt the information it\ncontains.\nFigure 9.10: Using a ticket and letting Alice set up a connection to Bob.\nNote 9.3 (Advanced: The Needham-Schroeder protocol)\nThe protocol shown in Figure 9.10 is actually a variant of a well-known example\nof an authentication protocol using a KDC, known as the Needham-Schroeder\nauthentication protocol , named after its inventors [Needham and Schroeder,\n1978]. The Needham-Schroeder protocol, shown in Figure 9.11 is a so-called\nmultiway challenge-response protocol and works as follows.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 517\nFigure 9.9: The principle of using a Key Distribution Center (KDC).\npasses KB,KDC(KA,B)back to Alice, and lets her take care of connecting to Bob.\nThis leads to the protocol shown in Figure 9.10. The message KB,KDC(KA,B)\nis also known as a ticket . It is Alice\u2019s job to pass this ticket to Bob. Note\nthat Bob is still the only one who can make sensible use of the ticket, as he is\nthe only one besides the KDC who knows how to decrypt the information it\ncontains.\nFigure 9.10: Using a ticket and letting Alice set up a connection to Bob.\nNote 9.3 (Advanced: The Needham-Schroeder protocol)\nThe protocol shown in Figure 9.10 is actually a variant of a well-known example\nof an authentication protocol using a KDC, known as the Needham-Schroeder\nauthentication protocol , named after its inventors [Needham and Schroeder,\n1978]. The Needham-Schroeder protocol, shown in Figure 9.11 is a so-called\nmultiway challenge-response protocol and works as follows.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "518 CHAPTER 9. SECURITY\nFigure 9.11: The Needham-Schroeder authentication protocol.\nWhen Alice wants to set up a secure channel with Bob, she sends a request to\nthe KDC containing a challenge RA, along with her identity Aand, of course, that\nof Bob. The KDC responds by giving her the ticket KB,KDC(KA,B), along with the\nsecret key KA,Bthat she can subsequently share with Bob.\nThe challenge RA1that Alice sends to the KDC along with her request to set\nup a channel to Bob is also known as a nonce. A nonce is a random number that\nis used only once, such as one chosen from a very large set. The main purpose of\na nonce is to uniquely relate two messages to each other, in this case message 1\nand message 2. In particular, by including RA1again in message 2, Alice will\nknow for sure that message 2 is sent as a response to message 1, and that it is not,\nfor example, a replay of an older message.\nTo understand the problem at hand, assume that we did not use nonces, and\nthat Chuck has stolen one of Bob\u2019s old keys, say Kold\nB,KDC. In addition, Chuck\nhas intercepted an old response KA,KDC(B,KA,B,Kold\nB,KDC(A,KA,B))that the KDC\nhad returned to a previous request from Alice to talk to Bob. Meanwhile, Bob\nwill have negotiated a new shared secret key with the KDC. However, Chuck\npatiently waits until Alice again requests to set up a secure channel with Bob. At\nthat point, he replays the old response, and fools Alice into making her believe\nshe is talking to Bob, because he can decrypt the ticket and prove he knows the\nshared secret key KA,B. Clearly this is unacceptable and must be defended against.\nBy including a nonce, such an attack is impossible, because replaying an older\nmessage (having a different nonce) will immediately be discovered.\nMessage 2 also contains B, the identity of Bob. By including B, the KDC pro-\ntects Alice against the following attack. Suppose that Bwas left out of message 2.\nIn that case, Chuck could modify message 1 by replacing the identity of Bob with\nhis own identity, say C. The KDC would then think Alice wants to set up a secure\nchannel to Chuck, and responds accordingly. As soon as Alice wants to contact\nBob, Chuck intercepts the message and fools Alice into believing she is talking\nto Bob. By copying the identity of the other party from message 1 to message 2,\nAlice will immediately detect that her request had been modi\ufb01ed.\nAfter the KDC has passed the ticket to Alice, the secure channel between Alice\nand Bob can be set up. Alice starts with sending message 3, which contains the\nticket to Bob, and a challenge RA2encrypted with the shared key KA,Bthat the\nKDC had just generated. Bob then decrypts the ticket to \ufb01nd the shared key, and\nreturns a response RA2\u00001 along with a challenge RBfor Alice.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n518 CHAPTER 9. SECURITY\nFigure 9.11: The Needham-Schroeder authentication protocol.\nWhen Alice wants to set up a secure channel with Bob, she sends a request to\nthe KDC containing a challenge RA, along with her identity Aand, of course, that\nof Bob. The KDC responds by giving her the ticket KB,KDC(KA,B), along with the\nsecret key KA,Bthat she can subsequently share with Bob.\nThe challenge RA1that Alice sends to the KDC along with her request to set\nup a channel to Bob is also known as a nonce. A nonce is a random number that\nis used only once, such as one chosen from a very large set. The main purpose of\na nonce is to uniquely relate two messages to each other, in this case message 1\nand message 2. In particular, by including RA1again in message 2, Alice will\nknow for sure that message 2 is sent as a response to message 1, and that it is not,\nfor example, a replay of an older message.\nTo understand the problem at hand, assume that we did not use nonces, and\nthat Chuck has stolen one of Bob\u2019s old keys, say Kold\nB,KDC. In addition, Chuck\nhas intercepted an old response KA,KDC(B,KA,B,Kold\nB,KDC(A,KA,B))that the KDC\nhad returned to a previous request from Alice to talk to Bob. Meanwhile, Bob\nwill have negotiated a new shared secret key with the KDC. However, Chuck\npatiently waits until Alice again requests to set up a secure channel with Bob. At\nthat point, he replays the old response, and fools Alice into making her believe\nshe is talking to Bob, because he can decrypt the ticket and prove he knows the\nshared secret key KA,B. Clearly this is unacceptable and must be defended against.\nBy including a nonce, such an attack is impossible, because replaying an older\nmessage (having a different nonce) will immediately be discovered.\nMessage 2 also contains B, the identity of Bob. By including B, the KDC pro-\ntects Alice against the following attack. Suppose that Bwas left out of message 2.\nIn that case, Chuck could modify message 1 by replacing the identity of Bob with\nhis own identity, say C. The KDC would then think Alice wants to set up a secure\nchannel to Chuck, and responds accordingly. As soon as Alice wants to contact\nBob, Chuck intercepts the message and fools Alice into believing she is talking\nto Bob. By copying the identity of the other party from message 1 to message 2,\nAlice will immediately detect that her request had been modi\ufb01ed.\nAfter the KDC has passed the ticket to Alice, the secure channel between Alice\nand Bob can be set up. Alice starts with sending message 3, which contains the\nticket to Bob, and a challenge RA2encrypted with the shared key KA,Bthat the\nKDC had just generated. Bob then decrypts the ticket to \ufb01nd the shared key, and\nreturns a response RA2\u00001 along with a challenge RBfor Alice.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 519\nThe following remark regarding message 4 is in order. In general, by returning\nRA2\u00001and not just RA2, Bob not only proves he knows the shared secret key, but\nalso that he has actually decrypted the challenge. Again, this ties message 4 to\nmessage 3 in the same way that the nonce RAtied message 2 to message 1. The\nprotocol is thus more protected against replays. However, in this special case, it\nwould have been suf\ufb01cient to just return KA,B(RA2,RB), for the simple reason that\nthis message has not yet been used anywhere in the protocol before. KA,B(RA2,RB)\nalready proves that Bob has been capable of decrypting the challenge sent in\nmessage 3. Message 4 as shown in Figure 9.11 is due to historical reasons.\nThe Needham-Schroeder protocol as presented here still has the weak point\nthat if Chuck ever got a hold of an old key Kold\nA,B, he could replay message 3 and\nget Bob to set up a channel. Bob will then believe he is talking to Alice, while,\nin fact, Chuck is at the other end. In this case, we need to relate message 3 to\nmessage 1, that is, make the key dependent on the initial request from Alice to set\nup a channel with Bob. The solution is shown in Figure 9.12.\nFigure 9.12: Protection against malicious reuse of a previously generated\nsession key in the Needham-Schroeder protocol.\nThe trick is to incorporate a nonce in the request sent by Alice to the KDC.\nHowever, the nonce has to come from Bob: this assures Bob that whoever wants\nto set up a secure channel with him, will have gotten the appropriate information\nfrom the KDC. Therefore, Alice \ufb01rst requests Bob to send her a nonce RB1,\nencrypted with the key shared between Bob and the KDC. Alice incorporates this\nnonce in her request to the KDC, which will then subsequently decrypt it and\nput the result in the generated ticket. In this way, Bob will know for sure that the\nsession key is tied to the original request from Alice to talk to Bob.\nAuthentication using public-key cryptography\nLet us now look at authentication with a public-key cryptosystem that does\nnot require a KDC. Again, consider the situation that Alice wants to set\nup a secure channel to Bob, and that both are in the possession of each\nother\u2019s public key. A typical authentication protocol based on public-key\ncryptography is shown in Figure 9.13 which we explain next.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 519\nThe following remark regarding message 4 is in order. In general, by returning\nRA2\u00001and not just RA2, Bob not only proves he knows the shared secret key, but\nalso that he has actually decrypted the challenge. Again, this ties message 4 to\nmessage 3 in the same way that the nonce RAtied message 2 to message 1. The\nprotocol is thus more protected against replays. However, in this special case, it\nwould have been suf\ufb01cient to just return KA,B(RA2,RB), for the simple reason that\nthis message has not yet been used anywhere in the protocol before. KA,B(RA2,RB)\nalready proves that Bob has been capable of decrypting the challenge sent in\nmessage 3. Message 4 as shown in Figure 9.11 is due to historical reasons.\nThe Needham-Schroeder protocol as presented here still has the weak point\nthat if Chuck ever got a hold of an old key Kold\nA,B, he could replay message 3 and\nget Bob to set up a channel. Bob will then believe he is talking to Alice, while,\nin fact, Chuck is at the other end. In this case, we need to relate message 3 to\nmessage 1, that is, make the key dependent on the initial request from Alice to set\nup a channel with Bob. The solution is shown in Figure 9.12.\nFigure 9.12: Protection against malicious reuse of a previously generated\nsession key in the Needham-Schroeder protocol.\nThe trick is to incorporate a nonce in the request sent by Alice to the KDC.\nHowever, the nonce has to come from Bob: this assures Bob that whoever wants\nto set up a secure channel with him, will have gotten the appropriate information\nfrom the KDC. Therefore, Alice \ufb01rst requests Bob to send her a nonce RB1,\nencrypted with the key shared between Bob and the KDC. Alice incorporates this\nnonce in her request to the KDC, which will then subsequently decrypt it and\nput the result in the generated ticket. In this way, Bob will know for sure that the\nsession key is tied to the original request from Alice to talk to Bob.\nAuthentication using public-key cryptography\nLet us now look at authentication with a public-key cryptosystem that does\nnot require a KDC. Again, consider the situation that Alice wants to set\nup a secure channel to Bob, and that both are in the possession of each\nother\u2019s public key. A typical authentication protocol based on public-key\ncryptography is shown in Figure 9.13 which we explain next.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "520 CHAPTER 9. SECURITY\nFigure 9.13: Mutual authentication in a public-key cryptosystem.\nAlice starts with sending a challenge RAto Bob encrypted with his public\nkey K+\nB. It is Bob\u2019s job to decrypt the message and return the challenge to\nAlice. Because Bob is the only person that can decrypt the message (using the\nprivate key that is associated with the public key Alice used), Alice will know\nthat she is talking to Bob. Note that it is important that Alice is guaranteed to\nbe using Bob\u2019s public key, and not the public key of someone impersonating\nBob. How such guarantees can be given is discussed later in this chapter.\nWhen Bob receives Alice\u2019s request to set up a channel, he returns the\ndecrypted challenge, along with his own challenge RBto authenticate Alice.\nIn addition, he generates a session key KA,Bthat can be used for further\ncommunication. Bob\u2019s response to Alice\u2019s challenge, his own challenge, and\nthe session key are put into a message encrypted with the public key K+\nA\nbelonging to Alice, shown as message 2 in Figure 9.13. Only Alice will be\ncapable of decrypting this message using her private key K\u0000\nA.\nAlice, \ufb01nally, returns her response to Bob\u2019s challenge using the session\nkey KA,Bgenerated by Bob. In that way, she will have proven that she could\ndecrypt message 2, and thus that she is actually Alice to whom Bob is talking.\nMessage integrity and con\ufb01dentiality\nBesides authentication, a secure channel should also provide guarantees for\nmessage integrity and con\ufb01dentiality. Message integrity means that messages\nare protected against surreptitious modi\ufb01cation; con\ufb01dentiality ensures that\nmessages cannot be intercepted and read by eavesdroppers. Con\ufb01dentiality\nis easily established by simply encrypting a message before sending it. En-\ncryption can take place either through a secret key shared with the receiver\nor alternatively by using the receiver\u2019s public key. However, protecting a\nmessage against modi\ufb01cations is somewhat more complicated, as we discuss\nnext.\nDigital signatures\nMessage integrity often goes beyond the actual transfer through a secure\nchannel. Consider the situation in which Bob has just sold Alice a collector\u2019s\nitem of some vinyl record for $500. The whole deal was done through e-mail.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n520 CHAPTER 9. SECURITY\nFigure 9.13: Mutual authentication in a public-key cryptosystem.\nAlice starts with sending a challenge RAto Bob encrypted with his public\nkey K+\nB. It is Bob\u2019s job to decrypt the message and return the challenge to\nAlice. Because Bob is the only person that can decrypt the message (using the\nprivate key that is associated with the public key Alice used), Alice will know\nthat she is talking to Bob. Note that it is important that Alice is guaranteed to\nbe using Bob\u2019s public key, and not the public key of someone impersonating\nBob. How such guarantees can be given is discussed later in this chapter.\nWhen Bob receives Alice\u2019s request to set up a channel, he returns the\ndecrypted challenge, along with his own challenge RBto authenticate Alice.\nIn addition, he generates a session key KA,Bthat can be used for further\ncommunication. Bob\u2019s response to Alice\u2019s challenge, his own challenge, and\nthe session key are put into a message encrypted with the public key K+\nA\nbelonging to Alice, shown as message 2 in Figure 9.13. Only Alice will be\ncapable of decrypting this message using her private key K\u0000\nA.\nAlice, \ufb01nally, returns her response to Bob\u2019s challenge using the session\nkey KA,Bgenerated by Bob. In that way, she will have proven that she could\ndecrypt message 2, and thus that she is actually Alice to whom Bob is talking.\nMessage integrity and con\ufb01dentiality\nBesides authentication, a secure channel should also provide guarantees for\nmessage integrity and con\ufb01dentiality. Message integrity means that messages\nare protected against surreptitious modi\ufb01cation; con\ufb01dentiality ensures that\nmessages cannot be intercepted and read by eavesdroppers. Con\ufb01dentiality\nis easily established by simply encrypting a message before sending it. En-\ncryption can take place either through a secret key shared with the receiver\nor alternatively by using the receiver\u2019s public key. However, protecting a\nmessage against modi\ufb01cations is somewhat more complicated, as we discuss\nnext.\nDigital signatures\nMessage integrity often goes beyond the actual transfer through a secure\nchannel. Consider the situation in which Bob has just sold Alice a collector\u2019s\nitem of some vinyl record for $500. The whole deal was done through e-mail.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 521\nIn the end, Alice sends Bob a message con\ufb01rming that she will buy the record\nfor $500. In addition to authentication, there are at least two issues that need\nto be taken care of regarding the integrity of the message.\n1.Alice needs to be assured that Bob will not maliciously change the\n$500 mentioned in her message into something higher, and claim she\npromised more than $500.\n2.Bob needs to be assured that Alice cannot deny ever having sent the\nmessage, for example, because she had second thoughts.\nThese two issues can be dealt with if Alice digitally signs the message in\nsuch a way that her signature is uniquely tied to its content. The unique\nassociation between a message and its signature prevents that modi\ufb01cations to\nthe message will go unnoticed. In addition, if Alice\u2019s signature can be veri\ufb01ed\nto be genuine, she cannot later repudiate the fact that she signed the message.\nThere are several ways to place digital signatures. One popular form is to\nuse a public-key cryptosystem, as shown in Figure 9.14. When Alice sends a\nmessage mto Bob, she encrypts it with her private key K\u0000\nA, and sends it off\nto Bob. If she also wants to keep the message content a secret, she can use\nBob\u2019s public key and send K+\nB(m,K\u0000\nA(m)), which combines mand the version\nsigned by Alice.\nFigure 9.14: Digital signing a message using public-key cryptography.\nWhen the message arrives at Bob, he can decrypt it using Alice\u2019s public\nkey. If he can be assured that the public key is indeed owned by Alice, then\ndecrypting the signed version of mand successfully comparing it to mcan\nmean only that it came from Alice. Alice is protected against any malicious\nmodi\ufb01cations to mby Bob, because Bob will always have to prove that the\nmodi\ufb01ed version of mwas also signed by Alice. In other words, the decrypted\nmessage alone essentially never counts as proof. It is also in Bob\u2019s own interest\nto keep the signed version of mto protect himself against repudiation by Alice.\nThere are a number of problems with this scheme, although the protocol\nin itself is correct. First, the validity of Alice\u2019s signature holds only as long as\nAlice\u2019s private key remains a secret. If Alice wants to bail out of the deal even\nafter sending Bob her con\ufb01rmation, she could claim that her private key was\nstolen before the message was sent.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 521\nIn the end, Alice sends Bob a message con\ufb01rming that she will buy the record\nfor $500. In addition to authentication, there are at least two issues that need\nto be taken care of regarding the integrity of the message.\n1.Alice needs to be assured that Bob will not maliciously change the\n$500 mentioned in her message into something higher, and claim she\npromised more than $500.\n2.Bob needs to be assured that Alice cannot deny ever having sent the\nmessage, for example, because she had second thoughts.\nThese two issues can be dealt with if Alice digitally signs the message in\nsuch a way that her signature is uniquely tied to its content. The unique\nassociation between a message and its signature prevents that modi\ufb01cations to\nthe message will go unnoticed. In addition, if Alice\u2019s signature can be veri\ufb01ed\nto be genuine, she cannot later repudiate the fact that she signed the message.\nThere are several ways to place digital signatures. One popular form is to\nuse a public-key cryptosystem, as shown in Figure 9.14. When Alice sends a\nmessage mto Bob, she encrypts it with her private key K\u0000\nA, and sends it off\nto Bob. If she also wants to keep the message content a secret, she can use\nBob\u2019s public key and send K+\nB(m,K\u0000\nA(m)), which combines mand the version\nsigned by Alice.\nFigure 9.14: Digital signing a message using public-key cryptography.\nWhen the message arrives at Bob, he can decrypt it using Alice\u2019s public\nkey. If he can be assured that the public key is indeed owned by Alice, then\ndecrypting the signed version of mand successfully comparing it to mcan\nmean only that it came from Alice. Alice is protected against any malicious\nmodi\ufb01cations to mby Bob, because Bob will always have to prove that the\nmodi\ufb01ed version of mwas also signed by Alice. In other words, the decrypted\nmessage alone essentially never counts as proof. It is also in Bob\u2019s own interest\nto keep the signed version of mto protect himself against repudiation by Alice.\nThere are a number of problems with this scheme, although the protocol\nin itself is correct. First, the validity of Alice\u2019s signature holds only as long as\nAlice\u2019s private key remains a secret. If Alice wants to bail out of the deal even\nafter sending Bob her con\ufb01rmation, she could claim that her private key was\nstolen before the message was sent.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "522 CHAPTER 9. SECURITY\nAnother problem occurs when Alice decides to change her private key.\nDoing so may in itself be not such a bad idea, as changing keys from time to\ntime generally helps against intrusion. However, once Alice has changed her\nkey, her statement sent to Bob becomes worthless. What may be needed in\nsuch cases is a central authority that keeps track of when keys are changed, in\naddition to using timestamps when signing messages.\nNote 9.4 (More information: Signing a message digest)\nAnother problem with this scheme is that Alice encrypts the entire message\nwith her private key. Such an encryption may be costly in terms of processing\nrequirements (or even mathematically infeasible as we assume that the message\ninterpreted as a binary number is bounded by a prede\ufb01ned maximum), and is\nactually unnecessary. Recall that we need to uniquely associate a signature with a\nonly speci\ufb01c message. A cheaper and arguably more elegant scheme is to use a\nmessage digest .\nA message digest is a \ufb01xed-length bit string hthat has been computed from\nan arbitrary-length message mby means of a cryptographic hash function H. Ifm\nis changed to m0, its hash H(m0)will be different from h=H(m)so that it can\neasily be detected that a modi\ufb01cation has taken place.\nTo digitally sign a message, Alice can \ufb01rst compute a message digest and\nsubsequently encrypt the digest with her private key, as shown in Figure 9.15. The\nencrypted digest is sent along with the message to Bob. Note that the message\nitself is sent as plaintext: everyone is allowed to read it. If con\ufb01dentiality is\nrequired, then the message should also be encrypted with Bob\u2019s public key.\nFigure 9.15: Digitally signing a message using a message digest.\nWhen Bob receives the message and its encrypted digest, he need merely\ndecrypt the digest with Alice\u2019s public key, and separately calculate the message\ndigest. If the digest calculated from the received message and the decrypted\ndigest match, Bob knows the message has been signed by Alice.\nSession keys\nDuring the establishment of a secure channel, after the authentication phase\nhas completed, the communicating parties generally use a unique shared\nsession key for con\ufb01dentiality. The session key is safely discarded when\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n522 CHAPTER 9. SECURITY\nAnother problem occurs when Alice decides to change her private key.\nDoing so may in itself be not such a bad idea, as changing keys from time to\ntime generally helps against intrusion. However, once Alice has changed her\nkey, her statement sent to Bob becomes worthless. What may be needed in\nsuch cases is a central authority that keeps track of when keys are changed, in\naddition to using timestamps when signing messages.\nNote 9.4 (More information: Signing a message digest)\nAnother problem with this scheme is that Alice encrypts the entire message\nwith her private key. Such an encryption may be costly in terms of processing\nrequirements (or even mathematically infeasible as we assume that the message\ninterpreted as a binary number is bounded by a prede\ufb01ned maximum), and is\nactually unnecessary. Recall that we need to uniquely associate a signature with a\nonly speci\ufb01c message. A cheaper and arguably more elegant scheme is to use a\nmessage digest .\nA message digest is a \ufb01xed-length bit string hthat has been computed from\nan arbitrary-length message mby means of a cryptographic hash function H. Ifm\nis changed to m0, its hash H(m0)will be different from h=H(m)so that it can\neasily be detected that a modi\ufb01cation has taken place.\nTo digitally sign a message, Alice can \ufb01rst compute a message digest and\nsubsequently encrypt the digest with her private key, as shown in Figure 9.15. The\nencrypted digest is sent along with the message to Bob. Note that the message\nitself is sent as plaintext: everyone is allowed to read it. If con\ufb01dentiality is\nrequired, then the message should also be encrypted with Bob\u2019s public key.\nFigure 9.15: Digitally signing a message using a message digest.\nWhen Bob receives the message and its encrypted digest, he need merely\ndecrypt the digest with Alice\u2019s public key, and separately calculate the message\ndigest. If the digest calculated from the received message and the decrypted\ndigest match, Bob knows the message has been signed by Alice.\nSession keys\nDuring the establishment of a secure channel, after the authentication phase\nhas completed, the communicating parties generally use a unique shared\nsession key for con\ufb01dentiality. The session key is safely discarded when\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 523\nthe channel is no longer used. An alternative would have been to use the\nsame keys for con\ufb01dentiality as those that are used for setting up the secure\nchannel. However, there are a number of important bene\ufb01ts to using session\nkeys [Kaufman et al., 2003].\nFirst, when a key is used often, it becomes easier to reveal it. In a sense,\ncryptographic keys are subject to \u201cwear and tear\u201d just like ordinary keys.\nThe basic idea is that if an intruder can intercept a lot of data that have been\nencrypted using the same key, it becomes possible to mount attacks to \ufb01nd\ncertain characteristics of the keys used, and possibly reveal the plaintext or\nthe key itself. For this reason, it is much safer to use the authentication keys\nas little as possible. In addition, such keys are often exchanged using some\nrelatively time-expensive out-of-band mechanism such as regular mail or\ntelephone. Exchanging keys that way should be kept to a minimum.\nAnother important reason for generating a unique key for each secure\nchannel is to ensure protection against replay attacks as we have come across\npreviously a number of times. By using a unique session key each time a\nsecure channel is set up, the communicating parties are at least protected\nagainst replaying an entire session. To protect replaying individual messages\nfrom a previous session, additional measures are generally needed such as\nincluding timestamps or sequence numbers as part of the message content.\nSuppose that message integrity and con\ufb01dentiality were achieved by using\nthe same key used for session establishment. In that case, whenever the key\nis compromised, an intruder may be able to decrypt messages transferred\nduring an old conversation, clearly not a desirable feature. Instead, it is much\nsafer to use per-session keys, because if such a key is compromised, at worst,\nonly a single session is affected. Messages sent during other sessions stay\ncon\ufb01dential.\nRelated to this last point is that Alice may want to exchange some con\ufb01den-\ntial data with Bob, but she does not trust him so much that she would give him\ninformation in the form of data that have been encrypted with long-lasting\nkeys. She may want to reserve such keys for highly-con\ufb01dential messages that\nshe exchanges with parties she really trusts. In such cases, using a relatively\ncheap session key to talk to Bob is suf\ufb01cient.\nBy and large, authentication keys are often established in such a way that\nreplacing them is relatively expensive. Therefore, the combination of such\nlong-lasting keys with the much cheaper and more temporary session keys is\noften a good choice for implementing secure channels for exchanging data.\nSecure group communication\nSo far, we have concentrated on setting up a secure communication channel\nbetween two parties. In distributed systems, however, it is often necessary\nto enable secure communication between more than just two parties. A\ntypical example is that of a replicated server for which all communication\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 523\nthe channel is no longer used. An alternative would have been to use the\nsame keys for con\ufb01dentiality as those that are used for setting up the secure\nchannel. However, there are a number of important bene\ufb01ts to using session\nkeys [Kaufman et al., 2003].\nFirst, when a key is used often, it becomes easier to reveal it. In a sense,\ncryptographic keys are subject to \u201cwear and tear\u201d just like ordinary keys.\nThe basic idea is that if an intruder can intercept a lot of data that have been\nencrypted using the same key, it becomes possible to mount attacks to \ufb01nd\ncertain characteristics of the keys used, and possibly reveal the plaintext or\nthe key itself. For this reason, it is much safer to use the authentication keys\nas little as possible. In addition, such keys are often exchanged using some\nrelatively time-expensive out-of-band mechanism such as regular mail or\ntelephone. Exchanging keys that way should be kept to a minimum.\nAnother important reason for generating a unique key for each secure\nchannel is to ensure protection against replay attacks as we have come across\npreviously a number of times. By using a unique session key each time a\nsecure channel is set up, the communicating parties are at least protected\nagainst replaying an entire session. To protect replaying individual messages\nfrom a previous session, additional measures are generally needed such as\nincluding timestamps or sequence numbers as part of the message content.\nSuppose that message integrity and con\ufb01dentiality were achieved by using\nthe same key used for session establishment. In that case, whenever the key\nis compromised, an intruder may be able to decrypt messages transferred\nduring an old conversation, clearly not a desirable feature. Instead, it is much\nsafer to use per-session keys, because if such a key is compromised, at worst,\nonly a single session is affected. Messages sent during other sessions stay\ncon\ufb01dential.\nRelated to this last point is that Alice may want to exchange some con\ufb01den-\ntial data with Bob, but she does not trust him so much that she would give him\ninformation in the form of data that have been encrypted with long-lasting\nkeys. She may want to reserve such keys for highly-con\ufb01dential messages that\nshe exchanges with parties she really trusts. In such cases, using a relatively\ncheap session key to talk to Bob is suf\ufb01cient.\nBy and large, authentication keys are often established in such a way that\nreplacing them is relatively expensive. Therefore, the combination of such\nlong-lasting keys with the much cheaper and more temporary session keys is\noften a good choice for implementing secure channels for exchanging data.\nSecure group communication\nSo far, we have concentrated on setting up a secure communication channel\nbetween two parties. In distributed systems, however, it is often necessary\nto enable secure communication between more than just two parties. A\ntypical example is that of a replicated server for which all communication\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "524 CHAPTER 9. SECURITY\nbetween the replicas should be protected against modi\ufb01cation, fabrication,\nand interception, just as in the case of two-party secure channels. In this\nsection, we take a closer look at secure group communication.\nCon\ufb01dential group communication\nFirst, consider the problem of protecting communication between a group of N\nusers against eavesdropping. To ensure con\ufb01dentiality, a simple scheme is to\nlet all group members share the same secret key, which is used to encrypt and\ndecrypt all messages transmitted between group members. Because the secret\nkey in this scheme is shared by all members, it is necessary that all members\nare trusted to indeed keep the key a secret. This prerequisite alone makes the\nuse of a single shared secret key for con\ufb01dential group communication more\nvulnerable to attacks compared to two-party secure channels.\nAn alternative solution is to use a separate shared secret key between\neach pair of group members. As soon as one member turns out to be leaking\ninformation, the others can simply stop sending messages to that member, but\nstill use the keys they were using to communicate with each other. However,\ninstead of having to maintain one key, it is now necessary to maintain N(N\u0000\n1)/2 keys, which may be a dif\ufb01cult problem by itself.\nUsing a public-key cryptosystem can improve matters. In that case, each\nmember has its own (public key, private key) , pair, in which the public key can\nbe used by all members for sending con\ufb01dential messages. In this case, a\ntotal of Nkey pairs are needed. If one member ceases to be trustworthy, it is\nsimply removed from the group without having been able to compromise the\nother keys.\nSecure replicated servers\nNow consider a completely different problem: a client issues a request to a\ngroup of replicated servers. The servers may have been replicated for reasons\nof fault tolerance or performance, but in any case, the client expects the\nresponse to be trustworthy. In other words, regardless of whether the group\nof servers is subject to Byzantine failures, a client expects that the returned\nresponse has not been subject to corruption.\nA solution to protect the client against such situations is to collect the re-\nsponses from all servers and authenticate each one of them. If a majority exists\namong the responses from the noncorrupted (i.e., authenticated) servers, the\nclient can trust the response to be correct as well. Unfortunately, this approach\nreveals the replication of the servers, violating replication transparency.\nNote 9.5 (Advanced: Secret sharing)\nThe essence of secure and transparently replicated servers lies in what is known as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n524 CHAPTER 9. SECURITY\nbetween the replicas should be protected against modi\ufb01cation, fabrication,\nand interception, just as in the case of two-party secure channels. In this\nsection, we take a closer look at secure group communication.\nCon\ufb01dential group communication\nFirst, consider the problem of protecting communication between a group of N\nusers against eavesdropping. To ensure con\ufb01dentiality, a simple scheme is to\nlet all group members share the same secret key, which is used to encrypt and\ndecrypt all messages transmitted between group members. Because the secret\nkey in this scheme is shared by all members, it is necessary that all members\nare trusted to indeed keep the key a secret. This prerequisite alone makes the\nuse of a single shared secret key for con\ufb01dential group communication more\nvulnerable to attacks compared to two-party secure channels.\nAn alternative solution is to use a separate shared secret key between\neach pair of group members. As soon as one member turns out to be leaking\ninformation, the others can simply stop sending messages to that member, but\nstill use the keys they were using to communicate with each other. However,\ninstead of having to maintain one key, it is now necessary to maintain N(N\u0000\n1)/2 keys, which may be a dif\ufb01cult problem by itself.\nUsing a public-key cryptosystem can improve matters. In that case, each\nmember has its own (public key, private key) , pair, in which the public key can\nbe used by all members for sending con\ufb01dential messages. In this case, a\ntotal of Nkey pairs are needed. If one member ceases to be trustworthy, it is\nsimply removed from the group without having been able to compromise the\nother keys.\nSecure replicated servers\nNow consider a completely different problem: a client issues a request to a\ngroup of replicated servers. The servers may have been replicated for reasons\nof fault tolerance or performance, but in any case, the client expects the\nresponse to be trustworthy. In other words, regardless of whether the group\nof servers is subject to Byzantine failures, a client expects that the returned\nresponse has not been subject to corruption.\nA solution to protect the client against such situations is to collect the re-\nsponses from all servers and authenticate each one of them. If a majority exists\namong the responses from the noncorrupted (i.e., authenticated) servers, the\nclient can trust the response to be correct as well. Unfortunately, this approach\nreveals the replication of the servers, violating replication transparency.\nNote 9.5 (Advanced: Secret sharing)\nThe essence of secure and transparently replicated servers lies in what is known as\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 525\nsecret sharing . Essentially, when multiple users (or processes) share a secret, none\nof them knows the entire secret. Instead, the secret can be revealed only if they\nall get together. Such schemes can be extremely useful. Consider, for example,\nlaunching a nuclear missile. Such an act generally requires the authorization of\nat least two people. Each of them holds a private key that should be used in\ncombination with the other to actually launch a missile. Using only a single key\nwill not do.\nIn the case of secure, replicated servers, what we are seeking is a solution\nby which at most kout of the Nservers can produce an incorrect answer, and\nof those kservers, at most c\u0014khave actually been corrupted. Note that this\nrequirement makes the service itself k-fault tolerant as discussed in Section 8.2.\nThe difference lies in the fact that we now classify a corrupted server as being\nfaulty.\nFollowing Reiter [1994], consider the situation in which the servers are actively\nreplicated. In other words, a request is sent to all servers simultaneously, and\nsubsequently handled by each of them. Each server produces a response that it\nreturns to the client. For a securely replicated group of servers, we require that\neach server accompanies its response with a digital signature. If riis the response\nfrom server Si, let md(ri)denote the message digest computed by server Si. This\ndigest is signed with Si\u2019s private key K\u0000\ni.\nSuppose that we want to protect the client against at most ccorrupted servers.\nIn other words, the server group should be able to tolerate corruption by at most\ncservers, and still be capable of producing a response that the client can put its\ntrust in. If the signatures of the individual servers could be combined in such\na way that at least c+1signatures are needed to construct a valid signature for\nthe response, then this would solve our problem. In other words, we want to let\nthe replicated servers generate a secret valid signature with the property that c\ncorrupted servers alone are not enough to produce that signature.\nAs an example, consider a group of \ufb01ve replicated servers that should be able\nto tolerate two corrupted servers, and still produce a response that a client can\ntrust. Each server Sisends its response rito the client, along with its signature\nsig(Si,ri) = K\u0000\ni(md(ri)). Consequently, the client will eventually have received\n\ufb01ve tripletshri,md(ri),sig(Si,ri)ifrom which it should derive the correct response.\nThis situation is shown in Figure 9.16.\nEach digest md(ri)is also calculated by the client. If riis incorrect, then\nnormally this can be detected by computing K+\ni(K\u0000\ni(md(ri))). However, this\nmethod can no longer be applied, because no individual server can be trusted.\nInstead, the client uses a special, publicly-known decryption function D, which\ntakes a set\nV=fsig(S,r),sig(S0,r0),sig(S00,r00)g\nofthree signatures as input, and produces a single digest as output:\ndout=D(V) =D(sig(S,r),sig(S0,r0),sig(S00,r00))\nFor details on D, see [Reiter, 1994]. There are 5!/(3!2!) =10possible combina-\ntions of three signatures that the client can use as input for D. If one of these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 525\nsecret sharing . Essentially, when multiple users (or processes) share a secret, none\nof them knows the entire secret. Instead, the secret can be revealed only if they\nall get together. Such schemes can be extremely useful. Consider, for example,\nlaunching a nuclear missile. Such an act generally requires the authorization of\nat least two people. Each of them holds a private key that should be used in\ncombination with the other to actually launch a missile. Using only a single key\nwill not do.\nIn the case of secure, replicated servers, what we are seeking is a solution\nby which at most kout of the Nservers can produce an incorrect answer, and\nof those kservers, at most c\u0014khave actually been corrupted. Note that this\nrequirement makes the service itself k-fault tolerant as discussed in Section 8.2.\nThe difference lies in the fact that we now classify a corrupted server as being\nfaulty.\nFollowing Reiter [1994], consider the situation in which the servers are actively\nreplicated. In other words, a request is sent to all servers simultaneously, and\nsubsequently handled by each of them. Each server produces a response that it\nreturns to the client. For a securely replicated group of servers, we require that\neach server accompanies its response with a digital signature. If riis the response\nfrom server Si, let md(ri)denote the message digest computed by server Si. This\ndigest is signed with Si\u2019s private key K\u0000\ni.\nSuppose that we want to protect the client against at most ccorrupted servers.\nIn other words, the server group should be able to tolerate corruption by at most\ncservers, and still be capable of producing a response that the client can put its\ntrust in. If the signatures of the individual servers could be combined in such\na way that at least c+1signatures are needed to construct a valid signature for\nthe response, then this would solve our problem. In other words, we want to let\nthe replicated servers generate a secret valid signature with the property that c\ncorrupted servers alone are not enough to produce that signature.\nAs an example, consider a group of \ufb01ve replicated servers that should be able\nto tolerate two corrupted servers, and still produce a response that a client can\ntrust. Each server Sisends its response rito the client, along with its signature\nsig(Si,ri) = K\u0000\ni(md(ri)). Consequently, the client will eventually have received\n\ufb01ve tripletshri,md(ri),sig(Si,ri)ifrom which it should derive the correct response.\nThis situation is shown in Figure 9.16.\nEach digest md(ri)is also calculated by the client. If riis incorrect, then\nnormally this can be detected by computing K+\ni(K\u0000\ni(md(ri))). However, this\nmethod can no longer be applied, because no individual server can be trusted.\nInstead, the client uses a special, publicly-known decryption function D, which\ntakes a set\nV=fsig(S,r),sig(S0,r0),sig(S00,r00)g\nofthree signatures as input, and produces a single digest as output:\ndout=D(V) =D(sig(S,r),sig(S0,r0),sig(S00,r00))\nFor details on D, see [Reiter, 1994]. There are 5!/(3!2!) =10possible combina-\ntions of three signatures that the client can use as input for D. If one of these\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "526 CHAPTER 9. SECURITY\ncombinations produces a correct digest md(ri)for some response ri, then the client\ncan consider rias being correct. In particular, it can trust that the response has\nbeen produced by at least three honest servers.\nFigure 9.16: Sharing a secret signature in a group of replicated servers.\nTo improve replication transparency, we can let each server Sibroadcast a\nmessage containing its response rito the other servers, along with the associated\nsignature sig(Si,ri). When a server has received at least c+1of such messages,\nincluding its own message, it attempts to compute a valid signature for one of the\nresponses. If this succeeds for, say, response rand the set Vofc+1signatures, the\nserver sends randVas a single message to the client. The client can subsequently\nverify the correctness of rby checking its signature, that is, whether md(r) =D(V).\nWhat we have just described is also known as an (m,n)-threshold scheme ,\nwith, in our example, m=c+1and n=N, the number of servers. In an\n(m,n)-threshold scheme, a message has been divided into npieces, known as\nshadows , since any mshadows can be used to reconstruct the original message,\nbut using m\u00001or fewer messages cannot. There are several ways to construct\n(m,n)-threshold schemes. Details can be found in [Schneier, 1996]\nExample: Kerberos\nIt should be clear by now that incorporating security into distributed systems\nis not trivial. Problems are caused by the fact that the entire system must be\nsecure; if some part is insecure, the whole system may be compromised. To\nassist the construction of distributed systems that can enforce a myriad of\nsecurity policies, a number of supporting systems have been developed that\ncan be used as a basis for further development. An important system that is\nwidely used is Kerberos [Steiner et al., 1988; Kohl et al., 1994]\nKerberos was developed at M.I.T. and is based on the Needham-Schroeder\nauthentication protocol . A detailed description of the Kerberos system can\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n526 CHAPTER 9. SECURITY\ncombinations produces a correct digest md(ri)for some response ri, then the client\ncan consider rias being correct. In particular, it can trust that the response has\nbeen produced by at least three honest servers.\nFigure 9.16: Sharing a secret signature in a group of replicated servers.\nTo improve replication transparency, we can let each server Sibroadcast a\nmessage containing its response rito the other servers, along with the associated\nsignature sig(Si,ri). When a server has received at least c+1of such messages,\nincluding its own message, it attempts to compute a valid signature for one of the\nresponses. If this succeeds for, say, response rand the set Vofc+1signatures, the\nserver sends randVas a single message to the client. The client can subsequently\nverify the correctness of rby checking its signature, that is, whether md(r) =D(V).\nWhat we have just described is also known as an (m,n)-threshold scheme ,\nwith, in our example, m=c+1and n=N, the number of servers. In an\n(m,n)-threshold scheme, a message has been divided into npieces, known as\nshadows , since any mshadows can be used to reconstruct the original message,\nbut using m\u00001or fewer messages cannot. There are several ways to construct\n(m,n)-threshold schemes. Details can be found in [Schneier, 1996]\nExample: Kerberos\nIt should be clear by now that incorporating security into distributed systems\nis not trivial. Problems are caused by the fact that the entire system must be\nsecure; if some part is insecure, the whole system may be compromised. To\nassist the construction of distributed systems that can enforce a myriad of\nsecurity policies, a number of supporting systems have been developed that\ncan be used as a basis for further development. An important system that is\nwidely used is Kerberos [Steiner et al., 1988; Kohl et al., 1994]\nKerberos was developed at M.I.T. and is based on the Needham-Schroeder\nauthentication protocol . A detailed description of the Kerberos system can\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.2. SECURE CHANNELS 527\nbe found in [Neuman et al., 2005] whereas practical information on running\nKerberos is described by Garman [2003]. A publicly available implementation\nof Kerberos, known as Shishi, is described in [Josefsson, 2015].\nKerberos can be viewed as a security system that assists clients in setting\nup a secure channel with any server that is part of a distributed system.\nSecurity is based on shared secret keys. There are two different components.\nThe Authentication Server (AS) is responsible for handling a login request\nfrom a user. The AS authenticates a user and provides a key that can be used\nto set up secure channels with servers. Setting up secure channels is handled\nby a Ticket Granting Service (TGS ). The TGS hands out special messages,\nknown as tickets , that are used to convince a server that the client is really\nwho he or she claims to be. We give concrete examples of tickets below.\nLet us take a look at how Alice logs onto a distributed system that uses\nKerberos and how she can set up a secure channel with server Bob. For Alice\nto log onto the system, she can use any workstation available. The workstation\nsends her name in plaintext to the AS, which returns a session key KA,TGS\nand a ticket that she will need to hand over to the TGS.\nThe ticket that is returned by the AS contains the identity of Alice, along\nwith a generated secret key that Alice and the TGS can use to communicate\nwith each other. The ticket itself will be handed over to the TGS by Alice.\nTherefore, it is important that no one but the TGS can read it. For this reason,\nthe ticket is encrypted with the secret key KAS,TGS shared between the AS\nand the TGS.\nFigure 9.17: Authentication in Kerberos.\nThis part of the login procedure is shown as messages 1, 2, and 3 in\nFigure 9.17, respectively. Message 1 is not really a message, but corresponds\nto Alice typing in her login name at a workstation. Message 2 contains that\nname and is sent to the AS. Message 3 contains the session key KA,TGS and\nthe ticket KAS,TGS(A,KA,TGS). To ensure privacy, message 3 is encrypted with\nthe secret key KA,ASshared between Alice and the AS.\nWhen the workstation receives the response from the AS, it prompts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.2. SECURE CHANNELS 527\nbe found in [Neuman et al., 2005] whereas practical information on running\nKerberos is described by Garman [2003]. A publicly available implementation\nof Kerberos, known as Shishi, is described in [Josefsson, 2015].\nKerberos can be viewed as a security system that assists clients in setting\nup a secure channel with any server that is part of a distributed system.\nSecurity is based on shared secret keys. There are two different components.\nThe Authentication Server (AS) is responsible for handling a login request\nfrom a user. The AS authenticates a user and provides a key that can be used\nto set up secure channels with servers. Setting up secure channels is handled\nby a Ticket Granting Service (TGS ). The TGS hands out special messages,\nknown as tickets , that are used to convince a server that the client is really\nwho he or she claims to be. We give concrete examples of tickets below.\nLet us take a look at how Alice logs onto a distributed system that uses\nKerberos and how she can set up a secure channel with server Bob. For Alice\nto log onto the system, she can use any workstation available. The workstation\nsends her name in plaintext to the AS, which returns a session key KA,TGS\nand a ticket that she will need to hand over to the TGS.\nThe ticket that is returned by the AS contains the identity of Alice, along\nwith a generated secret key that Alice and the TGS can use to communicate\nwith each other. The ticket itself will be handed over to the TGS by Alice.\nTherefore, it is important that no one but the TGS can read it. For this reason,\nthe ticket is encrypted with the secret key KAS,TGS shared between the AS\nand the TGS.\nFigure 9.17: Authentication in Kerberos.\nThis part of the login procedure is shown as messages 1, 2, and 3 in\nFigure 9.17, respectively. Message 1 is not really a message, but corresponds\nto Alice typing in her login name at a workstation. Message 2 contains that\nname and is sent to the AS. Message 3 contains the session key KA,TGS and\nthe ticket KAS,TGS(A,KA,TGS). To ensure privacy, message 3 is encrypted with\nthe secret key KA,ASshared between Alice and the AS.\nWhen the workstation receives the response from the AS, it prompts\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "528 CHAPTER 9. SECURITY\nAlice for her password (shown as message 4), which it uses to subsequently\ngenerate the shared key KA,AS. (It is relatively simple to take a character\nstring password, apply a cryptographic hash, and then take the \ufb01rst 56 bits\nas the secret key.) Note that this approach not only has the advantage that\nAlice\u2019s password is never sent as plaintext across the network, but also that\nthe workstation does not even have to temporarily store it. Moreover, as soon\nas it has generated the shared key KA,AS, the workstation will \ufb01nd the session\nkey KA,TGS, and can forget about Alice\u2019s password and use only the shared\nsecret KA,AS.\nAfter this part of the authentication has taken place, Alice can consider\nherself logged into the system through the current workstation. The ticket\nreceived from the AS is stored temporarily (typically for 8\u201324 hours), and\nwill be used for accessing remote services. Of course, if Alice leaves her\nworkstation, she should destroy any cached tickets. If she wants to talk to Bob,\nshe requests the TGS to generate a session key for Bob, shown as message 6 in\nFigure 9.17. The fact that Alice has the ticket KAS,TGS(A,KA,TGS)proves that\nshe is Alice. The TGS responds with a session key KA,B, again encapsulated\nin a ticket that Alice will later have to pass to Bob.\nMessage 6 also contains a timestamp, t, encrypted with the secret key\nshared between Alice and the TGS. This timestamp is used to prevent Chuck\nfrom maliciously replaying message 6 again, and trying to set up a channel to\nBob. The TGS will verify the timestamp before returning a ticket to Alice. If it\ndiffers more than a few minutes from the current time, the request for a ticket\nis rejected.\nThis scheme establishes what is known as single sign-on . As long as\nAlice does not change workstations, there is no need for her to authenticate\nherself to any other server that is part of the distributed system. This feature\nis important when having to deal with many different services that are spread\nacross multiple machines. In principle, servers in a way have delegated client\nauthentication to the AS and TGS, and will accept requests from any client that\nhas a valid ticket. Of course, services such as remote login will require that the\nassociated user has an account, but this is independent from authentication\nthrough Kerberos.\nFigure 9.18: Setting up a secure channel in Kerberos.\nSetting up a secure channel with Bob is now straightforward, and is shown\nin Figure 9.18. First, Alice sends to Bob a message containing the ticket she\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n528 CHAPTER 9. SECURITY\nAlice for her password (shown as message 4), which it uses to subsequently\ngenerate the shared key KA,AS. (It is relatively simple to take a character\nstring password, apply a cryptographic hash, and then take the \ufb01rst 56 bits\nas the secret key.) Note that this approach not only has the advantage that\nAlice\u2019s password is never sent as plaintext across the network, but also that\nthe workstation does not even have to temporarily store it. Moreover, as soon\nas it has generated the shared key KA,AS, the workstation will \ufb01nd the session\nkey KA,TGS, and can forget about Alice\u2019s password and use only the shared\nsecret KA,AS.\nAfter this part of the authentication has taken place, Alice can consider\nherself logged into the system through the current workstation. The ticket\nreceived from the AS is stored temporarily (typically for 8\u201324 hours), and\nwill be used for accessing remote services. Of course, if Alice leaves her\nworkstation, she should destroy any cached tickets. If she wants to talk to Bob,\nshe requests the TGS to generate a session key for Bob, shown as message 6 in\nFigure 9.17. The fact that Alice has the ticket KAS,TGS(A,KA,TGS)proves that\nshe is Alice. The TGS responds with a session key KA,B, again encapsulated\nin a ticket that Alice will later have to pass to Bob.\nMessage 6 also contains a timestamp, t, encrypted with the secret key\nshared between Alice and the TGS. This timestamp is used to prevent Chuck\nfrom maliciously replaying message 6 again, and trying to set up a channel to\nBob. The TGS will verify the timestamp before returning a ticket to Alice. If it\ndiffers more than a few minutes from the current time, the request for a ticket\nis rejected.\nThis scheme establishes what is known as single sign-on . As long as\nAlice does not change workstations, there is no need for her to authenticate\nherself to any other server that is part of the distributed system. This feature\nis important when having to deal with many different services that are spread\nacross multiple machines. In principle, servers in a way have delegated client\nauthentication to the AS and TGS, and will accept requests from any client that\nhas a valid ticket. Of course, services such as remote login will require that the\nassociated user has an account, but this is independent from authentication\nthrough Kerberos.\nFigure 9.18: Setting up a secure channel in Kerberos.\nSetting up a secure channel with Bob is now straightforward, and is shown\nin Figure 9.18. First, Alice sends to Bob a message containing the ticket she\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 529\ngot from the TGS, along with an encrypted timestamp. When Bob decrypts\nthe ticket, he notices that Alice is talking to him, because only the TGS could\nhave constructed the ticket. He also \ufb01nds the secret key KA,B, allowing him to\nverify the timestamp. At that point, Bob knows he is talking to Alice and not\nsomeone maliciously replaying message 1. By responding with KA,B(t+1),\nBob proves to Alice that he is indeed Bob.\n9.3 Access control\nIn the client-server model, which we have used so far, once a client and a\nserver have set up a secure channel, the client can issue requests that are to\nbe carried out by the server. Requests involve carrying out operations on\nresources that are controlled by the server. A general situation is that of an\nobject server that has a number of objects under its control. A request from\na client generally involves invoking a method of a speci\ufb01c object. Such a\nrequest can be carried out only if the client has suf\ufb01cient access rights for that\ninvocation. Formally, verifying access rights is referred to as access control ,\nwhereas authorization is about granting access rights. The two terms are\nstrongly related to each other and are often used in an interchangeable way.\nGeneral issues in access control\nIn order to understand the various issues involved in access control, the simple\nmodel shown in Figure 9.19 is generally adopted. It consists of subjects that\nissue a request to access an object . An object is very much like the objects\nwe have been discussing so far. It can be thought of as encapsulating its own\nstate and implementing the operations on that state. The operations of an\nobject that subjects can request to be carried out are made available through\ninterfaces. Subjects can best be thought of as being processes acting on behalf\nof users, but can also be objects that need the services of other objects in order\nto carry out their work.\nFigure 9.19: General model of controlling access to objects.\nControlling the access to an object is all about protecting the object against\ninvocations by subjects that are not allowed to have speci\ufb01c (or even any) of the\nmethods carried out. Also, protection may include object management issues,\nsuch as creating, renaming, or deleting objects. Protection is often enforced\nby a program called a reference monitor . A reference monitor records which\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 529\ngot from the TGS, along with an encrypted timestamp. When Bob decrypts\nthe ticket, he notices that Alice is talking to him, because only the TGS could\nhave constructed the ticket. He also \ufb01nds the secret key KA,B, allowing him to\nverify the timestamp. At that point, Bob knows he is talking to Alice and not\nsomeone maliciously replaying message 1. By responding with KA,B(t+1),\nBob proves to Alice that he is indeed Bob.\n9.3 Access control\nIn the client-server model, which we have used so far, once a client and a\nserver have set up a secure channel, the client can issue requests that are to\nbe carried out by the server. Requests involve carrying out operations on\nresources that are controlled by the server. A general situation is that of an\nobject server that has a number of objects under its control. A request from\na client generally involves invoking a method of a speci\ufb01c object. Such a\nrequest can be carried out only if the client has suf\ufb01cient access rights for that\ninvocation. Formally, verifying access rights is referred to as access control ,\nwhereas authorization is about granting access rights. The two terms are\nstrongly related to each other and are often used in an interchangeable way.\nGeneral issues in access control\nIn order to understand the various issues involved in access control, the simple\nmodel shown in Figure 9.19 is generally adopted. It consists of subjects that\nissue a request to access an object . An object is very much like the objects\nwe have been discussing so far. It can be thought of as encapsulating its own\nstate and implementing the operations on that state. The operations of an\nobject that subjects can request to be carried out are made available through\ninterfaces. Subjects can best be thought of as being processes acting on behalf\nof users, but can also be objects that need the services of other objects in order\nto carry out their work.\nFigure 9.19: General model of controlling access to objects.\nControlling the access to an object is all about protecting the object against\ninvocations by subjects that are not allowed to have speci\ufb01c (or even any) of the\nmethods carried out. Also, protection may include object management issues,\nsuch as creating, renaming, or deleting objects. Protection is often enforced\nby a program called a reference monitor . A reference monitor records which\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "530 CHAPTER 9. SECURITY\nsubject may do what, and decides whether a subject is allowed to have a\nspeci\ufb01c operation carried out. This monitor is called (e.g., by the underlying\ntrusted operating system) each time an object is invoked. Consequently, it\nis extremely important that the reference monitor is itself tamperproof: an\nattacker must not be able to fool around with it.\nAccess control matrix\nA common approach to modeling the access rights of subjects with respect to\nobjects is to construct an access control matrix . Each subject is represented\nby a row in this matrix; each object is represented by a column. If the matrix\nis denoted M, then an entry M[s,o]lists precisely which operations subject s\ncan request to be carried out on object o. In other words, whenever a subject s\nrequests the invocation of method mof object o, the reference monitor should\ncheck if mis listed in M[s,o]. Ifmis not listed in M[s,o], the invocation fails.\nConsidering that a system may easily need to support thousands of users\nand millions of objects that require protection, implementing an access control\nmatrix as a true matrix is not the way to go. Many entries in the matrix will\nbe empty: a single subject will generally have access to relatively few objects.\nTherefore, other, more ef\ufb01cient ways are followed to implement an access\ncontrol matrix.\nOne widely applied approach is to have each object maintain a list of the\naccess rights of subjects that want to access the object. In essence, this means\nthat the matrix is distributed column-wise across all objects, and that empty\nentries are left out. This type of implementation leads to what is called an\nAccess Control List (ACL ). Each object is assumed to have its own associated\nACL.\nAnother approach is to distribute the matrix row-wise by giving each\nsubject a list of capabilities it has for each object. In other words, a capability\ncorresponds to an entry in the access control matrix. Not having a capability\nfor a speci\ufb01c object means that the subject has no access rights for that object.\nA capability can be compared to a ticket: its holder is given certain rights\nthat are associated with that ticket. It is also clear that a ticket should be\nprotected against modi\ufb01cations by its holder. One approach that is particu-\nlarly suited in distributed systems, is to protect (a list of) capabilities with a\nsignature. We return to these and other matters later when discussing security\nmanagement.\nThe difference between how ACLs and capabilities are used to protect the\naccess to an object is shown in Figure 9.20. Using ACLs, when a client sends\na request to a server, the server\u2019s reference monitor will check whether it\nknows the client and if that client is known and allowed to have the requested\noperation carried out, as shown in Figure 9.20(a).\nHowever, when using capabilities, a client simply sends its request to\nthe server. The server is not interested in whether it knows the client; the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n530 CHAPTER 9. SECURITY\nsubject may do what, and decides whether a subject is allowed to have a\nspeci\ufb01c operation carried out. This monitor is called (e.g., by the underlying\ntrusted operating system) each time an object is invoked. Consequently, it\nis extremely important that the reference monitor is itself tamperproof: an\nattacker must not be able to fool around with it.\nAccess control matrix\nA common approach to modeling the access rights of subjects with respect to\nobjects is to construct an access control matrix . Each subject is represented\nby a row in this matrix; each object is represented by a column. If the matrix\nis denoted M, then an entry M[s,o]lists precisely which operations subject s\ncan request to be carried out on object o. In other words, whenever a subject s\nrequests the invocation of method mof object o, the reference monitor should\ncheck if mis listed in M[s,o]. Ifmis not listed in M[s,o], the invocation fails.\nConsidering that a system may easily need to support thousands of users\nand millions of objects that require protection, implementing an access control\nmatrix as a true matrix is not the way to go. Many entries in the matrix will\nbe empty: a single subject will generally have access to relatively few objects.\nTherefore, other, more ef\ufb01cient ways are followed to implement an access\ncontrol matrix.\nOne widely applied approach is to have each object maintain a list of the\naccess rights of subjects that want to access the object. In essence, this means\nthat the matrix is distributed column-wise across all objects, and that empty\nentries are left out. This type of implementation leads to what is called an\nAccess Control List (ACL ). Each object is assumed to have its own associated\nACL.\nAnother approach is to distribute the matrix row-wise by giving each\nsubject a list of capabilities it has for each object. In other words, a capability\ncorresponds to an entry in the access control matrix. Not having a capability\nfor a speci\ufb01c object means that the subject has no access rights for that object.\nA capability can be compared to a ticket: its holder is given certain rights\nthat are associated with that ticket. It is also clear that a ticket should be\nprotected against modi\ufb01cations by its holder. One approach that is particu-\nlarly suited in distributed systems, is to protect (a list of) capabilities with a\nsignature. We return to these and other matters later when discussing security\nmanagement.\nThe difference between how ACLs and capabilities are used to protect the\naccess to an object is shown in Figure 9.20. Using ACLs, when a client sends\na request to a server, the server\u2019s reference monitor will check whether it\nknows the client and if that client is known and allowed to have the requested\noperation carried out, as shown in Figure 9.20(a).\nHowever, when using capabilities, a client simply sends its request to\nthe server. The server is not interested in whether it knows the client; the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 531\n(a)\n(b)\nFigure 9.20: Comparison between ACLs and capabilities for protecting objects.\n(a) Using an ACL. (b) Using capabilities.\ncapability says enough. Therefore, the server need only check whether the\ncapability is valid and whether the requested operation is listed in the capa-\nbility. This approach to protecting objects by means of capabilities is shown in\nFigure 9.20(b).\nNote 9.6 (Advanced: Protection domains)\nACLs and capabilities help in ef\ufb01ciently implementing an access control matrix\nby ignoring all empty entries. Nevertheless, an ACL or a capability list can still\nbecome quite large if no further measures are taken.\nOne general way of reducing ACLs is to make use of protection domains.\nFormally, a protection domain is a set of (object, access rights) , pairs. Each pair\nspeci\ufb01es for a given object exactly which operations are allowed to be carried\nout [Saltzer and Schroeder, 1975]. Requests for carrying out an operation are\nalways issued within a domain. Therefore, whenever a subject requests an\noperation to be carried out at an object, the reference monitor \ufb01rst looks up the\nprotection domain associated with that request. Then, given the domain, the\nreference monitor can subsequently check whether the request is allowed to be\ncarried out. Different uses of protection domains exist.\nOne approach is to construct groups of users. Consider, for example, a Web\npage on a company\u2019s internal intranet. Such a page should be available to every\nemployee, but otherwise to no one else. Instead of adding an entry for each\npossible employee to the ACL for that Web page, it may be decided to have\na separate group Employee containing all current employees. Whenever a user\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 531\n(a)\n(b)\nFigure 9.20: Comparison between ACLs and capabilities for protecting objects.\n(a) Using an ACL. (b) Using capabilities.\ncapability says enough. Therefore, the server need only check whether the\ncapability is valid and whether the requested operation is listed in the capa-\nbility. This approach to protecting objects by means of capabilities is shown in\nFigure 9.20(b).\nNote 9.6 (Advanced: Protection domains)\nACLs and capabilities help in ef\ufb01ciently implementing an access control matrix\nby ignoring all empty entries. Nevertheless, an ACL or a capability list can still\nbecome quite large if no further measures are taken.\nOne general way of reducing ACLs is to make use of protection domains.\nFormally, a protection domain is a set of (object, access rights) , pairs. Each pair\nspeci\ufb01es for a given object exactly which operations are allowed to be carried\nout [Saltzer and Schroeder, 1975]. Requests for carrying out an operation are\nalways issued within a domain. Therefore, whenever a subject requests an\noperation to be carried out at an object, the reference monitor \ufb01rst looks up the\nprotection domain associated with that request. Then, given the domain, the\nreference monitor can subsequently check whether the request is allowed to be\ncarried out. Different uses of protection domains exist.\nOne approach is to construct groups of users. Consider, for example, a Web\npage on a company\u2019s internal intranet. Such a page should be available to every\nemployee, but otherwise to no one else. Instead of adding an entry for each\npossible employee to the ACL for that Web page, it may be decided to have\na separate group Employee containing all current employees. Whenever a user\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "532 CHAPTER 9. SECURITY\naccesses the Web page, the reference monitor need only check whether that user\nis an employee. Which users belong to the group Employee is kept in a separate\nlist (which, of course, is protected against unauthorized access).\nMatters can be made more \ufb02exible by introducing hierarchical groups. For\nexample, if an organization has three different branches at, say, Amsterdam,\nNew York, and San Francisco, it may want to subdivide its Employee group into\nsubgroups, one for each city, leading to an organization as shown in Figure 9.21.\nAccessing Web pages of the organization\u2019s intranet should be permitted by\nall employees. However, changing for example Web pages associated with the\nAmsterdam branch should be permitted only by a subset of employees in Am-\nsterdam. If user Dick from Amsterdam wants to read a Web page from the\nintranet, the reference monitor needs to \ufb01rst look up the subsets Employee _AMS ,\nEmployee _NYC , and Employee _SFthat jointly comprise the set Employee . It then\nhas to check if one of these sets contains Dick. The advantage of having hier-\narchical groups is that managing group membership is relatively easy, and that\nvery large groups can be constructed ef\ufb01ciently. An obvious disadvantage is\nthat looking up a member can be quite costly if the membership database is\ndistributed.\nFigure 9.21: The hierarchical organization of protection domains as\ngroups of users.\nInstead of letting the reference monitor do all the work, an alternative is to\nlet each subject carry a certi\ufb01cate listing the groups it belongs to. So, whenever\nDick wants to read a Web page from the company\u2019s intranet, he hands over his\ncerti\ufb01cate to the reference monitor stating that he is a member of Employee _AMS .\nTo guarantee that the certi\ufb01cate is genuine and has not been tampered with, it\nshould be protected by means of, for example, a digital signature. Certi\ufb01cates are\nseen to be comparable to capabilities.\nRelated to having groups as protection domains, it is also possible to imple-\nment protection domains as roles . In role-based access control, a user always\nlogs into the system with a speci\ufb01c role, which is often associated with a function\nthe user has in an organization [Sandhu et al., 1996]. A user may have several\nfunctions. For example, Dick could simultaneously be head of a department,\nmanager of a project, and member of a personnel search committee. Depending\non the role he takes when logging in, he may be assigned different privileges. In\nother words, his role determines the protection domain (i.e., group) in which he\nwill operate.\nWhen assigning roles to users and requiring that users take on a speci\ufb01c\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n532 CHAPTER 9. SECURITY\naccesses the Web page, the reference monitor need only check whether that user\nis an employee. Which users belong to the group Employee is kept in a separate\nlist (which, of course, is protected against unauthorized access).\nMatters can be made more \ufb02exible by introducing hierarchical groups. For\nexample, if an organization has three different branches at, say, Amsterdam,\nNew York, and San Francisco, it may want to subdivide its Employee group into\nsubgroups, one for each city, leading to an organization as shown in Figure 9.21.\nAccessing Web pages of the organization\u2019s intranet should be permitted by\nall employees. However, changing for example Web pages associated with the\nAmsterdam branch should be permitted only by a subset of employees in Am-\nsterdam. If user Dick from Amsterdam wants to read a Web page from the\nintranet, the reference monitor needs to \ufb01rst look up the subsets Employee _AMS ,\nEmployee _NYC , and Employee _SFthat jointly comprise the set Employee . It then\nhas to check if one of these sets contains Dick. The advantage of having hier-\narchical groups is that managing group membership is relatively easy, and that\nvery large groups can be constructed ef\ufb01ciently. An obvious disadvantage is\nthat looking up a member can be quite costly if the membership database is\ndistributed.\nFigure 9.21: The hierarchical organization of protection domains as\ngroups of users.\nInstead of letting the reference monitor do all the work, an alternative is to\nlet each subject carry a certi\ufb01cate listing the groups it belongs to. So, whenever\nDick wants to read a Web page from the company\u2019s intranet, he hands over his\ncerti\ufb01cate to the reference monitor stating that he is a member of Employee _AMS .\nTo guarantee that the certi\ufb01cate is genuine and has not been tampered with, it\nshould be protected by means of, for example, a digital signature. Certi\ufb01cates are\nseen to be comparable to capabilities.\nRelated to having groups as protection domains, it is also possible to imple-\nment protection domains as roles . In role-based access control, a user always\nlogs into the system with a speci\ufb01c role, which is often associated with a function\nthe user has in an organization [Sandhu et al., 1996]. A user may have several\nfunctions. For example, Dick could simultaneously be head of a department,\nmanager of a project, and member of a personnel search committee. Depending\non the role he takes when logging in, he may be assigned different privileges. In\nother words, his role determines the protection domain (i.e., group) in which he\nwill operate.\nWhen assigning roles to users and requiring that users take on a speci\ufb01c\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 533\nrole when logging in, it should also be possible for users to change their roles\nif necessary. For example, it may be required to allow Dick as head of the\ndepartment to occasionally change to his role of project manager. Note that such\nchanges are dif\ufb01cult to express when implementing protection domains only as\ngroups.\nBesides using protection domains, ef\ufb01ciency can be further improved by\n(hierarchically) grouping objects based on the operations they provide. For\nexample, instead of considering individual objects, objects are grouped according\nto the interfaces they provide, possibly using subtyping (also referred to as\ninterface inheritance, see Gamma et al. [1994]). to achieve a hierarchy. In this\ncase, when a subject requests an operation to be carried out at an object, the\nreference monitor looks up to which interface the operation for that object belongs.\nIt then checks whether the subject is allowed to call an operation belonging to\nthat interface, rather than if it can call the operation for the speci\ufb01c object.\nCombining protection domains and grouping of objects is also possible. Using\nboth techniques, along with speci\ufb01c data structures and restricted operations\non objects, Gladney [1997] describes how to implement ACLs for very large\ncollections of objects that are used in digital libraries.\nFirewalls\nSo far, we have shown how protection can be established using cryptographic\ntechniques, combined with some implementation of an access control matrix.\nThese approaches work \ufb01ne as long as all communicating parties play accord-\ning to the same set of rules. Such rules may be enforced when developing a\nstand-alone distributed system that is more or less isolated from the rest of\nthe world. However, matters become more complicated when communication\nwith the rest of the world is necessary, like sending mail, accessing Web sites,\nor making local resources available.\nTo protect resources under these circumstances, a different approach is\nneeded. In practice, what happens is that external access to any part of a\ndistributed system is controlled by a special kind of reference monitor known\nas a \ufb01rewall [Cheswick and Bellovin, 2000; Zwicky et al., 2000]. Firewalls\nform one of the most often used protection mechanisms in networked systems.\nEssentially, a \ufb01rewall disconnects any part of a distributed system from\nthe outside world, as shown in Figure 9.22. All outgoing, but especially\nall incoming packets are routed through a special computer and inspected\nbefore they are passed through. Unauthorized traf\ufb01c is discarded and not\nallowed to continue. An important issue is that the \ufb01rewall itself should be\nheavily protected against any kind of security threat: it should never fail.\nEqually important is that the rules that prescribe what can pass through\nare consistent and establish what is intended. As reported by Wool [2010],\nproperly con\ufb01guring a \ufb01rewall is a considerable challenge.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 533\nrole when logging in, it should also be possible for users to change their roles\nif necessary. For example, it may be required to allow Dick as head of the\ndepartment to occasionally change to his role of project manager. Note that such\nchanges are dif\ufb01cult to express when implementing protection domains only as\ngroups.\nBesides using protection domains, ef\ufb01ciency can be further improved by\n(hierarchically) grouping objects based on the operations they provide. For\nexample, instead of considering individual objects, objects are grouped according\nto the interfaces they provide, possibly using subtyping (also referred to as\ninterface inheritance, see Gamma et al. [1994]). to achieve a hierarchy. In this\ncase, when a subject requests an operation to be carried out at an object, the\nreference monitor looks up to which interface the operation for that object belongs.\nIt then checks whether the subject is allowed to call an operation belonging to\nthat interface, rather than if it can call the operation for the speci\ufb01c object.\nCombining protection domains and grouping of objects is also possible. Using\nboth techniques, along with speci\ufb01c data structures and restricted operations\non objects, Gladney [1997] describes how to implement ACLs for very large\ncollections of objects that are used in digital libraries.\nFirewalls\nSo far, we have shown how protection can be established using cryptographic\ntechniques, combined with some implementation of an access control matrix.\nThese approaches work \ufb01ne as long as all communicating parties play accord-\ning to the same set of rules. Such rules may be enforced when developing a\nstand-alone distributed system that is more or less isolated from the rest of\nthe world. However, matters become more complicated when communication\nwith the rest of the world is necessary, like sending mail, accessing Web sites,\nor making local resources available.\nTo protect resources under these circumstances, a different approach is\nneeded. In practice, what happens is that external access to any part of a\ndistributed system is controlled by a special kind of reference monitor known\nas a \ufb01rewall [Cheswick and Bellovin, 2000; Zwicky et al., 2000]. Firewalls\nform one of the most often used protection mechanisms in networked systems.\nEssentially, a \ufb01rewall disconnects any part of a distributed system from\nthe outside world, as shown in Figure 9.22. All outgoing, but especially\nall incoming packets are routed through a special computer and inspected\nbefore they are passed through. Unauthorized traf\ufb01c is discarded and not\nallowed to continue. An important issue is that the \ufb01rewall itself should be\nheavily protected against any kind of security threat: it should never fail.\nEqually important is that the rules that prescribe what can pass through\nare consistent and establish what is intended. As reported by Wool [2010],\nproperly con\ufb01guring a \ufb01rewall is a considerable challenge.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "534 CHAPTER 9. SECURITY\nFigure 9.22: A common implementation of a \ufb01rewall.\nFirewalls essentially come in two different \ufb02avors that are often combined.\nAn important type of \ufb01rewall is a packet-\ufb01ltering gateway . This type of\n\ufb01rewall operates as a router and makes decisions as to whether or not to pass\na network packet based on the source and destination address as contained\nin the packet\u2019s header. Typically, the packet-\ufb01ltering gateway shown on the\noutside LAN in Figure 9.22 would protect against incoming packets, whereas\nthe one on the inside LAN would \ufb01lter outgoing packets.\nFor example, to protect an internal Web server against requests from hosts\nthat are not on the internal network, a packet-\ufb01ltering gateway could decide\nto drop all incoming packets addressed to the Web server.\nMore subtle is the situation in which a company\u2019s network consists of\nmultiple local-area networks. Each LAN can be protected by means of a\npacket-\ufb01ltering gateway, which is con\ufb01gured to pass incoming traf\ufb01c only if it\noriginated from a host on one of the other LANs. In this way, a private virtual\nnetwork can be set up.\nThe other type of \ufb01rewall is an application-level gateway . In contrast to a\npacket-\ufb01ltering gateway, which inspects only the header of network packets,\nthis type of \ufb01rewall actually inspects the content of an incoming or outgoing\nmessage. A typical example is a mail gateway that discards incoming or\noutgoing mail exceeding a certain size. More sophisticated mail gateways\nexist that are, for example, capable of \ufb01ltering spam e-mail.\nAnother example of an application-level gateway is one that allows external\naccess to a digital library server, but will supply only abstracts of documents.\nIf an external user wants more, an electronic payment protocol is started.\nUsers inside the \ufb01rewall have direct access to the library service.\nA special kind of application-level gateway is what is known as a proxy\ngateway . This type of \ufb01rewall works as a front end to a speci\ufb01c kind of\napplication, and ensures that only those messages are passed that meet certain\ncriteria. Consider, for example, sur\ufb01ng the Web. Many Web pages contain\nscripts or applets that are to be executed in a user\u2019s browser. To prevent such\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n534 CHAPTER 9. SECURITY\nFigure 9.22: A common implementation of a \ufb01rewall.\nFirewalls essentially come in two different \ufb02avors that are often combined.\nAn important type of \ufb01rewall is a packet-\ufb01ltering gateway . This type of\n\ufb01rewall operates as a router and makes decisions as to whether or not to pass\na network packet based on the source and destination address as contained\nin the packet\u2019s header. Typically, the packet-\ufb01ltering gateway shown on the\noutside LAN in Figure 9.22 would protect against incoming packets, whereas\nthe one on the inside LAN would \ufb01lter outgoing packets.\nFor example, to protect an internal Web server against requests from hosts\nthat are not on the internal network, a packet-\ufb01ltering gateway could decide\nto drop all incoming packets addressed to the Web server.\nMore subtle is the situation in which a company\u2019s network consists of\nmultiple local-area networks. Each LAN can be protected by means of a\npacket-\ufb01ltering gateway, which is con\ufb01gured to pass incoming traf\ufb01c only if it\noriginated from a host on one of the other LANs. In this way, a private virtual\nnetwork can be set up.\nThe other type of \ufb01rewall is an application-level gateway . In contrast to a\npacket-\ufb01ltering gateway, which inspects only the header of network packets,\nthis type of \ufb01rewall actually inspects the content of an incoming or outgoing\nmessage. A typical example is a mail gateway that discards incoming or\noutgoing mail exceeding a certain size. More sophisticated mail gateways\nexist that are, for example, capable of \ufb01ltering spam e-mail.\nAnother example of an application-level gateway is one that allows external\naccess to a digital library server, but will supply only abstracts of documents.\nIf an external user wants more, an electronic payment protocol is started.\nUsers inside the \ufb01rewall have direct access to the library service.\nA special kind of application-level gateway is what is known as a proxy\ngateway . This type of \ufb01rewall works as a front end to a speci\ufb01c kind of\napplication, and ensures that only those messages are passed that meet certain\ncriteria. Consider, for example, sur\ufb01ng the Web. Many Web pages contain\nscripts or applets that are to be executed in a user\u2019s browser. To prevent such\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 535\ncode to be downloaded to the inside LAN, all Web traf\ufb01c could be directed\nthrough a Web proxy gateway. This gateway accepts regular HTTP requests,\neither from inside or outside the \ufb01rewall. In other words, it appears to its\nusers as a normal Web server. However, it \ufb01lters all incoming and outgoing\ntraf\ufb01c, either by discarding certain requests and pages, or modifying pages\nwhen they contain executable code.\nSecure mobile code\nAn important issue in modern distributed systems is the ability to migrate\ncode between hosts instead of just migrating passive data. However, mobile\ncode introduces a number of serious security threats. For one thing, hosts need\nto be protected against malicious agents or downloaded programs. The latter\nis becoming increasingly important in view of the popularity of smartphones.\nMost users of distributed systems will not be experts in systems technology\nand have no way of telling whether the program they are fetching from\nanother host can be trusted not to corrupt their device. In many cases it\nmay be dif\ufb01cult even for an expert to detect that a program is actually being\ndownloaded at all.\nUnless security measures are taken, once a malicious program has settled\nitself in a computer, it can easily corrupt its host. We are faced with an access\ncontrol problem: the program should not be allowed unauthorized access to\nthe host\u2019s resources. As we shall see, protecting a host against downloaded\nmalicious programs is not always easy. The problem is not so much as to avoid\ndownloading of programs. Instead, what we are looking for is supporting\nmobile code that we can allow access to local resources in a \ufb02exible, yet fully\ncontrolled manner.\nOne approach to protection against potentially malicious code, is to con-\nstruct a sandbox. A sandbox is a technique by which a downloaded program\nis executed in such a way that each of its instructions can be fully controlled.\nIf an attempt is made to execute an instruction that has been forbidden by\nthe host, execution of the program will be stopped. Likewise, execution is\nhalted when an instruction accesses certain registers or areas in memory that\nthe host has not allowed.\nOne approach toward implementing a sandbox is to check the executable\ncode when it is downloaded and to insert additional instructions for situations\nthat can be checked only at runtime [Wahbe et al., 1993]. Matters become\nmuch simpler when dealing with interpreted code. Let us brie\ufb02y consider the\napproach taken in Java (see also Oaks [2001]). Each Java program consists of a\nnumber of classes from which objects are created. There are no global variables\nand functions; everything has to be declared as part of a class. Program\nexecution starts at a method called main . A Java program is compiled to a set\nof instructions that are interpreted by what is called the Java Virtual Machine\n(JVM ). For a client to download and execute a compiled Java program, it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 535\ncode to be downloaded to the inside LAN, all Web traf\ufb01c could be directed\nthrough a Web proxy gateway. This gateway accepts regular HTTP requests,\neither from inside or outside the \ufb01rewall. In other words, it appears to its\nusers as a normal Web server. However, it \ufb01lters all incoming and outgoing\ntraf\ufb01c, either by discarding certain requests and pages, or modifying pages\nwhen they contain executable code.\nSecure mobile code\nAn important issue in modern distributed systems is the ability to migrate\ncode between hosts instead of just migrating passive data. However, mobile\ncode introduces a number of serious security threats. For one thing, hosts need\nto be protected against malicious agents or downloaded programs. The latter\nis becoming increasingly important in view of the popularity of smartphones.\nMost users of distributed systems will not be experts in systems technology\nand have no way of telling whether the program they are fetching from\nanother host can be trusted not to corrupt their device. In many cases it\nmay be dif\ufb01cult even for an expert to detect that a program is actually being\ndownloaded at all.\nUnless security measures are taken, once a malicious program has settled\nitself in a computer, it can easily corrupt its host. We are faced with an access\ncontrol problem: the program should not be allowed unauthorized access to\nthe host\u2019s resources. As we shall see, protecting a host against downloaded\nmalicious programs is not always easy. The problem is not so much as to avoid\ndownloading of programs. Instead, what we are looking for is supporting\nmobile code that we can allow access to local resources in a \ufb02exible, yet fully\ncontrolled manner.\nOne approach to protection against potentially malicious code, is to con-\nstruct a sandbox. A sandbox is a technique by which a downloaded program\nis executed in such a way that each of its instructions can be fully controlled.\nIf an attempt is made to execute an instruction that has been forbidden by\nthe host, execution of the program will be stopped. Likewise, execution is\nhalted when an instruction accesses certain registers or areas in memory that\nthe host has not allowed.\nOne approach toward implementing a sandbox is to check the executable\ncode when it is downloaded and to insert additional instructions for situations\nthat can be checked only at runtime [Wahbe et al., 1993]. Matters become\nmuch simpler when dealing with interpreted code. Let us brie\ufb02y consider the\napproach taken in Java (see also Oaks [2001]). Each Java program consists of a\nnumber of classes from which objects are created. There are no global variables\nand functions; everything has to be declared as part of a class. Program\nexecution starts at a method called main . A Java program is compiled to a set\nof instructions that are interpreted by what is called the Java Virtual Machine\n(JVM ). For a client to download and execute a compiled Java program, it\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "536 CHAPTER 9. SECURITY\nis therefore necessary that the client process is running the JVM. The JVM\nwill subsequently handle the actual execution of the downloaded program by\ninterpreting each of its instructions, starting at the instructions that comprise\nmain . Note that this model corresponds to a process virtual machine , as\ndiscussed in Section 3.2\nIn a Java sandbox, protection starts by ensuring that the component that\nhandles the transfer of a program to the client machine can be trusted. Down-\nloading in Java is taken care of by a set of class loaders . Each class loader is\nresponsible for fetching a speci\ufb01ed class from a server and installing it in the\nclient\u2019s address space so that the JVM can create objects from it. Class loaders\nare derived from existing, trusted loaders that will automatically enforce\nspeci\ufb01c policies associated with the sandbox.\nThe second component of a Java sandbox consists of a byte code veri\ufb01er ,\nwhich checks whether a downloaded class obeys the security rules of the\nsandbox. In particular, the veri\ufb01er checks that the class contains no illegal\ninstructions or instructions that could somehow corrupt the stack or memory.\nNot all classes are checked. Figure 9.23 shows that only the ones that are\ndownloaded from an external server to the client are veri\ufb01ed. In this case,\nclasses that are located on the client\u2019s machine are trusted. Whether or not\nlocal classes are veri\ufb01ed is another policy that can be set.\nFigure 9.23: The organization of a Java sandbox.\nFinally, when a class has been securely downloaded and veri\ufb01ed, the\nJVM can instantiate objects from it and execute those object\u2019s methods. To\nactually prevent objects from unauthorized access to the client\u2019s resources, a\nsecurity manager is used to perform various checks at runtime. Java programs\nintended to be downloaded are forced to make use of the security manager;\nthere is no way they can circumvent it. The security manager thus plays the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n536 CHAPTER 9. SECURITY\nis therefore necessary that the client process is running the JVM. The JVM\nwill subsequently handle the actual execution of the downloaded program by\ninterpreting each of its instructions, starting at the instructions that comprise\nmain . Note that this model corresponds to a process virtual machine , as\ndiscussed in Section 3.2\nIn a Java sandbox, protection starts by ensuring that the component that\nhandles the transfer of a program to the client machine can be trusted. Down-\nloading in Java is taken care of by a set of class loaders . Each class loader is\nresponsible for fetching a speci\ufb01ed class from a server and installing it in the\nclient\u2019s address space so that the JVM can create objects from it. Class loaders\nare derived from existing, trusted loaders that will automatically enforce\nspeci\ufb01c policies associated with the sandbox.\nThe second component of a Java sandbox consists of a byte code veri\ufb01er ,\nwhich checks whether a downloaded class obeys the security rules of the\nsandbox. In particular, the veri\ufb01er checks that the class contains no illegal\ninstructions or instructions that could somehow corrupt the stack or memory.\nNot all classes are checked. Figure 9.23 shows that only the ones that are\ndownloaded from an external server to the client are veri\ufb01ed. In this case,\nclasses that are located on the client\u2019s machine are trusted. Whether or not\nlocal classes are veri\ufb01ed is another policy that can be set.\nFigure 9.23: The organization of a Java sandbox.\nFinally, when a class has been securely downloaded and veri\ufb01ed, the\nJVM can instantiate objects from it and execute those object\u2019s methods. To\nactually prevent objects from unauthorized access to the client\u2019s resources, a\nsecurity manager is used to perform various checks at runtime. Java programs\nintended to be downloaded are forced to make use of the security manager;\nthere is no way they can circumvent it. The security manager thus plays the\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 537\nrole of a reference monitor we discussed earlier. Java is extremely \ufb02exible\nwhen it comes to con\ufb01guring the sandbox: the security manager makes use\nof an extensive set of permissions that can be set by the client. The security\nmanager\u2019s role is to check the permissions of, for example, \ufb01le operations,\nnetwork operations, class-loading operations, runtime inspections, to name a\nfew.\nA next step toward increased \ufb02exibility is to require that each downloaded\nprogram can be authenticated, and to subsequently enforce a speci\ufb01c security\npolicy based on where the program came from. Demanding that programs\ncan be authenticated is relatively easy: mobile code can be signed, just like any\nother document. This code-signing approach is often applied as an alternative\nto sandboxing as well. In effect, only code from trusted servers is accepted.\nNote 9.7 (Advanced: Enforcing a security policy)\nThe dif\ufb01cult part is enforcing a security policy. Wallach et al. [1997] propose three\nmechanisms in the case of Java programs. The \ufb01rst approach is based on the\nuse of object references as capabilities. To access a local resource such as a \ufb01le, a\nprogram must have been given a reference to a speci\ufb01c object that handles \ufb01le\noperations when it was downloaded. If no such reference is given, there is no way\nthat \ufb01les can be accessed. This principle is shown in Figure 9.24.\nFigure 9.24: The principle of using Java object references as capabilities.\nAll interfaces to objects that implement the \ufb01le system are initially hidden\nfrom the program by simply not handing out any references to these interfaces.\nJava\u2019s strong type checking ensures that it is impossible to construct a reference\nto one of these interfaces at runtime. In addition, we can use the property of Java\nto keep certain variables and methods completely internal to a class. In particular,\na program can be prevented from instantiating its own \ufb01le-handling objects, by\nessentially hiding the operation that creates new objects from a given class. (In\nJava terminology, a constructor is made private to its associated class.)\nThe second mechanism for enforcing a security policy is (extended) stack\nintrospection . In essence, any call to a method mof a local resource is preceded\nby a call to a special procedure enable_privilege that checks whether the caller is\nauthorized to invoke mon that resource. If the invocation is authorized, the caller\nis given temporary privileges for the duration of the call. Before returning control\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 537\nrole of a reference monitor we discussed earlier. Java is extremely \ufb02exible\nwhen it comes to con\ufb01guring the sandbox: the security manager makes use\nof an extensive set of permissions that can be set by the client. The security\nmanager\u2019s role is to check the permissions of, for example, \ufb01le operations,\nnetwork operations, class-loading operations, runtime inspections, to name a\nfew.\nA next step toward increased \ufb02exibility is to require that each downloaded\nprogram can be authenticated, and to subsequently enforce a speci\ufb01c security\npolicy based on where the program came from. Demanding that programs\ncan be authenticated is relatively easy: mobile code can be signed, just like any\nother document. This code-signing approach is often applied as an alternative\nto sandboxing as well. In effect, only code from trusted servers is accepted.\nNote 9.7 (Advanced: Enforcing a security policy)\nThe dif\ufb01cult part is enforcing a security policy. Wallach et al. [1997] propose three\nmechanisms in the case of Java programs. The \ufb01rst approach is based on the\nuse of object references as capabilities. To access a local resource such as a \ufb01le, a\nprogram must have been given a reference to a speci\ufb01c object that handles \ufb01le\noperations when it was downloaded. If no such reference is given, there is no way\nthat \ufb01les can be accessed. This principle is shown in Figure 9.24.\nFigure 9.24: The principle of using Java object references as capabilities.\nAll interfaces to objects that implement the \ufb01le system are initially hidden\nfrom the program by simply not handing out any references to these interfaces.\nJava\u2019s strong type checking ensures that it is impossible to construct a reference\nto one of these interfaces at runtime. In addition, we can use the property of Java\nto keep certain variables and methods completely internal to a class. In particular,\na program can be prevented from instantiating its own \ufb01le-handling objects, by\nessentially hiding the operation that creates new objects from a given class. (In\nJava terminology, a constructor is made private to its associated class.)\nThe second mechanism for enforcing a security policy is (extended) stack\nintrospection . In essence, any call to a method mof a local resource is preceded\nby a call to a special procedure enable_privilege that checks whether the caller is\nauthorized to invoke mon that resource. If the invocation is authorized, the caller\nis given temporary privileges for the duration of the call. Before returning control\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "538 CHAPTER 9. SECURITY\nto the invoker when mis \ufb01nished, the special procedure disable_privilege is\ninvoked to disable these privileges.\nTo enforce calls to enable_privilege and disable_privilege , a developer of\ninterfaces to local resources could be required to insert these calls in the appro-\npriate places. However, it is much better to let the Java interpreter handle the\ncalls automatically. This is the standard approach followed in, for example, Web\nbrowsers for dealing with Java applets. An elegant solution is as follows. When-\never an invocation to a local resource is made, the Java interpreter automatically\ncalls enable_privilege , which subsequently checks whether the call is permitted.\nIf so, a call to disable_privilege is pushed on the stack to ensure that privileges\nare disabled when the method call returns. This approach prevents malicious\nprogrammers from circumventing the rules.\nFigure 9.25: The principle of stack introspection.\nAnother important advantage of using the stack is that it enables a much better\nway of checking privileges. Suppose that a program invokes a local object O1,\nwhich, in turn, invokes object O2. Although O1may have permission to invoke\nO2, if the invoker of O1is not trusted to invoke a speci\ufb01c method belonging to O2,\nthen this chain of invocations should not be allowed. Stack introspection makes it\neasy to check such chains, as the interpreter need merely inspect each stack frame\nstarting at the top to see if there is a frame having the right privileges enabled\n(in which case the call is permitted), or if there is a frame that explicitly forbids\naccess to the current resource (in which case the call is immediately terminated).\nThis approach is shown in Figure 9.25.\nIn essence, stack introspection allows for the attachment of privileges to classes\nor methods, and the checking of those privileges for each caller separately. In this\nway, it is possible to implement class-based protection domains, as is explained in\ndetail in [Gong and Schemers, 1998].\nThe third approach to enforcing a security policy is by means of name space\nmanagement . The idea is put forth below. To give programs access to local\nresources, they \ufb01rst need to attain access by including the appropriate \ufb01les that\ncontain the classes implementing those resources. Inclusion requires that a name\nis given to the interpreter, which then resolves it to a class, which is subsequently\nloaded at runtime. To enforce a security policy for a speci\ufb01c downloaded program,\nthe same name can be resolved to different classes, depending on where the\ndownloaded program came from. Typically, name resolution is handled by class\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n538 CHAPTER 9. SECURITY\nto the invoker when mis \ufb01nished, the special procedure disable_privilege is\ninvoked to disable these privileges.\nTo enforce calls to enable_privilege and disable_privilege , a developer of\ninterfaces to local resources could be required to insert these calls in the appro-\npriate places. However, it is much better to let the Java interpreter handle the\ncalls automatically. This is the standard approach followed in, for example, Web\nbrowsers for dealing with Java applets. An elegant solution is as follows. When-\never an invocation to a local resource is made, the Java interpreter automatically\ncalls enable_privilege , which subsequently checks whether the call is permitted.\nIf so, a call to disable_privilege is pushed on the stack to ensure that privileges\nare disabled when the method call returns. This approach prevents malicious\nprogrammers from circumventing the rules.\nFigure 9.25: The principle of stack introspection.\nAnother important advantage of using the stack is that it enables a much better\nway of checking privileges. Suppose that a program invokes a local object O1,\nwhich, in turn, invokes object O2. Although O1may have permission to invoke\nO2, if the invoker of O1is not trusted to invoke a speci\ufb01c method belonging to O2,\nthen this chain of invocations should not be allowed. Stack introspection makes it\neasy to check such chains, as the interpreter need merely inspect each stack frame\nstarting at the top to see if there is a frame having the right privileges enabled\n(in which case the call is permitted), or if there is a frame that explicitly forbids\naccess to the current resource (in which case the call is immediately terminated).\nThis approach is shown in Figure 9.25.\nIn essence, stack introspection allows for the attachment of privileges to classes\nor methods, and the checking of those privileges for each caller separately. In this\nway, it is possible to implement class-based protection domains, as is explained in\ndetail in [Gong and Schemers, 1998].\nThe third approach to enforcing a security policy is by means of name space\nmanagement . The idea is put forth below. To give programs access to local\nresources, they \ufb01rst need to attain access by including the appropriate \ufb01les that\ncontain the classes implementing those resources. Inclusion requires that a name\nis given to the interpreter, which then resolves it to a class, which is subsequently\nloaded at runtime. To enforce a security policy for a speci\ufb01c downloaded program,\nthe same name can be resolved to different classes, depending on where the\ndownloaded program came from. Typically, name resolution is handled by class\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.3. ACCESS CONTROL 539\nloaders, which need to be adapted to implement this approach. Details of how\nthis can be done can be found in [Wallach et al., 1997].\nThe approach described so far associates privileges with classes or methods\nbased on where a downloaded program came from. By virtue of the Java inter-\npreter, it is possible to enforce security policies through the mechanisms described\nabove. In this sense, the security architecture becomes highly language dependent,\nand will need to be developed anew for other languages. Language-independent\nsolutions, such as, for example, described in [Jaeger et al., 1999], require a more\ngeneral approach to enforcing security, and are also harder to implement. In\nthese cases, support is needed from a secure operating system that is aware of\ndownloaded mobile code and which enforces all calls to local resources to run\nthrough the kernel where subsequent checking is done.\nDenial of service\nAccess control is generally about carefully ensuring that resources are accessed\nonly by authorized processes. A particularly annoying type of attack that\nis related to access control is maliciously preventing authorized processes\nfrom accessing resources. Defenses against such ( denial-of-service (DoS)\nattacks ) have become increasingly important as distributed systems opened\nup through the Internet. Where DoS attacks that come from one or a few\nsources can often be handled quite effectively, matters become much more\ndif\ufb01cult when having to deal with distributed denial of service (DDoS ).\nIn DDoS attacks, a huge collection of processes jointly attempt to bring\ndown a networked service. In these cases, we often see that the attackers\nhave succeeded in hijacking a large group of machines which unknowingly\nparticipate in the attack. Specht and Lee [2004] distinguish attacks aimed at\nbandwidth depletion and those aimed at resource depletion.\nBandwidth depletion can be accomplished by simply sending many mes-\nsages to a single machine. The effect is that normal messages will hardly be\nable to reach the receiver. Resource depletion attacks concentrate on letting\nthe receiver use up resources on otherwise useless messages. A well-known\nresource-depletion attack is TCP SYN-\ufb02ooding. In this case, the attacker\nattempts to initiate a huge amount of connections (i.e., send SYN packets\nas part of the three-way handshake), but will otherwise never respond to\nacknowledgments from the receiver. The effect is that the server will quickly\nrun out of socket descriptors, preventing that any further connections can be\nestablished.\nThere is no single method to protect against DDoS attacks. One problem\nis that attackers make use of innocent victims by secretly installing software\non their machines, effectively creating what are known as botnets [Silva et al.,\n2013]. In these cases, the only solution is to have machines continuously mon-\nitor their state by checking \ufb01les for pollution. Considering the ease by which\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.3. ACCESS CONTROL 539\nloaders, which need to be adapted to implement this approach. Details of how\nthis can be done can be found in [Wallach et al., 1997].\nThe approach described so far associates privileges with classes or methods\nbased on where a downloaded program came from. By virtue of the Java inter-\npreter, it is possible to enforce security policies through the mechanisms described\nabove. In this sense, the security architecture becomes highly language dependent,\nand will need to be developed anew for other languages. Language-independent\nsolutions, such as, for example, described in [Jaeger et al., 1999], require a more\ngeneral approach to enforcing security, and are also harder to implement. In\nthese cases, support is needed from a secure operating system that is aware of\ndownloaded mobile code and which enforces all calls to local resources to run\nthrough the kernel where subsequent checking is done.\nDenial of service\nAccess control is generally about carefully ensuring that resources are accessed\nonly by authorized processes. A particularly annoying type of attack that\nis related to access control is maliciously preventing authorized processes\nfrom accessing resources. Defenses against such ( denial-of-service (DoS)\nattacks ) have become increasingly important as distributed systems opened\nup through the Internet. Where DoS attacks that come from one or a few\nsources can often be handled quite effectively, matters become much more\ndif\ufb01cult when having to deal with distributed denial of service (DDoS ).\nIn DDoS attacks, a huge collection of processes jointly attempt to bring\ndown a networked service. In these cases, we often see that the attackers\nhave succeeded in hijacking a large group of machines which unknowingly\nparticipate in the attack. Specht and Lee [2004] distinguish attacks aimed at\nbandwidth depletion and those aimed at resource depletion.\nBandwidth depletion can be accomplished by simply sending many mes-\nsages to a single machine. The effect is that normal messages will hardly be\nable to reach the receiver. Resource depletion attacks concentrate on letting\nthe receiver use up resources on otherwise useless messages. A well-known\nresource-depletion attack is TCP SYN-\ufb02ooding. In this case, the attacker\nattempts to initiate a huge amount of connections (i.e., send SYN packets\nas part of the three-way handshake), but will otherwise never respond to\nacknowledgments from the receiver. The effect is that the server will quickly\nrun out of socket descriptors, preventing that any further connections can be\nestablished.\nThere is no single method to protect against DDoS attacks. One problem\nis that attackers make use of innocent victims by secretly installing software\non their machines, effectively creating what are known as botnets [Silva et al.,\n2013]. In these cases, the only solution is to have machines continuously mon-\nitor their state by checking \ufb01les for pollution. Considering the ease by which\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "540 CHAPTER 9. SECURITY\na virus can spread over the Internet, relying only on this countermeasure is\nnot feasible.\nMuch better is to continuously monitor network traf\ufb01c, for example, start-\ning at the egress routers where packets leave an organization\u2019s network.\nExperience shows that by dropping packets whose source address does not\nbelong to the organization\u2019s network we can prevent a lot of havoc. In general,\nthe more packets can be \ufb01ltered close to the sources, the better.\nAlternatively, it is also possible to concentrate on ingress routers, that\nis, where traf\ufb01c \ufb02ows into an organization\u2019s network. The problem is that\ndetecting an attack at an ingress router is too late as the network will probably\nalready be unreachable for regular traf\ufb01c. Better is to have routers further in\nthe Internet, such as in the networks of ISPs, start dropping packets when they\nsuspect that an attack is going on. In general, a myriad of techniques need to\nbe deployed, whereas new attacks continue to emerge. A practical overview\nof the state-of-the-art in denial-of-service attacks and solutions is provided by\nZargar et al. [2013], with a strong focus on application-level \ufb02ooding attacks\n(which are increasingly prevalent). An alternative overview concentrating\nmore on network-level solutions is given by Peng et al. [2007].\n9.4 Secure naming\nA topic that has received increasingly more attention is that of secure naming.\nSimply put: when a client retrieves an object based on some name, how\ndoes it know that it got back the correct object? The whole issue is rather\nfundamental: when resolving a name in DNS, how does the client know\nit is returned the correct address? When looking up an object using some\ncombination of a URL and database query, how does the receiver know it is\nreturned what was requested? To be more precise, we have three issues to\nworry about [Smetters and Jacobson, 2009]:\n1.Validity : is the object returned a complete, unaltered copy of what was\nstored at the server?\n2.Provenance : can the server that returned the object be trusted as a\ngenuine supplier? For example, it may be the case that a client is\nreturned a cached version of the original object.\n3.Relevance : is what was returned relevant considering what was asked?\nA partial, and well-known solution to secure naming is to securely bind\nthe name of an object to its content through hashing. Simply put: take hash(O)\nas the name of an object O. This is a form of a self-certifying name and at\nleast allows the client to check for validity. Self-certifying names have been\npioneered in SFS [Fu et al., 2000]. If we can assume that each object Ohas a\nknown, and certi\ufb01ed public key K+\nO, we can also take hash(K+\nO) as its name,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n540 CHAPTER 9. SECURITY\na virus can spread over the Internet, relying only on this countermeasure is\nnot feasible.\nMuch better is to continuously monitor network traf\ufb01c, for example, start-\ning at the egress routers where packets leave an organization\u2019s network.\nExperience shows that by dropping packets whose source address does not\nbelong to the organization\u2019s network we can prevent a lot of havoc. In general,\nthe more packets can be \ufb01ltered close to the sources, the better.\nAlternatively, it is also possible to concentrate on ingress routers, that\nis, where traf\ufb01c \ufb02ows into an organization\u2019s network. The problem is that\ndetecting an attack at an ingress router is too late as the network will probably\nalready be unreachable for regular traf\ufb01c. Better is to have routers further in\nthe Internet, such as in the networks of ISPs, start dropping packets when they\nsuspect that an attack is going on. In general, a myriad of techniques need to\nbe deployed, whereas new attacks continue to emerge. A practical overview\nof the state-of-the-art in denial-of-service attacks and solutions is provided by\nZargar et al. [2013], with a strong focus on application-level \ufb02ooding attacks\n(which are increasingly prevalent). An alternative overview concentrating\nmore on network-level solutions is given by Peng et al. [2007].\n9.4 Secure naming\nA topic that has received increasingly more attention is that of secure naming.\nSimply put: when a client retrieves an object based on some name, how\ndoes it know that it got back the correct object? The whole issue is rather\nfundamental: when resolving a name in DNS, how does the client know\nit is returned the correct address? When looking up an object using some\ncombination of a URL and database query, how does the receiver know it is\nreturned what was requested? To be more precise, we have three issues to\nworry about [Smetters and Jacobson, 2009]:\n1.Validity : is the object returned a complete, unaltered copy of what was\nstored at the server?\n2.Provenance : can the server that returned the object be trusted as a\ngenuine supplier? For example, it may be the case that a client is\nreturned a cached version of the original object.\n3.Relevance : is what was returned relevant considering what was asked?\nA partial, and well-known solution to secure naming is to securely bind\nthe name of an object to its content through hashing. Simply put: take hash(O)\nas the name of an object O. This is a form of a self-certifying name and at\nleast allows the client to check for validity. Self-certifying names have been\npioneered in SFS [Fu et al., 2000]. If we can assume that each object Ohas a\nknown, and certi\ufb01ed public key K+\nO, we can also take hash(K+\nO) as its name,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 541\nhaving the advantage that to a certain extent provenance can be checked. This\napproach has been explored in GlobeDoc [Popescu et al., 2005].\nThe problem with these self-certifying names, is that they act as pure\nidenti\ufb01ers and are, in principle, not very human friendly. Also, if we simply\ntake the hash of an object as its name, then every change of that object will\nlead to a different name. This problem is somewhat remedied by the approach\nin GlobeDoc, but then we need to make sure we are using the correct public\nkey, and also make sure that we are returned the current version of the object.\nIn practice, assuming there is some form of public-key infrastructure as we\nwill discuss later in this chapter, combined with being able to ef\ufb01ciently check\nwith the origin server of the object, should solve the latter problem.\nAs a result, as soon as we want to use human-friendly naming for objects,\nwe \ufb01nd ourselves looking at the problem of securely binding such a name to,\nperhaps, a self-certifying name, which introduces the problem of not being\nable to check for relevance: does the name-resolution process that returns a\nself-certifying but human-unreadable identi\ufb01er, return the identi\ufb01er of the\nobject we were looking for in the \ufb01rst place?\nA way out, and extensively discussed by Ghodsi et al. [2011], is to let the\nname of an object Otake the formhhash(K+\nO),labeli, where label is a human-\nfriendly name that can be used to look up the object. A label can be just a\ntag, but also a globally unique hierarchically organized name, such as a URL.\nWhen an object is retrieved, it will be signed by the server as discussed before.\nThe receiver takes the object\u2019s public key, veri\ufb01es that this is the one that was\nused in the name, and subsequently checks if the object is genuine. Obviously,\nthe label should be part of the object\u2019s content, otherwise it will be impossible\nto verify that the label also belonged to the object. For example, the origin\nserver may separately need to sign the label with the same key as used for the\nobject itself.\n9.5 Security management\nWe now take a closer look at security management. First, we need to consider\nthe general management of cryptographic keys, and especially the means by\nwhich public keys are distributed. As it turns out, certi\ufb01cates play an impor-\ntant role here. Second, we discuss the problem of securely managing a group\nof servers by concentrating on the problem of adding a new group member\nthat is trusted by the current members. Clearly, in the face of distributed\nand replicated services, it is important that security is not compromised by\nadmitting a malicious process to a group. Third, we pay attention to au-\nthorization management by looking at capabilities and what are known as\nattribute certi\ufb01cates. An important issue in distributed systems with respect\nto authorization management is that one process can delegate some or all of\nits access rights to another process. Delegating rights in a secure way has its\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 541\nhaving the advantage that to a certain extent provenance can be checked. This\napproach has been explored in GlobeDoc [Popescu et al., 2005].\nThe problem with these self-certifying names, is that they act as pure\nidenti\ufb01ers and are, in principle, not very human friendly. Also, if we simply\ntake the hash of an object as its name, then every change of that object will\nlead to a different name. This problem is somewhat remedied by the approach\nin GlobeDoc, but then we need to make sure we are using the correct public\nkey, and also make sure that we are returned the current version of the object.\nIn practice, assuming there is some form of public-key infrastructure as we\nwill discuss later in this chapter, combined with being able to ef\ufb01ciently check\nwith the origin server of the object, should solve the latter problem.\nAs a result, as soon as we want to use human-friendly naming for objects,\nwe \ufb01nd ourselves looking at the problem of securely binding such a name to,\nperhaps, a self-certifying name, which introduces the problem of not being\nable to check for relevance: does the name-resolution process that returns a\nself-certifying but human-unreadable identi\ufb01er, return the identi\ufb01er of the\nobject we were looking for in the \ufb01rst place?\nA way out, and extensively discussed by Ghodsi et al. [2011], is to let the\nname of an object Otake the formhhash(K+\nO),labeli, where label is a human-\nfriendly name that can be used to look up the object. A label can be just a\ntag, but also a globally unique hierarchically organized name, such as a URL.\nWhen an object is retrieved, it will be signed by the server as discussed before.\nThe receiver takes the object\u2019s public key, veri\ufb01es that this is the one that was\nused in the name, and subsequently checks if the object is genuine. Obviously,\nthe label should be part of the object\u2019s content, otherwise it will be impossible\nto verify that the label also belonged to the object. For example, the origin\nserver may separately need to sign the label with the same key as used for the\nobject itself.\n9.5 Security management\nWe now take a closer look at security management. First, we need to consider\nthe general management of cryptographic keys, and especially the means by\nwhich public keys are distributed. As it turns out, certi\ufb01cates play an impor-\ntant role here. Second, we discuss the problem of securely managing a group\nof servers by concentrating on the problem of adding a new group member\nthat is trusted by the current members. Clearly, in the face of distributed\nand replicated services, it is important that security is not compromised by\nadmitting a malicious process to a group. Third, we pay attention to au-\nthorization management by looking at capabilities and what are known as\nattribute certi\ufb01cates. An important issue in distributed systems with respect\nto authorization management is that one process can delegate some or all of\nits access rights to another process. Delegating rights in a secure way has its\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "542 CHAPTER 9. SECURITY\nown subtleties as we also discuss in this section.\nKey management\nSo far, we have described various cryptographic protocols in which we (im-\nplicitly) assumed that various keys were readily available. For example, in\nthe case of public-key cryptosystems, we assumed that a sender of a message\nhad the public key of the receiver at its disposal so that it could encrypt the\nmessage to ensure con\ufb01dentiality. Likewise, in the case of authentication\nusing a key distribution center (KDC), we assumed each party already shared\na secret key with the KDC.\nHowever, establishing and distributing keys is not a trivial matter. For\nexample, distributing secret keys by means of an unsecured channel is out\nof the question and in many cases we need to resort to out-of-band methods.\nAlso, mechanisms are needed to revoke keys, that is, prevent a key from being\nused after it has been compromised or invalidated.\nKey establishment\nLet us start with considering how session keys can be established. When Alice\nwants to set up a secure channel with Bob, she may \ufb01rst use Bob\u2019s public\nkey to initiate communication as shown in Figure 9.13. If Bob accepts, he\ncan subsequently generate the session key and return it to Alice, encrypted\nwith Alice\u2019s public key. By encrypting the shared session key before its\ntransmission, it can be safely passed across the network.\nA similar scheme can be used to generate and distribute a session key\nwhen Alice and Bob already share a secret key. However, both methods\nrequire that the communicating parties already have the means available to\nestablish a secure channel. In other words, some form of key establishment\nand distribution must already have taken place. The same argument applies\nwhen a shared secret key is established by means of a trusted third party, such\nas a KDC.\nAn elegant and widely-applied scheme for establishing a shared key across\nan insecure channel is the Dif\ufb01e-Hellman key exchange [Dif\ufb01e and Hellman,\n1976]. The protocol works as follows. Suppose that Alice and Bob want to\nestablish a shared secret key. The \ufb01rst requirement is that they agree on\ntwo large numbers, nand gthat are subject to a number of mathematical\nproperties (which we do not discuss here). Both nand gcan be made public;\nthere is no need to hide them from outsiders. Alice picks a large random\nnumber, say x, which she keeps secret. Likewise, Bob picks his own secret\nlarge number, say y. At this point there is enough information to construct a\nsecret key, as shown in Figure 9.26.\nAlice starts by sending gxmod nto Bob, along with nand g. It is impor-\ntant to note that this information can be sent as plaintext, as it is virtually\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n542 CHAPTER 9. SECURITY\nown subtleties as we also discuss in this section.\nKey management\nSo far, we have described various cryptographic protocols in which we (im-\nplicitly) assumed that various keys were readily available. For example, in\nthe case of public-key cryptosystems, we assumed that a sender of a message\nhad the public key of the receiver at its disposal so that it could encrypt the\nmessage to ensure con\ufb01dentiality. Likewise, in the case of authentication\nusing a key distribution center (KDC), we assumed each party already shared\na secret key with the KDC.\nHowever, establishing and distributing keys is not a trivial matter. For\nexample, distributing secret keys by means of an unsecured channel is out\nof the question and in many cases we need to resort to out-of-band methods.\nAlso, mechanisms are needed to revoke keys, that is, prevent a key from being\nused after it has been compromised or invalidated.\nKey establishment\nLet us start with considering how session keys can be established. When Alice\nwants to set up a secure channel with Bob, she may \ufb01rst use Bob\u2019s public\nkey to initiate communication as shown in Figure 9.13. If Bob accepts, he\ncan subsequently generate the session key and return it to Alice, encrypted\nwith Alice\u2019s public key. By encrypting the shared session key before its\ntransmission, it can be safely passed across the network.\nA similar scheme can be used to generate and distribute a session key\nwhen Alice and Bob already share a secret key. However, both methods\nrequire that the communicating parties already have the means available to\nestablish a secure channel. In other words, some form of key establishment\nand distribution must already have taken place. The same argument applies\nwhen a shared secret key is established by means of a trusted third party, such\nas a KDC.\nAn elegant and widely-applied scheme for establishing a shared key across\nan insecure channel is the Dif\ufb01e-Hellman key exchange [Dif\ufb01e and Hellman,\n1976]. The protocol works as follows. Suppose that Alice and Bob want to\nestablish a shared secret key. The \ufb01rst requirement is that they agree on\ntwo large numbers, nand gthat are subject to a number of mathematical\nproperties (which we do not discuss here). Both nand gcan be made public;\nthere is no need to hide them from outsiders. Alice picks a large random\nnumber, say x, which she keeps secret. Likewise, Bob picks his own secret\nlarge number, say y. At this point there is enough information to construct a\nsecret key, as shown in Figure 9.26.\nAlice starts by sending gxmod nto Bob, along with nand g. It is impor-\ntant to note that this information can be sent as plaintext, as it is virtually\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 543\nFigure 9.26: The principle of Dif\ufb01e-Hellman key exchange.\nimpossible to compute xgiven gxmod n. When Bob receives the message,\nhe subsequently calculates (gxmod n)ywhich is mathematically equal to\ngxymod n. In addition, he sends gymod nto Alice, who can then compute\n(gymod n)x=gxymod n. Consequently, both Alice and Bob, and only those\ntwo, will now have the shared secret key gxymod n. Note that neither of them\nneeded to make their private number ( xand y, respectively), known to the\nother.\nDif\ufb01e-Hellman can be viewed as a public-key cryptosystem. In the case\nof Alice, xis her private key, whereas gxmod nis her public key. Securely\ndistributing the public key is essential to making Dif\ufb01e-Hellman work.\nKey distribution\nOne of the more dif\ufb01cult parts in key management is the actual distribution\nof initial keys. In a symmetric cryptosystem, the initial shared secret key must\nbe communicated along a secure channel that provides authentication as well\nas con\ufb01dentiality, as shown in Figure 9.27. If there are no keys available to\nAlice and Bob to set up such a secure channel, it is necessary to distribute the\nkey out-of-band. In other words, Alice and Bob will have to get in touch with\neach other using some other communication channel.\nIn the case of a public-key cryptosystem, we need to distribute the public\nkey in such a way that the receivers can be sure that the key is indeed paired\nto a claimed private key. In other words, as shown in Figure 9.27, although\nthe public key itself may be sent as plaintext, it is necessary that the channel\nthrough which it is sent can provide authentication. The private key, of course,\nneeds to be sent across a secure channel providing authentication as well as\ncon\ufb01dentiality.\nWhen it comes to key distribution, the authenticated distribution of public\nkeys is perhaps the most interesting. In practice, public-key distribution takes\nplace by means of public-key certi\ufb01cates . Such a certi\ufb01cate consists of a public\nkey together with a string identifying the entity to which that key is associated.\nThe entity could be a user, but also a host or some special device. The public\nkey and identi\ufb01er have together been signed by a certi\ufb01cation authority , and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 543\nFigure 9.26: The principle of Dif\ufb01e-Hellman key exchange.\nimpossible to compute xgiven gxmod n. When Bob receives the message,\nhe subsequently calculates (gxmod n)ywhich is mathematically equal to\ngxymod n. In addition, he sends gymod nto Alice, who can then compute\n(gymod n)x=gxymod n. Consequently, both Alice and Bob, and only those\ntwo, will now have the shared secret key gxymod n. Note that neither of them\nneeded to make their private number ( xand y, respectively), known to the\nother.\nDif\ufb01e-Hellman can be viewed as a public-key cryptosystem. In the case\nof Alice, xis her private key, whereas gxmod nis her public key. Securely\ndistributing the public key is essential to making Dif\ufb01e-Hellman work.\nKey distribution\nOne of the more dif\ufb01cult parts in key management is the actual distribution\nof initial keys. In a symmetric cryptosystem, the initial shared secret key must\nbe communicated along a secure channel that provides authentication as well\nas con\ufb01dentiality, as shown in Figure 9.27. If there are no keys available to\nAlice and Bob to set up such a secure channel, it is necessary to distribute the\nkey out-of-band. In other words, Alice and Bob will have to get in touch with\neach other using some other communication channel.\nIn the case of a public-key cryptosystem, we need to distribute the public\nkey in such a way that the receivers can be sure that the key is indeed paired\nto a claimed private key. In other words, as shown in Figure 9.27, although\nthe public key itself may be sent as plaintext, it is necessary that the channel\nthrough which it is sent can provide authentication. The private key, of course,\nneeds to be sent across a secure channel providing authentication as well as\ncon\ufb01dentiality.\nWhen it comes to key distribution, the authenticated distribution of public\nkeys is perhaps the most interesting. In practice, public-key distribution takes\nplace by means of public-key certi\ufb01cates . Such a certi\ufb01cate consists of a public\nkey together with a string identifying the entity to which that key is associated.\nThe entity could be a user, but also a host or some special device. The public\nkey and identi\ufb01er have together been signed by a certi\ufb01cation authority , and\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "544 CHAPTER 9. SECURITY\n(a)\n(b)\nFigure 9.27: (a) Secret-key distribution. (b) Public-key distribution (see\nalso [Menezes et al., 1996]).\nthis signature has been placed on the certi\ufb01cate as well. (The identity of the\ncerti\ufb01cation authority is naturally part of the certi\ufb01cate.) Signing takes place\nby means of a private key K\u0000\nCAthat belongs to the certi\ufb01cation authority. The\ncorresponding public key K+\nCAis assumed to be well known. For example,\nthe public keys of various certi\ufb01cation authorities are built into most Web\nbrowsers and shipped with the binaries.\nUsing a public-key certi\ufb01cate works as follows. Assume that a client wishes\nto ascertain that the public key found in the certi\ufb01cate indeed belongs to the\nidenti\ufb01ed entity. It then uses the public key of the associated certi\ufb01cation\nauthority to verify the certi\ufb01cate\u2019s signature. If the signature on the certi\ufb01cate\nmatches the (public key, identi\ufb01er) , the client accepts that the public key indeed\nbelongs to the identi\ufb01ed entity.\nIt is important to note that by accepting the certi\ufb01cate as being in order, the\nclient actually trusts that the certi\ufb01cate has not been forged. In particular, the\nclient must assume that the public key K+\nCAindeed belongs to the associated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n544 CHAPTER 9. SECURITY\n(a)\n(b)\nFigure 9.27: (a) Secret-key distribution. (b) Public-key distribution (see\nalso [Menezes et al., 1996]).\nthis signature has been placed on the certi\ufb01cate as well. (The identity of the\ncerti\ufb01cation authority is naturally part of the certi\ufb01cate.) Signing takes place\nby means of a private key K\u0000\nCAthat belongs to the certi\ufb01cation authority. The\ncorresponding public key K+\nCAis assumed to be well known. For example,\nthe public keys of various certi\ufb01cation authorities are built into most Web\nbrowsers and shipped with the binaries.\nUsing a public-key certi\ufb01cate works as follows. Assume that a client wishes\nto ascertain that the public key found in the certi\ufb01cate indeed belongs to the\nidenti\ufb01ed entity. It then uses the public key of the associated certi\ufb01cation\nauthority to verify the certi\ufb01cate\u2019s signature. If the signature on the certi\ufb01cate\nmatches the (public key, identi\ufb01er) , the client accepts that the public key indeed\nbelongs to the identi\ufb01ed entity.\nIt is important to note that by accepting the certi\ufb01cate as being in order, the\nclient actually trusts that the certi\ufb01cate has not been forged. In particular, the\nclient must assume that the public key K+\nCAindeed belongs to the associated\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 545\ncerti\ufb01cation authority. If in doubt, it should be possible to verify the validity\nofK+\nCAthrough another certi\ufb01cate coming from a different, possibly more\ntrusted certi\ufb01cation authority.\nNote 9.8 (More information: Lifetime of certi\ufb01cates)\nAn important issue concerning certi\ufb01cates is their longevity. First let us consider\nthe situation in which a certi\ufb01cation authority hands out lifelong certi\ufb01cates.\nEssentially, what the certi\ufb01cate then states is that the public key will always be\nvalid for the entity identi\ufb01ed by the certi\ufb01cate. Clearly, such a statement is not\nwhat we want. If the private key of the identi\ufb01ed entity is ever compromised, no\nunsuspecting client should ever be able to use the public key (let alone malicious\nclients). In that case, we need a mechanism to revoke the certi\ufb01cate by making it\npublicly known that the certi\ufb01cate is no longer valid.\nThere are several ways to revoke a certi\ufb01cate. One common approach is\nwith a Certi\ufb01cate Revocation List (CRL ) published regularly by the certi\ufb01cation\nauthority. Whenever a client checks a certi\ufb01cate, it will have to check the CRL to\nsee whether the certi\ufb01cate has been revoked or not. This means that the client\nwill at least have to contact the certi\ufb01cation authority each time a new CRL is\npublished. Note that if a CRL is published daily, it also takes a day to revoke\na certi\ufb01cate. Meanwhile, a compromised certi\ufb01cate can be falsely used until it\nis published on the next CRL. Consequently, the time between publishing CRLs\ncannot be too long.\nAn alternative approach is to restrict the lifetime of each certi\ufb01cate. In essence,\nthis approach is analogous to handing out leases as we discussed in Section 7.4.\nThe validity of a certi\ufb01cate automatically expires after some time. If for whatever\nreason the certi\ufb01cate should be revoked before it expires, the certi\ufb01cation authority\ncan still publish it on a CRL. However, this approach will still force clients to\ncheck the latest CRL whenever verifying a certi\ufb01cate.\nA \ufb01nal extreme case is to reduce the lifetime of a certi\ufb01cate to nearly zero. In\neffect, this means that certi\ufb01cates are no longer used; instead, a client will always\nhave to contact the certi\ufb01cation authority to check the validity of a public key. As\na consequence, the certi\ufb01cation authority must be continuously online.\nIn practice, certi\ufb01cates are handed out with restricted lifetimes. In the case\nof Internet applications, the expiration time is often as much as a year [Stein,\n1998]. Such an approach requires that CRLs are published regularly, but that they\nare also inspected when certi\ufb01cates are checked. Practice indicates that client\napplications hardly ever consult CRLs and simply assume a certi\ufb01cate to be valid\nuntil it expires.\nSecure group management\nMany security systems make use of special services such as Key Distribution\nCenters (KDCs) or Certi\ufb01cation Authorities (CAs). These services demonstrate\na dif\ufb01cult problem in distributed systems. In the \ufb01rst place, they must be\ntrusted. To enhance the trust in security services, it is necessary to provide a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 545\ncerti\ufb01cation authority. If in doubt, it should be possible to verify the validity\nofK+\nCAthrough another certi\ufb01cate coming from a different, possibly more\ntrusted certi\ufb01cation authority.\nNote 9.8 (More information: Lifetime of certi\ufb01cates)\nAn important issue concerning certi\ufb01cates is their longevity. First let us consider\nthe situation in which a certi\ufb01cation authority hands out lifelong certi\ufb01cates.\nEssentially, what the certi\ufb01cate then states is that the public key will always be\nvalid for the entity identi\ufb01ed by the certi\ufb01cate. Clearly, such a statement is not\nwhat we want. If the private key of the identi\ufb01ed entity is ever compromised, no\nunsuspecting client should ever be able to use the public key (let alone malicious\nclients). In that case, we need a mechanism to revoke the certi\ufb01cate by making it\npublicly known that the certi\ufb01cate is no longer valid.\nThere are several ways to revoke a certi\ufb01cate. One common approach is\nwith a Certi\ufb01cate Revocation List (CRL ) published regularly by the certi\ufb01cation\nauthority. Whenever a client checks a certi\ufb01cate, it will have to check the CRL to\nsee whether the certi\ufb01cate has been revoked or not. This means that the client\nwill at least have to contact the certi\ufb01cation authority each time a new CRL is\npublished. Note that if a CRL is published daily, it also takes a day to revoke\na certi\ufb01cate. Meanwhile, a compromised certi\ufb01cate can be falsely used until it\nis published on the next CRL. Consequently, the time between publishing CRLs\ncannot be too long.\nAn alternative approach is to restrict the lifetime of each certi\ufb01cate. In essence,\nthis approach is analogous to handing out leases as we discussed in Section 7.4.\nThe validity of a certi\ufb01cate automatically expires after some time. If for whatever\nreason the certi\ufb01cate should be revoked before it expires, the certi\ufb01cation authority\ncan still publish it on a CRL. However, this approach will still force clients to\ncheck the latest CRL whenever verifying a certi\ufb01cate.\nA \ufb01nal extreme case is to reduce the lifetime of a certi\ufb01cate to nearly zero. In\neffect, this means that certi\ufb01cates are no longer used; instead, a client will always\nhave to contact the certi\ufb01cation authority to check the validity of a public key. As\na consequence, the certi\ufb01cation authority must be continuously online.\nIn practice, certi\ufb01cates are handed out with restricted lifetimes. In the case\nof Internet applications, the expiration time is often as much as a year [Stein,\n1998]. Such an approach requires that CRLs are published regularly, but that they\nare also inspected when certi\ufb01cates are checked. Practice indicates that client\napplications hardly ever consult CRLs and simply assume a certi\ufb01cate to be valid\nuntil it expires.\nSecure group management\nMany security systems make use of special services such as Key Distribution\nCenters (KDCs) or Certi\ufb01cation Authorities (CAs). These services demonstrate\na dif\ufb01cult problem in distributed systems. In the \ufb01rst place, they must be\ntrusted. To enhance the trust in security services, it is necessary to provide a\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "546 CHAPTER 9. SECURITY\nhigh degree of protection against all kinds of security threats. For example,\nas soon as a CA has been compromised, it becomes impossible to verify the\nvalidity of a public key, making the entire security system worthless.\nOn the other hand, it is also necessary that many security services offer\nhigh availability. For example, in the case of a KDC, each time two processes\nwant to set up a secure channel, at least one of them will need to contact the\nKDC for a shared secret key. If the KDC is not available, secure communication\ncannot be established unless an alternative technique for key establishment is\navailable, such as the Dif\ufb01e-Hellman key exchange.\nThe solution to high availability is replication. On the other hand, replica-\ntion makes a server more vulnerable to security attacks. We already discussed\nhow secure group communication can take place by sharing a secret among\nthe group members. In effect, no single group member is capable of compro-\nmising certi\ufb01cates, making the group itself highly secure.\nNote 9.9 (Advanced: Managing group membership)\nWhat remains to consider is how to actually manage a group of replicated servers.\nReiter et al. [1994] propose the following solution. The problem that needs to be\nsolved is to ensure that when a process asks to join a group G, the integrity of the\ngroup is not compromised. A group Gis assumed to use a secret key CKGshared\nby all group members for encrypting group messages. In addition, it also uses a\npublic/private key pair (K+\nG,K\u0000\nG)for communication with nongroup members.\nWhenever a process Pwants to join a group G, it sends a join request JR\nidentifying GandP,P\u2019s local time T, a generated reply pad RPand a generated\nsecret key KP,G.RPandKP,Gare jointly encrypted using the group\u2019s public key\nK+\nG, as shown as message 1 in Figure 9.28. The use of RPandKP,Gis explained in\nmore detail below. The join request JRis signed by P, and is sent along with a\ncerti\ufb01cate containing P\u2019s public key. We have used the widely-applied notation\n[M]Ato denote that message Mhas been signed by subject A.\nFigure 9.28: Securely admitting a new group member.\nWhen a group member Qreceives such a join request, it \ufb01rst authenticates\nP, after which communication with the other group members takes place to see\nwhether Pcan be admitted as a group member. Authentication of Ptakes place in\nthe usual way by means of the certi\ufb01cate. The timestamp Tis used to make sure\nthat the certi\ufb01cate was still valid at the time it was sent. (Note that we need to be\nsure that the time has not been tampered with as well.) Group member Qveri\ufb01es\nthe signature of the certi\ufb01cation authority and subsequently extracts P\u2019s public\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n546 CHAPTER 9. SECURITY\nhigh degree of protection against all kinds of security threats. For example,\nas soon as a CA has been compromised, it becomes impossible to verify the\nvalidity of a public key, making the entire security system worthless.\nOn the other hand, it is also necessary that many security services offer\nhigh availability. For example, in the case of a KDC, each time two processes\nwant to set up a secure channel, at least one of them will need to contact the\nKDC for a shared secret key. If the KDC is not available, secure communication\ncannot be established unless an alternative technique for key establishment is\navailable, such as the Dif\ufb01e-Hellman key exchange.\nThe solution to high availability is replication. On the other hand, replica-\ntion makes a server more vulnerable to security attacks. We already discussed\nhow secure group communication can take place by sharing a secret among\nthe group members. In effect, no single group member is capable of compro-\nmising certi\ufb01cates, making the group itself highly secure.\nNote 9.9 (Advanced: Managing group membership)\nWhat remains to consider is how to actually manage a group of replicated servers.\nReiter et al. [1994] propose the following solution. The problem that needs to be\nsolved is to ensure that when a process asks to join a group G, the integrity of the\ngroup is not compromised. A group Gis assumed to use a secret key CKGshared\nby all group members for encrypting group messages. In addition, it also uses a\npublic/private key pair (K+\nG,K\u0000\nG)for communication with nongroup members.\nWhenever a process Pwants to join a group G, it sends a join request JR\nidentifying GandP,P\u2019s local time T, a generated reply pad RPand a generated\nsecret key KP,G.RPandKP,Gare jointly encrypted using the group\u2019s public key\nK+\nG, as shown as message 1 in Figure 9.28. The use of RPandKP,Gis explained in\nmore detail below. The join request JRis signed by P, and is sent along with a\ncerti\ufb01cate containing P\u2019s public key. We have used the widely-applied notation\n[M]Ato denote that message Mhas been signed by subject A.\nFigure 9.28: Securely admitting a new group member.\nWhen a group member Qreceives such a join request, it \ufb01rst authenticates\nP, after which communication with the other group members takes place to see\nwhether Pcan be admitted as a group member. Authentication of Ptakes place in\nthe usual way by means of the certi\ufb01cate. The timestamp Tis used to make sure\nthat the certi\ufb01cate was still valid at the time it was sent. (Note that we need to be\nsure that the time has not been tampered with as well.) Group member Qveri\ufb01es\nthe signature of the certi\ufb01cation authority and subsequently extracts P\u2019s public\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 547\nkey from the certi\ufb01cate to check the validity of JR. At that point, a group-speci\ufb01c\nprotocol is followed to see whether all group members agree on admitting P.\nIfPis allowed to join the group, Qreturns a group admittance message\nGA, shown as message 2 in Figure 9.28 identifying Pand containing a nonce N.\nThe reply pad RPis used to encrypt the group\u2019s communication key CKG. In\naddition, Pwill also need the group\u2019s private key K\u0000\nG, which is encrypted with\nCKG. Message GAis subsequently signed by Qusing key KP,G.\nProcess Pcan now authenticate Q, because only a true group member can\nhave discovered the secret key KP,G. The nonce Nin this protocol is not used for\nsecurity; instead, when Psends back Nencrypted with KP,G(message 3), Qthen\nknows that Phas received all the necessary keys, and has therefore now indeed\njoined the group.\nNote that instead of using the reply pad RP,PandQcould also have encrypted\nCKGusing P\u2019s public key. However, because RPis used only once, namely for the\nencryption of the group\u2019s communication key in message GA, using RPis safer.\nIfP\u2019s private key were ever to revealed, it would become possible to also reveal\nCKG, which would compromise the secrecy of all group communication.\nAuthorization management\nManaging security in distributed systems is also concerned with managing\naccess rights. So far, we have hardly touched upon the issue of how access\nrights are initially granted to users or groups of users, and how they are\nsubsequently maintained in an unforgeable way. It is time to correct this\nomission.\nIn nondistributed systems, managing access rights is relatively easy. When\na new user is added to the system, that user is given initial rights, for example,\nto create \ufb01les and subdirectories in a speci\ufb01c directory, create processes, use\nCPU time, and so on. In other words, a complete account for a user is set up\nfor one speci\ufb01c machine in which all rights have been speci\ufb01ed in advance by\nthe system administrators.\nIn a distributed system, matters are complicated by the fact that resources\nare spread across several machines. If the approach for nondistributed systems\nwere to be followed, it would be necessary to create an account for each user on\neach machine. In essence, this is the approach followed in network operating\nsystems. Matters can be simpli\ufb01ed a bit by creating a single account on a\ncentral server. That server is consulted each time a user accesses certain\nresources or machines.\nCapabilities and attribute certi\ufb01cates\nA much better approach that has been widely applied in distributed systems\nis the use of capabilities. A capability is an unforgeable data structure for\na speci\ufb01c resource, specifying exactly the access rights that the holder of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 547\nkey from the certi\ufb01cate to check the validity of JR. At that point, a group-speci\ufb01c\nprotocol is followed to see whether all group members agree on admitting P.\nIfPis allowed to join the group, Qreturns a group admittance message\nGA, shown as message 2 in Figure 9.28 identifying Pand containing a nonce N.\nThe reply pad RPis used to encrypt the group\u2019s communication key CKG. In\naddition, Pwill also need the group\u2019s private key K\u0000\nG, which is encrypted with\nCKG. Message GAis subsequently signed by Qusing key KP,G.\nProcess Pcan now authenticate Q, because only a true group member can\nhave discovered the secret key KP,G. The nonce Nin this protocol is not used for\nsecurity; instead, when Psends back Nencrypted with KP,G(message 3), Qthen\nknows that Phas received all the necessary keys, and has therefore now indeed\njoined the group.\nNote that instead of using the reply pad RP,PandQcould also have encrypted\nCKGusing P\u2019s public key. However, because RPis used only once, namely for the\nencryption of the group\u2019s communication key in message GA, using RPis safer.\nIfP\u2019s private key were ever to revealed, it would become possible to also reveal\nCKG, which would compromise the secrecy of all group communication.\nAuthorization management\nManaging security in distributed systems is also concerned with managing\naccess rights. So far, we have hardly touched upon the issue of how access\nrights are initially granted to users or groups of users, and how they are\nsubsequently maintained in an unforgeable way. It is time to correct this\nomission.\nIn nondistributed systems, managing access rights is relatively easy. When\na new user is added to the system, that user is given initial rights, for example,\nto create \ufb01les and subdirectories in a speci\ufb01c directory, create processes, use\nCPU time, and so on. In other words, a complete account for a user is set up\nfor one speci\ufb01c machine in which all rights have been speci\ufb01ed in advance by\nthe system administrators.\nIn a distributed system, matters are complicated by the fact that resources\nare spread across several machines. If the approach for nondistributed systems\nwere to be followed, it would be necessary to create an account for each user on\neach machine. In essence, this is the approach followed in network operating\nsystems. Matters can be simpli\ufb01ed a bit by creating a single account on a\ncentral server. That server is consulted each time a user accesses certain\nresources or machines.\nCapabilities and attribute certi\ufb01cates\nA much better approach that has been widely applied in distributed systems\nis the use of capabilities. A capability is an unforgeable data structure for\na speci\ufb01c resource, specifying exactly the access rights that the holder of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "548 CHAPTER 9. SECURITY\nthe capability has with respect to that resource. Different implementations\nof capabilities exist. Here, we brie\ufb02y discuss the implementation as used\nin the Amoeba operating system [Tanenbaum et al., 1986]. Although an\nexample from some time ago, its simplicity makes it an excellent candidate\nfor understanding the underlying principles.\nAmoeba was one of the \ufb01rst object-based distributed systems. An object\nresides at a server while clients are offered transparent access by means of a\nproxy. To invoke an operation on an object, a client passes a capability to its\nlocal operating system, which then locates the server where the object resides\nand subsequently does an RPC to that server.\nA capability is a 128-bit identi\ufb01er, internally organized as shown in Fig-\nure 9.29. The \ufb01rst 48 bits are initialized by the object\u2019s server when the object\nis created and effectively form a machine-independent identi\ufb01er of the object\u2019s\nserver, referred to as the server port .\n48 bits 24 bits 8 bits 48 bits\nServer port Object Rights Check\nFigure 9.29: A capability in Amoeba.\nThe next 24 bits are used to identify the object at the given server. (Note\nthat the server port together with the object identi\ufb01er form a 72-bit systemwide\nunique identi\ufb01er.) The next 8 bits are used to specify the access rights of\nthe holder of the capability. Finally, the 48-bits check \ufb01eld is used to make a\ncapability unforgeable, as we explain below.\nWhen an object is created, its server picks a random check \ufb01eld and stores\nit both in the capability as well as internally in its own tables. All the right\nbits in a new capability are initially on, and it is this owner capability that\nis returned to the client. When the capability is sent back to the server in a\nrequest to perform an operation, the check \ufb01eld is veri\ufb01ed.\nNote 9.10 (Advanced: Restricted capabilities)\nTo create a restricted capability, a client can pass a capability back to the server,\nalong with a bit mask for the new rights. The server takes the original check \ufb01eld\nfrom its tables, XOR s it with the new rights (which must be a subset of the rights\nin the capability), and then runs the result through a one-way function.\nThe server then creates a new capability, with the same value in the object\n\ufb01eld, but with the new rights bits in the rights \ufb01eld and the output of the one-way\nfunction in the check \ufb01eld. The new capability is then returned to the caller. The\nclient may send this new capability to another process, if it wishes.\nThe method of generating restricted capabilities is illustrated in Figure 9.30\nIn this example, the owner has turned off all the rights except one. For example,\nthe restricted capability might allow the object to be read, but nothing else.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n548 CHAPTER 9. SECURITY\nthe capability has with respect to that resource. Different implementations\nof capabilities exist. Here, we brie\ufb02y discuss the implementation as used\nin the Amoeba operating system [Tanenbaum et al., 1986]. Although an\nexample from some time ago, its simplicity makes it an excellent candidate\nfor understanding the underlying principles.\nAmoeba was one of the \ufb01rst object-based distributed systems. An object\nresides at a server while clients are offered transparent access by means of a\nproxy. To invoke an operation on an object, a client passes a capability to its\nlocal operating system, which then locates the server where the object resides\nand subsequently does an RPC to that server.\nA capability is a 128-bit identi\ufb01er, internally organized as shown in Fig-\nure 9.29. The \ufb01rst 48 bits are initialized by the object\u2019s server when the object\nis created and effectively form a machine-independent identi\ufb01er of the object\u2019s\nserver, referred to as the server port .\n48 bits 24 bits 8 bits 48 bits\nServer port Object Rights Check\nFigure 9.29: A capability in Amoeba.\nThe next 24 bits are used to identify the object at the given server. (Note\nthat the server port together with the object identi\ufb01er form a 72-bit systemwide\nunique identi\ufb01er.) The next 8 bits are used to specify the access rights of\nthe holder of the capability. Finally, the 48-bits check \ufb01eld is used to make a\ncapability unforgeable, as we explain below.\nWhen an object is created, its server picks a random check \ufb01eld and stores\nit both in the capability as well as internally in its own tables. All the right\nbits in a new capability are initially on, and it is this owner capability that\nis returned to the client. When the capability is sent back to the server in a\nrequest to perform an operation, the check \ufb01eld is veri\ufb01ed.\nNote 9.10 (Advanced: Restricted capabilities)\nTo create a restricted capability, a client can pass a capability back to the server,\nalong with a bit mask for the new rights. The server takes the original check \ufb01eld\nfrom its tables, XOR s it with the new rights (which must be a subset of the rights\nin the capability), and then runs the result through a one-way function.\nThe server then creates a new capability, with the same value in the object\n\ufb01eld, but with the new rights bits in the rights \ufb01eld and the output of the one-way\nfunction in the check \ufb01eld. The new capability is then returned to the caller. The\nclient may send this new capability to another process, if it wishes.\nThe method of generating restricted capabilities is illustrated in Figure 9.30\nIn this example, the owner has turned off all the rights except one. For example,\nthe restricted capability might allow the object to be read, but nothing else.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 549\nThe meaning of the rights \ufb01eld is different for each object type since the legal\noperations themselves also vary from object type to object type.\nFigure 9.30: Generation of a restricted capability from an owner capability.\nWhen the restricted capability comes back to the server, the server sees from\ntherights \ufb01eld that it is not an owner capability because at least one bit is turned\noff. The server then fetches the original random number from its tables, XOR s it\nwith the rights \ufb01eld from the capability, and runs the result through the one-way\nfunction. If the result agrees with the check \ufb01eld, the capability is accepted.\nIt should be obvious from this algorithm that a user who tries to add rights\nthat he does not have will simply invalidate the capability. Inverting the check \ufb01eld\nin a restricted capability to get the argument ( CXOR 00000001) in Figure 9.30\nis impossible because the function fis a one-way function. It is through this\ncryptographic technique that capabilities are protected from tampering. Note that\nfessentially does the same as computing a message digest as discussed earlier.\nChanging anything in the original message (like inverting a bit), will immediately\nbe detected.\nA generalization of capabilities that is sometimes used in distributed\nsystems is the attribute certi\ufb01cate . Unlike the certi\ufb01cates discussed above\nthat are used to verify the validity of a public key, attribute certi\ufb01cates are\nused to list certain (attribute, value) pairs that apply to an identi\ufb01ed entity. In\nparticular, attribute certi\ufb01cates can be used to list the access rights that the\nholder of a certi\ufb01cate has with respect to the identi\ufb01ed resource.\nLike other certi\ufb01cates, attribute certi\ufb01cates are handed out by special cer-\nti\ufb01cation authorities, usually called attribute certi\ufb01cation authorities . Com-\npared to Amoeba\u2019s capabilities, such an authority corresponds to an object\u2019s\nserver. In general, however, the attribute certi\ufb01cation authority and the server\nmanaging the entity for which a certi\ufb01cate has been created need not be the\nsame. The access rights listed in a certi\ufb01cate are signed by the attribute certi\ufb01-\ncation authority. More on the evolution of capability-based access control is\ndiscussed by Karp et al. [2010] and Franqueira and Wieringa [2012].\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 549\nThe meaning of the rights \ufb01eld is different for each object type since the legal\noperations themselves also vary from object type to object type.\nFigure 9.30: Generation of a restricted capability from an owner capability.\nWhen the restricted capability comes back to the server, the server sees from\ntherights \ufb01eld that it is not an owner capability because at least one bit is turned\noff. The server then fetches the original random number from its tables, XOR s it\nwith the rights \ufb01eld from the capability, and runs the result through the one-way\nfunction. If the result agrees with the check \ufb01eld, the capability is accepted.\nIt should be obvious from this algorithm that a user who tries to add rights\nthat he does not have will simply invalidate the capability. Inverting the check \ufb01eld\nin a restricted capability to get the argument ( CXOR 00000001) in Figure 9.30\nis impossible because the function fis a one-way function. It is through this\ncryptographic technique that capabilities are protected from tampering. Note that\nfessentially does the same as computing a message digest as discussed earlier.\nChanging anything in the original message (like inverting a bit), will immediately\nbe detected.\nA generalization of capabilities that is sometimes used in distributed\nsystems is the attribute certi\ufb01cate . Unlike the certi\ufb01cates discussed above\nthat are used to verify the validity of a public key, attribute certi\ufb01cates are\nused to list certain (attribute, value) pairs that apply to an identi\ufb01ed entity. In\nparticular, attribute certi\ufb01cates can be used to list the access rights that the\nholder of a certi\ufb01cate has with respect to the identi\ufb01ed resource.\nLike other certi\ufb01cates, attribute certi\ufb01cates are handed out by special cer-\nti\ufb01cation authorities, usually called attribute certi\ufb01cation authorities . Com-\npared to Amoeba\u2019s capabilities, such an authority corresponds to an object\u2019s\nserver. In general, however, the attribute certi\ufb01cation authority and the server\nmanaging the entity for which a certi\ufb01cate has been created need not be the\nsame. The access rights listed in a certi\ufb01cate are signed by the attribute certi\ufb01-\ncation authority. More on the evolution of capability-based access control is\ndiscussed by Karp et al. [2010] and Franqueira and Wieringa [2012].\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "550 CHAPTER 9. SECURITY\nDelegation\nNow consider the following problem. A user wants to have a large \ufb01le printed\nfor which he has read-only access rights. In order not to bother others too\nmuch, the user sends a request to the print server, asking it to start printing\nthe \ufb01le no earlier than 2 o\u2019clock in the morning. Instead of sending the entire\n\ufb01le to the printer, the user passes the \ufb01le name to the printer so that it can\ncopy it to its spooling directory, if necessary, when actually needed.\nAlthough this scheme seems to be perfectly in order, there is one major\nproblem: the printer will generally not have the appropriate access permis-\nsions to the named \ufb01le. In other words, if no special measures are taken, as\nsoon as the print server wants to read the \ufb01le in order to print it, the system\nwill deny the server access to the \ufb01le. This problem could have been solved if\nthe user had temporarily delegated his access rights for the \ufb01le to the print\nserver.\nDelegation of access rights is an important technique for implementing\nprotection in computer systems and distributed systems, in particular. The\nbasic idea is simple: by passing certain access rights from one process to\nanother, it becomes easier to distribute work between several processes without\nadversely affecting the protection of resources. In the case of distributed\nsystems, processes may run on different machines and even within different\nadministrative domains. Delegation can avoid much overhead as protection\ncan often be handled locally.\nThere are several ways to implement delegation. A general approach as\ndescribed by Neuman [1993] and implemented in the Kerberos system, is to\nmake use of a proxy. A proxy in the context of security in computer systems\nis a token that allows its owner to operate with the same or restricted rights\nand privileges as the subject that granted the token. (Note that this notion of a\nproxy is different from a proxy as a synonym for a client-side stub. Although\nwe try to avoid overloading terms, we make an exception here as the term\n\u201cproxy\u201d in the de\ufb01nition above is too widely used to ignore.) A process can\ncreate a proxy with at best the same rights and privileges it has itself. If a\nprocess creates a new proxy based on one it currently has, the derived proxy\nwill have at least the same restrictions as the original one, and possibly more.\nBefore considering a general scheme for delegation, consider the following\ntwo approaches. First, delegation is relatively simple if Alice knows everyone.\nIf she wants to delegate rights to Bob, she merely needs to construct a certi\ufb01-\ncate saying \u201cAlice says Bob has rights R,\u201d such as [A,B,R]A. If Bob wants to\npass some of these rights to Charlie, he will ask Charlie to contact Alice and\nask her for an appropriate certi\ufb01cate.\nIn a second simple case Alice can simply construct a certi\ufb01cate saying\n\u201cThe bearer of this certi\ufb01cate has rights R.\u201d However, in this case we need to\nprotect the certi\ufb01cate against illegal copying, as is done with securely passing\ncapabilities between processes. Neuman\u2019s scheme handles this case, as well\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n550 CHAPTER 9. SECURITY\nDelegation\nNow consider the following problem. A user wants to have a large \ufb01le printed\nfor which he has read-only access rights. In order not to bother others too\nmuch, the user sends a request to the print server, asking it to start printing\nthe \ufb01le no earlier than 2 o\u2019clock in the morning. Instead of sending the entire\n\ufb01le to the printer, the user passes the \ufb01le name to the printer so that it can\ncopy it to its spooling directory, if necessary, when actually needed.\nAlthough this scheme seems to be perfectly in order, there is one major\nproblem: the printer will generally not have the appropriate access permis-\nsions to the named \ufb01le. In other words, if no special measures are taken, as\nsoon as the print server wants to read the \ufb01le in order to print it, the system\nwill deny the server access to the \ufb01le. This problem could have been solved if\nthe user had temporarily delegated his access rights for the \ufb01le to the print\nserver.\nDelegation of access rights is an important technique for implementing\nprotection in computer systems and distributed systems, in particular. The\nbasic idea is simple: by passing certain access rights from one process to\nanother, it becomes easier to distribute work between several processes without\nadversely affecting the protection of resources. In the case of distributed\nsystems, processes may run on different machines and even within different\nadministrative domains. Delegation can avoid much overhead as protection\ncan often be handled locally.\nThere are several ways to implement delegation. A general approach as\ndescribed by Neuman [1993] and implemented in the Kerberos system, is to\nmake use of a proxy. A proxy in the context of security in computer systems\nis a token that allows its owner to operate with the same or restricted rights\nand privileges as the subject that granted the token. (Note that this notion of a\nproxy is different from a proxy as a synonym for a client-side stub. Although\nwe try to avoid overloading terms, we make an exception here as the term\n\u201cproxy\u201d in the de\ufb01nition above is too widely used to ignore.) A process can\ncreate a proxy with at best the same rights and privileges it has itself. If a\nprocess creates a new proxy based on one it currently has, the derived proxy\nwill have at least the same restrictions as the original one, and possibly more.\nBefore considering a general scheme for delegation, consider the following\ntwo approaches. First, delegation is relatively simple if Alice knows everyone.\nIf she wants to delegate rights to Bob, she merely needs to construct a certi\ufb01-\ncate saying \u201cAlice says Bob has rights R,\u201d such as [A,B,R]A. If Bob wants to\npass some of these rights to Charlie, he will ask Charlie to contact Alice and\nask her for an appropriate certi\ufb01cate.\nIn a second simple case Alice can simply construct a certi\ufb01cate saying\n\u201cThe bearer of this certi\ufb01cate has rights R.\u201d However, in this case we need to\nprotect the certi\ufb01cate against illegal copying, as is done with securely passing\ncapabilities between processes. Neuman\u2019s scheme handles this case, as well\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.5. SECURITY MANAGEMENT 551\nas avoiding the issue that Alice needs to know everyone to whom rights need\nto be delegated.\nA proxy in Neuman\u2019s scheme has two parts, as illustrated in Figure 9.31.\nLetAbe the process that created the proxy. The \ufb01rst part of the proxy is a set\nC=R,S+proxy , consisting of a set Rof access rights that have been delegated by\nA, along with a publicly known part of a secret that is used to authenticate the\nholder of the certi\ufb01cate. We will explain the use of S+proxy below. The certi\ufb01cate\ncarries the signature sig(A,C)ofA, to protect it against modi\ufb01cations. The\nsecond part contains the other part of the secret, denoted as S\u0000proxy . It is\nessential that S\u0000proxy is protected against disclosure when delegating rights to\nanother process.\nFigure 9.31: The general structure of a proxy as used for delegation.\nAnother way of looking at the proxy is as follows. If Alice wants to\ndelegate some of her rights to Bob, she makes a list of rights ( R) that Bob\ncan exercise. By signing the list, she prevents Bob from tampering with it.\nHowever, having only a signed list of rights is often not enough. If Bob wants\nto exercise his rights, he may have to prove that he actually got the list from\nAlice and did not, for example, steal it from someone else. Therefore, Alice\ncomes up with a very nasty question ( S+proxy ) that only she knows the answer\nto (S\u0000proxy ). Anyone can easily verify the correctness of the answer when given\nthe question. The question is appended to the list before Alice adds her\nsignature.\nWhen delegating some of her rights, Alice gives the signed list of rights,\nalong with the nasty question, to Bob. She also gives Bob the answer ensuring\nthat no one can intercept it. Bob now has a list of rights, signed by Alice,\nwhich he can hand over to, say, Charlie, when necessary. Charlie will ask him\nthe nasty question at the bottom of the list. If Bob knows the answer to it,\nCharlie will know that Alice had indeed delegated the listed rights to Bob.\nAn important property of this scheme is that Alice need not be consulted.\nIn fact, Bob may decide to pass on (some of) the rights on the list to Dave. In\ndoing so, he will also tell Dave the answer to the question, so that Dave can\nprove the list was handed over to him by someone entitled to it. Alice never\nneeds to know about Dave at all.\nA protocol for delegating and exercising rights is shown in Figure 9.32.\nAssume that Alice and Bob share a secret key KA,Bthat can be used for\nencrypting messages they send to each other. Then, Alice \ufb01rst sends Bob\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.5. SECURITY MANAGEMENT 551\nas avoiding the issue that Alice needs to know everyone to whom rights need\nto be delegated.\nA proxy in Neuman\u2019s scheme has two parts, as illustrated in Figure 9.31.\nLetAbe the process that created the proxy. The \ufb01rst part of the proxy is a set\nC=R,S+proxy , consisting of a set Rof access rights that have been delegated by\nA, along with a publicly known part of a secret that is used to authenticate the\nholder of the certi\ufb01cate. We will explain the use of S+proxy below. The certi\ufb01cate\ncarries the signature sig(A,C)ofA, to protect it against modi\ufb01cations. The\nsecond part contains the other part of the secret, denoted as S\u0000proxy . It is\nessential that S\u0000proxy is protected against disclosure when delegating rights to\nanother process.\nFigure 9.31: The general structure of a proxy as used for delegation.\nAnother way of looking at the proxy is as follows. If Alice wants to\ndelegate some of her rights to Bob, she makes a list of rights ( R) that Bob\ncan exercise. By signing the list, she prevents Bob from tampering with it.\nHowever, having only a signed list of rights is often not enough. If Bob wants\nto exercise his rights, he may have to prove that he actually got the list from\nAlice and did not, for example, steal it from someone else. Therefore, Alice\ncomes up with a very nasty question ( S+proxy ) that only she knows the answer\nto (S\u0000proxy ). Anyone can easily verify the correctness of the answer when given\nthe question. The question is appended to the list before Alice adds her\nsignature.\nWhen delegating some of her rights, Alice gives the signed list of rights,\nalong with the nasty question, to Bob. She also gives Bob the answer ensuring\nthat no one can intercept it. Bob now has a list of rights, signed by Alice,\nwhich he can hand over to, say, Charlie, when necessary. Charlie will ask him\nthe nasty question at the bottom of the list. If Bob knows the answer to it,\nCharlie will know that Alice had indeed delegated the listed rights to Bob.\nAn important property of this scheme is that Alice need not be consulted.\nIn fact, Bob may decide to pass on (some of) the rights on the list to Dave. In\ndoing so, he will also tell Dave the answer to the question, so that Dave can\nprove the list was handed over to him by someone entitled to it. Alice never\nneeds to know about Dave at all.\nA protocol for delegating and exercising rights is shown in Figure 9.32.\nAssume that Alice and Bob share a secret key KA,Bthat can be used for\nencrypting messages they send to each other. Then, Alice \ufb01rst sends Bob\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "552 CHAPTER 9. SECURITY\nthe certi\ufb01cate C=R,S+proxy , signed with sig(A,C)(and denoted again as\n[R,S+proxy]A). There is no need to encrypt this message: it can be sent as\nplaintext. Only the private part of the secret needs to be encrypted, shown as\nKA,B(S\u0000proxy)in message 1.\nFigure 9.32: Using a proxy to delegate and prove ownership of access rights.\nNow suppose that Bob wants an operation to be carried out at an object\nthat resides at a speci\ufb01c server. Also, assume that Alice is authorized to have\nthat operation carried out, and that she has delegated those rights to Bob.\nTherefore, Bob hands over his credentials to the server in the form of the\nsigned certi\ufb01cate [R,S+proxy]A.\nAt that point, the server will be able to verify that Chas not been tampered\nwith: any modi\ufb01cation to the list of rights, or the nasty question will be\nnoticed, because both have been jointly signed by Alice. However, the server\ndoes not know yet whether Bob is the rightful owner of the certi\ufb01cate. To\nverify this, the server must use the secret that came with C.\nThere are several ways to implement S+proxy and S\u0000proxy . For example,\nassume S+proxy is a public key and S\u0000proxy the corresponding private key. Z\ncan then challenge Bob by sending him a nonce N, encrypted with S+proxy . By\ndecrypting S+\nproxy(N)and returning N, Bob proves he knows the secret and is\nthus the rightful holder of the certi\ufb01cate. There are other ways to implement\nsecure delegation as well, but the basic idea is always: show you know a\nsecret.\n9.6 Summary\nSecurity plays an extremely important role in distributed systems. A dis-\ntributed system should provide the mechanisms that allow a variety of differ-\nent security policies to be enforced. Developing and properly applying those\nmechanisms generally makes security a dif\ufb01cult engineering exercise.\nThree important issues can be distinguished. The \ufb01rst issue is that a\ndistributed system should offer facilities to establish secure channels between\nprocesses. A secure channel, in principle, provides the means to mutually\nauthenticate the communicating parties, and protect messages against tam-\npering during their transmission. A secure channel generally also provides\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n552 CHAPTER 9. SECURITY\nthe certi\ufb01cate C=R,S+proxy , signed with sig(A,C)(and denoted again as\n[R,S+proxy]A). There is no need to encrypt this message: it can be sent as\nplaintext. Only the private part of the secret needs to be encrypted, shown as\nKA,B(S\u0000proxy)in message 1.\nFigure 9.32: Using a proxy to delegate and prove ownership of access rights.\nNow suppose that Bob wants an operation to be carried out at an object\nthat resides at a speci\ufb01c server. Also, assume that Alice is authorized to have\nthat operation carried out, and that she has delegated those rights to Bob.\nTherefore, Bob hands over his credentials to the server in the form of the\nsigned certi\ufb01cate [R,S+proxy]A.\nAt that point, the server will be able to verify that Chas not been tampered\nwith: any modi\ufb01cation to the list of rights, or the nasty question will be\nnoticed, because both have been jointly signed by Alice. However, the server\ndoes not know yet whether Bob is the rightful owner of the certi\ufb01cate. To\nverify this, the server must use the secret that came with C.\nThere are several ways to implement S+proxy and S\u0000proxy . For example,\nassume S+proxy is a public key and S\u0000proxy the corresponding private key. Z\ncan then challenge Bob by sending him a nonce N, encrypted with S+proxy . By\ndecrypting S+\nproxy(N)and returning N, Bob proves he knows the secret and is\nthus the rightful holder of the certi\ufb01cate. There are other ways to implement\nsecure delegation as well, but the basic idea is always: show you know a\nsecret.\n9.6 Summary\nSecurity plays an extremely important role in distributed systems. A dis-\ntributed system should provide the mechanisms that allow a variety of differ-\nent security policies to be enforced. Developing and properly applying those\nmechanisms generally makes security a dif\ufb01cult engineering exercise.\nThree important issues can be distinguished. The \ufb01rst issue is that a\ndistributed system should offer facilities to establish secure channels between\nprocesses. A secure channel, in principle, provides the means to mutually\nauthenticate the communicating parties, and protect messages against tam-\npering during their transmission. A secure channel generally also provides\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "9.6. SUMMARY 553\ncon\ufb01dentiality so that no one but the communicating parties can read the\nmessages that go through the channel.\nAn important design issue is whether to use only a symmetric cryptosys-\ntem (which is based on shared secret keys), or to combine it with a public-key\nsystem. Current practice shows the use of public-key cryptography for dis-\ntributing short-term shared secret keys. The latter are known as session\nkeys.\nThe second issue in secure distributed systems is access control, or autho-\nrization. Authorization deals with protecting resources in such a way that\nonly processes that have the proper access rights can actual access and use\nthose resources. Access control always take place after a process has been\nauthenticated. Related to access control is preventing denial-of-service, which\nturns out to a dif\ufb01cult problem for systems that are accessible through the\nInternet.\nThere are two ways of implementing access control. First, each resource\ncan maintain an access control list, listing exactly the access rights of each\nuser or process. Alternatively, a process can carry a certi\ufb01cate stating precisely\nwhat its rights are for a particular set of resources. The main bene\ufb01t of using\ncerti\ufb01cates is that a process can easily pass its ticket to another process, that is,\ndelegate its access rights. Certi\ufb01cates, however, have the drawback that they\nare often dif\ufb01cult to revoke.\nSpecial attention is needed when dealing with access control in the case\nof mobile code. Several proposals have been made, of which the sandbox\nis currently the most widely-applied one. However, sandboxes are rather\nrestrictive, and more \ufb02exible approaches based on true protection domains\nhave been devised as well.\nThe third issue in secure distributed systems concerns management. There\nare essentially two important subtopics: key management and authorization\nmanagement. Key management includes the distribution of cryptographic\nkeys, for which certi\ufb01cates as issued by trusted third parties play an important\nrole. Important with respect to authorization management are attribute\ncerti\ufb01cates and delegation.\nFinally, special attention is required to handling secure names. A practical\nsolution is to name an object by taking the hash of its public key, along with a\nhuman-readable label (which should also be securely bound to the object).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\n9.6. SUMMARY 553\ncon\ufb01dentiality so that no one but the communicating parties can read the\nmessages that go through the channel.\nAn important design issue is whether to use only a symmetric cryptosys-\ntem (which is based on shared secret keys), or to combine it with a public-key\nsystem. Current practice shows the use of public-key cryptography for dis-\ntributing short-term shared secret keys. The latter are known as session\nkeys.\nThe second issue in secure distributed systems is access control, or autho-\nrization. Authorization deals with protecting resources in such a way that\nonly processes that have the proper access rights can actual access and use\nthose resources. Access control always take place after a process has been\nauthenticated. Related to access control is preventing denial-of-service, which\nturns out to a dif\ufb01cult problem for systems that are accessible through the\nInternet.\nThere are two ways of implementing access control. First, each resource\ncan maintain an access control list, listing exactly the access rights of each\nuser or process. Alternatively, a process can carry a certi\ufb01cate stating precisely\nwhat its rights are for a particular set of resources. The main bene\ufb01t of using\ncerti\ufb01cates is that a process can easily pass its ticket to another process, that is,\ndelegate its access rights. Certi\ufb01cates, however, have the drawback that they\nare often dif\ufb01cult to revoke.\nSpecial attention is needed when dealing with access control in the case\nof mobile code. Several proposals have been made, of which the sandbox\nis currently the most widely-applied one. However, sandboxes are rather\nrestrictive, and more \ufb02exible approaches based on true protection domains\nhave been devised as well.\nThe third issue in secure distributed systems concerns management. There\nare essentially two important subtopics: key management and authorization\nmanagement. Key management includes the distribution of cryptographic\nkeys, for which certi\ufb01cates as issued by trusted third parties play an important\nrole. Important with respect to authorization management are attribute\ncerti\ufb01cates and delegation.\nFinally, special attention is required to handling secure names. A practical\nsolution is to name an object by taking the hash of its public key, along with a\nhuman-readable label (which should also be securely bound to the object).\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "", "Bibliography\nAbadi M. and Needham R. Prudent Engineering Practice for Cryptographic Protocols.\nIEEE Transactions on Software Engineering , 22(1):6\u201315, Jan. 1996. Cited on 516\nAbdullahi S. and Ringwood G. Garbage Collecting the Internet: A Survey of Dis-\ntributed Garbage Collection. ACM Computing Surveys , 30(3):330\u2013373, Sept. 1998.\nCited on 244\nAberer K., Alima L. O., Ghodsi A., Girdzijauskas S., Hauswirth M., and Haridi S. The\nEssence of P2P: A Reference Architecture for Overlay Networks. In 5th International\nConference on Peer-to-Peer Computing , pages 11\u201320, Los Alamitos, CA., Aug. 2005.\nIEEE, IEEE Computer Society Press. Cited on 81\nAbraham J. and others . Properties and Performance of the Prototype Instrument for\nthe Pierre Auger Observatory. Nuclear Instruments and Methods in Physics Research\nSection A: Accelerators, Spectrometers, Detectors and Associated Equipment , 523(1\u00e2 \u02d8A\u00b8 S2):\n50 \u2013 95, 2004. Cited on 19\nAdar E. and Huberman B. A. Free Riding on Gnutella. Hewlett Packard, Information\nDynamics Lab, Jan. 2000. Cited on 91\nAdelstein F., Gupta S., Richard G., and Schwiebert L. Fundamentals of Mobile and\nPervasive Computing . McGraw-Hill, New York, NY, 2005. Cited on 44\nAdve S. V . and Boehm H.-J. Memory Models: A Case for Rethinking Parallel Languages\nand Hardware. Communications of the ACM , 53(8):90\u2013101, Aug. 2010. Cited on 367\nAger B., M\u00fchlbauer W., Smaragdakis G., and Uhlig S. Comparing DNS Resolvers in\nthe Wild. In 10th Internet Measurement Conference , pages 15\u201321, New York, NY, 2010.\nACM Press. Cited on 277\nAguilera M. and Terry D. The Many Faces of Consistency. Data Engineering , page 3,\n2016. Cited on 358\nAhlgren B., Dannewitz C., Imbrenda C., Kutscher D., and Ohlman B. A Survey of\nInformation-centric Networking. IEEE Communications Magazine , 50(7):26\u201336, July\n2012. Cited on 240, 345\nAkgul F. ZeroMQ . Packt Publishing, Birmingham, UK, 2013. Cited on 199\nAkyildiz I. F. and Kasimoglu I. H. Wireless Sensor and Actor Networks: Research\nChallenges. Ad Hoc Networks , 2:351\u2013367, 2004. Cited on 47\nAkyildiz I. F., Su W., Sankarasubramaniam Y., and Cayirci E. A Survey on Sensor\nNetworks. IEEE Communications Magazine , 40(8):102\u2013114, Aug. 2002. Cited on 47\nAkyildiz I. F., Wang X., and Wang W. Wireless Mesh Networks: A Survey. Computer\nNetworks , 47(4):445\u2013487, Mar. 2005. Cited on 47\n555\nBibliography\nAbadi M. and Needham R. Prudent Engineering Practice for Cryptographic Protocols.\nIEEE Transactions on Software Engineering , 22(1):6\u201315, Jan. 1996. Cited on 516\nAbdullahi S. and Ringwood G. Garbage Collecting the Internet: A Survey of Dis-\ntributed Garbage Collection. ACM Computing Surveys , 30(3):330\u2013373, Sept. 1998.\nCited on 244\nAberer K., Alima L. O., Ghodsi A., Girdzijauskas S., Hauswirth M., and Haridi S. The\nEssence of P2P: A Reference Architecture for Overlay Networks. In 5th International\nConference on Peer-to-Peer Computing , pages 11\u201320, Los Alamitos, CA., Aug. 2005.\nIEEE, IEEE Computer Society Press. Cited on 81\nAbraham J. and others . Properties and Performance of the Prototype Instrument for\nthe Pierre Auger Observatory. Nuclear Instruments and Methods in Physics Research\nSection A: Accelerators, Spectrometers, Detectors and Associated Equipment , 523(1\u00e2 \u02d8A\u00b8 S2):\n50 \u2013 95, 2004. Cited on 19\nAdar E. and Huberman B. A. Free Riding on Gnutella. Hewlett Packard, Information\nDynamics Lab, Jan. 2000. Cited on 91\nAdelstein F., Gupta S., Richard G., and Schwiebert L. Fundamentals of Mobile and\nPervasive Computing . McGraw-Hill, New York, NY, 2005. Cited on 44\nAdve S. V . and Boehm H.-J. Memory Models: A Case for Rethinking Parallel Languages\nand Hardware. Communications of the ACM , 53(8):90\u2013101, Aug. 2010. Cited on 367\nAger B., M\u00fchlbauer W., Smaragdakis G., and Uhlig S. Comparing DNS Resolvers in\nthe Wild. In 10th Internet Measurement Conference , pages 15\u201321, New York, NY, 2010.\nACM Press. Cited on 277\nAguilera M. and Terry D. The Many Faces of Consistency. Data Engineering , page 3,\n2016. Cited on 358\nAhlgren B., Dannewitz C., Imbrenda C., Kutscher D., and Ohlman B. A Survey of\nInformation-centric Networking. IEEE Communications Magazine , 50(7):26\u201336, July\n2012. Cited on 240, 345\nAkgul F. ZeroMQ . Packt Publishing, Birmingham, UK, 2013. Cited on 199\nAkyildiz I. F. and Kasimoglu I. H. Wireless Sensor and Actor Networks: Research\nChallenges. Ad Hoc Networks , 2:351\u2013367, 2004. Cited on 47\nAkyildiz I. F., Su W., Sankarasubramaniam Y., and Cayirci E. A Survey on Sensor\nNetworks. IEEE Communications Magazine , 40(8):102\u2013114, Aug. 2002. Cited on 47\nAkyildiz I. F., Wang X., and Wang W. Wireless Mesh Networks: A Survey. Computer\nNetworks , 47(4):445\u2013487, Mar. 2005. Cited on 47\n555", "556 BIBLIOGRAPHY\nAli W., Shamsuddin S. M., and Ismail A. S. A Survey of Web Caching and Prefetching.\nInternational Journal of Advances in Soft Computing and Its Applications , 3(1):18\u201344, 2011.\nCited on 412\nAllani M., Garbinato B., and Pedone F. Application Layer Multicast. In Garbinato B.,\nMirando H., and Rodrigues L., editors, Middleware for Network Eccentric and Mobile\nApplications , pages 191\u2013214. Springer-Verlag, Berlin, 2009. Cited on 222\nAllen R. and Lowe-Norris A. Windows 2000 Active Directory . O\u2019Reilly & Associates,\nSebastopol, CA., 2nd edition, 2003. Cited on 288\nAlonso G., Casati F., Kuno H., and Machiraju V . Web Services: Concepts, Architectures\nand Applications . Springer-Verlag, Berlin, 2004. Cited on 6, 34\nAlvisi L. and Marzullo K. Message Logging: Pessimistic, Optimistic, Causal, and\nOptimal. IEEE Transactions on Software Engineering , 24(2):149\u2013159, Feb. 1998. Cited\non 496, 497\nAmar L., Barak A., and Shiloh A. The MOSIX Direct File System Access Method for\nSupporting Scalable Cluster File Systems. Cluster Computing , 7(2):141\u2013150, Apr. 2004.\nCited on 27\nAmza C., Cox A., Dwarkadas S., Keleher P ., Lu H., Rajamony R., Yu W., and\nZwaenepoel W. TreadMarks: Shared Memory Computing on Networks of Worksta-\ntions. Computer , 29(2):18\u201328, Feb. 1996. Cited on 26\nAnderson T., Bershad B., Lazowska E., and Levy H. Scheduler Activations: Ef\ufb01cient\nKernel Support for the User-Level Management of Parallelism. In 13th Symposium\non Operating System Principles , pages 95\u2013109, New York, NY, Oct. 1991. ACM, ACM\nPress. Cited on 110\nAndrews G. Foundations of Multithreaded, Parallel, and Distributed Programming . Addison-\nWesley, Reading, MA., 2000. Cited on 298\nAndroutsellis-Theotokis S. and Spinellis D. A Survey of Peer-to-Peer Content Distri-\nbution Technologies. ACM Computing Surveys , 36(4):335\u2013371, Dec. 2004. Cited on\n81\nAranha C., Both C., Dear\ufb01eld B., Elkins C., Ross A., Squibb J., and Taylor M. IBM\nWebSphere MQ V7.1 and V7.5 Features and Enhancements . Redbooks. IBM, Feb. 2013.\nCited on 212\nArkills B. LDAP Directories Explained: An Introduction and Analysis . Addison-Wesley,\nReading, MA., 2003. Cited on 285\nArmbrust M., Fox A., Grif\ufb01th R., Joseph A. D., Katz R. H., Konwinski A., Lee G.,\nPatterson D. A., Rabkin A., Stoica I., and Zaharia M. A View of Cloud Computing.\nCommunications of the ACM , 53(4):50\u201358, Apr. 2010. Cited on 31\nAron M., Sanders D., Druschel P ., and Zwaenepoel W. Scalable Content-aware Re-\nquest Distribution in Cluster-based Network Servers. In USENIX Annual Technical\nConference , pages 323\u2013336, San Diego, CA, June 2000. USENIX. Cited on 144\nAttiya H. and Welch J. Distributed Computing Fundamentals, Simulations, and Advanced\nTopics . John Wiley, New York, 2nd edition, 2004. Cited on 298\nAtxutegi E., Liberal F., Saiz E., and Ibarrola E. Toward Standardized Internet Speed\nMeasurements for End Users: Current Technical Constraints. IEEE Communications\nMagazine , 54(9):50\u201357, Sept. 2016. Cited on 413\nAvizienis A., Laprie J.-C., Randell B., and Landwehr C. Basic Concepts and Taxonomy\nof Dependable and Secure Computing. IEEE Transactions on Dependable and Secure\nComputing , 1(1):11\u201333, Jan. 2004. Cited on 427\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n556 BIBLIOGRAPHY\nAli W., Shamsuddin S. M., and Ismail A. S. A Survey of Web Caching and Prefetching.\nInternational Journal of Advances in Soft Computing and Its Applications , 3(1):18\u201344, 2011.\nCited on 412\nAllani M., Garbinato B., and Pedone F. Application Layer Multicast. In Garbinato B.,\nMirando H., and Rodrigues L., editors, Middleware for Network Eccentric and Mobile\nApplications , pages 191\u2013214. Springer-Verlag, Berlin, 2009. Cited on 222\nAllen R. and Lowe-Norris A. Windows 2000 Active Directory . O\u2019Reilly & Associates,\nSebastopol, CA., 2nd edition, 2003. Cited on 288\nAlonso G., Casati F., Kuno H., and Machiraju V . Web Services: Concepts, Architectures\nand Applications . Springer-Verlag, Berlin, 2004. Cited on 6, 34\nAlvisi L. and Marzullo K. Message Logging: Pessimistic, Optimistic, Causal, and\nOptimal. IEEE Transactions on Software Engineering , 24(2):149\u2013159, Feb. 1998. Cited\non 496, 497\nAmar L., Barak A., and Shiloh A. The MOSIX Direct File System Access Method for\nSupporting Scalable Cluster File Systems. Cluster Computing , 7(2):141\u2013150, Apr. 2004.\nCited on 27\nAmza C., Cox A., Dwarkadas S., Keleher P ., Lu H., Rajamony R., Yu W., and\nZwaenepoel W. TreadMarks: Shared Memory Computing on Networks of Worksta-\ntions. Computer , 29(2):18\u201328, Feb. 1996. Cited on 26\nAnderson T., Bershad B., Lazowska E., and Levy H. Scheduler Activations: Ef\ufb01cient\nKernel Support for the User-Level Management of Parallelism. In 13th Symposium\non Operating System Principles , pages 95\u2013109, New York, NY, Oct. 1991. ACM, ACM\nPress. Cited on 110\nAndrews G. Foundations of Multithreaded, Parallel, and Distributed Programming . Addison-\nWesley, Reading, MA., 2000. Cited on 298\nAndroutsellis-Theotokis S. and Spinellis D. A Survey of Peer-to-Peer Content Distri-\nbution Technologies. ACM Computing Surveys , 36(4):335\u2013371, Dec. 2004. Cited on\n81\nAranha C., Both C., Dear\ufb01eld B., Elkins C., Ross A., Squibb J., and Taylor M. IBM\nWebSphere MQ V7.1 and V7.5 Features and Enhancements . Redbooks. IBM, Feb. 2013.\nCited on 212\nArkills B. LDAP Directories Explained: An Introduction and Analysis . Addison-Wesley,\nReading, MA., 2003. Cited on 285\nArmbrust M., Fox A., Grif\ufb01th R., Joseph A. D., Katz R. H., Konwinski A., Lee G.,\nPatterson D. A., Rabkin A., Stoica I., and Zaharia M. A View of Cloud Computing.\nCommunications of the ACM , 53(4):50\u201358, Apr. 2010. Cited on 31\nAron M., Sanders D., Druschel P ., and Zwaenepoel W. Scalable Content-aware Re-\nquest Distribution in Cluster-based Network Servers. In USENIX Annual Technical\nConference , pages 323\u2013336, San Diego, CA, June 2000. USENIX. Cited on 144\nAttiya H. and Welch J. Distributed Computing Fundamentals, Simulations, and Advanced\nTopics . John Wiley, New York, 2nd edition, 2004. Cited on 298\nAtxutegi E., Liberal F., Saiz E., and Ibarrola E. Toward Standardized Internet Speed\nMeasurements for End Users: Current Technical Constraints. IEEE Communications\nMagazine , 54(9):50\u201357, Sept. 2016. Cited on 413\nAvizienis A., Laprie J.-C., Randell B., and Landwehr C. Basic Concepts and Taxonomy\nof Dependable and Secure Computing. IEEE Transactions on Dependable and Secure\nComputing , 1(1):11\u201333, Jan. 2004. Cited on 427\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 557\nAwadallah A. and Rosenblum M. The vMatrix: A Network of Virtual Machine\nMonitors for Dynamic Content Distribution. In 7th Web Caching Workshop , Aug. 2002.\nCited on 118\nBabaoglu O. and Toueg S. Non-Blocking Atomic Commitment. In Mullender S., editor,\nDistributed Systems , pages 147\u2013168. Addison-Wesley, Wokingham, 2nd edition, 1993.\nCited on 487\nBader M. Space-Filling Curves, An Introduction with Applications in Scienti\ufb01c Computing .\nSpringer-Verlag, Berlin, 2013. Cited on 290\nBailis P ., Ghodsi A., Hellerstein J. M., and Stoica I. Bolt-on Causal Consistency. In\nSIGMOD International Conference on Management Of Data , pages 761\u2013772, New York,\nNY, 2013. ACM, ACM Press. Cited on 374\nBalakrishnan H., Kaashoek M. F., Karger D., Morris R., and Stoica I. Looking up Data\nin P2P Systems. Communications of the ACM , 46(2):43\u201348, Feb. 2003. Cited on 82\nBalazinska M., Balakrishnan H., and Karger D. INS/Twine: A Scalable Peer-to-Peer\nArchitecture for Intentional Resource Discovery. In 1st International Conference on\nPervasive Computing , volume 2414 of Lecture Notes in Computer Science , pages 195\u2013210,\nBerlin, Aug. 2002. Springer-Verlag. Cited on 292\nBaldauf M., Dustdar S., and Rosenberg F. A Survey on Context-aware Systems. Int. J.\nAd Hoc Ubiquitous Comput. , 2:263\u2013277, June 2007. Cited on 42\nBaldoni R., Beraldi R., Quema V ., Querzoni L., and Tucci-Piergiovanni S. TERA: Topic-\nbased Event Routing for Peer-to-Peer Architectures. In International Conference on\nDistributed Event-Based Systems , pages 2\u201313, New York, NY, 2007. ACM Press. Cited\non 347\nBaldoni R., Querzoni L., Tarkoma S., and Virgillito A. Distributed Event Routing in\nPublish/Subscribe Communication Systems: a Survey. In Garbinato B., Miranda H.,\nand Rodrigues L., editors, Middleware for Network Eccentric and Mobile Applications ,\npages 219\u2013244. Springer-Verlag, Berlin, 2009. Cited on 344\nBallintijn G. Locating Objects in a Wide-area System . PhD thesis, Vrije Universiteit\nAmsterdam, 2003. Cited on 251\nBanaei-Kashani F. and Shahab C. Criticality-based Analysis and Design of Unstructured\nPeer-to-Peer Networks as \u201cComplex Systems\u201d. In 3rd International Symposium on\nCluster Computing and the Grid , pages 351\u2013356, Los Alamitos, CA., May 2003. IEEE,\nIEEE Computer Society Press. Cited on 227\nBaquero C. and Preguica N. Why Logical Clocks Are Easy. Communications of the ACM ,\n59(4):43\u201347, Mar. 2016. Cited on 317\nBaratto R. A., Nieh J., and Kim L. THINC: A Remote Display Architecture for Thin-\nClient Computing. In 20th Symposium on Operating System Principles , pages 277\u2013290,\nNew York, NY, Oct. 2005. ACM, ACM Press. Cited on 127\nBarborak M., Malek M., and Dahbura A. The Consensus Problem in Fault-Tolerant\nComputing. ACM Computing Surveys , 25(2):171\u2013220, June 1993. Cited on 460\nBarham P ., Dragovic B., Fraser K., Hand S., Harris T., Ho A., Neugebar R., Pratt I.,\nand War\ufb01eld A. Xen and the Art of Virtualization. In 19th Symposium on Operating\nSystem Principles , pages 164\u2013177, New York, NY, Oct. 2003. ACM, ACM Press. Cited\non 122\nBarron D. Pascal \u2013 The Language and its Implementation . John Wiley, New York, 1981.\nCited on 158\nBarroso L. and H\u00f6lze U. The Datacenter as a Computer: An Introduction to the Design of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 557\nAwadallah A. and Rosenblum M. The vMatrix: A Network of Virtual Machine\nMonitors for Dynamic Content Distribution. In 7th Web Caching Workshop , Aug. 2002.\nCited on 118\nBabaoglu O. and Toueg S. Non-Blocking Atomic Commitment. In Mullender S., editor,\nDistributed Systems , pages 147\u2013168. Addison-Wesley, Wokingham, 2nd edition, 1993.\nCited on 487\nBader M. Space-Filling Curves, An Introduction with Applications in Scienti\ufb01c Computing .\nSpringer-Verlag, Berlin, 2013. Cited on 290\nBailis P ., Ghodsi A., Hellerstein J. M., and Stoica I. Bolt-on Causal Consistency. In\nSIGMOD International Conference on Management Of Data , pages 761\u2013772, New York,\nNY, 2013. ACM, ACM Press. Cited on 374\nBalakrishnan H., Kaashoek M. F., Karger D., Morris R., and Stoica I. Looking up Data\nin P2P Systems. Communications of the ACM , 46(2):43\u201348, Feb. 2003. Cited on 82\nBalazinska M., Balakrishnan H., and Karger D. INS/Twine: A Scalable Peer-to-Peer\nArchitecture for Intentional Resource Discovery. In 1st International Conference on\nPervasive Computing , volume 2414 of Lecture Notes in Computer Science , pages 195\u2013210,\nBerlin, Aug. 2002. Springer-Verlag. Cited on 292\nBaldauf M., Dustdar S., and Rosenberg F. A Survey on Context-aware Systems. Int. J.\nAd Hoc Ubiquitous Comput. , 2:263\u2013277, June 2007. Cited on 42\nBaldoni R., Beraldi R., Quema V ., Querzoni L., and Tucci-Piergiovanni S. TERA: Topic-\nbased Event Routing for Peer-to-Peer Architectures. In International Conference on\nDistributed Event-Based Systems , pages 2\u201313, New York, NY, 2007. ACM Press. Cited\non 347\nBaldoni R., Querzoni L., Tarkoma S., and Virgillito A. Distributed Event Routing in\nPublish/Subscribe Communication Systems: a Survey. In Garbinato B., Miranda H.,\nand Rodrigues L., editors, Middleware for Network Eccentric and Mobile Applications ,\npages 219\u2013244. Springer-Verlag, Berlin, 2009. Cited on 344\nBallintijn G. Locating Objects in a Wide-area System . PhD thesis, Vrije Universiteit\nAmsterdam, 2003. Cited on 251\nBanaei-Kashani F. and Shahab C. Criticality-based Analysis and Design of Unstructured\nPeer-to-Peer Networks as \u201cComplex Systems\u201d. In 3rd International Symposium on\nCluster Computing and the Grid , pages 351\u2013356, Los Alamitos, CA., May 2003. IEEE,\nIEEE Computer Society Press. Cited on 227\nBaquero C. and Preguica N. Why Logical Clocks Are Easy. Communications of the ACM ,\n59(4):43\u201347, Mar. 2016. Cited on 317\nBaratto R. A., Nieh J., and Kim L. THINC: A Remote Display Architecture for Thin-\nClient Computing. In 20th Symposium on Operating System Principles , pages 277\u2013290,\nNew York, NY, Oct. 2005. ACM, ACM Press. Cited on 127\nBarborak M., Malek M., and Dahbura A. The Consensus Problem in Fault-Tolerant\nComputing. ACM Computing Surveys , 25(2):171\u2013220, June 1993. Cited on 460\nBarham P ., Dragovic B., Fraser K., Hand S., Harris T., Ho A., Neugebar R., Pratt I.,\nand War\ufb01eld A. Xen and the Art of Virtualization. In 19th Symposium on Operating\nSystem Principles , pages 164\u2013177, New York, NY, Oct. 2003. ACM, ACM Press. Cited\non 122\nBarron D. Pascal \u2013 The Language and its Implementation . John Wiley, New York, 1981.\nCited on 158\nBarroso L. and H\u00f6lze U. The Datacenter as a Computer: An Introduction to the Design of\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "558 BIBLIOGRAPHY\nWarehouse-Scale Machines . Synthesis Lectures on Computer Architectures. Morgan\nand Claypool, San Rafael, CA, 2009. Cited on 117, 147\nBaryshnikov Y., Coffman E. G., Pierre G., Rubenstein D., Squillante M., and Yimwad-\nsana T. Predictability of Web-Server Traf\ufb01c Congestion. In 10th Web Caching Workshop ,\npages 97\u2013103. IEEE, Sept. 2005. Cited on 415\nBaset S. and Schulzrinne H. An Analysis of the Skype Peer-to-Peer Internet Telephony\nProtocol. In 25th INFOCOM Conference , pages 1\u201311, Los Alamitos, CA., Apr. 2006.\nIEEE, IEEE Computer Society Press. Cited on 20, 88\nBasile C., Whisnant K., Kalbarczyk Z., and Iyer R. K. Loose Synchronization of\nMultithreaded Replicas. In 21st Symposium on Reliable Distributed Systems , pages\n250\u2013255, Los Alamitos, CA., 2002. IEEE, IEEE Computer Society Press. Cited on 394\nBasile C., Kalbarczyk Z., and Iyer R. K. A Preemptive Deterministic Scheduling\nAlgorithm for Multithreaded Replicas. In International Conference on Dependable\nSystems and Networks , pages 149\u2013158, Los Alamitos, CA., June 2003. IEEE Computer\nSociety Press. Cited on 394\nBass L., Clements P ., and Kazman R. Software Architecture in Practice . Addison-Wesley,\nReading, MA., 2nd edition, 2003. Cited on 56, 76\nBavier A., Bowman M., Chun B., Culler D., Karlin S., Muir S., Peterson L., Roscoe\nT., Spalink T., and Wawrzoniak M. Operating System Support for Planetary-Scale\nNetwork Services. In 1st Symposium on Networked Systems Design and Implementation ,\npages 245\u2013266, Berkeley, CA, Mar. 2004. USENIX, USENIX. Cited on 149\nBen-Ari M. Principles of Concurrent and Distributed Programming . Prentice Hall, Engle-\nwood Cliffs, N.J., 2nd edition, 2006. Cited on 25\nBernstein P . Middleware: A Model for Distributed System Services. Communications of\nthe ACM , 39(2):87\u201398, Feb. 1996. Cited on 5, 34\nBernstein P . and Newcomer E. Principles of Transaction Processing . Morgan Kaufman,\nSan Mateo, CA., 2nd edition, 2009. Cited on 35, 484\nBernstein P ., Hadzilacos V ., and Goodman N. Concurrency Control and Recovery in\nDatabase Systems . Addison-Wesley, Reading, MA., 1987. Cited on 490\nBershad B., Zekauskas M., and Sawdon W. The Midway Distributed Shared Memory\nSystem. In COMPCON , pages 528\u2013537. IEEE, 1993. Cited on 371\nBharambe A. R., Agrawal M., and Seshan S. Mercury: Supporting Scalable Multi-\nAttribute Range Queries. In SIGCOMM , pages 353\u2013366, New York, NY, Aug. 2004.\nACM Press. Cited on 294\nBilal S. M., Bernardos C. J., and Guerrero C. Position-based Routing in Vehicular\nNetworks: A Survey. Journal of Network and Computer Applications , 36(2):685\u2013697,\nMar. 2013. Cited on 340\nBirman K. Guide to Reliable Distributed Systems: Building High-Assurance Applications\nand Cloud-Hosted Services . Springer-Verlag, Berlin, 2012. Cited on 131, 463\nBirman K. A Response to Cheriton and Skeen\u2019s Criticism of Causal and Totally Ordered\nCommunication. Operating Systems Review , 28(1):11\u201321, Jan. 1994. Cited on 321\nBirman K. and Joseph T. Reliable Communication in the Presence of Failures. ACM\nTransactions on Computer Systems , 5(1):47\u201376, Feb. 1987. Cited on 479\nBirman K. and Renesse R.van , editors. Reliable Distributed Computing with the Isis\nToolkit . IEEE Computer Society Press, Los Alamitos, CA., 1994. Cited on 321\nBirman K., Schiper A., and Stephenson P . Lightweight Causal and Atomic Group\nMulticast. ACM Transactions on Computer Systems , 9(3):272\u2013314, Aug. 1991. Cited on\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n558 BIBLIOGRAPHY\nWarehouse-Scale Machines . Synthesis Lectures on Computer Architectures. Morgan\nand Claypool, San Rafael, CA, 2009. Cited on 117, 147\nBaryshnikov Y., Coffman E. G., Pierre G., Rubenstein D., Squillante M., and Yimwad-\nsana T. Predictability of Web-Server Traf\ufb01c Congestion. In 10th Web Caching Workshop ,\npages 97\u2013103. IEEE, Sept. 2005. Cited on 415\nBaset S. and Schulzrinne H. An Analysis of the Skype Peer-to-Peer Internet Telephony\nProtocol. In 25th INFOCOM Conference , pages 1\u201311, Los Alamitos, CA., Apr. 2006.\nIEEE, IEEE Computer Society Press. Cited on 20, 88\nBasile C., Whisnant K., Kalbarczyk Z., and Iyer R. K. Loose Synchronization of\nMultithreaded Replicas. In 21st Symposium on Reliable Distributed Systems , pages\n250\u2013255, Los Alamitos, CA., 2002. IEEE, IEEE Computer Society Press. Cited on 394\nBasile C., Kalbarczyk Z., and Iyer R. K. A Preemptive Deterministic Scheduling\nAlgorithm for Multithreaded Replicas. In International Conference on Dependable\nSystems and Networks , pages 149\u2013158, Los Alamitos, CA., June 2003. IEEE Computer\nSociety Press. Cited on 394\nBass L., Clements P ., and Kazman R. Software Architecture in Practice . Addison-Wesley,\nReading, MA., 2nd edition, 2003. Cited on 56, 76\nBavier A., Bowman M., Chun B., Culler D., Karlin S., Muir S., Peterson L., Roscoe\nT., Spalink T., and Wawrzoniak M. Operating System Support for Planetary-Scale\nNetwork Services. In 1st Symposium on Networked Systems Design and Implementation ,\npages 245\u2013266, Berkeley, CA, Mar. 2004. USENIX, USENIX. Cited on 149\nBen-Ari M. Principles of Concurrent and Distributed Programming . Prentice Hall, Engle-\nwood Cliffs, N.J., 2nd edition, 2006. Cited on 25\nBernstein P . Middleware: A Model for Distributed System Services. Communications of\nthe ACM , 39(2):87\u201398, Feb. 1996. Cited on 5, 34\nBernstein P . and Newcomer E. Principles of Transaction Processing . Morgan Kaufman,\nSan Mateo, CA., 2nd edition, 2009. Cited on 35, 484\nBernstein P ., Hadzilacos V ., and Goodman N. Concurrency Control and Recovery in\nDatabase Systems . Addison-Wesley, Reading, MA., 1987. Cited on 490\nBershad B., Zekauskas M., and Sawdon W. The Midway Distributed Shared Memory\nSystem. In COMPCON , pages 528\u2013537. IEEE, 1993. Cited on 371\nBharambe A. R., Agrawal M., and Seshan S. Mercury: Supporting Scalable Multi-\nAttribute Range Queries. In SIGCOMM , pages 353\u2013366, New York, NY, Aug. 2004.\nACM Press. Cited on 294\nBilal S. M., Bernardos C. J., and Guerrero C. Position-based Routing in Vehicular\nNetworks: A Survey. Journal of Network and Computer Applications , 36(2):685\u2013697,\nMar. 2013. Cited on 340\nBirman K. Guide to Reliable Distributed Systems: Building High-Assurance Applications\nand Cloud-Hosted Services . Springer-Verlag, Berlin, 2012. Cited on 131, 463\nBirman K. A Response to Cheriton and Skeen\u2019s Criticism of Causal and Totally Ordered\nCommunication. Operating Systems Review , 28(1):11\u201321, Jan. 1994. Cited on 321\nBirman K. and Joseph T. Reliable Communication in the Presence of Failures. ACM\nTransactions on Computer Systems , 5(1):47\u201376, Feb. 1987. Cited on 479\nBirman K. and Renesse R.van , editors. Reliable Distributed Computing with the Isis\nToolkit . IEEE Computer Society Press, Los Alamitos, CA., 1994. Cited on 321\nBirman K., Schiper A., and Stephenson P . Lightweight Causal and Atomic Group\nMulticast. ACM Transactions on Computer Systems , 9(3):272\u2013314, Aug. 1991. Cited on\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 559\n482\nBirrell A. and Nelson B. Implementing Remote Procedure Calls. ACM Transactions on\nComputer Systems , 2(1):39\u201359, Feb. 1984. Cited on 173\nBishop M. Computer Security: Art and Science . Addison-Wesley, Reading, MA., 2003.\nCited on 506\nBlack A. and Artsy Y. Implementing Location Independent Invocation. IEEE Transac-\ntions on Parallel and Distributed Systems , 1(1):107\u2013119, Jan. 1990. Cited on 245\nBlair G. and Stefani J.-B. Open Distributed Processing and Multimedia . Addison-Wesley,\nReading, MA., 1998. Cited on 13\nBlake G., Dreslinski R. G., Mudge T., and Flautner K. Evolution of Thread-Level\nParallelism in Desktop Applications. SIGARCH Computer Architecture News , 38(3):\n302\u2013313, 2010. Cited on 113\nBlaze M. Caching in Large-Scale Distributed File Systems . PhD thesis, Department of\nComputer Science, Princeton University, Jan. 1993. Cited on 388\nBonnet P ., Gehrke J., and Seshadri P . Towards Sensor Database Systems. In 2nd\nInternational Conference on Mobile Data Management , volume 1987 of Lecture Notes in\nComputer Science , pages 3\u201314, Berlin, Jan. 2002. Springer-Verlag. Cited on 48\nBrewer E. CAP Twelve Years Later: How the \"Rules\" Have Changed. Computer , 45(2):\n23\u201329, Feb. 2012. Cited on 461, 462\nBudhijara N., Marzullo K., Schneider F., and Toueg S. The Primary-Backup Ap-\nproach. In Mullender S., editor, Distributed Systems , pages 199\u2013216. Addison-Wesley,\nWokingham, 2nd edition, 1993. Cited on 399\nBudhiraja N. and Marzullo K. Tradeoffs in Implementing Primary-Backup Protocols.\nTechnical Report TR 92-1307, Department of Computer Science, Cornell University,\n1992. Cited on 400\nBuford J. and Yu H. Peer-to-Peer Networking and Applications: Synopsis and Research\nDirections. In Shen et al. [2010], pages 3\u201345. Cited on 81\nBuford J., Yu H., and Lua E. P2P Networking and Applications . Morgan Kaufman, San\nMateo, CA., 2009. Cited on 81\nCabri G., Leonardi L., and Zambonelli F. Mobile-Agent Coordination Models for\nInternet Applications. Computer , 33(2):82\u201389, Feb. 2000. Cited on 67\nCachin C., Guerraoui R., and Rodrigues L. Introduction to Reliable and Secure Distributed\nProgramming . Springer-Verlag, Berlin, 2nd edition, 2011. Cited on 298, 430, 436, 437\nCallaghan B. NFS Illustrated . Addison-Wesley, Reading, MA., 2000. Cited on 94, 282\nCandea G., Brown A. B., Fox A., and Patterson D. Recovery-Oriented Computing:\nBuilding Multitier Dependability. Computer , 37(11):60\u201367, Nov. 2004a. Cited on 499\nCandea G., Kawamoto S., Fujiki Y., Friedman G., and Fox A. Microreboot: A Technique\nfor Cheap Recovery. In 6th Symposium on Operating System Design and Implementation ,\npages 31\u201344, Berkeley, CA, Dec. 2004b. USENIX, USENIX. Cited on 499\nCandea G., Kiciman E., Kawamoto S., and Fox A. Autonomous Recovery in Compo-\nnentized Internet Applications. Cluster Computing , 9(2):175\u2013190, Feb. 2006. Cited on\n499\nCantin J., Lipasti M., and Smith J. The Complexity of Verifying Memory Coherence\nand Consistency. IEEE Transactions on Parallel and Distributed Systems , 16(7):663\u2013671,\nJuly 2005. Cited on 372\nCao L. and Ozsu T. Evaluation of Strong Consistency Web Caching Techniques. World\nWide Web , 5(2):95\u2013123, June 2002. Cited on 411\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 559\n482\nBirrell A. and Nelson B. Implementing Remote Procedure Calls. ACM Transactions on\nComputer Systems , 2(1):39\u201359, Feb. 1984. Cited on 173\nBishop M. Computer Security: Art and Science . Addison-Wesley, Reading, MA., 2003.\nCited on 506\nBlack A. and Artsy Y. Implementing Location Independent Invocation. IEEE Transac-\ntions on Parallel and Distributed Systems , 1(1):107\u2013119, Jan. 1990. Cited on 245\nBlair G. and Stefani J.-B. Open Distributed Processing and Multimedia . Addison-Wesley,\nReading, MA., 1998. Cited on 13\nBlake G., Dreslinski R. G., Mudge T., and Flautner K. Evolution of Thread-Level\nParallelism in Desktop Applications. SIGARCH Computer Architecture News , 38(3):\n302\u2013313, 2010. Cited on 113\nBlaze M. Caching in Large-Scale Distributed File Systems . PhD thesis, Department of\nComputer Science, Princeton University, Jan. 1993. Cited on 388\nBonnet P ., Gehrke J., and Seshadri P . Towards Sensor Database Systems. In 2nd\nInternational Conference on Mobile Data Management , volume 1987 of Lecture Notes in\nComputer Science , pages 3\u201314, Berlin, Jan. 2002. Springer-Verlag. Cited on 48\nBrewer E. CAP Twelve Years Later: How the \"Rules\" Have Changed. Computer , 45(2):\n23\u201329, Feb. 2012. Cited on 461, 462\nBudhijara N., Marzullo K., Schneider F., and Toueg S. The Primary-Backup Ap-\nproach. In Mullender S., editor, Distributed Systems , pages 199\u2013216. Addison-Wesley,\nWokingham, 2nd edition, 1993. Cited on 399\nBudhiraja N. and Marzullo K. Tradeoffs in Implementing Primary-Backup Protocols.\nTechnical Report TR 92-1307, Department of Computer Science, Cornell University,\n1992. Cited on 400\nBuford J. and Yu H. Peer-to-Peer Networking and Applications: Synopsis and Research\nDirections. In Shen et al. [2010], pages 3\u201345. Cited on 81\nBuford J., Yu H., and Lua E. P2P Networking and Applications . Morgan Kaufman, San\nMateo, CA., 2009. Cited on 81\nCabri G., Leonardi L., and Zambonelli F. Mobile-Agent Coordination Models for\nInternet Applications. Computer , 33(2):82\u201389, Feb. 2000. Cited on 67\nCachin C., Guerraoui R., and Rodrigues L. Introduction to Reliable and Secure Distributed\nProgramming . Springer-Verlag, Berlin, 2nd edition, 2011. Cited on 298, 430, 436, 437\nCallaghan B. NFS Illustrated . Addison-Wesley, Reading, MA., 2000. Cited on 94, 282\nCandea G., Brown A. B., Fox A., and Patterson D. Recovery-Oriented Computing:\nBuilding Multitier Dependability. Computer , 37(11):60\u201367, Nov. 2004a. Cited on 499\nCandea G., Kawamoto S., Fujiki Y., Friedman G., and Fox A. Microreboot: A Technique\nfor Cheap Recovery. In 6th Symposium on Operating System Design and Implementation ,\npages 31\u201344, Berkeley, CA, Dec. 2004b. USENIX, USENIX. Cited on 499\nCandea G., Kiciman E., Kawamoto S., and Fox A. Autonomous Recovery in Compo-\nnentized Internet Applications. Cluster Computing , 9(2):175\u2013190, Feb. 2006. Cited on\n499\nCantin J., Lipasti M., and Smith J. The Complexity of Verifying Memory Coherence\nand Consistency. IEEE Transactions on Parallel and Distributed Systems , 16(7):663\u2013671,\nJuly 2005. Cited on 372\nCao L. and Ozsu T. Evaluation of Strong Consistency Web Caching Techniques. World\nWide Web , 5(2):95\u2013123, June 2002. Cited on 411\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "560 BIBLIOGRAPHY\nCardellini V ., Casalicchio E., Colajanni M., and Yu P . The State of the Art in Locally\nDistributed Web-Server Systems. ACM Computing Surveys , 34(2):263\u2013311, June 2002.\nCited on 143\nCarriero N. and Gelernter D. Linda in Context. Communications of the ACM , 32(4):\n444\u2013458, 1989. Cited on 68\nCarzaniga A., Rutherford M. J., and Wolf A. L. A Routing Scheme for Content-Based\nNetworking. In 23rd INFOCOM Conference , Los Alamitos, CA., Mar. 2004. IEEE,\nIEEE Computer Society Press. Cited on 346\nCarzaniga A., Picco G. P ., and Vigna G. Is Code Still Moving Around? Looking Back\nat a Decade of Code Mobility. In 29th International Conference on Software Engineering\n(companian) , pages 9\u201320, Los Alamitos, CA., 2007. IEEE Computer Society Press.\nCited on 153\nCastro M. and Liskov B. Practical Byzantine Fault Tolerance and Proactive Recovery.\nACM Transactions on Computer Systems , 20(4):398\u2013461, Nov. 2002. Cited on 456, 458\nCastro M., Druschel P ., Hu Y. C., and Rowstron A. Topology-aware Routing in\nStructured Peer-to-Peer Overlay Networks. Technical Report MSR-TR-2002-82,\nMicrosoft Research, Cambridge, UK, June 2002a. Cited on 250\nCastro M., Druschel P ., Kermarrec A.-M., and Rowstron A. Scribe: A Large-Scale and\nDecentralized Application-Level Multicast Infrastructure. IEEE Journal on Selected\nAreas in Communication , 20(8):100\u2013110, Oct. 2002b. Cited on 222, 223\nCastro M., Rodrigues R., and Liskov B. BASE: Using Abstraction to Improve Fault\nTolerance. ACM Transactions on Computer Systems , 21(3):236\u2013269, Aug. 2003. Cited\non 458\nCastro M., Costa M., and Rowstron A. Debunking Some Myths about Structured\nand Unstructured Overlays. In 2nd Symposium on Networked Systems Design and\nImplementation , Berkeley, CA, Mar. 2005. USENIX, USENIX. Cited on 352\nChandra T., Griesemer R., and Redstone J. Paxos Made Live: An Engineering Perspec-\ntive. In 26th Symposium on Principles of Distributed Computing , pages 398\u2013407, New\nYork, NY, Aug. 2007. ACM, ACM Press. Cited on 442\nChaudhari S. S. and Biradar R. C. Survey of Bandwidth Estimation Techniques in\nCommunication Networks. Wireless Personal Communications , 83(2):1425\u20131476, 2015.\nCited on 413\nCheriton D. and Mann T. Decentralizing a Global Naming Service for Improved\nPerformance and Fault Tolerance. ACM Transactions on Computer Systems , 7(2):\n147\u2013183, May 1989. Cited on 264\nCheriton D. and Skeen D. Understanding the Limitations of Causally and Totally\nOrdered Communication. In 14th Symposium on Operating System Principles , pages\n44\u201357. ACM, Dec. 1993. Cited on 321\nCheswick W. and Bellovin S. Firewalls and Internet Security . Addison-Wesley, Reading,\nMA., 2nd edition, 2000. Cited on 533\nChisnall D. The De\ufb01nitive Guide to the Xen Hypervisor . Prentice Hall, Englewood Cliffs,\nN.J., 2007. Cited on 122\nChondros N., Kokordelis K., and Roussopoulos M. On the Practicality of Practical\nByzantine Fault Tolerance. In Middleware 2012 , volume 7662 of Lecture Notes in\nComputer Science , pages 436\u2013455, Berlin, 2012. ACM/IFIP/USENIX, Springer-Verlag.\nCited on 458\nChow R. and Johnson T. Distributed Operating Systems and Algorithms . Addison-Wesley,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n560 BIBLIOGRAPHY\nCardellini V ., Casalicchio E., Colajanni M., and Yu P . The State of the Art in Locally\nDistributed Web-Server Systems. ACM Computing Surveys , 34(2):263\u2013311, June 2002.\nCited on 143\nCarriero N. and Gelernter D. Linda in Context. Communications of the ACM , 32(4):\n444\u2013458, 1989. Cited on 68\nCarzaniga A., Rutherford M. J., and Wolf A. L. A Routing Scheme for Content-Based\nNetworking. In 23rd INFOCOM Conference , Los Alamitos, CA., Mar. 2004. IEEE,\nIEEE Computer Society Press. Cited on 346\nCarzaniga A., Picco G. P ., and Vigna G. Is Code Still Moving Around? Looking Back\nat a Decade of Code Mobility. In 29th International Conference on Software Engineering\n(companian) , pages 9\u201320, Los Alamitos, CA., 2007. IEEE Computer Society Press.\nCited on 153\nCastro M. and Liskov B. Practical Byzantine Fault Tolerance and Proactive Recovery.\nACM Transactions on Computer Systems , 20(4):398\u2013461, Nov. 2002. Cited on 456, 458\nCastro M., Druschel P ., Hu Y. C., and Rowstron A. Topology-aware Routing in\nStructured Peer-to-Peer Overlay Networks. Technical Report MSR-TR-2002-82,\nMicrosoft Research, Cambridge, UK, June 2002a. Cited on 250\nCastro M., Druschel P ., Kermarrec A.-M., and Rowstron A. Scribe: A Large-Scale and\nDecentralized Application-Level Multicast Infrastructure. IEEE Journal on Selected\nAreas in Communication , 20(8):100\u2013110, Oct. 2002b. Cited on 222, 223\nCastro M., Rodrigues R., and Liskov B. BASE: Using Abstraction to Improve Fault\nTolerance. ACM Transactions on Computer Systems , 21(3):236\u2013269, Aug. 2003. Cited\non 458\nCastro M., Costa M., and Rowstron A. Debunking Some Myths about Structured\nand Unstructured Overlays. In 2nd Symposium on Networked Systems Design and\nImplementation , Berkeley, CA, Mar. 2005. USENIX, USENIX. Cited on 352\nChandra T., Griesemer R., and Redstone J. Paxos Made Live: An Engineering Perspec-\ntive. In 26th Symposium on Principles of Distributed Computing , pages 398\u2013407, New\nYork, NY, Aug. 2007. ACM, ACM Press. Cited on 442\nChaudhari S. S. and Biradar R. C. Survey of Bandwidth Estimation Techniques in\nCommunication Networks. Wireless Personal Communications , 83(2):1425\u20131476, 2015.\nCited on 413\nCheriton D. and Mann T. Decentralizing a Global Naming Service for Improved\nPerformance and Fault Tolerance. ACM Transactions on Computer Systems , 7(2):\n147\u2013183, May 1989. Cited on 264\nCheriton D. and Skeen D. Understanding the Limitations of Causally and Totally\nOrdered Communication. In 14th Symposium on Operating System Principles , pages\n44\u201357. ACM, Dec. 1993. Cited on 321\nCheswick W. and Bellovin S. Firewalls and Internet Security . Addison-Wesley, Reading,\nMA., 2nd edition, 2000. Cited on 533\nChisnall D. The De\ufb01nitive Guide to the Xen Hypervisor . Prentice Hall, Englewood Cliffs,\nN.J., 2007. Cited on 122\nChondros N., Kokordelis K., and Roussopoulos M. On the Practicality of Practical\nByzantine Fault Tolerance. In Middleware 2012 , volume 7662 of Lecture Notes in\nComputer Science , pages 436\u2013455, Berlin, 2012. ACM/IFIP/USENIX, Springer-Verlag.\nCited on 458\nChow R. and Johnson T. Distributed Operating Systems and Algorithms . Addison-Wesley,\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 561\nReading, MA., 1997. Cited on 493\nChu Y., Rao S. G., Seshan S., and Zhang H. A Case for End System Multicast. IEEE\nJournal on Selected Areas in Communication , 20(8):1456\u20131471, Oct. 2002. Cited on 223\nClark C., Fraser K., Hand S., Hansen J. G., Jul E., Limpach C., Pratt I., and War\ufb01eld A.\nLive Migration of Virtual Machines. In 2nd Symposium on Networked Systems Design\nand Implementation , Berkeley, CA, May 2005. USENIX, USENIX. Cited on 158, 159\nClark D. The Design Philosophy of the DARPA Internet Protocols. In SIGCOMM ,\npages 106\u2013114, New York, NY, Sept. 1989. ACM, ACM Press. Cited on 131\nCohen B. Incentives Build Robustness in Bittorrent. In 1st Workshop on Economics of\nPeer-to-Peer Systems , June 2003. Cited on 91\nCohen E. and Shenker S. Replication Strategies in Unstructured Peer-to-Peer Networks.\nInSIGCOMM , pages 177\u2013190, New York, NY, Aug. 2002. ACM, ACM Press. Cited\non 86\nComer D. Internetworking with TCP/IP , Volume I: Principles, Protocols, and Architecture .\nPrentice Hall, Upper Saddle River, N.J., 6th edition, 2013. Cited on 168\nConti M., Gregori E., and Lapenna W. Content Delivery Policies in ReplicatedWeb\nServices: Client-Side vs. Server-Side. Cluster Computing , 8:47\u201360, Jan. 2005. Cited on\n417\nCorbett J. C., Dean J., Epstein M., Fikes A., Frost C., Furman J. J., Ghemawat S., Gubarev\nA., Heiser C., Hochschild P ., Hsieh W., Kanthak S., Kogan E., Li H., Lloyd A., Melnik\nS., Mwaura D., Nagle D., Quinlan S., Rao R., Rolig L., Saito Y., Szymaniak M.,\nTaylor C., Wang R., and Woodford D. Spanner: Google&rsquo;s globally distributed\ndatabase. ACM Transactions on Computer Systems , 31(3):8:1\u20138:22, Aug. 2013. Cited on\n309, 310\nCristian F. Probabilistic Clock Synchronization. Distributed Computing , 3:146\u2013158, 1989.\nCited on 304\nCristian F. Understanding Fault-Tolerant Distributed Systems. Communications of the\nACM , 34(2):56\u201378, Feb. 1991. Cited on 427\nDabek F., Cox R., Kaashoek F., and Morris R. Vivaldi: A Decentralized Network\nCoordinate System. In SIGCOMM , New York, NY, Aug. 2004a. ACM, ACM Press.\nCited on 342, 343\nDabek F., Li J., Sit E., Robertson J., Kaashoek M. F., and Morris R. Designing a dht for\nlow latency and high throughput. In 1st Symposium on Networked Systems Design and\nImplementation , pages 85\u201398, Berkeley, CA, Mar. 2004b. USENIX, USENIX. Cited on\n251\nDavies S. and Broadhurst P . WebSphere MQ V6 Fundamentals . Redbooks. IBM, Nov.\n2005. Cited on 212\nDay J. and Zimmerman H. The OSI Reference Model. Proceedings of the IEEE , 71(12):\n1334\u20131340, Dec. 1983. Cited on 164\nDeering S. and Cheriton D. Multicast Routing in Datagram Internetworks and Extended\nLANs. ACM Transactions on Computer Systems , 8(2):85\u2013110, May 1990. Cited on 242\nDeering S., Estrin D., Farinacci D., Jacobson V ., Liu C.-G., and Wei L. The PIM\nArchitecture for Wide-Area Multicast Routing. IEEE/ACM Transactions on Networking ,\n4(2):153\u2013162, Apr. 1996. Cited on 242\nDemers A., Greene D., Hauser C., Irish W., Larson J., Shenker S., Sturgis H., Swinehart\nD., and Terry D. Epidemic Algorithms for Replicated Database Maintenance. In 6th\nSymposium on Principles of Distributed Computing , pages 1\u201312. ACM, Aug. 1987. Cited\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 561\nReading, MA., 1997. Cited on 493\nChu Y., Rao S. G., Seshan S., and Zhang H. A Case for End System Multicast. IEEE\nJournal on Selected Areas in Communication , 20(8):1456\u20131471, Oct. 2002. Cited on 223\nClark C., Fraser K., Hand S., Hansen J. G., Jul E., Limpach C., Pratt I., and War\ufb01eld A.\nLive Migration of Virtual Machines. In 2nd Symposium on Networked Systems Design\nand Implementation , Berkeley, CA, May 2005. USENIX, USENIX. Cited on 158, 159\nClark D. The Design Philosophy of the DARPA Internet Protocols. In SIGCOMM ,\npages 106\u2013114, New York, NY, Sept. 1989. ACM, ACM Press. Cited on 131\nCohen B. Incentives Build Robustness in Bittorrent. In 1st Workshop on Economics of\nPeer-to-Peer Systems , June 2003. Cited on 91\nCohen E. and Shenker S. Replication Strategies in Unstructured Peer-to-Peer Networks.\nInSIGCOMM , pages 177\u2013190, New York, NY, Aug. 2002. ACM, ACM Press. Cited\non 86\nComer D. Internetworking with TCP/IP , Volume I: Principles, Protocols, and Architecture .\nPrentice Hall, Upper Saddle River, N.J., 6th edition, 2013. Cited on 168\nConti M., Gregori E., and Lapenna W. Content Delivery Policies in ReplicatedWeb\nServices: Client-Side vs. Server-Side. Cluster Computing , 8:47\u201360, Jan. 2005. Cited on\n417\nCorbett J. C., Dean J., Epstein M., Fikes A., Frost C., Furman J. J., Ghemawat S., Gubarev\nA., Heiser C., Hochschild P ., Hsieh W., Kanthak S., Kogan E., Li H., Lloyd A., Melnik\nS., Mwaura D., Nagle D., Quinlan S., Rao R., Rolig L., Saito Y., Szymaniak M.,\nTaylor C., Wang R., and Woodford D. Spanner: Google&rsquo;s globally distributed\ndatabase. ACM Transactions on Computer Systems , 31(3):8:1\u20138:22, Aug. 2013. Cited on\n309, 310\nCristian F. Probabilistic Clock Synchronization. Distributed Computing , 3:146\u2013158, 1989.\nCited on 304\nCristian F. Understanding Fault-Tolerant Distributed Systems. Communications of the\nACM , 34(2):56\u201378, Feb. 1991. Cited on 427\nDabek F., Cox R., Kaashoek F., and Morris R. Vivaldi: A Decentralized Network\nCoordinate System. In SIGCOMM , New York, NY, Aug. 2004a. ACM, ACM Press.\nCited on 342, 343\nDabek F., Li J., Sit E., Robertson J., Kaashoek M. F., and Morris R. Designing a dht for\nlow latency and high throughput. In 1st Symposium on Networked Systems Design and\nImplementation , pages 85\u201398, Berkeley, CA, Mar. 2004b. USENIX, USENIX. Cited on\n251\nDavies S. and Broadhurst P . WebSphere MQ V6 Fundamentals . Redbooks. IBM, Nov.\n2005. Cited on 212\nDay J. and Zimmerman H. The OSI Reference Model. Proceedings of the IEEE , 71(12):\n1334\u20131340, Dec. 1983. Cited on 164\nDeering S. and Cheriton D. Multicast Routing in Datagram Internetworks and Extended\nLANs. ACM Transactions on Computer Systems , 8(2):85\u2013110, May 1990. Cited on 242\nDeering S., Estrin D., Farinacci D., Jacobson V ., Liu C.-G., and Wei L. The PIM\nArchitecture for Wide-Area Multicast Routing. IEEE/ACM Transactions on Networking ,\n4(2):153\u2013162, Apr. 1996. Cited on 242\nDemers A., Greene D., Hauser C., Irish W., Larson J., Shenker S., Sturgis H., Swinehart\nD., and Terry D. Epidemic Algorithms for Replicated Database Maintenance. In 6th\nSymposium on Principles of Distributed Computing , pages 1\u201312. ACM, Aug. 1987. Cited\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "562 BIBLIOGRAPHY\non 229, 231, 233\nDemirbas M. and Kulkarni S. Beyond TrueTime: Using Augmented Time for Improving\nGoogle Spanner. In 7th International Workshop on Large Scale Distributed Systems and\nMiddleware , New York, NY, 2013. ACM Press. Cited on 310\nDey A. Context-Aware Computing. In Krumm J., editor, Ubiquitous Computing Funda-\nmentals , pages 321\u2013352. CRC Press, Boca Raton, FL, 2010. Cited on 42\nDey A. and Abowd G. Towards a Better Understanding of Context and Contex-\nAwareness. In Workshop on the What, Who, Where, When, Why and How of Context-\nAwareness , New York, NY, Apr. 2000. ACM, ACM Press. Cited on 42\nDif\ufb01e W. and Hellman M. New Directions in Cryptography. IEEE Transactions on\nInformation Theory , IT-22(6):644\u2013654, Nov. 1976. Cited on 542\nDilley J., Maggs B., Parikh J., Prokop H., Sitaraman R., and Weihl B. Globally Dis-\ntributed Content Delivery. IEEE Internet Computing , 6(5):50\u201358, Sept. 2002. Cited on\n415\nDiot C., Levine B., Lyles B., Kassem H., and Balensiefen D. Deployment Issues for the\nIP Multicast Service and Architecture. IEEE Network , 14(1):78\u201388, Jan. 2000. Cited\non 221\nDonnet B., Gueye B., and Kaafar M. A Survey on Network Coordinates Systems,\nDesign, and Security. IEEE Communications Surveys & Tutorials , 12(4), Dec. 2010.\nCited on 340\nDoorn J. H. and Rivero L. C., editors. Database Integrity: Challenges and Solutions . Idea\nGroup, Hershey, PA, 2002. Cited on 504\nDroms R. Dynamic Host Con\ufb01guration Protocol. RFC 2161, Apr. 1997. Cited on 43\nDubois M., Scheurich C., and Briggs F. Synchronization, Coherence, and Event\nOrdering in Multiprocessors. Computer , 21(2):9\u201321, Feb. 1988. Cited on 365\nDunagan J., Harvey N. J. A., Jones M. B., Kostic D., Theimer M., and Wolman A.\nFUSE: Lightweight Guaranteed Distributed Failure Noti\ufb01cation. In 6th Symposium\non Operating System Design and Implementation , Berkeley, CA, Dec. 2004. USENIX,\nUSENIX. Cited on 463\nDuvvuri V ., Shenoy P ., and Tewari R. Adaptive Leases: A Strong Consistency Mecha-\nnism for the World Wide Web. IEEE Transactions on Knowledge and Data Engineering ,\n15(5):1266\u20131276, Sept. 2003. Cited on 391\nEddon G. and Eddon H. Inside Distributed COM . Microsoft Press, Redmond, WA, 1998.\nCited on 188\nEl-Sayed A., Roca V ., and Mathy L. A Survey of Proposals for an Alternative Group\nCommunication Service. IEEE Network , 17(1):46\u201351, Jan. 2003. Cited on 222\nElnozahy E., Alvisi L., Wang Y.-M., and Johnson D. A Survey of Rollback-Recovery\nProtocols in Message-Passing Systems. ACM Computing Surveys , 34(3):375\u2013408, Sept.\n2002. Cited on 493, 498\nElnozahy E. N. and Plank J. S. Checkpointing for Peta-Scale Systems: A Look into the\nFuture of Practical Rollback-Recovery. IEEE Transactions on Dependable and Secure\nComputing , 1(2):97\u2013108, Apr. 2004. Cited on 496\nElson J., Girod L., and Estrin D. Fine-Grained Network Time Synchronization using\nReference Broadcasts. In 5th Symposium on Operating System Design and Implementa-\ntion, pages 147\u2013163, New York, NY, Dec. 2002. USENIX, ACM Press. Cited on 307,\n308\nEngelmann C., Ong H., and Scott S. Middleware in Modern High Performance\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n562 BIBLIOGRAPHY\non 229, 231, 233\nDemirbas M. and Kulkarni S. Beyond TrueTime: Using Augmented Time for Improving\nGoogle Spanner. In 7th International Workshop on Large Scale Distributed Systems and\nMiddleware , New York, NY, 2013. ACM Press. Cited on 310\nDey A. Context-Aware Computing. In Krumm J., editor, Ubiquitous Computing Funda-\nmentals , pages 321\u2013352. CRC Press, Boca Raton, FL, 2010. Cited on 42\nDey A. and Abowd G. Towards a Better Understanding of Context and Contex-\nAwareness. In Workshop on the What, Who, Where, When, Why and How of Context-\nAwareness , New York, NY, Apr. 2000. ACM, ACM Press. Cited on 42\nDif\ufb01e W. and Hellman M. New Directions in Cryptography. IEEE Transactions on\nInformation Theory , IT-22(6):644\u2013654, Nov. 1976. Cited on 542\nDilley J., Maggs B., Parikh J., Prokop H., Sitaraman R., and Weihl B. Globally Dis-\ntributed Content Delivery. IEEE Internet Computing , 6(5):50\u201358, Sept. 2002. Cited on\n415\nDiot C., Levine B., Lyles B., Kassem H., and Balensiefen D. Deployment Issues for the\nIP Multicast Service and Architecture. IEEE Network , 14(1):78\u201388, Jan. 2000. Cited\non 221\nDonnet B., Gueye B., and Kaafar M. A Survey on Network Coordinates Systems,\nDesign, and Security. IEEE Communications Surveys & Tutorials , 12(4), Dec. 2010.\nCited on 340\nDoorn J. H. and Rivero L. C., editors. Database Integrity: Challenges and Solutions . Idea\nGroup, Hershey, PA, 2002. Cited on 504\nDroms R. Dynamic Host Con\ufb01guration Protocol. RFC 2161, Apr. 1997. Cited on 43\nDubois M., Scheurich C., and Briggs F. Synchronization, Coherence, and Event\nOrdering in Multiprocessors. Computer , 21(2):9\u201321, Feb. 1988. Cited on 365\nDunagan J., Harvey N. J. A., Jones M. B., Kostic D., Theimer M., and Wolman A.\nFUSE: Lightweight Guaranteed Distributed Failure Noti\ufb01cation. In 6th Symposium\non Operating System Design and Implementation , Berkeley, CA, Dec. 2004. USENIX,\nUSENIX. Cited on 463\nDuvvuri V ., Shenoy P ., and Tewari R. Adaptive Leases: A Strong Consistency Mecha-\nnism for the World Wide Web. IEEE Transactions on Knowledge and Data Engineering ,\n15(5):1266\u20131276, Sept. 2003. Cited on 391\nEddon G. and Eddon H. Inside Distributed COM . Microsoft Press, Redmond, WA, 1998.\nCited on 188\nEl-Sayed A., Roca V ., and Mathy L. A Survey of Proposals for an Alternative Group\nCommunication Service. IEEE Network , 17(1):46\u201351, Jan. 2003. Cited on 222\nElnozahy E., Alvisi L., Wang Y.-M., and Johnson D. A Survey of Rollback-Recovery\nProtocols in Message-Passing Systems. ACM Computing Surveys , 34(3):375\u2013408, Sept.\n2002. Cited on 493, 498\nElnozahy E. N. and Plank J. S. Checkpointing for Peta-Scale Systems: A Look into the\nFuture of Practical Rollback-Recovery. IEEE Transactions on Dependable and Secure\nComputing , 1(2):97\u2013108, Apr. 2004. Cited on 496\nElson J., Girod L., and Estrin D. Fine-Grained Network Time Synchronization using\nReference Broadcasts. In 5th Symposium on Operating System Design and Implementa-\ntion, pages 147\u2013163, New York, NY, Dec. 2002. USENIX, ACM Press. Cited on 307,\n308\nEngelmann C., Ong H., and Scott S. Middleware in Modern High Performance\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 563\nComputing System Architectures. In International Conference on Computational Science ,\nvolume 4488 of Lecture Notes in Computer Science , pages 784\u2013791, Berlin, May 2007.\nSpringer-Verlag. Cited on 27\nErd\u00f6s P . and R\u00e9nyi A. On Random Graphs. Publicationes Mathematicae , 6:290\u2013297, 1959.\nCited on 226\nEscriva R., Wong B., and Sirer E. G. HyperDex: A Distributed, Searchable Key-value\nStore. In SIGCOMM , pages 25\u201336, New York, NY, 2012. ACM Press. Cited on 294\nEsposito C., Cotroneo D., and Russo S. On Reliability in Publish/Subscribe Services.\nComputer Networks , X(0):xxx, 2013. Cited on 473\nEugster P ., Guerraoui R., Kermarrec A.-M., and Massouli\u00e9 L. Epidemic Information\nDissemination in Distributed Systems. Computer , 37(5):60\u201367, May 2004. Cited on\n229\nFerguson N., Schneier B., and Kohno T. Cryptography Engineering: Design Principles and\nPractical Applications . John Wiley, New York, 2010. Cited on 512, 516\nFielding R. Architectural Styles and the Design of Network-based Software Architectures .\nPh.d., University of California, Irvine, 2000. Cited on 64\nFielding R., Gettys J., Mogul J., Frystyk H., Masinter L., Leach P ., and Berners-Lee T.\nHypertext Transfer Protocol \u2013 HTTP/1.1. RFC 2616, June 1999. Cited on 170\nFischer M., Lynch N., and Patterson M. Impossibility of Distributed Consensus with\none Faulty Processor. Journal of the ACM , 32(2):374\u2013382, Apr. 1985. Cited on 459, 460\nFloyd S., Jacobson V ., McCanne S., Liu C.-G., and Zhang L. A Reliable Multicast\nFramework for Light-weight Sessions and Application Level Framing. IEEE/ACM\nTransactions on Networking , 5(6):784\u2013803, Dec. 1997. Cited on 474, 475\nFokkink W. Distributed Algorithms: An Intuitive Approach . MIT Press, Cambridge, MA.,\n2013. Cited on 298\nFoster I., Kesselman C., and Tuecke S. The Anatomy of the Grid, Enabling Scalable\nVirtual Organizations. Journal of Supercomputer Applications , 15(3):200\u2013222, Fall 2001.\nCited on 28, 29\nFoster I. and others . The Open Grid Services Architecture, Version 1.5. GGF Informa-\ntional Document GFD-I.080, June 2006. Cited on 29\nFowler R. Decentralized Object Finding Using Forwarding Addresses . Ph.D., University of\nWashington, Seattle, 1985. Cited on 243\nFox A. and Brewer E. Harvest, Yield, and Scalable Tolerant Systems. In 7th WorksopHot\nTopics in Operating Systems , pages 174\u2013178, Los Alamitos, CA., Mar. 1999. IEEE, IEEE\nComputer Society Press. Cited on 461\nFranklin M. J., Carey M. J., and Livny M. Transactional Client-Server Cache Consistency:\nAlternatives and Performance. ACM Transactions on Database Systems , 22(3):315\u2013363,\nSept. 1997. Cited on 404\nFranqueira V . N. L. and Wieringa R. J. Role-based Access Control in Retrospect.\nComputer , 45(6):81\u201388, 2012. Cited on 549\nFu K., Kaashoek M. F., and Mazi\u00e8res D. Fast and Secure Distributed Read-Only File\nSystem. In 4th Symposium on Operating System Design and Implementation , San Diego,\nCA, Oct. 2000. USENIX. Cited on 540\nFuggetta A., Picco G. P ., and Vigna G. Understanding Code Mobility. IEEE Transactions\non Software Engineering , 24(5):342\u2013361, May 1998. Cited on 155, 156\nGamma E., Helm R., Johnson R., and Vlissides J. Design Patterns, Elements of Reusable\nObject-Oriented Software . Addison-Wesley, Reading, MA., 1994. Cited on 72, 533\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 563\nComputing System Architectures. In International Conference on Computational Science ,\nvolume 4488 of Lecture Notes in Computer Science , pages 784\u2013791, Berlin, May 2007.\nSpringer-Verlag. Cited on 27\nErd\u00f6s P . and R\u00e9nyi A. On Random Graphs. Publicationes Mathematicae , 6:290\u2013297, 1959.\nCited on 226\nEscriva R., Wong B., and Sirer E. G. HyperDex: A Distributed, Searchable Key-value\nStore. In SIGCOMM , pages 25\u201336, New York, NY, 2012. ACM Press. Cited on 294\nEsposito C., Cotroneo D., and Russo S. On Reliability in Publish/Subscribe Services.\nComputer Networks , X(0):xxx, 2013. Cited on 473\nEugster P ., Guerraoui R., Kermarrec A.-M., and Massouli\u00e9 L. Epidemic Information\nDissemination in Distributed Systems. Computer , 37(5):60\u201367, May 2004. Cited on\n229\nFerguson N., Schneier B., and Kohno T. Cryptography Engineering: Design Principles and\nPractical Applications . John Wiley, New York, 2010. Cited on 512, 516\nFielding R. Architectural Styles and the Design of Network-based Software Architectures .\nPh.d., University of California, Irvine, 2000. Cited on 64\nFielding R., Gettys J., Mogul J., Frystyk H., Masinter L., Leach P ., and Berners-Lee T.\nHypertext Transfer Protocol \u2013 HTTP/1.1. RFC 2616, June 1999. Cited on 170\nFischer M., Lynch N., and Patterson M. Impossibility of Distributed Consensus with\none Faulty Processor. Journal of the ACM , 32(2):374\u2013382, Apr. 1985. Cited on 459, 460\nFloyd S., Jacobson V ., McCanne S., Liu C.-G., and Zhang L. A Reliable Multicast\nFramework for Light-weight Sessions and Application Level Framing. IEEE/ACM\nTransactions on Networking , 5(6):784\u2013803, Dec. 1997. Cited on 474, 475\nFokkink W. Distributed Algorithms: An Intuitive Approach . MIT Press, Cambridge, MA.,\n2013. Cited on 298\nFoster I., Kesselman C., and Tuecke S. The Anatomy of the Grid, Enabling Scalable\nVirtual Organizations. Journal of Supercomputer Applications , 15(3):200\u2013222, Fall 2001.\nCited on 28, 29\nFoster I. and others . The Open Grid Services Architecture, Version 1.5. GGF Informa-\ntional Document GFD-I.080, June 2006. Cited on 29\nFowler R. Decentralized Object Finding Using Forwarding Addresses . Ph.D., University of\nWashington, Seattle, 1985. Cited on 243\nFox A. and Brewer E. Harvest, Yield, and Scalable Tolerant Systems. In 7th WorksopHot\nTopics in Operating Systems , pages 174\u2013178, Los Alamitos, CA., Mar. 1999. IEEE, IEEE\nComputer Society Press. Cited on 461\nFranklin M. J., Carey M. J., and Livny M. Transactional Client-Server Cache Consistency:\nAlternatives and Performance. ACM Transactions on Database Systems , 22(3):315\u2013363,\nSept. 1997. Cited on 404\nFranqueira V . N. L. and Wieringa R. J. Role-based Access Control in Retrospect.\nComputer , 45(6):81\u201388, 2012. Cited on 549\nFu K., Kaashoek M. F., and Mazi\u00e8res D. Fast and Secure Distributed Read-Only File\nSystem. In 4th Symposium on Operating System Design and Implementation , San Diego,\nCA, Oct. 2000. USENIX. Cited on 540\nFuggetta A., Picco G. P ., and Vigna G. Understanding Code Mobility. IEEE Transactions\non Software Engineering , 24(5):342\u2013361, May 1998. Cited on 155, 156\nGamma E., Helm R., Johnson R., and Vlissides J. Design Patterns, Elements of Reusable\nObject-Oriented Software . Addison-Wesley, Reading, MA., 1994. Cited on 72, 533\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "564 BIBLIOGRAPHY\nGarbacki P ., Epema D., and Steen M.van . The Design and Evaluation of a Self-\nOrganizing Super-Peer Network. IEEE Transactions on Computers , 59(3):317\u2013331, Mar.\n2010. Cited on 88\nGarcia-Molina H. Elections in a Distributed Computing System. IEEE Transactions on\nComputers , 31(1):48\u201359, Jan. 1982. Cited on 330\nGarman J. Kerberos: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol, CA., 2003.\nCited on 527\nGelernter D. and Carriero N. Coordination Languages and their Signi\ufb01cance. Commu-\nnications of the ACM , 35(2):96\u2013107, Feb. 1992. Cited on 67\nGhodsi A. Multicast and Bulk Lookup in Structured Overlay Networks. In Shen et al.\n[2010], pages 933\u2013958. Cited on 228\nGhodsi A., Koponen T., Rajahalme J., Sarolahti P ., and Shenker S. Naming in Content-\noriented Architectures. In ACM SIGCOMM Workshop on Information-centric Network-\ning, pages 1\u20136, New York, NY, 2011. ACM Press. Cited on 541\nGifford D. Weighted Voting for Replicated Data. In 7th Symposium on Operating System\nPrinciples , pages 150\u2013162. ACM, Dec. 1979. Cited on 402\nGilbert S. and Lynch N. Brewer\u2019s Conjecture and the Feasibility of Consistent, Available,\nPartition-tolerant Web Services. ACM SIGACT News , 33(2):51\u201359, June 2002. Cited\non 461\nGilbert S. and Lynch N. Perspectives on the CAP Theorem. Computer , 45(2):30\u201335, Feb.\n2012. Cited on 461\nGkantsidis C., Mihail M., and Saberi A. Random Walks in Peer-to-Peer Networks:\nAlgorithms and Evaluation. Performance Evaluation , 63:241\u2013263, Mar. 2006. Cited on\n85\nGladney H. Access Control for Large Collections. ACM Transactions on Information\nSystems , 15(2):154\u2013194, Apr. 1997. Cited on 533\nGollmann D. Computer Security . John Wiley, New York, 2nd edition, 2006. Cited on 504\nGong L. and Schemers R. Implementing Protection Domains in the Java Development\nKit 1.2. In Symposium on Network and Distributed System Security , pages 125\u2013134, San\nDiego, CA, Mar. 1998. Internet Society. Cited on 538\nGonz\u00e1lez M. C., Hidalgo C. A., and Barab\u00e1si A.-L. Understanding Individual Human\nMobility Patterns. Nature , 453:779\u2013782, June 2008. Cited on 46\nGray C. and Cheriton D. Leases: An Ef\ufb01cient Fault-Tolerant Mechanism for Distributed\nFile Cache Consistency. In 12th Symposium on Operating System Principles , pages\n202\u2013210, New York, NY, Dec. 1989. ACM, ACM Press. Cited on 391\nGray J. Notes on Database Operating Systems. In Bayer R., Graham R., and Seegmuller\nG., editors, Operating Systems: An Advanced Course , volume 60 of Lecture Notes in\nComputer Science , pages 393\u2013481. Springer-Verlag, Berlin, 1978. Cited on 484\nGray J. and Reuter A. Transaction Processing: Concepts and Techniques . Morgan Kaufman,\nSan Mateo, CA., 1993. Cited on 35\nGray J., Helland P ., O\u2019Neil P ., and Sashna D. The Dangers of Replication and a Solution.\nInSIGMOD International Conference on Management Of Data , pages 173\u2013182, Montreal,\nJune 1996. ACM. Cited on 358\nGropp W., Lusk E., and Skjellum A. Using MPI-2, Portable Parallel Programming with the\nMessage-Passing Interface . MIT Press, Cambridge, MA., 3rd edition, 2016. Cited on\n206\nGroup A. W. AMQP , Protocol speci\ufb01cation, Version 0-9-1, Nov. 2008. Cited on 218\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n564 BIBLIOGRAPHY\nGarbacki P ., Epema D., and Steen M.van . The Design and Evaluation of a Self-\nOrganizing Super-Peer Network. IEEE Transactions on Computers , 59(3):317\u2013331, Mar.\n2010. Cited on 88\nGarcia-Molina H. Elections in a Distributed Computing System. IEEE Transactions on\nComputers , 31(1):48\u201359, Jan. 1982. Cited on 330\nGarman J. Kerberos: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol, CA., 2003.\nCited on 527\nGelernter D. and Carriero N. Coordination Languages and their Signi\ufb01cance. Commu-\nnications of the ACM , 35(2):96\u2013107, Feb. 1992. Cited on 67\nGhodsi A. Multicast and Bulk Lookup in Structured Overlay Networks. In Shen et al.\n[2010], pages 933\u2013958. Cited on 228\nGhodsi A., Koponen T., Rajahalme J., Sarolahti P ., and Shenker S. Naming in Content-\noriented Architectures. In ACM SIGCOMM Workshop on Information-centric Network-\ning, pages 1\u20136, New York, NY, 2011. ACM Press. Cited on 541\nGifford D. Weighted Voting for Replicated Data. In 7th Symposium on Operating System\nPrinciples , pages 150\u2013162. ACM, Dec. 1979. Cited on 402\nGilbert S. and Lynch N. Brewer\u2019s Conjecture and the Feasibility of Consistent, Available,\nPartition-tolerant Web Services. ACM SIGACT News , 33(2):51\u201359, June 2002. Cited\non 461\nGilbert S. and Lynch N. Perspectives on the CAP Theorem. Computer , 45(2):30\u201335, Feb.\n2012. Cited on 461\nGkantsidis C., Mihail M., and Saberi A. Random Walks in Peer-to-Peer Networks:\nAlgorithms and Evaluation. Performance Evaluation , 63:241\u2013263, Mar. 2006. Cited on\n85\nGladney H. Access Control for Large Collections. ACM Transactions on Information\nSystems , 15(2):154\u2013194, Apr. 1997. Cited on 533\nGollmann D. Computer Security . John Wiley, New York, 2nd edition, 2006. Cited on 504\nGong L. and Schemers R. Implementing Protection Domains in the Java Development\nKit 1.2. In Symposium on Network and Distributed System Security , pages 125\u2013134, San\nDiego, CA, Mar. 1998. Internet Society. Cited on 538\nGonz\u00e1lez M. C., Hidalgo C. A., and Barab\u00e1si A.-L. Understanding Individual Human\nMobility Patterns. Nature , 453:779\u2013782, June 2008. Cited on 46\nGray C. and Cheriton D. Leases: An Ef\ufb01cient Fault-Tolerant Mechanism for Distributed\nFile Cache Consistency. In 12th Symposium on Operating System Principles , pages\n202\u2013210, New York, NY, Dec. 1989. ACM, ACM Press. Cited on 391\nGray J. Notes on Database Operating Systems. In Bayer R., Graham R., and Seegmuller\nG., editors, Operating Systems: An Advanced Course , volume 60 of Lecture Notes in\nComputer Science , pages 393\u2013481. Springer-Verlag, Berlin, 1978. Cited on 484\nGray J. and Reuter A. Transaction Processing: Concepts and Techniques . Morgan Kaufman,\nSan Mateo, CA., 1993. Cited on 35\nGray J., Helland P ., O\u2019Neil P ., and Sashna D. The Dangers of Replication and a Solution.\nInSIGMOD International Conference on Management Of Data , pages 173\u2013182, Montreal,\nJune 1996. ACM. Cited on 358\nGropp W., Lusk E., and Skjellum A. Using MPI-2, Portable Parallel Programming with the\nMessage-Passing Interface . MIT Press, Cambridge, MA., 3rd edition, 2016. Cited on\n206\nGroup A. W. AMQP , Protocol speci\ufb01cation, Version 0-9-1, Nov. 2008. Cited on 218\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 565\nGuerraoui R. and Schiper A. Software-Based Replication for Fault Tolerance. Computer ,\n30(4):68\u201374, Apr. 1997. Cited on 433\nGuerraoui R., Kne\u017eevi\u00b4 c N., Qu\u00e9ma V ., and Vukoli\u00b4 c M. The next 700 bft protocols. In\n5th EuroSys (European Conference on Computer Systems) , pages 363\u2013376, New York,\nNY, 2010. ACM Press. Cited on 458\nGuichard J., Faucheur F. L., and Vasseur J.-P . De\ufb01nitive MPLS Network Designs . Cisco\nPress, Indianapolis, IN, 2005. Cited on 413\nGusella R. and Zatti S. The Accuracy of the Clock Synchronization Achieved by\nTEMPO in Berkeley UNIX 4.3BSD. IEEE Transactions on Software Engineering , 15(7):\n847\u2013853, July 1989. Cited on 306\nGuttman E. Autocon\ufb01guration for IP Networking: Enabling Local Communication.\nIEEE Internet Computing , 5:81\u201386, 2001. Cited on 43\nHadzilacos V . and Toueg S. Fault-Tolerant Broadcasts and Related Problems. In\nMullender S., editor, Distributed Systems , pages 97\u2013145. Addison-Wesley, Wokingham,\n2nd edition, 1993. Cited on 427, 481\nHajjat M., Sun X., Sung Y.-W. E., Maltz D., Rao S., Sripanidkulchai K., and Tawarmalani\nM. Cloudward Bound: Planning for Bene\ufb01cial Migration of Enterprise Applications\nto the Cloud. In SIGCOMM , pages 243\u2013254, New York, NY, 2010. ACM Press. Cited\non 32, 33, 34\nHelder D. A. and Jamin S. End-Host Multicast Communication Using Switch-Trees\nProtocols. In 2nd International Symposium on Cluster Computing and the Grid , pages\n419\u2013424, Los Alamitos, CA., May 2002. IEEE, IEEE Computer Society Press. Cited\non 224, 225\nHenning M. A New Approach to Object-Oriented Middleware. IEEE Internet Computing ,\n8(1):66\u201375, Jan. 2004. Cited on 136\nHenning M. and Spruiell M. Distributed Programming with Ice . ZeroC Inc., Brisbane,\nAustralia, May 2005. Cited on 136\nHerlihy M. and Shavit N. The Art of Multiprocessor Programming . Morgan Kaufman,\nSan Mateo, CA., 2008. Cited on 25, 104, 367\nHintjens P . ZeroMQ . O\u2019Reilly & Associates, Sebastopol, CA., 2013. Cited on 199\nHohpe G. and Woolf B. Enterprise Integration Patterns: Designing, Building, and Deploying\nMessaging Solutions . Addison-Wesley, Reading, MA., 2004. Cited on 34, 38, 212\nHorauer M. Clock Synchronization in Distributed Systems . Ph.D., University of Vienna,\nDepartment of Computer Science, Feb. 2004. Cited on 303\nHorowitz M. and Lunt S. FTP Security Extensions. RFC 2228, Oct. 1997. Cited on 169\nHosseini M., Ahmed D., Shirmohammadi S., and Georganas N. A Survey of\nApplication-Layer Multicast Protocols. IEEE Communications Surveys & Tutorials , 9\n(3):58\u201374, 2007. Cited on 222\nHowes T. The String Representation of LDAP Search Filters. RFC 2254, Dec. 1997.\nCited on 287\nHuffaker B., Fomenkov M., Plummer D. J., Moore D., and Claffy K. Distance Metrics\nin the Internet. In International Telecommunications Symposium , Los Alamitos, CA.,\nSept. 2002. IEEE, IEEE Computer Society Press. Cited on 413\nHui P ., Chaintreau A., Scott J., Gass R., Crowcroft J., and Diot C. Pocket Switched\nNetworks and Human Mobility in Conference Environments. In SIGCOMMWorkshop\non Delay-tolerant Networking , pages 244\u2013251, New York, NY, 2005. ACM Press. Cited\non 45\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 565\nGuerraoui R. and Schiper A. Software-Based Replication for Fault Tolerance. Computer ,\n30(4):68\u201374, Apr. 1997. Cited on 433\nGuerraoui R., Kne\u017eevi\u00b4 c N., Qu\u00e9ma V ., and Vukoli\u00b4 c M. The next 700 bft protocols. In\n5th EuroSys (European Conference on Computer Systems) , pages 363\u2013376, New York,\nNY, 2010. ACM Press. Cited on 458\nGuichard J., Faucheur F. L., and Vasseur J.-P . De\ufb01nitive MPLS Network Designs . Cisco\nPress, Indianapolis, IN, 2005. Cited on 413\nGusella R. and Zatti S. The Accuracy of the Clock Synchronization Achieved by\nTEMPO in Berkeley UNIX 4.3BSD. IEEE Transactions on Software Engineering , 15(7):\n847\u2013853, July 1989. Cited on 306\nGuttman E. Autocon\ufb01guration for IP Networking: Enabling Local Communication.\nIEEE Internet Computing , 5:81\u201386, 2001. Cited on 43\nHadzilacos V . and Toueg S. Fault-Tolerant Broadcasts and Related Problems. In\nMullender S., editor, Distributed Systems , pages 97\u2013145. Addison-Wesley, Wokingham,\n2nd edition, 1993. Cited on 427, 481\nHajjat M., Sun X., Sung Y.-W. E., Maltz D., Rao S., Sripanidkulchai K., and Tawarmalani\nM. Cloudward Bound: Planning for Bene\ufb01cial Migration of Enterprise Applications\nto the Cloud. In SIGCOMM , pages 243\u2013254, New York, NY, 2010. ACM Press. Cited\non 32, 33, 34\nHelder D. A. and Jamin S. End-Host Multicast Communication Using Switch-Trees\nProtocols. In 2nd International Symposium on Cluster Computing and the Grid , pages\n419\u2013424, Los Alamitos, CA., May 2002. IEEE, IEEE Computer Society Press. Cited\non 224, 225\nHenning M. A New Approach to Object-Oriented Middleware. IEEE Internet Computing ,\n8(1):66\u201375, Jan. 2004. Cited on 136\nHenning M. and Spruiell M. Distributed Programming with Ice . ZeroC Inc., Brisbane,\nAustralia, May 2005. Cited on 136\nHerlihy M. and Shavit N. The Art of Multiprocessor Programming . Morgan Kaufman,\nSan Mateo, CA., 2008. Cited on 25, 104, 367\nHintjens P . ZeroMQ . O\u2019Reilly & Associates, Sebastopol, CA., 2013. Cited on 199\nHohpe G. and Woolf B. Enterprise Integration Patterns: Designing, Building, and Deploying\nMessaging Solutions . Addison-Wesley, Reading, MA., 2004. Cited on 34, 38, 212\nHorauer M. Clock Synchronization in Distributed Systems . Ph.D., University of Vienna,\nDepartment of Computer Science, Feb. 2004. Cited on 303\nHorowitz M. and Lunt S. FTP Security Extensions. RFC 2228, Oct. 1997. Cited on 169\nHosseini M., Ahmed D., Shirmohammadi S., and Georganas N. A Survey of\nApplication-Layer Multicast Protocols. IEEE Communications Surveys & Tutorials , 9\n(3):58\u201374, 2007. Cited on 222\nHowes T. The String Representation of LDAP Search Filters. RFC 2254, Dec. 1997.\nCited on 287\nHuffaker B., Fomenkov M., Plummer D. J., Moore D., and Claffy K. Distance Metrics\nin the Internet. In International Telecommunications Symposium , Los Alamitos, CA.,\nSept. 2002. IEEE, IEEE Computer Society Press. Cited on 413\nHui P ., Chaintreau A., Scott J., Gass R., Crowcroft J., and Diot C. Pocket Switched\nNetworks and Human Mobility in Conference Environments. In SIGCOMMWorkshop\non Delay-tolerant Networking , pages 244\u2013251, New York, NY, 2005. ACM Press. Cited\non 45\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "566 BIBLIOGRAPHY\nHui P ., Yoneki E., Chan S. Y., and Crowcroft J. Distributed Community Detection in\nDelay Tolerant Networks. In 2nd International Workshop on Mobility in the Evolving\nInternet Architecture , pages 7:1\u20137:8, New York, NY, 2007. ACM Press. Cited on 46\nHui P ., Crowcroft J., and Yoneki E. BUBBLE Rap: Social-based Forwarding in Delay\nTolerant Networks. IEEE Transations on Mobile Computing , 2011. Cited on 46\nHunt G., Nahum E., and Tracey J. Enabling Content-Based Load Distribution for\nScalable Services. Technical report, IBM T.J. Watson Research Center, May 1997.\nCited on 143\nHutto P . and Ahamad M. Slow Memory: Weakening Consistency to Enhance Concur-\nrency in Distributed Shared Memories. In 10th International Conference on Distributed\nComputing Systems , pages 302\u2013311, Paris, France, May 1990. IEEE. Cited on 368\nISO . Open Distributed Processing Reference Model. International Standard ISO/IEC\nIS 10746, 1995. Cited on 8\nJackson M. Social and Economic Networks . Princeton University Press, Princeton, NJ,\n2008. Cited on 45\nJaeger T., Prakash A., Liedtke J., and Islam N. Flexible Control of Downloaded\nExecutable Content. ACM Transactions on Information and System Security , 2(2):\n177\u2013228, May 1999. Cited on 539\nJalote P . Fault Tolerance in Distributed Systems . Prentice Hall, Englewood Cliffs, N.J.,\n1994. Cited on 403, 424\nJanic M. Multicast in Network and Application Layer . Ph.d., Delft University of Technology,\nThe Netherlands, Oct. 2005. Cited on 221\nJaniga M. J., Dibner G., and Governali F. J. Internet Infrastructure: Content Delivery.\nGoldman Sachs Global Equity Research, Apr. 2001. Cited on 414\nJelasity M. and Babaoglu O. T-Man: Gossip-based Overlay Topology Management. In\n3rd International Workshop on Engineering Self-Organising Applications , volume 3910\nofLecture Notes in Computer Science , pages 1\u201315, Berlin, June 2006. Springer-Verlag.\nCited on 353\nJelasity M. and Kermarrec A.-M. Ordered Slicing of Very Large-Scale Overlay Networks.\nIn6th International Conference on Peer-to-Peer Computing , pages 117\u2013124, Los Alamitos,\nCA., Sept. 2006. IEEE Computer Society Press. Cited on 352\nJelasity M., Montresor A., and Babaoglu O. Gossip-based Aggregation in Large\nDynamic Networks. ACM Transactions on Computer Systems , 23(3):219\u2013252, Aug. 2005.\nCited on 349\nJelasity M., Voulgaris S., Guerraoui R., Kermarrec A.-M., and Steen M.van . Gossip-\nbased Peer Sampling. ACM Transactions on Computer Systems , 25(3), Aug. 2007. Cited\non 230, 351\nJohnson B. An Introduction to the Design and Analysis of Fault-Tolerant Systems. In\nPradhan D., editor, Fault-Tolerant Computer System Design , pages 1\u201387. Prentice Hall,\nUpper Saddle River, N.J., 1995. Cited on 431\nJohnson D., Perkins C., and Arkko J. Mobility Support for IPv6. RFC 3775, June 2004.\nCited on 245\nJosefsson S. Shishi \u2013 Kerberos 5 Implementation . Samurai Media Limited, Wickford, UK,\n2015. Cited on 527\nJoseph J., Ernest M., and Fellenstein C. Evolution of grid computing architecture and\ngrid adoption models. IBM Systems Journal , 43(4):624\u2013645, Apr. 2004. Cited on 29\nJul E., Levy H., Hutchinson N., and Black A. Fine-Grained Mobility in the Emerald\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n566 BIBLIOGRAPHY\nHui P ., Yoneki E., Chan S. Y., and Crowcroft J. Distributed Community Detection in\nDelay Tolerant Networks. In 2nd International Workshop on Mobility in the Evolving\nInternet Architecture , pages 7:1\u20137:8, New York, NY, 2007. ACM Press. Cited on 46\nHui P ., Crowcroft J., and Yoneki E. BUBBLE Rap: Social-based Forwarding in Delay\nTolerant Networks. IEEE Transations on Mobile Computing , 2011. Cited on 46\nHunt G., Nahum E., and Tracey J. Enabling Content-Based Load Distribution for\nScalable Services. Technical report, IBM T.J. Watson Research Center, May 1997.\nCited on 143\nHutto P . and Ahamad M. Slow Memory: Weakening Consistency to Enhance Concur-\nrency in Distributed Shared Memories. In 10th International Conference on Distributed\nComputing Systems , pages 302\u2013311, Paris, France, May 1990. IEEE. Cited on 368\nISO . Open Distributed Processing Reference Model. International Standard ISO/IEC\nIS 10746, 1995. Cited on 8\nJackson M. Social and Economic Networks . Princeton University Press, Princeton, NJ,\n2008. Cited on 45\nJaeger T., Prakash A., Liedtke J., and Islam N. Flexible Control of Downloaded\nExecutable Content. ACM Transactions on Information and System Security , 2(2):\n177\u2013228, May 1999. Cited on 539\nJalote P . Fault Tolerance in Distributed Systems . Prentice Hall, Englewood Cliffs, N.J.,\n1994. Cited on 403, 424\nJanic M. Multicast in Network and Application Layer . Ph.d., Delft University of Technology,\nThe Netherlands, Oct. 2005. Cited on 221\nJaniga M. J., Dibner G., and Governali F. J. Internet Infrastructure: Content Delivery.\nGoldman Sachs Global Equity Research, Apr. 2001. Cited on 414\nJelasity M. and Babaoglu O. T-Man: Gossip-based Overlay Topology Management. In\n3rd International Workshop on Engineering Self-Organising Applications , volume 3910\nofLecture Notes in Computer Science , pages 1\u201315, Berlin, June 2006. Springer-Verlag.\nCited on 353\nJelasity M. and Kermarrec A.-M. Ordered Slicing of Very Large-Scale Overlay Networks.\nIn6th International Conference on Peer-to-Peer Computing , pages 117\u2013124, Los Alamitos,\nCA., Sept. 2006. IEEE Computer Society Press. Cited on 352\nJelasity M., Montresor A., and Babaoglu O. Gossip-based Aggregation in Large\nDynamic Networks. ACM Transactions on Computer Systems , 23(3):219\u2013252, Aug. 2005.\nCited on 349\nJelasity M., Voulgaris S., Guerraoui R., Kermarrec A.-M., and Steen M.van . Gossip-\nbased Peer Sampling. ACM Transactions on Computer Systems , 25(3), Aug. 2007. Cited\non 230, 351\nJohnson B. An Introduction to the Design and Analysis of Fault-Tolerant Systems. In\nPradhan D., editor, Fault-Tolerant Computer System Design , pages 1\u201387. Prentice Hall,\nUpper Saddle River, N.J., 1995. Cited on 431\nJohnson D., Perkins C., and Arkko J. Mobility Support for IPv6. RFC 3775, June 2004.\nCited on 245\nJosefsson S. Shishi \u2013 Kerberos 5 Implementation . Samurai Media Limited, Wickford, UK,\n2015. Cited on 527\nJoseph J., Ernest M., and Fellenstein C. Evolution of grid computing architecture and\ngrid adoption models. IBM Systems Journal , 43(4):624\u2013645, Apr. 2004. Cited on 29\nJul E., Levy H., Hutchinson N., and Black A. Fine-Grained Mobility in the Emerald\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 567\nSystem. ACM Transactions on Computer Systems , 6(1):109\u2013133, Feb. 1988. Cited on 245\nKahn D. The Codebreakers . Macmillan, New York, 1967. Cited on 512\nKarl H. and Willig A. Protocols and Architectures for Wireless Sensor Networks . John Wiley,\nNew York, 2005. Cited on 47\nKarp A., Haury H., and Davis M. From ABAC to ZBAC: The Evolution of Access\nControl Models. Information Systems Security Association Journal , 8(4):22\u201330, Apr.\n2010. Cited on 549\nKasera S., Kurose J., and Towsley D. Scalable Reliable Multicast Using Multiple Multi-\ncast Groups. In International Conference on Measurements and Modeling of Computer\nSystems , pages 64\u201374, Seattle, WA, June 1997. ACM. Cited on 475\nKaufman C., Perlman R., and Speciner M. Network Security: Private Communication in a\nPublic World . Prentice Hall, Englewood Cliffs, N.J., 2nd edition, 2003. Cited on 515,\n523\nKemme B., Jimenez Peris R., and Patino-Martinez M. Replicated Databases . Synthesis\nLectures on Computer Architectures. Morgan and Claypool, San Rafael, CA, 2010.\nCited on 385\nKermarrec A.-M. and Trianta\ufb01llou P . XL Peer-to-peer Pub/Sub Systems. ACM Comput-\ning Surveys , 46(2):16:1\u201316:45, Nov. 2013. Cited on 344\nKhosha\ufb01an S. and Buckiewicz M. Introduction to Groupware, Work\ufb02ow, and Workgroup\nComputing . John Wiley, New York, 1995. Cited on 212\nKim M., Fielding J. J., and Kotz D. Risks of Using AP Locations Discovered Through\nWar Driving. In 4th International Conference Pervasive Computing , volume 3968 of\nLecture Notes in Computer Science , pages 67\u201382, Berlin, May 2006a. Springer-Verlag.\nCited on 339\nKim M., Kotz D., and Kim S. Extracting a Mobility Model from Real User Traces. In\n25th INFOCOMConference , Los Alamitos, CA., Apr. 2006b. IEEE Computer Society\nPress. Cited on 46\nKirsch J. and Amir Y. Paxos for System Builders. Technical Report CNDS-2008-2, John\nHopkins University, Mar. 2008. Cited on 438, 442\nKleiman S. Vnodes: an Architecture for Multiple File System Types in UNIX. In\nSummer Technical Conference , pages 238\u2013247, Atlanta, GA, June 1986. USENIX. Cited\non 95\nKohl J., Neuman B., and T\u2019so T. The Evolution of the Kerberos Authentication System.\nIn Brazier F. and Johansen D., editors, Distributed Open Systems , pages 78\u201394. IEEE\nComputer Society Press, Los Alamitos, CA., 1994. Cited on 526\nKopetz H. and Verissimo P . Real Time and Dependability Concepts. In Mullender\nS., editor, Distributed Systems , pages 411\u2013446. Addison-Wesley, Wokingham, 2nd\nedition, 1993. Cited on 424\nKoren I. and Krishna C. M. Fault-Tolerant Systems . Morgan Kaufman, San Mateo, CA.,\n2007. Cited on 424, 455\nKotla R., Alvisi L., Dahlin M., Clement A., and Wong E. Zyzzyva: Speculative\nByzantine Fault Tolerance. ACM Transactions on Computer Systems , 27(4), Dec. 2009.\nCited on 458\nKrakowiak S. Middleware Architecture with Patterns and Frameworks . Creative Commons,\n2009. Cited on 57\nKreitz G. and Niemel\u00e4 F. Spotify \u2013 Large Scale, Low Latency, P2P Music-on-Demand\nStreaming. In 10th International Conference on Peer-to-Peer Computing , pages 266\u2013275,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 567\nSystem. ACM Transactions on Computer Systems , 6(1):109\u2013133, Feb. 1988. Cited on 245\nKahn D. The Codebreakers . Macmillan, New York, 1967. Cited on 512\nKarl H. and Willig A. Protocols and Architectures for Wireless Sensor Networks . John Wiley,\nNew York, 2005. Cited on 47\nKarp A., Haury H., and Davis M. From ABAC to ZBAC: The Evolution of Access\nControl Models. Information Systems Security Association Journal , 8(4):22\u201330, Apr.\n2010. Cited on 549\nKasera S., Kurose J., and Towsley D. Scalable Reliable Multicast Using Multiple Multi-\ncast Groups. In International Conference on Measurements and Modeling of Computer\nSystems , pages 64\u201374, Seattle, WA, June 1997. ACM. Cited on 475\nKaufman C., Perlman R., and Speciner M. Network Security: Private Communication in a\nPublic World . Prentice Hall, Englewood Cliffs, N.J., 2nd edition, 2003. Cited on 515,\n523\nKemme B., Jimenez Peris R., and Patino-Martinez M. Replicated Databases . Synthesis\nLectures on Computer Architectures. Morgan and Claypool, San Rafael, CA, 2010.\nCited on 385\nKermarrec A.-M. and Trianta\ufb01llou P . XL Peer-to-peer Pub/Sub Systems. ACM Comput-\ning Surveys , 46(2):16:1\u201316:45, Nov. 2013. Cited on 344\nKhosha\ufb01an S. and Buckiewicz M. Introduction to Groupware, Work\ufb02ow, and Workgroup\nComputing . John Wiley, New York, 1995. Cited on 212\nKim M., Fielding J. J., and Kotz D. Risks of Using AP Locations Discovered Through\nWar Driving. In 4th International Conference Pervasive Computing , volume 3968 of\nLecture Notes in Computer Science , pages 67\u201382, Berlin, May 2006a. Springer-Verlag.\nCited on 339\nKim M., Kotz D., and Kim S. Extracting a Mobility Model from Real User Traces. In\n25th INFOCOMConference , Los Alamitos, CA., Apr. 2006b. IEEE Computer Society\nPress. Cited on 46\nKirsch J. and Amir Y. Paxos for System Builders. Technical Report CNDS-2008-2, John\nHopkins University, Mar. 2008. Cited on 438, 442\nKleiman S. Vnodes: an Architecture for Multiple File System Types in UNIX. In\nSummer Technical Conference , pages 238\u2013247, Atlanta, GA, June 1986. USENIX. Cited\non 95\nKohl J., Neuman B., and T\u2019so T. The Evolution of the Kerberos Authentication System.\nIn Brazier F. and Johansen D., editors, Distributed Open Systems , pages 78\u201394. IEEE\nComputer Society Press, Los Alamitos, CA., 1994. Cited on 526\nKopetz H. and Verissimo P . Real Time and Dependability Concepts. In Mullender\nS., editor, Distributed Systems , pages 411\u2013446. Addison-Wesley, Wokingham, 2nd\nedition, 1993. Cited on 424\nKoren I. and Krishna C. M. Fault-Tolerant Systems . Morgan Kaufman, San Mateo, CA.,\n2007. Cited on 424, 455\nKotla R., Alvisi L., Dahlin M., Clement A., and Wong E. Zyzzyva: Speculative\nByzantine Fault Tolerance. ACM Transactions on Computer Systems , 27(4), Dec. 2009.\nCited on 458\nKrakowiak S. Middleware Architecture with Patterns and Frameworks . Creative Commons,\n2009. Cited on 57\nKreitz G. and Niemel\u00e4 F. Spotify \u2013 Large Scale, Low Latency, P2P Music-on-Demand\nStreaming. In 10th International Conference on Peer-to-Peer Computing , pages 266\u2013275,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "568 BIBLIOGRAPHY\nLos Alamitos, CA., Aug. 2010. IEEE, IEEE Computer Society Press. Cited on 20\nKshemkalyani A. and Singhal M. Distributed Computing, Principles, Algorithms, and\nSystems . Cambridge University Press, Cambridge, UK, 2008. Cited on 322, 451, 453\nKumar K., Liu J., Lu Y.-H., and Bhargava B. A Survey of Computation Of\ufb02oading for\nMobile Systems. Mobile Networks and Applications , 18(1):129\u2013140, 2013. Cited on 153\nLagar-Cavilla H. A., Whitney J. A., Scannell A. M., Patchin P ., Rumble S. M., Lara E.de ,\nBrudno M., and Satyanarayanan M. SnowFlock: Rapid Virtual Machine Cloning for\nCloud Computing. In 4th EuroSys (European Conference on Computer Systems) , pages\n1\u201312, New York, NY, 2009. ACM Press. Cited on 160\nLai A. and Nieh J. Limits of Wide-Area Thin-Client Computing. In International\nConference on Measurements and Modeling of Computer Systems , pages 228\u2013239, New\nYork, NY, June 2002. ACM, ACM Press. Cited on 126\nLaMarca A. and Lara E.de . Location Systems: An Introduction to the Technology Behind\nLocation Awareness . Morgan & Claypool, San Rafael, CA, 2008. Cited on 338\nLamport L. The Part-Time Parliament. ACM Transactions on Computer Systems , 16(2):\n133\u2013169, May 1998. Cited on 438\nLamport L. Paxos Made Simple. ACM SIGACT News , 32(4):51\u201358, Dec. 2001. Cited on\n438\nLamport L. Time, Clocks, and the Ordering of Events in a Distributed System. Commu-\nnications of the ACM , 21(7):558\u2013565, July 1978. Cited on 310, 402\nLamport L. How to Make a Multiprocessor Computer that Correctly Executes Mul-\ntiprocessor Programs. IEEE Transactions on Computers , C-29(9):690\u2013691, Sept. 1979.\nCited on 364\nLamport L., Shostak R., and Paese M. The Byzantine Generals Problem. ACM\nTransactions on Programming Languages and Systems , 4(3):382\u2013401, July 1982. Cited on\n429\nLampson B. How to Build a Highly Available System using Consensus. In Babaoglu\nO. and Marzullo K., editors, 12th International Workshop on Distributed Algorithms ,\nvolume 1151 of Lecture Notes in Computer Science , pages 1\u201317, Berlin, Oct. 1996.\nSpringer-Verlag. Cited on 438\nLampson B., Abadi M., Burrows M., and Wobber E. Authentication in Distributed\nSystems: Theory and Practice. ACM Transactions on Computer Systems , 10(4):265\u2013310,\nNov. 1992. Cited on 513\nLaprie J.-C. Dependability \u2013 Its Attributes, Impairments and Means. In Randell B.,\nLaprie J.-C., Kopetz H., and Littlewood B., editors, Predictably Dependable Computing\nSystems , pages 3\u201324. Springer-Verlag, Berlin, 1995. Cited on 502\nLaurie B. and Laurie P . Apache: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol,\nCA., 3rd edition, 2002. Cited on 141\nLawder J. and King P . Querying Multi-dimensional Data Indexed Using Hilbert\nSpace-Filling Curve. ACM Sigmod Record , 30(1):19\u201324, Mar. 2000. Cited on 290\nLeff A. and Ray\ufb01eld J. T. Alternative Edge-server Architectures for Enterprise JavaBeans\nApplications. In Middleware 2004 , volume 3231 of Lecture Notes in Computer Science ,\npages 195\u2013211, Berlin, Oct. 2004. ACM/IFIP/USENIX, Springer-Verlag. Cited on 91\nLevien R., editor. Signposts in Cyberspace: The Domain Name System and Internet Nav-\nigation . National Academic Research Council, Washington, DC, 2005. Cited on\n271\nLevine B. and Garcia-Luna-Aceves J. A Comparison of Reliable Multicast Protocols.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n568 BIBLIOGRAPHY\nLos Alamitos, CA., Aug. 2010. IEEE, IEEE Computer Society Press. Cited on 20\nKshemkalyani A. and Singhal M. Distributed Computing, Principles, Algorithms, and\nSystems . Cambridge University Press, Cambridge, UK, 2008. Cited on 322, 451, 453\nKumar K., Liu J., Lu Y.-H., and Bhargava B. A Survey of Computation Of\ufb02oading for\nMobile Systems. Mobile Networks and Applications , 18(1):129\u2013140, 2013. Cited on 153\nLagar-Cavilla H. A., Whitney J. A., Scannell A. M., Patchin P ., Rumble S. M., Lara E.de ,\nBrudno M., and Satyanarayanan M. SnowFlock: Rapid Virtual Machine Cloning for\nCloud Computing. In 4th EuroSys (European Conference on Computer Systems) , pages\n1\u201312, New York, NY, 2009. ACM Press. Cited on 160\nLai A. and Nieh J. Limits of Wide-Area Thin-Client Computing. In International\nConference on Measurements and Modeling of Computer Systems , pages 228\u2013239, New\nYork, NY, June 2002. ACM, ACM Press. Cited on 126\nLaMarca A. and Lara E.de . Location Systems: An Introduction to the Technology Behind\nLocation Awareness . Morgan & Claypool, San Rafael, CA, 2008. Cited on 338\nLamport L. The Part-Time Parliament. ACM Transactions on Computer Systems , 16(2):\n133\u2013169, May 1998. Cited on 438\nLamport L. Paxos Made Simple. ACM SIGACT News , 32(4):51\u201358, Dec. 2001. Cited on\n438\nLamport L. Time, Clocks, and the Ordering of Events in a Distributed System. Commu-\nnications of the ACM , 21(7):558\u2013565, July 1978. Cited on 310, 402\nLamport L. How to Make a Multiprocessor Computer that Correctly Executes Mul-\ntiprocessor Programs. IEEE Transactions on Computers , C-29(9):690\u2013691, Sept. 1979.\nCited on 364\nLamport L., Shostak R., and Paese M. The Byzantine Generals Problem. ACM\nTransactions on Programming Languages and Systems , 4(3):382\u2013401, July 1982. Cited on\n429\nLampson B. How to Build a Highly Available System using Consensus. In Babaoglu\nO. and Marzullo K., editors, 12th International Workshop on Distributed Algorithms ,\nvolume 1151 of Lecture Notes in Computer Science , pages 1\u201317, Berlin, Oct. 1996.\nSpringer-Verlag. Cited on 438\nLampson B., Abadi M., Burrows M., and Wobber E. Authentication in Distributed\nSystems: Theory and Practice. ACM Transactions on Computer Systems , 10(4):265\u2013310,\nNov. 1992. Cited on 513\nLaprie J.-C. Dependability \u2013 Its Attributes, Impairments and Means. In Randell B.,\nLaprie J.-C., Kopetz H., and Littlewood B., editors, Predictably Dependable Computing\nSystems , pages 3\u201324. Springer-Verlag, Berlin, 1995. Cited on 502\nLaurie B. and Laurie P . Apache: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol,\nCA., 3rd edition, 2002. Cited on 141\nLawder J. and King P . Querying Multi-dimensional Data Indexed Using Hilbert\nSpace-Filling Curve. ACM Sigmod Record , 30(1):19\u201324, Mar. 2000. Cited on 290\nLeff A. and Ray\ufb01eld J. T. Alternative Edge-server Architectures for Enterprise JavaBeans\nApplications. In Middleware 2004 , volume 3231 of Lecture Notes in Computer Science ,\npages 195\u2013211, Berlin, Oct. 2004. ACM/IFIP/USENIX, Springer-Verlag. Cited on 91\nLevien R., editor. Signposts in Cyberspace: The Domain Name System and Internet Nav-\nigation . National Academic Research Council, Washington, DC, 2005. Cited on\n271\nLevine B. and Garcia-Luna-Aceves J. A Comparison of Reliable Multicast Protocols.\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 569\nACM Multimedia Systems Journal , 6(5):334\u2013348, 1998. Cited on 474\nLewis B. and Berg D. J. Multithreaded Programming with Pthreads . Prentice Hall,\nEnglewood Cliffs, N.J., 2nd edition, 1998. Cited on 104\nLi A., Yang X., Kandula S., and Zhang M. CloudCmp: Comparing Public Cloud\nProviders. In 10th Internet Measurement Conference , pages 1\u201314, New York, NY, Nov.\n2010. ACM Press. Cited on 32\nLilja D. Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and\nComparisons. ACM Computing Surveys , 25(3):303\u2013338, Sept. 1993. Cited on 404\nLin M.-J. and Marzullo K. Directional Gossip: Gossip in a Wide-Area Network. In\nProceedings 3rd European Dependable Computing Conf. , volume 1667 of Lecture Notes in\nComputer Science , pages 364\u2013379. Springer-Verlag, Berlin, Sept. 1999. Cited on 232\nLin S.-D., Lian Q., Chen M., , and Zhang Z. A Practical Distributed Mutual Exclusion\nProtocol in Dynamic Peer-to-Peer Systems. In 3rd International Workshop on Peer-to-\nPeer Systems , volume 3279 of Lecture Notes in Computer Science , pages 11\u201321, Berlin,\nFeb. 2004. Springer-Verlag. Cited on 326, 328\nLing B. C., Kiciman E., and Fox A. Session State: Beyond Soft State. In 1st Symposium\non Networked Systems Design and Implementation , pages 295\u2013308, Berkeley, CA, Mar.\n2004. USENIX, USENIX. Cited on 132\nLiu C. and Albitz P . DNS and BIND . O\u2019Reilly & Associates, Sebastopol, CA., 5th\nedition, 2006. Cited on 170, 271\nLiu C.-G., Estrin D., Shenker S., and Zhang L. Local Error Recovery in SRM: Compari-\nson of Two Approaches. IEEE/ACM Transactions on Networking , 6(6):686\u2013699, Dec.\n1998. Cited on 475\nLiu F. and Solihin Y. Understanding the Behavior and Implications of Context Switch\nMisses. ACM Transactions on Architecture and Code Optimization , 7(4):21:1\u201321:28, Dec.\n2010. Cited on 108\nLo V ., Zhou D., Liu Y., GauthierDickey C., and Li J. Scalable Supernode Selection\nin Peer-to-Peer Overlay Networks. In 2nd Hot Topics in Peer-to-Peer Systems , pages\n18\u201327, Los Alamitos, CA., July 2005. IEEE Computer Society Press. Cited on 335\nLottiaux R., Gallard P ., Vallee G., and Morin C. OpenMosix, OpenSSI and Kerrighed:\nA Comparative Study. In 5th International Symposium on Cluster Computing and the\nGrid , pages 1016\u20131023, Los Alamitos, CA., May 2005. IEEE Computer Society Press.\nCited on 27\nLua E., Crowcroft J., Pias M., Sharma R., and Lim S. A Survey and Comparison of\nPeer-to-Peer Overlay Network Schemes. IEEE Communications Surveys & Tutorials , 7\n(2):22\u201373, Apr. 2005. Cited on 23, 81\nLui J., Misra V ., and Rubenstein D. On the Robustness of Soft State Protocols. In 12th\nInternational Conference on Network Protocols , pages 50\u201360, Los Alamitos, CA., Oct.\n2004. IEEE, IEEE Computer Society Press. Cited on 131\nLv Q., Cao P ., Cohen E., Li K., and Shenker S. Search and Replication in Unstructured\nPeer-to-Peer Networks. In 16th International Conference on Supercomputing , pages\n84\u201395, New York, NY, June 2002. ACM, ACM Press. Cited on 85, 86\nLynch N. Distributed Algorithms . Morgan Kaufman, San Mateo, CA., 1996. Cited on\n298, 330\nMaassen J., Kielmann T., and Bal H. E. Parallel Application Experience with Replicated\nMethod Invocation. Concurrency & Computation: Practice and Experience , 13(8-9):\n681\u2013712, 2001. Cited on 395\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 569\nACM Multimedia Systems Journal , 6(5):334\u2013348, 1998. Cited on 474\nLewis B. and Berg D. J. Multithreaded Programming with Pthreads . Prentice Hall,\nEnglewood Cliffs, N.J., 2nd edition, 1998. Cited on 104\nLi A., Yang X., Kandula S., and Zhang M. CloudCmp: Comparing Public Cloud\nProviders. In 10th Internet Measurement Conference , pages 1\u201314, New York, NY, Nov.\n2010. ACM Press. Cited on 32\nLilja D. Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and\nComparisons. ACM Computing Surveys , 25(3):303\u2013338, Sept. 1993. Cited on 404\nLin M.-J. and Marzullo K. Directional Gossip: Gossip in a Wide-Area Network. In\nProceedings 3rd European Dependable Computing Conf. , volume 1667 of Lecture Notes in\nComputer Science , pages 364\u2013379. Springer-Verlag, Berlin, Sept. 1999. Cited on 232\nLin S.-D., Lian Q., Chen M., , and Zhang Z. A Practical Distributed Mutual Exclusion\nProtocol in Dynamic Peer-to-Peer Systems. In 3rd International Workshop on Peer-to-\nPeer Systems , volume 3279 of Lecture Notes in Computer Science , pages 11\u201321, Berlin,\nFeb. 2004. Springer-Verlag. Cited on 326, 328\nLing B. C., Kiciman E., and Fox A. Session State: Beyond Soft State. In 1st Symposium\non Networked Systems Design and Implementation , pages 295\u2013308, Berkeley, CA, Mar.\n2004. USENIX, USENIX. Cited on 132\nLiu C. and Albitz P . DNS and BIND . O\u2019Reilly & Associates, Sebastopol, CA., 5th\nedition, 2006. Cited on 170, 271\nLiu C.-G., Estrin D., Shenker S., and Zhang L. Local Error Recovery in SRM: Compari-\nson of Two Approaches. IEEE/ACM Transactions on Networking , 6(6):686\u2013699, Dec.\n1998. Cited on 475\nLiu F. and Solihin Y. Understanding the Behavior and Implications of Context Switch\nMisses. ACM Transactions on Architecture and Code Optimization , 7(4):21:1\u201321:28, Dec.\n2010. Cited on 108\nLo V ., Zhou D., Liu Y., GauthierDickey C., and Li J. Scalable Supernode Selection\nin Peer-to-Peer Overlay Networks. In 2nd Hot Topics in Peer-to-Peer Systems , pages\n18\u201327, Los Alamitos, CA., July 2005. IEEE Computer Society Press. Cited on 335\nLottiaux R., Gallard P ., Vallee G., and Morin C. OpenMosix, OpenSSI and Kerrighed:\nA Comparative Study. In 5th International Symposium on Cluster Computing and the\nGrid , pages 1016\u20131023, Los Alamitos, CA., May 2005. IEEE Computer Society Press.\nCited on 27\nLua E., Crowcroft J., Pias M., Sharma R., and Lim S. A Survey and Comparison of\nPeer-to-Peer Overlay Network Schemes. IEEE Communications Surveys & Tutorials , 7\n(2):22\u201373, Apr. 2005. Cited on 23, 81\nLui J., Misra V ., and Rubenstein D. On the Robustness of Soft State Protocols. In 12th\nInternational Conference on Network Protocols , pages 50\u201360, Los Alamitos, CA., Oct.\n2004. IEEE, IEEE Computer Society Press. Cited on 131\nLv Q., Cao P ., Cohen E., Li K., and Shenker S. Search and Replication in Unstructured\nPeer-to-Peer Networks. In 16th International Conference on Supercomputing , pages\n84\u201395, New York, NY, June 2002. ACM, ACM Press. Cited on 85, 86\nLynch N. Distributed Algorithms . Morgan Kaufman, San Mateo, CA., 1996. Cited on\n298, 330\nMaassen J., Kielmann T., and Bal H. E. Parallel Application Experience with Replicated\nMethod Invocation. Concurrency & Computation: Practice and Experience , 13(8-9):\n681\u2013712, 2001. Cited on 395\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "570 BIBLIOGRAPHY\nMadden S. R., Franklin M. J., Hellerstein J. M., and Hong W. TinyDB: An Acquisitional\nQuery Processing System for Sensor Networks. ACM Transactions on Database\nSystems , 30(1):122\u2013173, 2005. Cited on 49\nMahajan P ., Alvisi L., and Dahlin M. Consistency, availability, and convergence.\nTechnical Report TR-11-22, University of Texas at Austin, May 2011. Cited on 375\nMalone T. and Crowston K. The Interdisciplinary Study of Coordination. ACM\nComputing Surveys , 26(1):87\u2013119, Mar. 1994. Cited on 297\nMao Z. M., Cranor C. D., Douglis F., Rabinovich M., Spatscheck O., and Wang J. A\nPrecise and Ef\ufb01cient Evaluation of the Proximity between Web Clients and their\nLocal DNS Servers. In USENIX Annual Technical Conference , pages 229\u2013242, Berkeley,\nCA, June 2002. USENIX, USENIX. Cited on 146\nMarzullo K. and Owicki S. Maintaining The Time in a Distributed System. In 2nd\nSymposium on Principles of Distributed Computing , pages 295\u2013305, New York, NY, 1983.\nACM, ACM Press. Cited on 309\nMattern F. and Floerkemeier C. From the Internet of Computers to the Internet of Things ,\npages 242\u2013259. Springer-Verlag, Berlin, 2010. Cited on 40\nMazouni K., Garbinato B., and Guerraoui R. Building Reliable Client-Server Software\nUsing Actively Replicated Objects. In Graham I., Magnusson B., Meyer B., and\nNerson J.-M., editors, Technology of Object Oriented Languages and Systems , pages\n37\u201353. Prentice Hall, Englewood Cliffs, N.J., 1995. Cited on 395\nMehta N., Medvidovic N., and Phadke S. Towards A Taxonomy Of Software Connec-\ntors. In 22nd International Conference on Software Engineering , pages 178\u2013187, New\nYork, NY, June 2000. ACM, ACM Press. Cited on 56\nMeling H. and Jehl L. Tutorial Summary: Paxos Explained from Scratch. In 17th\nInternational Conference on Principles of Distributed Systems , pages 1\u201310. Springer, 2013.\nCited on 443\nMenasce D. and Almeida V . Capacity Planning for Web Services . Prentice Hall, Englewood\nCliffs, N.J., 2002. Cited on 16\nMenezes A. J., Oorschot P . C.van , and Vanstone S. A. Handbook of Applied Cryptography .\nCRC Press, Boca Raton, 3rd edition, 1996. Cited on 512, 544\nMeyerovich L. A. and Bodik R. Fast and Parallel Webpage Layout. In 19th International\nWorld Wide Web Conference , pages 711\u2013720, New York, NY, 2010. ACM Press. Cited\non 113\nMiklas A., Gollu K., Chan K., Saroiu S., Gummamdi K., and Lara E.de . Exploiting\nSocial Interactions in Mobile Systems. In 9th Conference on Ubiquitous Computing\n(UbiComp) , volume 4717 of Lecture Notes in Computer Science , pages 409\u2013428, Berlin,\nSept. 2007. Springer-Verlag. Cited on 45\nMills D. Network Time Protocol (version 3): Speci\ufb01cation, Implementation, and\nAnalysis. RFC 1305, July 1992. Cited on 306\nMills D. L. Computer Network Time Synchronization: The Network Time Protocol on Earth\nand in Space . CRC Press, Boca Raton, FL, 2nd edition, 2011. Cited on 306\nMilojicic D., Douglis F., Paindaveine Y., Wheeler R., and Zhou S. Process Migration.\nACM Computing Surveys , 32(3):241\u2013299, Sept. 2000. Cited on 152\nMin S. L. and Baer J.-L. Design and Analysis of a Scalable Cache Coherence Scheme\nBased on Clocks and Timestamps. IEEE Transactions on Parallel and Distributed\nSystems , 3(1):25\u201344, Jan. 1992. Cited on 404\nMockapetris P . Domain Names - Concepts and Facilities. RFC 1034, Nov. 1987. Cited\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n570 BIBLIOGRAPHY\nMadden S. R., Franklin M. J., Hellerstein J. M., and Hong W. TinyDB: An Acquisitional\nQuery Processing System for Sensor Networks. ACM Transactions on Database\nSystems , 30(1):122\u2013173, 2005. Cited on 49\nMahajan P ., Alvisi L., and Dahlin M. Consistency, availability, and convergence.\nTechnical Report TR-11-22, University of Texas at Austin, May 2011. Cited on 375\nMalone T. and Crowston K. The Interdisciplinary Study of Coordination. ACM\nComputing Surveys , 26(1):87\u2013119, Mar. 1994. Cited on 297\nMao Z. M., Cranor C. D., Douglis F., Rabinovich M., Spatscheck O., and Wang J. A\nPrecise and Ef\ufb01cient Evaluation of the Proximity between Web Clients and their\nLocal DNS Servers. In USENIX Annual Technical Conference , pages 229\u2013242, Berkeley,\nCA, June 2002. USENIX, USENIX. Cited on 146\nMarzullo K. and Owicki S. Maintaining The Time in a Distributed System. In 2nd\nSymposium on Principles of Distributed Computing , pages 295\u2013305, New York, NY, 1983.\nACM, ACM Press. Cited on 309\nMattern F. and Floerkemeier C. From the Internet of Computers to the Internet of Things ,\npages 242\u2013259. Springer-Verlag, Berlin, 2010. Cited on 40\nMazouni K., Garbinato B., and Guerraoui R. Building Reliable Client-Server Software\nUsing Actively Replicated Objects. In Graham I., Magnusson B., Meyer B., and\nNerson J.-M., editors, Technology of Object Oriented Languages and Systems , pages\n37\u201353. Prentice Hall, Englewood Cliffs, N.J., 1995. Cited on 395\nMehta N., Medvidovic N., and Phadke S. Towards A Taxonomy Of Software Connec-\ntors. In 22nd International Conference on Software Engineering , pages 178\u2013187, New\nYork, NY, June 2000. ACM, ACM Press. Cited on 56\nMeling H. and Jehl L. Tutorial Summary: Paxos Explained from Scratch. In 17th\nInternational Conference on Principles of Distributed Systems , pages 1\u201310. Springer, 2013.\nCited on 443\nMenasce D. and Almeida V . Capacity Planning for Web Services . Prentice Hall, Englewood\nCliffs, N.J., 2002. Cited on 16\nMenezes A. J., Oorschot P . C.van , and Vanstone S. A. Handbook of Applied Cryptography .\nCRC Press, Boca Raton, 3rd edition, 1996. Cited on 512, 544\nMeyerovich L. A. and Bodik R. Fast and Parallel Webpage Layout. In 19th International\nWorld Wide Web Conference , pages 711\u2013720, New York, NY, 2010. ACM Press. Cited\non 113\nMiklas A., Gollu K., Chan K., Saroiu S., Gummamdi K., and Lara E.de . Exploiting\nSocial Interactions in Mobile Systems. In 9th Conference on Ubiquitous Computing\n(UbiComp) , volume 4717 of Lecture Notes in Computer Science , pages 409\u2013428, Berlin,\nSept. 2007. Springer-Verlag. Cited on 45\nMills D. Network Time Protocol (version 3): Speci\ufb01cation, Implementation, and\nAnalysis. RFC 1305, July 1992. Cited on 306\nMills D. L. Computer Network Time Synchronization: The Network Time Protocol on Earth\nand in Space . CRC Press, Boca Raton, FL, 2nd edition, 2011. Cited on 306\nMilojicic D., Douglis F., Paindaveine Y., Wheeler R., and Zhou S. Process Migration.\nACM Computing Surveys , 32(3):241\u2013299, Sept. 2000. Cited on 152\nMin S. L. and Baer J.-L. Design and Analysis of a Scalable Cache Coherence Scheme\nBased on Clocks and Timestamps. IEEE Transactions on Parallel and Distributed\nSystems , 3(1):25\u201344, Jan. 1992. Cited on 404\nMockapetris P . Domain Names - Concepts and Facilities. RFC 1034, Nov. 1987. Cited\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 571\non 265, 271\nMohan C., Strong R., and Finkelstein S. Method for Distributed Transaction Commit\nand Recovery using Byzantine Agreement within Clusters of Processors. In 2nd\nSymposium on Principles of Distributed Computing , pages 89\u2013103, New York, NY, 1983.\nACM Press. Cited on 429\nMottola L. and Picco G. P . Programming Wireless Sensor Networks: Fundamental\nConcepts and State of the Art. ACM Computing Surveys , 43(3):19:1\u201319:51, Apr. 2011.\nCited on 48\nMPI Forum . MPI: A Message-Passing Interface Standard, Version 3.0. Technical report,\nUniversity of Tennessee, Knoxville, Tennessee, Sept. 2012. Cited on 206\nM\u00fchl G., Fiege L., and Pietzuch P . Distributed Event-Based Systems . Springer-Verlag,\nBerlin, 2006. Cited on 67\nMuntz D. and Honeyman P . Multi-level Caching in Distributed File Systems. In Winter\nTechnical Conference , pages 305\u2013313, San Francisco, CA, Jan. 1992. USENIX. Cited on\n388\nMurty J. Programming Amazon Web Services . O\u2019Reilly & Associates, Sebastopol, CA.,\n2008. Cited on 31, 65\nNaur P . and Randell B. Report on the NATO Software Engineering Conference 1968.\nTechnical report, Scienti\ufb01c Affairs Division NATO, Brussels, Belgium, Oct. 1968.\nCited on 7\nNayate A., Dahlin M., and Iyengar A. Transparent Information Dissemination. In\nMiddleware 2004 , volume 3231 of Lecture Notes in Computer Science , pages 212\u2013231,\nBerlin, Oct. 2004. ACM/IFIP/USENIX, Springer-Verlag. Cited on 91\nNeedham R. and Schroeder M. Using Encryption for Authentication in Large Networks\nof Computers. Communications of the ACM , 21(12):993\u2013999, Dec. 1978. Cited on 517\nNelson B. Remote Procedure Call . Ph.D., Carnegie-Mellon University, 1981. Cited on 469\nNeuman B. Proxy-Based Authorization and Accounting for Distributed Systems.\nIn13th International Conference on Distributed Computing Systems , pages 283\u2013291,\nPittsburgh, May 1993. IEEE. Cited on 550\nNeuman B. Scale in Distributed Systems. In Casavant T. and Singhal M., editors,\nReadings in Distributed Computing Systems , pages 463\u2013489. IEEE Computer Society\nPress, Los Alamitos, CA., 1994. Cited on 15, 20\nNeuman C., Yu T., Hartman S., and Raeburn K. The Kerberos Network Authentication\nService. RFC 4120, July 2005. Cited on 527\nNewman M. Networks, An Introduction . Oxford University Press, Oxford, UK, 2010.\nCited on 46\nNg E. and Zhang H. Predicting Internet Network Distance with Coordinates-Based\nApproaches. In 21st INFOCOM Conference , Los Alamitos, CA., June 2002. IEEE, IEEE\nComputer Society Press. Cited on 341\nNoble B., Fleis B., and Kim M. A Case for Fluid Replication. In NetStore\u201999 , Seattle,\nWA, Oct. 1999. Cited on 388\nNyers L. and Jelasity M. A Comparative Study of Spanning Tree and Gossip Protocols\nfor Aggregation. Concurrency & Computation: Practice and Experience , 2015. Cited on\n477\nNygren E., Sitaraman R. K., and Sun J. The Akamai Network: A Platform for High-\nPerformance Internet Applications. Operating Systems Review , 44(3):2\u201319, July 2010.\nCited on 415\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 571\non 265, 271\nMohan C., Strong R., and Finkelstein S. Method for Distributed Transaction Commit\nand Recovery using Byzantine Agreement within Clusters of Processors. In 2nd\nSymposium on Principles of Distributed Computing , pages 89\u2013103, New York, NY, 1983.\nACM Press. Cited on 429\nMottola L. and Picco G. P . Programming Wireless Sensor Networks: Fundamental\nConcepts and State of the Art. ACM Computing Surveys , 43(3):19:1\u201319:51, Apr. 2011.\nCited on 48\nMPI Forum . MPI: A Message-Passing Interface Standard, Version 3.0. Technical report,\nUniversity of Tennessee, Knoxville, Tennessee, Sept. 2012. Cited on 206\nM\u00fchl G., Fiege L., and Pietzuch P . Distributed Event-Based Systems . Springer-Verlag,\nBerlin, 2006. Cited on 67\nMuntz D. and Honeyman P . Multi-level Caching in Distributed File Systems. In Winter\nTechnical Conference , pages 305\u2013313, San Francisco, CA, Jan. 1992. USENIX. Cited on\n388\nMurty J. Programming Amazon Web Services . O\u2019Reilly & Associates, Sebastopol, CA.,\n2008. Cited on 31, 65\nNaur P . and Randell B. Report on the NATO Software Engineering Conference 1968.\nTechnical report, Scienti\ufb01c Affairs Division NATO, Brussels, Belgium, Oct. 1968.\nCited on 7\nNayate A., Dahlin M., and Iyengar A. Transparent Information Dissemination. In\nMiddleware 2004 , volume 3231 of Lecture Notes in Computer Science , pages 212\u2013231,\nBerlin, Oct. 2004. ACM/IFIP/USENIX, Springer-Verlag. Cited on 91\nNeedham R. and Schroeder M. Using Encryption for Authentication in Large Networks\nof Computers. Communications of the ACM , 21(12):993\u2013999, Dec. 1978. Cited on 517\nNelson B. Remote Procedure Call . Ph.D., Carnegie-Mellon University, 1981. Cited on 469\nNeuman B. Proxy-Based Authorization and Accounting for Distributed Systems.\nIn13th International Conference on Distributed Computing Systems , pages 283\u2013291,\nPittsburgh, May 1993. IEEE. Cited on 550\nNeuman B. Scale in Distributed Systems. In Casavant T. and Singhal M., editors,\nReadings in Distributed Computing Systems , pages 463\u2013489. IEEE Computer Society\nPress, Los Alamitos, CA., 1994. Cited on 15, 20\nNeuman C., Yu T., Hartman S., and Raeburn K. The Kerberos Network Authentication\nService. RFC 4120, July 2005. Cited on 527\nNewman M. Networks, An Introduction . Oxford University Press, Oxford, UK, 2010.\nCited on 46\nNg E. and Zhang H. Predicting Internet Network Distance with Coordinates-Based\nApproaches. In 21st INFOCOM Conference , Los Alamitos, CA., June 2002. IEEE, IEEE\nComputer Society Press. Cited on 341\nNoble B., Fleis B., and Kim M. A Case for Fluid Replication. In NetStore\u201999 , Seattle,\nWA, Oct. 1999. Cited on 388\nNyers L. and Jelasity M. A Comparative Study of Spanning Tree and Gossip Protocols\nfor Aggregation. Concurrency & Computation: Practice and Experience , 2015. Cited on\n477\nNygren E., Sitaraman R. K., and Sun J. The Akamai Network: A Platform for High-\nPerformance Internet Applications. Operating Systems Review , 44(3):2\u201319, July 2010.\nCited on 415\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "572 BIBLIOGRAPHY\nOaks S. Java Security . O\u2019Reilly & Associates, Sebastopol, CA., 2nd edition, 2001. Cited\non 535\nOASIS . AMQP , Protocol speci\ufb01cation, Version 1.0, Oct. 2011. Cited on 218\nObraczka K. Multicast Transport Protocols: A Survey and Taxonomy. IEEE Communi-\ncations Magazine , 36(1):94\u2013102, Jan. 1998. Cited on 221\nOikonomou K. and Stavrakakis I. Performance Analysis of Probabilistic Flooding\nUsing Random Graphs. In World of Wireless, Mobile and Multimedia Networks, 2007.\nWoWMoM 2007. IEEE International Symposium on a , pages 1\u20136, June 2007. doi:\n10.1109/WOWMOM.2007.4351694. Cited on 227\nOMG . The Common Object Request Broker: Core Speci\ufb01cation, revision 3.0.3. OMG\nDocument formal/04-03-12, Object Management Group, Framingham, MA, Mar.\n2004a. Cited on 135\nOMG . UML 2.0 Superstructure Speci\ufb01cation. OMG Document ptc/04-10-02, Object\nManagement Group, Framingham, MA, Oct. 2004b. Cited on 56\nOppenheimer D., Albrecht J., Patterson D., and Vahdat A. Design and Implementation\nTradeoffs for Wide-Area Resource Discovery. In 14th International Symposium on\nHigh Performance Distributed Computing , Los Alamitos, CA., July 2005. IEEE, IEEE\nComputer Society Press. Cited on 293\nOracle . Java Remote Method Invocation Speci\ufb01cation , 2010. Cited on 170\nOram A., editor. Peer-to-Peer: Harnessing the Power of Disruptive Technologies . O\u2019Reilly &\nAssociates, Sebastopol, CA., 2001. Cited on 23\n\u00d6zsu T. and Valduriez P . Principles of Distributed Database Systems . Springer-Verlag,\nBerlin, 3rd edition, 2011. Cited on 81, 385, 490\nPai V ., Aron M., Banga G., Svendsen M., Druschel P ., Zwaenepoel W., and Nahum\nE. Locality-Aware Request Distribution in Cluster-Based Network Servers. In\n8th International Conference on Architectural Support for Programming Languages and\nOperating Systems , pages 205\u2013216, New York, NY, Oct. 1998. ACM, ACM Press. Cited\non 143\nPanzieri F. and Shrivastava S. Rajdoot: A Remote Procedure Call Mechanism with\nOrphan Detection and Killing. IEEE Transactions on Software Engineering , 14(1):30\u201337,\nJan. 1988. Cited on 470\nPappas V ., Massey D., Terzis A., and Zhang L. A Comparative Study of the DNS\nDesign with DHT-Based Alternatives. In 25th INFOCOMConference , Los Alamitos,\nCA., May 2006. IEEE, IEEE Computer Society Press. Cited on 277\nParlavantzas N. and Coulson G. Designing and Constructing Modi\ufb01able Middleware\nusing Component Frameworks. IET Software , 1(4):113\u2013126, Aug. 2007. Cited on 72,\n75\nPautasso C., Zimmermann O., and Leymann F. Restful Web Services vs. \"Big\" Web\nServices: Making the Right Architectural Decision. In 17th International World Wide\nWeb Conference , pages 805\u2013814, New York, NY, Aug. 2008. ACM Press. Cited on 64,\n65\nPease M., Shostak R., and Lamport L. Reaching Agreement in the Presence of Faults.\nJournal of the ACM , 27(2):228\u2013234, Apr. 1980. Cited on 429\nPeng T., Leckie C., and Ramamohanarao K. Survey of Network-based Defense Mecha-\nnisms Countering the DoS and DDoS Problems. ACM Computing Surveys , 39(1):3,\n2007. Cited on 540\nPerkins C. IP Mobility Support in IPv4, Revised. RFC 5944, Nov. 2010. Cited on 44\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n572 BIBLIOGRAPHY\nOaks S. Java Security . O\u2019Reilly & Associates, Sebastopol, CA., 2nd edition, 2001. Cited\non 535\nOASIS . AMQP , Protocol speci\ufb01cation, Version 1.0, Oct. 2011. Cited on 218\nObraczka K. Multicast Transport Protocols: A Survey and Taxonomy. IEEE Communi-\ncations Magazine , 36(1):94\u2013102, Jan. 1998. Cited on 221\nOikonomou K. and Stavrakakis I. Performance Analysis of Probabilistic Flooding\nUsing Random Graphs. In World of Wireless, Mobile and Multimedia Networks, 2007.\nWoWMoM 2007. IEEE International Symposium on a , pages 1\u20136, June 2007. doi:\n10.1109/WOWMOM.2007.4351694. Cited on 227\nOMG . The Common Object Request Broker: Core Speci\ufb01cation, revision 3.0.3. OMG\nDocument formal/04-03-12, Object Management Group, Framingham, MA, Mar.\n2004a. Cited on 135\nOMG . UML 2.0 Superstructure Speci\ufb01cation. OMG Document ptc/04-10-02, Object\nManagement Group, Framingham, MA, Oct. 2004b. Cited on 56\nOppenheimer D., Albrecht J., Patterson D., and Vahdat A. Design and Implementation\nTradeoffs for Wide-Area Resource Discovery. In 14th International Symposium on\nHigh Performance Distributed Computing , Los Alamitos, CA., July 2005. IEEE, IEEE\nComputer Society Press. Cited on 293\nOracle . Java Remote Method Invocation Speci\ufb01cation , 2010. Cited on 170\nOram A., editor. Peer-to-Peer: Harnessing the Power of Disruptive Technologies . O\u2019Reilly &\nAssociates, Sebastopol, CA., 2001. Cited on 23\n\u00d6zsu T. and Valduriez P . Principles of Distributed Database Systems . Springer-Verlag,\nBerlin, 3rd edition, 2011. Cited on 81, 385, 490\nPai V ., Aron M., Banga G., Svendsen M., Druschel P ., Zwaenepoel W., and Nahum\nE. Locality-Aware Request Distribution in Cluster-Based Network Servers. In\n8th International Conference on Architectural Support for Programming Languages and\nOperating Systems , pages 205\u2013216, New York, NY, Oct. 1998. ACM, ACM Press. Cited\non 143\nPanzieri F. and Shrivastava S. Rajdoot: A Remote Procedure Call Mechanism with\nOrphan Detection and Killing. IEEE Transactions on Software Engineering , 14(1):30\u201337,\nJan. 1988. Cited on 470\nPappas V ., Massey D., Terzis A., and Zhang L. A Comparative Study of the DNS\nDesign with DHT-Based Alternatives. In 25th INFOCOMConference , Los Alamitos,\nCA., May 2006. IEEE, IEEE Computer Society Press. Cited on 277\nParlavantzas N. and Coulson G. Designing and Constructing Modi\ufb01able Middleware\nusing Component Frameworks. IET Software , 1(4):113\u2013126, Aug. 2007. Cited on 72,\n75\nPautasso C., Zimmermann O., and Leymann F. Restful Web Services vs. \"Big\" Web\nServices: Making the Right Architectural Decision. In 17th International World Wide\nWeb Conference , pages 805\u2013814, New York, NY, Aug. 2008. ACM Press. Cited on 64,\n65\nPease M., Shostak R., and Lamport L. Reaching Agreement in the Presence of Faults.\nJournal of the ACM , 27(2):228\u2013234, Apr. 1980. Cited on 429\nPeng T., Leckie C., and Ramamohanarao K. Survey of Network-based Defense Mecha-\nnisms Countering the DoS and DDoS Problems. ACM Computing Surveys , 39(1):3,\n2007. Cited on 540\nPerkins C. IP Mobility Support in IPv4, Revised. RFC 5944, Nov. 2010. Cited on 44\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 573\nPerkins C., Johnson D., and Arkko J. Mobility Support in IPv6. RFC 6275, July 2011.\nCited on 44\nPeterson L., Bavier A., Fiuczynski M. E., and Muir S. Experiences Building PlanetLab.\nIn7th Symposium on Operating System Design and Implementation , pages 351\u2013366,\nBerkeley, CA, Nov. 2006. USENIX, USENIX. Cited on 149\nP\ufb02eeger C. Security in Computing . Prentice Hall, Upper Saddle River, N.J., 3rd edition,\n2003. Cited on 502\nPike R., Presotto D., Dorward S., Flandrena B., Thompson K., Trickey H., and Winter-\nbottom P . Plan 9 from Bell Labs. Computing Systems , 8(3):221\u2013254, Summer 1995.\nCited on 258\nPinzari G. NX X Protocol Compression. Technical Report D-309/3-NXP-DOC, NoMa-\nchine, Rome, Italy, Sept. 2003. Cited on 126\nPitoura E. and Samaras G. Locating Objects in Mobile Computing. IEEE Transactions\non Knowledge and Data Engineering , 13(4):571\u2013592, July 2001. Cited on 251\nPlainfosse D. and Shapiro M. A Survey of Distributed Garbage Collection Techniques.\nInProceedings International Workshop on Memory Management , volume 986 of Lecture\nNotes in Computer Science , pages 211\u2013249. Springer-Verlag, Berlin, Sept. 1995. Cited\non 244\nPlummer D. Ethernet Address Resolution Protocol. RFC 826, Nov. 1982. Cited on 242\nPodling S. and Boszormenyi L. A Survey of Web Cache Replacement Strategies. ACM\nComputing Surveys , 35(4):374\u2013398, Dec. 2003. Cited on 412\nPopek G. J. and Goldberg R. P . Formal Requirements for Virtualizable Third Generation\nArchitectures. Communications of the ACM , 17(7):412\u2013421, July 1974. Cited on 120,\n121\nPopescu A., Constantinescu D., Erman D., and Ilie D. A Survey of Reliable Multicast\nCommunication. In 3rd Conference on Next Generation Internet Networks , pages 111\u2013\n118, May 2007. Cited on 473\nPopescu A. M., Tudorache G. I., Peng B., and Kemp A. H. Surveying Position Based\nRouting Protocols for Wireless Sensor and Ad-hoc Networks. International Journal on\nCommunication Networks and Information Security , 4(1):41\u201367, Apr. 2012. Cited on 340\nPopescu B., Sacha J., Steen M.van , Crispo B., Tanenbaum A., and Kuz I. Securely\nReplicated Web Documents. In 19th International Parallel & Distributed Processing\nSymposium , Los Alamitos, CA., Apr. 2005. IEEE, IEEE Computer Society Press. Cited\non 541\nPoslad S. Ubiquitous Computing: Smart Devices, Environments and Interactions . John\nWiley, New York, 2009. Cited on 41, 43\nPostel J. Simple Mail Transfer Protocol. RFC 821, Aug. 1982. Cited on 212\nPostel J. and Reynolds J. File Transfer Protocol. RFC 995, Oct. 1985. Cited on 169\nPouwelse J. A., Garbacki P ., Epema D. H. J., and Sips H. J. The Bittorrent P2P File-\nSharing System: Measurements and Analysis. In 4th International Workshop on\nPeer-to-Peer Systems , volume 3640 of Lecture Notes in Computer Science , pages 205\u2013216,\nBerlin, Feb. 2005. Springer-Verlag. Cited on 91\nPradhan D. Fault-Tolerant Computer System Design . Prentice Hall, Englewood Cliffs,\nN.J., 1996. Cited on 425\nPrisco R. D., Lampson B., and Lynch N. Revisiting the Paxos Algorithm. In Mavroni-\ncolas M. and Tsigas P ., editors, 11th International Workshop on Distributed Algorithms ,\nvolume 1320 of Lecture Notes in Computer Science . Springer-Verlag, Berlin, Sept. 1997.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 573\nPerkins C., Johnson D., and Arkko J. Mobility Support in IPv6. RFC 6275, July 2011.\nCited on 44\nPeterson L., Bavier A., Fiuczynski M. E., and Muir S. Experiences Building PlanetLab.\nIn7th Symposium on Operating System Design and Implementation , pages 351\u2013366,\nBerkeley, CA, Nov. 2006. USENIX, USENIX. Cited on 149\nP\ufb02eeger C. Security in Computing . Prentice Hall, Upper Saddle River, N.J., 3rd edition,\n2003. Cited on 502\nPike R., Presotto D., Dorward S., Flandrena B., Thompson K., Trickey H., and Winter-\nbottom P . Plan 9 from Bell Labs. Computing Systems , 8(3):221\u2013254, Summer 1995.\nCited on 258\nPinzari G. NX X Protocol Compression. Technical Report D-309/3-NXP-DOC, NoMa-\nchine, Rome, Italy, Sept. 2003. Cited on 126\nPitoura E. and Samaras G. Locating Objects in Mobile Computing. IEEE Transactions\non Knowledge and Data Engineering , 13(4):571\u2013592, July 2001. Cited on 251\nPlainfosse D. and Shapiro M. A Survey of Distributed Garbage Collection Techniques.\nInProceedings International Workshop on Memory Management , volume 986 of Lecture\nNotes in Computer Science , pages 211\u2013249. Springer-Verlag, Berlin, Sept. 1995. Cited\non 244\nPlummer D. Ethernet Address Resolution Protocol. RFC 826, Nov. 1982. Cited on 242\nPodling S. and Boszormenyi L. A Survey of Web Cache Replacement Strategies. ACM\nComputing Surveys , 35(4):374\u2013398, Dec. 2003. Cited on 412\nPopek G. J. and Goldberg R. P . Formal Requirements for Virtualizable Third Generation\nArchitectures. Communications of the ACM , 17(7):412\u2013421, July 1974. Cited on 120,\n121\nPopescu A., Constantinescu D., Erman D., and Ilie D. A Survey of Reliable Multicast\nCommunication. In 3rd Conference on Next Generation Internet Networks , pages 111\u2013\n118, May 2007. Cited on 473\nPopescu A. M., Tudorache G. I., Peng B., and Kemp A. H. Surveying Position Based\nRouting Protocols for Wireless Sensor and Ad-hoc Networks. International Journal on\nCommunication Networks and Information Security , 4(1):41\u201367, Apr. 2012. Cited on 340\nPopescu B., Sacha J., Steen M.van , Crispo B., Tanenbaum A., and Kuz I. Securely\nReplicated Web Documents. In 19th International Parallel & Distributed Processing\nSymposium , Los Alamitos, CA., Apr. 2005. IEEE, IEEE Computer Society Press. Cited\non 541\nPoslad S. Ubiquitous Computing: Smart Devices, Environments and Interactions . John\nWiley, New York, 2009. Cited on 41, 43\nPostel J. Simple Mail Transfer Protocol. RFC 821, Aug. 1982. Cited on 212\nPostel J. and Reynolds J. File Transfer Protocol. RFC 995, Oct. 1985. Cited on 169\nPouwelse J. A., Garbacki P ., Epema D. H. J., and Sips H. J. The Bittorrent P2P File-\nSharing System: Measurements and Analysis. In 4th International Workshop on\nPeer-to-Peer Systems , volume 3640 of Lecture Notes in Computer Science , pages 205\u2013216,\nBerlin, Feb. 2005. Springer-Verlag. Cited on 91\nPradhan D. Fault-Tolerant Computer System Design . Prentice Hall, Englewood Cliffs,\nN.J., 1996. Cited on 425\nPrisco R. D., Lampson B., and Lynch N. Revisiting the Paxos Algorithm. In Mavroni-\ncolas M. and Tsigas P ., editors, 11th International Workshop on Distributed Algorithms ,\nvolume 1320 of Lecture Notes in Computer Science . Springer-Verlag, Berlin, Sept. 1997.\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "574 BIBLIOGRAPHY\nCited on 438\nQin F., Tucek J., Sundaresan J., and Zhou Y. Rx: Treating Bugs as Allergies - A\nSafe Method to Survive Software Failures. In 20th Symposium on Operating System\nPrinciples , pages 235\u2013248, New York, NY, Oct. 2005. ACM, ACM Press. Cited on 499\nQiu L., Padmanabhan V ., and Voelker G. On the Placement of Web Server Replicas. In\n20th INFOCOM Conference , pages 1587\u20131596, Los Alamitos, CA., Apr. 2001. IEEE,\nIEEE Computer Society Press. Cited on 383, 384\nRabinovich M. and Spastscheck O. Web Caching and Replication . Addison-Wesley,\nReading, MA., 2002. Cited on 91, 409\nRabinovich M., Rabinovich I., Rajaraman R., and Aggarwal A. A Dynamic Object\nReplication and Migration Protocol for an Internet Hosting Service. In 19th Interna-\ntional Conference on Distributed Computing Systems , pages 101\u2013113, Austin, TX, June\n1999. IEEE. Cited on 386\nRadia S. Names, Contexts, and Closure Mechanisms in Distributed Computing Environments .\nPh.D., University of Waterloo, Ontario, 1989. Cited on 260\nRadoslavov P ., Govindan R., and Estrin D. Topology-Informed Internet Replica Place-\nment. In 6th Web Caching Workshop , Amsterdam, June 2001. North-Holland. Cited\non 383\nRai V ., Sivasubramanian S., Bhulai S., Garbacki P ., and Steen M.van . A Multi Phased\nApproach for Modeling and Analysis of the Bittorrent Protocol. In 27th International\nConference on Distributed Computing Systems , Los Alamitos, CA., June 2007. IEEE,\nIEEE Computer Society Press. Cited on 93\nRamanathan P ., Shin K., and Butler R. Fault-Tolerant Clock Synchronization in Dis-\ntributed Systems. Computer , 23(10):33\u201342, Oct. 1990. Cited on 303\nRaynal M. and Singhal M. Logical Time: Capturing Causality in Distributed Systems.\nComputer , 29(2):49\u201356, Feb. 1996. Cited on 312\nReiter M. How to Securely Replicate Services. ACM Transactions on Programming\nLanguages and Systems , 16(3):986\u20131009, May 1994. Cited on 525\nReiter M., Birman K., and Renesse R.van . A Security Architecture for Fault-Tolerant\nSystems. ACM Transactions on Computer Systems , 12(4):340\u2013371, Nov. 1994. Cited on\n546\nReynolds J. and Postel J. Assigned Numbers. RFC 1700, Oct. 1994. Cited on 129\nRicart G. and Agrawala A. An Optimal Algorithm for Mutual Exclusion in Computer\nNetworks. Communications of the ACM , 24(1):9\u201317, Jan. 1981. Cited on 323\nRichardson T., Stafford-Fraser Q., Wood K. R., and Hopper A. Virtual Network\nComputing. IEEE Internet Computing , 2(1):33\u201338, Jan. 1998. Cited on 127\nRisson J. and Moors T. Survey of Research towards Robust Peer-to-Peer Networks:\nSearch Methods. Computer Networks , 50(17):3485\u20133521, 2006. Cited on 85\nRizzo L. Effective Erasure Codes for Reliable Computer Communication Protocols.\nACM Computer Communications Review , 27(2):24\u201336, Apr. 1997. Cited on 491\nRobbins K. and Robbins S. UNIX Systems Programming . Prentice Hall, Englewood\nCliffs, N.J., 2003. Cited on 104, 111\nRobin J. S. and Irvine C. E. Analysis of the Intel Pentium\u2019s Ability to Support a\nSecure Virtual Machine Monitor. In 9th USENIX Security Symposium , pages 129\u2013144,\nBerkeley, CA, 2000. USENIX. Cited on 121\nRodrigues L., Fonseca H., and Verissimo P . Totally Ordered Multicast in Large-Scale\nSystems. In 16th International Conference on Distributed Computing Systems , pages\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n574 BIBLIOGRAPHY\nCited on 438\nQin F., Tucek J., Sundaresan J., and Zhou Y. Rx: Treating Bugs as Allergies - A\nSafe Method to Survive Software Failures. In 20th Symposium on Operating System\nPrinciples , pages 235\u2013248, New York, NY, Oct. 2005. ACM, ACM Press. Cited on 499\nQiu L., Padmanabhan V ., and Voelker G. On the Placement of Web Server Replicas. In\n20th INFOCOM Conference , pages 1587\u20131596, Los Alamitos, CA., Apr. 2001. IEEE,\nIEEE Computer Society Press. Cited on 383, 384\nRabinovich M. and Spastscheck O. Web Caching and Replication . Addison-Wesley,\nReading, MA., 2002. Cited on 91, 409\nRabinovich M., Rabinovich I., Rajaraman R., and Aggarwal A. A Dynamic Object\nReplication and Migration Protocol for an Internet Hosting Service. In 19th Interna-\ntional Conference on Distributed Computing Systems , pages 101\u2013113, Austin, TX, June\n1999. IEEE. Cited on 386\nRadia S. Names, Contexts, and Closure Mechanisms in Distributed Computing Environments .\nPh.D., University of Waterloo, Ontario, 1989. Cited on 260\nRadoslavov P ., Govindan R., and Estrin D. Topology-Informed Internet Replica Place-\nment. In 6th Web Caching Workshop , Amsterdam, June 2001. North-Holland. Cited\non 383\nRai V ., Sivasubramanian S., Bhulai S., Garbacki P ., and Steen M.van . A Multi Phased\nApproach for Modeling and Analysis of the Bittorrent Protocol. In 27th International\nConference on Distributed Computing Systems , Los Alamitos, CA., June 2007. IEEE,\nIEEE Computer Society Press. Cited on 93\nRamanathan P ., Shin K., and Butler R. Fault-Tolerant Clock Synchronization in Dis-\ntributed Systems. Computer , 23(10):33\u201342, Oct. 1990. Cited on 303\nRaynal M. and Singhal M. Logical Time: Capturing Causality in Distributed Systems.\nComputer , 29(2):49\u201356, Feb. 1996. Cited on 312\nReiter M. How to Securely Replicate Services. ACM Transactions on Programming\nLanguages and Systems , 16(3):986\u20131009, May 1994. Cited on 525\nReiter M., Birman K., and Renesse R.van . A Security Architecture for Fault-Tolerant\nSystems. ACM Transactions on Computer Systems , 12(4):340\u2013371, Nov. 1994. Cited on\n546\nReynolds J. and Postel J. Assigned Numbers. RFC 1700, Oct. 1994. Cited on 129\nRicart G. and Agrawala A. An Optimal Algorithm for Mutual Exclusion in Computer\nNetworks. Communications of the ACM , 24(1):9\u201317, Jan. 1981. Cited on 323\nRichardson T., Stafford-Fraser Q., Wood K. R., and Hopper A. Virtual Network\nComputing. IEEE Internet Computing , 2(1):33\u201338, Jan. 1998. Cited on 127\nRisson J. and Moors T. Survey of Research towards Robust Peer-to-Peer Networks:\nSearch Methods. Computer Networks , 50(17):3485\u20133521, 2006. Cited on 85\nRizzo L. Effective Erasure Codes for Reliable Computer Communication Protocols.\nACM Computer Communications Review , 27(2):24\u201336, Apr. 1997. Cited on 491\nRobbins K. and Robbins S. UNIX Systems Programming . Prentice Hall, Englewood\nCliffs, N.J., 2003. Cited on 104, 111\nRobin J. S. and Irvine C. E. Analysis of the Intel Pentium\u2019s Ability to Support a\nSecure Virtual Machine Monitor. In 9th USENIX Security Symposium , pages 129\u2013144,\nBerkeley, CA, 2000. USENIX. Cited on 121\nRodrigues L., Fonseca H., and Verissimo P . Totally Ordered Multicast in Large-Scale\nSystems. In 16th International Conference on Distributed Computing Systems , pages\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 575\n503\u2013510, Hong Kong, May 1996. IEEE. Cited on 402\nRodriguez P ., Spanner C., and Biersack E. Analysis of Web Caching Architecture:\nHierarchical and Distributed Caching. IEEE/ACM Transactions on Networking , 21(4):\n404\u2013418, Aug. 2001. Cited on 411\nRosenblum M. and Gar\ufb01nkel T. Virtual Machine Monitors: Current Technology and\nFuture Trends. Computer , 38(5):39\u201347, May 2005. Cited on 120\nRoussos G., Marsh A. J., and Maglavera S. Enabling Pervasive Computing with Smart\nPhones. IEEE Pervasive Computing , 4(2):20\u201326, Apr. 2005. Cited on 40\nRowstron A. and Druschel P . Pastry: Scalable, Distributed Object Location and Routing\nfor Large-Scale Peer-to-Peer Systems. In Middleware 2001 , volume 2218 of Lecture\nNotes in Computer Science , pages 329\u2013350, Berlin, Nov. 2001. Springer-Verlag. Cited\non 222, 251\nSagan H. Space-Filling Curves . Springer-Verlag, Berlin, 1994. Cited on 290\nSaltzer J. and Kaashoek M. Principles of Computer System Design, An Introduction .\nMorgan Kaufman, San Mateo, CA., 2009. Cited on 76\nSaltzer J. and Schroeder M. The Protection of Information in Computer Systems.\nProceedings of the IEEE , 63(9):1278\u20131308, Sept. 1975. Cited on 531\nSaltzer J., Reed D., and Clark D. End-to-End Arguments in System Design. ACM\nTransactions on Computer Systems , 2(4):277\u2013288, Nov. 1984. Cited on 321\nSandhu R. S., Coyne E. J., Feinstein H. L., and Youman C. E. Role-Based Access Control\nModels. Computer , 29(2):38\u201347, Feb. 1996. Cited on 532\nSantoro N. Design and Analysis of Distributed Algorithms . John Wiley, New York, 2007.\nCited on 298\nSaroiu S., Gummadi P . K., and Gribble S. D. Measuring and Analyzing the Character-\nistics of Napster and Gnutella Hosts. ACM Multimedia Systems , 9(2):170\u2013184, Aug.\n2003. Cited on 91\nSaxena P . and Rai J. A Survey of Permission-based Distributed Mutual Exclusion\nAlgorithms. Computer Standards and Interfaces , 25(2):159\u2013181, May 2003. Cited on 322\nSchildt H. Java: The Complete Reference . Oracle Press, New York, NY, 9th edition, 2010.\nCited on 138\nSchlosser M., Sintek M., Decker S., and Nejdl W. HyperCuP \u00e2 \u02d8A\u00b8 S Hypercubes, Ontolo-\ngies, and Ef\ufb01cient Search on Peer-to-Peer Networks. In 1st International Workshop on\nAgents and Peer-to-Peer Computing , volume 2530 of Lecture Notes in Computer Science ,\npages 112\u2013124, Berlin, July 2002. Springer-Verlag. Cited on 227\nSchmidt A. Implicit Human Computer Interaction Through Context. Personal and\nUbiquitous Computing , 4(2-3):191\u2013199, June 2000. Cited on 41\nSchmidt C. and Parashar M. Squid: Enabling Search in DHT-based systems. Journal of\nParallel and Distributed Computing , 68:962\u2013975, 2008. Cited on 291\nSchmidt D., Stal M., Rohnert H., and Buschmann F. Pattern-Oriented Software Archi-\ntecture \u2013 Patterns for Concurrent and Networked Objects . John Wiley, New York, 2000.\nCited on 73\nSchneider F. Implementing Fault-Tolerant Services Using the State Machine Approach:\nA Tutorial. ACM Computing Surveys , 22(4):299\u2013320, Dec. 1990. Cited on 314, 389\nSchneier B. Secrets and Lies . John Wiley, New York, 2000. Cited on 512\nSchneier B. Applied Cryptography . John Wiley, New York, 2nd edition, 1996. Cited on\n512, 526\nSchulzrinne H., Casner S., Frederick R., and Jacobson V . RTP: A Transport Protocol for\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 575\n503\u2013510, Hong Kong, May 1996. IEEE. Cited on 402\nRodriguez P ., Spanner C., and Biersack E. Analysis of Web Caching Architecture:\nHierarchical and Distributed Caching. IEEE/ACM Transactions on Networking , 21(4):\n404\u2013418, Aug. 2001. Cited on 411\nRosenblum M. and Gar\ufb01nkel T. Virtual Machine Monitors: Current Technology and\nFuture Trends. Computer , 38(5):39\u201347, May 2005. Cited on 120\nRoussos G., Marsh A. J., and Maglavera S. Enabling Pervasive Computing with Smart\nPhones. IEEE Pervasive Computing , 4(2):20\u201326, Apr. 2005. Cited on 40\nRowstron A. and Druschel P . Pastry: Scalable, Distributed Object Location and Routing\nfor Large-Scale Peer-to-Peer Systems. In Middleware 2001 , volume 2218 of Lecture\nNotes in Computer Science , pages 329\u2013350, Berlin, Nov. 2001. Springer-Verlag. Cited\non 222, 251\nSagan H. Space-Filling Curves . Springer-Verlag, Berlin, 1994. Cited on 290\nSaltzer J. and Kaashoek M. Principles of Computer System Design, An Introduction .\nMorgan Kaufman, San Mateo, CA., 2009. Cited on 76\nSaltzer J. and Schroeder M. The Protection of Information in Computer Systems.\nProceedings of the IEEE , 63(9):1278\u20131308, Sept. 1975. Cited on 531\nSaltzer J., Reed D., and Clark D. End-to-End Arguments in System Design. ACM\nTransactions on Computer Systems , 2(4):277\u2013288, Nov. 1984. Cited on 321\nSandhu R. S., Coyne E. J., Feinstein H. L., and Youman C. E. Role-Based Access Control\nModels. Computer , 29(2):38\u201347, Feb. 1996. Cited on 532\nSantoro N. Design and Analysis of Distributed Algorithms . John Wiley, New York, 2007.\nCited on 298\nSaroiu S., Gummadi P . K., and Gribble S. D. Measuring and Analyzing the Character-\nistics of Napster and Gnutella Hosts. ACM Multimedia Systems , 9(2):170\u2013184, Aug.\n2003. Cited on 91\nSaxena P . and Rai J. A Survey of Permission-based Distributed Mutual Exclusion\nAlgorithms. Computer Standards and Interfaces , 25(2):159\u2013181, May 2003. Cited on 322\nSchildt H. Java: The Complete Reference . Oracle Press, New York, NY, 9th edition, 2010.\nCited on 138\nSchlosser M., Sintek M., Decker S., and Nejdl W. HyperCuP \u00e2 \u02d8A\u00b8 S Hypercubes, Ontolo-\ngies, and Ef\ufb01cient Search on Peer-to-Peer Networks. In 1st International Workshop on\nAgents and Peer-to-Peer Computing , volume 2530 of Lecture Notes in Computer Science ,\npages 112\u2013124, Berlin, July 2002. Springer-Verlag. Cited on 227\nSchmidt A. Implicit Human Computer Interaction Through Context. Personal and\nUbiquitous Computing , 4(2-3):191\u2013199, June 2000. Cited on 41\nSchmidt C. and Parashar M. Squid: Enabling Search in DHT-based systems. Journal of\nParallel and Distributed Computing , 68:962\u2013975, 2008. Cited on 291\nSchmidt D., Stal M., Rohnert H., and Buschmann F. Pattern-Oriented Software Archi-\ntecture \u2013 Patterns for Concurrent and Networked Objects . John Wiley, New York, 2000.\nCited on 73\nSchneider F. Implementing Fault-Tolerant Services Using the State Machine Approach:\nA Tutorial. ACM Computing Surveys , 22(4):299\u2013320, Dec. 1990. Cited on 314, 389\nSchneier B. Secrets and Lies . John Wiley, New York, 2000. Cited on 512\nSchneier B. Applied Cryptography . John Wiley, New York, 2nd edition, 1996. Cited on\n512, 526\nSchulzrinne H., Casner S., Frederick R., and Jacobson V . RTP: A Transport Protocol for\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "576 BIBLIOGRAPHY\nReal-Time Applications. RFC 3550, July 2003. Cited on 169\nSebesta R. Programming the World Wide Web . Addison-Wesley, Reading, MA., 8th\nedition, 2006. Cited on 100\nSereno M. and Gaeta R. Generalized Probabilistic Flooding in Unstructured Peer-to-\nPeer Networks. IEEE Transactions on Parallel and Distributed Systems , 22(12):2055\u20132062,\n2011. ISSN 1045-9219. Cited on 227\nSetty V ., Voulgaris S., Vitenberg R., and Steen M.van . PolderCast: Fast, Robust, and\nScalable Architecture for P2P Pub/Sub. In Middleware 2012 , volume 7662 of Lecture\nNotes in Computer Science , pages 271\u2013291, Berlin, Dec. 2012. ACM/IFIP/USENIX,\nSpringer-Verlag. Cited on 347\nShapiro M., Dickman P ., and Plainfosse D. SSP Chains: Robust, Distributed References\nSupporting Acyclic Garbage Collection. Technical Report 1799, INRIA, Rocquencourt,\nFrance, Nov. 1992. Cited on 243\nShaw M. and Clements P . A Field Guide to Boxology: Preliminary Classi\ufb01cation of\nArchitectural Styles for Software Systems. In 21st International Computer Software and\nApplications Conference , pages 6\u201313, Aug. 1997. Cited on 56\nShen X., Yu H., Buford J., and Akon M., editors. Handbook of Peer-to-Peer Networking .\nSpringer-Verlag, Berlin, 2010. Cited on 559, 564\nShepler S., Callaghan B., Robinson D., Thurlow R., Beame C., Eisler M., and Noveck D.\nNetwork File System (NFS) Version 4 Protocol. RFC 3530, Apr. 2003. Cited on 94,\n262\nSheth A. P . and Larson J. A. Federated Database Systems for Managing Distributed,\nHeterogeneous, and Autonomous Databases. ACM Computing Surveys , 22(3):183\u2013236,\nSept. 1990. Cited on 386\nShin M., Park M., Oh D., Kim B., and Lee J. Survey on the Clock Synchronization\nSchemes for Propagation Delay Measurement. International Journal of Advanced\nScience and Technology , 35:139\u2013140, Oct. 2011. Cited on 303\nShoch J. Internetwork Naming, Addressing, and Routing. In 17th International Computer\nConference , pages 72\u201379, Los Alamitos, CA., 1978. IEEE, IEEE Computer Society\nPress. Cited on 240\nShooman M. L. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis,\nand Design . John Wiley, New York, 2002. Cited on 424\nShriram A. and Kaur J. Empirical Evaluation of Techniques for Measuring Available\nBandwidth. In 26th INFOCOM Conference , pages 2162\u20132170, Los Alamitos, CA., 2007.\nIEEE, IEEE Computer Society Press. Cited on 413\nSilberschatz A., Galvin P ., and Gagne G. Operating System Concepts . John Wiley, New\nYork, 9th edition, 2012. Cited on 258\nSilva S. S., Silva R. M., Pinto R. C., and Salles R. M. Botnets: A survey. Computer\nNetworks , 57(2):378 \u2013 403, 2013. Cited on 539\nSinghal M. and Shivaratri N. Advanced Concepts in Operating Systems: Distributed,\nDatabase, and Multiprocessor Operating Systems . McGraw-Hill, New York, 1994. Cited\non 492\nSivasubramanian S., Pierre G., and Steen M.van . Replicating Web Applications On-\nDemand. In 1st International Conference on Services Computing , pages 227\u2013236, Los\nAlamitos, CA., Sept. 2004a. IEEE, IEEE Computer Society Press. Cited on 418\nSivasubramanian S., Szymaniak M., Pierre G., and Steen M.van . Replication for Web\nHosting Systems. ACM Computing Surveys , 36(3):1\u201344, Sept. 2004b. Cited on 146, 412\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n576 BIBLIOGRAPHY\nReal-Time Applications. RFC 3550, July 2003. Cited on 169\nSebesta R. Programming the World Wide Web . Addison-Wesley, Reading, MA., 8th\nedition, 2006. Cited on 100\nSereno M. and Gaeta R. Generalized Probabilistic Flooding in Unstructured Peer-to-\nPeer Networks. IEEE Transactions on Parallel and Distributed Systems , 22(12):2055\u20132062,\n2011. ISSN 1045-9219. Cited on 227\nSetty V ., Voulgaris S., Vitenberg R., and Steen M.van . PolderCast: Fast, Robust, and\nScalable Architecture for P2P Pub/Sub. In Middleware 2012 , volume 7662 of Lecture\nNotes in Computer Science , pages 271\u2013291, Berlin, Dec. 2012. ACM/IFIP/USENIX,\nSpringer-Verlag. Cited on 347\nShapiro M., Dickman P ., and Plainfosse D. SSP Chains: Robust, Distributed References\nSupporting Acyclic Garbage Collection. Technical Report 1799, INRIA, Rocquencourt,\nFrance, Nov. 1992. Cited on 243\nShaw M. and Clements P . A Field Guide to Boxology: Preliminary Classi\ufb01cation of\nArchitectural Styles for Software Systems. In 21st International Computer Software and\nApplications Conference , pages 6\u201313, Aug. 1997. Cited on 56\nShen X., Yu H., Buford J., and Akon M., editors. Handbook of Peer-to-Peer Networking .\nSpringer-Verlag, Berlin, 2010. Cited on 559, 564\nShepler S., Callaghan B., Robinson D., Thurlow R., Beame C., Eisler M., and Noveck D.\nNetwork File System (NFS) Version 4 Protocol. RFC 3530, Apr. 2003. Cited on 94,\n262\nSheth A. P . and Larson J. A. Federated Database Systems for Managing Distributed,\nHeterogeneous, and Autonomous Databases. ACM Computing Surveys , 22(3):183\u2013236,\nSept. 1990. Cited on 386\nShin M., Park M., Oh D., Kim B., and Lee J. Survey on the Clock Synchronization\nSchemes for Propagation Delay Measurement. International Journal of Advanced\nScience and Technology , 35:139\u2013140, Oct. 2011. Cited on 303\nShoch J. Internetwork Naming, Addressing, and Routing. In 17th International Computer\nConference , pages 72\u201379, Los Alamitos, CA., 1978. IEEE, IEEE Computer Society\nPress. Cited on 240\nShooman M. L. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis,\nand Design . John Wiley, New York, 2002. Cited on 424\nShriram A. and Kaur J. Empirical Evaluation of Techniques for Measuring Available\nBandwidth. In 26th INFOCOM Conference , pages 2162\u20132170, Los Alamitos, CA., 2007.\nIEEE, IEEE Computer Society Press. Cited on 413\nSilberschatz A., Galvin P ., and Gagne G. Operating System Concepts . John Wiley, New\nYork, 9th edition, 2012. Cited on 258\nSilva S. S., Silva R. M., Pinto R. C., and Salles R. M. Botnets: A survey. Computer\nNetworks , 57(2):378 \u2013 403, 2013. Cited on 539\nSinghal M. and Shivaratri N. Advanced Concepts in Operating Systems: Distributed,\nDatabase, and Multiprocessor Operating Systems . McGraw-Hill, New York, 1994. Cited\non 492\nSivasubramanian S., Pierre G., and Steen M.van . Replicating Web Applications On-\nDemand. In 1st International Conference on Services Computing , pages 227\u2013236, Los\nAlamitos, CA., Sept. 2004a. IEEE, IEEE Computer Society Press. Cited on 418\nSivasubramanian S., Szymaniak M., Pierre G., and Steen M.van . Replication for Web\nHosting Systems. ACM Computing Surveys , 36(3):1\u201344, Sept. 2004b. Cited on 146, 412\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 577\nSivasubramanian S., Pierre G., Steen M.van , and Alonso G. Analysis of Caching and\nReplication Strategies for Web Applications. IEEE Internet Computing , 11(1):60\u201366,\nJan. 2007. Cited on 417\nSivrikaya F. and Yener B. Time Synchronization in Sensor Networks: A Survey. IEEE\nNetwork , 18(4):45\u201350, July 2004. Cited on 307\nSkeen D. Nonblocking Commit Protocols. In SIGMOD International Conference on\nManagement Of Data , pages 133\u2013142. ACM, 1981. Cited on 489\nSkeen D. and Stonebraker M. A Formal Model of Crash Recovery in a Distributed\nSystem. IEEE Transactions on Software Engineering , SE-9(3):219\u2013228, Mar. 1983. Cited\non 489\nSmetters D. and Jacobson V . Securing Network Content. Technical report, PARC, 2009.\nCited on 540\nSmith J. and Nair R. The Architecture of Virtual Machines. Computer , 38(5):32\u201338, May\n2005a. Cited on 118\nSmith J. and Nair R. Virtual Machines: Versatile Platforms for Systems and Processes .\nMorgan Kaufman, San Mateo, CA., 2005b. Cited on 120, 121\nSoltesz S., P\u00f6tzl H., Fiuczynski M. E., Bavier A., and Peterson L. Container-Based\nOperating System Virtualization: A Scalable, High-Performance Alternative to\nHypervisors. In 2nd EuroSys (European Conference on Computer Systems) , pages\n275\u2013287, New York, NY, Mar. 2007. ACM, ACM Press. Cited on 151\nSong C., Qu Z., Blumm N., and Barabasi A.-L. Limits of Predictability in Human\nMobility. Science , 327(2):1018\u20131021, Feb. 2010. Cited on 47\nSpecht S. M. and Lee R. B. Distributed Denial of Service: Taxonomies of Attacks, Tools,\nand Countermeasures. In Int\u2019l Workshop on Security in Parallel and Distributed Systems ,\npages 543\u2013550, Sept. 2004. Cited on 539\nSpector A. Performing Remote Operations Ef\ufb01ciently on a Local Computer Network.\nCommunications of the ACM , 25(4):246\u2013260, Apr. 1982. Cited on 466\nSpyropoulos T., Rais R. N. B., Turletti T., Obraczka K., and Vasilakos A. Routing for\nDisruption Tolerant Networks: Taxonomy and Design. Wireless Networks , 16(8):\n2349\u20132370, 2010. Cited on 45\nSrinivasan S. Kilim: A Server Framework with Lightweight Actors, Isolation Types and\nZero-Copy Messaging . Ph.d., University of Cambridge, Computer Laboratory, Feb.\n2010. Cited on 111\nSrisuresh P . and Holdrege M. IP Network Address Translator (NAT) Terminology and\nConsiderations. RFC 2663, Aug. 1999. Cited on 143\nStankovic J. A. Research Directions for the Internet of Things. IEEE Internet of Things\nJournal , 1(1):3\u20139, Feb. 2014. Cited on 40\nStein L. Web Security, A Step-by-Step Reference Guide . Addison-Wesley, Reading, MA.,\n1998. Cited on 545\nSteinder M. and Sethi A. A Survey of Fault Localization Techniques in Computer\nNetworks. Science of Computer Programming , 53:165\u2013194, May 2004. Cited on 499\nSteiner J., Neuman C., and Schiller J. Kerberos: An Authentication Service for Open\nNetwork Systems. In Winter Technical Conference , pages 191\u2013202. USENIX, 1988.\nCited on 526\nSteinmetz R. and Nahrstedt K. Multimedia Systems . Springer-Verlag, Berlin, 2004. Cited\non 142\nStevens W. TCP/IP Illustrated, Volume 1: The Protocols . Addison-Wesley, Reading, MA.,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 577\nSivasubramanian S., Pierre G., Steen M.van , and Alonso G. Analysis of Caching and\nReplication Strategies for Web Applications. IEEE Internet Computing , 11(1):60\u201366,\nJan. 2007. Cited on 417\nSivrikaya F. and Yener B. Time Synchronization in Sensor Networks: A Survey. IEEE\nNetwork , 18(4):45\u201350, July 2004. Cited on 307\nSkeen D. Nonblocking Commit Protocols. In SIGMOD International Conference on\nManagement Of Data , pages 133\u2013142. ACM, 1981. Cited on 489\nSkeen D. and Stonebraker M. A Formal Model of Crash Recovery in a Distributed\nSystem. IEEE Transactions on Software Engineering , SE-9(3):219\u2013228, Mar. 1983. Cited\non 489\nSmetters D. and Jacobson V . Securing Network Content. Technical report, PARC, 2009.\nCited on 540\nSmith J. and Nair R. The Architecture of Virtual Machines. Computer , 38(5):32\u201338, May\n2005a. Cited on 118\nSmith J. and Nair R. Virtual Machines: Versatile Platforms for Systems and Processes .\nMorgan Kaufman, San Mateo, CA., 2005b. Cited on 120, 121\nSoltesz S., P\u00f6tzl H., Fiuczynski M. E., Bavier A., and Peterson L. Container-Based\nOperating System Virtualization: A Scalable, High-Performance Alternative to\nHypervisors. In 2nd EuroSys (European Conference on Computer Systems) , pages\n275\u2013287, New York, NY, Mar. 2007. ACM, ACM Press. Cited on 151\nSong C., Qu Z., Blumm N., and Barabasi A.-L. Limits of Predictability in Human\nMobility. Science , 327(2):1018\u20131021, Feb. 2010. Cited on 47\nSpecht S. M. and Lee R. B. Distributed Denial of Service: Taxonomies of Attacks, Tools,\nand Countermeasures. In Int\u2019l Workshop on Security in Parallel and Distributed Systems ,\npages 543\u2013550, Sept. 2004. Cited on 539\nSpector A. Performing Remote Operations Ef\ufb01ciently on a Local Computer Network.\nCommunications of the ACM , 25(4):246\u2013260, Apr. 1982. Cited on 466\nSpyropoulos T., Rais R. N. B., Turletti T., Obraczka K., and Vasilakos A. Routing for\nDisruption Tolerant Networks: Taxonomy and Design. Wireless Networks , 16(8):\n2349\u20132370, 2010. Cited on 45\nSrinivasan S. Kilim: A Server Framework with Lightweight Actors, Isolation Types and\nZero-Copy Messaging . Ph.d., University of Cambridge, Computer Laboratory, Feb.\n2010. Cited on 111\nSrisuresh P . and Holdrege M. IP Network Address Translator (NAT) Terminology and\nConsiderations. RFC 2663, Aug. 1999. Cited on 143\nStankovic J. A. Research Directions for the Internet of Things. IEEE Internet of Things\nJournal , 1(1):3\u20139, Feb. 2014. Cited on 40\nStein L. Web Security, A Step-by-Step Reference Guide . Addison-Wesley, Reading, MA.,\n1998. Cited on 545\nSteinder M. and Sethi A. A Survey of Fault Localization Techniques in Computer\nNetworks. Science of Computer Programming , 53:165\u2013194, May 2004. Cited on 499\nSteiner J., Neuman C., and Schiller J. Kerberos: An Authentication Service for Open\nNetwork Systems. In Winter Technical Conference , pages 191\u2013202. USENIX, 1988.\nCited on 526\nSteinmetz R. and Nahrstedt K. Multimedia Systems . Springer-Verlag, Berlin, 2004. Cited\non 142\nStevens W. TCP/IP Illustrated, Volume 1: The Protocols . Addison-Wesley, Reading, MA.,\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "578 BIBLIOGRAPHY\n1994. Cited on 59\nStevens W. UNIX Network Programming \u2013 Networking APIs: Sockets and XTI . Prentice\nHall, Englewood Cliffs, N.J., 2nd edition, 1998. Cited on 112, 195\nStevens W. UNIX Network Programming \u2013 Interprocess Communication . Prentice Hall,\nEnglewood Cliffs, N.J., 2nd edition, 1999. Cited on 104, 111, 188\nStevens W. and Rago S. Advanced Programming in the UNIX Environment . Addison-\nWesley, Reading, MA., 2nd edition, 2005. Cited on 106\nStewart R. Stream Control Transmission Protocol. RFC 4960, Sept. 2007. Cited on 169\nStoica I., Morris R., Liben-Nowell D., Karger D. R., Kaashoek M. F., Dabek F., and\nBalakrishnan H. Chord: A Scalable Peer-to-peer Lookup Protocol for Internet\nApplications. IEEE/ACM Transactions on Networking , 11(1):17\u201332, Feb. 2003. Cited on\n83, 247, 249\nStratan C., Sacha J., Napper J., Costa P ., and Pierre G. The XtreemOS Resource Selection\nService. ACM Transactions of Autonomous and Adaptive Systems , 7(4), Dec. 2012. Cited\non 294\nStrauss J., Katabi D., and Kaashoek F. A Measurement Study of Available Bandwidth\nEstimation Tools. In 3rd Internet Measurement Conference , pages 39\u201344, New York,\nNY, 2003. ACM Press. Cited on 413\nSugerman J., Venkitachalam G., and Lim B.-H. Virtualizing I/O Devices on VMware\nWorkstation s Hosted Virtual Machine Monitor. In USENIX Annual Technical Confer-\nence, pages 1\u201314, Berkeley, CA, June 2001. USENIX, USENIX. Cited on 121\nSundararaman B., Buy U., and Kshemkalyani A. D. Clock Synchronization for Wireless\nSensor Networks: A Survey. Ad-Hoc Networks , 3(3):281\u2013323, May 2005. Cited on 307\nSzymaniak M., Pierre G., and Steen M.van . Scalable Cooperative Latency Estimation.\nIn10th International Conference on Parallel and Distributed Systems , pages 367\u2013376, Los\nAlamitos, CA., July 2004. IEEE, IEEE Computer Society Press. Cited on 342\nSzymaniak M., Pierre G., and Steen M.van . Latency-driven replica placement. IPSJ\nDigital Courier , 2:561\u2013572, 2006. Cited on 384\nSzymaniak M., Pierre G., Simons-Nikolova M., and Steen M.van . Enabling Ser-\nvice Adaptability with Versatile Anycast. Concurrency & Computation: Practice and\nExperience , 19(13):1837\u20131863, Sept. 2007. Cited on 147\nSzymaniak M., Presotto D., Pierre G., and Steen M.van . Practical Large-Scale Latency\nEstimation. Computer Networks , 52(7):1343\u20131364, May 2008. Cited on 342\nTaiani F., Fabre J.-C., and Killijian M.-O. A Multi-Level Meta-Object Protocol for\nFault-Tolerance in Complex Architectures. In International Conference on Dependable\nSystems and Networks , pages 270\u2013279, Los Alamitos, CA., June 2005. IEEE Computer\nSociety Press. Cited on 394\nTan S.-W., Waters G., and Crawford J. A Survey and Performance Evaluation of\nScalable Tree-based Application Layer Multicast Protocols. Technical Report 9-03,\nUniversity of Kent, UK, July 2003. Cited on 224\nTanenbaum A. Modern Operating Systems . Prentice Hall, Upper Saddle River, N.J., 3rd\nedition, 2001. Cited on 258\nTanenbaum A. and Wetherall D. Computer Networks . Prentice Hall, Upper Saddle River,\nN.J., 5th edition, 2010. Cited on 165, 464\nTanenbaum A. and Woodhull A. Operating Systems, Design and Implementation . Prentice\nHall, Englewood Cliffs, N.J., 3rd edition, 2006. Cited on 97\nTanenbaum A., Mullender S., and Renesse R.van . Using Sparse Capabilities in a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n578 BIBLIOGRAPHY\n1994. Cited on 59\nStevens W. UNIX Network Programming \u2013 Networking APIs: Sockets and XTI . Prentice\nHall, Englewood Cliffs, N.J., 2nd edition, 1998. Cited on 112, 195\nStevens W. UNIX Network Programming \u2013 Interprocess Communication . Prentice Hall,\nEnglewood Cliffs, N.J., 2nd edition, 1999. Cited on 104, 111, 188\nStevens W. and Rago S. Advanced Programming in the UNIX Environment . Addison-\nWesley, Reading, MA., 2nd edition, 2005. Cited on 106\nStewart R. Stream Control Transmission Protocol. RFC 4960, Sept. 2007. Cited on 169\nStoica I., Morris R., Liben-Nowell D., Karger D. R., Kaashoek M. F., Dabek F., and\nBalakrishnan H. Chord: A Scalable Peer-to-peer Lookup Protocol for Internet\nApplications. IEEE/ACM Transactions on Networking , 11(1):17\u201332, Feb. 2003. Cited on\n83, 247, 249\nStratan C., Sacha J., Napper J., Costa P ., and Pierre G. The XtreemOS Resource Selection\nService. ACM Transactions of Autonomous and Adaptive Systems , 7(4), Dec. 2012. Cited\non 294\nStrauss J., Katabi D., and Kaashoek F. A Measurement Study of Available Bandwidth\nEstimation Tools. In 3rd Internet Measurement Conference , pages 39\u201344, New York,\nNY, 2003. ACM Press. Cited on 413\nSugerman J., Venkitachalam G., and Lim B.-H. Virtualizing I/O Devices on VMware\nWorkstation s Hosted Virtual Machine Monitor. In USENIX Annual Technical Confer-\nence, pages 1\u201314, Berkeley, CA, June 2001. USENIX, USENIX. Cited on 121\nSundararaman B., Buy U., and Kshemkalyani A. D. Clock Synchronization for Wireless\nSensor Networks: A Survey. Ad-Hoc Networks , 3(3):281\u2013323, May 2005. Cited on 307\nSzymaniak M., Pierre G., and Steen M.van . Scalable Cooperative Latency Estimation.\nIn10th International Conference on Parallel and Distributed Systems , pages 367\u2013376, Los\nAlamitos, CA., July 2004. IEEE, IEEE Computer Society Press. Cited on 342\nSzymaniak M., Pierre G., and Steen M.van . Latency-driven replica placement. IPSJ\nDigital Courier , 2:561\u2013572, 2006. Cited on 384\nSzymaniak M., Pierre G., Simons-Nikolova M., and Steen M.van . Enabling Ser-\nvice Adaptability with Versatile Anycast. Concurrency & Computation: Practice and\nExperience , 19(13):1837\u20131863, Sept. 2007. Cited on 147\nSzymaniak M., Presotto D., Pierre G., and Steen M.van . Practical Large-Scale Latency\nEstimation. Computer Networks , 52(7):1343\u20131364, May 2008. Cited on 342\nTaiani F., Fabre J.-C., and Killijian M.-O. A Multi-Level Meta-Object Protocol for\nFault-Tolerance in Complex Architectures. In International Conference on Dependable\nSystems and Networks , pages 270\u2013279, Los Alamitos, CA., June 2005. IEEE Computer\nSociety Press. Cited on 394\nTan S.-W., Waters G., and Crawford J. A Survey and Performance Evaluation of\nScalable Tree-based Application Layer Multicast Protocols. Technical Report 9-03,\nUniversity of Kent, UK, July 2003. Cited on 224\nTanenbaum A. Modern Operating Systems . Prentice Hall, Upper Saddle River, N.J., 3rd\nedition, 2001. Cited on 258\nTanenbaum A. and Wetherall D. Computer Networks . Prentice Hall, Upper Saddle River,\nN.J., 5th edition, 2010. Cited on 165, 464\nTanenbaum A. and Woodhull A. Operating Systems, Design and Implementation . Prentice\nHall, Englewood Cliffs, N.J., 3rd edition, 2006. Cited on 97\nTanenbaum A., Mullender S., and Renesse R.van . Using Sparse Capabilities in a\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 579\nDistributed Operating System. In 6th International Conference on Distributed Computing\nSystems , pages 558\u2013563, Cambridge, MA, May 1986. IEEE. Cited on 548\nTanisch P . Atomic Commit in Concurrent Computing. IEEE Concurrency , 8(4):34\u201341,\nOct. 2000. Cited on 484\nTarkoma S. Overlay Networks, Toward Information Networking . CRC Press, Boca Raton,\nFL, 2010. Cited on 3, 81\nTarkoma S. and Kangasharju J. Mobile Middleware: Supporting Applications and Services .\nJohn Wiley, New York, 2009. Cited on 44\nTartalja I. and Milutinovic V . Classifying Software-Based Cache Coherence Solutions.\nIEEE Software , 14(3):90\u2013101, May 1997. Cited on 404\nTaylor M. WebSphere MQ Primer: An Introduction to Messaging and WebSphere MQ .\nRedbooks. IBM, Dec. 2012. Cited on 212\nTel G. Introduction to Distributed Algorithms . Cambridge University Press, Cambridge,\nUK, 2nd edition, 2000. Cited on 298, 330\nTerry D. Replicated Data Management for Mobile Computing . Synthesis Lectures on Data\nManagement. Morgan and Claypool, San Rafael, CA, 2008. Cited on 377\nTerry D., Demers A., Petersen K., Spreitzer M., Theimer M., and Welsh B. Session\nGuarantees for Weakly Consistent Replicated Data. In 3rd International Conference on\nParallel and Distributed Information Systems , pages 140\u2013149, Los Alamitos, CA., Sept.\n1994. IEEE, IEEE Computer Society Press. Cited on 377, 380, 382\nTerry D., Petersen K., Spreitzer M., and Theimer M. The Case for Non-transparent\nReplication: Examples from Bayou. IEEE Data Engineering , 21(4):12\u201320, Dec. 1998.\nCited on 377\nThomas R. A Majority Consensus Approach to Concurrency Control for Multiple Copy\nDatabases. ACM Transactions on Database Systems , 4(2):180\u2013209, June 1979. Cited on\n402\nTIBCO . TIB/Rendezvous Concepts, Release 7.4 . TIBCO Software Inc., Palo Alto, CA, July\n2005. Cited on 344\nTowsley D., Kurose J., and Pingali S. A Comparison of Sender-Initiated and Receiver-\nInitiated Reliable Multicast Protocols. IEEE Journal on Selected Areas in Communication ,\n15(3):398\u2013407, Apr. 1997. Cited on 474\nTrivedi K. Probability and Statistics with Reliability, Queuing and Computer Science Applica-\ntions . John Wiley, New York, 2nd edition, 2002. Cited on 17\nTsafrir D. The Context-Switch Overhead In\ufb02icted by Hardware Interrupts (and the\nEnigma of Do-nothing Loops). In 2007 Workshop on Experimental Computer Science ,\nNew York, NY, 2007. ACM Press. Cited on 107, 108\nTsui A. W., Lin W.-C., Chen W.-J., Huang P ., and Chu H.-H. Accuracy Performance\nAnalysis between War Driving and War Walking in Metropolitan Wi-Fi Localization.\nIEEE Transations on Mobile Computing , 9(11):1551\u20131562, 2010. Cited on 339\nTurek J. and Shasha S. The Many Faces of Consensus in Distributed Systems. Computer ,\n25(6):8\u201317, June 1992. Cited on 459, 460\nUmar A. Object-Oriented Client/Server Internet Environments . Prentice Hall, Upper\nSaddle River, N.J., 1997. Cited on 78\nUPnP Forum . UPnP Device Architecture Version 1.1, Oct. 2008. Cited on 43\nUzunov A. V ., Fernandez E. B., and Falkner K. Engineering Security into Distributed\nSystems: A Survey of Methodologies. Journal of Universal Computer Science , 18(20):\n2920\u20133006, 2012. Cited on 509\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 579\nDistributed Operating System. In 6th International Conference on Distributed Computing\nSystems , pages 558\u2013563, Cambridge, MA, May 1986. IEEE. Cited on 548\nTanisch P . Atomic Commit in Concurrent Computing. IEEE Concurrency , 8(4):34\u201341,\nOct. 2000. Cited on 484\nTarkoma S. Overlay Networks, Toward Information Networking . CRC Press, Boca Raton,\nFL, 2010. Cited on 3, 81\nTarkoma S. and Kangasharju J. Mobile Middleware: Supporting Applications and Services .\nJohn Wiley, New York, 2009. Cited on 44\nTartalja I. and Milutinovic V . Classifying Software-Based Cache Coherence Solutions.\nIEEE Software , 14(3):90\u2013101, May 1997. Cited on 404\nTaylor M. WebSphere MQ Primer: An Introduction to Messaging and WebSphere MQ .\nRedbooks. IBM, Dec. 2012. Cited on 212\nTel G. Introduction to Distributed Algorithms . Cambridge University Press, Cambridge,\nUK, 2nd edition, 2000. Cited on 298, 330\nTerry D. Replicated Data Management for Mobile Computing . Synthesis Lectures on Data\nManagement. Morgan and Claypool, San Rafael, CA, 2008. Cited on 377\nTerry D., Demers A., Petersen K., Spreitzer M., Theimer M., and Welsh B. Session\nGuarantees for Weakly Consistent Replicated Data. In 3rd International Conference on\nParallel and Distributed Information Systems , pages 140\u2013149, Los Alamitos, CA., Sept.\n1994. IEEE, IEEE Computer Society Press. Cited on 377, 380, 382\nTerry D., Petersen K., Spreitzer M., and Theimer M. The Case for Non-transparent\nReplication: Examples from Bayou. IEEE Data Engineering , 21(4):12\u201320, Dec. 1998.\nCited on 377\nThomas R. A Majority Consensus Approach to Concurrency Control for Multiple Copy\nDatabases. ACM Transactions on Database Systems , 4(2):180\u2013209, June 1979. Cited on\n402\nTIBCO . TIB/Rendezvous Concepts, Release 7.4 . TIBCO Software Inc., Palo Alto, CA, July\n2005. Cited on 344\nTowsley D., Kurose J., and Pingali S. A Comparison of Sender-Initiated and Receiver-\nInitiated Reliable Multicast Protocols. IEEE Journal on Selected Areas in Communication ,\n15(3):398\u2013407, Apr. 1997. Cited on 474\nTrivedi K. Probability and Statistics with Reliability, Queuing and Computer Science Applica-\ntions . John Wiley, New York, 2nd edition, 2002. Cited on 17\nTsafrir D. The Context-Switch Overhead In\ufb02icted by Hardware Interrupts (and the\nEnigma of Do-nothing Loops). In 2007 Workshop on Experimental Computer Science ,\nNew York, NY, 2007. ACM Press. Cited on 107, 108\nTsui A. W., Lin W.-C., Chen W.-J., Huang P ., and Chu H.-H. Accuracy Performance\nAnalysis between War Driving and War Walking in Metropolitan Wi-Fi Localization.\nIEEE Transations on Mobile Computing , 9(11):1551\u20131562, 2010. Cited on 339\nTurek J. and Shasha S. The Many Faces of Consensus in Distributed Systems. Computer ,\n25(6):8\u201317, June 1992. Cited on 459, 460\nUmar A. Object-Oriented Client/Server Internet Environments . Prentice Hall, Upper\nSaddle River, N.J., 1997. Cited on 78\nUPnP Forum . UPnP Device Architecture Version 1.1, Oct. 2008. Cited on 43\nUzunov A. V ., Fernandez E. B., and Falkner K. Engineering Security into Distributed\nSystems: A Survey of Methodologies. Journal of Universal Computer Science , 18(20):\n2920\u20133006, 2012. Cited on 509\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "580 BIBLIOGRAPHY\nRenesse R.van and Altinbuken D. Paxos Made Moderately Complex. ACM Computing\nSurveys , 47(3):42:1\u201342:36, Feb. 2015. Cited on 438\nRenesse R.van , Birman K., Cooper R., Glade B., and Stephenson P . The Horus System.\nIn Birman K. and Renesse R.van , editors, Reliable and Distributed Computing with the\nIsis Toolkit , pages 133\u2013147. IEEE Computer Society Press, Los Alamitos, CA., 1994.\nCited on 6\nSteen M.van and Ballintijn G. Achieving Scalability in Hierarchical Location Services.\nIn26th International Computer Software and Applications Conference , pages 899\u2013905,\nLos Alamitos, CA., Aug. 2002. IEEE, IEEE Computer Society Press. Cited on 256\nSteen M.van , Hauck F., Homburg P ., and Tanenbaum A. Locating Objects in Wide-Area\nSystems. IEEE Communications Magazine , 36(1):104\u2013109, Jan. 1998. Cited on 251\nVaquero L. M., Rodero-Merino L., Caceres J., and Lindner M. A Break in the Clouds:\nTowards a Cloud De\ufb01nition. ACM Computer Communications Review , 39(1):50\u201355,\nDec. 2008. Cited on 30\nVasudevan S., Kurose J. F., and Towsley D. F. Design and Analysis of a Leader Election\nAlgorithm for Mobile Ad Hoc Networks. In 12th International Conference on Network\nProtocols , pages 350\u2013360, Los Alamitos, CA., Oct. 2004. IEEE, IEEE Computer Society\nPress. Cited on 333, 335\nVega-Redondo F. Complex Social Networks . Cambridge University Press, Cambridge,\nUK, 2007. Cited on 45\nVeiga L. and Ferreira P . Asynchronous Complete Distributed Garbage Collection. In\n19th International Parallel & Distributed Processing Symposium , Los Alamitos, CA., Apr.\n2005. IEEE, IEEE Computer Society Press. Cited on 244\nVelazquez M. A Survey of Distributed Mutual Exclusion Algorithms. Technical Report\nCS-93-116, University of Colorado at Boulder, Sept. 1993. Cited on 322\nVidela A. and Williams J. RabitMQ in Action . Manning, Shelter Island, NY, 2012. Cited\non 218\nVixie P . What DNS Is Not. Communications of the ACM , 52(12):43\u201347, Dec. 2009. Cited\non 278\nVixie P . Rate-Limiting State. Communications of the ACM , 57(4):40\u201343, Apr. 2014. Cited\non 278\nVogels W. Tracking Service Availability in Long Running Business Activities. In 1st\nInternational Conference on Service Oriented Computing , volume 2910 of Lecture Notes\nin Computer Science , pages 395\u2013408, Berlin, Dec. 2003. Springer-Verlag. Cited on 463\nVogels W. Eventually consistent. Communications of the ACM , 52(1):40\u201344, Jan. 2009.\nCited on 373\nVoorsluys W., Broberg J., Venugopal S., and Buyya R. Cost of Virtual Machine Live\nMigration in Clouds: A Performance Evaluation. In 1st International Conference on\nCloud Computing , volume 5931 of Lecture Notes in Computer Science , pages 254\u2013265,\nBerlin, Dec. 2009. Springer-Verlag. Cited on 159, 160\nVoulgaris S., Gavidia D., and Steen. M.van . CYCLON: Inexpensive Membership Man-\nagement for Unstructured P2P Overlays. Journal of Network and Systems Management ,\n13(2):197\u2013217, June 2005. Cited on 352\nVoulgaris S., Rivi\u00e8re E., Kermarrec A.-M., and Steen M.van . Sub-2-Sub: Self-Organizing\nContent-Based Publish and Subscribe for Dynamic and Large Scale Collaborative\nNetworks. In 5th International Workshop on Peer-to-Peer Systems , Feb. 2006. Cited on\n347\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n580 BIBLIOGRAPHY\nRenesse R.van and Altinbuken D. Paxos Made Moderately Complex. ACM Computing\nSurveys , 47(3):42:1\u201342:36, Feb. 2015. Cited on 438\nRenesse R.van , Birman K., Cooper R., Glade B., and Stephenson P . The Horus System.\nIn Birman K. and Renesse R.van , editors, Reliable and Distributed Computing with the\nIsis Toolkit , pages 133\u2013147. IEEE Computer Society Press, Los Alamitos, CA., 1994.\nCited on 6\nSteen M.van and Ballintijn G. Achieving Scalability in Hierarchical Location Services.\nIn26th International Computer Software and Applications Conference , pages 899\u2013905,\nLos Alamitos, CA., Aug. 2002. IEEE, IEEE Computer Society Press. Cited on 256\nSteen M.van , Hauck F., Homburg P ., and Tanenbaum A. Locating Objects in Wide-Area\nSystems. IEEE Communications Magazine , 36(1):104\u2013109, Jan. 1998. Cited on 251\nVaquero L. M., Rodero-Merino L., Caceres J., and Lindner M. A Break in the Clouds:\nTowards a Cloud De\ufb01nition. ACM Computer Communications Review , 39(1):50\u201355,\nDec. 2008. Cited on 30\nVasudevan S., Kurose J. F., and Towsley D. F. Design and Analysis of a Leader Election\nAlgorithm for Mobile Ad Hoc Networks. In 12th International Conference on Network\nProtocols , pages 350\u2013360, Los Alamitos, CA., Oct. 2004. IEEE, IEEE Computer Society\nPress. Cited on 333, 335\nVega-Redondo F. Complex Social Networks . Cambridge University Press, Cambridge,\nUK, 2007. Cited on 45\nVeiga L. and Ferreira P . Asynchronous Complete Distributed Garbage Collection. In\n19th International Parallel & Distributed Processing Symposium , Los Alamitos, CA., Apr.\n2005. IEEE, IEEE Computer Society Press. Cited on 244\nVelazquez M. A Survey of Distributed Mutual Exclusion Algorithms. Technical Report\nCS-93-116, University of Colorado at Boulder, Sept. 1993. Cited on 322\nVidela A. and Williams J. RabitMQ in Action . Manning, Shelter Island, NY, 2012. Cited\non 218\nVixie P . What DNS Is Not. Communications of the ACM , 52(12):43\u201347, Dec. 2009. Cited\non 278\nVixie P . Rate-Limiting State. Communications of the ACM , 57(4):40\u201343, Apr. 2014. Cited\non 278\nVogels W. Tracking Service Availability in Long Running Business Activities. In 1st\nInternational Conference on Service Oriented Computing , volume 2910 of Lecture Notes\nin Computer Science , pages 395\u2013408, Berlin, Dec. 2003. Springer-Verlag. Cited on 463\nVogels W. Eventually consistent. Communications of the ACM , 52(1):40\u201344, Jan. 2009.\nCited on 373\nVoorsluys W., Broberg J., Venugopal S., and Buyya R. Cost of Virtual Machine Live\nMigration in Clouds: A Performance Evaluation. In 1st International Conference on\nCloud Computing , volume 5931 of Lecture Notes in Computer Science , pages 254\u2013265,\nBerlin, Dec. 2009. Springer-Verlag. Cited on 159, 160\nVoulgaris S., Gavidia D., and Steen. M.van . CYCLON: Inexpensive Membership Man-\nagement for Unstructured P2P Overlays. Journal of Network and Systems Management ,\n13(2):197\u2013217, June 2005. Cited on 352\nVoulgaris S., Rivi\u00e8re E., Kermarrec A.-M., and Steen M.van . Sub-2-Sub: Self-Organizing\nContent-Based Publish and Subscribe for Dynamic and Large Scale Collaborative\nNetworks. In 5th International Workshop on Peer-to-Peer Systems , Feb. 2006. Cited on\n347\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID", "BIBLIOGRAPHY 581\nVoulgaris S., Dobson M., and Steen M.van . Decentralized Network-level Synchroniza-\ntion in Mobile Ad Hoc Networks. ACM Transactions on Sensor Networks , 12(1), 2016.\nCited on 50, 52\nVoydock V . and Kent S. Security Mechanisms in High-Level Network Protocols. ACM\nComputing Surveys , 15(2):135\u2013171, June 1983. Cited on 513\nVu Q., Lupu M., and Ooi B. Peer-to-Peer Computing, Principles and Applications . Springer-\nVerlag, Berlin, 2010. Cited on 81\nWahbe R., Lucco S., Anderson T., and Graham S. Ef\ufb01cient Software-based Fault\nIsolation. In 14th Symposium on Operating System Principles , pages 203\u2013216, Asheville,\nNorth Carolina, Dec. 1993. ACM. Cited on 535\nWaldo J., Wyant G., Wollrath A., and Kendall S. A Note on Distributed Computing.\nIn2nd Workshop on Mobile Object Systems , volume 1222 of Lecture Notes in Computer\nScience , pages 1\u201310, Berlin, July 1997. Springer-Verlag. Cited on 11\nWal\ufb01sh M., Balakrishnan H., , and Shenker S. Untangling the Web from DNS. In 1st\nSymposium on Networked Systems Design and Implementation , pages 225\u2013238, Berkeley,\nCA, Mar. 2004. USENIX, USENIX. Cited on 276\nWallach D., Balfanz D., Dean D., and Felten E. Extensible Security Architectures for\nJava. In 16th Symposium on Operating System Principles , pages 116\u2013128, St. Malo,\nFrance, Oct. 1997. ACM. Cited on 537, 539\nWams J. Uni\ufb01ed Messaging and Micro-Objects . PhD thesis, VU University Amsterdam,\n2011. Cited on 11, 12\nWelsh M. and Mainland G. Programming Sensor Networks Using Abstract Regions.\nIn1st Symposium on Networked Systems Design and Implementation , Berkeley, CA, Mar.\n2004. USENIX, USENIX. Cited on 48\nWendell P . and Freedman M. J. Going Viral: Flash Crowds in an Open CDN. In 11th\nInternet Measurement Conference , pages 549\u2013558, New York, NY, 2011. ACM Press.\nCited on 410\nWessels D. Squid: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol, CA., 2004.\nCited on 411\nWieringa R. and Jonge W.de . Object Identi\ufb01ers, Keys, and Surrogates\u2013Object Identi\ufb01ers\nRevisited. Theory and Practice of Object Systems , 1(2):101\u2013114, 1995. Cited on 239\nWiesmann M., Pedone F., Schiper A., Kemme B., and Alonso G. Understanding\nReplication in Databases and Distributed Systems. In 20th International Conference on\nDistributed Computing Systems , pages 264\u2013274, Taipei, Taiwan, Apr. 2000. IEEE. Cited\non 358\nWollrath A., Riggs R., and Waldo J. A Distributed Object Model for the Java System.\nComputing Systems , 9(4):265\u2013290, Fall 1996. Cited on 182\nWolman A., Voelker G., Sharma N., Cardwell N., Karlin A., and Levy H. On the\nScale and Performance of Cooperative Web Proxy Caching. In 17th Symposium on\nOperating System Principles , pages 16\u201331, Kiawah Island, SC, Dec. 1999. ACM. Cited\non 410\nWool A. Trends in Firewall Con\ufb01guration Errors: Measuring the Holes in Swiss Cheese.\nIEEE Internet Computing , 14(4):58\u201365, 2010. Cited on 533\nWright G. and Stevens W. TCP/IP Illustrated, Volume 2: The Implementation . Addison-\nWesley, Reading, MA., 1995. Cited on 59\nXylomenos G., Ververidis C., Siris V ., Fotiou N., Tsilopoulos C., Vasilakos X., Katsaros\nK., and Polyzos G. A Survey of Information-centric Networking Research. IEEE\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre\nBIBLIOGRAPHY 581\nVoulgaris S., Dobson M., and Steen M.van . Decentralized Network-level Synchroniza-\ntion in Mobile Ad Hoc Networks. ACM Transactions on Sensor Networks , 12(1), 2016.\nCited on 50, 52\nVoydock V . and Kent S. Security Mechanisms in High-Level Network Protocols. ACM\nComputing Surveys , 15(2):135\u2013171, June 1983. Cited on 513\nVu Q., Lupu M., and Ooi B. Peer-to-Peer Computing, Principles and Applications . Springer-\nVerlag, Berlin, 2010. Cited on 81\nWahbe R., Lucco S., Anderson T., and Graham S. Ef\ufb01cient Software-based Fault\nIsolation. In 14th Symposium on Operating System Principles , pages 203\u2013216, Asheville,\nNorth Carolina, Dec. 1993. ACM. Cited on 535\nWaldo J., Wyant G., Wollrath A., and Kendall S. A Note on Distributed Computing.\nIn2nd Workshop on Mobile Object Systems , volume 1222 of Lecture Notes in Computer\nScience , pages 1\u201310, Berlin, July 1997. Springer-Verlag. Cited on 11\nWal\ufb01sh M., Balakrishnan H., , and Shenker S. Untangling the Web from DNS. In 1st\nSymposium on Networked Systems Design and Implementation , pages 225\u2013238, Berkeley,\nCA, Mar. 2004. USENIX, USENIX. Cited on 276\nWallach D., Balfanz D., Dean D., and Felten E. Extensible Security Architectures for\nJava. In 16th Symposium on Operating System Principles , pages 116\u2013128, St. Malo,\nFrance, Oct. 1997. ACM. Cited on 537, 539\nWams J. Uni\ufb01ed Messaging and Micro-Objects . PhD thesis, VU University Amsterdam,\n2011. Cited on 11, 12\nWelsh M. and Mainland G. Programming Sensor Networks Using Abstract Regions.\nIn1st Symposium on Networked Systems Design and Implementation , Berkeley, CA, Mar.\n2004. USENIX, USENIX. Cited on 48\nWendell P . and Freedman M. J. Going Viral: Flash Crowds in an Open CDN. In 11th\nInternet Measurement Conference , pages 549\u2013558, New York, NY, 2011. ACM Press.\nCited on 410\nWessels D. Squid: The De\ufb01nitive Guide . O\u2019Reilly & Associates, Sebastopol, CA., 2004.\nCited on 411\nWieringa R. and Jonge W.de . Object Identi\ufb01ers, Keys, and Surrogates\u2013Object Identi\ufb01ers\nRevisited. Theory and Practice of Object Systems , 1(2):101\u2013114, 1995. Cited on 239\nWiesmann M., Pedone F., Schiper A., Kemme B., and Alonso G. Understanding\nReplication in Databases and Distributed Systems. In 20th International Conference on\nDistributed Computing Systems , pages 264\u2013274, Taipei, Taiwan, Apr. 2000. IEEE. Cited\non 358\nWollrath A., Riggs R., and Waldo J. A Distributed Object Model for the Java System.\nComputing Systems , 9(4):265\u2013290, Fall 1996. Cited on 182\nWolman A., Voelker G., Sharma N., Cardwell N., Karlin A., and Levy H. On the\nScale and Performance of Cooperative Web Proxy Caching. In 17th Symposium on\nOperating System Principles , pages 16\u201331, Kiawah Island, SC, Dec. 1999. ACM. Cited\non 410\nWool A. Trends in Firewall Con\ufb01guration Errors: Measuring the Holes in Swiss Cheese.\nIEEE Internet Computing , 14(4):58\u201365, 2010. Cited on 533\nWright G. and Stevens W. TCP/IP Illustrated, Volume 2: The Implementation . Addison-\nWesley, Reading, MA., 1995. Cited on 59\nXylomenos G., Ververidis C., Siris V ., Fotiou N., Tsilopoulos C., Vasilakos X., Katsaros\nK., and Polyzos G. A Survey of Information-centric Networking Research. IEEE\ndownloaded by HUSNI @TRUNOJOYO.AC.ID DS 3.01 pre", "582 BIBLIOGRAPHY\nCommunications Surveys & Tutorials , 16(2):1024\u20131049, 2014. Cited on 345\nYang B. and Garcia-Molina H. Designing a Super-Peer Network. In 19th International\nConference on Data Engineering , pages 49\u201360, Los Alamitos, CA., Mar. 2003. IEEE,\nIEEE Computer Society Press. Cited on 87\nYang M., Zhang Z., Li X., and Dai Y. An Empirical Study of Free-Riding Behavior in the\nMaze P2P File-Sharing System. In 4th International Workshop on Peer-to-Peer Systems ,\nLecture Notes in Computer Science, Berlin, Feb. 2005. Springer-Verlag. Cited on 91\nYellin D. Competitive Algorithms for the Dynamic Selection of Component Implemen-\ntations. IBM Systems Journal , 42(1):85\u201397, Jan. 2003. Cited on 75\nYi S., Li C., and Li Q. A Survey of Fog Computing: Concepts, Applications and Issues.\nInWorkshop on Mobile Big Data , pages 37\u201342, New York, NY, 2015. ACM Press. Cited\non 91\nYu H. and Vahdat A. Ef\ufb01cient Numerical Error Bounding for Replicated Network\nServices. In Abbadi A. E., Brodie M. L., Chakravarthy S., Dayal U., Kamel N.,\nSchlageter G., and Whang K.-Y., editors, 26th International Conference on Very Large\nData Bases , pages 123\u2013133, San Mateo, CA., Sept. 2000. Morgan Kaufman. Cited on\n396\nYu H. and Vahdat A. Design and Evaluation of a Conit-Based Continuous Consistency\nModel for Replicated Services. ACM Transactions on Computer Systems , 20(3):239\u2013282,\n2002. Cited on 359, 360, 413\nZargar S. T., Joshi J., and Tipper D. A Survey of Defense Mechanisms Against\nDistributed Denial of Service (DDoS) Flooding Attacks. IEEE Communications\nSurveys & Tutorials , 15(4):2046\u20132069, 2013. Cited on 540\nZhang Q., Cheng L., and Boutaba R. Cloud Computing: State of the Art and Research\nChallenges. Journal of Internet Services and Applications , 1(1):7\u201318, May 2010. Cited on\n30\nZhao F. and Guibas L. Wireless Sensor Networks . Morgan Kaufman, San Mateo, CA.,\n2004. Cited on 47\nZhuang S. Q., Geels D., Stoica I., and Katz R. H. On Failure Detection Algorithms\nin Overlay Networks. In 24th INFOCOM Conference , Los Alamitos, CA., Mar. 2005.\nIEEE, IEEE Computer Society Press. Cited on 463\nZogg J.-M. GPS Basics. Technical Report GPS-X-02007, UBlox, Mar. 2002. Cited on 338\nZwicky E., Cooper S., Chapman D., and Russell D. Building Internet Firewalls . O\u2019Reilly\n& Associates, Sebastopol, CA., 2nd edition, 2000. Cited on 533\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID\n582 BIBLIOGRAPHY\nCommunications Surveys & Tutorials , 16(2):1024\u20131049, 2014. Cited on 345\nYang B. and Garcia-Molina H. Designing a Super-Peer Network. In 19th International\nConference on Data Engineering , pages 49\u201360, Los Alamitos, CA., Mar. 2003. IEEE,\nIEEE Computer Society Press. Cited on 87\nYang M., Zhang Z., Li X., and Dai Y. An Empirical Study of Free-Riding Behavior in the\nMaze P2P File-Sharing System. In 4th International Workshop on Peer-to-Peer Systems ,\nLecture Notes in Computer Science, Berlin, Feb. 2005. Springer-Verlag. Cited on 91\nYellin D. Competitive Algorithms for the Dynamic Selection of Component Implemen-\ntations. IBM Systems Journal , 42(1):85\u201397, Jan. 2003. Cited on 75\nYi S., Li C., and Li Q. A Survey of Fog Computing: Concepts, Applications and Issues.\nInWorkshop on Mobile Big Data , pages 37\u201342, New York, NY, 2015. ACM Press. Cited\non 91\nYu H. and Vahdat A. Ef\ufb01cient Numerical Error Bounding for Replicated Network\nServices. In Abbadi A. E., Brodie M. L., Chakravarthy S., Dayal U., Kamel N.,\nSchlageter G., and Whang K.-Y., editors, 26th International Conference on Very Large\nData Bases , pages 123\u2013133, San Mateo, CA., Sept. 2000. Morgan Kaufman. Cited on\n396\nYu H. and Vahdat A. Design and Evaluation of a Conit-Based Continuous Consistency\nModel for Replicated Services. ACM Transactions on Computer Systems , 20(3):239\u2013282,\n2002. Cited on 359, 360, 413\nZargar S. T., Joshi J., and Tipper D. A Survey of Defense Mechanisms Against\nDistributed Denial of Service (DDoS) Flooding Attacks. IEEE Communications\nSurveys & Tutorials , 15(4):2046\u20132069, 2013. Cited on 540\nZhang Q., Cheng L., and Boutaba R. Cloud Computing: State of the Art and Research\nChallenges. Journal of Internet Services and Applications , 1(1):7\u201318, May 2010. Cited on\n30\nZhao F. and Guibas L. Wireless Sensor Networks . Morgan Kaufman, San Mateo, CA.,\n2004. Cited on 47\nZhuang S. Q., Geels D., Stoica I., and Katz R. H. On Failure Detection Algorithms\nin Overlay Networks. In 24th INFOCOM Conference , Los Alamitos, CA., Mar. 2005.\nIEEE, IEEE Computer Society Press. Cited on 463\nZogg J.-M. GPS Basics. Technical Report GPS-X-02007, UBlox, Mar. 2002. Cited on 338\nZwicky E., Cooper S., Chapman D., and Russell D. Building Internet Firewalls . O\u2019Reilly\n& Associates, Sebastopol, CA., 2nd edition, 2000. Cited on 533\nDS 3.01 pre downloaded by HUSNI @TRUNOJOYO.AC.ID"]