{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/response_modes.html",
        "title": "Response Modes - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Response Modes[\uf0c1](#response-modes \"Permalink to this heading\")\n\nRight now, we support the following options:\n\n*   `refine`: **_create and refine_** an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.\n    \n    **Details:** the first chunk is used in a query using the `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used in another query with the `refine_template` prompt. And so on until all chunks have been parsed.\n    \n    If a chunk is too large to fit within the window (considering the prompt size), it is split using a `TokenTextSplitter` (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks of the original chunks collection (and thus queried with the `refine_template` as well).\n    \n    Good for more detailed answers.\n    \n*   `compact` (default): similar to `refine` but **_compact_** (concatenate) the chunks beforehand, resulting in less LLM calls.\n    \n    **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window (considering the maximum prompt size between `text_qa_template` and `refine_template`). If the text is too long to fit in one prompt, it is split in as many parts as needed (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).\n    \n    Each text part is considered a \u201cchunk\u201d and is sent to the `refine` synthesizer.\n    \n    In short, it is like `refine`, but with less LLM calls.\n    \n*   `tree_summarize`: Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call and so on, until there\u2019s only one chunk left, and thus only one final answer.\n    \n    **Details:** concatenate the chunks as much as possible to fit within the context window using the `summary_template` prompt, and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against `summary_template` (there is no **_refine_** query !) and get as many answers.\n    \n    If there is only one answer (because there was only one chunk), then it\u2019s the final answer.\n    \n    If there are more than one answer, these themselves are considered as chunks and sent recursively to the `tree_summarize` process (concatenated/splitted-to-fit/queried).\n    \n    Good for summarization purposes.\n    \n*   `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick summarization purposes, but may lose detail due to truncation.\n    \n*   `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking `response.source_nodes`.\n    \n*   `accumulate`: Given a set of text chunks and the query, apply the query to each text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\n    \n*   `compact_accumulate`: The same as accumulate, but will \u201ccompact\u201d each LLM prompt similar to `compact`, and run the same query against each text chunk.\n    \n\nSee [Response Synthesizer](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/response_synthesizers/root.html) to learn more."
}