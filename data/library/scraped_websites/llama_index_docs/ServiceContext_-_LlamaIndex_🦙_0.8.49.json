{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/service_context.html",
        "title": "ServiceContext - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\n## ServiceContext[\uf0c1](#servicecontext \"Permalink to this heading\")\n\n## Concept[\uf0c1](#concept \"Permalink to this heading\")\n\nThe `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the [global configuration](#setting-global-configuration), as well as [local configurations](#setting-local-configuration) at specific parts of the pipeline.\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\n### Configuring the service context[\uf0c1](#configuring-the-service-context \"Permalink to this heading\")\n\nThe `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.\n\n@dataclass\nclass ServiceContext:\n    \\# The LLM used to generate natural language responses to queries.\n    \\# If not provided, defaults to gpt-3.5-turbo from OpenAI\n    \\# If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp\n    llm: LLM\n\n    \\# The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM's context window.\n    prompt\\_helper: PromptHelper\n\n    \\# The embedding model used to generate vector representations of text.\n    \\# If not provided, defaults to text-embedding-ada-002\n    \\# If your OpenAI key is not set, defaults to BAAI/bge-small-en\n    embed\\_model: BaseEmbedding\n\n    \\# The parser that converts documents into nodes.\n    node\\_parser: NodeParser\n\n    \\# The callback manager object that calls it's handlers on events. Provides basic logging and tracing capabilities.\n    callback\\_manager: CallbackManager\n\n    @classmethod\n    def from\\_defaults(cls, ...) \\-> \"ServiceContext\":\n      ...\n\nWe also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method for convenience (so you don\u2019t have to manually construct them).\n\n**Kwargs for node parser**:\n\n*   `chunk_size`: The size of the text chunk for a node . Is used for the node parser when they aren\u2019t provided.\n    \n*   `chunk_overlap`: The amount of overlap between nodes (i.e. text chunks).\n    \n\n**Kwargs for prompt helper**:\n\n*   `context_window`: The size of the context window of the LLM. Typically we set this automatically with the model metadata. But we also allow explicit override via this parameter for additional control (or in case the default is not available for certain latest models)\n    \n*   `num_output`: The number of maximum output from the LLM. Typically we set this automatically given the model metadata. This parameter does not actually limit the model output, it affects the amount of \u201cspace\u201d we save for the output, when computing available context window size for packing text from retrieved Nodes.\n    \n\nHere\u2019s a complete example that sets up all objects using their default settings:\n\nfrom llama\\_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.text\\_splitter import TokenTextSplitter\nfrom llama\\_index.node\\_parser import SimpleNodeParser\n\nllm \\= OpenAI(model\\='text-davinci-003', temperature\\=0, max\\_tokens\\=256)\nembed\\_model \\= OpenAIEmbedding()\nnode\\_parser \\= SimpleNodeParser.from\\_defaults(\n  text\\_splitter\\=TokenTextSplitter(chunk\\_size\\=1024, chunk\\_overlap\\=20)\n)\nprompt\\_helper \\= PromptHelper(\n  context\\_window\\=4096,\n  num\\_output\\=256,\n  chunk\\_overlap\\_ratio\\=0.1,\n  chunk\\_size\\_limit\\=None\n)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n  llm\\=llm,\n  embed\\_model\\=embed\\_model,\n  node\\_parser\\=node\\_parser,\n  prompt\\_helper\\=prompt\\_helper\n)\n\n### Setting global configuration[\uf0c1](#setting-global-configuration \"Permalink to this heading\")\n\nYou can set a service context as the global default that applies to the entire LlamaIndex pipeline:\n\nfrom llama\\_index import set\\_global\\_service\\_context\nset\\_global\\_service\\_context(service\\_context)\n\n### Setting local configuration[\uf0c1](#setting-local-configuration \"Permalink to this heading\")\n\nYou can pass in a service context to specific part of the pipeline to override the default configuration:\n\nquery\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)"
}