{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/development/contributing.html",
        "title": "Contributing to LlamaIndex - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\n## Contributing to LlamaIndex[\uf0c1](#contributing-to-llamaindex \"Permalink to this heading\")\n\nInterested in contributing to LlamaIndex? Here\u2019s how to get started!\n\n## Contribution Guideline[\uf0c1](#contribution-guideline \"Permalink to this heading\")\n\nThe best part of LlamaIndex is our community of users and contributors.\n\n### What should I work on?[\uf0c1](#what-should-i-work-on \"Permalink to this heading\")\n\n1.  \ud83c\udd95 Extend core modules\n    \n2.  \ud83d\udc1b Fix bugs\n    \n3.  \ud83c\udf89 Add usage examples\n    \n4.  \ud83e\uddea Add experimental features\n    \n5.  \ud83d\udcc4 Improve code quality & documentation\n    \n\nAlso, join our Discord for ideas and discussions: [https://discord.gg/dGcwcsnxhU](https://discord.gg/dGcwcsnxhU).\n\n### 1\\. \ud83c\udd95 Extend Core Modules[\uf0c1](#extend-core-modules \"Permalink to this heading\")\n\nThe most impactful way to contribute to LlamaIndex is extending our core modules:\n\n[![LlamaIndex modules](https://github.com/jerryjliu/llama_index/raw/main/docs/_static/contribution/contrib.png)](https://github.com/jerryjliu/llama_index/raw/main/docs/_static/contribution/contrib.png)\n\nWe welcome contributions in _all_ modules shown above. So far, we have implemented a core set of functionalities for each. As a contributor, you can help each module unlock its full potential.\n\n**NOTE**: We are making rapid improvements to the project, and as a result, some interfaces are still volatile. Specifically, we are actively working on making the following components more modular and extensible (uncolored boxes above): core indexes, document stores, index queries, query runner\n\n#### Module Details[\uf0c1](#module-details \"Permalink to this heading\")\n\nBelow, we will describe what each module does, give a high-level idea of the interface, show existing implementations, and give some ideas for contribution.\n\n* * *\n\n#### Data Loaders[\uf0c1](#data-loaders \"Permalink to this heading\")\n\nA data loader ingests data of any format from anywhere into `Document` objects, which can then be parsed and indexed.\n\n**Interface**: `load_data` takes arbitrary arguments as input (e.g. path to data), and outputs a sequence of `Document` objects.\n\n**Examples**:\n\n*   [Google Sheets Loader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/google_sheets)\n    \n*   [Gmail Loader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/gmail)\n    \n*   [Github Repository Loader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/github_repo)\n    \n\nContributing a data loader is easy and super impactful for the community. The preferred way to contribute is making a PR at [LlamaHub Github](https://github.com/emptycrown/llama-hub).\n\n**Ideas**\n\n*   Want to load something but there\u2019s no LlamaHub data loader for it yet? Make a PR!\n    \n\n* * *\n\n#### Node Parser[\uf0c1](#node-parser \"Permalink to this heading\")\n\nA node parser parses `Document` objects into `Node` objects (atomic unit of data that LlamaIndex operates over, e.g., chunk of text, image, or table). It is responsible for splitting text (via text splitters) and explicitly modelling the relationship between units of data (e.g. A is the source of B, C is a chunk after D).\n\n**Interface**: `get_nodes_from_documents` takes a sequence of `Document` objects as input, and outputs a sequence of `Node` objects.\n\n**Examples**:\n\n*   [Simple Node Parser](https://github.com/jerryjliu/llama_index/blob/main/llama_index/node_parser/simple.py)\n    \n\nSee [the API reference](https://gpt-index.readthedocs.io/en/latest/api_reference/index.html) for full details.\n\n**Ideas**:\n\n*   Add new `Node` relationships to model to model hierarchical documents (e.g. play-act-scene, chapter-section-heading).\n    \n\n* * *\n\n#### Text Splitters[\uf0c1](#text-splitters \"Permalink to this heading\")\n\nText splitter splits a long text `str` into smaller text `str` chunks with desired size and splitting \u201cstrategy\u201d since LLMs have a limited context window size, and the quality of text chunk used as context impacts the quality of query results.\n\n**Interface**: `split_text` takes a `str` as input, and outputs a sequence of `str`\n\n**Examples**:\n\n*   [Token Text Splitter](https://github.com/jerryjliu/llama_index/blob/main/llama_index/langchain_helpers/text_splitter.py#L26)\n    \n*   [Sentence Splitter](https://github.com/jerryjliu/llama_index/blob/main/llama_index/langchain_helpers/text_splitter.py#L276)\n    \n*   [Code Splitter](https://github.com/jerryjliu/llama_index/blob/main/llama_index/langchain_helpers/text_splitter.py#L476)\n    \n\n* * *\n\n#### Document/Index/KV Stores[\uf0c1](#document-index-kv-stores \"Permalink to this heading\")\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize Document Stores (where ingested documents (i.e., `Node` objects) are stored), and Index Stores (where index metadata are stored)\n\nWe have an underlying key-value abstraction backing the document/index stores. Currently we support in-memory and MongoDB storage for these stores. Open to contributions!\n\nSee [Storage guide](https://gpt-index.readthedocs.io/en/latest/how_to/storage.html) for details.\n\n* * *\n\n#### Managed Index[\uf0c1](#managed-index \"Permalink to this heading\")\n\nA managed index is used to represent an index that\u2019s managed via an API, exposing API calls to index documents and query documents.\n\nCurrently we support the [VectaraIndex](https://github.com/run-llama/llama_index/tree/ca09272af000307762d301c99da46ddc70d3bfd2/llama_index/indices/managed/vectara). Open to contributions!\n\nSee [Managed Index docs](https://gpt-index.readthedocs.io/en/stable/community/integrations/managed_indices.html) for details.\n\n* * *\n\n#### Vector Stores[\uf0c1](#vector-stores \"Permalink to this heading\")\n\nOur vector store classes store embeddings and support lookup via similarity search. These serve as the main data store and retrieval engine for our vector index.\n\n**Interface**:\n\n*   `add` takes in a sequence of `NodeWithEmbeddings` and insert the embeddings (and possibly the node contents & metadata) into the vector store.\n    \n*   `delete` removes entries given document IDs.\n    \n*   `query` retrieves top-k most similar entries given a query embedding.\n    \n\n**Examples**:\n\n*   [Pinecone](https://github.com/jerryjliu/llama_index/blob/main/llama_index/vector_stores/pinecone.py)\n    \n*   [Faiss](https://github.com/jerryjliu/llama_index/blob/main/llama_index/vector_stores/faiss.py)\n    \n*   [Chroma](https://github.com/jerryjliu/llama_index/blob/main/llama_index/vector_stores/chroma.py)\n    \n*   [DashVector](https://github.com/jerryjliu/llama_index/blob/main/llama_index/vector_stores/dashvector.py)\n    \n\n**Ideas**:\n\n*   See a vector database out there that we don\u2019t support yet? Make a PR!\n    \n\nSee [reference](https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_stores/stores.html) for full details.\n\n* * *\n\n#### Retrievers[\uf0c1](#retrievers \"Permalink to this heading\")\n\nOur retriever classes are lightweight classes that implement a `retrieve` method. They may take in an index class as input - by default, each of our indices (list, vector, keyword) have an associated retriever. The output is a set of `NodeWithScore` objects (a `Node` object with an extra `score` field).\n\nYou may also choose to implement your own retriever classes on top of your own data if you wish.\n\n**Interface**:\n\n*   `retrieve` takes in a `str` or `QueryBundle` as input, and outputs a list of `NodeWithScore` objects\n    \n\n**Examples**:\n\n*   [Vector Index Retriever](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/vector_store/retrievers.py)\n    \n*   [List Index Retriever](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/list/retrievers.py)\n    \n*   [Transform Retriever](https://github.com/jerryjliu/llama_index/blob/main/llama_index/retrievers/transform_retriever.py)\n    \n\n**Ideas**:\n\n*   Besides the \u201cdefault\u201d retrievers built on top of each index, what about fancier retrievers? E.g. retrievers that take in other retrievers as input? Or other types of data?\n    \n\n* * *\n\n#### Query Engines[\uf0c1](#query-engines \"Permalink to this heading\")\n\nOur query engine classes are lightweight classes that implement a `query` method; the query returns a response type. For instance, they may take in a retriever class as input; our `RetrieverQueryEngine` takes in a `retriever` as input as well as a `BaseSynthesizer` class for response synthesis, and the `query` method performs retrieval and synthesis before returning the final result. They may take in other query engine classes in as input too.\n\n**Interface**:\n\n*   `query` takes in a `str` or `QueryBundle` as input, and outputs a `Response` object.\n    \n\n**Examples**:\n\n*   [Retriever Query Engine](https://github.com/jerryjliu/llama_index/blob/main/llama_index/query_engine/retriever_query_engine.py)\n    \n*   [Transform Query Engine](https://github.com/jerryjliu/llama_index/blob/main/llama_index/query_engine/transform_query_engine.py)\n    \n\n* * *\n\n#### Query Transforms[\uf0c1](#query-transforms \"Permalink to this heading\")\n\nA query transform augments a raw query string with associated transformations to improve index querying. This can interpreted as a pre-processing stage, before the core index query logic is executed.\n\n**Interface**: `run` takes in a `str` or `Querybundle` as input, and outputs a transformed `QueryBundle`.\n\n**Examples**:\n\n*   [Hypothetical Document Embeddings](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/query/query_transform/base.py#L77)\n    \n*   [Query Decompose](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/query/query_transform/base.py#L124)\n    \n\nSee [guide](https://gpt-index.readthedocs.io/en/latest/how_to/query/query_transformations.html#hyde-hypothetical-document-embeddings) for more information.\n\n* * *\n\n#### Token Usage Optimizers[\uf0c1](#token-usage-optimizers \"Permalink to this heading\")\n\nA token usage optimizer refines the retrieved `Nodes` to reduce token usage during response synthesis.\n\n**Interface**: `optimize` takes in the `QueryBundle` and a text chunk `str`, and outputs a refined text chunk `str` that yields a more optimized response\n\n**Examples**:\n\n*   [Sentence Embedding Optimizer](https://github.com/jerryjliu/llama_index/blob/main/llama_index/optimization/optimizer.py)\n    \n\n* * *\n\n#### Node Postprocessors[\uf0c1](#node-postprocessors \"Permalink to this heading\")\n\nA node postprocessor refines a list of retrieve nodes given configuration and context.\n\n**Interface**: `postprocess_nodes` takes a list of `Nodes` and extra metadata (e.g. similarity and query), and outputs a refined list of `Nodes`.\n\n**Examples**:\n\n*   [Keyword Postprocessor](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/postprocessor/node.py#L32): filters nodes based on keyword match\n    \n*   [Similarity Postprocessor](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/postprocessor/node.py#L62): filers nodes based on similarity threshold\n    \n*   [Prev Next Postprocessor](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/postprocessor/node.py#L135): fetches additional nodes to augment context based on node relationships.\n    \n\n* * *\n\n#### Output Parsers[\uf0c1](#output-parsers \"Permalink to this heading\")\n\nA output parser enables us to extract structured output from the plain text output generated by the LLM.\n\n**Interface**:\n\n*   `format`: formats a query `str` with structured output formatting instructions, and outputs the formatted `str`\n    \n*   `parse`: takes a `str` (from LLM response) as input, and gives a parsed tructured output (optionally also validated, error-corrected).\n    \n\n**Examples**:\n\n*   [Guardrails Output Parser](https://github.com/jerryjliu/llama_index/blob/main/llama_index/output_parsers/guardrails.py)\n    \n*   [Langchain Output Parser](https://github.com/jerryjliu/llama_index/blob/main/llama_index/output_parsers/langchain.py)\n    \n\nSee [guide](https://gpt-index.readthedocs.io/en/latest/how_to/output_parsing.html) for more information.\n\n* * *\n\n### 2\\. \ud83d\udc1b Fix Bugs[\uf0c1](#fix-bugs \"Permalink to this heading\")\n\nMost bugs are reported and tracked in the [Github Issues Page](https://github.com/jerryjliu/llama_index/issues). We try our best in triaging and tagging these issues:\n\n*   Issues tagged as `bug` are confirmed bugs.\n    \n*   New contributors may want to start with issues tagged with `good first issue`.\n    \n\nPlease feel free to open an issue and/or assign an issue to yourself.\n\n### 3\\. \ud83c\udf89 Add Usage Examples[\uf0c1](#add-usage-examples \"Permalink to this heading\")\n\nIf you have applied LlamaIndex to a unique use-case (e.g. interesting dataset, customized index structure, complex query), we would love your contribution in the form of:\n\n1.  a guide: e.g. [guide to LlamIndex + Structured Data](https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html)\n    \n2.  an example notebook: e.g. [Composable Indices Demo](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/ComposableIndices-Prior.ipynb)\n    \n\n### 4\\. \ud83e\uddea Add Experimental Features[\uf0c1](#add-experimental-features \"Permalink to this heading\")\n\nIf you have a crazy idea, make a PR for it! Whether if it\u2019s the latest research, or what you thought of in the shower, we\u2019d love to see creative ways to improve LlamaIndex.\n\n### 5\\. \ud83d\udcc4 Improve Code Quality & Documentation[\uf0c1](#improve-code-quality-documentation \"Permalink to this heading\")\n\nWe would love your help in making the project cleaner, more robust, and more understandable. If you find something confusing, it most likely is for other people as well. Help us be better!\n\n## Development Guideline[\uf0c1](#development-guideline \"Permalink to this heading\")\n\n### Environment Setup[\uf0c1](#environment-setup \"Permalink to this heading\")\n\nLlamaIndex is a Python package. We\u2019ve tested primarily with Python versions >= 3.8. Here\u2019s a quick and dirty guide to getting your environment setup.\n\nFirst, create a fork of LlamaIndex, by clicking the \u201cFork\u201d button on the [LlamaIndex Github page](https://github.com/jerryjliu/llama_index). Following [these steps](https://docs.github.com/en/get-started/quickstart/fork-a-repo) for more details on how to fork the repo and clone the forked repo.\n\nThen, create a new Python virtual environment using poetry.\n\n*   [Install poetry](https://python-poetry.org/docs/#installation) - this will help you manage package dependencies\n    \n*   `poetry shell` - this command creates a virtual environment, which keeps installed packages contained to this project\n    \n*   `poetry install --with dev,docs` - this will install all dependencies needed for most local development\n    \n\nNow you should be set!\n\n### Validating your Change[\uf0c1](#validating-your-change \"Permalink to this heading\")\n\nLet\u2019s make sure to `format/lint` our change. For bigger changes, let\u2019s also make sure to `test` it and perhaps create an `example notebook`.\n\n#### Formatting/Linting[\uf0c1](#formatting-linting \"Permalink to this heading\")\n\nYou can format and lint your changes with the following commands in the root directory:\n\nYou can also make use of our pre-commit hooks by setting up git hook scripts:\n\nWe run an assortment of linters: `black`, `ruff`, `mypy`.\n\n#### Testing[\uf0c1](#testing \"Permalink to this heading\")\n\nFor bigger changes, you\u2019ll want to create a unit test. Our tests are in the `tests` folder. We use `pytest` for unit testing. To run all unit tests, run the following in the root dir:\n\npip install \\-r data\\_requirements.txt\npytest tests\n\n### Creating a pull request[\uf0c1](#creating-a-pull-request \"Permalink to this heading\")\n\nSee [these instructions](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) to open a pull request against the main LlamaIndex repo."
}