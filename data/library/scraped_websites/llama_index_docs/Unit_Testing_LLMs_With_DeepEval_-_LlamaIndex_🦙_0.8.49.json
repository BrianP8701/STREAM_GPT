{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/community/integrations/deepeval.html",
        "title": "Unit Testing LLMs With DeepEval - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[DeepEval](https://github.com/confident-ai/deepeval) provides unit testing for AI agents and LLM-powered applications. It provides a really simple interface for LlamaIndex developers to write tests and helps developers ensure AI applications run as expected.\n\nDeepEval provides an opinionated framework to measure responses and is completely open-source.\n\n## Installation and Setup[\uf0c1](#installation-and-setup \"Permalink to this heading\")\n\nAdding [DeepEval](https://github.com/confident-ai/deepeval) is simple, just install and configure it:\n\npip install \\-q \\-q llama-index\npip install \\-U deepeval\n\nOnce installed , you can get set up and start writing tests.\n\n\\# Optional step: Login to get a nice dashboard for your tests later!\n\\# During this step - make sure to save your project as llama\ndeepeval login\ndeepeval test generate test\\_sample.py\n\nYou can then run tests as such:\n\ndeepeval test run test\\_sample.py\n\nAfter running this, you will get a beautiful dashboard like so:\n\n![Sample dashboard](https://raw.githubusercontent.com/confident-ai/deepeval/main/docs/assets/dashboard-screenshot.png)\n\n## Use With Your LlamaIndex[\uf0c1](#use-with-your-llamaindex \"Permalink to this heading\")\n\nDeepEval integrates nicely with LlamaIndex\u2019s `BaseEvaluator` class. Below is an example of the factual consistency documentation.\n\nfrom llama\\_index.response.schema import Response\nfrom typing import List\nfrom llama\\_index.schema import Document\nfrom deepeval.metrics.factual\\_consistency import FactualConsistencyMetric\n\nfrom llama\\_index import (\n    TreeIndex,\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    Response,\n)\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.evaluation import FaithfulnessEvaluator\n\nimport os\nimport openai\n\napi\\_key \\= \"sk-XXX\"\nopenai.api\\_key \\= api\\_key\n\ngpt4 \\= OpenAI(temperature\\=0, model\\=\"gpt-4\", api\\_key\\=api\\_key)\nservice\\_context\\_gpt4 \\= ServiceContext.from\\_defaults(llm\\=gpt4)\n\n### Getting a lLamaHub Loader[\uf0c1](#getting-a-llamahub-loader \"Permalink to this heading\")\n\nfrom llama\\_index import download\\_loader\n\nWikipediaReader \\= download\\_loader(\"WikipediaReader\")\n\nloader \\= WikipediaReader()\ndocuments \\= loader.load\\_data(pages\\=\\['Tokyo'\\])\ntree\\_index \\= TreeIndex.from\\_documents(documents\\=documents)\nvector\\_index \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\\_gpt4\n)\n\nWe then build an evaluator based on the `BaseEvaluator` class that requires an `evaluate` method.\n\nIn this example, we show you how to write a factual consistency check.\n\nfrom typing import Any, Optional, Sequence\nfrom llama\\_index.evaluation.base import BaseEvaluator, EvaluationResult\n\nclass FactualConsistencyEvaluator(BaseEvaluator):\n    def evaluate(\n        self,\n        query: Optional\\[str\\] \\= None,\n        contexts: Optional\\[Sequence\\[str\\]\\] \\= None,\n        response: Optional\\[str\\] \\= None,\n        \\*\\*kwargs: Any,\n    ) \\-> EvaluationResult:\n        \"\"\"Evaluate factual consistency metrics\"\"\"\n        if response is None or contexts is None:\n            raise ValueError('Please provide \"response\" and \"contexts\".')\n        metric \\= FactualConsistencyMetric()\n        context \\= \" \".join(\\[d for d in contexts\\])\n        score \\= metric.measure(output\\=response, context\\=context)\n        return EvaluationResult(\n            response\\=response,\n            contexts\\=contexts,\n            passing\\=metric.is\\_successful(),\n            score\\=score,\n        )\n\nevaluator \\= FactualConsistencyEvaluator()\n\nYou can then evaluate as such:\n\nquery\\_engine \\= tree\\_index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"How did Tokyo get its name?\")\neval\\_result \\= evaluator.evaluate\\_response(response\\=response)"
}