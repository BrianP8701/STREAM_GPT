{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/ingestion.html",
        "title": "Building Data Ingestion from Scratch - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Building Data Ingestion from Scratch[\uf0c1](#building-data-ingestion-from-scratch \"Permalink to this heading\")\n\nIn this tutorial, we show you how to build a data ingestion pipeline into a vector database.\n\nWe use Pinecone as the vector database.\n\nWe will show how to do the following:\n\n1.  How to load in documents.\n    \n2.  How to use a text splitter to split documents.\n    \n3.  How to **manually** construct nodes from each text chunk.\n    \n4.  \\[Optional\\] Add metadata to each Node.\n    \n5.  How to generate embeddings for each text chunk.\n    \n6.  How to insert into a vector database.\n    \n\n## Pinecone[\uf0c1](#pinecone \"Permalink to this heading\")\n\nYou will need a [pinecone.io](https://www.pinecone.io/) api key for this tutorial. You can [sign up for free](https://app.pinecone.io/?sessionType=signup) to get a Starter account.\n\nIf you create a Starter account, you can name your application anything you like.\n\nOnce you have an account, navigate to \u2018API Keys\u2019 in the Pinecone console. You can use the default key or create a new one for this tutorial.\n\nSave your api key and its environment (`gcp_starter` for free accounts). You will need them below.\n\n## OpenAI[\uf0c1](#openai \"Permalink to this heading\")\n\nYou will need an [OpenAI](https://openai.com/) api key for this tutorial. Login to your [platform.openai.com](https://platform.openai.com/) account, click on your profile picture in the upper right corner, and choose \u2018API Keys\u2019 from the menu. Create an API key for this tutorial and save it. You will need it below.\n\n## Environment[\uf0c1](#environment \"Permalink to this heading\")\n\nFirst we add our dependencies.\n\n!pip \\-q install python-dotenv pinecone-client llama-index pymupdf\n\n### Set Environment Variables[\uf0c1](#set-environment-variables \"Permalink to this heading\")\n\nWe create a file for our environment variables. Do not commit this file or share it!\n\nNote: Google Colabs will let you create but not open a .env\n\ndotenv\\_path \\= (  \\# Google Colabs will not let you open a .env, but you can set\n    \"env\"\n)\nwith open(dotenv\\_path, \"w\") as f:\n    f.write('PINECONE\\_API\\_KEY=\"<your api key>\"\\\\n')\n    f.write('PINECONE\\_ENVIRONMENT=\"gcp-starter\"\\\\n')\n    f.write('OPENAI\\_API\\_KEY=\"<your api key>\"\\\\n')\n\nSet your OpenAI api key, and Pinecone api key and environment in the file we created.\n\nimport os\nfrom dotenv import load\\_dotenv\n\nload\\_dotenv(dotenv\\_path\\=dotenv\\_path)\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can start loading data into Pinecone.\n\nNote: Do not save your API keys in the code or add pinecone\\_env to your repo!\n\napi\\_key \\= os.environ\\[\"PINECONE\\_API\\_KEY\"\\]\nenvironment \\= os.environ\\[\"PINECONE\\_ENVIRONMENT\"\\]\npinecone.init(api\\_key\\=api\\_key, environment\\=environment)\n\nindex\\_name \\= \"llamaindex-rag-fs\"\n\n\\# \\[Optional\\] Delete the index before re-running the tutorial.\n\\# pinecone.delete\\_index(index\\_name)\n\n\\# dimensions are for text-embedding-ada-002\npinecone.create\\_index(\n    index\\_name, dimension\\=1536, metric\\=\"euclidean\", pod\\_type\\=\"p1\"\n)\n\npinecone\\_index \\= pinecone.Index(index\\_name)\n\n\\# \\[Optional\\] drop contents in index - will not work on free accounts\npinecone\\_index.delete(deleteAll\\=True)\n\n### Create PineconeVectorStore[\uf0c1](#create-pineconevectorstore \"Permalink to this heading\")\n\nSimple wrapper abstraction to use in LlamaIndex. Wrap in StorageContext so we can easily load in Nodes.\n\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\n\nvector\\_store \\= PineconeVectorStore(pinecone\\_index\\=pinecone\\_index)\n\n## Build an Ingestion Pipeline from Scratch[\uf0c1](#build-an-ingestion-pipeline-from-scratch \"Permalink to this heading\")\n\nWe show how to build an ingestion pipeline as mentioned in the introduction.\n\nNote that steps (2) and (3) can be handled via our `NodeParser` abstractions, which handle splitting and node creation.\n\nFor the purposes of this tutorial, we show you how to create these objects manually.\n\n### 1\\. Load Data[\uf0c1](#load-data \"Permalink to this heading\")\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\n\\--2023-10-13 01:45:14--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 128.84.21.199\nConnecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) \\[application/pdf\\]\nSaving to: \u2018data/llama2.pdf\u2019\n\ndata/llama2.pdf     100%\\[===================>\\]  13.03M  7.59MB/s    in 1.7s    \n\n2023-10-13 01:45:16 (7.59 MB/s) - \u2018data/llama2.pdf\u2019 saved \\[13661300/13661300\\]\n\nfile\\_path \\= \"./data/llama2.pdf\"\ndoc \\= fitz.open(file\\_path)\n\n### 2\\. Use a Text Splitter to Split Documents[\uf0c1](#use-a-text-splitter-to-split-documents \"Permalink to this heading\")\n\nHere we import our `SentenceSplitter` to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible.\n\nfrom llama\\_index.text\\_splitter import SentenceSplitter\n\ntext\\_splitter \\= SentenceSplitter(\n    chunk\\_size\\=1024,\n    \\# separator=\" \",\n)\n\ntext\\_chunks \\= \\[\\]\n\\# maintain relationship with source doc index, to help inject doc metadata in (3)\ndoc\\_idxs \\= \\[\\]\nfor doc\\_idx, page in enumerate(doc):\n    page\\_text \\= page.get\\_text(\"text\")\n    cur\\_text\\_chunks \\= text\\_splitter.split\\_text(page\\_text)\n    text\\_chunks.extend(cur\\_text\\_chunks)\n    doc\\_idxs.extend(\\[doc\\_idx\\] \\* len(cur\\_text\\_chunks))\n\n### 3\\. Manually Construct Nodes from Text Chunks[\uf0c1](#manually-construct-nodes-from-text-chunks \"Permalink to this heading\")\n\nWe convert each chunk into a `TextNode` object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.\n\nWe inject metadata from the document into each node.\n\nThis essentially replicates logic in our `SimpleNodeParser`.\n\nfrom llama\\_index.schema import TextNode\n\nnodes \\= \\[\\]\nfor idx, text\\_chunk in enumerate(text\\_chunks):\n    node \\= TextNode(\n        text\\=text\\_chunk,\n    )\n    src\\_doc\\_idx \\= doc\\_idxs\\[idx\\]\n    src\\_page \\= doc\\[src\\_doc\\_idx\\]\n    nodes.append(node)\n\n\\# print a sample node\nprint(nodes\\[0\\].get\\_content(metadata\\_mode\\=\"all\"))\n\n### 5\\. Generate Embeddings for each Node[\uf0c1](#generate-embeddings-for-each-node \"Permalink to this heading\")\n\nGenerate document embeddings for each Node using our OpenAI embedding model (`text-embedding-ada-002`).\n\nStore these on the `embedding` property on each Node.\n\nfrom llama\\_index.embeddings import OpenAIEmbedding\n\nembed\\_model \\= OpenAIEmbedding()\n\nfor node in nodes:\n    node\\_embedding \\= embed\\_model.get\\_text\\_embedding(\n        node.get\\_content(metadata\\_mode\\=\"all\")\n    )\n    node.embedding \\= node\\_embedding\n\n### 6\\. Load Nodes into a Vector Store[\uf0c1](#load-nodes-into-a-vector-store \"Permalink to this heading\")\n\nWe now insert these nodes into our `PineconeVectorStore`.\n\n**NOTE**: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use `VectorStoreIndex` in the next section to fast-trak retrieval/querying.\n\n## Retrieve and Query from the Vector Store[\uf0c1](#retrieve-and-query-from-the-vector-store \"Permalink to this heading\")\n\nNow that our ingestion is complete, we can retrieve/query this vector store.\n\n**NOTE**: We can use our high-level `VectorStoreIndex` abstraction here. See the next section to see how to define retrieval at a lower-level!\n\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.storage import StorageContext\n\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store)\n\nquery\\_engine \\= index.as\\_query\\_engine()\n\nquery\\_str \\= \"Can you tell me about the key concepts for safety finetuning\"\n\nresponse \\= query\\_engine.query(query\\_str)"
}