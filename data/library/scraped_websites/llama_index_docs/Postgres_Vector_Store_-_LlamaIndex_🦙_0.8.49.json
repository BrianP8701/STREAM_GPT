{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/postgres.html",
        "title": "Postgres Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\n## Postgres Vector Store[\uf0c1](#postgres-vector-store \"Permalink to this heading\")\n\nIn this notebook we are going to show how to use [Postgresql](https://www.postgresql.org/) and [pgvector](https://github.com/pgvector/pgvector) to perform vector searches in LlamaIndex\n\n\\# import logging\n\\# import sys\n\n\\# Uncomment to see debug logs\n\\# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama\\_index import SimpleDirectoryReader, StorageContext\nfrom llama\\_index.indices.vector\\_store import VectorStoreIndex\nfrom llama\\_index.vector\\_stores import PGVectorStore\nimport textwrap\nimport openai\n\n## Setup OpenAI[\uf0c1](#setup-openai \"Permalink to this heading\")\n\nThe first step is to configure the openai key. It will be used to created embeddings for the documents loaded into the index\n\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"<your key>\"\nopenai.api\\_key \\= \"<your key>\"\n\n## Loading documents[\uf0c1](#loading-documents \"Permalink to this heading\")\n\nLoad the documents stored in the `paul_graham_essay` using the SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\nprint(\"Document ID:\", documents\\[0\\].doc\\_id)\n\nDocument ID: d05d1211-b9af-4b05-8da6-956e4b389467\n\n## Create the Database[\uf0c1](#create-the-database \"Permalink to this heading\")\n\nUsing an existing postgres running at localhost, create the database we\u2019ll be using.\n\nimport psycopg2\n\nconnection\\_string \\= \"postgresql://postgres:password@localhost:5432\"\ndb\\_name \\= \"vector\\_db\"\nconn \\= psycopg2.connect(connection\\_string)\nconn.autocommit \\= True\n\nwith conn.cursor() as c:\n    c.execute(f\"DROP DATABASE IF EXISTS {db\\_name}\")\n    c.execute(f\"CREATE DATABASE {db\\_name}\")\n\n## Create the index[\uf0c1](#create-the-index \"Permalink to this heading\")\n\nHere we create an index backed by Postgres using the documents loaded previously. PGVectorStore takes a few arguments.\n\nfrom sqlalchemy import make\\_url\n\nurl \\= make\\_url(connection\\_string)\nvector\\_store \\= PGVectorStore.from\\_params(\n    database\\=db\\_name,\n    host\\=url.host,\n    password\\=url.password,\n    port\\=url.port,\n    user\\=url.username,\n    table\\_name\\=\"paul\\_graham\\_essay\",\n    embed\\_dim\\=1536,  \\# openai embedding dimension\n)\n\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context, show\\_progress\\=True\n)\nquery\\_engine \\= index.as\\_query\\_engine()\n\n## Query the index[\uf0c1](#query-the-index \"Permalink to this heading\")\n\nWe can now ask questions using our index.\n\nresponse \\= query\\_engine.query(\"What did the author do?\")\n\nprint(textwrap.fill(str(response), 100))\n\nThe author worked on writing and programming before college. They wrote short stories and tried\nwriting programs on an IBM 1401 computer. They also built a microcomputer and started programming on\nit, writing simple games and a word processor. In college, the author initially planned to study\nphilosophy but switched to AI. They were inspired by a novel called The Moon is a Harsh Mistress and\na PBS documentary featuring Terry Winograd using SHRDLU.\n\nresponse \\= query\\_engine.query(\"What happened in the mid 1980s?\")\n\nprint(textwrap.fill(str(response), 100))\n\nIn the mid-1980s, the author spent a significant amount of time working on a book called \"On Lisp\"\nand had obtained a contract to publish it. They were paid large amounts of money for their work,\nwhich allowed them to save enough to go back to RISD (Rhode Island School of Design) and pay off\ntheir college loans. They also learned valuable lessons during this time, such as the importance of\nhaving technology companies run by product people rather than sales people, the drawbacks of editing\ncode by too many people, and the significance of being the \"entry level\" option in a competitive\nmarket.\n\n## Querying existing index[\uf0c1](#querying-existing-index \"Permalink to this heading\")\n\nvector\\_store \\= PGVectorStore.from\\_params(\n    database\\=\"vector\\_db\",\n    host\\=\"localhost\",\n    password\\=\"password\",\n    port\\=5432,\n    user\\=\"postgres\",\n    table\\_name\\=\"paul\\_graham\\_essay\",\n    embed\\_dim\\=1536,  \\# openai embedding dimension\n)\n\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store\\=vector\\_store)\nquery\\_engine \\= index.as\\_query\\_engine()\n\nresponse \\= query\\_engine.query(\"What did the author do?\")\n\nprint(textwrap.fill(str(response), 100))\n\nThe author worked on writing and programming before college. They wrote short stories and tried\nwriting programs on an IBM 1401 computer. They also built a microcomputer and started programming on\nit, writing simple games and a word processor. In college, the author initially planned to study\nphilosophy but switched to AI due to their interest in intelligent computers. They taught themselves\nAI by learning Lisp.\n\n## Hybrid Search[\uf0c1](#hybrid-search \"Permalink to this heading\")\n\nTo enable hybrid search, you need to:\n\n1.  pass in `hybrid_search=True` when constructing the `PGVectorStore` (and optionally configure `text_search_config` with the desired language)\n    \n2.  pass in `vector_store_query_mode=\"hybrid\"` when constructing the query engine (this config is passed to the retriever under the hood). You can also optionally set the `sparse_top_k` to configure how many results we should obtain from sparse text search (default is using the same value as `similarity_top_k`).\n    \n\nfrom sqlalchemy import make\\_url\n\nurl \\= make\\_url(connection\\_string)\nhybrid\\_vector\\_store \\= PGVectorStore.from\\_params(\n    database\\=db\\_name,\n    host\\=url.host,\n    password\\=url.password,\n    port\\=url.port,\n    user\\=url.username,\n    table\\_name\\=\"paul\\_graham\\_essay\\_hybrid\\_search\",\n    embed\\_dim\\=1536,  \\# openai embedding dimension\n    hybrid\\_search\\=True,\n    text\\_search\\_config\\=\"english\",\n)\n\nstorage\\_context \\= StorageContext.from\\_defaults(\n    vector\\_store\\=hybrid\\_vector\\_store\n)\nhybrid\\_index \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\n/Users/suo/dev/llama\\_index/llama\\_index/vector\\_stores/postgres.py:217: SAWarning: TypeDecorator TSVector() will not produce a cache key because the \\`\\`cache\\_ok\\`\\` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this warning at: https://sqlalche.me/e/20/cprf)\n  session.commit()\n\nhybrid\\_query\\_engine \\= hybrid\\_index.as\\_query\\_engine(\n    vector\\_store\\_query\\_mode\\=\"hybrid\", sparse\\_top\\_k\\=2\n)\nhybrid\\_response \\= hybrid\\_query\\_engine.query(\n    \"Who does Paul Graham think of with the word schtick\"\n)"
}