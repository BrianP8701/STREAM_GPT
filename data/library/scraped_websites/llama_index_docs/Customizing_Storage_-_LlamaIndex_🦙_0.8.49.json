{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/customization.html",
        "title": "Customizing Storage - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Customizing Storage[\uf0c1](#customizing-storage \"Permalink to this heading\")\n\nBy default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code:\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"Summarize the documents.\")\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.\n\n![](https://docs.llamaindex.ai/en/stable/_images/storage.png)\n\n## Low-Level API[\uf0c1](#low-level-api \"Permalink to this heading\")\n\nTo do this, instead of the high-level API,\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\nwe use a lower-level API that gives more granular control:\n\nfrom llama\\_index.storage.docstore import SimpleDocumentStore\nfrom llama\\_index.storage.index\\_store import SimpleIndexStore\nfrom llama\\_index.vector\\_stores import SimpleVectorStore\nfrom llama\\_index.node\\_parser import SimpleNodeParser\n\n\\# create parser and parse document into nodes\nparser \\= SimpleNodeParser.from\\_defaults()\nnodes \\= parser.get\\_nodes\\_from\\_documents(documents)\n\n\\# create storage context using default stores\nstorage\\_context \\= StorageContext.from\\_defaults(\n    docstore\\=SimpleDocumentStore(),\n    vector\\_store\\=SimpleVectorStore(),\n    index\\_store\\=SimpleIndexStore(),\n)\n\n\\# create (or load) docstore and add nodes\nstorage\\_context.docstore.add\\_documents(nodes)\n\n\\# build index\nindex \\= VectorStoreIndex(nodes, storage\\_context\\=storage\\_context)\n\n\\# save index\nindex.storage\\_context.persist(persist\\_dir\\=\"<persist\\_dir>\")\n\n\\# can also set index\\_id to save multiple indexes to the same folder\nindex.set\\_index\\_id(\"<index\\_id>\")\nindex.storage\\_context.persist(persist\\_dir\\=\"<persist\\_dir>\")\n\n\\# to load index later, make sure you setup the storage context\n\\# this will loaded the persisted stores from persist\\_dir\nstorage\\_context \\= StorageContext.from\\_defaults(\n    persist\\_dir\\=\"<persist\\_dir>\"\n)\n\n\\# then load the index object\nfrom llama\\_index import load\\_index\\_from\\_storage\nloaded\\_index \\= load\\_index\\_from\\_storage(storage\\_context)\n\n\\# if loading an index from a persist\\_dir containing multiple indexes\nloaded\\_index \\= load\\_index\\_from\\_storage(storage\\_context, index\\_id\\=\"<index\\_id>\")\n\n\\# if loading multiple indexes from a persist dir\nloaded\\_indicies \\= load\\_index\\_from\\_storage(storage\\_context, index\\_ids\\=\\[\"<index\\_id>\", ...\\])\n\nYou can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores. See [Document Stores](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/docstores.html), [Vector Stores](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/vector_stores.html), [Index Stores](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/index_stores.html) guides for more details.\n\nFor saving and loading a graph/composable index, see the [full guide here](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/composability.html).\n\n## Vector Store Integrations and Storage[\uf0c1](#vector-store-integrations-and-storage \"Permalink to this heading\")\n\nMost of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to explicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.\n\nThe vector stores that support this practice are:\n\n*   CognitiveSearchVectorStore\n    \n*   ChatGPTRetrievalPluginClient\n    \n*   CassandraVectorStore\n    \n*   ChromaVectorStore\n    \n*   EpsillaVectorStore\n    \n*   DocArrayHnswVectorStore\n    \n*   DocArrayInMemoryVectorStore\n    \n*   LanceDBVectorStore\n    \n*   MetalVectorStore\n    \n*   MilvusVectorStore\n    \n*   MyScaleVectorStore\n    \n*   OpensearchVectorStore\n    \n*   PineconeVectorStore\n    \n*   QdrantVectorStore\n    \n*   RedisVectorStore\n    \n*   WeaviateVectorStore\n    \n\nA small example using Pinecone is below:\n\nimport pinecone\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\n\n\\# Creating a Pinecone index\napi\\_key \\= \"api\\_key\"\npinecone.init(api\\_key\\=api\\_key, environment\\=\"us-west1-gcp\")\npinecone.create\\_index(\n    \"quickstart\",\n    dimension\\=1536,\n    metric\\=\"euclidean\",\n    pod\\_type\\=\"p1\"\n)\nindex \\= pinecone.Index(\"quickstart\")\n\n\\# construct vector store\nvector\\_store \\= PineconeVectorStore(pinecone\\_index\\=index)\n\n\\# create storage context\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"./data\").load\\_data()\n\n\\# create index, which will insert documents/vectors to pinecone\nindex \\= VectorStoreIndex.from\\_documents(documents, storage\\_context\\=storage\\_context)\n\nIf you have an existing vector store with data already loaded in, you can connect to it and directly create a `VectorStoreIndex` as follows:\n\nindex \\= pinecone.Index(\"quickstart\")\nvector\\_store \\= PineconeVectorStore(pinecone\\_index\\=index)\nloaded\\_index \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store\\=vector\\_store)"
}