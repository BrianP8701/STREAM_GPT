{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/question_and_answer/terms_definitions_tutorial.html",
        "title": "A Guide to Extracting Terms and Definitions - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\nLlama Index has many use cases (semantic search, summarization, etc.) that are [well documented](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/use_cases.html). However, this doesn\u2019t mean we can\u2019t apply Llama Index to very specific use cases!\n\nIn this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using [Streamlit](https://streamlit.io/), we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.\n\nThis tutorial assumes you have Python3.9+ and the following packages installed:\n\n*   llama-index\n    \n*   streamlit\n    \n\nAt the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.\n\nThe final version of this tutorial can be found [here](https://github.com/logan-markewich/llama_index_starter_pack) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/llamaindex/llama_index_term_definition_demo).\n\n## Uploading Text[\uf0c1](#uploading-text \"Permalink to this heading\")\n\nStep one is giving users a way to upload documents. Let\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.\n\nimport streamlit as st\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\ndocument\\_text \\= st.text\\_area(\"Or enter raw text\")\nif st.button(\"Extract Terms and Definitions\") and document\\_text:\n    with st.spinner(\"Extracting...\"):\n        extracted\\_terms \\= document text  \\# this is a placeholder!\n    st.write(extracted\\_terms)\n\nSuper simple right! But you\u2019ll notice that the app doesn\u2019t do anything useful yet. To use llama\\_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what\u2019s best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\n\n## LLM Settings[\uf0c1](#llm-settings \"Permalink to this heading\")\n\nThis next step introduces some tabs to our app, to separate it into different panes that provide different features. Let\u2019s create a tab for LLM settings and for uploading text:\n\nimport os\nimport streamlit as st\n\nDEFAULT\\_TERM\\_STR \\= (\n    \"Make a list of terms and definitions that are defined in the context, \"\n    \"with one pair on each line. \"\n    \"If a term is missing it's definition, use your best judgment. \"\n    \"Write each line as as follows:\\\\nTerm: <term> Definition: <definition>\"\n)\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\nsetup\\_tab, upload\\_tab \\= st.tabs(\\[\"Setup\", \"Upload/Extract Terms\"\\])\n\nwith setup\\_tab:\n    st.subheader(\"LLM Setup\")\n    api\\_key \\= st.text\\_input(\"Enter your OpenAI API key here\", type\\=\"password\")\n    llm\\_name \\= st.selectbox('Which LLM?', \\[\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"\\])\n    model\\_temperature \\= st.slider(\"LLM Temperature\", min\\_value\\=0.0, max\\_value\\=1.0, step\\=0.1)\n    term\\_extract\\_str \\= st.text\\_area(\"The query to extract terms and definitions with.\", value\\=DEFAULT\\_TERM\\_STR)\n\nwith upload\\_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document\\_text \\= st.text\\_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document\\_text:\n        with st.spinner(\"Extracting...\"):\n            extracted\\_terms \\= document text  \\# this is a placeholder!\n        st.write(extracted\\_terms)\n\nNow our app has two tabs, which really helps with the organization. You\u2019ll also noticed I added a default prompt to extract terms \u2013 you can change this later once you try extracting some terms, it\u2019s just the prompt I arrived at after experimenting a bit.\n\nSpeaking of extracting terms, it\u2019s time to add some functions to do just that!\n\n## Extracting and Storing Terms[\uf0c1](#extracting-and-storing-terms \"Permalink to this heading\")\n\nNow that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!\n\nWe can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.\n\nfrom llama\\_index import Document, SummaryIndex, LLMPredictor, ServiceContext, load\\_index\\_from\\_storage\nfrom llama\\_index.llms import OpenAI\n\ndef get\\_llm(llm\\_name, model\\_temperature, api\\_key, max\\_tokens\\=256):\n    os.environ\\['OPENAI\\_API\\_KEY'\\] \\= api\\_key\n    return OpenAI(temperature\\=model\\_temperature, model\\=llm\\_name, max\\_tokens\\=max\\_tokens)\n\ndef extract\\_terms(documents, term\\_extract\\_str, llm\\_name, model\\_temperature, api\\_key):\n    llm \\= get\\_llm(llm\\_name, model\\_temperature, api\\_key, max\\_tokens\\=1024)\n\n    service\\_context \\= ServiceContext.from\\_defaults(llm\\=llm,\n                                                   chunk\\_size\\=1024)\n\n    temp\\_index \\= SummaryIndex.from\\_documents(documents, service\\_context\\=service\\_context)\n    query\\_engine \\= temp\\_index.as\\_query\\_engine(response\\_mode\\=\"tree\\_summarize\")\n    terms\\_definitions \\= str(query\\_engine.query(term\\_extract\\_str))\n    terms\\_definitions \\= \\[x for x in terms\\_definitions.split(\"\\\\n\") if x and 'Term:' in x and 'Definition:' in x\\]\n    \\# parse the text into a dict\n    terms\\_to\\_definition \\= {x.split(\"Definition:\")\\[0\\].split(\"Term:\")\\[\\-1\\].strip(): x.split(\"Definition:\")\\[\\-1\\].strip() for x in terms\\_definitions}\n    return terms\\_to\\_definition\n\nNow, using the new functions, we can finally extract our terms!\n\n...\nwith upload\\_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document\\_text \\= st.text\\_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document\\_text:\n        with st.spinner(\"Extracting...\"):\n            extracted\\_terms \\= extract\\_terms(\\[Document(text\\=document\\_text)\\],\n                                            term\\_extract\\_str, llm\\_name,\n                                            model\\_temperature, api\\_key)\n        st.write(extracted\\_terms)\n\nThere\u2019s a lot going on now, let\u2019s take a moment to go over what is happening.\n\n`get_llm()` is instantiating the LLM based on the user configuration from the setup tab. Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).\n\n`extract_terms()` is where all the good stuff happens. First, we call `get_llm()` with `max_tokens=1024`, since we don\u2019t want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set). Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output. When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.\n\nNext, we create a temporary summary index and pass in our service context. A summary index will read every single piece of text in our index, which is perfect for extracting terms. Finally, we use our pre-defined query text to extract terms, using `response_mode=\"tree_summarize`. This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children. Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.\n\nLastly, we do some minor post processing. We assume the model followed instructions and put a term/definition pair on each line. If a line is missing the `Term:` or `Definition:` labels, we skip it. Then, we convert this to a dictionary for easy storage!\n\n## Dry Run Test[\uf0c1](#dry-run-test \"Permalink to this heading\")\n\nWell, actually I hope you\u2019ve been testing as we went. But now, let\u2019s try one complete test.\n\n1.  Refresh the app\n    \n2.  Enter your LLM settings\n    \n3.  Head over to the query tab\n    \n4.  Ask the following: `What is a bunnyhug?`\n    \n5.  The app should give some nonsense response. If you didn\u2019t know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies!\n    \n6.  Let\u2019s add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.`\n    \n7.  Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it!\n    \n8.  If we open the terms tab, the term and definition we just extracted should be displayed\n    \n9.  Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\n    \n\n## Improvement #1 - Create a Starting Index[\uf0c1](#improvement-1-create-a-starting-index \"Permalink to this heading\")\n\nWith our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app\u2019s query capabilities? We can do just that! First, let\u2019s make a small change to our app so that we save the index to disk after every upload:\n\ndef insert\\_terms(terms\\_to\\_definition):\n    for term, definition in terms\\_to\\_definition.items():\n        doc \\= Document(text\\=f\"Term: {term}\\\\nDefinition: {definition}\")\n        st.session\\_state\\['llama\\_index'\\].insert(doc)\n    \\# TEMPORARY - save to disk\n    st.session\\_state\\['llama\\_index'\\].storage\\_context.persist()\n\nNow, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text [here](https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/data/nyc_text.txt).\n\nIf you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.\n\nAfter inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:\n\n@st.cache\\_resource\ndef initialize\\_index(llm\\_name, model\\_temperature, api\\_key):\n    \"\"\"Load the Index object.\"\"\"\n    llm \\= get\\_llm(llm\\_name, model\\_temperature, api\\_key)\n\n    service\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n    index \\= load\\_index\\_from\\_storage(service\\_context\\=service\\_context)\n\n    return index\n\nDid you remember to save that giant list of extracted terms in a notepad? Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:\n\n...\nif \"all\\_terms\" not in st.session\\_state:\n    st.session\\_state\\[\"all\\_terms\"\\] \\= DEFAULT\\_TERMS\n...\n\nRepeat the above anywhere where we were previously resetting the `all_terms` values.\n\n## Improvement #2 - (Refining) Better Prompts[\uf0c1](#improvement-2-refining-better-prompts \"Permalink to this heading\")\n\nIf you play around with the app a bit now, you might notice that it stopped following our prompt! Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.\n\nThis is due to the concept of \u201crefining\u201d answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.\n\nSo, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let\u2019s create those now, using the [default prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py) and [chat specific prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py) as a guide. Using a new file `constants.py`, let\u2019s create some new query templates:\n\nfrom llama\\_index.prompts import PromptTemplate, SelectorPromptTemplate, ChatPromptTemplate\nfrom llama\\_index.prompts.utils import is\\_chat\\_model\nfrom llama\\_index.llms.base import ChatMessage, MessageRole\n\n\\# Text QA templates\nDEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL \\= (\n    \"Context information is below. \\\\n\"\n    \"---------------------\\\\n\"\n    \"{context\\_str}\"\n    \"\\\\n\\---------------------\\\\n\"\n    \"Given the context information answer the following question \"\n    \"(if you don't know the answer, use the best of your knowledge): {query\\_str}\\\\n\"\n)\nTEXT\\_QA\\_TEMPLATE \\= PromptTemplate(DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL)\n\n\\# Refine templates\nDEFAULT\\_REFINE\\_PROMPT\\_TMPL \\= (\n    \"The original question is as follows: {query\\_str}\\\\n\"\n    \"We have provided an existing answer: {existing\\_answer}\\\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\\\n\"\n    \"------------\\\\n\"\n    \"{context\\_msg}\\\\n\"\n    \"------------\\\\n\"\n    \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"\n)\nDEFAULT\\_REFINE\\_PROMPT \\= PromptTemplate(DEFAULT\\_REFINE\\_PROMPT\\_TMPL)\n\nCHAT\\_REFINE\\_PROMPT\\_TMPL\\_MSGS \\= \\[\n    ChatMessage(content\\=\"{query\\_str}\", role\\=MessageRole.USER),\n    ChatMessage(content\\=\"{existing\\_answer}\", role\\=MessageRole.ASSISTANT),\n    ChatMessage(\n        content\\=\"We have the opportunity to refine the above answer \"\n        \"(only if needed) with some more context below.\\\\n\"\n        \"------------\\\\n\"\n        \"{context\\_msg}\\\\n\"\n        \"------------\\\\n\"\n        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n        \"If you can't improve the existing answer, just repeat it again.\",\n        role\\=MessageRole.USER,\n    ),\n\\]\n\nCHAT\\_REFINE\\_PROMPT \\= ChatPromptTemplate(CHAT\\_REFINE\\_PROMPT\\_TMPL\\_MSGS)\n\n\\# refine prompt selector\nREFINE\\_TEMPLATE \\= SelectorPromptTemplate(\n    default\\_template\\=DEFAULT\\_REFINE\\_PROMPT,\n    conditionals\\=\\[(is\\_chat\\_model, CHAT\\_REFINE\\_PROMPT)\\],\n)\n\nThat seems like a lot of code, but it\u2019s not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.\n\nAnother thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \u201chuman\u201d message.\n\nSo, now we can import these prompts into our app and use them during the query.\n\nfrom constants import REFINE\\_TEMPLATE, TEXT\\_QA\\_TEMPLATE\n...\n    if \"llama\\_index\" in st.session\\_state:\n        query\\_text \\= st.text\\_input(\"Ask about a term or definition:\")\n        if query\\_text:\n            query\\_text \\= query\\_text  \\# Notice we removed the old instructions\n            with st.spinner(\"Generating answer...\"):\n                response \\= st.session\\_state\\[\"llama\\_index\"\\].query(\n                    query\\_text, similarity\\_top\\_k\\=5, response\\_mode\\=\"compact\",\n                    text\\_qa\\_template\\=TEXT\\_QA\\_TEMPLATE, refine\\_template\\=REFINE\\_TEMPLATE\n                )\n            st.markdown(str(response))\n...\n\nIf you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!\n\n## Improvement #3 - Image Support[\uf0c1](#improvement-3-image-support \"Permalink to this heading\")\n\nLlama index also supports images! Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text. We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.\n\nIf you get an import error about PIL, install it using `pip install Pillow` first.\n\nfrom PIL import Image\nfrom llama\\_index.readers.file.base import DEFAULT\\_FILE\\_EXTRACTOR, ImageParser\n\n@st.cache\\_resource\ndef get\\_file\\_extractor():\n    image\\_parser \\= ImageParser(keep\\_image\\=True, parse\\_text\\=True)\n    file\\_extractor \\= DEFAULT\\_FILE\\_EXTRACTOR\n    file\\_extractor.update(\n        {\n            \".jpg\": image\\_parser,\n            \".png\": image\\_parser,\n            \".jpeg\": image\\_parser,\n        }\n    )\n\n    return file\\_extractor\n\nfile\\_extractor \\= get\\_file\\_extractor()\n...\nwith upload\\_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\", key\\=\"init\\_index\\_1\"):\n        st.session\\_state\\[\"llama\\_index\"\\] \\= initialize\\_index(\n            llm\\_name, model\\_temperature, api\\_key\n        )\n        st.session\\_state\\[\"all\\_terms\"\\] \\= DEFAULT\\_TERMS\n\n    if \"llama\\_index\" in st.session\\_state:\n        st.markdown(\n            \"Either upload an image/screenshot of a document, or enter the text manually.\"\n        )\n        uploaded\\_file \\= st.file\\_uploader(\n            \"Upload an image/screenshot of a document:\", type\\=\\[\"png\", \"jpg\", \"jpeg\"\\]\n        )\n        document\\_text \\= st.text\\_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (\n            uploaded\\_file or document\\_text\n        ):\n            st.session\\_state\\[\"terms\"\\] \\= {}\n            terms\\_docs \\= {}\n            with st.spinner(\"Extracting (images may be slow)...\"):\n                if document\\_text:\n                    terms\\_docs.update(\n                        extract\\_terms(\n                            \\[Document(text\\=document\\_text)\\],\n                            term\\_extract\\_str,\n                            llm\\_name,\n                            model\\_temperature,\n                            api\\_key,\n                        )\n                    )\n                if uploaded\\_file:\n                    Image.open(uploaded\\_file).convert(\"RGB\").save(\"temp.png\")\n                    img\\_reader \\= SimpleDirectoryReader(\n                        input\\_files\\=\\[\"temp.png\"\\], file\\_extractor\\=file\\_extractor\n                    )\n                    img\\_docs \\= img\\_reader.load\\_data()\n                    os.remove(\"temp.png\")\n                    terms\\_docs.update(\n                        extract\\_terms(\n                            img\\_docs,\n                            term\\_extract\\_str,\n                            llm\\_name,\n                            model\\_temperature,\n                            api\\_key,\n                        )\n                    )\n            st.session\\_state\\[\"terms\"\\].update(terms\\_docs)\n\n        if \"terms\" in st.session\\_state and st.session\\_state\\[\"terms\"\\]:\n            st.markdown(\"Extracted terms\")\n            st.json(st.session\\_state\\[\"terms\"\\])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert\\_terms(st.session\\_state\\[\"terms\"\\])\n                st.session\\_state\\[\"all\\_terms\"\\].update(st.session\\_state\\[\"terms\"\\])\n                st.session\\_state\\[\"terms\"\\] \\= {}\n                st.experimental\\_rerun()\n\nHere, we added the option to upload a file using Streamlit. Then the image is opened and saved to disk (this seems hacky but it keeps things simple). Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.\n\nNow that we have the documents, we can call `extract_terms()` the same as before.\n\n## Conclusion/TLDR[\uf0c1](#conclusion-tldr \"Permalink to this heading\")\n\nIn this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\n\n*   Using different indexes for different use cases (List vs. Vector index)\n    \n*   Storing global state values with Streamlit\u2019s `session_state` concept\n    \n*   Customizing internal prompts with Llama Index\n    \n*   Reading text from images with Llama Index\n    \n\nThe final version of this tutorial can be found [here](https://github.com/logan-markewich/llama_index_starter_pack) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/llamaindex/llama_index_term_definition_demo)."
}