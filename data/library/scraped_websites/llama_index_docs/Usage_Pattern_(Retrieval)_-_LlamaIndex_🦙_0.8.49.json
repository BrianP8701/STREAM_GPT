{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/evaluation/usage_pattern_retrieval.html",
        "title": "Usage Pattern (Retrieval) - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Usage Pattern (Retrieval)[\uf0c1](#usage-pattern-retrieval \"Permalink to this heading\")\n\n## Using `RetrieverEvaluator`[\uf0c1](#using-retrieverevaluator \"Permalink to this heading\")\n\nThis runs evaluation over a single query + ground-truth document set given a retriever.\n\nThe standard practice is to specify a set of valid metrics with `from_metrics`.\n\nfrom llama\\_index.evaluation import RetrieverEvaluator\n\n\\# define retriever somewhere (e.g. from index)\n\\# retriever = index.as\\_retriever(similarity\\_top\\_k=2)\nretriever \\= ...\n\nretriever\\_evaluator \\= RetrieverEvaluator.from\\_metric\\_names(\n    \\[\"mrr\", \"hit\\_rate\"\\], retriever\\=retriever\n)\n\nretriever\\_evaluator.evaluate(\n    query\\=\"query\",\n    expected\\_ids\\=\\[\"node\\_id1\", \"node\\_id2\"\\]\n)\n\n## Building an Evaluation Dataset[\uf0c1](#building-an-evaluation-dataset \"Permalink to this heading\")\n\nYou can manually curate a retrieval evaluation dataset of questions + node id\u2019s. We also offer synthetic dataset generation over an existing text corpus with our `generate_question_context_pairs` function:\n\nfrom llama\\_index.evaluation import generate\\_question\\_context\\_pairs\n\nqa\\_dataset \\= generate\\_question\\_context\\_pairs(\n    nodes,\n    llm\\=llm,\n    num\\_questions\\_per\\_chunk\\=2\n)\n\nThe returned result is a `EmbeddingQAFinetuneDataset` object (containing `queries`, `relevant_docs`, and `corpus`).\n\n### Plugging it into `RetrieverEvaluator`[\uf0c1](#plugging-it-into-retrieverevaluator \"Permalink to this heading\")\n\nWe offer a convenience function to run a `RetrieverEvaluator` over a dataset in batch mode.\n\neval\\_results \\= await retriever\\_evaluator.aevaluate\\_dataset(qa\\_dataset)\n\nThis should run much faster than you trying to call `.evaluate` on each query separately."
}