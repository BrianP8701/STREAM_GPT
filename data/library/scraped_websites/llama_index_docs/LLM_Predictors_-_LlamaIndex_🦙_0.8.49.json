{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/llm_predictor.html",
        "title": "LLM Predictors - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## LLM Predictors[\uf0c1](#module-llama_index.llm_predictor \"Permalink to this heading\")\n\nInit params.\n\n_pydantic model_ llama\\_index.llm\\_predictor.LLMPredictor[\uf0c1](#llama_index.llm_predictor.LLMPredictor \"Permalink to this definition\")\n\nLLM predictor class.\n\nA lightweight wrapper on top of LLMs that handles: - conversion of prompts to the string input format expected by LLMs - logging of prompts and responses to a callback manager\n\nNOTE: Mostly keeping around for legacy reasons. A potential future path is to deprecate this class and move all functionality into the LLM class.\n\nShow JSON schema\n\n{\n   \"title\": \"LLMPredictor\",\n   \"description\": \"LLM predictor class.\\\\n\\\\nA lightweight wrapper on top of LLMs that handles:\\\\n- conversion of prompts to the string input format expected by LLMs\\\\n- logging of prompts and responses to a callback manager\\\\n\\\\nNOTE: Mostly keeping around for legacy reasons. A potential future path is to\\\\ndeprecate this class and move all functionality into the LLM class.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"system\\_prompt\": {\n         \"title\": \"System Prompt\",\n         \"type\": \"string\"\n      },\n      \"query\\_wrapper\\_prompt\": {\n         \"title\": \"Query Wrapper Prompt\"\n      }\n   }\n}\n\nConfig\n\n*   **arbitrary\\_types\\_allowed**: _bool = True_\n    \n\nFields\n\n*   `query_wrapper_prompt (Optional[llama_index.prompts.base.BasePromptTemplate])`\n    \n*   `system_prompt (Optional[str])`\n    \n\n_field_ query\\_wrapper\\_prompt_: Optional\\[[BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")\\]_ _\\= None_[\uf0c1](#llama_index.llm_predictor.LLMPredictor.query_wrapper_prompt \"Permalink to this definition\")\n\n_field_ system\\_prompt_: Optional\\[str\\]_ _\\= None_[\uf0c1](#llama_index.llm_predictor.LLMPredictor.system_prompt \"Permalink to this definition\")\n\n_async_ apredict(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[BaseModel\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.LLMPredictor.apredict \"Permalink to this definition\")\n\nAsync predict.\n\n_async_ astream(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[BaseModel\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 AsyncGenerator\\[str, None\\][\uf0c1](#llama_index.llm_predictor.LLMPredictor.astream \"Permalink to this definition\")\n\nAsync stream.\n\n_classmethod_ class\\_name() \u2192 str[\uf0c1](#llama_index.llm_predictor.LLMPredictor.class_name \"Permalink to this definition\")\n\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\n_classmethod_ construct(_\\_fields\\_set: Optional\\[SetStr\\] \\= None_, _\\*\\*values: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.construct \"Permalink to this definition\")\n\nCreates a new model setting \\_\\_dict\\_\\_ and \\_\\_fields\\_set\\_\\_ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _update: Optional\\[DictStrAny\\] \\= None_, _deep: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.copy \"Permalink to this definition\")\n\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nParameters\n\n*   **include** \u2013 fields to include in new model\n    \n*   **exclude** \u2013 fields to exclude from new model, as with values this takes precedence over include\n    \n*   **update** \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n    \n*   **deep** \u2013 set to True to make a deep copy of the model\n    \n\nReturns\n\nnew model instance\n\ndict(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_) \u2192 DictStrAny[\uf0c1](#llama_index.llm_predictor.LLMPredictor.dict \"Permalink to this definition\")\n\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n_classmethod_ from\\_dict(_data: Dict\\[str, Any\\]_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.llm_predictor.LLMPredictor.from_dict \"Permalink to this definition\")\n\n_classmethod_ from\\_json(_data\\_str: str_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.llm_predictor.LLMPredictor.from_json \"Permalink to this definition\")\n\n_classmethod_ from\\_orm(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.from_orm \"Permalink to this definition\")\n\njson(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_, _encoder: Optional\\[Callable\\[\\[Any\\], Any\\]\\] \\= None_, _models\\_as\\_dict: bool \\= True_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.llm_predictor.LLMPredictor.json \"Permalink to this definition\")\n\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\n_classmethod_ parse\\_file(_path: Union\\[str, Path\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.parse_file \"Permalink to this definition\")\n\n_classmethod_ parse\\_obj(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.parse_obj \"Permalink to this definition\")\n\n_classmethod_ parse\\_raw(_b: Union\\[str, bytes\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.parse_raw \"Permalink to this definition\")\n\npredict(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[BaseModel\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.LLMPredictor.predict \"Permalink to this definition\")\n\nPredict.\n\n_classmethod_ schema(_by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_) \u2192 DictStrAny[\uf0c1](#llama_index.llm_predictor.LLMPredictor.schema \"Permalink to this definition\")\n\n_classmethod_ schema\\_json(_\\*_, _by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.llm_predictor.LLMPredictor.schema_json \"Permalink to this definition\")\n\nstream(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[BaseModel\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 Generator\\[str, None, None\\][\uf0c1](#llama_index.llm_predictor.LLMPredictor.stream \"Permalink to this definition\")\n\nStream.\n\nto\\_dict(_\\*\\*kwargs: Any_) \u2192 Dict\\[str, Any\\][\uf0c1](#llama_index.llm_predictor.LLMPredictor.to_dict \"Permalink to this definition\")\n\nto\\_json(_\\*\\*kwargs: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.LLMPredictor.to_json \"Permalink to this definition\")\n\n_classmethod_ update\\_forward\\_refs(_\\*\\*localns: Any_) \u2192 None[\uf0c1](#llama_index.llm_predictor.LLMPredictor.update_forward_refs \"Permalink to this definition\")\n\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\n\n_classmethod_ validate(_value: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.LLMPredictor.validate \"Permalink to this definition\")\n\n_property_ callback\\_manager_: [CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")_[\uf0c1](#llama_index.llm_predictor.LLMPredictor.callback_manager \"Permalink to this definition\")\n\nGet callback manager.\n\n_property_ llm_: [LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\")_[\uf0c1](#llama_index.llm_predictor.LLMPredictor.llm \"Permalink to this definition\")\n\nGet LLM.\n\n_property_ metadata_: [LLMMetadata](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLMMetadata \"llama_index.llms.base.LLMMetadata\")_[\uf0c1](#llama_index.llm_predictor.LLMPredictor.metadata \"Permalink to this definition\")\n\nGet LLM metadata.\n\n_pydantic model_ llama\\_index.llm\\_predictor.StructuredLLMPredictor[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor \"Permalink to this definition\")\n\nStructured LLM predictor class.\n\nParameters\n\n**llm\\_predictor** (_BaseLLMPredictor_) \u2013 LLM Predictor to use.\n\nShow JSON schema\n\n{\n   \"title\": \"StructuredLLMPredictor\",\n   \"description\": \"Structured LLM predictor class.\\\\n\\\\nArgs:\\\\n    llm\\_predictor (BaseLLMPredictor): LLM Predictor to use.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"system\\_prompt\": {\n         \"title\": \"System Prompt\",\n         \"type\": \"string\"\n      },\n      \"query\\_wrapper\\_prompt\": {\n         \"title\": \"Query Wrapper Prompt\"\n      }\n   }\n}\n\nConfig\n\n*   **arbitrary\\_types\\_allowed**: _bool = True_\n    \n\nFields\n\n*   `query_wrapper_prompt (Optional[llama_index.prompts.base.BasePromptTemplate])`\n    \n*   `system_prompt (Optional[str])`\n    \n\n_field_ query\\_wrapper\\_prompt_: Optional\\[[BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")\\]_ _\\= None_[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.query_wrapper_prompt \"Permalink to this definition\")\n\n_field_ system\\_prompt_: Optional\\[str\\]_ _\\= None_[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.system_prompt \"Permalink to this definition\")\n\n_async_ apredict(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[Any\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.apredict \"Permalink to this definition\")\n\nAsync predict the answer to a query.\n\nParameters\n\n**prompt** ([_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")) \u2013 BasePromptTemplate to use for prediction.\n\nReturns\n\nTuple of the predicted answer and the formatted prompt.\n\nReturn type\n\nTuple\\[str, str\\]\n\n_async_ astream(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[BaseModel\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 AsyncGenerator\\[str, None\\][\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.astream \"Permalink to this definition\")\n\nAsync stream.\n\n_classmethod_ class\\_name() \u2192 str[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.class_name \"Permalink to this definition\")\n\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\n_classmethod_ construct(_\\_fields\\_set: Optional\\[SetStr\\] \\= None_, _\\*\\*values: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.construct \"Permalink to this definition\")\n\nCreates a new model setting \\_\\_dict\\_\\_ and \\_\\_fields\\_set\\_\\_ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _update: Optional\\[DictStrAny\\] \\= None_, _deep: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.copy \"Permalink to this definition\")\n\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nParameters\n\n*   **include** \u2013 fields to include in new model\n    \n*   **exclude** \u2013 fields to exclude from new model, as with values this takes precedence over include\n    \n*   **update** \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n    \n*   **deep** \u2013 set to True to make a deep copy of the model\n    \n\nReturns\n\nnew model instance\n\ndict(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_) \u2192 DictStrAny[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.dict \"Permalink to this definition\")\n\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n_classmethod_ from\\_dict(_data: Dict\\[str, Any\\]_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.from_dict \"Permalink to this definition\")\n\n_classmethod_ from\\_json(_data\\_str: str_, _\\*\\*kwargs: Any_) \u2192 Self[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.from_json \"Permalink to this definition\")\n\n_classmethod_ from\\_orm(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.from_orm \"Permalink to this definition\")\n\njson(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_, _encoder: Optional\\[Callable\\[\\[Any\\], Any\\]\\] \\= None_, _models\\_as\\_dict: bool \\= True_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.json \"Permalink to this definition\")\n\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\n_classmethod_ parse\\_file(_path: Union\\[str, Path\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.parse_file \"Permalink to this definition\")\n\n_classmethod_ parse\\_obj(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.parse_obj \"Permalink to this definition\")\n\n_classmethod_ parse\\_raw(_b: Union\\[str, bytes\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.parse_raw \"Permalink to this definition\")\n\npredict(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[Any\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.predict \"Permalink to this definition\")\n\nPredict the answer to a query.\n\nParameters\n\n**prompt** ([_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")) \u2013 BasePromptTemplate to use for prediction.\n\nReturns\n\nTuple of the predicted answer and the formatted prompt.\n\nReturn type\n\nTuple\\[str, str\\]\n\n_classmethod_ schema(_by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_) \u2192 DictStrAny[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.schema \"Permalink to this definition\")\n\n_classmethod_ schema\\_json(_\\*_, _by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.schema_json \"Permalink to this definition\")\n\nstream(_prompt: [BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")_, _output\\_cls: Optional\\[Any\\] \\= None_, _\\*\\*prompt\\_args: Any_) \u2192 Generator\\[str, None, None\\][\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.stream \"Permalink to this definition\")\n\nStream the answer to a query.\n\nNOTE: this is a beta feature. Will try to build or use better abstractions about response handling.\n\nParameters\n\n**prompt** ([_BasePromptTemplate_](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")) \u2013 BasePromptTemplate to use for prediction.\n\nReturns\n\nThe predicted answer.\n\nReturn type\n\nstr\n\nto\\_dict(_\\*\\*kwargs: Any_) \u2192 Dict\\[str, Any\\][\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.to_dict \"Permalink to this definition\")\n\nto\\_json(_\\*\\*kwargs: Any_) \u2192 str[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.to_json \"Permalink to this definition\")\n\n_classmethod_ update\\_forward\\_refs(_\\*\\*localns: Any_) \u2192 None[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.update_forward_refs \"Permalink to this definition\")\n\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\n\n_classmethod_ validate(_value: Any_) \u2192 Model[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.validate \"Permalink to this definition\")\n\n_property_ callback\\_manager_: [CallbackManager](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.CallbackManager \"llama_index.callbacks.base.CallbackManager\")_[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.callback_manager \"Permalink to this definition\")\n\nGet callback manager.\n\n_property_ llm_: [LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\")_[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.llm \"Permalink to this definition\")\n\nGet LLM.\n\n_property_ metadata_: [LLMMetadata](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLMMetadata \"llama_index.llms.base.LLMMetadata\")_[\uf0c1](#llama_index.llm_predictor.StructuredLLMPredictor.metadata \"Permalink to this definition\")\n\nGet LLM metadata."
}