{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/index_guide.html",
        "title": "How Each Index Works - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## How Each Index Works[\uf0c1](#how-each-index-works \"Permalink to this heading\")\n\nThis guide describes how each index works with diagrams.\n\nSome terminology:\n\n*   **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n    \n*   **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to [specify different response modes](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/usage_pattern.html#setting-response-mode) here.\n    \n\n## Summary Index (formerly List Index)[\uf0c1](#summary-index-formerly-list-index \"Permalink to this heading\")\n\nThe summary index simply stores Nodes as a sequential chain.\n\n![](https://docs.llamaindex.ai/en/stable/_images/list.png)\n\n### Querying[\uf0c1](#querying \"Permalink to this heading\")\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into our Response Synthesis module.\n\n![](https://docs.llamaindex.ai/en/stable/_images/list_query.png)\n\nThe summary index does offer numerous ways of querying a summary index, from an embedding-based query which will fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n![](https://docs.llamaindex.ai/en/stable/_images/list_filter_query.png)\n\n## Vector Store Index[\uf0c1](#vector-store-index \"Permalink to this heading\")\n\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores.html#vector-store-index).\n\n![](https://docs.llamaindex.ai/en/stable/_images/vector_store.png)\n\n### Querying[\uf0c1](#id1 \"Permalink to this heading\")\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing those into our Response Synthesis module.\n\n![](https://docs.llamaindex.ai/en/stable/_images/vector_store_query.png)\n\n## Tree Index[\uf0c1](#tree-index \"Permalink to this heading\")\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n![](https://docs.llamaindex.ai/en/stable/_images/tree.png)\n\n### Querying[\uf0c1](#id2 \"Permalink to this heading\")\n\nQuerying a tree index involves traversing from root nodes down to leaf nodes. By default, (`child_branch_factor=1`), a query chooses one child node given a parent node. If `child_branch_factor=2`, a query chooses two child nodes per level.\n\n![](https://docs.llamaindex.ai/en/stable/_images/tree_query.png)\n\n## Keyword Table Index[\uf0c1](#keyword-table-index \"Permalink to this heading\")\n\nThe keyword table index extracts keywords from each Node and builds a mapping from each keyword to the corresponding Nodes of that keyword.\n\n![](https://docs.llamaindex.ai/en/stable/_images/keyword.png)\n\n### Querying[\uf0c1](#id3 \"Permalink to this heading\")\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted Node keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our Response Synthesis module.\n\n![](https://docs.llamaindex.ai/en/stable/_images/keyword_query.png)"
}