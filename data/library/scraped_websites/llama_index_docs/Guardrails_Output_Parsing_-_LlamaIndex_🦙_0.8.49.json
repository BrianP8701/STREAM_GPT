{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/output_parsing/GuardrailsDemo.html",
        "title": "Guardrails Output Parsing - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Guardrails Output Parsing[\uf0c1](#guardrails-output-parsing \"Permalink to this heading\")\n\n## Load documents, build the VectorStoreIndex[\uf0c1](#load-documents-build-the-vectorstoreindex \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\nimport openai\n\nopenai.api\\_key \\= \"<YOUR\\_OPENAI\\_API\\_KEY>\"\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\n    \"../../../examples/paul\\_graham\\_essay/data/\"\n).load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents, chunk\\_size\\=512)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_documents\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_documents\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_documents\\] Total embedding token usage: 18579 tokens\n> \\[build\\_index\\_from\\_documents\\] Total embedding token usage: 18579 tokens\n\n## Define Query + Guardrails Spec[\uf0c1](#define-query-guardrails-spec \"Permalink to this heading\")\n\nfrom llama\\_index.output\\_parsers import GuardrailsOutputParser\nfrom llama\\_index.llm\\_predictor import StructuredLLMPredictor\n\nllm\\_predictor \\= StructuredLLMPredictor()\n\n**Define custom QA and Refine Prompts**\n\nfrom llama\\_index.prompts import PromptTemplate\nfrom llama\\_index.prompts.default\\_prompts import (\n    DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL,\n    DEFAULT\\_REFINE\\_PROMPT\\_TMPL,\n)\n\n**Define Guardrails Spec**\n\n\\# You can either define a RailSpec and initialise a Guard object from\\_rail\\_string()\n\\# OR define Pydantic classes and initialise a Guard object from\\_pydantic()\n\\# For more info: https://docs.guardrailsai.com/defining\\_guards/pydantic/\n\\# Guardrails recommends Pydantic\n\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport guardrails as gd\n\nclass Point(BaseModel):\n    \\# In all the fields below, you can define validators as well\n    \\# Left out for brevity\n    explanation: str \\= Field()\n    explanation2: str \\= Field()\n    explanation3: str \\= Field()\n\nclass BulletPoints(BaseModel):\n    points: List\\[Point\\] \\= Field(\n        description\\=\"Bullet points regarding events in the author's life.\"\n    )\n\n\\# Define the prompt\nprompt \\= \"\"\"\nQuery string here.\n\n${gr.xml\\_prefix\\_prompt}\n\n${output\\_schema}\n\n${gr.json\\_suffix\\_prompt\\_v2\\_wo\\_none}\n\"\"\"\n\n\\# Create a guard object\nguard \\= gd.Guard.from\\_pydantic(output\\_class\\=BulletPoints, prompt\\=prompt)\n\n\\# Create output parse object\noutput\\_parser \\= GuardrailsOutputParser(guard, llm\\=llm\\_predictor.llm)\n\n\\# NOTE: we use the same output parser for both prompts, though you can choose to use different parsers\n\\# NOTE: here we add formatting instructions to the prompts.\n\nfmt\\_qa\\_tmpl \\= output\\_parser.format(DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL)\nfmt\\_refine\\_tmpl \\= output\\_parser.format(DEFAULT\\_REFINE\\_PROMPT\\_TMPL)\n\nqa\\_prompt \\= PromptTemplate(fmt\\_qa\\_tmpl, output\\_parser\\=output\\_parser)\nrefine\\_prompt \\= PromptTemplate(fmt\\_refine\\_tmpl, output\\_parser\\=output\\_parser)\n\n\\# take a look at the new QA template!\nprint(fmt\\_qa\\_tmpl)\n\nContext information is below.\n---------------------\n{context\\_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query\\_str}\nAnswer: \n\n\nGiven below is XML that describes the information to extract from this document and the tags to extract it into.\n\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\"/>\n            <string name=\"explanation2\"/>\n            <string name=\"explanation3\"/>\n        </object>\n    </list>\n</output>\n\n\n\nONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    text\\_qa\\_template\\=qa\\_prompt,\n    refine\\_template\\=refine\\_prompt,\n    llm\\_predictor\\=llm\\_predictor,\n)\nresponse \\= query\\_engine.query(\n    \"What are the three items the author did growing up?\",\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 754 tokens\n> \\[query\\] Total LLM token usage: 754 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 11 tokens\n> \\[query\\] Total embedding token usage: 11 tokens\n\n{\n  \"output\": {\n    \"list\": {\n      \"name\": \"points\",\n      \"description\": \"Bullet points regarding events in the author's life.\",\n      \"object\": {\n        \"string\": \\[\n          {\n            \"name\": \"explanation\",\n            \"content\": \"Writing short stories\"\n          },\n          {\n            \"name\": \"explanation2\",\n            \"content\": \"Programming on the IBM 1401\"\n          },\n          {\n            \"name\": \"explanation3\",\n            \"content\": \"Building a microcomputer\"\n          }\n        \\]\n      }\n    }\n  }\n}"
}