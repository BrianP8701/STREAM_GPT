{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/query_modules/structured_outputs/output_parser.html",
        "title": "Output Parsing - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Output Parsing[\uf0c1](#output-parsing \"Permalink to this heading\")\n\nLlamaIndex supports integrations with output parsing modules offered by other frameworks. These output parsing modules can be used in the following ways:\n\n*   To provide formatting instructions for any prompt / query (through `output_parser.format`)\n    \n*   To provide \u201cparsing\u201d for LLM outputs (through `output_parser.parse`)\n    \n\n## Guardrails[\uf0c1](#guardrails \"Permalink to this heading\")\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama\\_index.output\\_parsers import GuardrailsOutputParser\nfrom llama\\_index.llm\\_predictor import StructuredLLMPredictor\nfrom llama\\_index.prompts import PromptTemplate\nfrom llama\\_index.prompts.default\\_prompts import DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL, DEFAULT\\_REFINE\\_PROMPT\\_TMPL\n\n\\# load documents, build index\ndocuments \\= SimpleDirectoryReader('../paul\\_graham\\_essay/data').load\\_data()\nindex \\= VectorStoreIndex(documents, chunk\\_size\\=512)\nllm\\_predictor \\= StructuredLLMPredictor()\n\n\\# specify StructuredLLMPredictor\n\\# this is a special LLMPredictor that allows for structured outputs\n\n\\# define query / output spec\nrail\\_spec \\= (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml\\_prefix\\_prompt\n\n{output\\_schema}\n\n@json\\_suffix\\_prompt\\_v2\\_wo\\_none\n</prompt>\n</rail>\n\"\"\")\n\n\\# define output parser\noutput\\_parser \\= GuardrailsOutputParser.from\\_rail\\_string(rail\\_spec, llm\\=llm\\_predictor.llm)\n\n\\# format each prompt with output parser instructions\nfmt\\_qa\\_tmpl \\= output\\_parser.format(DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL)\nfmt\\_refine\\_tmpl \\= output\\_parser.format(DEFAULT\\_REFINE\\_PROMPT\\_TMPL)\n\nqa\\_prompt \\= PromptTemplate(fmt\\_qa\\_tmpl, output\\_parser\\=output\\_parser)\nrefine\\_prompt \\= PromptTemplate(fmt\\_refine\\_tmpl, output\\_parser\\=output\\_parser)\n\n\\# obtain a structured response\nquery\\_engine \\= index.as\\_query\\_engine(\n    service\\_context\\=ServiceContext.from\\_defaults(\n        llm\\_predictor\\=llm\\_predictor\n    ),\n    text\\_qa\\_template\\=qa\\_prompt,\n    refine\\_template\\=refine\\_prompt,\n)\nresponse \\= query\\_engine.query(\n    \"What are the three items the author did growing up?\",\n)\nprint(response)\n\nOutput:\n\n{'points': \\[{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}\\]}\n\n## Langchain[\uf0c1](#langchain \"Permalink to this heading\")\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama\\_index.output\\_parsers import LangchainOutputParser\nfrom llama\\_index.llm\\_predictor import StructuredLLMPredictor\nfrom llama\\_index.prompts import PromptTemplate\nfrom llama\\_index.prompts.default\\_prompts import DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL, DEFAULT\\_REFINE\\_PROMPT\\_TMPL\nfrom langchain.output\\_parsers import StructuredOutputParser, ResponseSchema\n\n\\# load documents, build index\ndocuments \\= SimpleDirectoryReader('../paul\\_graham\\_essay/data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nllm\\_predictor \\= StructuredLLMPredictor()\n\n\\# define output schema\nresponse\\_schemas \\= \\[\n    ResponseSchema(name\\=\"Education\", description\\=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name\\=\"Work\", description\\=\"Describes the author's work experience/background.\")\n\\]\n\n\\# define output parser\nlc\\_output\\_parser \\= StructuredOutputParser.from\\_response\\_schemas(response\\_schemas)\noutput\\_parser \\= LangchainOutputParser(lc\\_output\\_parser)\n\n\\# format each prompt with output parser instructions\nfmt\\_qa\\_tmpl \\= output\\_parser.format(DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL)\nfmt\\_refine\\_tmpl \\= output\\_parser.format(DEFAULT\\_REFINE\\_PROMPT\\_TMPL)\nqa\\_prompt \\= PromptTemplate(fmt\\_qa\\_tmpl, output\\_parser\\=output\\_parser)\nrefine\\_prompt \\= PromptTemplate(fmt\\_refine\\_tmpl, output\\_parser\\=output\\_parser)\n\n\\# query index\nquery\\_engine \\= index.as\\_query\\_engine(\n    service\\_context\\=ServiceContext.from\\_defaults(\n        llm\\_predictor\\=llm\\_predictor\n    ),\n    text\\_qa\\_template\\=qa\\_prompt,\n    refine\\_template\\=refine\\_prompt,\n)\nresponse \\= query\\_engine.query(\n    \"What are a few things the author did growing up?\",\n)\nprint(str(response))\n\nOutput:\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}"
}