{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html",
        "title": "HuggingFace LLM - StableLM - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## HuggingFace LLM - StableLM[\uf0c1](#huggingface-llm-stablelm \"Permalink to this heading\")\n\n## Load documents, build the VectorStoreIndex[\uf0c1](#load-documents-build-the-vectorstoreindex \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama\\_index.llms import HuggingFaceLLM\n\nINFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNote: NumExpr detected 16 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\nNumExpr defaulting to 8 threads.\n\n/home/loganm/miniconda3/envs/gpt\\_index/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../../data/paul\\_graham\").load\\_data()\n\n\\# setup prompts - specific to StableLM\nfrom llama\\_index.prompts import PromptTemplate\n\nsystem\\_prompt \\= \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n\\- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n\\- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n\\- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n\\- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\n\\# This will wrap the default prompts that are internal to llama-index\nquery\\_wrapper\\_prompt \\= PromptTemplate(\"<|USER|>{query\\_str}<|ASSISTANT|>\")\n\nimport torch\n\nllm \\= HuggingFaceLLM(\n    context\\_window\\=4096,\n    max\\_new\\_tokens\\=256,\n    generate\\_kwargs\\={\"temperature\": 0.7, \"do\\_sample\": False},\n    system\\_prompt\\=system\\_prompt,\n    query\\_wrapper\\_prompt\\=query\\_wrapper\\_prompt,\n    tokenizer\\_name\\=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model\\_name\\=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device\\_map\\=\"auto\",\n    stopping\\_ids\\=\\[50278, 50279, 50277, 1, 0\\],\n    tokenizer\\_kwargs\\={\"max\\_length\": 4096},\n    \\# uncomment this if using CUDA to reduce memory usage\n    \\# model\\_kwargs={\"torch\\_dtype\": torch.float16}\n)\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=1024, llm\\=llm)\n\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 \\[00:24<00:00, 12.21s/it\\]\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\n\\# set Logging to DEBUG for more detailed outputs\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\n\nSetting \\`pad\\_token\\_id\\` to \\`eos\\_token\\_id\\`:0 for open-end generation.\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 2126 tokens\n> \\[get\\_response\\] Total LLM token usage: 2126 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\nThe author is a computer scientist who has written several books on programming languages and software development. He worked on the IBM 1401 and wrote a program to calculate pi. He also wrote a program to predict how high a rocket ship would fly. The program was written in Fortran and used a TRS-80 microcomputer. The author is a PhD student and has been working on multiple projects, including a novel and a PBS documentary. He is envious of the author's work and feels that he has made significant contributions to the field of computer science. He is working on multiple projects and is envious of the author's work. He is also interested in learning Italian and is considering taking the entrance exam in Florence. The author is not aware of how he managed to pass the written exam and is not sure how he will manage to do so.\n\n## Query Index - Streaming[\uf0c1](#query-index-streaming \"Permalink to this heading\")\n\nquery\\_engine \\= index.as\\_query\\_engine(streaming\\=True)\n\n\\# set Logging to DEBUG for more detailed outputs\nresponse\\_stream \\= query\\_engine.query(\"What did the author do growing up?\")\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 0 tokens\n\nSetting \\`pad\\_token\\_id\\` to \\`eos\\_token\\_id\\`:0 for open-end generation.\n\n\\> \\[get\\_response\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n\\# can be slower to start streaming since llama-index often involves many LLM calls\nresponse\\_stream.print\\_response\\_stream()\n\nThe author is a computer scientist who has written several books on programming languages and software development. He worked on the IBM 1401 and wrote a program to calculate pi. He also wrote a program to predict how high a rocket ship would fly. The program was written in Fortran and used a TRS-80 microcomputer. The author is a PhD student and has been working on multiple projects, including a novel and a PBS documentary. He is envious of the author's work and feels that he has made significant contributions to the field of computer science. He is working on multiple projects and is envious of the author's work. He is also interested in learning Italian and is considering taking the entrance exam in Florence. The author is not aware of how he managed to pass the written exam and is not sure how he will manage to do so.<|USER|>\n\n\\# can also get a normal response object\nresponse \\= response\\_stream.get\\_response()\nprint(response)\n\n\\# can also iterate over the generator yourself\ngenerated\\_text \\= \"\"\nfor text in response.response\\_gen:\n    generated\\_text += text"
}