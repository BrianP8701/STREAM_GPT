["STREAM AI 1\n 1\n Real-Time, Multithreaded Anomaly Detection System with Autonomous\n Synchronization & Self-Correction\n Brian Przezdziecki\n University of Buffalo (SUNY)\n", "STREAM AI 1\n 2\n Abstract\n Real-time anomaly detection is crucial during the printing process, especially when\n looking for issues like over or under extrusion. We employ a MobileNet model to classify these\n anomalies. Our system has two main components: Tip Tracking and Anomaly Detection. In the\n Tip Tracking phase, we estimate the locations of the tips using gcode data. Then, we use the\n YOLO model to align these predictions with live video footage, ensuring synchronization and\n correcting any prediction inaccuracies. Given the need for real-time performance on a Raspberry\n Pi, our system is optimized for computational efficiency. It's designed to operate in real-time,\n running alongside video feeds, and strikes a balance between accuracy, autonomy, robustness and\n speed.\n", "STREAM AI 1\n 3\n Problem\n We need to spot errors in 3D printing as they happen. The main mistakes are tied to how\n much material comes out - too much or too little (Under/Over Extrusion).\n Classes:\n Over\n Normal\n Under\n Another problem arises: Given a frame, the only relevant information is the area around the tip in\n the direction of recently extruded material. This is why we need to track the tip.\n", "STREAM AI 1\n 4\n Solution\n The given diagram displays how the system is divided into distinct threads and illustrates\n the process flow. All threads access some shared global variables. The tracker thread initiates and\n manages all other threads. This system primarily revolves around three key components:\n 1. Initialization: Predicting the path of tip tracking and aligning with the video.\n 2. Error Correction: Correcting errors through a feedback loop.\n 3. Analytics: Identifying anomalies.\n For the first two steps, we utilize a fine-tuned Yolov8. For the anomaly identification, we rely on\n a fine-tuned MobileNetv3 model.\n Why we need Tip Tracking & Examples of Video and Anomalies\n", "STREAM AI 1\n 5\n Challenges\n This system is designed with simplicity and versatility in mind. Our goal is to make it\n readily adoptable for anyone, ensuring it can seamlessly integrate with various 3D printers in\n diverse settings. There's no need for a precise camera placement, giving users flexibility. The\n system adjusts to different resolutions, simplifying setup processes.\n It's built to work with any printer that operates on g-code, even if the g-code\n interpretations vary. With edge deployment capabilities, it can run on a Raspberry Pi, streaming\n in real time. Given that the YOLO model may occasionally err in object detection, our system is\n made to account for these discrepancies. Moreover, users will benefit from a clear visualization\n of the tip tracking procedure.\n Given these constraints and requirements, the ensuing challenges and our tailored\n solutions will be elaborated below.\n", "STREAM AI 1\n 6\n Model Training\n https://github.com/BrianP8701/Anomaly_Classification\n Object Detection for Tip Tracking\n We utilized the YOLOv8s model, a prominent object detection system, and fine-tuned it\n through Roboflow, a platform that streamlines the computer vision data process.\n Input Size Dataset Size Augmentations Precision Recall MAP50\n 640x640\n 2407\n 1200\n 0.967\n 0.931\n 0.968\n We added onto our dataset and retrained multiple times, aiming to fill in gaps and\n weaknesses of the model, adding null images and wrong detections etc. We applied\n augmentations of blur, noise and brightness.\n", "STREAM AI 1\n 7\n Object Classification for Anomalies\n We tested various models, settings, and preprocessing methods to find the best model.\n Our data is limited because only a few frames in each video show over or under extrusion,\n making data collection slow. Here's our data pipeline:\n Examining the pipeline: I developed several GUIs to address the initial data collection\n bottleneck. I opted for an 85x85 crop size as it best captured material near the extrusion tip. I\n used grayscale to reduce dimensions. In some images, edges and material are barely visible.\n Therefore, a filter was used in preprocessing to enhance these crucial edges.\n I initially attempted a filter that segmented the image into\n distinct color intensities, using the original image's color intensity\n standard deviation. While somewhat effective, it missed the\n subtlest edges.\n", "STREAM AI 1\n 8\n Drawing inspiration from the convolutional operations in CNNs, I\n experimented with various kernels and matrix filter combinations on the\n images. However, no single combination consistently worked across all\n images. I tested standard filters such as the Non-local Means Denoising\n from cv2 and the Unsharp Mask from PIL. Yet, neither succeeded in\n enhancing the faint edges.\n An observation was made, each image features a limited set of objects and hues: material,\n tip, bed, shadows, and gleam. Each color intensity in the image uniquely follows a normal\n distribution with its distinct mean and variance. Hence, Gaussian Mixture Models are apt for this\n task.\n", "STREAM AI 1\n 9\n Using Gaussian Mixture Models, we can sharply define even the subtlest edges. I tested\n various component numbers, indicating the assumed Gaussians in the mixture. Next, we\n prioritize sensitivity around brighter colors since it's challenging to discern edges when material\n accumulates. This focus ignores the darker bed, which isn't of concern. This final refinement\n resulted in our preprocessing step, dubbed GMMS.\n Through experimentation, GMMS6 is found to perform the best when training models.\n", "STREAM AI 1\n 10\n Back to examining the pipeline, the\n images are resized to 224x224, suitable\n for EfficientNet, MobileNet, and\n ResNet, which have trained on the\n 224x224-sized ImageNet dataset,\n making them adept at recognizing\n features of this scale. Post-resizing, the\n images undergo augmentation and are split into training, validation, and test datasets.\n Dataset Summary:\n Under: 436 images | Normal: 463 images | Over: 446 images\n Extrusion Classification Dataset\n Augmentations for each class include rotations, brightness, saturation, contrast, hue\n adjustments, and flips (both horizontal and vertical) creating 60 new images per class. The data\n distribution is 75% for training, 15% for validation, and 10% for testing.\n Training Summary:\n Fine tuning, model checkpointing, 4 batches, ~12 epochs, learning rate decay, early stopping.\n learning_rate=0.01, momentum=0.9, step_size=5, gamma=0.1\n After trials of various models, settings, hyperparameters and preprocessing steps, the\n combination of the described preprocessing and training procedures outlined above yielded the\n highest accuracy. The standout was a fine tuned MobileNetv3Large model, which not only won\n in accuracy but also had superior computational efficiency.\n", "STREAM AI 1\n 11\n To the left shows one of the\n comparison rounds, where we identified\n our best performing model. Here we\n compare 3 different settings and their\n combinations: Fine tuning\n MobileNetv3Small vs MobileNetv3Large.\n Using no GMM preprocessing vs using\n GMM preprocessing vs using GMMS\n preprocessing with additional sensitivity\n to more intense pixels.\n The results are fitting, showing that\n indeed using GMMS with extra\n sensitivity is beneficial, and training the\n larger model results in better\n performance. The training and validation\n metrics are nearly indistinguishable\n indicating that it is not overfitting.\n Best performing model accuracy:\n 0.9025974025974026\n", "STREAM AI 1\n 12\n Implementation\n In this section I will describe the design choices and details of the system.\n https://github.com/BrianP8701/STREAM.AI\n Input\n 1. G-Code:\n Instruction set for 3D print.\nSimple and sequential: it runs from start to finish.\nSpecifies the top speed at any moment and provides movement coordinates.\n -\n-\n-\n- The printer moves directly from one point to the next without deviation.\n 2. Video Stream:\n - Live footage of the 3D printer bed.\n- The angle of the camera relative to the printer is expected to be roughly consistent.\n 3. Signal Stream:\n - Managed by the SKR board, which controls the 3D printer.\n- A hardware connection links the SKR board to our Raspberry Pi.\n- The Raspberry Pi gets a signal every time a movement completes. This is the signal, and\n consists of the bed position and time. [time, x, y, z]\n - This data streams in real time, parallel to the printing process.\n Initialization\n First and foremost, the system, starting in the main Tracker Thread, begins receiving and\n routing frames and signals through their corresponding routers. These routers will pass the data\n to the correct objects and threads through the shared global variable space based on the current\n state of the system (Initializing or Tracking).\n", "STREAM AI 1\n 13\n The duty of the initialization phase is to autonomously adapt to the camera position. It\n primarily performs these tasks:\n 1. Make initial predictions:\n a. To transform G-code into predictions for every\n moment in time, we first break down the G-code\ninto a sequence of motion instructions between\ncorners (or waypoints) where there are changes in\ndirection or speed.\n b. For each of these motions, we calculate the optimal\nspeed profile considering constraints like maximum\nspeeds and acceleration. We need to look at the future 2 moves as well, to\ndetermine what the final speed at the end of the motion should be.\n c. Given the optimal speed profiles for each motion, we\u2019ll calculate how long each\nmove should take using kinematics. One problem cannot be solved analytically,\nand needs to be solved iteratively. The iterative method tweaks the distance over\nwhich we accelerate or decelerate, attempting to converge on the desired final\nspeed without exceeding the machine's constraints.\n d. After computing the speed profile, the script then divides the motion into frames\n(based on the given frames-per-second parameter) and predicts the position and\nangle of the machine's tip for each frame. By the end of this process, we have a\ndetailed account of where the machine should be and at what angle for every\nsingle frame of the motion.\n 2. Derive millimeter to pixel ratio:\n For robustness, the system will autonomously map bed predictions to screen\npredictions without requiring users to enter any configuring variables. By waiting\nfor two signals and observing their millimeter disparity on the bed, we can\ncorrelate this with the pixel difference YOLO detects, facilitating the necessary\nconversion. We slide a 640x640 window across the image to run YOLO.\n 3. Synchronize predictions with video:\n To initiate the process, we align our screen predictions using the timestamp of the\nfirst YOLO-signal inference from the video.\n", "STREAM AI 1\n 14\n Predictions are imperfect due to accumulated small tracking errors. Reasons include:\n 1. Merlin firmware in our 3D printer employs Beziet\u2019s\n 6th order function for movement extrapolation, while\nour algorithm uses a simpler 2nd order function.\nHowever, our model considers acceleration.\n 2. Some printers adjust movement factors, like speed\n and acceleration.\n 3. Minor deviations arise when the printer halts.\n4. Merlin Firmware has intricate mechanisms for gcode\n interpretation.\n Instead of replicating Merlin's exact approach, we designed a straightforward interpreter with\nreal-time error feedback loops, enhancing system robustness and compatibility with various 3D\nprinters.\n Error Correction\n Predictions face two error types: Temporal and Spatial, addressed by the ErrorCorrection thread.\n 1. Spatial Error: Mainly arises from incorrect ratios or YOLO inference. These are rare, typically\n minor initial tracking deviations. For demonstration, an exaggerated error was intentionally\n introduced in the Spatial Error Correction video.\n Measure\n Detect\n Correct\n Everytime a signal arrives, we run\nYOLO to see where the tip is. We\ncompare this to our prediction and\nmeasure the difference in pixels.\n When there is a consistent spatial error\nof the same magnitude and direction for\nsufficient time.\n Shift all\npredictions by\nspatial error.\n 2. Temporal Error: This was a significant challenge. Without input signals, gauging temporal\n error was tough. The ultimate solution simply involved immediate measurement and correction\n", "STREAM AI 1\n 15\n of temporal discrepancies. This error stems from varying interpretations of gcode, especially\n differing accelerations.\n Measure\n Detect\n Everytime a signal arrives, we\ncompare the time of the signal,\ncompared to our predictions. We\nmeasure error in seconds.\n We don\u2019t look for any consistent\nerrors. When there is an error, we\nsimply perform a correction.\n Correct\n Pause or skip\nahead to realign.\n Analytics\n Incorporating a real-time feedback loop (autonomous corrections) results in near-flawless tip\n tracking. Using YOLO isn't constant; it's employed during signals and error measurements. This\n enables efficient pinpointing of the recently extruded material around the tip\u2014our target. Given\n our knowledge from the gcode about the printer's movement direction, we crop accordingly. This\n cropped image then enters our data preprocessing pipeline and undergoes classification by the\n tailored MobileNet model, determining whether extrusion is under, normal, or over.\n", "STREAM AI 1\n 16\n Metrics\n To evaluate our system, we'll focus on its computational efficiency, while the accuracy is\n derived from the MobileNet model's test performance. We'll assess:\n - RAM Usage: We'll monitor RAM consumption over time.\n-\n Inference Intervals: Given the adaptive nature of our MobileNet model, observing the\nintervals between successive inferences provides a measure of system speed. The model\nrefrains from inferring with a full buffer.\n These metrics will be gauged on both my device and a Raspberry Pi 4, with respective\n specifications provided below.\n Deployment to Raspberry Pi\n Raspberry Pi Metrics\n Future Improvements\n", "STREAM AI 1\n 17\n 1. Processor (Chipset) Details:\n - Model: Apple M1 Pro\n- Total Number of Cores: 14\n 2. Memory:\n - Size: 16 GB\n- Type: LPDDR5\n- Manufacturer: Hynix\n 3. Graphics/GPU:\n - Chipset Model: Apple M1 Pro\n- Metal Support: Metal 3\n 4. Display:\n - Type: Built-in Liquid Retina XDR Display\n- Resolution: 3024 x 1964 Retina\n 5. Storage:\n - Type: SSD (APPLE SSD AP0512R)\n- Capacity: 500.28 GB\n- Protocol: Apple Fabric (indicating it's an NVMe SSD)\n- Free Space: 120.4 GB\n 6. File System:\n- Type: APFS\n https://qengineering.eu/install-opencv-on-raspberry-pi.html\nhttps://qengineering.eu/install-pytorch-on-raspberry-pi-4.html\nhttps://gist.github.com/wenig/8bab88dede5c838660dd05b8e5b2e23b\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html\n"]