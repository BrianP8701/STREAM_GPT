{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html",
        "title": "Starter Tutorial - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Starter Tutorial[\uf0c1](#starter-tutorial \"Permalink to this heading\")\n\nHere is a starter example for using LlamaIndex.\n\n## Download[\uf0c1](#download \"Permalink to this heading\")\n\nLlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository. We first want to download this `examples` folder. An easy way to do this is to just clone the repo:\n\n$ git clone https://github.com/jerryjliu/llama\\_index.git\n\nNext, navigate to your newly-cloned repository, and verify the contents:\n\n$ cd llama\\_index\n$ ls\nLICENSE                data\\_requirements.txt  tests/\nMANIFEST.in            examples/              pyproject.toml\nMakefile               experimental/          requirements.txt\nREADME.md              llama\\_index/             setup.py\n\nWe now want to navigate to the following folder:\n\n$ cd examples/paul\\_graham\\_essay\n\nThis contains LlamaIndex examples around Paul Graham\u2019s essay, [\u201cWhat I Worked On\u201d](http://paulgraham.com/worked.html). A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.\n\n## Build and Query Index[\uf0c1](#build-and-query-index \"Permalink to this heading\")\n\nCreate a new `.py` file with the following:\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nYou should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\n## Viewing Queries and Events Using Logging[\uf0c1](#viewing-queries-and-events-using-logging \"Permalink to this heading\")\n\nIn a Jupyter notebook, you can view info and/or debugging logging using the following snippet:\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nYou can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\n\nTo view all requests made to your LLMs, you can set the `openai` logging flag:\n\nThis will print out every request and response made via `openai`. Change it back to `None` to turn off.\n\n## Saving and Loading[\uf0c1](#saving-and-loading \"Permalink to this heading\")\n\nBy default, data is stored in-memory. To persist to disk (under `./storage`):\n\nindex.storage\\_context.persist()\n\nTo reload from disk:\n\nfrom llama\\_index import StorageContext, load\\_index\\_from\\_storage\n\n\\# rebuild storage context\nstorage\\_context \\= StorageContext.from\\_defaults(persist\\_dir\\=\"./storage\")\n\\# load index\nindex \\= load\\_index\\_from\\_storage(storage\\_context)\n\nNext Steps\n\n*   learn more about the [high-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html).\n    \n*   tell me how to [customize things](https://docs.llamaindex.ai/en/stable/getting_started/customization.html).\n    \n*   curious about a specific module? check out the guides \ud83d\udc48\n    \n*   have a use case in mind? check out the [end-to-end tutorials](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/use_cases.html)"
}