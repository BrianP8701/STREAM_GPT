{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.html",
        "title": "Using LlamaIndex with Pinecone - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Using LlamaIndex with Pinecone[\uf0c1](#using-llamaindex-with-pinecone \"Permalink to this heading\")\n\nTest complex queries over both text-davinci-003 and ChatGPT\n\n\\# My OpenAI Key\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"\"\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleKeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n)\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\nfrom llama\\_index.llms import OpenAI\n\n## Load Datasets[\uf0c1](#load-datasets \"Permalink to this heading\")\n\nLoad Wikipedia pages\n\nwiki\\_titles \\= \\[\n    \"Toronto\",\n    \"Seattle\",\n    \"San Francisco\",\n    \"Chicago\",\n    \"Boston\",\n    \"Washington, D.C.\",\n    \"Cambridge, Massachusetts\",\n    \"Houston\",\n\\]\npinecone\\_titles \\= \\[\n    \"toronto\",\n    \"seattle\",\n    \"san-francisco\",\n    \"chicago\",\n    \"boston\",\n    \"dc\",\n    \"cambridge\",\n    \"houston\",\n\\]\n\nfrom pathlib import Path\n\nimport requests\n\nfor title in wiki\\_titles:\n    response \\= requests.get(\n        \"https://en.wikipedia.org/w/api.php\",\n        params\\={\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"extracts\",\n            \\# 'exintro': True,\n            \"explaintext\": True,\n        },\n    ).json()\n    page \\= next(iter(response\\[\"query\"\\]\\[\"pages\"\\].values()))\n    wiki\\_text \\= page\\[\"extract\"\\]\n\n    data\\_path \\= Path(\"data\")\n    if not data\\_path.exists():\n        Path.mkdir(data\\_path)\n\n    with open(data\\_path / f\"{title}.txt\", \"w\") as fp:\n        fp.write(wiki\\_text)\n\n\\# Load all wiki documents\ncity\\_docs \\= {}\nfor wiki\\_title in wiki\\_titles:\n    city\\_docs\\[wiki\\_title\\] \\= SimpleDirectoryReader(\n        input\\_files\\=\\[f\"data/{wiki\\_title}.txt\"\\]\n    ).load\\_data()\n\n## Initialize Pinecone Indexes[\uf0c1](#initialize-pinecone-indexes \"Permalink to this heading\")\n\nimport pinecone\nimport os\n\napi\\_key \\= \"\"\nenvironment \\= \"eu-west1-gcp\"\nindex\\_name \\= \"quickstart\"\n\nos.environ\\[\"PINECONE\\_API\\_KEY\"\\] \\= api\\_key\n\nllm \\= OpenAI(temperature\\=0, model\\=\"gpt-3.5-turbo\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n### Recommended Option: Pass API key via env variable, and index\\_name & environment as argument[\uf0c1](#recommended-option-pass-api-key-via-env-variable-and-index-name-environment-as-argument \"Permalink to this heading\")\n\n\\# Build city document index\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\ncity\\_indices \\= {}\nfor pinecone\\_title, wiki\\_title in zip(pinecone\\_titles, wiki\\_titles):\n    metadata\\_filters \\= {\"wiki\\_title\": wiki\\_title}\n    vector\\_store \\= PineconeVectorStore(\n        index\\_name\\=index\\_name,\n        environment\\=environment,\n        metadata\\_filters\\=metadata\\_filters,\n    )\n    storage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\n    city\\_indices\\[wiki\\_title\\] \\= VectorStoreIndex.from\\_documents(\n        city\\_docs\\[wiki\\_title\\],\n        storage\\_context\\=storage\\_context,\n        service\\_context\\=service\\_context,\n    )\n    \\# set summary text for city\n    city\\_indices\\[wiki\\_title\\].index\\_struct.index\\_id \\= pinecone\\_title\n\n### Alternative Option: instantiate pinecone client first, then pass to PineconeVectorStore[\uf0c1](#alternative-option-instantiate-pinecone-client-first-then-pass-to-pineconevectorstore \"Permalink to this heading\")\n\npinecone.init(api\\_key\\=api\\_key, environment\\=environment)\n\npinecone\\_index \\= pinecone.Index(index\\_name)\n\n\\# Build city document index\ncity\\_indices \\= {}\nfor pinecone\\_title, wiki\\_title in zip(pinecone\\_titles, wiki\\_titles):\n    metadata\\_filters \\= {\"wiki\\_title\": wiki\\_title}\n    vector\\_store \\= PineconeVectorStore(\n        pinecone\\_index\\=pinecone\\_index, metadata\\_filters\\=metadata\\_filters\n    )\n    storage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\n    city\\_indices\\[wiki\\_title\\] \\= VectorStoreIndex.from\\_documents(\n        city\\_docs\\[wiki\\_title\\],\n        storage\\_context\\=storage\\_context,\n        service\\_context\\=service\\_context,\n    )\n    \\# set summary text for city\n    city\\_indices\\[wiki\\_title\\].index\\_struct.index\\_id \\= pinecone\\_title\n\n### Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\nresponse \\= (\n    city\\_indices\\[\"Boston\"\\]\n    .as\\_query\\_engine(service\\_context\\=service\\_context)\n    .query(\"Tell me about the arts and culture of Boston\")\n)\n\nprint(str(response))\nprint(response.get\\_formatted\\_sources())\n\n### Build Graph: Keyword Table Index on top of vector indices![\uf0c1](#build-graph-keyword-table-index-on-top-of-vector-indices \"Permalink to this heading\")\n\nWe compose a keyword table index on top of all the vector indices.\n\nfrom llama\\_index.indices.composability.graph import ComposableGraph\n\n\\# set summaries for each city\nindex\\_summaries \\= {}\nfor wiki\\_title in wiki\\_titles:\n    \\# set summary text for city\n    index\\_summaries\\[wiki\\_title\\] \\= f\"Wikipedia articles about {wiki\\_title}\"\n\ngraph \\= ComposableGraph.from\\_indices(\n    SimpleKeywordTableIndex,\n    \\[index for \\_, index in city\\_indices.items()\\],\n    \\[summary for \\_, summary in index\\_summaries.items()\\],\n    max\\_keywords\\_per\\_chunk\\=50,\n)\n\ncustom\\_query\\_engines \\= {\n    graph.root\\_id: graph.root\\_index.as\\_query\\_engine(\n        retriever\\_mode\\=\"simple\", service\\_context\\=service\\_context\n    )\n}\n\nquery\\_engine \\= graph.as\\_query\\_engine(\n    custom\\_query\\_engines\\=custom\\_query\\_engines,\n)\n\n### Compare Queries (text-davinci-003 vs. ChatGPT)[\uf0c1](#compare-queries-text-davinci-003-vs-chatgpt \"Permalink to this heading\")\n\n**Simple Query**\n\nquery\\_str \\= \"Tell me more about Boston\"\nresponse\\_chatgpt \\= query\\_engine.query(query\\_str)\n\nprint(response\\_chatgpt)\nprint(response\\_chatgpt.get\\_formatted\\_sources())"
}