{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/query_transformations/SimpleIndexDemo-multistep.html",
        "title": "Multi-Step Query Engine - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Multi-Step Query Engine[\uf0c1](#multi-step-query-engine \"Permalink to this heading\")\n\nWe have a multi-step query engine that\u2019s able to decompose a complex query into sequential subquestions. This guide walks you through how to set it up!\n\n## Load documents, build the VectorStoreIndex[\uf0c1](#load-documents-build-the-vectorstoreindex \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n)\nfrom llama\\_index.llms import OpenAI\nfrom IPython.display import Markdown, display\n\n\\# LLM Predictor (gpt-3)\ngpt3 \\= OpenAI(temperature\\=0, model\\=\"text-davinci-003\")\nservice\\_context\\_gpt3 \\= ServiceContext.from\\_defaults(llm\\=gpt3)\n\n\\# LLMPredictor (gpt-4)\ngpt4 \\= OpenAI(temperature\\=0, model\\=\"gpt-4\")\nservice\\_context\\_gpt4 \\= ServiceContext.from\\_defaults(llm\\=gpt4)\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../paul\\_graham\\_essay/data\").load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\nfrom llama\\_index.indices.query.query\\_transform.base import (\n    StepDecomposeQueryTransform,\n)\nfrom llama\\_index import LLMPredictor\n\n\\# gpt-4\nstep\\_decompose\\_transform \\= StepDecomposeQueryTransform(\n    LLMPredictor(llm\\=gpt4), verbose\\=True\n)\n\n\\# gpt-3\nstep\\_decompose\\_transform\\_gpt3 \\= StepDecomposeQueryTransform(\n    LLMPredictor(llm\\=gpt3), verbose\\=True\n)\n\nindex\\_summary \\= \"Used to answer questions about the author\"\n\n\\# set Logging to DEBUG for more detailed outputs\nfrom llama\\_index.query\\_engine.multistep\\_query\\_engine import (\n    MultiStepQueryEngine,\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context\\_gpt4)\nquery\\_engine \\= MultiStepQueryEngine(\n    query\\_engine\\=query\\_engine,\n    query\\_transform\\=step\\_decompose\\_transform,\n    index\\_summary\\=index\\_summary,\n)\nresponse\\_gpt4 \\= query\\_engine.query(\n    \"Who was in the first batch of the accelerator program the author\"\n    \" started?\",\n)\n\ndisplay(Markdown(f\"<b>{response\\_gpt4}</b>\"))\n\nsub\\_qa \\= response\\_gpt4.metadata\\[\"sub\\_qa\"\\]\ntuples \\= \\[(t\\[0\\], t\\[1\\].response) for t in sub\\_qa\\]\nprint(tuples)\n\nresponse\\_gpt4 \\= query\\_engine.query(\n    \"In which city did the author found his first company, Viaweb?\",\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context\\_gpt3)\nquery\\_engine \\= MultiStepQueryEngine(\n    query\\_engine\\=query\\_engine,\n    query\\_transform\\=step\\_decompose\\_transform\\_gpt3,\n    index\\_summary\\=index\\_summary,\n)\n\nresponse\\_gpt3 \\= query\\_engine.query(\n    \"In which city did the author found his first company, Viaweb?\",\n)"
}