{
    ".gitignore": "private.txt\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n.DS_Store\ndata/test/\nstream_gpt/constants/keys.py\ntests/arbritrary.py\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n.coverage\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n",
    "LICENSE": "MIT License\n\nCopyright (c) 2023 BrianP8701\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
    "README.md": "# STREAM_GPT",
    "data/library/DL_MIT.txt": " DEEP LEARNING The MIT Press Essential Knowledge Series\nA complete list of the titles in this series appears at the back of this book. DEEP LEARNING\nJOHN D. KELLEHER\nThe MIT Press | Cambridge, Massachusetts | London, England \u00a9 2019 The Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by\nany electronic or mechanical means (including photocopying, recording,\nor information storage and retrieval) without permission in writing from\nthe publisher.\nThis book was set in Chaparral Pro by Toppan Best-set Premedia Limited.\nPrinted and bound in the United States of America.\nLibrary of Congress Cataloging-in-Publication Data\nNames: Kelleher, John D., 1974- author.\nTitle: Deep learning / John D. Kelleher.\nDescription: Cambridge, MA : The MIT Press, [2019] | Series:\nThe MIT press essential knowledge series | Includes bibliographical\nreferences and index.\nIdentifiers: LCCN 2018059550 | ISBN 9780262537551 (pbk. : alk. paper)\nSubjects: LCSH: Machine learning. | Artificial intelligence.\nClassification: LCC Q325.5 .K454 2019 | DDC 006.3/1\u2014dc23 LC record\navailable at https://lccn.loc.gov/2018059550\n10 9 8 7 6 5 4 3 2 1 CONTENTS\nSeries Foreword vii\nPreface ix\nAcknowledgments xi\n1 Introduction to Deep Learning 1\n2 Conceptual Foundations 39\n3 Neural Networks: The Building Blocks of Deep\nLearning 65\n4 A Brief History of Deep Learning 101\n5 Convolutional and Recurrent Neural Networks 159\n6 Learning Functions 185\n7 The Future of Deep Learning 231\nGlossary 251\nNotes 257\nReferences 261\nFurther Readings 267\nIndex 269  SERIES FOREWORD\nThe MIT Press Essential Knowledge series offers acces-\nsible, concise, beautifully produced pocket- size books on\ntopics of current interest. Written by leading thinkers,\nthe books in this series deliver expert overviews of sub-\njects that range from the cultural and the historical to the\nscientific and the technical.\nIn today\u2019s era of instant information gratification, we\nhave ready access to opinions, rationalizations, and super-\nficial descriptions. Much harder to come by is the founda-\ntional knowledge that informs a principled understanding\nof the world. Essential Knowledge books fill that need.\nSynthesizing specialized subject matter for nonspecialists\nand engaging critical topics through fundamentals, each\nof these compact volumes offers readers a point of access\nto complex ideas.\nBruce Tidor\nProfessor of Biological Engineering and Computer Science\nMassachusetts Institute of Technology  PREFACE\nDeep learning is enabling innovation and change across\nall aspects of our modern lives. Most of the artificial intel-\nligence breakthroughs that you hear about in the media\nare based on deep learning. As a result, whether you are\na business person interested in improving the efficiency\nof your organization, a policymaker concerned with eth-\nics and privacy in a Big Data world, a researcher working\nwith complex data, or a curious citizen who wants a better\nsense of the potential of artificial intelligence and how it\nwill change your life, it is important for you to have an\nunderstanding of deep learning.\nThe goal of this book is to enable the general reader\nto gain an understanding of what deep learning is, where\nit has come from, how it works, what it makes possible\n(and what it doesn\u2019t), and how the field is likely to develop\nin the next ten years. The fact that deep learning is a set\nof algorithms and models means that understanding deep\nlearning requires understanding how these algorithms\nand models process data. As a result, this book is not\npurely descriptive and definitional; it also includes expla-\nnations of algorithms. I have attempted to present the\ntechnical material in an accessible way. From my teaching\nexperience, I have found that for technical topics the most accessible presentation is to explain the fundamental con-\ncepts in a step-b y- step manner. So, although I have tried\nto keep the mathematical content to a minimum, where\nI felt it was necessary to include it I have endeavored to\nwalk you through the mathematical equations in as clear\nand direct a manner as I can. I have supplemented these\nexplanations with examples and illustrations.\nWhat is really wondrous about deep learning is not\nthe complexity of the math it is built on, but rather, that it\ncan perform such a diverse set of exciting and impressive\ntasks using such simple calculations. Don\u2019t be surprised to\nfind yourself saying: \u201cIs that all it\u2019s doing?\u201d In fact, a deep\nlearning model really is just a lot (admittedly, an awful\nlot) of multiplications and additions with a few nonlinear\nmappings (which I will explain) added in. Yet, despite this\nsimplicity, these models can, among other achievements,\nbeat the Go world champion, define the state- of- the- art\nin computer vision and machine translation, and drive a\ncar. This book is an introductory text about deep learning,\nbut I hope that it is an introduction that has enough depth\nthat you will come back to the book as your confidence\nwith the material grows.\nx PREFACE ACKNOWLEDGMENTS\nThis book would not have been possible without the sacri-\nfices made by my wife, Aphra, and my family, in particular\nmy parents John and Betty Kelleher. I have also received\na huge amount of support from friends, especially Alan\nMcDonnell, Ionela Lungu, Simon Dobnik, Lorraine Byrne,\nNoel Fitzpatrick, and Josef van Genabith.\nI would also like to acknowledge the help I have re-\nceived from the staff at the MIT Press, and from a number\nof people who have read sections of the book and provided\nfeedback. MIT Press organized three anonymous review-\ners who read and commented on a draft of the book. I\nthank these reviewers for their time and helpful feedback.\nAlso a number of people read draft chapters from the book\nand I wish to take this opportunity to publicly acknowl-\nedge their help, so my thanks to: Mike Dillinger, Magda-\nlena Kacmajor, Elizabeth Kelleher, John Bernard Kelleher,\nAphra Kerr, Filip Klubi\u010dka, and Abhijit Mahalunkar. This\nbook has been informed by the many conversations I have\nhad with colleagues and students about deep learning, in\nparticular those with Robert Ross and Giancarlo Salton.\nThis book is dedicated to my sister Elizabeth (Liz)\nKelleher in recognition of her love and support, and her\npatience with a brother who can\u2019t stop explaining things.  1\nINTRODUCTION TO\nDEEP LEARNING\nDeep learning is the subfield of artificial intelligence that\nfocuses on creating large neural network models that are\ncapable of making accurate data- driven decisions. Deep\nlearning is particularly suited to contexts where the data is\ncomplex and where there are large datasets available. To-\nday most online companies and high-e nd consumer tech-\nnologies use deep learning. Among other things, Facebook\nuses deep learning to analyze text in online conversations.\nGoogle, Baidu, and Microsoft all use deep learning for im-\nage search, and also for machine translation. All modern\nsmart phones have deep learning systems running on\nthem; for example, deep learning is now the standard\ntechnology for speech recognition, and also for face de-\ntection on digital cameras. In the healthcare sector, deep\nlearning is used to process medical images (X-r ays, CT, and\nMRI scans) and diagnose health conditions. Deep learning\nis also at the core of self-d riving cars, where it is used for localization and mapping, motion planning and steering,\nand environment perception, as well as tracking driver\nstate.\nPerhaps the best- known example of deep learning is\nDeepMind\u2019s AlphaGo.1 Go is a board game similar to Chess.\nAlphaGo was the first computer program to beat a profes-\nsional Go player. In March 2016, it beat the top Korean\nprofessional, Lee Sedol, in a match watched by more than\ntwo hundred million people. The following year, in 2017,\nAlphaGo beat the world\u2019s No. 1 ranking player, China\u2019s\nKe Jie.\nIn 2016 AlphaGo\u2019s success was very surprising. At\nthe time, most people expected that it would take many\nmore years of research before a computer would be able\nto compete with top level human Go players. It had been\nknown for a long time that programming a computer to\nplay Go was much more difficult than programming it to\nplay Chess. There are many more board configurations\npossible in Go than there are in Chess. This is because Go\nhas a larger board and simpler rules than Chess. There are,\nin fact, more possible board configurations in Go than\nthere are atoms in the universe. This massive search space\nand Go\u2019s large branching factor (the number of board\nconfigurations that can be reached in one move) makes\nGo an incredibly challenging game for both humans and\ncomputers.\nOne way of illustrating the relative difficulty Go\nand Chess presented to computer programs is through\n2 ChAPtER 1 a historical comparison of how Go and Chess programs\ncompeted with human players. In 1967, MIT\u2019s MacHack- 6\nChess program could successfully compete with humans\nand had an Elo rating2 well above novice level, and, by May\n1997, DeepBlue was capable of beating the Chess world\nchampion Gary Kasparov. In comparison, the first com-\nplete Go program wasn\u2019t written until 1968 and strong\nhuman players were still able to easily beat the best Go\nprograms in 1997.\nThe time lag between the development of Chess and\nGo computer programs reflects the difference in compu-\ntational difficulty between these two games. However, a\nsecond historic comparison between Chess and Go illus-\ntrates the revolutionary impact that deep learning has\nhad on the ability of computer programs to compete with\nhumans at Go. It took thirty years for Chess programs to\nprogress from human level competence in 1967 to world\nchampion level in 1997. However, with the development\nof deep learning it took only seven years for computer Go\nprograms to progress from advanced amateur to world\nchampion; as recently as 2009 the best Go program in\nthe world was rated at the low-e nd of advanced amateur.\nThis acceleration in performance through the use of deep\nlearning is nothing short of extraordinary, but it is also\nindicative of the types of progress that deep learning has\nenabled in a number of fields.\nIntRoduCtIon to dEEP LEARnIng 3 AlphaGo uses deep learning to evaluate board configu-\nrations and to decide on the next move to make. The fact\nthat AlphaGo used deep learning to decide what move to\nmake next is a clue to understanding why deep learning\nis useful across so many different domains and applica-\ntions. Decision- making is a crucial part of life. One way\nto make decisions is to base them on your \u201cintuition\u201d or\nyour \u201cgut feeling.\u201d However, most people would agree that\nthe best way to make decisions is to base them on the rel-\nevant data. Deep learning enables data- driven decisions by\nidentifying and extracting patterns from large datasets\nthat accurately map from sets of complex inputs to good\ndecision outcomes.\nArtificial Intelligence, Machine Learning, and\nDeep Learning\nDeep learning has emerged from research in artificial\nintelligence and machine learning. Figure 1.1 illustrates\nthe relationship between artificial intelligence, machine\nlearning, and deep learning.\nThe field of artificial intelligence was born at a\nworkshop at Dartmouth College in the summer of 1956.\nResearch on a number of topics was presented at the\nworkshop including mathematical theorem proving, nat-\nural language processing, planning for games, computer\n4 ChAPtER 1 Deep learning enables\ndata- driven decisions\nby identifying and\nextracting patterns\nfrom large datasets that\naccurately map from\nsets of complex inputs\nto good decision\noutcomes. Artificial\nintelligence\nMachine\nlearning\nDeep\nlearning\nFigure 1.1 The relationship between artificial intelligence, machine\nlearning, and deep learning.\nprograms that could learn from examples, and neural net-\nworks. The modern field of machine learning draws on the\nlast two topics: computers that could learn from examples,\nand neural network research.\nMachine learning involves the development and eval-\nuation of algorithms that enable a computer to extract (or\nlearn) functions from a dataset (sets of examples). To un-\nderstand what machine learning means we need to under-\nstand three terms: dataset, algorithm, and function.\nIn its simplest form, a dataset is a table where each row\ncontains the description of one example from a domain,\n6 ChAPtER 1 Table 1.1. A dataset of loan applicants and their known\ncredit solvency ratings\nID Annual Income Current Debt Credit Solvency\n1 $150 - $100 100\n2 $250 - $300 - 50\n3 $450 - $250 400\n4 $200 - $350 - 300\nand each column contains the information for one of the\nfeatures in a domain. For example, table 1.1 illustrates\nan example dataset for a loan application domain. This\ndataset lists the details of four example loan applications.\nExcluding the ID feature, which is only for ease of refer-\nence, each example is described using three features: the\napplicant\u2019s annual income, their current debt, and their\ncredit solvency.\nAn algorithm is a process (or recipe, or program) that\na computer can follow. In the context of machine learning,\nan algorithm defines a process to analyze a dataset and\nidentify recurring patterns in the data. For example, the\nalgorithm might find a pattern that relates a person\u2019s an-\nnual income and current debt to their credit solvency rat-\ning. In mathematics, relationships of this type are referred\nto as functions.\nA function is a deterministic mapping from a set of\ninput values to one or more output values. The fact that\nIntRoduCtIon to dEEP LEARnIng 7 the mapping is deterministic means that for any specific\nset of inputs a function will always return the same out-\nputs. For example, addition is a deterministic mapping,\nand so 2+2 is always equal to 4. As we will discuss later,\nwe can create functions for domains that are more com-\nplex than basic arithmetic, we can for example define a\nfunction that takes a person\u2019s income and debt as inputs\nand returns their credit solvency rating as the output\nvalue. The concept of a function is very important to\ndeep learning so it is worth repeating the definition for\nemphasis: a function is simply a mapping from inputs to\noutputs. In fact, the goal of machine learning is to learn\nfunctions from data. A function can be represented in\nmany different ways: it can be as simple as an arithmetic\noperation (e.g., addition or subtraction are both functions\nthat take inputs and return a single output), a sequence\nof if- then- else rules, or it can have a much more complex\nrepresentation.\nOne way to represent a function is to use a neural\nnetwork. Deep learning is the subfield of machine learn-\ning that focuses on deep neural network models. In fact,\nthe patterns that deep learning algorithms extract from\ndatasets are functions that are represented as neural\nnetworks. Figure 1.2 illustrates the structure of a neural\nnetwork. The boxes on the left of the figure represent the\nmemory locations where inputs are presented to the net-\nwork. Each of the circles in this figure is called a neuron\n8 ChAPtER 1 A function is a\ndeterministic mapping\nfrom a set of input\nvalues to one or more\noutput values. and each neuron implements a function: it takes a number\nof values as input and maps them to an output value. The\narrows in the network show how the outputs of each neu-\nron are passed as inputs to other neurons. In this network,\ninformation flows from left to right. For example, if this\nnetwork were trained to predict a person\u2019s credit solvency,\nbased on their income and debt, it would receive the in-\ncome and debt as inputs on the left of the network and\noutput the credit solvency score through the neuron on\nthe right.\nA neural network uses a divide-a nd- conquer strategy\nto learn a function: each neuron in the network learns a\nsimple function, and the overall (more complex) function,\ndefined by the network, is created by combining these\nsimpler functions. Chapter 3 will describe how a neural\nnetwork processes information.\nFigure 1.2 Schematic illustration of a neural network.\n10 ChAPtER 1 What Is Machine Learning?\nA machine learning algorithm is a search process designed\nto choose the best function, from a set of possible func-\ntions, to explain the relationships between features in a\ndataset. To get an intuitive understanding of what is in-\nvolved in extracting, or learning, a function from data, ex-\namine the following set of sample inputs to an unknown\nfunction and the outputs it returns. Given these examples,\ndecide which arithmetic operation (addition, subtraction,\nmultiplication, or division) is the best choice to explain\nthe mapping the unknown function defines between its\ninputs and output:\nfunction(Inputs)=Output\nfunction(5,5)=25\nfunction(2,6)=12\nfunction(4,4)=16\nfunction(2,2)=04\nMost people would agree that multiplication is the best\nchoice because it provides the best match to the observed\nrelationship, or mapping, from the inputs to the outputs:\nIntRoduCtIon to dEEP LEARnIng 11 5\u00d75=25\n2\u00d76=12\n4\u00d77=28\n2\u00d72=04\nIn this particular instance, choosing the best func-\ntion is relatively straightforward, and a human can do it\nwithout the aid of a computer. However, as the number\nof inputs to the unknown function increases (perhaps\nto hundreds or thousands of inputs), and the variety of\npotential functions to be considered gets larger, the task\nbecomes much more difficult. It is in these contexts that\nharnessing the power of machine learning to search for\nthe best function, to match the patterns in the dataset,\nbecomes necessary.\nMachine learning involves a two- step process: train-\ning and inference. During training, a machine learning\nalgorithm processes a dataset and chooses the function\nthat best matches the patterns in the data. The extracted\nfunction will be encoded in a computer program in a par-\nticular form (such as if- then- else rules or parameters of\na specified equation). The encoded function is known as\na model, and the analysis of the data in order to extract\nthe function is often referred to as training the model.\n12 ChAPtER 1 Essentially, models are functions encoded as computer\nprograms. However, in machine learning the concepts of\nfunction and model are so closely related that the distinc-\ntion is often skipped over and the terms may even be used\ninterchangeably.\nIn the context of deep learning, the relationship be-\ntween functions and models is that the function extracted\nfrom a dataset during training is represented as a neural\nnetwork model, and conversely a neural network model\nencodes a function as a computer program. The standard\nprocess used to train a neural network is to begin train-\ning with a neural network where the parameters of the\nnetwork are randomly initialized (we will explain network\nparameters later; for now just think of them as values that\ncontrol how the function the network encodes works).\nThis randomly initialized network will be very inaccurate\nin terms of its ability to match the relationship between\nthe various input values and target outputs for the ex-\namples in the dataset. The training process then proceeds\nby iterating through the examples in the dataset, and,\nfor each example, presenting the input values to the net-\nwork and then using the difference between the output\nreturned by the network and the correct output for the ex-\nample listed in the dataset to update the network\u2019s param-\neters so that it matches the data more closely. Once the\nmachine learning algorithm has found a function that is\nsufficiently accurate (in terms of the outputs it generates\nIntRoduCtIon to dEEP LEARnIng 13 matching the correct outputs listed in the dataset) for\nthe problem we are trying to solve, the training process\nis completed, and the final model is returned by the algo-\nrithm. This is the point at which the learning in machine\nlearning stops.\nOnce training has finished, the model is fixed. The sec-\nond stage in machine learning is inference. This is when\nthe model is applied to new examples\u2014 examples for\nwhich we do not know the correct output value, and there-\nfore we want the model to generate estimates of this value\nfor us. Most of the work in machine learning is focused on\nhow to train accurate models (i.e., extracting an accurate\nfunction from data). This is because the skills and meth-\nods required to deploy a trained machine learning model\ninto production, in order to do inference on new examples\nat scale, are different from those that a typical data scien-\ntist will possess. There is a growing recognition within the\nindustry of the distinctive skills needed to deploy artifi-\ncial intelligence systems at scale, and this is reflected in a\ngrowing interest in the field known as DevOps, a term de-\nscribing the need for collaboration between development\nand operations teams (the operations team being the\nteam responsible for deploying a developed system into\nproduction and ensuring that these systems are stable and\nscalable). The terms MLOps, for machine learning opera-\ntions, and AIOps, for artificial intelligence operations, are\nalso used to describe the challenges of deploying a trained\n14 ChAPtER 1 model. The questions around model deployment are be-\nyond the scope of this book, so we will instead focus on\ndescribing what deep learning is, what it can be used for,\nhow it has evolved, and how we can train accurate deep\nlearning models.\nOne relevant question here is: why is extracting a\nfunction from data useful? The reason is that once a func-\ntion has been extracted from a dataset it can be applied\nto unseen data, and the values returned by the function\nin response to these new inputs can provide insight into\nthe correct decisions for these new problems (i.e., it can\nbe used for inference). Recall that a function is simply a\ndeterministic mapping from inputs to outputs. The sim-\nplicity of this definition, however, hides the variety that\nexists within the set of functions. Consider the following\nexamples:\n\u2022 Spam filtering is a function that takes an email as\ninput and returns a value that classifies the email as\nspam (or not).\n\u2022 Face recognition is a function that takes an image as\ninput and returns a labeling of the pixels in the image\nthat demarcates the face in the image.\n\u2022 Gene prediction is a function that takes a genomic DNA\nsequence as input and returns the regions of the DNA\nthat encode a gene.\nIntRoduCtIon to dEEP LEARnIng 15 \u2022 Speech recognition is a function that takes an audio\nspeech signal as input and returns a textual transcription\nof the speech.\n\u2022 Machine translation is a function that takes a sentence\nin one language as input and returns the translation of\nthat sentence in another language.\nIt is because the solutions to so many problems across so\nmany domains can be framed as functions that machine\nlearning has become so important in recent years.\nWhy Is Machine Learning Difficult?\nThere are a number of factors that make the machine\nlearning task difficult, even with the help of a computer.\nFirst, most datasets will include noise3 in the data, so\nsearching for a function that matches the data exactly is\nnot necessarily the best strategy to follow, as it is equiva-\nlent to learning the noise. Second, it is often the case that\nthe set of possible functions is larger than the set of ex-\namples in the dataset. This means that machine learning\nis an ill-p osed problem: the information given in the prob-\nlem is not sufficient to find a single best solution; instead\nmultiple possible solutions will match the data. We can\nuse the problem of selecting the arithmetic operation (ad-\ndition, subtraction, multiplication, or division) that best\n16 ChAPtER 1 matches a set of example input- output mappings for an\nunknown function to illustrate the concept of an ill- posed\nproblem. Here are the example mappings for this function\nselection problem:\nfunction(Inputs)=Output\nfunction(1,1)=1\nfunction(2,1)=2\nfunction(3,1)=3\nGiven these examples, multiplication and division are bet-\nter matches for the unknown function than addition and\nsubtraction. However, it is not possible to decide whether\nthe unknown function is actually multiplication or divi-\nsion using this sample of data, because both operations\nare consistent with all the examples provided. Conse-\nquently, this is an ill-p osed problem: it is not possible to\nselect a single best answer given the information provided\nin the problem.\nOne strategy to solve an ill-p osed problem is to col-\nlect more data (more examples) in the hope that the new\nexamples will help us to discriminate between the cor-\nrect underlying function and the remaining alternatives.\nFrequently, however, this strategy is not feasible, either\nIntRoduCtIon to dEEP LEARnIng 17 because the extra data is not available or is too expensive\nto collect. Instead, machine learning algorithms overcome\nthe ill-p osed nature of the machine learning task by sup-\nplementing the information provided by the data with a\nset of assumptions about the characteristics of the best\nfunction, and use these assumptions to influence the pro-\ncess used by the algorithm that selects the best function\n(or model). These assumptions are known as the inductive\nbias of the algorithm because in logic a process that infers\na general rule from a set of specific examples is known as\ninductive reasoning. For example, if all the swans that you\nhave seen in your life are white, you might induce from\nthese examples the general rule that all swans are white.\nThis concept of inductive reasoning relates to machine\nlearning because a machine learning algorithm induces (or\nextracts) a general rule (a function) from a set of specific\nexamples (the dataset). Consequently, the assumptions\nthat bias a machine learning algorithm are, in effect, bias-\ning an inductive reasoning process, and this is why they\nare known as the inductive bias of the algorithm.\nSo, a machine learning algorithm uses two sources of\ninformation to select the best function: one is the dataset,\nand the other (the inductive bias) is the assumptions that\nbias the algorithm to prefer some functions over others,\nirrespective of the patterns in the dataset. The inductive\nbias of a machine learning algorithm can be understood\nas providing the algorithm with a perspective on a dataset.\n18 ChAPtER 1 However, just as in the real world, where there is no single\nbest perspective that works in all situations, there is no\nsingle best inductive bias that works well for all datasets.\nThis is why there are so many different machine learning\nalgorithms: each algorithm encodes a different inductive\nbias. The assumptions encoded in the design of a machine\nleanring algorithm can vary in strength. The stronger the\nassumptions the less freedom the algorithm is given in se-\nlecting a function that fits the patterns in the dataset. In a\nsense, the dataset and inductive bias counterbalance each\nother: machine learning algorithms that have a strong in-\nductive bias pay less attention to the dataset when selecting\na function. For example, if a machine learning algorithm\nis coded to prefer a very simple function, no matter how\ncomplex the patterns in the data, then it has a very strong\ninductive bias.\nIn chapter 2 we will explain how we can use the equa-\ntion of a line as a template structure to define a function.\nThe equation of the line is a very simple type of mathemat-\nical function. Machine learning algorithms that use the\nequation of a line as the template structure for the func-\ntions they fit to a dataset make the assumption that the\nmodel they generate should encode a simple linear map-\nping from inputs to output. This assumption is an exam-\nple of an inductive bias. It is, in fact, an example of a strong\ninductive bias, as no matter how complex (or nonlinear)\nIntRoduCtIon to dEEP LEARnIng 19 the patterns in the data are the algorithm will be restricted\n(or biased) to fit a linear model to it.\nOne of two things can go wrong if we choose a machine\nlearning algorithm with the wrong bias. First, if the in-\nductive bias of a machine learning algorithm is too strong,\nthen the algorithm will ignore important information in\nthe data and the returned function will not capture the\nnuances of the true patterns in the data. In other words,\nthe returned function will be too simple for the domain,4\nand the outputs it generates will not be accurate. This\noutcome is known as the function underfitting the data.\nAlternatively, if the bias is too weak (or permissive), the\nalgorithm is allowed too much freedom to find a function\nthat closely fits the data. In this case, the returned func-\ntion is likely to be too complex for the domain, and, more\nproblematically, the function is likely to fit to the noise in\nthe sample of the data that was supplied to the algorithm\nduring training. Fitting to the noise in the training data\nwill reduce the function\u2019s ability to generalize to new data\n(data that is not in the training sample). This outcome is\nknown as overfitting the data. Finding a machine learning\nalgorithm that balances data and inductive bias appropri-\nately for a given domain is the key to learning a function\nthat neither underfits or overfits the data, and that, there-\nfore, generalizes successfully in that domain (i.e., that is\naccurate at inference, or processing new examples that\nwere not in the training data).\n20 ChAPtER 1 However, in domains that are complex enough to war-\nrant the use of machine learning, it is not possible in ad-\nvance to know what are the correct assumptions to use\nto bias the selection of the correct model from the data.\nConsequently, data scientists must use their intuition (i.e.,\nmake informed guesses) and also use trial- and- error ex-\nperimentation in order to find the best machine learning\nalgorithm to use in a given domain.\nNeural networks have a relatively weak inductive bias.\nAs a result, generally, the danger with deep learning is that\nthe neural network model will overfit, rather than under-\nfit, the data. It is because neural networks pay so much\nattention to the data that they are best suited to contexts\nwhere there are very large datasets. The larger the dataset,\nthe more information the data provides, and therefore\nit becomes more sensible to pay more attention to the\ndata. Indeed, one of the most important factors driving\nthe emergence of deep learning over the last decade has\nbeen the emergence of Big Data. The massive datasets\nthat have become available through online social plat-\nforms and the proliferation of sensors have combined to\nprovide the data necessary to train neural network mod-\nels to support new applications in a range of domains. To\ngive a sense of the scale of the big data used in deep learn-\ning research, Facebook\u2019s face recognition software, Deep-\nFace, was trained on a dataset of four million facial images\nIntRoduCtIon to dEEP LEARnIng 21 belonging to more than four thousand identities (Taigman\net al. 2014).\nThe Key Ingredients of Machine Learning\nThe above example of deciding which arithmetic opera-\ntion best explains the relationship between inputs and\noutputs in a set of data illustrates the three key ingredi-\nents in machine learning:\n1. Data (a set of historical examples).\n2. A set of functions that the algorithm will search\nthrough to find the best match with the data.\n3. Some measure of fitness that can be used to evaluate\nhow well each candidate function matches the data.\nAll three of these ingredients must be correct if a machine\nlearning project is to succeed; below we describe each of\nthese ingredients in more detail.\nWe have already introduced the concept of a dataset\nas a two- dimensional table (or n \u00d7 m matrix),5 where each\nrow contains the information for one example, and each\ncolumn contains the information for one of the features\nin the domain. For example, table 1.2 illustrates how the\nsample inputs and outputs of the first unknown arithmetic\n22 ChAPtER 1 function problem in the chapter can be represented as a\ndataset. This dataset contains four examples (also known\nas instances), and each example is represented using two\ninput features and one output (or target) feature. De-\nsigning and selecting the features to represent the ex-\namples is a very important step in any machine learning\nproject.\nAs is so often the case in computer science, and ma-\nchine learning, there is a tradeoff in feature selection. If\nwe choose to include only a minimal number of features\nin the dataset, then it is likely that a very informative\nfeature will be excluded from the data, and the function\nreturned by the machine learning algorithm will not work\nwell. Conversely, if we choose to include as many features\nas possible in the domain, then it is likely that irrelevant\nor redundant features will be included, and this will also\nlikely result in the function not working well. One reason\nfor this is that the more redundant or irrelevant features\nthat are included, the greater the probability for the ma-\nchine learning algorithm to extract patterns that are based\non spurious correlations between these features. In these\ncases, the algorithm gets confused between the real pat-\nterns in the data and the spurious patterns that only ap-\npear in the data due to the particular sample of examples\nthat have been included in the dataset.\nFinding the correct set of features to include in a\ndataset involves engaging with experts who understand\nIntRoduCtIon to dEEP LEARnIng 23 the domain, using statistical analysis of the distribution\nof individual features and also the correlations between\npairs of features, and a trial-a nd- error process of building\nmodels and checking the performance of the models when\nparticular features are included or excluded. This process\nof dataset design is a labor- intensive task that often takes\nup a significant portion of the time and effort expended\non a machine learning project. It is, however, a critical task\nif the project is to succeed. Indeed, identifying which fea-\ntures are informative for a given task is frequently where\nthe real value of machine learning projects emerge.\nThe second ingredient in a machine learning project is\nthe set of candidate functions that the algorithm will con-\nsider as the potential explanation of the patterns in the\ndata. In the unknown arithmetic function scenario previ-\nously given, the set of considered functions was explicitly\nspecified and restricted to four: addition, subtraction, mul-\ntiplication, or division. More generally, the set of functions\nis implicitly defined through the inductive bias of the\nTable 1.2. A simple tabular dataset\nInput 1 Input 2 Target\n5 5 25\n2 6 12\n4 4 16\n2 2 04\n24 ChAPtER 1 machine learning algorithm and the function representa-\ntion (or model) that is being used. For example, a neural\nnetwork model is a very flexible function representation.\nThe third and final ingredient to machine learning is\nthe measure of fitness. The measure of fitness is a function\nthat takes the outputs from a candidate function, gener-\nated when the machine learning algorithm applies the can-\ndidate function to the data, and compares these outputs\nwith the data, in some way. The result of this comparison\nis a value that describes the fitness of the candidate func-\ntion relative to the data. A fitness function that would\nwork for our unknown arithmetic function scenario is to\ncount in how many of the examples a candidate function\nreturns a value that exactly matches the target specified\nin the data. Multiplication would score four out of four\non this fitness measure, addition would score one out of\nfour, and division and subtraction would both score zero\nout of four. There are a large variety of fitness functions\nthat can be used in machine learning, and the selection of\nthe correct fitness function is crucial to the success of a\nmachine learning project. The design of new fitness func-\ntions is a rich area of research in machine learning. Vary-\ning how the dataset is represented, and how the candidate\nfunctions and the fitness function are defined, results in\nthree different categories of machine learning: supervised,\nunsupervised, and reinforcement learning.\nIntRoduCtIon to dEEP LEARnIng 25 Supervised, Unsupervised, and Reinforcement Learning\nSupervised machine learning is the most common type of\nmachine learning. In supervised machine learning, each\nexample in the dataset is labeled with the expected output\n(or target) value. For example, if we were using the dataset\nin table 1.1 to learn a function that maps from the inputs\nof annual income and debt to a credit solvency score, the\ncredit solvency feature in the dataset would be the target\nfeature. In order to use supervised machine learning, our\ndataset must list the value of the target feature for every\nexample in the dataset. These target feature values can\nsometimes be very difficult, and expensive, to collect. In\nsome cases, we must pay human experts to label each ex-\nample in a dataset with the correct target value. However,\nthe benefit of having these target values in the dataset is\nthat the machine learning algorithm can use these values\nto help the learning process. It does this by comparing the\noutputs a function produces with the target outputs speci-\nfied in the dataset, and using the difference (or error) to\nevaluate the fitness of the candidate function, and use the\nfitness evaluation to guide the search for the best func-\ntion. It is because of this feedback from the target labels\nin the dataset to the algorithm that this type of machine\nlearning is considered supervised. This is the type of ma-\nchine learning that was demonstrated by the example of\n26 ChAPtER 1 choosing between different arithmetic functions to ex-\nplain the behavior of an unknown function.\nUnsupervised machine learning is generally used for\nclustering data. For example, this type of data analysis\nis useful for customer segmentation, where a company\nwishes to segment its customer base into coherent groups\nso that it can target marketing campaigns and/or product\ndesigns to each group. In unsupervised machine learning,\nthere are no target values in the dataset. Consequently,\nthe algorithm cannot directly evaluate the fitness of a\ncandidate function against the target values in the dataset.\nInstead, the machine learning algorithm tries to identify\nfunctions that map similar examples into clusters, such\nthat the examples in a cluster are more similar to the other\nexamples in the same cluster than they are to examples in\nother clusters. Note that the clusters are not prespecified,\nor at most they are initially very underspecified. For ex-\nample, the data scientist might provide the algorithm with\na target number of clusters, based on some intuition about\nthe domain, without providing explicit information on\nrelative sizes of the clusters or regarding the characteris-\ntics of examples that belong in each cluster. Unsupervised\nmachine learning algorithms often begin by guessing an\ninitial clustering of the examples and then iteratively\nadjusting the clusters (by dropping instances from one\ncluster and adding them to another) so as to improve the\nfitness of the cluster set. The fitness functions used in\nIntRoduCtIon to dEEP LEARnIng 27 unsupervised machine learning generally reward candi-\ndate functions that result in higher similarity within in-\ndividual clusters and, also, high diversity between clusters.\nReinforcement learning is most relevant for online\ncontrol tasks, such as robot control and game playing. In\nthese scenarios, an agent needs to learn a policy for how it\nshould act in an environment in order to be rewarded. In\nreinforcement learning, the goal of the agent is to learn\na mapping from its current observation of the environ-\nment and its own internal state (its memory) to what\naction it should take: for instance, should the robot move\nforward or backward or should the computer program move\nthe pawn or take the queen. The output of this policy (func-\ntion) is the action that the agent should take next, given\nthe current context. In these types of scenarios, it is dif-\nficult to create historic datasets, and so reinforcement\nlearning is often carried out in situ: an agent is released\ninto an environment where it experiments with different\npolicies (starting with a potentially random policy) and\nover time updates its policy in response to the rewards it\nreceives from the environment. If an action results in a\npositive reward, the mapping from the relevant observa-\ntions and state to that action is reinforced in the policy,\nwhereas if an action results in a negative reward, the map-\nping is weakened. Unlike in supervised and unsupervised\nmachine learning, in reinforcement learning, the fact\nthat learning is done in situ means that the training and\n28 ChAPtER 1 inference stages are interleaved and ongoing. The agent\ninfers what action it should do next and uses the feedback\nfrom the environment to learn how to update its policy.\nA distinctive aspect of reinforcement learning is that the\ntarget output of the learned function (the agent\u2019s actions)\nis decoupled from the reward mechanism. The reward\nmay be dependent on multiple actions and there may be\nno reward feedback, either positive or negative, available\ndirectly after an action has been performed. For example,\nin a chess scenario, the reward may be +1 if the agent wins\nthe game and - 1 if the agent loses. However, this reward\nfeedback will not be available until the last move of the\ngame has been completed. So, one of the challenges in re-\ninforcement learning is designing training mechanisms\nthat can distribute the reward appropriately back through\na sequence of actions so that the policy can be updated\nappropriately. Google\u2019s DeepMind Technologies gener-\nated a lot of interest by demonstrating how reinforcement\nlearning could be used to train a deep learning model to\nlearn control policies for seven different Atari computer\ngames (Mnih et al. 2013). The input to the system was\nthe raw pixel values from the screen, and the control poli-\ncies specified what joystick action the agent should take at\neach point in the game. Computer game environments are\nparticularly suited to reinforcement learning as the agent\ncan be allowed to play many thousands of games against\nthe computer game system in order to learn a successful\nIntRoduCtIon to dEEP LEARnIng 29 policy, without incurring the cost of creating and labeling\na large dataset of example situations with correct joystick\nactions. The DeepMind system got so good at the games\nthat it outperformed all previous computer systems on six\nof the seven games, and outperformed human experts on\nthree of the games.\nDeep learning can be applied to all three machine\nlearning scenarios: supervised, unsupervised, and rein-\nforcement. Supervised machine learning is, however, the\nmost common type of machine learning. Consequently,\nthe majority of this book will focus on deep learning in a\nsupervised learning context. However, most of the deep\nlearning concerns and principles introduced in the super-\nvised learning context also apply to unsupervised and re-\ninforcement learning.\nWhy Is Deep Learning So Successful?\nIn any data-d riven process the primary determinant of\nsuccess is knowing what to measure and how to measure it.\nThis is why the processes of feature selection and feature\ndesign are so important to machine learning. As discussed\nabove, these tasks can require domain expertise, statis-\ntical analysis of the data, and iterations of experiments\nbuilding models with different feature sets. Consequently,\ndataset design and preparation can consume a significant\n30 ChAPtER 1 In any data- driven\nprocess the primary\ndeterminant of success\nis knowing what to\nmeasure and how to\nmeasure it. portion of time and resources expended in the project, in\nsome cases approaching up to 80% of the total budget of\na project (Kelleher and Tierney 2018). Feature design is\none task in which deep learning can have a significant ad-\nvantage over traditional machine learning. In traditional\nmachine learning, the design of features often requires a\nlarge amount of human effort. Deep learning takes a dif-\nferent approach to feature design, by attempting to auto-\nmatically learn the features that are most useful for the\ntask from the raw data.\nTo give an example of feature design, a person\u2019s body\nmass index (BMI) is the ratio of a person\u2019s weight (in ki-\nlograms) divided by their height (in meters squared). In a\nmedical setting, BMI is used to categorize people as under-\nweight, normal, overweight, or obese. Categorizing people\nin this way can be useful in predicting the likelihood of\na person developing a weight- related medical condition,\nsuch as diabetes. BMI is used for this categorization be-\ncause it enables doctors to categorize people in a manner\nthat is relevant to these weight-r elated medical condi-\ntions. Generally, as people get taller they also get heavier.\nHowever, most weight- related medical conditions (such as\ndiabetes) are not affected by a person\u2019s height but rather\nthe amount they are overweight compared to other peo-\nple of a similar stature. BMI is a useful feature to use for\nthe medical categorization of a person\u2019s weight because\nit takes the effect of height on weight into account. BMI\n32 ChAPtER 1 is an example of a feature that is derived (or calculated)\nfrom raw features; in this case the raw features are weight\nand height. BMI is also an example of how a derived fea-\nture can be more useful in making a decision than the raw\nfeatures that it is derived from. BMI is a hand- designed\nfeature: Adolphe Quetelet designed it in the eighteenth\ncentury.\nAs mentioned above, during a machine learning proj-\nect a lot of time and effort is spent on identifying, or de-\nsigning, (derived) features that are useful for the task the\nproject is trying to solve. The advantage of deep learn-\ning is that it can learn useful derived features from data\nautomatically (we will discuss how it does this in later\nchapters). Indeed, given large enough datasets, deep\nlearning has proven to be so effective in learning fea-\ntures that deep learning models are now more accurate\nthan many of the other machine learning models that use\nhand- engineered features. This is also why deep learning\nis so effective in domains where examples are described\nwith very large numbers of features. Technically datasets\nthat contain large numbers of features are called high-\ndimensional. For example, a dataset of photos with a fea-\nture for each pixel in a photo would be high-d imensional.\nIn complex high-d imensional domains, it is extremely\ndifficult to hand- engineer features: consider the chal-\nlenges of hand- engineering features for face recognition\nor machine translation. So, in these complex domains,\nIntRoduCtIon to dEEP LEARnIng 33 adopting a strategy whereby the features are automati-\ncally learned from a large dataset makes sense. Related\nto this ability to automatically learn useful features, deep\nlearning also has the ability to learn complex nonlinear\nmappings between inputs and outputs; we will explain\nthe concept of a nonlinear mapping in chapter 3, and in\nchapter 6 we will explain how these mappings are learned\nfrom data.\nSummary and the Road Ahead\nThis chapter has focused on positioning deep learning\nwithin the broader field of machine learning. Consequently,\nmuch of this chapter has been devoted to introducing ma-\nchine learning. In particular, the concept of a function as a\ndeterministic mapping from inputs to outputs was intro-\nduced, and the goal of machine learning was explained as\nfinding a function that matches the mappings from input\nfeatures to the output features that are observed in the\nexamples in the dataset.\nWithin this machine learning context, deep learn-\ning was introduced as the subfield of machine learning\nthat focuses on the design and evaluation of training\nalgorithms and model architectures for modern neural\nnetworks. One of the distinctive aspects of deep learn-\ning within machine learning is the approach it takes to\n34 ChAPtER 1 feature design. In most machine learning projects, feature\ndesign is a human-i ntensive task that can require deep\ndomain expertise and consume a lot of time and project\nbudget. Deep learning models, on the other hand, have\nthe ability to learn useful features from low-l evel raw\ndata, and complex nonlinear mappings from inputs to\noutputs. This ability is dependent on the availability of\nlarge datasets; however, when such datasets are available,\ndeep learning can frequently outperform other machine\nlearning approaches. Furthermore, this ability to learn\nuseful features from large datasets is why deep learning\ncan often generate highly accurate models for complex do-\nmains, be it in machine translation, speech processing, or\nimage or video processing. In a sense, deep learning has\nunlocked the potential of big data. The most noticeable\nimpact of this development has been the integration of\ndeep learning models into consumer devices. However,\nthe fact that deep learning can be used to analyze massive\ndatasets also has implications for our individual privacy\nand civil liberty (Kelleher and Tierney 2018). This is why\nunderstanding what deep learning is, how it works, and\nwhat it can and can\u2019t be used for, is so important. The\nroad ahead is as follows:\n\u2022 Chapter 2 introduces some of the foundational\nconcepts of deep learning, including what a model is,\nhow the parameters of a model can be set using data, and\nIntRoduCtIon to dEEP LEARnIng 35 how we can create complex models by combining simple\nmodels.\n\u2022 Chapter 3 explains what neural networks are, how\nthey work, and what we mean by a deep neural\nnetwork.\n\u2022 Chapter 4 presents a history of deep learning. This\nhistory focuses on the major conceptual and technical\nbreakthroughs that have contributed to the development\nof the field of machine learning. In particular, it provides\na context and explanation for why deep learning has seen\nsuch rapid development in recent years.\n\u2022 Chapter 5 describes the current state of the field, by\nintroducing the two deep neural architectures that are\nthe most popular today: convolutional neural networks\nand recurrent neural networks. Convolutional neural\nnetworks are ideally suited to processing image and\nvideo data. Recurrent neural networks are ideally suited\nto processing sequential data such as speech, text, or\ntime- series data. Understanding the differences and\ncommonalities across these two architectures will give\nyou an awareness of how a deep neural network can be\ntailored to the characteristics of a specific type of data,\nand also an appreciation of the breadth of the design\nspace of possible network architectures.\n36 ChAPtER 1 \u2022 Chapter 6 explains how deep neural networks\nmodels are trained, using the gradient descent and\nbackpropagation algorithms. Understanding these two\nalgorithms will give you a real insight into the state\nof artificial intelligence. For example, it will help you\nto understand why, given enough data, it is currently\npossible to train a computer to do a specific task within a\nwell- defined domain at a level beyond human capabilities,\nbut also why a more general form of intelligence is still an\nopen research challenge for artificial intelligence.\n\u2022 Chapter 7 looks to the future in the field of deep\nlearning. It reviews the major trends driving the\ndevelopment of deep learning at present, and how they\nare likely to contribute to the development of the field\nin the coming years. The chapter also discusses some of\nthe challenges the field faces, in particular the challenge\nof understanding and interpreting how a deep neural\nnetwork works.\nIntRoduCtIon to dEEP LEARnIng 37  2\nCONCEPTUAL FOUNDATIONS\nThis chapter introduces some of the foundational concepts\nthat underpin deep learning. The basis of this chapter is\nto decouple the initial presentation of these concepts from\nthe technical terminology used in deep learning, which is\nintroduced in subsequent chapters.\nA deep learning network is a mathematical model that\nis (loosely) inspired by the structure of the brain. Conse-\nquently, in order to understand deep learning it is helpful\nto have an intuitive understanding of what a mathemati-\ncal model is, how the parameters of a model can be set,\nhow we can combine (or compose) models, and how we\ncan use geometry to understand how a model processes\ninformation. What Is a Mathematical Model?\nIn its simplest form, a mathematical model is an equa-\ntion that describes how one or more input variables are\nrelated to an output variable. In this form a mathematical\nmodel is the same as a function: a mapping from inputs\nto outputs.\nIn any discussion relating to models, it is important\nto remember the statement by George Box that all models\nare wrong but some are useful! For a model to be useful it\nmust have a correspondence with the real world. This cor-\nrespondence is most obvious in terms of the meaning that\ncan be associated with a variable. For example, in isola-\ntion a value such as 78,000 has no meaning because it has\nno correspondence with concepts in the real world. But\nyearly income=$78,000 tells us how the number describes\nan aspect of the real world. Once the variables in a model\nhave a meaning, we can understand the model as describ-\ning a process through which different aspects of the world\ninteract and cause new events. The new events are then\ndescribed by the outputs of the model.\nA very simple template for a model is the equation of\na line:\ny=mx+c\nIn this equation y is the output variable, x is the input\nvariable, and m and c are two parameters of the model\n40 ChAPtER 2 that we can set to adjust the relationship the model de-\nfines between the input and the output.\nImagine we have a hypothesis that yearly income af-\nfects a person\u2019s happiness and we wish to describe the\nrelationship between these two variables.1 Using the equa-\ntion of a line, we could define a model to describe this\nrelationship as follows:\nhappiness=m\u00d7income+c\nThis model has a meaning because the variables in the\nmodel (as distinct from the parameters of the model)\nhave a correspondence with concepts from the real world.\nTo complete our model, we have to set the values of the\nmodel\u2019s parameters: m and c. Figure 2.1 illustrates how\nvarying the values of each of these parameters changes\nthe relationship defined by the model between income and\nhappiness.\nOne important thing to notice in this figure is that no\nmatter what values we set the model parameters to, the re-\nlationship defined by the model between the input and the\noutput variable can be plotted as a line. This is not surpris-\ning because we used the equation of a line as the template\nto define our model, and this is why mathematical models\nthat are based on the equation of a line are known as linear\nmodels. The other important thing to notice in the figure\nConCEPtuAL FoundAtIons 41 01\n8\n)01\nfo\ntuo( 6\nssenippaH\n4\n2\nc = 1,m = 0.08\nc = 1,m = 0.06\nc = 4,m = 0.02\n0\n0 20 40 60 80 100\nIncome ($1,000s)\nFigure 2.1 Three different linear models of how income affects happiness.\nis how changing the parameters of the model changes the\nrelationship between income and happiness.\n(c=1,m=0.08)\nThe solid steep line, with parameters ,\nis a model of the world in which people with zero income\nhave a happiness level of 1, and increases in income have\na significant effect on people\u2019s happiness. The dashed line,\nwith parameters (c=1,m=0.06), is a model in which peo-\nple with zero income have a happiness level of 1 and in-\ncreased income increases happiness, but at the slower rate\ncompared to the world modeled by the solid line. Finally,\n42 ChAPtER 2 the dotted line, parameters (c=4,m=0.02), is a model\nof the world where no one is particularly unhappy\u2014 even\npeople with zero income have a happiness of 4 out of 10\u2014\nand although increases in income do affect happiness, the\neffect is moderate. This third model assumes that income\nhas a relatively weak effect on happiness.\nMore generally, the differences between the three\nmodels in figure 2.1 show how making changes to the\nparameters of a linear model changes the model. Chang-\ning c causes the line to move up and done. This is most\nclearly seen if we focus on the y- axis: notice that the line\ndefined by a model always crosses (or intercepts) the\ny- axis at the value that c is set to. This is why the c pa-\nrameter in a linear model is known as the intercept. The\nintercept can be understood as specifying the value of the\noutput variable when the input variable is zero. Chang-\ning the m parameter changes the angle (or slope) of the\nline. The slope parameter controls how quickly changes in\nincome effect changes in happiness. In a sense, the slope\nvalue is a measure of how important income is to happi-\nness. If income is very important (i.e., if small changes in\nincome result in big changes in happiness), then the slope\nparameter of our model should be set to a large value. An-\nother way of understanding this is to think of a slope pa-\nrameter of a linear model as describing the importance, or\nweight, of the input variable in determining the value of\nthe output.\nConCEPtuAL FoundAtIons 43 Linear Models with Multiple Inputs\nThe equation of a line can be used as a template for math-\nematical models that have more than one input variable.\nFor example, imagine yourself in a scenario where you\nhave been hired by a financial institution to act as a loan\nofficer and your job involves deciding whether or not a\nloan application should be granted. From interviewing\ndomain experts you come up with a hypothesis that a use-\nful way to model a person\u2019s credit solvency is to consider\nboth their yearly income and their current debts. If we as-\nsume that there is a linear relationship between these two\ninput variables and a person\u2019s credit solvency, then the\nappropriate mathematical model, written out in English\nwould be:\nsolvency=(income\u00d7weight for income)\n+(debt\u00d7weight for debt)++intercept\nNotice that in this model the m parameter has been re-\nplaced by a separate weight for each input variable, with\neach weight representing the importance of its associated\ninput in determining the output. In mathematical nota-\ntion this model would be written as:\ny=(input \u00d7weight )+(input \u00d7weight )+c\n1 1 2 2\n44 ChAPtER 2 where y represents the credit solvency output, input rep-\n1\nresents the income variable, input represents the debt\n2\nvariable, and c represents the intercept. Using the idea of\nadding a new weight for each new input to the model al-\nlows us to scale the equation of a line to as many inputs as\nwe like. All the models defined in this way are still linear\nwithin the dimensions defined by the number of inputs\nand the output. What this means is that a linear model\nwith two inputs and one output defines a flat plane rather\nthan a line because that is what a two-d imensional line\nthat has been extruded to three dimensions looks like.\nIt can become tedious to write out a mathematical\nmodel that has a lot of inputs, so mathematicians like to\nwrite things in as compact a form as possible. With this\nin mind, the above equation is sometimes written in the\nshort form:\nn\ny=\u2211(input \u00d7weight )+c\ni i\ni=1\nThis notation tells us that to calculate the output variable\ny we must first go through all n inputs and multiple each\ninput by its corresponding weight, then we should sum\ntogether the results of these n multiplications, and finally\nwe add the c intercept parameter to the result of the sum-\nmation. The \u2211 symbol tells us that we use addition to\ncombine the results of the multiplications, and the index i\nConCEPtuAL FoundAtIons 45 tells us that we multiply each input by the weight with the\nsame index. We can make our notation even more compact\nby treating the intercept as a weight. One way to do this is\nto assume an input that is always equal to 1 and to treat\n0\nthe intercept as the weight on this input, that is, weight .\n0\nDoing this allows us to write out the model as follows:\nn\ny=\u2211(input \u00d7weight )\ni i\ni=0\nNotice that the index now starts at 0, rather than 1, be-\ncause we are now assuming an extra input, input =1, and\n0\nwe have relabeled the intercept weight .\n0\nAlthough we can write down a linear model in a num-\nber of different ways, the core of a linear model is that the\noutput is calculated as the sum of the n input values mul-\ntiplied by their corresponding weights. Consequently, this\ntype of model defines a calculation known as a weighted\nsum, because we weight each input and sum the results.\nAlthough a weighted sum is easy to calculate, it turns out\nto be very useful in many situations, and it is the basic cal-\nculation used in every neuron in a neural network.\nSetting the Parameters of a Linear Model\nLet us return to our working scenario where we wish\nto create a model that enables us to calculate the credit\n46 ChAPtER 2 The multiplication of\ninputs by weights,\nfollowed by a\nsummation, is known\nas a weighted sum. solvency of individuals who have applied for a financial\nloan. For simplicity in presentation we will ignore the\nintercept parameter in this discussion as it is treated the\nsame as the other parameters (i.e., the weights on the in-\nputs). So, dropping the intercept parameter, we have the\nfollowing linear model (or weighted sum) of the relation-\nship between a person\u2019s income and debt to their credit\nsolvency:\nsolvency=(income\u00d7weight for income)\n+(debt\u00d7weight for debt)\nIn order to complete our model, we need to specify the pa-\nrameters of the model; that is, we need to specify the value\nof the weight for each input. One way to do this would be\nto use our domain expertise to come up with values for\neach of the parameters.\nFor example, if we assume that an increase in a per-\nson\u2019s income has a bigger impact on their credit solvency\nthan a similar increase in their debt, we should set the\nweighting for income to be larger than that of the debt.\nThe following model encodes this assumption; in par-\nticular this model specifies that income is three times\nas important as debt in determining a person\u2019s credit\nsolvency:\nsolvency=(income\u00d73)+(debt\u00d71)\n48 ChAPtER 2 The drawback with using domain knowledge to set the\nparameters of a model is that experts often disagree. For\nexample, you may think that weighting income as three\ntimes as important as debt is not realistic; in that case the\nmodel can be adjusted by, for example, setting both in-\ncome and debt to have an equal weighting, which would be\nequivalent to assuming that income and debt are equally\nimportant in determining credit solvency. One way to\navoid arguments between experts is to use data to set the\nparameters. This is where machine learning helps. The\nlearning done by machine learning is finding the param-\neters (or weights) of a model using a dataset.\nLearning Model Parameters from Data\nLater in the book we will describe the standard algorithm\nused to learn the weights for a linear model, known as the\ngradient descent algorithm. However, we can give a brief\npreview of the algorithm here. We start with a dataset con-\ntaining a set of examples for which we have both the input\nvalues (income and debt) and the output value (credit sol-\nvency). Table 2.1 illustrates such a dataset from our credit\nsolvency scenario.2\nWe then begin the process of learning the weights by\nguessing initial values for each weight. It is very likely that\nthis initial, guessed, model will be a very bad model. This\nConCEPtuAL FoundAtIons 49 The learning done by\nmachine learning is\nfinding the parameters\n(or weights) of a model\nusing a dataset. Table 2.1. A dataset of loan applications and known\ncredit solvency rating of the applicant\nID Annual income Current debt Credit solvency\n1 $150 - $100 100\n2 $250 - $300 - 50\n3 $450 - $250 400\n4 $200 - $350 - 300\nis not a problem, however, because we will use the dataset\nto iteratively update the weights so that the model gets\nbetter and better, in terms of how well it matches the data.\nFor the purpose of the example, we will use the model de-\nscribed above as our initial (guessed) model:\nsolvency=(income\u00d73)+(debt\u00d71)\nThe general process for improving the weights of the\nmodel is to select an example from the dataset and feed\nthe input values from the example into the model. This\nallows us to calculate an estimate of the output value for\nthe example. Once we have this estimated output, we can\ncalculate the error of the model on the example by sub-\ntracting the estimated output from the correct output for\nthe example listed in the dataset. Using the error of the\nmodel on the example, we can improve how well the model\nConCEPtuAL FoundAtIons 51 fits the data by updating the weights in the model using\nthe following strategy, or learning rule:\n\u2022 If the error is 0, then we should not change the weights\nof the model.\n\u2022 If the error is positive, then the output of the model\nwas too low, so we should increase the output of the\nmodel for this example by increasing the weights for\nall the inputs that had positive values for the example\nand decreasing the weights for all the inputs that had\nnegative values for the example.\n\u2022 If the error is negative, then the output of the model\nwas too high, so we should decrease the output of the\nmodel for this example by decreasing the weights for\nall the inputs that had positive values for the example\nand increasing the weights for all the inputs that had\nnegative values for the example.\nTo illustrate the weight update process we will use ex-\nample 1 from table 2.1 (income = 150, debt = -1 00, and\nsolvency = 100) to test the accuracy of our guessed model\nand update the weights according to the resulting error.\nsolvency=(income\u00d73)+(debt\u00d71)\n=(150\u00d73)+(\u2212100\u00d71)\n=350\n52 ChAPtER 2 When the input values for the example are passed into\nthe model, the credit solvency estimate returned by the\nmodel is 350. This is larger than the credit solvency listed\nfor this example in the dataset, which is 100. As a result,\nthe error of the model is negative (100 \u2013 350 = \u20132 50);\ntherefore, following the learning rule described above, we\nshould decrease the output of the model for this example\nby decreasing the weights for positive inputs and increas-\ning the weights for negative inputs. For this example, the\nincome input had a positive value and the debt input had\na negative value. If we decrease the weight for income by 1\nand increase the weight for debt by 1, we end up with the\nfollowing model:\nsolvency=(income\u00d72)+(debt\u00d72)\nWe can test if this weight update has improved the\nmodel by checking if the new model generates a better\nestimate for the example than the old model. The follow-\ning illustrates pushing the same example through the new\nmodel:\nsolvency=(income\u00d72)+(debt\u00d72)\n=(150\u00d72)+(\u2212100\u00d72)\n=100\nThis time the credit solvency estimate generated by the\nmodel matches the value in the dataset, showing that the\nConCEPtuAL FoundAtIons 53 updated model fits the data more closely than the original\nmodel. In fact, this new model generates the correct out-\nput for all the examples in the dataset.\nIn this example, we only needed to update the weights\nonce in order to find a set of weights that made the be-\nhavior of the model consistent with all the examples in\nthe dataset. Typically, however, it takes many iterations\nof presenting examples and updating weights to get a\ngood model. Also, in this example, we have, for the sake\nof simplicity, assumed that the weights are updated by\neither adding or subtracting 1 from them. Generally, in\nmachine learning, the calculation of how much to update\neach weight by is more complicated than this. However,\nthese differences aside, the general process outlined here\nfor updating the weights (or parameters) of a model in or-\nder to fit the model to a dataset is the learning process at\nthe core of deep learning.\nCombining Models\nWe now understand how we can specify a linear model to\nestimate an applicant\u2019s credit solvency, and how we can\nmodify the parameters of the model in order to fit the\nmodel to a dataset. However, as a loan officer our job is\nnot simply to calculate an applicant\u2019s credit solvency; we\nhave to decide whether to grant the loan application or\nnot. In other words, we need a rule that will take a credit\n54 ChAPtER 2 solvency score as input and return a decision on the loan\napplication. For example, we might use the decision rule\nthat a person with a credit solvency above 200 will be granted\na loan. This decision rule is also a model: it maps an input\nvariable, in this case credit solvency, to an output variable,\nloan decision.\nUsing this decision rule we can adjudicate on a loan\napplication by first using the model of credit solvency to\nconvert a loan applicant\u2019s profile (described in terms of the\nannual income and debt) into a credit solvency score, and\nthen passing the resulting credit solvency score through\nour decision rule model to generate the loan decision. We\ncan write this process out in a pseudomathematical short-\nhand as follows:\nloan decision\n=decision rule(solvency=(income\u00d72)+(debt\u00d72))\nUsing this notation, the entire decision process for ad-\njudicating the loan application for example 1 from\ntable 2.1 is:\nloan decision\n=decision rule(solvency=(income\u00d72)+(debt\u00d72))\n=decision rule(solvency=(150\u00d72)+(\u2212100\u00d72))\n=decision rule(solvency=100)\n=reject\nConCEPtuAL FoundAtIons 55 We are now in a position where we can use a model\n(composed of two simpler models, a decision rule and a\nweighted sum) to describe how a loan decision is made.\nWhat is more, if we use data from previous loan applica-\ntions to set the parameters (i.e., the weights) of the model,\nour model will correspond to how we have processed pre-\nvious loan applications. This is useful because we can use\nthis model to process new applications in a way that is\nconsistent with previous decisions. If a new loan applica-\ntion is submitted, we simply use our model to process the\napplication and generate a decision. It is this ability to\napply a mathematical model to new examples that makes\nmathematical modeling so useful.\nWhen we use the output of one model as the input\nto another model, we are creating a third model by com-\nbining two models. This strategy of building a complex\nmodel by combining smaller simpler models is at the core\nof deep learning networks. As we will see, a neural net-\nwork is composed of a large number of small units called\nneurons. Each of these neurons is a simple model in its\nown right that maps from a set of inputs to an output.\nThe overall model implemented by the network is cre-\nated by feeding the outputs from one group of neurons as\ninputs into a second group of neurons and then feeding\nthe outputs of the second group of neurons as inputs to\na third group of neurons, as so on, until the final output\nof the model is generated. The core idea is that feeding\n56 ChAPtER 2 the outputs of some neuron as inputs to other neurons\nenables these subsequent neurons to learn to solve a dif-\nferent part of the overall problem the network is trying to\nsolve by building on the partial solutions implemented by\nthe earlier neurons\u2014 in a similar way to the way the deci-\nsion rule generates the final adjudication for a loan appli-\ncation by building on the calculation of the credit solvency\nmodel. We will return to this topic of model composition\nin subsequent chapters.\nInput Spaces, Weight Spaces, and Activation Spaces\nAlthough mathematical models can be written out as\nequations, it is often useful to understand the geomet-\nric meaning of a model. For example, the plots in figure\n2.1 helped us understand how changes in the parameters\nof a linear model changed the relationship between the\nvariables that the model defined. There are a number of\ngeometric spaces that it is useful to distinguish between,\nand understand, when we are discussing neural networks.\nThese are the input space, the weight space, and the activa-\ntion space of a neuron. We can use the decision model for\nloan applications that we defined in the previous section\nto explain these three different types of spaces.\nWe will begin by describing the concept of an input\nspace. Our loan decision model took two inputs: the\nConCEPtuAL FoundAtIons 57 annual income and current debt of the applicant. Table\n2.1 listed these input values for four example loan applica-\ntions. We can plot the input space of this model by treating\neach of the input variables as the axis of a coordinate sys-\ntem. This coordinate space is referred to as the input space\nbecause each point in this space defines a possible com-\nbination of input values to the model. For example, the\nplot at the top-l eft of figure 2.2 shows the position of each\nof the four example loan applications within the models\ninput space.\nThe weight space for a model describes the universe of\npossible weight combinations that a model might use. We\ncan plot the weight space for a model by defining a coor-\ndinate system with one axis per weight in the model. The\nloan decision model has only two weights, one weight for\nthe annual income input, and one weight for the current\ndebt input. Consequently, the weight space for this model\nhas two dimensions. The plot at the top-r ight of figure\n2.2 illustrates a portion of the weight space for this model.\nThe location of the weight combination used by the model\n2,2 is highlighted in this figure. Each point within this co-\nordinate system describes a possible set of weights for the\nmodel, and therefore corresponds to a different weighted\nsum function within the model. Consequently, moving\nfrom one location to another within this weight space is\nequivalent to changing the model because it changes the\nmapping from inputs to output that the model defines.\n58 ChAPtER 2 Figure 2.2 There are four different coordinate spaces related to the\nprocessing of the loan decision model: top-l eft plots the input space; top-r ight\nplots the weight space; bottom-l eft plots the activation (or decision) space;\nand bottom-r ight plots the input space with the decision boundary plotted.\nConCEPtuAL FoundAtIons 59 A linear model maps a set of input values to a point\nin a new space by applying a weighted sum calculation to\nthe inputs: multiply each input by a weight, and sum the\nresults of the multiplication. In our loan decision model it\nis in this space that we apply our decision rule. Thus, we\ncould call this space the decision space, but, for reasons\nthat will become clear when we describe the structure of\na neuron in the next chapter, we call this space the activa-\ntion space. The axes of a model\u2019s activation space corre-\nspond to the weighted inputs to the model. Consequently,\neach point in the activation space defines a set of weighted\ninputs. Applying a decision rule, such as our rule that a\nperson with a credit solvency above 200 will be granted a loan,\nto each point in this activation space, and recording the\nresult of the decision for each point, enables us to plot the\ndecision boundary of the model in this space. The decision\nboundary divides those points in the activation space that\nexceed the threshold, from those points in the space below\nthe threshold. The plot in the bottom-l eft of figure 2.2 il-\nlustrates the activation space for our loan decision model.\nThe positions of the four example loan applications listed\nin table 2.1 when they are projected into this activation\nspace are shown. The diagonal black line in this figure\nshows the decision boundary. Using this threshold, loan\napplication number three is granted and the other loan\napplications are rejected. We can, if we wish, project the\ndecision boundary back into the original input space by\n60 ChAPtER 2 recording for each location in the input space which side of\nthe decision boundary in the activation space it is mapped\nto by the weighted sum function. The plot at the bottom-\nright of figure 2.2 shows the decision boundary in the\noriginal input space (note the change in the values on the\naxes) and was generated using this process. We will return\nto the concepts of weight spaces and decision boundar-\nies in next chapter when we describe how adjusting the\nparameters of a neuron changes the set of input combina-\ntions that cause the neuron to output a high activation.\nSummary\nThe main idea presented in this chapter is that a linear\nmathematical model, be it expressed as an equation or\nplotted as a line, describes a relationship between a set of\ninputs and an output. Be aware that not all mathematical\nmodels are linear models, and we will come across nonlin-\near models in this book. However, the fundamental cal-\nculation of a weighted sum of inputs does define a linear\nmodel. Another big idea introduced in this chapter is that\na linear model (a weighted sum) has a set of parameters,\nthat is, the weights used in the weighted sum. By chang-\ning these parameters we can change the relationship the\nmodel describes between the inputs and the output. If we\nwish we could set these weights by hand using our domain\nConCEPtuAL FoundAtIons 61 expertise; however, we can also use machine learning to\nset the weights of the model so that the behavior of the\nmodel fits the patterns found in a dataset. The last big\nidea introduced in this chapter was that we can build com-\nplex models by combining simpler models. This is done by\nusing the output from one (or more) models as input(s)\nto another model. We used this technique to define our\ncomposite model to make loan decisions. As we will see in\nthe next chapter, the structure of a neuron in a neural net-\nwork is very similar to the structure of this loan decision\nmodel. Just like this model, a neuron calculates a weighted\nsum of its inputs and then feeds the result of this calcula-\ntion into a second model that decides whether the neuron\nactivates or not.\nThe focus of this chapter has been to introduce some\nfoundational concepts before we introduce the terminol-\nogy of machine learning and deep learning. To give a quick\noverview of how the concepts introduced in this chapter\nmap over to machine learning terminology, our loan deci-\nsion model is equivalent to a two- input neuron that uses\na threshold activation function. The two financial indica-\ntors (annual income and current debt) are analogous to\nthe inputs the neuron receives. The terms input vector or\nfeature vector are sometimes used to refer to the set of in-\ndicators describing a single example; in this context an ex-\nample is a single loan applicant, described in terms of two\nfeatures: annual income and current debt. Also, just like\n62 ChAPtER 2 the loan decision model, a neuron associates a weight with\neach input. And, again, just like the loan decision model, a\nneuron multiplies each input by its associated weight and\nsums the results of these multiplications in order to calcu-\nlate an overall score for the inputs. Finally, similar to the\nway we applied a threshold to the credit solvency score to\nconvert it into a decision of whether to grant or reject the\nloan application, a neuron applies a function (known as\nan activation function) to convert the overall score of the\ninputs. In the earliest types of neurons, these activation\nfunctions were actually threshold functions that worked\nin exactly the same way as the score threshold used in this\ncredit scoring example. In more recent neural networks,\ndifferent types of activation functions (for example, the\nlogistic, tanh, or ReLU functions) are used. We will intro-\nduce these activation functions in the next chapter.\nConCEPtuAL FoundAtIons 63  3\nNEURAL NETWORKS:\nTHE BUILDING BLOCKS\nOF DEEP LEARNING\nThe term deep learning describes a family of neural network\nmodels that have multiple layers of simple information\nprocessing programs, known as neurons, in the network.\nThe focus of this chapter is to provide a clear and com-\nprehensive introduction to how these neurons work and\nare interconnected in artificial neural networks. In later\nchapters, we will explain how neural networks are trained\nusing data.\nA neural network is a computational model that is in-\nspired by the structure of the human brain. The human\nbrain is composed of a massive number of nerve cells,\ncalled neurons. In fact, some estimates put the number\nof neurons in the human brain at one hundred billion\n(Herculano- Houzel 2009). Neurons have a simple three-\npart structure consisting of: a cell body, a set of fibers\ncalled dendrites, and a single long fiber called an axon. Figure 3.1 illustrates the structure of a neuron and how\nit connects to other neurons in the brain. The dendrites\nand the axon stem from the cell body, and the dendrites of\none neuron are connected to the axons of other neurons.\nThe dendrites act as input channels to the neuron and re-\nceive signals sent from other neurons along their axons.\nThe axon acts as the output channel of a neuron, and so\nother neurons, whose dendrites are connected to the axon,\nreceive the signals sent along the axon as inputs.\nNeurons work in a very simple manner. If the incom-\ning stimuli are strong enough, the neuron transmits an\nelectrical pulse, called an action potential, along its axon\nto the other neurons that are connected to it. So, a neuron\nacts as an all-o r- none switch, that takes in a set of inputs\nand either outputs an action potential or no output.\nThis explanation of the human brain is a significant\nsimplification of the biological reality, but it does capture\nFigure 3.1 The structure of a neuron in the brain.\n66 ChAPtER 3 the main points necessary to understand the analogy\nbetween the structure of the human brain and compu-\ntational models called neural networks. These points of\nanalogy are: (1) the brain is composed of a large number\nof interconnected and simple units called neurons; (2) the\nfunctioning of the brain can be understood as processing\ninformation, encoded as high or low electrical signals, or\nactivation potentials, that spread across the network of\nneurons; and (3) each neuron receives a set of stimuli from\nits neighbors and maps these inputs to either a high- or\nlow- value output. All computational models of neural net-\nworks have these characteristics.\nArtificial Neural Networks\nAn artificial neural network consists of a network of\nsimple information processing units, called neurons. The\npower of neural networks to model complex relationships\nis not the result of complex mathematical models, but\nrather emerges from the interactions between a large set\nof simple neurons.\nFigure 3.2 illustrates the structure of a neural net-\nwork. It is standard to think of the neurons in a neural net-\nwork as organized into layers. The depicted network has\nfive layers: one input layer, three hidden layers, and one\noutput layer. A hidden layer is just a layer that is neither\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 67 the input nor the output layer. Deep learning networks\nare neural networks that have many hidden layers of neu-\nrons. The minimum number of hidden layers necessary to\nbe considered deep is two. However, most deep learning\nnetworks have many more than two hidden layers. The\nimportant point is that the depth of a network is mea-\nsured in terms of the number of hidden layers, plus the\noutput layer.\nIn figure 3.2, the squares in the input layer represent\nlocations in memory that are used to present inputs to\nthe network. These locations can be thought of as sensing\nneurons. There is no processing of information in these\nsensing neurons; the output of each of these neurons is\nsimply the value of the data stored at the memory location.\nInput Hidden Hidden Hidden Output\nlayer layer1 layer2 layer3 layer4\nFigure 3.2 Topological illustration of a simple neural network.\n68 ChAPtER 3 Deep learning networks\nare neural networks that\nhave many hidden layers\nof neurons. The circles in the figure represent the information proc-\nessing neurons in the network. Each of these neurons\ntakes a set of numeric values as input and maps them to\na single output value. Each input to a processing neuron\nis either the output of a sensing neuron or the output of\nanother processing neuron.\nThe arrows in figure 3.2 illustrate how information\nflows through the network from the output of one neu-\nron to the input of another neuron. Each connection in\na network connects two neurons and each connection is\ndirected, which means that information carried along a\nconnection only flows in one direction. Each of the con-\nnections in a network has a weight associated with it. A\nconnection weight is simply a number, but these weights\nare very important. The weight of a connection affects\nhow a neuron processes the information it receives along\nthe connection, and, in fact, training an artificial neural\nnetwork, essentially, involves searching for the best (or\noptimal) set of weights.\nHow an Artificial Neuron Processes Information\nThe processing of information within a neuron, that is,\nthe mapping from inputs to an output, is very similar\nto the loan decision model that we developed in chapter\n2. Recall that the loan decision model first calculated a\n70 ChAPtER 3 weighted sum over the input features (income and debt).\nThe weights used in the weighted sum were adjusted using\na dataset so that the results of the weighted sum calcula-\ntion, given an loan applicant\u2019s income and debt as inputs,\nwas an accurate estimate of the applicant\u2019s credit solvency\nscore. The second stage of processing in the loan decision\nmodel involved passing the result of the weighted sum\ncalculation (the estimated credit solvency score) through\na decision rule. This decision rule was a function that\nmapped a credit solvency score to a decision on whether a\nloan application was granted or rejected.\nA neuron also implements a two- stage process to map\ninputs to an output. The first stage of processing involves\nthe calculation of a weighted sum of the inputs to the neu-\nron. Then the result of the weighted sum calculation is\npassed through a second function that maps the results of\nthe weighted sum score to the neuron\u2019s final output value.\nWhen we are designing a neuron, we can used many differ-\nent types of functions for this second stage or processing;\nit may be as simple as the decision rule we used for our\nloan decision model, or it may be more complex. Typically\nthe output value of a neuron is known as its activation\nvalue, so this second function, which maps from the result\nof the weighted sum to the activation value of the neuron,\nis known as an activation function.\nFigure 3.3 illustrates how these stages of processing\nare reflected in the structure of an artificial neuron. In\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 71 x\n1 w\n1\nx 2 w 2\nx 3 w 3 \u03a3\u03d5 Output\nw 4\nx\n4\n.\n. wn\n.\nx\nn\nFigure 3.3 The structure of an artificial neuron.\nfigure 3.3, the \u03a3 symbol represents the calculation of the\nweighted sum, and the \u03c6 symbol represents the activation\nfunction processing the weighted sum and generating the\noutput from the neuron.\nThe neuron in figure 3.3 receives n inputs [x ,\u2026,x ]\n1 n\non n different input connections, and each connection has\nan associated weight [w ,\u2026,w ]. The weighted sum cal-\n1 n\nculation involves the multiplication of inputs by weights\nand the summation of the resulting values. Mathemati-\ncally this calculation is written as:\nz=(x \u00d7w )+(x \u00d7w )+\u2026+(x \u00d7w )\n1 1 2 2 n n\nThis calculation can also be written in a more compact\nmathematical form as:\nz=\u03a3n x \u00d7w\ni=1 i i\n72 ChAPtER 3 For example, assuming a neuron received the inputs\n[x =3,x =9] and had the following weights [w =\u22123,\n1 2 1\nw =1], the weighted sum calculation would be:\n2\nz=(3\u00d7\u22123)+(9\u00d71)\n=0\nThe second stage of processing within a neuron is to\npass the result of the weighted sum, the z value, through\nan activation function. Figure 3.4 plots the shape of a num-\nber of possible activation functions, as the input to each\nfunction, z, ranges across an interval, either [- 1, ..., +1] or\n[- 10, ..., +10] depending on which interval best illustrates\nthe shape of the function. Figure 3.4 (top) plots a thresh-\nold activation function. The decision rule we used in the\nloan decision model was an example of a threshold func-\ntion; the threshold used in that decision rule was whether\nthe credit solvency score was above 200. Threshold acti-\nvations were common in early neural network research.\nFigure 3.4 (middle) plots the logistic and tanh activation\nfunctions. The units employing these activation functions\nwere popular in multilayer networks until quite recently.\nFigure 3.4 (bottom) plots the rectifier (or hinge, or posi-\ntive linear) activation function. This activation function is\nvery popular in modern deep learning networks; in 2011\nthe rectifier activation function was shown to enable bet-\nter training in deep networks (Glorot et al. 2011). In fact,\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 73 .noitcnuf\nraenil\ndeifitcer\n:mottob\n;snoitcnuf\nhnat\ndna\ncitsigol\n:elddim\n;noitcnuf\ndlohserht\n:poT\n4.3\nerugiF as will be discussed in chapter 4, during the review of the\nhistory of deep learning, one of the trends in neural net-\nwork research has been a shift from threshold activation\nto logistic and tanh activations, and then onto rectifier\nactivation functions.\nReturning to the example, the result of the weighted\nsummation step was z=0. Figure 3.4 (middle plot, solid\nline) plots the logistic function. Assuming that the neuron\nis using a logistic activation function, this plot shows how\nthe result of the summation will be mapped to an output\nactivation: logistic(0)=0.5. The calculation of the output\nactivation of this neuron can be summarized as:\n( n )\nOutput=activation_function z= \u03a3x \u00d7w\ni i\ni=1\n=logistic(z=(3\u00d7\u22123)+(9\u00d71))\n=logistic(z=0)\n=0.5\nNotice that the processing of information in this neuron\nis nearly identical to the processing of information in the\nloan decision model we developed in the last chapter. The\nmajor difference is that we have replaced the decision\nthreshold rule that mapped the weighted sum score to an\naccepted or rejected output with a logistic function that\nmaps the weighted sum score to a value between 0 and 1.\nDepending on the location of this neuron in the network,\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 75 the output activation of the neuron, in this instance\ny=0.5, will either be passed as input to one or more neu-\nrons in the next layer in the network, or will be part of\nthe overall output of the network. If a neuron is at the\noutput layer, the interpretation of what its output value\nmeans would be dependent on the task that the neuron\nis designed to model. If a neuron is in one of the hidden\nlayers of the network, then it may not be possible to put\na meaningful interpretation on the output of the neuron\napart from the general interpretation that it represents\nsome sort of derived feature (similar to the BMI feature we\ndiscussed in chapter 1) that the network has found useful\nin generating its outputs. We will return to the challenge\nof interpreting the meaning of activations within a neural\nnetwork in chapter 7.\nThe key point to remember from this section is that\na neuron, the fundamental building block of neural net-\nworks and deep learning, is defined by a simple two- step\nsequence of operations: calculating a weighted sum and\nthen passing the result through an activation function.\nFigure 3.4 illustrates that neither the tanh nor the\nlogistic function is a linear function. In fact, the plots of\nboth of these functions have a distinctive s- shaped (rather\nthan linear) profile. Not all activation functions have an\ns- shape (for example, the threshold and rectifier are not\ns- shaped), but all activation functions do apply a nonlin-\near mapping to the output of the weighted sum. In fact,\n76 ChAPtER 3 it is the introduction of the nonlinear mapping into the\nprocessing of a neuron that is the reason why activation\nfunctions are used.\nWhy Is an Activation Function Necessary?\nTo understand why a nonlinear mapping is needed in a\nneuron, it is first necessary to understand that, essentially,\nall a neural network does is define a mapping from inputs\nto outputs, be it from a game position in Go to an evalu-\nation of that position, or from an X-r ay to a diagnosis of\na patient. Neurons are the basic building blocks of neural\nnetworks, and therefore they are the basic building blocks\nof the mapping a network defines. The overall mapping\nfrom inputs to outputs that a network defines is com-\nposed of the mappings from inputs to outputs that each of\nthe neurons within the network implement. The implica-\ntion of this is that if all the neurons within a network were\nrestricted to linear mappings (i.e., weighted sum calcula-\ntions), the overall network would be restricted to a linear\nmapping from inputs to outputs. However, many of the re-\nlationships in the world that we might want to model are\nnonlinear, and if we attempt to model these relationships\nusing a linear model, then the model will be very inaccu-\nrate. Attempting to model a nonlinear relationship with\na linear model would be an example of the underfitting\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 77 problem we discussed in chapter 1: underfitting occurs\nwhen the model used to encode the patterns in a dataset\nis too simple and as a result it is not accurate.\nA linear relationship exists between two things when\nan increase in one always results in an increase or decrease\nin the other at a constant rate. For example, if an employee\nis on a fixed hourly rate, which does not vary at weekends\nor if they do overtime, then there is a linear relationship\nbetween the number of hours they work and their pay. A\nplot of their hours worked versus their pay will result in\na straight line; the steeper the line the higher their fixed\nhourly rate of pay. However, if we make the payment sys-\ntem for our hypothetical employee just slightly more com-\nplex, by, for example, increasing their hourly rate of pay\nwhen they do overtime or work weekends, then the rela-\ntionship between the number of hours they work and their\npay is no longer linear. Neural networks, and in particular\ndeep learning networks, are typically used to model rela-\ntionships that are much more complex than this employ-\nee\u2019s pay. Modeling these relationships accurately requires\nthat a network be able to learn and represent complex\nnonlinear mappings. So, in order to enable a neural net-\nwork to implement such nonlinear mappings, a nonlinear\nstep (the activation function) must be included within the\nprocessing of the neurons in the network.\nIn principle, using any nonlinear function as an activa-\ntion function enables a neural network to learn a nonlinear\n78 ChAPtER 3 mapping from inputs to outputs. However, as we shall see\nlater, most of the activation functions plotted in figure 3.4\nhave nice mathematical properties that are helpful when\ntraining a neural network, and this is why they are so pop-\nular in neural network research.\nThe fact that the introduction of a nonlinearity into\nthe processing of the neurons enables the network to\nlearn a nonlinear mapping between input(s) and output\nis another illustration of the fact that the overall behav-\nior of the network emerges from the interactions of the\nprocessing carried out by individual neurons within the\nnetwork. Neural networks solve problems using a divide-\nand- conquer strategy: each of the neurons in a network\nsolves one component of the larger problem, and the\noverall problem is solved by combining these component\nsolutions. An important aspect of the power of neural\nnetworks is that during training, as the weights on the\nconnections within the network are set, the network is\nin effect learning a decomposition of the larger problem,\nand the individual neurons are learning how to solve and\ncombine solutions to the components within this problem\ndecomposition.\nWithin a neural network, some neurons may use dif-\nferent activation functions from other neurons in the net-\nwork. Generally, however, all the neurons within a given\nlayer of a network will be of the same type (i.e., they will\nall use the same activation function). Also, sometimes\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 79 neurons are referred to as units, with a distinction made\nbetween units based on the activation function the units\nuse: neurons that use a threshold activation function are\nknown as threshold units, units that use a logistic acti-\nvation function are known as logistic units, and neurons\nthat use the rectifier activation function are known as\nrectified linear units, or ReLUs. For example, a network\nmay have a layer of ReLUs connected to a layer of logistic\nunits. The decision regarding which activation functions\nto use in the neurons in a network is made by the data\nscientist who is designing the network. To make this deci-\nsion, a data scientist may run a number of experiments\nto test which activation functions give the best perfor-\nmance on a dataset. However, frequently data scientists\ndefault to using whichever activation function is popular\nat a given point. For example, currently ReLUs are the\nmost popular type of unit in neural networks, but this\nmay change as new activation functions are developed and\ntested. As we will discuss at the end of this chapter, the\nelements of a neural network that are set manually by the\ndata scientist prior to the training process are known as\nhyperparameters.\nThe term hyperparameter is used to describe the\nmanually fixed parts of the model in order to distinguish\nthem from the parameters of the model, which are the\nparts of the model that are set automatically, by the ma-\nchine learning algorithm, during the training process. The\n80 ChAPtER 3 Neural networks solve\nproblems using a divide-\nand- conquer strategy:\neach of the neurons in\na network solves one\ncomponent of the larger\nproblem, and the overall\nproblem is solved by\ncombining these\ncomponent solutions. parameters of a neural network are the weights used in\nthe weighted sum calculations of the neurons in the net-\nwork. As we touched on in chapters 1 and 2, the standard\ntraining process for setting the parameters of a neural\nnetwork is to begin by initializing the parameters (the\nnetwork\u2019s weights) to random values, and during train-\ning to use the performance of the network on the dataset\nto slowly adjust these weights so as to improve the ac-\ncuracy of the model on the data. Chapter 6 describes the\ntwo algorithms that are most commonly used to train a\nneural network: the gradient descent algorithm and the\nbackpropagation algorithm. What we will focus on next\nis understanding how changing the parameters of a neu-\nron affects how the neuron responds to the inputs it\nreceives.\nHow Does Changing the Parameters of a Neuron Affect\nIts Behavior?\nThe parameters of a neuron are the weights the neuron\nuses in the weighted sum calculation. Although the\nweighted sum calculation in a neuron is the same weighted\nsum used in a linear model, in a neuron the relationship\nbetween the weights and the final output of neuron is\nmore complex because the result of the weighted sum\nis passed through an activation function in order to\n82 ChAPtER 3 generate the final output. To understand how a neuron\nmakes a decision on a given input, we need to understand\nthe relationship between the neuron\u2019s weights, the input\nit receives, and the output it generates in response.\nThe relationship between a neuron\u2019s weights and the\noutput it generates for a given input is most easily under-\nstood in neurons that use a threshold activation function.\nA neuron using this type of activation function is equiva-\nlent to our loan decision model that used a decision rule\nto classify the credit solvency scores, generated by the\nweighted sum calculation, to reject or grant loan applica-\ntions. At the end of chapter 2, we introduced the concepts\nof an input space, a weight space, and an activation space\n(see figure 2.2). The input space for our two- input loan\ndecision model could be visualized as a two- dimensional\nspace, with one input (annual income) plotted along the x-\naxis, and the other input (current debt) on the y- axis. Each\npoint in this plot defined a potential combination of in-\nputs to the model, and the set of points in the input space\ndefines the set of possible inputs the model could process.\nThe weights used in the loan decision model can be un-\nderstood as dividing the input space into two regions: the\nfirst region contains all of the inputs that result in the loan\napplication being granted, and the other region contains\nall the inputs that result in the loan application being re-\njected. In that scenario, changing the weights used by the\ndecision model would change the set of loan applications\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 83 that were accepted or rejected. Intuitively, this makes\nsense because it changes the weighting that we put on an\napplicant\u2019s income relative to their debt when we are de-\nciding on granting the loan or not.\nWe can generalize the above analysis of the loan deci-\nsion model to a neuron in a neural network. The equivalent\nneuron structure to the loan decision model is a two-i nput\nneuron with a threshold activation function. The input\nspace for such a neuron has a similar structure to the in-\nput space for a loan decision model. Figure 3.5 presents\nthree plots of the input space for a two-i nput neuron us-\ning a threshold function that outputs a high activation if\nthe weighted sum result is greater than zero, and a low\nactivation otherwise. The differences between each of the\nplots in this figure is that the neuron defines a different\ndecision boundary in each case. In each plot, the decision\nboundary is marked with a black line.\nEach of the plots in figure 3.5 was created by first fix-\ning the weights of the neuron and then for each point in\nthe input space recording whether the neuron returned a\nhigh or low activation when the coordinates of the point\nwere used as the inputs to the neuron. The input points for\nwhich the neuron returned a high activation are plotted in\ngray, and the other points are plotted in white. The only\ndifference between the neurons used to create these plots\nwas the weights used in calculating the weighted sum of\nthe inputs. The arrow in each plot illustrates the weight\n84 ChAPtER 3 :mottob\n;]1=2w\n,2\n-=1w[\nrotcev\nthgiew\n:elddim\n;]1=2w\n,1=1w[\nrotcev\nthgiew\n:poT\n.noruen\ntupni\n-owt\na\nrof\nseiradnuob\n.]2\n-=2w\nnoisiceD ,1=1w[\nrotcev\n5.3\nerugiF thgiew vector used by the neuron to generate the plot. In this\ncontext, a vector describes the direction and distance of\na point from the origin.1 As we shall see, interpreting the\nset of weights used by a neuron as defining a vector (an\narrow from the origin to the coordinates of the weights)\nin the neuron\u2019s input space is useful in understanding how\nchanges in the weights change the decision boundary of\nthe neuron.\nThe weights used to create each plot change from one\nplot to the next. These changes are reflected in the direc-\ntion of the arrow (the weight vector) in each plot. Spe-\ncifically, changing the weights rotates the weight vector\naround the origin. Notice that the decision boundary in\neach plot is sensitive to the direction of the weight vector:\nin all the plots, the decision boundary is orthogonal (i.e.,\nat a right, or 90\u00b0, angle) to the weight vector. So, chang-\ning the weights not only rotates the weight vector, it also\nrotates the decision boundary of the neuron. This rotation\nchanges the set of inputs that the neuron outputs a high\nactivation in response to (the gray regions).\nTo understand why this decision boundary is always\northogonal to the weight vector, we have to shift our per-\nspective, for a moment, to linear algebra. Remember that\nevery point in the input space defines a potential combi-\nnation of input values to the neuron. Now, imagine each\nof these sets of input values as defining an arrow from the\norigin to the coordinates of the point in the input space.\n86 ChAPtER 3 There is one arrow for each point in the input space. Each\nof these arrows is very similar to the weight vector, ex-\ncept that it points to the coordinates of the inputs rather\nthan to the coordinates of the weights. When we treat a\nset of inputs as a vector, the weighted sum calculation is\nthe same as multiplying two vectors, the input vector by\nthe weight vector. In linear algebra terminology, multi-\nplying two vectors is known as the dot product operation.\nFor the purposes of this discussion, all we need to know\nabout the dot product is that the result of this operation\nis dependent on the angle between the two vectors that\nare multiplied. If the angle between the two vectors is less\nthan a right angle, then the result will be positive; other-\nwise, it will be negative. So, multiplying the weight vec-\ntor by an input vector will return a positive value for all\nthe input vectors at an angle less than a right angle to the\nweight vector, and a negative value for all the other vec-\ntors. The activation function used by this neuron returns\na high activation when positive values are input and a low\nactivation when negative values are input. Consequently,\nthe decision boundary lies at a right angle to the weight\nvector because all the inputs at an angle less than a right\nangle to the weight vector will result in a positive input\nto the activation function and, therefore, trigger a high-\noutput activation from the neuron; conversely, all the\nother inputs will result in a low- output activation from\nthe neuron.\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 87 Switching back to the plots in figure 3.5, although the\ndecision boundaries in each of the plots are at different\nangles, all the decision boundaries go through the point in\nspace that the weight vectors originate from (i.e., the ori-\ngin). This illustrates that changing the weights of a neuron\nrotates the neuron\u2019s decision boundary but does not trans-\nlate it. Translating the decision boundary means moving\nthe decision boundary up and down the weight vector, so\nthat the point where it meets the vector is not the origin.\nThe restriction that all decision boundaries must pass\nthrough the origin limits the distinctions that a neuron\ncan learn between input patterns. The standard way to\novercome this limitation is to extend the weighted sum\ncalculation so that it includes an extra element, known as\nthe bias term. This bias term is not the same as the induc-\ntive bias we discussed in chapter 1. It is more analogous\nto the intercept parameter in the equation of a line, which\nmoves the line up and down the y- axis. The purpose of this\nbias term is to move (or translate) the decision boundary\naway from the origin.\nThe bias term is simply an extra value that is included\nin the calculation of the weighted sum. It is introduced\ninto the neuron by adding the bias to the result of the\nweighted summation prior to passing it through the ac-\ntivation function. Here is the equation describing the\nprocessing stages in a neuron with the bias term repre-\nsented by the term b:\n88 ChAPtER 3 \uf8eb \uf8f6\n\uf8ec \uf8eb n \uf8f6 \uf8f7\nOutput=activation_function \uf8ecz= \uf8ed\uf8ec\u2211x i\u00d7w i\uf8f8\uf8f7+ (cid:27)b\n\uf8f7\n\uf8ec (cid:31)i=(cid:29)(cid:29)1 (cid:30)(cid:29)(cid:28) bias\uf8f7\n\uf8ed \uf8f8\nweighted sum\nFigure 3.6 illustrates how the value of the bias term affects\nthe decision boundary of a neuron. When the bias term is\nnegative, the decision boundary is moved away from the\norigin in the direction that the weight vector points to (as\nin the top and middle plots in figure 3.6); when the bias\nterm is positive, the decision boundary is translated in the\nopposite direction (see the bottom plot of figure 3.6). In\nboth cases, the decision boundary remains orthogonal to\nthe weight vector. Also, the size of the bias term affects\nthe amount the decision boundary is moved from the ori-\ngin; the larger the value of the bias term, the more the de-\ncision boundary is moved (compare the top plot of figure\n3.6 with the middle and bottom plots).\nInstead of manually setting the value of the bias term,\nit is preferable to allow a neuron to learn the appropriate\nbias. The simplest way to do this is to treat the bias term as\na weight and allow the neuron to learn the bias term at the\nsame time that it is learning the rest of the weights for its\ninputs. All that is required to achieve this is to augment all\nthe input vectors the neuron receives with an extra input\nthat is always set to 1. By convention, this input is input\n0 (x =1), and, consequently, the bias term is specified by\n0\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 89 ,1=1w[\n:poT\nrotcev\n.yradnuob\nthgiew\nnoisiced\n:mottob\neht\n;2\nno -\not\nmret lauqe\nsaib\nsaib\neht\ndna\nfo\n]1=2w\ntceffe\neht ,2\n-=1w[\netartsulli\nrotcev\ntaht\nthgiew\nnoruen\n:elddim\ntupni\n;1\n-owt -\not\na lauqe\nrof\nstolp saib\nyradnuob dna\n]1=2w\n.2\not\nnoisiceD ,1=1w[ lauqe\nsaib\nrotcev\ndna\n6.3\nerugiF thgiew ]2\n-=2w weight 0 (w ).2 Figure 3.7 illustrates the structure of an\n0\nartificial neuron when the bias term has been integrated\nas w .\n0\nWhen the bias term has been integrated into the\nweights of a neuron, the equation specifying the map-\nping from input(s) to output activation of the neuron can\nbe simplified (at least from a notational perspective) as\nfollows:\n\uf8eb n \uf8f6\nOutput=activation_function \uf8ed\uf8ecz=\u2211x i\u00d7w i\uf8f8\uf8f7\ni=0\nNotice that in this equation the index i goes from 0 to\nn, so that it now includes the fixed input, x =1, and the\n0\nbias term, w ; in the earlier version of this equation, the\n0\nindex only went from 1 to n. This new format means that\nthe neuron is able to learn the bias term, simply by learn-\ning the appropriate weight w , using the same process\n0\nx 1 w x 0 =1\n1\nx 2 w 2 w 0(originallyb)\nx 3 w 3 \u03a3\u03d5 Output\nw 4\nx4\n.\n. wn\n.\nx\nn\nFigure 3.7 An artificial neuron with a bias term included as w.\n0\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 91 that is used to learn the weights for the other inputs: at\nthe start of training, the bias term for each neuron in the\nnetwork will be initialized to a random value and then ad-\njusted, along with the weights of the network, in response\nto the performance of the network on the dataset.\nAccelerating Neural Network Training Using GPUs\nMerging the bias term is more than a notational conve-\nnience; it enables us to use specialized hardware to accel-\nerate the training of neural networks. The fact that a bias\nterm can be treated as the same as a weight means that the\ncalculation of the weighted sum of inputs (including the\naddition of the bias term) can be treated as the multipli-\ncation of two vectors. As we discussed earlier, during the\nexplanation of why the decision boundary was orthogonal\nto the weight vector, we can think of a set of inputs as a\nvector. Recognizing that much of the processing within a\nneural network involves vector and matrix multiplications\nopens up the possibility of using specialized hardware to\nspeed up these calculations. For example, graphics proc-\nessing units (GPUs) are hardware components that have\nspecifically been designed to do extremely fast matrix\nmultiplications.\nIn a standard feedforward network, all the neurons\nin one layer receive all the outputs (i.e., activations) from\n92 ChAPtER 3 all the neurons in the preceding layer. This means that all\nthe neurons in a layer receive the same set of inputs. As\na result, we can calculate the weighted sum calculation\nfor all the neurons in a layer using only a single vector by\nmatrix multiplication. Doing this is much faster than cal-\nculating a separate weighted sum for each neuron in the\nlayer. To do this calculation of weighted sums for an entire\nlayer of neurons in a single multiplication, we put the out-\nputs from the neurons in the preceding layer into a vector\nand store all the weights of the connections between the\ntwo layers of neurons in a matrix. We then multiply the\nvector by the matrix, and the resulting vector contains\nthe weighted sums for all the neurons.\nFigure 3.8 illustrates how the weighted summation\ncalculations for all the neurons in a layer in a network can\nbe calculated using a single matrix multiplication opera-\ntion. This figure is composed of two separate graphics: the\ngraphic on the left illustrates the connections between\nneurons in two layers of a network, and the graphic on\nthe right illustrates the matrix operation to calculate the\nweighted sums for the neurons in the second layer of the\nnetwork. To help maintain a correspondence between\nthe two graphics, the connections into neuron E are high-\nlighted in the graphic on the left, and the calculation of the\nweighted sum in neuron E is highlighted in the graphic on\nthe right.\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 93 Focusing on the graphic on the right, the 1\u00d73 vec-\ntor (1 row, 3 columns) on the bottom-l eft of this graphic,\nstores the activations for the neurons in layer 1 of the net-\nwork; note that these activations are the outputs from an\nactivation function \u03d5 (the particular activation function is\nnot specified\u2014 it could be a threshold function, a tanh, a\nlogistic function, or a rectified linear unit/ReLU function).\nThe 3\u00d74 matrix (three rows and four columns), in the top-\nright of the graphic, holds the weights for the connections\nbetween the two layers of neurons. In this matrix, each\ncolumn stores the weights for the connections coming into\none of the neurons in the second layer of the network. The\nfirst column stores the weights for neuron D, the second\ncolumn for neuron E, etc.3 Multiplying the 1\u00d73 vector of\nactivations from layer 1 by the 3\u00d74 weight matrix results\nin a 1\u00d74 vector corresponding to the weighted summa-\ntions for the four neurons in layer 2 of the network: z is\nD\nthe weighted sum of inputs for neuron D, z for neuron E,\nE\nand so on.\nTo generate the 1\u00d74 vector containing the weighted\nsummations for the neurons in layer 2, the activation\nvector is multiplied by each column in the matrix in turn.\nThis is done by multiplying the first (leftmost) element in\nthe vector by the first (topmost) element in the column,\nthen multiplying the second element in the vector by the\nelement in the second row in the column, and so on, un-\ntil each element in the vector has been multiplied by its\n94 ChAPtER 3 corresponding column element. Once all the multiplica-\ntions between the vector and the column have been com-\npleted, the results are summed together and the stored in\nthe output vector. Figure 3.8 illustrates multiplication of\nthe activation vector by the second column in the weight\nmatrix (the column containing the weights for inputs to\nneuron E) and the storing of the summation of these mul-\ntiplications in the output vector as the value z .\nE\nWeight matrix for\nedges in layer 2\nActivations from layer 1 Weighted sums for layer 2\nFigure 3.8 A graphical illustration of the topological connections of a\nspecific neuron E in a network, and the corresponding vector by matrix\nmultiplication that calculates the weighted summation of inputs for the\nneuron E, and its siblings in the same layer.5\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 95 Indeed, the calculation implemented by an entire neu-\nral network can be represented as a chain of matrix multi-\nplications, with an element- wise application of activation\nfunctions to the results of each multiplication. Figure 3.9\nillustrates how a neural network can be represented in\nboth graph form (on the left) and as a sequence of matrix\noperations (on the right). In the matrix representation,\nthe \u00d7 symbol represents standard matrix multiplication\n(described above) and the \u2192\u03d5\u2192 notation represents the\napplication of an activation function to each element in\nthe vector created by the preceding matrix multiplication.\nThe output of this element- wise application of the activa-\ntion function is a vector containing the activations for the\nneurons in a layer of the network. To help show the corre-\nspondence between the two representations, both figures\nshow the inputs to the network, I and I , the activations\n1 2\nfrom the three hidden units, A , A , and A , and the over-\n1 2 3\nall output of the network, y.\nHidden layer Output layer\nweight matrix weight matrix\nActivations Activations Output\ninput layer hidden layer\nFigure 3.9 A graph representation of a neural network (left), and the same\nnetwork represented as a sequence of matrix operations (right).6\n96 ChAPtER 3 As a side note, the matrix representation provides a\ntransparent view of the depth of a network; the network\u2019s\ndepth is counted as the number of layers that have a weight\nmatrix associated with them (or equivalently, the depth of\na network is the number of weight matrices required by\nthe network). This is why the input layer is not counted\nwhen calculating the depth of a network: it does not have\na weight matrix associated with it.\nAs mentioned above, the fact that the majority of cal-\nculations in a neural network can be represented as a se-\nquence of matrix operations has important computational\nimplications for deep learning. A neural network may con-\ntain over a million neurons, and the current trend is for the\nsize of these networks to double every two to three years.4\nFurthermore, deep learning networks are trained by itera-\ntively running a network on examples sampled from very\nlarge datasets and then updating the network parameters\n(i.e., the weights) to improve performance. Consequently,\ntraining a deep learning network can require very large\nnumbers of network runs, with each network run requir-\ning millions of calculations. This is why computational\nspeedups, such as those that can be achieved by using\nGPUs to perform matrix multiplications, have been so im-\nportant for the development of deep learning.\nThe relationship between GPUs and deep learning\nis not one- way. The growth in demand for GPUs gener-\nated by deep learning has had a significant impact on\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 97 GPU manufacturers. Deep learning has resulted in these\ncompanies refocusing their business. Traditionally, these\ncompanies would have focused on the computer games\nmarket, since the original motivation for developing GPU\nchips was to improve graphics rendering, and this had a\nnatural application to computer games. However, in re-\ncent years these companies have focused on positioning\nGPUs as hardware for deep learning and artificial intel-\nligence applications. Furthermore, GPU companies have\nalso invested to ensure that their products support the top\ndeep learning software frameworks.\nSummary\nThe primary theme in this chapter has been that deep\nlearning networks are composed of large numbers of\nsimple processing units that work together to learn and\nimplement complex mappings from large datasets. These\nsimple units, neurons, execute a two- stage process: first, a\nweighted summation over the inputs to the neuron is cal-\nculated, and second, the result of the weighted summation\nis passed through a nonlinear function, known as an acti-\nvation function. The fact that a weighted summation func-\ntion can be efficiently calculated across a layer of neurons\nusing a single matrix multiplication operation is impor-\ntant: it means that neural networks can be understood as a\n98 ChAPtER 3 sequence of matrix operations; this has permitted the use\nof GPUs, hardware optimized to perform fast matrix mul-\ntiplication, to speed up the training of networks, which in\nturn has enabled the size of networks to grow.\nThe compositional nature of neural networks means\nthat it is possible to understand at a very fundamental\nlevel how a neural network operates. Providing a compre-\nhensive description of this level of processing has been the\nfocus of this chapter. However, the compositional nature\nof neural networks also raises a raft of questions in rela-\ntion to how a network should be composed to solve a given\ntask, for example:\n\u2022 Which activation functions should the neurons in a\nnetwork use?\n\u2022 How many layers should there be in a network?\n\u2022 How many neurons should there be in each layer?\n\u2022 How should the neurons be connected together?\nUnfortunately, many of these questions cannot be an-\nswered at a level of pure principle. In machine learning\nterminology, the types of concepts these questions are\nabout are known as hyperparameters, as distinct from\nmodel parameters. The parameters of a neural network\nare the weights on the edges, and these are set by training\nnEuRAL nEtwoRks: thE BuILdIng BL oCks oF dEEP LEARnIng 99 the network using large datasets. By contrast, hyperpa-\nrameters are the parameters of a model (in these cases,\nthe parameters of a neural network architecture) and/or\ntraining algorithm that cannot be directly estimated from\nthe data but instead must be specified by the person cre-\nating the model, either through the use of heuristic rules,\nintuition, or trial and error. Often, much of the effort that\ngoes into the creation of a deep learning network involves\nexperimental work to answer the questions in relation to\nhyperparameters, and this process is known as hyperpa-\nrameter tuning. The next chapter will review the history\nand evolution of deep learning, and the challenges posed\nby many of these questions are themes running through\nthe review. Subsequent chapters in the book will explore\nhow answering these questions in different ways can cre-\nate networks with very different characteristics, each\nsuited to different types of tasks. For example, recurrent\nneural networks are best suited to processing sequential/\ntime- series data, whereas convolutional neural networks\nwere originally developed to process images. Both of these\nnetwork types are, however, built using the same funda-\nmental processing unit, the artificial neuron; the differ-\nences in the behavior and abilities of these networks stems\nfrom how these neurons are arranged and composed.\n100 ChAPtER 3 4\nA BRIEF HISTORY OF\nDEEP LEARNING\nThe history of deep learning can be described as three\nmajor periods of excitement and innovation, interspersed\nwith periods of disillusionment. Figure 4.1 shows a time-\nline of this history, which highlights these periods of ma-\njor research: on threshold logic units (early 1940s to the\nmid 1960s), connectionism (early 1980s to mid-1 990s),\nand deep learning (mid 2000s to the present). Figure 4.1\ndistinguishes some of the primary characteristics of the\nnetworks developed in each of these three periods. The\nchanges in these network characteristics highlight some\nof the major themes within the evolution of deep learning,\nincluding: the shift from binary to continuous values; the\nmove from threshold activation functions, to logistic and\ntanh activation, and then onto ReLU activation; and the\nprogressive deepening of the networks, from single layer, to multiple layer, and then onto deep networks. Finally,\nthe upper half of figure 4.1 presents some of the impor-\ntant conceptual breakthroughs, training algorithms, and\nmodel architectures that have contributed to the evolu-\ntion of deep learning.\nFigure 4.1 provides a map of the structure of this\nchapter, with the sequence of concepts introduced in the\nchapter generally following the chronology of this time-\nline. The two gray rectangles in figure 4.1 represent the\ndevelopment of two important deep learning network ar-\nchitectures: convolutional neural networks (CNNs), and\nrecurrent neural networks (RNNs). We will describe the\nevolution of these two network architectures in this chap-\nter, and chapter 5 will give a more detailed explanation of\nhow these networks work.\nEarly Research: Threshold Logic Units\nIn some of the literature on deep learning, the early neural\nnetwork research is categorized as being part of cybernet-\nics, a field of research that is concerned with developing\ncomputational models of control and learning in biologi-\ncal units. However, in figure 4.1, following the terminol-\nogy used in Nilsson (1965), this early work is categorized\nas research on threshold logic units because this term\ntransparently describes the main characteristics of the\n102 ChAPtER 4 .gninraeL\npeeD\nfo\nyrotsiH\n1.4\nerugiF\nA BRIEF hIstoRy oF dEEP LEARnIng 103 systems developed during this period. Most of the models\ndeveloped in the 1940s, \u201950s, and \u201960s processed Boolean\ninputs (true/false represented as +1/-1 or 1/0) and gener-\nated Boolean outputs. They also used threshold activation\nfunctions (introduced in chapter 3), and were restricted to\nsingle- layer networks; in other words, they were restricted\nto a single matrix of tunable weights. Frequently, the fo-\ncus of this early research was on understanding whether\ncomputational models based on artificial neurons had the\ncapacity to learn logical relations, such as conjunction or\ndisjunction.\nIn 1943, Walter McCulloch and Walter Pitts published\nan influential computational model of biological neurons\nin a paper entitled: \u201cA Logical Calculus of the Ideas Im-\nmanent in Nervous Activity\u201d (McCulloch and Pitts 1943).\nThe paper highlighted the all- or- none characteristic of\nneural activity in the brain and set out to mathematically\ndescribe neural activity in terms of a calculus of propo-\nsitional logic. In the McCulloch and Pitts model, all the\ninputs and the output to a neuron were either 0 or 1.\nFurthermore, each input was either excitatory (having a\nweight of +1) or inhibitory (having a weight of -1 ). A key\nconcept introduced in the McCulloch and Pitts model was\na summation of inputs followed by a threshold function\nbeing applied to the result of the summation. In the sum-\nmation, if an excitatory input was on, it added 1; if an in-\nhibitory input was on, it subtracted 1. If the result of the\n104 ChAPtER 4 summation was above a preset threshold, then the output\nof the neuron was 1; otherwise, it output a 0. In the paper,\nMcCulloch and Pitts demonstrated how logical operations\n(such as conjunction, disjunction, and negation) could be\nrepresented using this simple model. The McCulloch and\nPitts model integrated the majority of the elements that\nare present in the artificial neurons introduced in chapter\n3. In this model, however, the neuron was fixed; in other\nwords the weights and threshold were set by han.\nIn 1949, Donald O. Hebb published a book entitled\nThe Organization of Behavior, in which he set out a neu-\nropsychological theory (integrating psychology and the\nphysiology of the brain) to explain general human be-\nhavior. The fundamental premise of the theory was that\nbehavior emerged through the actions and interactions\nof neurons. For neural network research, the most im-\nportant idea in this book was a postulate, now known as\nHebb\u2019s postulate, which explained the creation of lasting\nmemory in animals based on a process of changes to the\nconnections between neurons:\nWhen an axon of a cell A is near enough to excite\na cell B and repeatedly or persistently takes part\nin firing it, some growth process or metabolic\nchange takes place in one or both cells such that A\u2019s\nefficiency, as one of the cells firing B, is increased.\n(Hebb 1949, p. 62)\nA BRIEF hIstoRy oF dEEP LEARnIng 105 This postulate was important because it asserted that in-\nformation was stored in the connections between neurons\n(i.e., in the weights of a network), and furthermore that\nlearning occurred by changing these connections based\non repeated patterns of activation (i.e., learning can take\nplace within a network by changing the weights of the\nnetwork).\nRosenblatt\u2019s Perceptron Training Rule\nIn the years following Hebb\u2019s publication, a number of re-\nsearchers proposed computational models of neuron activ-\nity that integrated the Boolean threshold activation units\nof McCulloch and Pitts, with a learning mechanism based\non adjusting the weights applied to the inputs. The best\nknown of these models was Frank Rosenblatt\u2019s perceptron\nmodel (Rosenblatt 1958). Conceptually, the perceptron\nmodel can be understood as a neural network consisting\nof a single artificial neuron that uses a threshold activa-\ntion unit. Importantly, a perceptron network only has a\nsingle layer of weights. The first implementation of a per-\nceptron was a software implementation on an IBM 704\nsystem (and this was probably the first implementation\nof any neural network). However, Rosenblatt always in-\ntended the perceptron to be a physical machine and it was\nlater implemented in custom-b uilt hardware known as the\n\u201cMark 1 perceptron.\u201d The Mark 1 perceptron received input\nfrom a camera that generated a 400- pixel image that was\n106 ChAPtER 4 passed into the machine via an array of 400 photocells\nthat were in turn connected to the neurons. The weights\non connections to the neurons were implemented using\nadjustable electrical resistors known as potentiometers,\nand weight adjustments were implemented by using elec-\ntric motors to adjust the potentiometers.\nRosenblatt proposed an error- correcting training pro-\ncedure for updating the weights of a perceptron so that it\ncould learn to distinguish between two classes of input:\ninputs for which the perceptron should produce the out-\nput y=+1, and inputs for which the perceptron should\nproduce the output y=\u22121 (Rosenblatt 1960). The train-\ning procedure assumes a set of Boolean encoded input pat-\nterns, each with an associated target output. At the start\nof training, the weights in the perceptron are initialized\nto random values. Training then proceeds by iterating\nthrough the training examples, and after each example\nhas been presented to the network, the weights of the net-\nwork are updated based on the error between the output\ngenerated by the perceptron and the target output speci-\nfied in the data. The training examples can be presented to\nthe network in any order and examples may be presented\nmultiple times before training is completed. A complete\ntraining pass through the set of examples is known as an\niteration, and training terminates when the perceptron\ncorrectly classifies all the examples in an iteration.\nA BRIEF hIstoRy oF dEEP LEARnIng 107 Rosenblatt defined a learning rule (known as the\nperceptron training rule) to update each weight in a per-\nceptron after a training example has been processed. The\nstrategy the rule used to update the weights is the same as\nthe three- condition strategy we introduced in chapter 2 to\nadjust the weights in the loan decision model:\n1. If the output of the model for an example matches the\noutput specified for that example in the dataset, then\ndon\u2019t update the weights.\n2. If the output of the model is too low for the current\nexample, then increase the output of the model by\nincreasing the weights for the inputs that had positive\nvalue for the example and decreasing the weights for the\ninputs that had a negative value for the example.\n3. If the output of the model is too high for the current\nexample, then reduce the output of the model by\ndecreasing the weights for the inputs that had a positive\nvalue and increasing the weights for the inputs that had a\nnegative value for the example.\nWritten out in an equation, Rosenblatt\u2019s learning rule\nupdates a weight i (w ) as:\ni\nwt+1 = wt +(\u03b7\u00d7(yt \u2212y\u02c6t)\u00d7xt)\ni i i\n108 ChAPtER 4 In this rule, wt+1 is the value of weight i after the net-\ni\nwork weights have been updated in response to the proc-\nessing of example t, wt is the value of weight i used during\ni\nthe processing of example t, \u03b7 is a preset positive constant\n(known as the learning rate, discussed below), yt is the ex-\npected output for example t as specified in the training\ndataset, y\u02c6t is the output generated by the perceptron for\nexample t, and xt is the component of input t that was\ni\nweighted by wt during the processing of the example.\ni\nAlthough it may look complex, the perceptron train-\ning rule is in fact just a mathematical specification of the\nthree- condition weight update strategy described above.\nThe primary part of the equation to understand is the\ncalculation of the difference between the expected output\nand what the perceptron actually predicted: yt \u2212y\u02c6t. The\noutcome of this subtraction tells us which of the three\nupdate conditions we are in. In understanding how this\nsubtraction works, it is important to remember that for\na perceptron model the desired output is always either\ny=+1 or y=\u22121. The first condition is when yt \u2212y\u02c6t =0;\nthen the output of the perceptron is correct and the\nweights are not changed.\nThe second weight update condition is when the out-\nput of the perceptron is too large. This condition can only\nbe occur when the correct output for example t is yt =\u22121\nand so this condition is triggered when yt \u2212y\u02c6t <0. In\nthis case, if the perceptron output for the example t is\nA BRIEF hIstoRy oF dEEP LEARnIng 109 y\u02c6t =+1, then the error term is negative (yt \u2212y\u02c6t =\u22122) and\nthe weight w is updated by +(\u03b7\u00d7\u22122\u00d7xt). Assuming, for\ni i\nthe purpose of this explanation, that \u03b7 is set to 0.5, then\nthis weight update simplifies to \u2212xt. In other words, when\ni\nthe perceptron\u2019s output is too large, the weight update\nrule subtracts the input values from the weights. This will\ndecrease the weights on inputs with positive values for the\nexample, and increase the weights on inputs with negative\nvalues for the example (subtracting a negative number is\nthe same as adding a positive number).\nThe third weight update condition is when the out-\nput of the perceptron is too small. This weight update\ncondition is the exact opposite of the second. It can only\noccur when yt =+1 and so is triggered when yt \u2212y\u02c6t >0.\nIn this case (yt \u2212y\u02c6t =2), and the weight is updated by\n+(\u03b7\u00d72\u00d7xt). Again assuming that \u03b7 is set to 0.5, then\ni\nthis update simplifies to +xt, which highlights that when\ni\nthe error of the perceptron is positive, the rule updates\nthe weight by adding the input to the weight. This has the\neffect of decreasing the weights on inputs with negative\nvalues for the example and increasing the weight on in-\nputs with positive values for the example.\nAt a number of points in the preceding paragraphs\nwe have referred to learning rate, \u03b7. The purpose of the\nlearning rate, \u03b7, is to control the size of the adjustments\nthat are applied to a weight. The learning rate is an ex-\nample of a hyperparameter that is preset before the model\nis trained. There is a tradeoff in setting the learning rate:\n110 ChAPtER 4 \u2022 If the learning rate is too small, it may take a very\nlong time for the training process to converge on an\nappropriate set of weights.\n\u2022 If the learning rate is too large, the network\u2019s weights\nmay jump around the weight space too much and the\ntraining may not converge at all.\nOne strategy for setting the learning rate is to set it to\na relatively small positive value (e.g., 0.01), and another\nstrategy is to initialize it to a larger value (e.g., 1.0)\nbut to systematically reduce it as the training progresses\n1\n(e.g., \u03b7t+1 =\u03b71\u00d7 ).\nt\nTo make this discussion regarding the learning rate\nmore concrete, imagine you are trying to solve a puzzle\nthat requires you to get a small ball to roll into a hole. You\nare able to control the direction and speed of the ball by\ntilting the surface that the ball is rolling on. If you tilt the\nsurface too steeply, the ball will move very fast and is likely\nto go past the hole, requiring you to adjust the surface\nagain, and if you overadjust you may end up repeatedly\ntilting the surface. On the other hand, if you only tilt the\nsurface a tiny bit, the ball may not start to move at all, or it\nmay move very slowly taking a long time to reach the hole.\nNow, in many ways the challenge of getting the ball to roll\ninto the hole is similar to the problem of finding the best\nset of weights for a network. Think of each point on the\nA BRIEF hIstoRy oF dEEP LEARnIng 111 surface the ball is rolling across as a possible set of network\nweights. The ball\u2019s position at each point in time specifies\nthe current set of weights of the network. The position\nof the hole specifies the optimal set of network weights for\nthe task we are training the network to complete. In this\ncontext, guiding the network to the optimal set of weights\nis analogous to guiding the ball to the hole. The learning\nrate allows us to control how quickly we move across the\nsurface as we search for the optimal set of weights. If we set\nthe learning rate to a high value, we move quickly across\nthe surface: we allow large updates to the weights at each\niteration, so there are big differences between the network\nweights in one iteration and the next. Or, using our rolling\nball analogy, the ball is moving very quickly, and just like\nin the puzzle when the ball is rolling too fast and passes\nthe hole, our search process may be moving so fast that it\nmisses the optimal set of weights. Conversely, if we set the\nlearning rate to a low value, we move very slowly across\nthe surface: we only allow small updates to the weights at\neach iteration; or, in other words, we only allow the ball\nto move very slowly. With a low learning rate, we are less\nlikely to miss the optimal set of weights, but it may take\nan inordinate amount of time to get to them. The strategy\nof starting with a high learning rate and then systemati-\ncally reducing it is equivalent to steeply tilting the puzzle\nsurface to get the ball moving and then reducing the tilt to\ncontrol the ball as it approaches the hole.\n112 ChAPtER 4 Rosenblatt proved that if a set of weights exists that\nenables the perceptron to properly classify all of the train-\ning examples correctly, the perceptron training algorithm\nwill eventually converge on this set of weights. This find-\ning is known as the perceptron convergence theorem\n(Rosenblatt 1962). The difficulty with training a percep-\ntron, however, is that it may require a substantial number\nof iterations through the data before the algorithm con-\nverges. Furthermore, for many problems it is unknown\nwhether an appropriate set of weights exists in advance;\nconsequently, if training has been going on for a long time,\nit is not possible to know whether the training process is\nsimply taking a long time to converge on the weights and\nterminate, or whether it will never terminate.\nThe Least Mean Squares Algorithm\nAround the same time that Rosenblatt was developing the\nperceptron, Bernard Widrow and Marcian Hoff were devel-\noping a very similar model called the ADALINE (short for\nadaptive linear neuron), along with a learning rule called\nthe LMS (least mean square) algorithm (Widrow and Hoff\n1960). An ADALINE network consists of a single neuron\nthat is very similar to a perceptron; the only difference is\nthat an ADALINE network does not use a threshold func-\ntion. In fact, the output of an ADALINE network is the just\nthe weighted sum of the inputs. This is why it is known\nas a linear neuron: a weighted sum is a linear function (it\nA BRIEF hIstoRy oF dEEP LEARnIng 113 defines a line), and so an ADALINE network implements\na linear mapping from inputs to output. The LMS rule is\nnearly identical to the perceptron learning rule, except\nthat the output of the perceptron for a given example y\u02c6t\nis replaced by the weighted sum of the inputs:\n\uf8eb \uf8eb \uf8eb n \uf8f6\uf8f6 \uf8f6\nwt i+1 =wt i + \uf8ed\uf8ec\u03b7\u00d7 \uf8ed\uf8ecyt\u2212 \uf8ed\uf8ec\u2211wt i \u00d7xt i\uf8f8\uf8f7 \uf8f8\uf8f7\u00d7xt i\uf8f8\uf8f7\ni=0\nThe logic of the LMS update rule is the same as that\nof the perceptron training rule. If the output is too large,\nthen weights that were applied to a positive input caused\nthe output to be larger, and these weights should be de-\ncreased, and those that were applied to a negative input\nshould be increased, thereby reducing the output the next\ntime this input pattern is received. And, by the same logic,\nif the output is too small, then weights that were applied\nto a positive input are increased and those that were ap-\nplied to a negative input should be decreased.\nOne of the important aspects of Widrow and Hoff\u2019s\nwork was to show that LMS rule could be used to train\nnetwork to predict a number of any value, not just a +1\nor - 1. This learning rule was called the least mean square\nalgorithm because using the LMS rule to iteratively ad-\njust the weights in a neuron is equivalent to minimizing\nthe average squared error on the training set. Today, the\nLMS learning rule is sometimes called the Widrow- Hoff\n114 ChAPtER 4 If the output of the\nmodel is too large, then\nweights associated with\npositive inputs should\nbe reduced, whereas if\nthe output is too small,\nthen these weights\nshould be increased. learning rule, after the inventors; however, it is more com-\nmonly called the delta rule because it uses the difference\n(or delta) between desired output and the actual output\nto calculate the weight adjustments. In other words, the\nLMS rule specifies that a weight should be adjusted in pro-\nportion to the difference between the output of an ADA-\nLINE network and the desired output: if the neuron makes\na large error, then the weights are adjusted by a large\namount, if the neuron makes a small error, then weights\nare adjusted by a small amount.\nToday, the perceptron is recognized as important mile-\nstone in the development of neural networks because it\nwas the first neural network to be implemented. However,\nmost modern algorithms for training neural networks are\nmore similar to the LMS algorithm. The LMS algorithm\nattempts to minimize the mean squared error of the net-\nwork. As will be discussed in chapter 6, technically this\niterative error reduction process involves a gradient de-\nscent down an error surface; and, today, nearly all neu-\nral networks are trained using some variant of gradient\ndescent.\nThe XOR Problem\nThe success of Rosenblatt, Widrow and Hoff, and others,\nin demonstrating that neural network models could au-\ntomatically learn to distinguish between different sets of\npatterns, generated a lot of excitement around artificial\n116 ChAPtER 4 intelligence and neural network research. However, in\n1969, Marvin Minsky and Seymour Papert published a\nbook entitled Perceptrons, which, in the annals of neural\nnetwork research, is attributed with single- handedly de-\nstroying this early excitement and optimism (Minsky and\nPapert 1969). Admittedly, throughout the 1960s neural\nnetwork research had suffered from a lot of hype, and a\nlack of success in terms of fulfilling the correspondingly\nhigh expectations. However, Minsky and Papert\u2019s book\nset out a very negative view of the representational power\nof neural networks, and after its publication funding for\nneural network research dried up.\nMinsky and Papert\u2019s book primarily focused on single\nlayer perceptrons. Remember that a single layer percep-\ntron is the same as a single neuron that uses a threshold\nactivation function, and so a single layer perceptron is re-\nstricted to implementing a linear (straight- line) decision\nboundary.1 This means that a single layer perceptron can\nonly learn to distinguish between two classes of inputs if\nit is possible to draw a straight line in the input space that\nhas all of the examples of one class on one side of the line\nand all examples of the other class on the other side of the\nline. Minsky and Papert highlighted this restriction as a\nweakness of these models.\nTo understand Minsky and Papert\u2019s criticism of single\nlayer perceptrons, we must first understand the concept\nof a linearly separable function. We will use a comparison\nA BRIEF hIstoRy oF dEEP LEARnIng 117 between the logical AND and OR functions with the logi-\ncal XOR function to explain the concept of a linearly sepa-\nrable function. The AND function takes two inputs, each\nof which can be either TRUE or FALSE, and returns TRUE\nif both inputs are TRUE. The plot on the left of figure 4.4\nshows the input space for the AND function and catego-\nrizes each of the four possible input combinations as ei-\nther resulting in an output value of TRUE (shown in the\nfigure by using a clear dot) or FALSE (shown in the figure\nby using black dots). This plot illustrates that is possible\nto draw a straight line between the inputs for which the\nAND function returns TRUE, (T,T), and the inputs for\nwhich the function returns FALSE, {(F,F), (F,T), (T,F)}.\nThe OR function is similar to the AND function, except\nthat it returns TRUE if either or both inputs are TRUE.\nThe middle plot in figure 4.4 shows that it is possible to\ndraw a line that separates the inputs that the OR function\nclassifies as TRUE, {(F,T), (T,F), (T,T)}, from those it clas-\nsifies as FALSE, (F,F). It is because we can draw a single\nstraight line in the input space of these functions that\ndivides the inputs belonging to one category of output\nfrom the inputs belonging to the other output category\nthat the AND and OR functions are linearly separable\nfunctions.\nThe XOR function is also similar in structure to the\nAND and OR functions; however, it only returns TRUE\nif one (but not both) of its inputs are TRUE. The plot on\n118 ChAPtER 4 Figure 4.2 Illustrations of the linearly separable function. In each figure,\nblack dots represent inputs for which the function returns FALSE, circles\nrepresent inputs for which the function returns TRUE. (T stands for true and\nF stands for false.)\nthe right of figure 4.2 shows the input space for the XOR\nfunction and categorizes each of the four possible input\ncombinations as returning either TRUE (shown in the fig-\nure by using a clear dot) or FALSE (shown in the figure by\nusing black dots). Looking at this plot you will see that it is\nnot possible to draw a straight line between the inputs the\nXOR function classifies as TRUE and those that it classi-\nfies as FALSE. It is because we cannot use a single straight\nline to separate the inputs belonging to different catego-\nries of outputs for the XOR function that this function is\nsaid to be a nonlinearly separable function. The fact that\nthe XOR function is nonlinearly separable does not make\nthe function unique, or even rare\u2014 there are many func-\ntions that are nonlinearly separable.\nThe key criticism that Minsky and Papert made of sin-\ngle layer perceptrons was that these single layer models\nA BRIEF hIstoRy oF dEEP LEARnIng 119 were unable to learn nonlinearly separable functions, such\nas the XOR function. The reason for this limitation is that\nthe decision boundary of a perceptron is linear and so a\nsingle layer perceptron cannot learn to distinguish be-\ntween the inputs that belong to one output category of a\nnonlinearly separable function from those that belong to\nthe other category.\nIt was known at the time of Minsky and Papert\u2019s\npublication that it was possible to construct neural net-\nworks that defined a nonlinear decision boundary, and\nthus learn nonlinearly separable functions (such as the\nXOR function). The key to creating networks with more\ncomplex (nonlinear) decision boundaries was to extend\nthe network to have multiple layers of neurons. For ex-\nample, figure 4.3 shows a two-l ayer network that imple-\nments the XOR function. In this network, the logical\nTRUE and FALSE values are mapped to numeric values:\nFALSE values are represented by 0, and TRUE values are\nrepresented by 1. In this network, units activate (out-\nput +1) if the weighted sum of inputs is \u22651; otherwise,\nthey output 0. Notice that the units in the hidden layer\nimplement the logical AND and OR functions. These can\nbe understood as intermediate steps to solving the XOR\nchallenge. The unit in the output layer implements the\nXOR by composing the outputs of these hidden layers. In\nother words, the unit in the output layer returns TRUE\nonly when the AND node is off (output=0) and the OR\n120 ChAPtER 4 Figure 4.3 A network that implements the XOR function. All processing\nunits use a threshold activation function with a threshold of \u22651.\nnode is on (output=1). However, it wasn\u2019t clear at the time\nhow to train networks with multiple layers. Also, at the\nend of their book, Minsky and Papert argued that \u201cin their\njudgment\u201d the research on extending neural networks\nto multiple layers was \u201csterile\u201d (Minsky and Papert 1969,\nsec. 13.2 page 23).\nIn a somewhat ironic historical twist, contempo-\nraneous with Minsky and Papert\u2019s publication, Alexey\nIvakhnenko, a Ukrainian researcher, proposed the group\nmethod for data handling (GMDH), and in 1971 published\nA BRIEF hIstoRy oF dEEP LEARnIng 121 a paper that described how it could be used to learn a neu-\nral network with eight layers (Ivakhnenko 1971). Today\nIvakhnenko\u2019s 1971 GMDH network is credited with be-\ning the first published example of a deep network trained\nfrom data (Schmidhuber 2015). However, for many years,\nIvaknenko\u2019s accomplishment was largely overlooked by the\nwider neural network community. As a consequence, very\nlittle of the current work in deep learning uses the GMDH\nmethod for training: in the intervening years other train-\ning algorithms, such as backpropagation (described below),\nbecame standardized in the community. At the same time\nof Ivakhnenko\u2019s overlooked accomplishment, Minsky\nand Papert\u2019s critique was proving persuasive and it her-\nalded the end of the first period of significant research on\nneural networks.\nThis first period of neural network research, did, how-\never, leave a legacy that shaped the development of the\nfield up to the present day. The basic internal structure\nof an artificial neuron was defined: a weighted sum of in-\nputs fed through an activation function. The concept of\nstoring information within the weights of a network was\ndeveloped. Furthermore, learning algorithms based on\niteratively adapting weights were proposed, along with\npractical learning rules, such as the LMS rule. In particu-\nlar, the LMS approach, of adjusting the weights of neu-\nrons in proportion to the difference between the output\nof the neuron and the desired output, is present in most\n122 ChAPtER 4 modern training algorithms. Finally, there was recogni-\ntion of the limitations of single layer networks, and an\nunderstanding that one way to address these limitations\nwas to extend the networks to include multiple layers of\nneurons. At this time, however, it was unclear how to train\nnetworks with multiple layers. Updating a weight requires\nan understanding of how the weight affects the error of\nthe network. For example, in the LMS rule if the output of\nthe neuron was too large, then weights that were applied\nto positive inputs caused the output to increase. There-\nfore, decreasing the size of these weight would reduce the\noutput and thereby reduce the error. But, in the late 1960s,\nthe question of how to model the relationship between the\nweights of the inputs to neurons in the hidden layers of\na network and the overall error of the network was still\nunanswered; and, without this estimation of the contri-\nbution of the weight to the error, it was not possible to\nadjust the weights in the hidden layers of a network. The\nproblem of attributing (or assigning) an amount of error\nto the components in a network is sometimes referred to\nas the credit assignment problem, or as the blame assign-\nment problem.\nConnectionism: Multilayer Perceptrons\nIn the 1980s, people began to reevaluate the criticisms of\nthe late 1960s as being overly severe. Two developments,\nA BRIEF hIstoRy oF dEEP LEARnIng 123 in particular, reinvigorated the field: (1) Hopfield net-\nworks; and (2) the backpropagation algorithm.\nIn 1982, John Hopfield published a paper where he\ndescribed a network that could function as an associative\nmemory (Hopfield 1982). During training, an associative\nmemory learns a set of input patterns. Once the associate\nmemory network has been trained, then, if a corrupted\nversion of one of the input patterns is presented to the\nnetwork, the network is able to regenerate the complete\ncorrect pattern. Associative memories are useful for a\nnumber of tasks, including pattern completion and error\ncorrection. Table 4.12 illustrates the tasks of pattern com-\npletion and error correction using the example of an asso-\nciative memory that has been trained to store information\non people\u2019s birthdays. In a Hopfield network, the memo-\nries, or input patterns, are encoded in binary strings; and,\nTable 4.1. Illustration of the uses of an association\nmemory for pattern completion and error correction\nTraining patterns Pattern completion\nJohn**12May Liz***????? \u2192 Liz***25Feb\nKerry*03Jan ???***10Mar \u2192 Des***10Mar\nLiz***25Feb Error correction\nDes***10Mar Kerry*01Apr \u2192 Kerry*03Jan\nJosef*13Dec Jxsuf*13Dec \u2192 Josef*13Dec\n124 ChAPtER 4 assuming binary patterns are relatively distinct from each\nother, a Hopfield network can store up to 0.138N of these\nstrings, where N is the number of neurons in the network.\nSo to store 10 distinct patterns requires a Hopfield net-\nwork with 73 neurons, and to store 14 distinct patterns\nrequires 100 neurons.\nBackpropagation and Vanishing Gradients\nIn 1986, a group of researchers known as the parallel\ndistributed processing (PDP) research group published a\ntwo- book overview of neural network research (Rumel-\nhart et al. 1986b, 1986c). These books proved to be in-\ncredibly popular, and chapter 8 in volume one described\nthe backpropagation algorithm (Rumelhart et al. 1986a).\nThe backpropagation algorithm has been invented a num-\nber of times,3 but it was this chapter by Rumelhart, Hin-\nton, and Williams, published by PDP, that popularized\nits use. The backpropagation algorithm is a solution to\nthe credit assignment problem and so it can be used to\ntrain a neural network that has hidden layers of neurons.\nThe backpropagation algorithm is possibly the most im-\nportant algorithm in deep learning. However, a clear and\ncomplete explanation of the backpropagation algorithm\nrequires first explaining the concept of an error gradient,\nand then the gradient descent algorithm. Consequently,\nthe in- depth explanation of backpropagation is post-\nponed until chapter 6, which begins with an explanation\nA BRIEF hIstoRy oF dEEP LEARnIng 125 of these necessary concepts. The general structure of the\nalgorithm, however, can be described relatively quickly.\nThe backpropagation algorithm starts by assigning ran-\ndom weights to each of the connections in the network.\nThe algorithm then iteratively updates the weights in the\nnetwork by showing training instances to the network and\nupdating the network weights until the network is work-\ning as expected. The core algorithm works in a two- stage\nprocess. In the first stage (known as the forward pass), an\ninput is presented to the network and the neuron activa-\ntions are allowed to flow forward through the network un-\ntil an output is generated. The second stage (known as the\nbackward pass) begins at the output layer and works back-\nward through the network until the input layer is reached.\nThis backward pass begins by calculating an error for each\nneuron in the output layer. This error is then used to up-\ndate the weights of these output neurons. Then the error\nof each output neuron is shared back (backpropagated) to\nthe hidden neurons that connect to it, in proportion to\nthe weights on the connections between the output neu-\nron and the hidden neuron. Once this sharing (or blame\nassignment) has been completed for a hidden neuron, the\ntotal blame attributable to that hidden neuron is summed\nand this total is used to update the weights on that neuron.\nThe backpropagation (or sharing back) of blame is then\nrepeated for the neurons that have not yet had blame at-\ntributed to them. This process of blame assignment and\n126 ChAPtER 4 weight updates continues back through the network until\nall the weights have been updated.\nA key innovation that enabled the backpropagation al-\ngorithm to work was a change in the activation functions\nused in the neurons. The networks that were developed\nin the early years of neural network research used thresh-\nold activation functions. The backpropagation algorithm\ndoes not work with threshold activation functions be-\ncause backpropagation requires that the activation func-\ntions used by the neurons in the network be differentiable.\nThreshold activation functions are not differentiable be-\ncause there is a discontinuity in the output of the function\nat the threshold. In other words, the slope of a threshold\nfunction at the threshold is infinite and therefore it is not\npossible to calculate the gradient of the function at that\npoint. This led to the use of differentiable activation func-\ntions in multilayer neural networks, such as the logistic\nand tanh functions.\nThere is, however, an inherent limitation with using\nthe backpropagation algorithm to train deep networks.\nIn the 1980s, researchers found that backpropagation\nworked well with relatively shallow networks (one or two\nlayers of hidden units), but that as the networks got deeper,\nthe networks either took an inordinate amount of time to\ntrain, or else they entirely failed to converge on a good set\nof weights. In 1991, Sepp Hochreiter (working with J\u00fcrgen\nSchmidhuber) identified the cause of this problem in his\nA BRIEF hIstoRy oF dEEP LEARnIng 127 diploma thesis (Hochreiter 1991). The problem is caused\nby the way the algorithm backpropagates errors. Fun-\ndamentally, the backpropagation algorithm is an imple-\nmentation of the chain rule from calculus. The chain rule\ninvolves the multiplication of terms, and backpropagat-\ning an error from one neuron back to another can involve\nmultiplying the error by a number terms with values less\nthan 1. These multiplications by values less than 1 happen\nrepeatedly as the error signal gets passed back through the\nnetwork. This results in the error signal becoming smaller\nand smaller as it is backpropagated through the network.\nIndeed, the error signal often diminishes exponentially\nwith respect to the distance from the output layer. The\neffect of this diminishing error is that the weights in the\nearly layers of a deep network are often adjusted by only\na tiny (or zero) amount during each training iteration. In\nother words, the early layers either train very, very slowly\nor do not move away from their random starting positions\nat all. However, the early layers in a neural network are\nvitally important to the success of the network, because\nit is the neurons in these layers that learn to detect the\nfeatures in the input that the later layers of the network\nuse as the fundamental building blocks of the representa-\ntions that ultimately determine the output of the network.\nFor technical reasons, which will be explained in chapter\n6, the error signal that is backpropagated through the net-\nwork is in fact the gradient of the error of the network,\n128 ChAPtER 4 and, as a result, this problem of the error signal rapidly di-\nminishing to near zero is known as the vanishing gradient\nproblem.\nConnectionism and Local versus Distributed\nRepresentations\nDespite the vanishing gradient problem, the backpropa-\ngation algorithm opened up the possibility of training\nmore complex (deeper) neural network architectures.\nThis aligned with the principle of connectionism. Connec-\ntionism is the idea that intelligent behavior can emerge\nfrom the interactions between large numbers of simple\nprocessing units. Another aspect of connectionism was\nthe idea of a distributed representation. A distinction can\nbe made in the representations used by neural networks\nbetween localist and distributed representations. In a lo-\ncalist representation there is a one- to- one correspondence\nbetween concepts and neurons, whereas in a distributed\nrepresentation each concept is represented by a pattern\nof activations across a set of neurons. Consequently, in a\ndistributed representation each concept is represented by\nthe activation of multiple neurons and the activation of\neach neuron contributes to the representation of multiple\nconcepts.\nTo illustrate the distinction between localist and dis-\ntributed representations, consider a scenario where (for\nsome unspecified reason) a set of neuron activations is\nA BRIEF hIstoRy oF dEEP LEARnIng 129 In a distributed\nrepresentation each\nconcept is represented\nby the activation of\nmultiple neurons and\nthe activation of each\nneuron contributes to\nthe representation of\nmultiple concepts. being used to represent the absence or presence of dif-\nferent foods. Furthermore, each food has two properties,\nthe country of origin of the recipe and its taste. The pos-\nsible countries of origin are: Italy, Mexico, or France; and,\nthe set of possible tastes are: Sweet, Sour, or Bitter. So, in\ntotal there are nine possible types of food: Italian+Sweet,\nItalian+Sour, Italian+Bitter, Mexican+Sweet, etc. Using a\nlocalist representation would require nine neurons, one\nneuron per food type. There are, however, a number of\nways to define a distributed representation of this do-\nmain. One approach is to assign a binary number to each\ncombination. This representation would require only four\nneurons, with the activation pattern 0000 representing\nItalian+Sweet, 0001 representing Italian+Sour, 0010 rep-\nresenting Italian+Bitter, and so on up to 1000 represent-\ning French+Bitter. This is a very compact representation.\nHowever, notice that in this representation the activation\nof each neuron in isolation has no independently mean-\ningful interpretation: the rightmost neuron would be ac-\ntive (***1) for Italian+Sour, Mexican+Sweet, Mexican+Bitter,\nand France+Sour, and without knowledge of the activa-\ntion of the other neurons, it is not possible know what\ncountry or taste is being represented. However, in a\ndeep network the lack of semantic interpretability of the\nactivations of hidden units is not a problem, so long as\nthe neurons in the output layer of the network are able\nto combine these representations in such a way so as to\nA BRIEF hIstoRy oF dEEP LEARnIng 131 generate the correct output. Another, more transparent,\ndistributed representation of this food domain is to use\nthree neurons to represent the countries and three neu-\nrons to represent the tastes. In this representation, the\nactivation pattern 100100 could represent Italian+Sweet,\n001100 could represent French+Sweet, and 001001 could\nrepresent French+Bitter. In this representation, the acti-\nvation of each neuron can be independently interpreted;\nhowever the distribution of activations across the set of\nneurons is required in order to retrieve the full description\nof the food (country+taste). Notice, however, that both of\nthese distributed representations are more compact than\nthe localist representation. This compactness can signifi-\ncantly reduce the number of weights required in a network,\nand this in turn can result in faster training times for the\nnetwork.\nThe concept of a distributed representation is very\nimportant within deep learning. Indeed, there is a good\nargument that deep learning might be more appropriately\nnamed representation learning\u2014 the argument being that\nthe neurons in the hidden layers of a network are learning\ndistributed representations of the input that are useful in-\ntermediate representations in the mapping from inputs to\noutputs that the network is attempting to learn. The task\nof the output layer of a network is then to learn how to\ncombine these intermediate representations so as to gen-\nerate the desired outputs. Consider again the network in\n132 ChAPtER 4 figure 4.3 that implements the XOR function. The hidden\nunits in this network learn an intermediate representa-\ntion of the input, which can be understood as composed\nof the AND and OR functions; the output layer then com-\nbines this intermediate representation to generate the\nrequired output. In a deep network with multiple hidden\nlayers, each subsequent hidden layer can be interpreted as\nlearning a representation that is an abstraction over the\noutputs of the preceding layer. It is this sequential abstrac-\ntion, through learning intermediate representations, that\nenables deep networks to learn such complex mappings\nfrom inputs to outputs.\nNetwork Architectures: Convolutional and Recurrent\nNeural Networks\nThere are a considerable number of ways in which a set\nof neurons can be connected together. The network ex-\namples presented so far in the book have been connected\ntogether in a relatively uncomplicated manner: neurons\nare organized into layers and each neuron in a layer is di-\nrectly connected to all of the neurons in the next layer of\nthe network. These networks are known as feedforward\nnetworks because there are no loops within the network\nconnections: all the connections point forward from the\ninput toward the output. Furthermore, all of our net-\nwork examples thus far would be considered to be fully\nconnected, because each neuron is connected to all the\nA BRIEF hIstoRy oF dEEP LEARnIng 133 neurons in the next layer. It is possible, and often use-\nful, to design and train networks that are not feedforward\nand/or that are not fully connected. When done correctly,\ntailoring network architectures can be understood as em-\nbedding into the network architecture information about\nthe properties of the problem that the network is trying\nto learn to model.\nA very successful example of incorporating domain\nknowledge into a network by tailoring the networks ar-\nchitecture is the design of convolutional neural networks\n(CNNs) for object recognition in images. In the 1960s,\nHubel and Wiesel carried out a series of experiments on\nthe visual cortex of cats (Hubel and Wiesel 1962, 1965).\nThese experiments used electrodes inserted into the\nbrains of sedated cats to study the response of the brain\ncells as the cats were presented with different visual stim-\nuli. Examples of the stimuli used included bright spots or\nlines of light appearing at a location in the visual field, or\nmoving across a region of the visual field. The experiments\nfound that different cells responded to different stimuli at\ndifferent locations in the visual field: in effect a single cell\nin the visual cortex would be wired to respond to a par-\nticular type of visual stimulus occurring within a particu-\nlar region of the visual field. The region of the visual field\nthat a cell responded to was known as the receptive field\nof the cell. Another outcome of these experiments was the\ndifferentiation between two types of cells: \u201csimple\u201d and\n134 ChAPtER 4 \u201ccomplex.\u201d For simple cells, the location of the stimulus is\ncritical with a slight displacement of the stimulus resulting\nin a significant reduction in the cell\u2019s response. Complex\ncells, however, respond to their target stimuli regardless\nof where in the field of vision the stimulus occurs. Hubel\nand Wiesel (1965) proposed that complex cells behaved as\nif they received projections from a large number of simple\ncells all of which respond to the same visual stimuli but\ndiffering in the position of their receptive fields. This hi-\nerarchy of simple cells feeding into complex cells results\nin funneling of stimuli from large areas of the visual field,\nthrough a set of simple cells, into a single complex cell. Fig-\nure 4.4 illustrates this funneling effect. This figure shows\na layer of simple cells each monitoring a receptive field at\na different location in the visual field. The receptive field\nof the complex cell covers the layer of simple cells, and\nVisual\nfield Layerof\nsimplecells\nComplex\ncell\nFigure 4.4 The funneling effect of receptive fields created by the hierarchy\nof simple and complex cells.\nA BRIEF hIstoRy oF dEEP LEARnIng 135 this complex cell activates if any of the simple cells in its\nreceptive field activates. In this way the complex cell can\nrespond to a visual stimulus if it occurs at any location\nin the visual field.\nIn the late 1970s and early 1980s, Kunihiko Fuku-\nshima was inspired by Hubel and Wiesel\u2019s analysis of the\nvisual cortex and developed a neural network architecture\nfor visual pattern recognition that was called the neocog-\nnitron (Fukushima 1980). The design of the neocognitron\nwas based on the observation that an image recogni-\ntion network should be able to recognize if a visual fea-\nture is present in an image irrespective of location in the\nimage\u2014 or, to put it slightly more technically, the network\nshould be able to do spatially invariant visual feature de-\ntection. For example, a face recognition network should\nbe able to recognize the shape of an eye no matter where\nin the image it occurs, similar to the way a complex cell\nin Hubel and Wiesel\u2019s hierarchical model could detect the\npresence of a visual feature irrespective of where in the\nvisual field it occurred.\nFukushima realized that the functioning of the simple\ncells in the Hubel and Wiesel hierarchy could be replicated\nin a neural network using a layer of neurons that all use\nthe same set of weights, but with each neuron receiving in-\nputs from fixed small regions (receptive fields) at different\nlocations in the input field. To understand the relationship\nbetween neurons sharing weights and spatially invariant\n136 ChAPtER 4 visual feature detection, imagine a neuron that receives a\nset of pixel values, sampled from a region of an image, as\nits inputs. The weights that this neuron applies to these\npixel values define a visual feature detection function that\nreturns true (high activation) if a particular visual feature\n(pattern) occurs in the input pixels, and false otherwise.\nConsequently, if a set of neurons all use the same weights,\nthey will all implement the same visual feature detector. If\nthe receptive fields of these neurons are then organized\nso that together they cover the entire image, then if the\nvisual feature occurs anywhere in the image at least one of\nthe neurons in the group will identify it and activate.\nFukushima also recognized that the Hubel and Wiesel\nfunneling effect (into complex cells) could be obtained by\nneurons in later layers also receiving as input the outputs\nfrom a fixed set of neurons in a small region of the preced-\ning layer. In this way, the neurons in the last layer of the\nnetwork each receive inputs from across the entire input\nfield allowing the network to identify the presence of a\nvisual feature anywhere in the visual input.\nSome of the weights in neocognitron were set by hand,\nand others were set using an unsupervised training pro-\ncess. In this training process, each time an example is pre-\nsented to the network a single layer of neurons that share\nthe same weights is selected from the layers that yielded\nlarge outputs in response to the input. The weights of the\nneurons in the selected layer are updated so as to reinforce\nA BRIEF hIstoRy oF dEEP LEARnIng 137 their response to that input pattern and the weights of\nneurons not in the layer are not updated. In 1989 Yann\nLeCun developed the convolutional neural network (CNN)\narchitecture specifically for the task of image processing\n(LeCun 1989). The CNN architecture shared many of\nthe design features found in the neocognitron; however,\nLeCun showed how these types of networks could be\ntrained using backpropagation. CNNs have proved to be\nincredibly successful in image processing and other tasks.\nA particularly famous CNN is the AlexNet network, which\nwon the ImageNet Large- Scale Visual Recognition Chal-\nlenge (ILSVRC) in 2012 (Krizhevsky et al. 2012). The goal\nof the ILSVRC competition is to identify objects in pho-\ntographs. The success of AlexNet at the ILSVRC competi-\ntion generated a lot of excitement about CNNs, and since\nAlexNet a number of other CNN architectures have won\nthe competition. CNNs are one of the most popular types\nof deep neural networks, and chapter 5 will provide a more\ndetailed explanation of them.\nRecurrent neural networks (RNNs) are another ex-\nample of a neural network architecture that has been tai-\nlored to the specific characteristics of a domain. RNNs are\ndesigned to process sequential data, such as language. An\nRNN network processes a sequence of data (such as a sen-\ntence) one input at a time. An RNN has only a single hid-\nden layer. However, the output from each of these hidden\nneurons is not only fed forward to the output neurons, it\n138 ChAPtER 4 is also temporarily stored in a buffer and then fed back\ninto all of the hidden neurons at the next input. Conse-\nquently, each time the network processes an input, each\nneuron in the hidden layer receives both the current input\nand the output the hidden layer generated in response to\nthe previous input. In order to understand this explana-\ntion, it may at this point be helpful to briefly skip forward\nto figure 5.2 to see an illustration of the structure of an\nRNN and the flow of information through the network.\nThis recurrent loop, of activations from the output of the\nhidden layer for one input being fed back into the hidden\nlayer alongside the next input, gives an RNN a memory\nthat enables it to process each input in the context of the\nprevious inputs it has processed.4 RNNs are considered\ndeep networks because this evolving memory can be con-\nsidered as deep as the sequence is long.\nAn early well-k nown RNN is the Elman network. In\n1990, Jeffrey Locke Elman published a paper that de-\nscribed an RNN that had been trained to predict the end-\nings of simple two- and three- word utterances (Elman\n1990). The model was trained on a synthesized dataset\nof simple sentences generated using an artificial gram-\nmar. The grammar was built using a lexicon of twenty-\nthree words, with each word assigned to a single lexical\ncategory (e.g., man=NOUN- HUM, woman=NOUN- HUM,\neat=VERB- EAT, cookie=NOUN- FOOD, etc.). Using this\nlexicon, the grammar defined fifteen sentence generation\nA BRIEF hIstoRy oF dEEP LEARnIng 139 templates (e.g., NOUN- HUM+VERB- EAT+NOUN- FOOD\nwhich would generate sentences such as man eat cookie).\nOnce trained, the model was able to generate reason-\nable continuations for sentences, such as woman+eat+? =\ncookie. Furthermore, once the network was started, it was\nable to generate longer strings consisting of multiple sen-\ntences, using the context it generated itself as the input\nfor the next word, as illustrated by this three- sentence\nexample:\ngirl eat bread dog move mouse mouse move book\nAlthough this sentence generation task was applied\nto a very simple domain, the ability of the RNN to gener-\nate plausible sentences was taken as evidence that neural\nnetworks could model linguistic productivity without re-\nquiring explicit grammatical rules. Consequently, Elman\u2019s\nwork had a huge impact on psycholinguistics and psychol-\nogy. The following quote, from Churchland 1996, illus-\ntrates the importance that some researchers attributed to\nElman\u2019s work:\nThe productivity of this network is of course a feeble\nsubset of the vast capacity that any normal English\nspeaker commands. But productivity is productivity,\nand evidently a recurrent network can possess\nit. Elman\u2019s striking demonstration hardly settles\n140 ChAPtER 4 the issue between the rule- centered approach to\ngrammar and the network approach. That will be\nsome time in working itself out. But the conflict is\nnow an even one. I\u2019ve made no secret where my own\nbets will be placed. (Churchland 1996, p. 143)5\nAlthough RNNs work well with sequential data, the\nvanishing gradient problem is particularly severe in these\nnetworks. In 1997, Sepp Hochreiter and J\u00fcrgen Schmid-\nhuber, the researchers who in 1991 had presented an ex-\nplanation of the vanishing gradient problem, proposed\nthe long short- term memory (LSTM) units as a solution\nto this problem in RNNs (Hochreiter and Schmidhuber\n1997). The name of these units draws on a distinction be-\ntween how a neural network encodes long- term memory\n(understood as concepts that are learned over a period of\ntime) through training and short- term memory (under-\nstood as the response of the system to immediate stim-\nuli). In a neural network, long- term memory is encoded\nthrough adjusting the weights of the network and once\ntrained these weights do not change. Short- term memory\nis encoded in a network through the activations that flow\nthrough the network and these activation values decay\nquickly. LSTM units are designed to enable the short- term\nmemory (the activations) in the network to be propagated\nover long periods of time (or sequences of inputs). The\ninternal structure of an LSTM is relatively complex, and\nA BRIEF hIstoRy oF dEEP LEARnIng 141 we will describe it in chapter 5. The fact that LSTM can\npropagate activations over long periods enables them to\nprocess sequences that include long- distance dependen-\ncies (interactions between elements in a sequence that\nare separated by two or more positions). For example,\nthe dependency between the subject and the verb in an\nEnglish sentence: The dog/dogs in that house is/are aggres-\nsive. This has made LSTM networks suitable for language\nprocessing, and for a number of years they have been the\ndefault neural network architecture for many natural\nlanguage processing models, including machine transla-\ntion. For example, the sequence- to- sequence (seq2seq)\nmachine translation architecture introduced in 2014 con-\nnects two LSTM networks in sequence (Sutskever et al.\n2014). The first LSTM network, the encoder, processes\nthe input sequence one input at a time, and generates a\ndistributed representation of that input. The first LSTM\nnetwork is called an encoder because it encodes the se-\nquence of words into a distributed representation. The\nsecond LSTM network, the decoder, is initialized with the\ndistributed representation of the input and is trained to\ngenerate the output sequence one element at a time us-\ning a feedback loop that feeds the most recent output ele-\nment generated by the network back in as the input for\nthe next time step. Today, this seq2seq architecture is the\nbasis for most modern machine translation systems, and\nis explained in more detail in chapter 5.\n142 ChAPtER 4 By the late 1990s, most of the conceptual require-\nments for deep learning were in place, including both the\nalgorithms to train networks with multiple layers, and\nthe network architectures that are still very popular today\n(CNNs and RNNs). However, the problem of the vanishing\ngradients still stifled the creation of deep networks. Also,\nfrom a commercial perspective, the 1990s (similar to the\n1960s) experienced a wave of hype based on neural net-\nworks and unrealized promises. At the same time, a num-\nber of breakthroughs in other forms of machine learning\nmodels, such as the development of support vector ma-\nchines (SVMs), redirected the focus of the machine learn-\ning research community away from neural networks: at\nthe time SVMs were achieving similar accuracy to neural\nnetwork models but were easier to train. Together these\nfactors led to a decline in neural network research that\nlasted up until the emergence of deep learning.\nThe Era of Deep Learning\nThe first recorded use of the term deep learning is credited\nto Rina Dechter (1986), although in Dechter\u2019s paper the\nterm was not used in relation to neural networks; and the\nfirst use of the term in relation to neural networks is cred-\nited to Aizenberg et al. (2000).6 In the mid- 2000s, inter-\nest in neural networks started to grow, and it was around\nA BRIEF hIstoRy oF dEEP LEARnIng 143 this time that the term deep learning came to prominence\nto describe deep neural networks. The term deep learn-\ning is used to emphasize the fact that the networks being\ntrained are much deeper than previous networks.\nOne of the early successes of this new era of neural\nnetwork research was when Geoffrey Hinton and his col-\nleagues demonstrated that it was possible to train a deep\nneural network using a process known as greedy layer-\nwise pretraining. Greedy layer- wise pretraining begins by\ntraining a single layer of neurons that receives input di-\nrectly from the raw input. There are a number of different\nways that this single layer of neurons can be trained, but\none popular way is to use an autoencoder. An autoencoder\nis a neural network with three layers: an input layer, a hid-\nden (encoding) layer, and an output (decoding) layer. The\nnetwork is trained to reconstruct the inputs it receives in\nthe output layer; in other words, the network is trained\nto output the exact same values that it received as input.\nA very important feature in these networks is that they\nare designed so that it is not possible for the network to\nsimply copy the inputs to the outputs. For example, an\nautoencoder may have fewer neurons in the hidden layer\nthan in the input and output layer. Because the autoen-\ncoder is trying to reconstruct the input at the output layer,\nthe fact that the information from the input must pass\nthrough this bottleneck in the hidden layer forces the au-\ntoencoder to learn an encoding of the input data in the\n144 ChAPtER 4 hidden layer that captures only the most important fea-\ntures in the input, and disregards redundant or superflu-\nous information.7\nLayer- Wise Pretraining Using Autoencoders\nIn layer- wise pretraining, the initial autoencoder learns an\nencoding for the raw inputs to the network. Once this en-\ncoding has been learned, the units in the hidden encoding\nlayer are fixed, and the output (decoding) layer is thrown\naway. Then a second autoencoder is trained\u2014 but this\nautoencoder is trained to reconstruct the representation\nof the data generated by passing it through the encoding\nlayer of the initial autoencoder. In effect, this second au-\ntoencoder is stacked on top of the encoding layer of the\nfirst autoencoder. This stacking of encoding layers is con-\nsidered to be a greedy process because each encoding layer\nis optimized independently of the later layers; in other\nwords, each autoencoder focuses on finding the best solu-\ntion for its immediate task (learning a useful encoding for\nthe data it must reconstruct) rather than trying to find a\nsolution to the overall problem for the network.\nOnce a sufficient number8 of encoding layers have\nbeen trained, a tuning phase can be applied. In the tuning\nphase, a final network layer is trained to predict the tar-\nget output for the network. Unlike the pretraining of the\nearlier layers of the network, the target output for the fi-\nnal layer is different from the input vector and is specified\nA BRIEF hIstoRy oF dEEP LEARnIng 145 in the training dataset. The simplest tuning is where the\npretrained layers are kept frozen (i.e., the weights in the\npretrained layers don\u2019t change during the tuning); how-\never, it is also feasible to train the entire network during\nthe tuning phase. If the entire network is trained during\ntuning, then the layer- wise pretraining is best understood\nas finding useful initial weights for the earlier layers in the\nnetwork. Also, it is not necessary that the final prediction\nmodel that is trained during tuning be a neural network.\nIt is quite possible to take the representations of the data\ngenerated by the layer- wise pretraining and use it as the\ninput representation for a completely different type of\nmachine learning algorithm, for example, a support vector\nmachine or a nearest neighbor algorithm. This scenario is\na very transparent example of how neural networks learn\nuseful representations of data prior to the final prediction\ntask being learned. Strictly speaking, the term pretraining\ndescribes only the layer- wise training of the autoencoders;\nhowever, the term is often used to refer to both the layer-\nwise training stage and the tuning stage of the model.\nFigure 4.5 shows the stages in layer- wise pretraining.\nThe figure on the left illustrates the training of the initial\nautoencoder where an encoding layer (the black circles) of\nthree units is attempting to learn a useful representation\nfor the task of reconstructing an input vector of length 4.\nThe figure in the middle of figure 4.5 shows the training of\na second autoencoder stacked on top of the encoding layer\n146 ChAPtER 4 Targetoutput\nB1 B2 B3 D4\nA1 A2 A3 A4 C3 C5 C1 C2\nB2 B4 B6 B1 B2 B3 B1 B2 B3\nA1 A2 A3 A4 A1 A2 A3 A4 A1 A2 A3 A4\nPretraininglayerB PretraininglayerC Tuning\nFigure 4.5 The pretraining and tuning stages in greedy layer- wise\npretraining. Black circles represent the neurons whose training is the primary\nobjective at each training stage. The gray background marks the components\nin the network that are frozen during each training stage.\nof the first autoencoder. In this autoencoder, a hidden\nlayer of two units is attempting to learn an encoding for an\ninput vector of length 3 (which in turn is an encoding of a\nvector of length 4). The grey background in each figure de-\nmarcates the components in the network that are frozen\nduring this training stage. The figure on the right shows\nthe tuning phase where a final output layer is trained to\npredict the target feature for the model. For this example,\nin the tuning phase the pretrained layers in the network\nhave been frozen.\nLayer- wise pretraining was important in the evolu-\ntion of deep learning because it was the first approach\nto training deep networks that was widely adopted.9\nHowever, today most deep learning networks are trained\nA BRIEF hIstoRy oF dEEP LEARnIng 147 without using layer- wise pretraining. In the mid- 2000s,\nresearchers began to appreciate that the vanishing gra-\ndient problem was not a strict theoretical limit, but was\ninstead a practical obstacle that could be overcome. The\nvanishing gradient problem does not cause the error gra-\ndients to disappear entirely; there are still gradients being\nbackpropagated through the early layers of the network, it\nis just that they are very small. Today, there are a number\nof factors that have been identified as important in suc-\ncessfully training a deep network.\nWeight Initialization and ReLU Activation Functions\nOne factor that is important in successfully training a\ndeep network is how the network weights are initialized.\nThe principles controlling how weight initialization af-\nfects the training of a network are still not clear. There\nare, however, weight initialization procedures that have\nbeen empirically shown to help with training a deep net-\nwork. Glorot initialization10 is a frequently used weight\ninitialization procedure for deep networks. It is based on\na number of assumptions but has empirical success to sup-\nport its use. To get an intuitive understanding of Glorot\ninitialization, consider the fact that there is typically a re-\nlationship between the magnitude of values in a set and\nthe variance of the set: generally the larger the values in\na set, the larger the variance of the set. So, if the variance\ncalculated on a set of gradients propagated through a layer\n148 ChAPtER 4 In the mid- 2000s,\nresearchers began to\nappreciate that the\nvanishing gradient\nproblem was not a strict\ntheoretical limit, but\nwas instead a practical\nobstacle that could be\novercome. at one point in the network is similar to the variance for\nthe set of gradients propagated through another layer in\na network, it is likely that the magnitude of the gradients\npropagated through both of these layers will also be simi-\nlar. Furthermore, the variance of gradients in a layer can\nbe related to the variance of the weights in the layer, so a\npotential strategy to maintain gradients flowing through\na network is to ensure similar variances across each of the\nlayer in a network. Glorot initialization is designed to ini-\ntialize the weight in a network in such a way that all of the\nlayers in a network will have a similar variance in terms\nof both forward pass activations and the gradients propa-\ngated during the backward pass in backpropagation. Glo-\nrot initialization defines a heuristic rule to meet this goal\nthat involves sampling the weights for a network using the\nfollowing uniform distribution (where w is the weight on a\nconnection between layer j and j+i that is being initialized,\nU[- a,a] is the uniform distribution over the interval (- a,a),\nn is the number of neurons in layer j, and the notation w\nj\n~ U indicates that the value of w is sampled from distribu-\ntion U)11:\n\uf8ee 6 6 \uf8f9\nw\u223cU\uf8ef\u2212 , \uf8fa\n\uf8f0 n j+n j+1 n j+n j+1 \uf8fb\nAnother factor that contributes to the success or\nfailure of training a deep network is the selection of the\n150 ChAPtER 4 activation function used in the neurons. Backpropagating\nan error gradient through a neuron involves multiplying\nthe gradient by the value of the derivative of the activation\nfunction at the activation value of the neuron recorded\nduring the forward pass. The derivatives of the logistic\nand tanh activation functions have a number of properties\nthat can exacerbate the vanishing gradient problem if they\nare used in this multiplication step. Figure 4.6 presents a\nplot of the logistic function and the derivative of the logis-\ntic function. The maximum value of the derivative is 0.25.\nConsequently, after an error gradient has been multiplied\n0.1\nLogistic(z)\n\u2202 logistic(z)\n\u2202 z 8.0\n)z(noitavitcA 6.0\n4.0\nmax = 0.25\n2.0\nsaturated = 0 saturated = 0 0.0\n\u221210 \u22125 0 5 10\nz\nFigure 4.6 Plots of the logistic function and the derivative of the logistic\nfunction.\nA BRIEF hIstoRy oF dEEP LEARnIng 151 by the value of the derivative of the logistic function at\nthe appropriate activation for the neuron, the maximum\nvalue the gradient will have is a quarter of the gradient\nprior to the multiplication. Another problem with using\nthe logistic function is that there are large portions of the\ndomain of the function where the function is saturated\n(returning values that very close to 0 or 1), and the rate\nof change of the function in these regions is near zero;\nthus, the derivative of the function is near 0. This is an\nundesirable property when backpropagating error gradi-\nents because the error gradients will be forced to zero (or\nclose to zero) when backpropagated through any neuron\nwhose activation is within one of these saturated regions.\nIn 2011 it was shown that switching to a rectified linear\nactivation function, g(x)=max(0,x), improved training\nfor deep feedforward neural networks (Glorot et al. 2011).\nNeurons that use a rectified linear activation function are\nknown as rectified linear units (ReLUs). One advantage\nof ReLUs is that the activation function is linear for the\npositive portion of its domain with a derivative equal to 1.\nThis means that gradients can flow easily through ReLUs\nthat have positive activation. However, the drawback of\nReLUs is that the gradient of the function for the nega-\ntive part of its domain is zero, so ReLUs do not train in\nthis portion of the domain. Although undesirable, this\nis not necessarily a fatal flaw for learning because when\nbackpropagating through a layer of ReLUs the gradients\n152 ChAPtER 4 can still flow through the ReLUs in the layers that have\npositive activation. Furthermore, there are a number of\nvariants of the basic ReLU that introduce a gradient on\nthe negative side of the domain, a commonly used variant\nbeing the leaky ReLU (Maas et al. 2013). Today, ReLUs (or\nvariants of ReLUs) are the most frequently used neurons\nin deep learning research.\nThe Virtuous Cycle: Better Algorithms, Faster Hardware,\nBigger Data\nAlthough improved weight initialization methods and\nnew activation functions have both contributed to the\ngrowth of deep learning, in recent years the two most\nimportant factors driving deep learning have been the\nspeedup in computer power and the massive increase in\ndataset sizes. From a computational perspective, a major\nbreakthrough for deep learning occurred in the late 2000s\nwith the adoption of graphical processing units (GPUs)\nby the deep learning community to speed up training. A\nneural network can be understood as a sequence of matrix\nmultiplications that are interspersed with the application\nof nonlinear activation functions, and GPUs are optimized\nfor very fast matrix multiplication. Consequently, GPUs\nare ideal hardware to speed up neural network train-\ning, and their use has made a significant contribution to\nthe development of the field. In 2004, Oh and Jung re-\nported a twentyfold performance increase using a GPU\nA BRIEF hIstoRy oF dEEP LEARnIng 153 implementation of a neural network (Oh and Jung 2004),\nand the following year two further papers were published\nthat demonstrated the potential of GPUs to speed up the\ntraining of neural networks: Steinkraus et al. (2005) used\nGPUs to train a two- layer neural network, and Chella-\npilla et al. (2006) used GPUs to train a CNN. However, at\nthat time there were significant programming challenges\nto using GPUs for training networks (the training algo-\nrithm had to be implemented as a sequence of graphics\noperations), and so the initial adoption of GPUs by neural\nnetwork researchers was relatively slow. These program-\nming challenges were significantly reduced in 2007 when\nNVIDIA (a GPU manufacturer) released a C- like program-\nming interface for GPUs called CUDA (compute unified\ndevice architecture).12 CUDA was specifically designed to\nfacilitate the use of GPUs for general computing tasks. In\nthe years following the release of CUDA, the use of GPUs\nto speed up neural network training became standard.\nHowever, even with these more powerful computer\nprocessors, deep learning would not have been possible\nunless massive datasets had also become available. The de-\nvelopment of the internet and social media platforms, the\nproliferation of smartphones and \u201cinternet of things\u201d sen-\nsors, has meant that the amount of data being captured\nhas grown at an incredible rate over the last ten years.\nThis has made it much easier for organizations to gather\nlarge datasets. This growth in data has been incredibly\n154 ChAPtER 4 important to deep learning because neural network mod-\nels scale well with larger data (and in fact they can struggle\nwith smaller datasets). It has also prompted organizations\nto consider how this data can be used to drive the develop-\nment of new applications and innovations. This in turn\nhas driven a need for new (more complex) computational\nmodels in order to deliver these new applications. And, the\ncombination of large data and more complex algorithms\nrequires faster hardware in order to make the necessary\ncomputational workload tractable. Figure 4.7 illustrates\nthe virtuous cycle between big data, algorithmic break-\nthroughs (e.g., better weight initialization, ReLUs, etc.),\nBig\ndata\nFaster\nhardware\nBetter\nalgorithms\nFigure 4.7 The virtuous cycle driving deep learning. Figure inspired by\nfigure 1.2 in Reagen et al. 2017.\nA BRIEF hIstoRy oF dEEP LEARnIng 155 and improved hardware that is driving the deep learning\nrevolution.\nSummary\nThe history of deep learning reveals a number of under-\nlying themes. There has been a shift from simple binary\ninputs to more complex continuous valued input. This\ntrend toward more complex inputs is set to continue\nbecause deep learning models are most useful in high-\ndimensional domains, such as image processing and lan-\nguage. Images often have thousands of pixels in them,\nand language processing requires the ability represents\nand process hundreds of thousands of different words.\nThis is why some of the best- known applications of deep\nlearning are in these domains, for example, Facebook\u2019s\nface- recognition software, and Google\u2019s neural machine\ntranslation system. However, there are a growing number\nof new domains where large and complex digital datasets\nare being gathered. One area where deep learning has the\npotential to make a significant impact within the coming\nyears is healthcare, and another complex domain is the\nsensor rich field of self- driving cars.\nSomewhat surprisingly, at the core of these powerful\nmodels are simple information processing units: neurons.\nThe connectionist idea that useful complex behavior can\n156 ChAPtER 4 emerge from the interactions between large numbers of\nsimple processing units is still valid today. This emergent\nbehavior arises through the sequences of layers in a net-\nwork learning a hierarchical abstraction of increasingly\ncomplex features. This hierarchical abstraction is achieved\nby each neuron learning a simple transformation of the\ninput it receives. The network as a whole then composes\nthese sequences of smaller transformations in order to\napply a complex (highly) nonlinear mapping to the input.\nThe output from the model is then generated by the final\noutput layers of neuron, based the learned representa-\ntion generated through the hierarchical abstraction. This\nis why depth is such an important factor in neural net-\nworks: the deeper the network, the more powerful the\nmodel becomes in terms of its ability to learn complex\nnonlinear mappings. In many domains, the relationship\nbetween input data and desired outputs involves just such\ncomplex nonlinear mappings, and it is in these domains\nthat deep learning models outdo other machine learning\napproaches.\nAn important design choice in creating a neural net-\nwork is deciding which activation function to use within\nthe neurons in a network. The activation function within\neach neuron in a network is how nonlinearity is intro-\nduced into the network, and as a result it is a necessary\ncomponent if the network is to learn a nonlinear mapping\nfrom inputs to output. As networks have evolved, so too\nA BRIEF hIstoRy oF dEEP LEARnIng 157 have the activation functions used in them. New activa-\ntion functions have emerged throughout the history of\ndeep learning, often driven by the need for functions with\nbetter properties for error- gradient propagation: a major\nfactor in the shift from threshold to logistic and tanh acti-\nvation functions was the need for differentiable functions\nin order to apply backpropagation; the more recent shift\nto ReLUs was, similarly, driven by the need to improve the\nflow of error gradients through the network. Research on\nactivations functions is ongoing, and new functions will\nbe developed and adopted in the coming years.\nAnother important design choice in creating a neural\nnetwork is to decide on the structure of the network: for\nexample, how should the neurons in the network be con-\nnected together? In the next chapter, we will discuss two\nvery different answers to this question: convolution neu-\nral networks and recurrent neural networks.\n158 ChAPtER 4 5\nCONVOLUTIONAL AND RECURRENT\nNEURAL NETWORKS\nTailoring the structure of a network to the specific char-\nacteristics of the data from a task domain can reduce the\ntraining time of the network, and improves the accuracy\nof the network. Tailoring can be done in a number of ways,\nsuch as: constraining the connections between neurons\nin adjacent layers to subsets (rather than having fully\nconnected layers); forcing neurons to share weights; or\nintroducing backward connections into the network. Tai-\nloring in these ways can be understood as building domain\nknowledge into the network. Another, related, perspec-\ntive is it helps the network to learn by constraining the\nset of possible functions that it can learn, and by so do-\ning guides the network to find a useful solution. It is not\nalways clear how to fit a network structure to a domain,\nbut for some domains where the data has a very regular\nstructure (e.g., sequential data such as text, or gridlike data such as images) there are well-k nown network ar-\nchitectures that have proved successful. This chapter will\nintroduce two of the most popular deep learning architec-\ntures: convolutional neural networks and recurrent neural\nnetworks.\nConvolutional Neural Networks\nConvolution neural networks (CNNs) were designed for\nimage recognition tasks and were originally applied to the\nchallenge of handwritten digit recognition (Fukushima\n1980; LeCun 1989). The basic design goal of CNNs was to\ncreate a network where the neurons in the early layer of\nthe network would extract local visual features, and neu-\nrons in later layers would combine these features to form\nhigher- order features. A local visual feature is a feature\nwhose extent is limited to a small patch, a set of neighbor-\ning pixels, in an image. For example, when applied to the\ntask of face recognition, the neurons in the early layers of a\nCNN learn to activate in response to simple local features\n(such as lines at a particular angle, or segments of curves),\nneurons deeper in the network combine these low-l evel\nfeatures into features that represent body parts (such as\neyes or noises), and the neurons in the final layers of the\nnetwork combine body part activations in order to be able\nto identify whole faces in an image.\n160 ChAPtER 5 Using this approach, the fundamental task in image\nrecognition is learning the feature detection functions\nthat can robustly identify the presence, or absence, of local\nvisual features in an image. The process of learning func-\ntions is at the core of neural networks, and is achieved by\nlearning the appropriate set of weights for the connec-\ntions in the network. CNNs learn the feature detection\nfunctions for local visual features in this way. However, a\nrelated challenge is designing the architecture of the net-\nwork so that the network will identify the presence of a\nlocal visual feature in an image irrespective of where in\nthe image it occurs. In other words, the feature detection\nfunctions must be able to work in a translation invariant\nmanner. For example, a face recognition system should be\nable to recognize the shape of an eye in an image whether\nthe eye is in the center of the image or in the top-r ight\ncorner of the image. This need for translation invariance\nhas been a primary design principle of CNNs for image\nprocessing, as Yann LeCun stated in 1989:\nIt seems useful to have a set of feature detectors that\ncan detect a particular instance of a feature anywhere\non the input plane. Since the precise location of a\nfeature is not relevant to the classification, we can\nafford to lose some position information in the\nprocess. (LeCun 1989, p. 14)\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 161 CNNs achieve this translation invariance of local vi-\nsual feature detection by using weight sharing between\nneurons. In an image recognition setting, the function\nimplemented by a neuron can be understood as a visual\nfeature detector. For example, neurons in the first hidden\nlayer of the network will receive a set of pixel values as\ninput and output a high activation if a particular pattern\n(local visual feature) is present in this set of pixels. The\nfact that the function implemented by a neuron is defined\nby the weights the neuron uses means that if two neurons\nuse the same set of weights then they both implement the\nsame function (feature detector). In chapter 4, we intro-\nduced the concept of a receptive field to describe the area\nthat a neuron receives its input from. If two neurons share\nthe same weights but have different receptive fields (i.e.,\neach neuron inspects different areas of the input), then\ntogether the neurons act as a feature detector that acti-\nvates if the feature occurs in either of the receptive fields.\nConsequently, it is possible to design a network with\ntranslation invariant feature detection by creating a set of\nneurons that share the same weights and that are orga-\nnized so that: (1) each neuron inspects a different portion\nof the image; and (2) together the receptive fields of the\nneurons cover the entire image.\nThe scenario of searching an image in a dark room with\na flashlight that has a narrow beam is sometimes used to\nexplain how a CNN searches an image for local features.\n162 ChAPtER 5 At each moment you can point the flashlight at a region of\nthe image and inspect that local region. In this flashlight\nmetaphor, the area of the image illuminated by the flash-\nlight at any moment is equivalent to the receptive field of a\nsingle neuron, and so pointing the flashlight at a location\nis equivalent to applying the feature detection function to\nthat local region. If, however, you want to be sure you in-\nspect the whole image, then you might decide to be more\nsystematic in how you direct the flashlight. For example,\nyou might begin by pointing the flashlight at the top- left\ncorner of the image and inspecting that region. You then\nmove the flashlight to the right, across the image, inspect-\ning each new location as it becomes visible, until you reach\nthe right side of the image. You then point the flashlight\nback to the left of the image, but just below where you\nbegan, and move across the image again. You repeat this\nprocess until you reach the bottom-r ight corner of the im-\nage. The process of sequentially searching across an im-\nage and at each location in the search applying the same\nfunction to the local (illuminated) region is the essence of\nconvolving a function across an image. Within a CNN, this\nsequential search across an image is implemented using\na set of neurons that share weights and whose union of\nreceptive fields covers the entire image.\nFigure 5.1 illustrates the different stages of processing\nthat are often found in a CNN. The 6\u00d76 matrix on the left\nof the figure represents the image that is the input to the\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 163 CNN. The 4\u00d74 matrix immediately to the right of the in-\nput represents a layer of neurons that together search the\nentire image for the presence of a particular local feature.\nEach neuron in this layer is connected to a different 3\u00d73\nreceptive field (area) in the image, and they all apply the\nsame weight matrix to their inputs:\n\uf8eew w w \uf8f9\n0 1 2\n\uf8ef \uf8fa\nw w w\n\uf8ef 3 4 5\uf8fa\n\uf8f0\uf8efw w w 8\uf8fb\uf8fa\n6 7\nThe receptive field of the neuron [0,0] (top- left) in\nthis layer is marked with the gray square covering the 3\u00d73\narea in the top- left of the input image. The dotted arrows\nemerging from each of the locations in this gray area rep-\nresent the inputs to neuron [0,0]. The receptive field of\nthe neighboring neuron [0,1] is indicated by 3\u00d73 square,\noutlined in bold in the input image. Notice that the recep-\ntive fields of these two neurons overlap. The amount of\noverlap of receptive fields is controlled by a hyperparam-\neter called the stride length. In this instance, the stride\nlength is one, meaning that for each position moved in\nthe layer the receptive field of the neuron is translated by\nthe same amount on the input. If the stride length hyper-\nparameter is increased, the amount of overlap between\nreceptive fields is decreased.\n164 ChAPtER 5 The receptive fields of both of these neurons ([0,0] and\n[0,1]) are matrices of pixel values and the weights used by\nthese neurons are also matrices. In computer vision, the\nmatrix of weights applied to an input is known as the ker-\nnel (or convolution mask); the operation of sequentially\npassing a kernel across an image and within each local\nregion, weighting each input and adding the result to its\nlocal neighbors, is known as a convolution. Notice that a\nconvolution operation does not include a nonlinear activa-\ntion function (this is applied at a later stage in processing).\nThe kernel defines the feature detection function that all\nthe neurons in the convolution implement. Convolving\na kernel across an image is equivalent to passing a local\nvisual feature detector across the image and recording all\nthe locations in the image where the visual feature was\npresent. The output from this process is a map of all the\nlocations in the image where the relevant visual feature oc-\ncurred. For this reason, the output of a convolution process\nis sometimes known as a feature map. As noted above, the\nconvolution operation does not include a nonlinear activa-\ntion function (it only involves a weighted summation of\nthe inputs). Consequently, it is standard to apply a nonlin-\nearity operation to a feature map. Frequently, this is done\nby applying a rectified linear function to each position in a\nfeature map; the rectified linear activation function is de-\nfined as: rectifier(z)=max(0,z). Passing a rectified linear\nactivation function over a feature map simply changes all\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 165 negative values to 0. In figure 5.1, the process of updat-\ning a feature map by applying a rectified linear activation\nfunction to each of its elements is represented by the layer\nlabeled Nonlinearity.\nThe quote from Yann LeCun, at the start of this sec-\ntion, mentions that the precise location of a feature in an\nimage may not be relevant to an image processing task.\nWith this in mind, CNNs often discard location informa-\ntion in favor of generalizing the network\u2019s ability to do\nimage classification. Typically, this is achieved by down-\nsampling the updated feature map using a pooling layer.\nIn some ways pooling is similar to the convolution opera-\ntion described above, in so far as pooling involves repeat-\nedly applying the same function across an input space. For\npooling, the input space is frequently a feature map whose\nelements have been updated using a rectified linear func-\ntion. Furthermore, each pooling operation has a receptive\nfield on the input space\u2014a lthough, for pooling, the recep-\ntive fields sometimes do not overlap. There are a number\nof different pooling functions used; the most common is\ncalled max pooling, which returns the maximum value of\nany of its inputs. Calculating the average value of the in-\nputs is also used as a pooling function.\nThe operation sequence of applying a convolution,\nfollowed by a nonlinearity, to the feature map, and then\ndown- sampling using pooling, is relatively standard across\nmost CNNs. Often these three operations are together\n166 ChAPtER 5 Convolving a kernel\nacross an image is\nequivalent to passing\na local visual feature\ndetector across the\nimage and recording all\nthe locations in the\nimage where the visual\nfeature was present. considered to define a convolutional layer in a network,\nand this is how they are presented in figure 5.1.\nThe fact that a convolution searches an entire image\nmeans that if the visual feature (pixel pattern) that the\nfunction (defined by shared kernel) detects occurs any-\nwhere in the image, its presence will be recorded in the\nfeature map (and if pooling is used, also in the subsequent\noutput from the pooling layer). In this way, a CNN sup-\nports translation invariant visual feature detection. How-\never, this has the limitation that the convolution can only\nidentify a single type of feature. CNNs generalize beyond\none feature by training multiple convolutional layers\nin parallel (or filters), with each filter learning a single\nConvolution:\nlayerofneurons\nInput Layerof\nwithsharedweights\nimage nonlinearity\nfunctions\nFeature\nmap\nPooling\nlayer Dense\nlayer\nConvolutionallayer\nFigure 5.1 Illustrations of the different stages of processing in a\nconvolutional layer. Note in this figure the Image and Feature Map are data\nstructures; the other stages represent operations on data.\n168 ChAPtER 5 kernel matrix (feature detection function). Note the con-\nvolution layer in figure 5.1 illustrates a single filter. The\noutputs of multiple filters can be integrated in a variety\nof ways. One way to integrate information from differ-\nent filters is to take the feature maps generated by the\nseparate filters and combine them into a single multifil-\nter feature map. A subsequent convolutional layer then\ntakes this multifilter feature map as input. Another other\nway to integrate information from different filter is to\nuse a densely connected layer of neurons. The final layer\nin figure 5.1 illustrates a dense layer. This dense layer\noperates in exactly the same way as a standard layer in\na fully connected feedforward network. Each neuron in\nthe dense layer is connected to all of the elements out-\nput by each of the filters, and each neuron learns a set\nof weights unique to itself that it applies to the inputs.\nThis means that each neuron in a dense layer can learn\na different way to integrate information from across the\ndifferent filters.\nThe AlexNet CNN, which won the ImageNet Large-\nScale Visual Recognition Challenge (ILSVRC) in 2012,\nhad five convolutional layers, followed by three dense lay-\ners. The first convolutional layer had ninety- six different\nkernels (or filters) and included a ReLU nonlinearity and\npooling. The second convolution layer had 256 kernels and\nalso included ReLU nonlinearity and pooling. The third,\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 169 fourth, and fifth convolutional layers did not include a\nnonlinearity step or pooling, and had 384, 384, and 256\nkernels, respectively. Following the fifth convolutional\nlayer, the network had three dense layers with 4096 neu-\nrons each. In total, AlexNet had sixty million weights and\n650,000 neurons. Although sixty million weights is a large\nnumber, the fact that many of the neurons shared weights\nactually reduced the number of weights in the network.\nThis reduction in the number of required weights is one\nof the advantages of CNN networks. In 2015, Microsoft\nResearch developed a CNN network called ResNet, which\nwon the ILSVRC 2015 challenge (He et al. 2016). The\nResNet architecture extended the standard CNN architec-\nture using skip- connections. A skip- connection takes the\noutput from one layer in the network and feeds it directly\ninto a layer that may be much deeper in the network. Us-\ning skip- connections it is possible to train very deep net-\nworks. In fact, the ResNet model developed by Microsoft\nResearch had a depth of 152 layers.\nRecurrent Neural Networks\nRecurrent neural networks (RNNs) are tailored to the\nprocessing of sequential data. An RNN processes a se-\nquence of data by processing each element in the sequence\none at time. An RNN network only has a single hidden\n170 ChAPtER 5 layer, but it also has a memory buffer that stores the out-\nput of this hidden layer for one input and feeds it back\ninto the hidden layer along with the next input from the\nsequence. This recurrent flow of information means that\nthe network processes each input within the context gen-\nerated by processing the previous input, which in turn was\nprocessed in the context of the input preceding it. In this\nway, the information that flows through the recurrent\nloop encodes contextual information from (potentially)\nall of the preceding inputs in the sequence. This allows\nthe network to maintain a memory of what it has seen\npreviously in the sequence to help it decide what to do\nwith the current input. The depth of an RNN arises from\nthe fact that the memory vector is propagated forward\nand evolved through each input in the sequence; as a re-\nsult an RNN network is considered as deep as a sequence\nis long.\nFigure 5.2 illustrates the architecture of an RNN and\nshows how information flows through the network as\nit processes a sequence. At each time step, the network\nin this figure receives a vector containing two elements\nas input. The schematic on the left of figure 5.2 (time\nstep=1.0) shows the flow of information in the network\nwhen it receives the first input in the sequence. This input\nvector is fed forward into the three neurons in the hid-\nden layer of the network. At the same time these neurons\nalso receive whatever information is stored in the memory\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 171 The depth of an RNN\narises from the fact that\nthe memory vector is\npropagated forward and\nevolved through each\ninput in the sequence;\nas a result an RNN\nnetwork is considered\nas deep as a sequence\nis long. buffer. Because this is the initial input, the memory buf-\nfer will only contain default initialization values. Each of\nthe neurons in the hidden layer will process the input and\ngenerate an activation. The schematic in the middle of fig-\nure 5.2 (time step=1.5) shows how this activation flows\non through the network: the activation of each neuron is\npassed to the output layer where it is processed to gener-\nate the output of the network, and it is also stored in the\nmemory buffer (overwriting whatever information was\nstored there). The elements of the memory buffer simply\nstore the information written to them; they do not trans-\nform it in any way. As a result, there are no weights on\nthe edges going from the hidden units to the buffer. There\nare, however, weights on all the other edges in the net-\nwork, including those from the memory buffer units to\nthe neurons in the hidden layer. At time step 2, the net-\nwork receives the next input from the sequence, and this\nis passed to the hidden layer neurons along with the infor-\nmation stored in the buffer. This time the buffer contains\nthe activations that were generated by the hidden neurons\nin response to the first input.\nFigure 5.3 shows an RNN that has been unrolled\nthrough time as it processes a sequence of inputs\n[X ,X ,\u2026,X ]. Each box in this figure represents a layer\n1 2 t\nof neurons. The box labeled h represents the state of\n0\nthe memory buffer when the network is initialized; the\nboxes labeled [h ,\u2026,h ] represent the hidden layer of the\n1 t\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 173 evitca\n.emit\neht\ntaht\nera\ndlob ta\nevitca\nni\nsworra ton\nera\nehT\ntaht\n.stupni\nsnoitcennoc\nfo\necneuqes\nwohs\na sworra\nsessecorp\ndehsad\nti\nsa\neht\nNNR\n;tniop\nna\nni emit\nnoitamrofni\nhcae\nta\nwolf\nfo\nwolf noitamrofni\nehT\n2.5\nfo\nerugiF shtap\n174 ChAPtER 5 Output: Y Y Y Y\n1 2 3 t\nh h h h h\n0 1 2 3 \u00b7\u00b7\u00b7 t\nInput: X1 X2 X3 Xt\nFigure 5.3 An RNN network unrolled through time as it processes a\nsequence of inputs [X ,X ,\u2026,X ].\n1 2 t\nnetwork at each time step; and the boxes labeled [Y ,\u2026,Y ]\n1 t\nrepresent the output layer of the network at each time\nstep. Each of the arrows in the figure represents a set of\nconnections between one layer and another layer. For ex-\nample, the vertical arrow from X to h represents the con-\n1 1\nnections between the input layer and the hidden layer at\ntime step 1. Similarly, the horizontal arrows connecting\nthe hidden layers represent the storing of the activations\nfrom a hidden state at one time step in the memory buffer\n(not shown) and the propagation of these activations to\nthe hidden layer at the next time step through the connec-\ntions from the memory buffer to the hidden state. At each\ntime step, an input from the sequence is presented to the\nnetwork and is fed forward to the hidden layer. The hid-\nden layer generates a vector of activations that is passed\nto the output layer and is also propagated forward to the\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 175 next time step along the horizontal arrows connecting the\nhidden states.\nAlthough RNNs can process a sequence of inputs,\nthey struggle with the problem of vanishing gradients.\nThis is because training an RNN to process a sequence of\ninputs requires the error to be backpropagated through\nthe entire length of the sequence. For example, for the\nnetwork in figure 5.3, the error calculated on the output\nY must be backpropagated through the entire network\nt\nso that it can be used to update the weights on the con-\nnections from h and X to h . This entails backpropagat-\n0 1 1\ning the error through all the hidden layers, which in turn\ninvolves repeatedly multiplying the error by the weights\non the connections feeding activations from one hidden\nlayer forward to the next hidden layer. A particular prob-\nlem with this process is that it is the same set of weights\nthat are used on all the connections between the hidden\nlayers: each horizontal arrow represents the same set of\nconnections between the memory buffer and the hidden\nlayer, and the weights on these connections are stationary\nthrough time (i.e., they don\u2019t change from one time step\nto the next during the processing of a given sequence of\ninputs). Consequently, backpropogating an error through\nk time steps involves (among other multiplications) mul-\ntiplying the error gradient by the same set of weights k\ntimes. This is equivalent to multiplying each error gradi-\nent by a weight raised to the power of k. If this weight is\n176 ChAPtER 5 less than 1, then when it is raised to a power, it diminishes\nat an exponential rate, and consequently, the error gra-\ndient also tends to diminish at an exponential rate with\nrespect to the length of the sequence\u2014 and vanish.\nLong short- term memory networks (LSTMs) are de-\nsigned to reduce the effect of vanishing gradients by re-\nmoving the repeated multiplication by the same weight\nvector during backpropagation in an RNN. At the core of\nan LSTM1 unit is a component called the cell. The cell is\nwhere the activation (the short- term memory) is stored\nand propagated forward. In fact, the cell often maintains\na vector of activations. The propagation of the activations\nwithin the cell through time is controlled by three compo-\nnents called gates: the forget gate, the input gate, and the\noutput gate. The forget gate is responsible for determining\nwhich activations in the cell should be forgotten at each\ntime step, the input gate controls how the activations in\nthe cell should be updated in response to the new input,\nand the output gate controls what activations should be\nused to generate the output in response to the current\ninput. Each of the gates consists of layers of standard neu-\nrons, with one neuron in the layer per activation in the\ncell state.\nFigure 5.4 illustrates the internal structure of an\nLSTM cell. Each of the arrows in this image represents a\nvector of activations. The cell runs along the top of the\nfigure from left (c ) to right (c ). Activations in the cell\nt\u22121 t\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 177 can take values in the range -1 to +1. Stepping through the\nprocessing for a single input, the input vector x is first\nt\nconcatenated with the hidden state vector that has been\npropagated forward from the preceding time step h .\nt\u22121\nWorking from left to right through the processing of the\ngates, the forget gate takes the concatenation of the input\nand the hidden state and passes this vector through a layer\nof neurons that use a sigmoid (also known as logistic)2 ac-\ntivation function. As a result of the neurons in the forget\nlayer using sigmoid activation functions the output of this\nforget layer is a vector of values in the range 0 to 1. The cell\nstate is then multiplied by this forget vector. The result\nof this multiplication is that activations in the cell state\nthat are multiplied by components in the forget vector\nwith values near 0 are forgotten, and activations that are\nmultiplied by forget vector components with values near 1\nare remembered. In effect, multiplying the cell state by the\noutput of a sigmoid layer acts as a filter on the cell state.\nNext, the input gate decides what information should\nbe added to the cell state. The processing in this step is\ndone by the components in the middle block of figure 5.4,\nmarked Input. This processing is broken down into two\nsubparts. First, the gate decides which elements in the\ncell state should be updated, and second it decides what\ninformation should be included in the update. The deci-\nsion regarding which elements in the cell state should be\nupdated is implemented using a similar filter mechanism\n178 ChAPtER 5 Forget Input Output\nc t 1 + c t\n\u2212 \u00d7\nT\n\u00d7 \u00d7\n\u03c3 \u03c3 T \u03c3\nh t 1 h t\n\u2212\nxt output\nFigure 5.4 Schematic of the internal structure of an LSTM unit: \u03c3\nrepresents a layer of neurons with sigmoid activations, T represents a layer\nof neurons with tanh activations, \u00d7 represents vector multiplication, and +\nrepresents vector addition. The figure is inspired by an image by Christopher\nOlah available at: http://colah.github.io/posts/2015-08-Understanding\n-LSTMs/.\nto the forget gate: the concatenated input x plus hidden\nt\nstate h is passed through a layer of sigmoid units to\nt\u22121\ngenerate a vector of elements, the same width as the cell,\nwhere each element in the vector is in the range 0 to 1;\nvalues near 0 indicate that the corresponding cell element\nwill not be updated, and values near 1 indicate that the\ncorresponding cell element will be updated. At the same\ntime that the filter vector is generated, the concatenated\ninput and hidden state are also passed through a layer\nof tanh units (i.e., neurons that use the tanh activation\nfunction). Again, there is one tanh unit for each activation\nin the LSTM cell. This vector represents the information\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 179 that may be added to the cell state. Tanh units are used\nto generate this update vector because tanh units out-\nput values in the range - 1 to +1, and consequently the\nvalue of the activations in the cell elements can be both\nincreased and decreased by an update.3 Once these two\nvectors have been generated, the final update vector is\ncalculated by multiplying the vector output from the\ntanh layer by the filter vector generated from the sigmoid\nlayer. The resulting vector is then added to the cell using\nvector addition.\nThe final stage of processing in an LSTM is to decide\nwhich elements of the cell should be output in response to\nthe current input. This processing is done by the compo-\nnents in the block marked Output (on the right of figure\n5.4). A candidate output vector is generated by passing\nthe cell through a tanh layer. At the same time, the con-\ncatenated input and propagated hidden state vector are\npassed through a layer of sigmoid units to create another\nfilter vector. The actual output vector is then calculated by\nmultiplying the candidate output vector by this filter vec-\ntor. The resulting vector is then passed to the output layer,\nand is also propagated forward to the next time step as the\nnew hidden state h .\nt\nThe fact that an LSTM unit contains multiple layers\nof neurons means that an LSTM is a network in itself.\nHowever, an RNN can be constructed by treating an LSTM\nas the hidden layer in the RNN. In this configuration, an\n180 ChAPtER 5 LSTM unit receives an input at each time step and gener-\nates an output for each input. RNNs that use LSTM units\nare often known as LSTM networks.\nLSTM networks are ideally suited for natural language\nprocessing (NLP). A key challenge in using a neural net-\nwork to do natural language processing is that the words\nin language must be converted into vectors of numbers.\nThe word2vec models, created by Tomas Mikolov and col-\nleagues at Google research, are one of the most popular\nways of doing this conversion (Mikolov et al. 2013). The\nword2vec models are based on the idea that words that\nappear in similar contexts have similar meanings. The\ndefinition of context here is surrounding words. So for ex-\nample, the words London and Paris are semantically simi-\nlar because each of them often co- occur with words that\nthe other word also co- occurs with, such as: capital, city,\nEurope, holiday, airport, and so on. The word2vec models\nare neural networks that implement this idea of seman-\ntic similarity by initially assigning random vectors to each\nword and then using co- occurrences within a corpus to it-\neratively update these vectors so that semantically similar\nwords end up with similar vectors. These vectors (known\nas word embeddings) are then used to represent a word\nwhen it is being input to a neural network.\nOne of the areas of NLP where deep learning has\nhad a major impact is in machine translation. Figure\n5.5 presents a high- level schematic of the seq2seq (or\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 181 Life is beautiful <eos>\nEncoder\nh1 h2 h3 h4 C d1 d2 d3\nDecoder\nbelle est vie La <eos>\nFigure 5.5 Schematic of the seq2seq (or encoder-d ecoder) architecture.\nencoder- decoder) architecture for neural machine transla-\ntion (Sutskever et al. 2014). This architecture is composed\nof two LSTM networks that have been joined together.\nThe first LSTM network processes the input sentence in\na word- by- word fashion. In this example, the source lan-\nguage is French. The words are entered into the system in\nreverse order as it has been found that this leads to better\ntranslations. The symbol eos is a special end of sentence\nsymbol. As each word is entered, the encoder updates the\nhidden state and propagates it forward to the next time\nstep. The hidden state generated by the encoder in re-\nsponse to the eos symbol is taken to be a vector represen-\ntation of the input sentence. This vector is passed as the\ninitial input to the decoder LSTM. The decoder is trained\nto output the translation sentence word by word, and af-\nter each word has been generated, this word is fed back\ninto the system as the input for the next time step. In a\n182 ChAPtER 5 way, the decoder is hallucinating the translation because\nit uses its own output to drive its own generation pro-\ncess. This process continues until the decoder outputs an\neos symbol.\nThe idea of using a vector of numbers to represent the\n(interlingual) meaning of a sentence is very powerful, and\nthis concept has been extended to the idea of using vectors\nto represent intermodal/multimodal representations. For\nexample, an exciting development in recent years has been\nthe development of automatic image captioning systems.\nThese systems can take an image as input and generate a\nnatural language description of the image. The basic struc-\nture of these systems is very similar to the neural machine\ntranslation architecture shown in figure 5.5. The main\ndifference is that the encoder LSTM network is replaced\nby a CNN architecture that processes the input image and\ngenerates a vector representation that is then propagated\nto the decoder LSTM (Xu et al. 2015). This is another ex-\nample of the power of deep learning arising from its ability\nto learn complex representations of information. In this\ninstance, the system learns intermodal representations\nthat enable information to flow from what is in an im-\nage to language. Combining CNN and RNN architectures\nis becoming more and more popular because it offers the\npotential to integrate the advantages of both systems and\nenables deep learning architectures to handle very com-\nplex data.\nConvoLutIonAL And RECuRREnt nEuRAL nEtwoRks 183 Irrespective of the network architecture we use, we\nneed to find the correct weights for the network if we\nwish to create an accurate model. The weights of a neu-\nron determine the transformation the neuron applies to\nits inputs. So, it is the weights of the network that define\nthe fundamental building blocks of the representation the\nnetwork learns. Today the standard method for finding\nthese weights is an algorithm that came to prominence in\nthe 1980s: backpropagation. The next chapter will present\na comprehensive introduction to this algorithm.\n184 ChAPtER 5 6\nLEARNING FUNCTIONS\nA neural network model, no matter how deep or complex,\nimplements a function, a mapping from inputs to outputs.\nThe function implemented by a network is determined\nby the weights the network uses. So, training a network\n(learning the function the network should implement) on\ndata involves searching for the set of weights that best\nenable the network to model the patterns in the data. The\nmost commonly used algorithm for learning patterns\nfrom data is the gradient descent algorithm. The gradi-\nent descent algorithm is very like the perceptron learn-\ning rule and the LMS algorithm described in chapter 4:\nit defines a rule to update the weights used in a function\nbased on the error of the function. By itself the gradient\ndescent algorithm can be used to train a single output neu-\nron. However, it cannot be used to train a deep network\nwith multiple hidden layers. This limitation is because of the credit assignment problem: how should the blame for\nthe overall error of a network be shared out among the\ndifferent neurons (including the hidden neurons) in the\nnetwork? Consequently, training a deep neural network\ninvolves using both the gradient descent algorithm and\nthe backpropagation algorithm in tandem.\nThe process used to train a deep neural network can\nbe characterized as: randomly initializing the weight of a\nnetwork, and then iteratively updating the weights of the\nnetwork, in response to the errors the network makes on a\ndataset, until the network is working as expected. Within\nthis training framework, the backpropagation algorithm\nsolves the credit (or blame) assignment problem, and the\ngradient descent algorithm defines the learning rule that\nactually updates the weights in the network.\nThis chapter is the most mathematical chapter in the\nbook. However, at a high level, all you need to know about\nthe backpropagation algorithm and the gradient descent\nalgorithm is that they can be used to train deep networks.\nSo, if you don\u2019t have the time to work through the details\nin this chapter, feel free to skim through it. If, however,\nyou wish to get a deeper understanding of these two al-\ngorithms, then I encourage you to engage with the mate-\nrial. These algorithms are at the core of deep learning and\nunderstanding how they work is, possibly, the most direct\nway of understanding its potentials and limitations. I have\nattempted to present the material in this chapter in an\n186 ChAPtER 6 accessible way, so if you are looking for a relatively gentle\nbut still comprehensive introduction to these algorithms,\nthen I believe that this will provide it for you. The chapter\nbegins by explaining the gradient descent algorithm, and\nthen explains how gradient descent can be used in con-\njunction with the backpropagation algorithm to train a\nneural network.\nGradient Descent\nA very simple type of function is a linear mapping from a\nsingle input to a single output. Table 6.1 presents a data-\nset with a single input feature and a single output. Figure\n6.1 presents a scatterplot of this data along with a plot of\nthe line that best fits this data. This line can be used as a\nfunction to map from an input value to a prediction of the\nTable 6.1. A sample dataset with one input feature, x,\nand an output (target) feature, y\nX Y\n0.72 0.54\n0.45 0.56\n0.23 0.38\n0.76 0.57\n0.14 0.17\nLEARnIng FunC tIons 187 0.1\n8.0\ny = 0.6746\n6.0\ny\n4.0\nx = 0.9\n2.0\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nx\nFigure 6.1 Scatterplot of data with \u201cbest fit\u201d line and the errors of the line\non each example plotted as vertical dashed line segments. The figure also\nshows the mapping defined by the line for input x=0.9 to output y=0.6746.\noutput value. For example, if x = 0.9, then the response\nreturned by this linear function is y = 0.6746. The error (or\nloss) of using this line as a model for the data is shown by\nthe dashed lines from the line to each datum.\nIn chapter 2, we described how a linear function can\nbe represented using the equation of a line:\ny=mx+c\nwhere m is the slope of the line, and c is the y-i ntercept,\nwhich specifies where the line crosses the y-a xis. For the\n188 ChAPtER 6 line in figure 6.1, c=0.203 and m=0.524; this is why the\nfunction returns the value y=0.6746 when x=0.9, as in\nthe following:\n0.6746=(0.524\u00d70.9)+0.203\nThe slope m and the y- intercept c are the parameters of\nthis model, and these parameters can be varied to fit the\nmodel to the data.\nThe equation of a line has a close relationship with the\nweighted sum operation used in a neuron. This becomes\napparent if we rewrite the equation of a line with model\nparameters rewritten as weights (c\u2192w ,m\u2192w ):\n0 1\ny=(w \u00d71)+(w \u00d7x)\n0 1\nDifferent lines (different linear models for the data) can\nbe created by varying either of these weights (or model pa-\nrameters). Figure 6.2 illustrates how a line changes as the\nintercept and slope of the line varies: the dashed line illus-\ntrates what happens if the y-i ntercept is increased, and the\ndotted line shows what happens if the slope is decreased.\nChanging the y-i ntercept w vertically translates the line,\n0\nwhereas modifying the slope w rotates the line around\n1\nthe point (x=0,y=intercept).\nEach of these new lines defines a different func-\ntion, mapping from x to y, and each function will have\nLEARnIng FunC tIons 189 0.1\n8.0\n6.0\ny\n4.0\n2.0\n[w0= 0.203, w1= 0.524]\n[w0= 0.400, w1= 0.524]\n[w0= 0.203, w1= 0.300] 0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nx\nFigure 6.2 Plot illustrating how a line changes as the intercept (w ) and\n0\nslope (w ) are varied.\n1\na different error with respect to how well it matches the\ndata. Looking at figure 6.2, we can see that the full line,\n[w =0.203,w =0.524], fits the data better than the\n0 1\nother two lines because on average it passes closer to the\ndata points. In other words, on average the error for this\nline for each data point is less than those of the other two\nlines. The total error of a model on a dataset can be mea-\nsured by summing together the error the model makes on\neach example in the dataset. The standard way to calculate\nthis total error is to use an equation known as the sum of\nsquared errors (SSE):\n190 ChAPtER 6 1 n\nSSE= \u2211(y \u2212y\u02c6 )2\nj j\n2\nj=1\nThis equation tells us how to add together the errors of a\nmodel on a dataset containing n examples. This equation\ncalculates for each of the n examples in the dataset the er-\nror of the model by subtracting the prediction of the target\nvalue returned by the model from the correct target value\nfor that example, as specified in the dataset. In this equa-\ntion y is the correct output value for target feature listed\nj\nin the dataset for example j, and y\u02c6 is the estimate of the\nj\ntarget value returned by the model for the same example.\nEach of these errors is then squared and these squared er-\nrors are then summed. Squaring the errors ensures that\nthey are all positive, and therefore in the summation the\nerrors for examples where the function underestimated\nthe target do not cancel out the errors on examples where\nit overestimated the target. The multiplication of the sum-\nmation of the errors by 1/2, although not important for\nthe current discussion, will become useful later. The lower\nthe SSE of a function, the better the function models the\ndata. Consequently, the sum of squared errors can be used\nas a fitness function to evaluate how well a candidate\nfunction (in this situation a model instantiating a line)\nmatches the data.\nLEARnIng FunC tIons 191 Figure 6.3 shows how the error of a linear model var-\nies as the parameters of the model change. These plots\nshow the SSE of a linear model on the example single-\ninput\u2013 single- output dataset listed in table 6.1. For each\nparameter there is a single best setting and as the param-\neter moves away from this setting (in either direction)\nthe error of the model increases. A consequence of this\nis that the error profile of the model as each parameter\nvaries is convex (bowl- shaped). This convex shape is par-\nticularly apparent in the top and middle plots in figure 6.3,\nwhich show that the SSE of the model is minimized when\nw =0.203 (lowest point of the curve in the top plot),\n0\nand when w =0.524 (lowest point of the curve in the\n1\nmiddle plot).\nIf we plot the error of the model as both parame-\nters are varied, we generate a three-d imensional convex\nbowl- shaped surface, known as an error surface. The\nbowl- shaped mesh in the plot at the bottom of figure 6.3\nillustrates this error surface. This error surface was cre-\nated by first defining a weight space. This weight space is\nrepresented by the flat grid at the bottom of the plot. Each\ncoordinate in this weight space defines a different line be-\ncause each coordinate specifies an intercept (a w value)\n0\nand slope (a w value). Consequently, moving across this\n1\nplanar weight space is equivalent to moving between dif-\nferent models. The second step in constructing the error\nsurface is to associate an elevation with each line (i.e.,\n192 ChAPtER 6 0.1\n8.0\n6.0\nESS\n4.0\n2.0\n0.0\n0.2 0.0 0.2 0.4 0.6\nw0 (y intercept)\n0.1\n8.0\n6.0\nESS\n4.0\n2.0\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0\nw1 (slope)\nESS\nw\n0\nw\n1\nFigure 6.3 Plots of the changes in the error (SSE) of a linear model as the\nparameters of the model change. Top: the SSE profile of a linear model with a\nfixed slope w =0.524 when w ranges across the interval 0.3 to 1. Middle:\n1 0\nthe SSE profile of a linear model with a y-i ntercept fixed at w =0.203 when\n0\nw ranges across the interval 0 to 1. Bottom: the error surface of the linear\n1\nmodel when both w and w are varied.\n0 1 coordinate) in the weight space. The elevation associated\nwith each weight space coordinate is the SSE of the model\ndefined by that coordinate; or, put more directly, the\nheight of the error surface above the weight space plane\nis the SSE of the corresponding linear model when it is\nused as a model for the dataset. The weight space coordi-\nnates that correspond with the lowest point of the error\nsurface define the linear model that has the lowest SSE on\nthe dataset (i.e., the linear model that best fits the data).\nThe shape of the error surface in the plot on the right\nof figure 6.3 indicates that there is only a single best linear\nmodel for this dataset because there is a single point at\nthe bottom of the bowl that has a lower elevation (lower\nerror) than any other points on the surface. Moving away\nfrom this best model (by varying the weights of the model)\nnecessarily involves moving to a model with a higher SSE.\nSuch a move is equivalent to moving to a new coordinate\nin the weight space, which has a higher elevation associ-\nated with it on the error surface. A convex or bowl- shaped\nerror surface is incredibly useful for learning a linear func-\ntion to model a dataset because it means that the learning\nprocess can be framed as a search for the lowest point on\nthe error surface. The standard algorithm used to find this\nlowest point is known as gradient descent.\nThe gradient descent algorithm begins by creat-\ning an initial model using a randomly selected a set of\nweights. Next the SSE of this randomly initialized model\n194 ChAPtER 6 A convex or bowl-\nshaped error surface\nis incredibly useful\nfor learning a linear\nfunction to model a\ndataset because it\nmeans that the learning\nprocess can be framed\nas a search for the\nlowest point on the\nerror surface. is calculated. Taken together, the guessed set of weights\nand the SSE of the corresponding model define the ini-\ntial starting point on the error surface for the search. It\nis very likely that the randomly initialized model will be\na bad model, so it is very likely that the search will begin\nat a location that has a high elevation on the error surface.\nThis bad start, however, is not a problem, because once\nthe search process is positioned on the error surface, the\nprocess can find a better set of weights by simply following\nthe gradient of the error surface downhill until it reaches\nthe bottom of the error surface (the location where mov-\ning in any direction results in an increase in SSE). This is\nwhy the algorithm is known as gradient descent: the gradi-\nent that the algorithm descends is the gradient of the error\nsurface of the model with respect to the data.\nAn important point is that the search does not pro-\ngress from the starting location to the valley floor in one\nweight update. Instead, it moves toward the bottom of the\nerror surface in an iterative manner, and during each itera-\ntion the current set of weights are updated so as to move\nto a nearby location in the weight space that has a lower\nSSE. Reaching the bottom of the error surface can take\na large number of iterations. An intuitive way of under-\nstanding the process is to imagine a hiker who is caught\non the side of a hill when a thick fog descends. Their car\nis parked at the bottom of the valley; however, due to the\nfog they can only see a few feet in any direction. Assuming\n196 ChAPtER 6 that the valley has a nice convex shape to it, they can still\nfind their way to their car, despite the fog, by repeatedly\ntaking small steps that move down the hill following the\nlocal gradient at the position they are currently located.\nA single run of a gradient descent search is illustrated in\nthe bottom plot of figure 6.3. The black curve plotted on\nthe error surface illustrates the path the search followed\ndown the surface, and the black line on the weight space\nplots the corresponding weight updates that occurred dur-\ning the journey down the error surface. Technically, the\ngradient descent algorithm is known as an optimization\nalgorithm because the goal of the algorithm is to find the\noptimal set of weights.\nThe most important component of the gradient de-\nscent algorithm is the rule that defines how the weights are\nupdated during each iteration of the algorithm. In order to\nunderstand how this rule is defined it is first necessary to\nunderstand that the error surface is made up of multiple\nerror gradients. For our simple example, the error surface\nis created by combining two error curves. One error curve\nis defined by the changes in the SSE as w changes, shown\n0\nin the top plot of figure 6.3. The other error curve is de-\nfined by the changes in the SSE as w changes, shown in\n1\nthe plot in the middle of figure 6.3. Notice that the gradi-\nent of each of these curves can vary along the curve, for\nexample, the w error curve has a steep gradient on the ex-\n0\ntreme left and right of the plot, but the gradient becomes\nLEARnIng FunC tIons 197 somewhat shallower in the middle of the curve. Also, the\ngradients of two different curves can vary dramatically; in\nthis particular example the w error curve generally has a\n0\nmuch steeper gradient than the w error curve.\n1\nThe fact that the error surface is composed of mul-\ntiple curves, each with a different gradient, is important\nbecause the gradient descent algorithm moves down the\ncombined error surface by independently updating each\nweight so as to move down the error curve associated\nwith that weight. In other words, during a single itera-\ntion of the gradient descent algorithm, w is updated to\n0\nmove down the w error curve and w is updated the move\n0 1\ndown the w error curve. Furthermore, the amount each\n1\nweight is updated in an iteration is proportional to the\nsteepness of the gradient of the weight\u2019s error curve, and\nthis gradient will vary from one iteration to the next as\nthe process moves down the error curve. For example, w\n0\nwill be updated by relatively large amounts in iterations\nwhere the search process is located high up on either side\nof the w error curve, but by smaller amounts in iterations\n0\nwhere the search process is nearer to the bottom of the w\n0\nerror curve.\nThe error curve associated with each weight is defined\nby how the SSE changes with respect to the change in the\nvalue of the weight. Calculus, and in particular differentia-\ntion, is the field of mathematics that deals with rates of\nchange. For example, taking the derivative of a function,\n198 ChAPtER 6 y=f(x), calculates the rate of change of y (the output)\nfor each unit change in x (the input). Furthermore, if a\nfunction takes multiple inputs [y=f(x ,\u2026,x )] then it\n1 n\nis possible to calculate the rate of change of the output,\ny, with respect to changes in each of these inputs, x , by\ni\ntaking the partial derivative of the function of with re-\nspect to each input. The partial derivative of a function\nwith respect to a particular input is calculated by first as-\nsuming that all the other inputs are held constant (and so\ntheir rate of change is 0 and they disappear from the cal-\nculation) and then taking the derivative of what remains.\nFinally, the rate of change of a function for a given input is\nalso known as the gradient of the function at the location\non the curve (defined by the function) that is specified by\nthe input. Consequently, the partial derivative of the SSE\nwith respect to a weight specifies how the output of the\nSSE changes as that weight changes, and so it specifies\nthe gradient of the error curve of the weight. This is ex-\nactly what is needed to define the gradient descent weight\nupdate rule: the partial derivative of the SSE with respect\nto a weight specifies how to calculate the gradient of the\nweight\u2019s error curve, and in turn this gradient specifies\nhow the weight should be updated to reduce the error (the\noutput of the SSE).\nThe partial derivative of a function with respect to a\nparticular variable is the derivative of the function when\nLEARnIng FunC tIons 199 all the other variables are held constant. As a result there\nis a different partial derivative of a function with respect\nto each variable, because a different set of terms are con-\nsidered constant in the calculation of each of the partial\nderivatives. Therefore, there is a different partial deriva-\ntive of the SSE for each weight, although they all have a\nsimilar form. This is why each of the weights is updated in-\ndependently in the gradient descent algorithm: the weight\nupdate rule is dependent on the partial derivative of the\nSSE for each weight, and because there is a different par-\ntial derivative for each weight, there is a separate weight\nupdate rule for each weight. Again, although the partial\nderivative for each weight is distinct, all of these deriva-\ntives have the same form, and so the weight update rule\nfor each weight will also have the same form. This simpli-\nfies the definition of the gradient descent algorithm. An-\nother simplifying factor is that the SSE is defined relative\nto a dataset with n examples. The relevance of this is that\nthe only variables in the SSE are the weights; the target\noutput y and the inputs x are all specified by the dataset\nfor each example, and so can be considered constants. As\na result, when calculating the partial derivative of the SSE\nwith respect to a weight, many of the terms in the equa-\ntion that do not include the weight can be deleted because\nthey are considered constants.\nThe relationship between the output of the SSE and\neach weight becomes more explicit if the SSE definition\n200 ChAPtER 6 is rewritten so that the term y\u02c6 , denoting the output pre-\nj\ndicted by the model, is replaced by the structure of the\nmodel generating the prediction. For the model with a\nsingle input x and a dummy input, x =1,this rewritten\n1 0\nversion of the SSE is:\n1 n\nSSE= \u2211(y \u2212(w \u00d7x +w \u00d7x ))2\nj 0 j,0 1 j,1\n2\nj=1\nThis equation uses a double subscript on the inputs, the\nfirst subscript j identifies the example (or row in the\ndataset) and the second subscript specifies the feature (or\ncolumn in the dataset) of the input. For example, x rep-\nj,1\nresents feature 1 from example j. This definition of the\nSSE can be generalized to a model with m inputs:\n1 n \uf8eb \uf8eb m \uf8f6\uf8f62\nSSE= 2\u2211 \uf8ed\uf8ecy j\u2212 \uf8ed\uf8ec\u2211w i\u00d7x j,i\uf8f8\uf8f7 \uf8f8\uf8f7\nj=1 i=0\nCalculating the partial derivative of the SSE with re-\nspect to a specific weight involves the application of the\nchain rule from calculus and a number of standard dif-\nferentiation rules. The result of this derivation is the fol-\nlowing equation (for simplicity of presentation we switch\nback to the notation y\u02c6 to represent the output from the\ni\nmodel):\nLEARnIng FunC tIons 201 \uf8eb \uf8f6\n\uf8ec \uf8f7\n\uf8ec \uf8f7\n\u2202SSE n \uf8ec \uf8f7\n=\u2211\uf8ec(y \u2212y\u02c6 )\u00d7 \u2212x \uf8f7\nj=1\uf8ec(cid:31)(cid:29)j(cid:30)(cid:29)(cid:28)j (cid:27)j,ii\n\u2202 w i \uf8f7\nerrorofthe rateofchangeof\n\uf8ec\uf8ec \uf8f7\noutputofthe weightedsum\n\uf8ec \uf8f7\n\uf8ecweightedsum withrespectto \uf8f7\n\uf8ed changeinwi \uf8f8\nThis partial derivative specifies how to calculate the\nerror gradient for weight w for the dataset where x is the\ni j,i\ninput associated with w for each example in the dataset.\ni\nThis calculation involves multiplying two terms, the error\nof the output and the rate of change of the output (i.e., the\nweighted sum) with respect to changes in the weight. One\nway of understanding this calculation is that if changing\nthe weight changes the output of the weighted sum by a\nlarge amount, then the gradient of the error with respect\nto the weight is large (steep) because changing the weight\nwill result in big changes in the error. However, this gradi-\nent is the uphill gradient, and we wish to move the weights\nso as to move down the error curve. So in the gradient\ndescent weight update rule (shown below) the \u201c\u2013 \u201d sign in\nfront of the input x is dropped. Using t to represent the\nj,i\niteration of the algorithm (an iteration involves a single\npass through the n examples in the dataset), the gradient\ndescent weight update rule is defined as:\n202 ChAPtER 6 \uf8eb \uf8f6\n\uf8ec n \uf8f7\nwt+1 =wt +\uf8ec\u03b7\u00d7\u2211((yt \u2212y\u02c6t)\u00d7xt )\uf8f7\ni i j j j,i\n\uf8ec \uf8f7\n(cid:31)j=1(cid:29)(cid:29)(cid:29)(cid:29)(cid:30)(cid:29)(cid:29)(cid:29)(cid:28)\uf8f7\n\uf8ec\n\uf8ed error gradient for wi \uf8f8\nThere are a number of notable factors about this\nweight update rule. First, the rule specifies how the weight\nw should be updated after iteration t through the dataset.\ni\nThis update is proportional to the gradient of the error\ncurve for the weight for that iteration (i.e., the summa-\ntion term, which actually defines the partial derivative\nof the SSE for that weight). Second, the weight update\nrule can be used to update the weights for functions with\nmultiple inputs. This means that the gradient descent al-\ngorithm can be used to descend error surfaces with more\nthan two weight coordinates. It is not possible to visual-\nize these error surfaces because they will have more than\nthree dimensions, but the basic principles of descending\nan error surface using the error gradient generalizes to\nlearning functions with multiple inputs. Third, although\nthe weight update rule has a similar structure for each\nweight, the rule does define a different update for each\nweight during each iteration because the update is de-\npendent on the inputs in the dataset examples to which\nthe weight is applied. Fourth, the summation in the rule\nLEARnIng FunC tIons 203 indicates that, in each iteration of the gradient descent al-\ngorithm, the current model should be applied to all n of\nthe examples in the dataset. This is one of the reasons why\ntraining a deep learning network is such a computation-\nally expensive task. Typically for very large datasets, the\ndataset is split up into batches of examples sampled from\nthe dataset, and each iteration of training is based on a\nbatch, rather than the entire dataset. Fifth, apart from the\nmodifications necessary to include the summation, this\nrule is identical to the LMS (also known as the Widrow-\nHoff or delta) learning rule introduced in chapter 4, and\nthe rule implements the same logic: if the output of the\nmodel is too large, then weights associated with positive\ninputs should be reduced; if the output is too small, then\nthese weights should be increased. Moreover, the purpose\nand function of the learning rate hyperparameter (\u03b7) is\nthe same as in the LMS rule: scale the weight adjustments\nto ensure that the adjustments aren\u2019t so large that the\nalgorithm misses (or steps over) the best set of weights.\nUsing this weight update rule, the gradient descent algo-\nrithm can be summarized as follows:\n1. Construct a model using an initial set of weights.\n2. Repeat until the model performance is good enough.\na. Apply the current model to the examples in the\ndataset.\n204 ChAPtER 6 b. Adjust each weight using the weight update\nrule.\n3. Return the final model.\nOne consequence of the independent updating of\nweights, and the fact that weight updates are proportional\nto the local gradient on the associated error curve, is that\nthe path the gradient descent algorithm follows to the\nlowest point on the error surface may not be a straight\nline. This is because the gradient of each of the component\nerror curves may not be equal at each location on the error\nsurface (the gradient for one of the weights may be steeper\nthan the gradient for the other weight). As a result, one\nweight may be updated by a larger amount than another\nweight during a given iteration, and thus the descent to\nthe valley floor may not follow a direct route. Figure 6.4\nillustrates this phenomenon. Figure 6.4 presents a set of\ntop- down views of a portion of a contour plot of an error\nsurface. This error surface is a valley that is quite long and\nnarrow with steeper sides and gentler sloping ends; the\nsteepness is reflected by the closeness of the contours. As\na result, the search initially moves across the valley before\nturning toward the center of the valley. The plot on the\nleft illustrates the first iteration of the gradient descent\nalgorithm. The initial starting point is the location where\nthe three arrows, in this plot, meet. The lengths of the\nLEARnIng FunC tIons 205 Figure 6.4 Top-d own views of a portion of a contour plot of an error surface,\nillustrating the gradient descent path across the error surface. Each of the\nthick arrows illustrates the overall movement of the weight vector for a\nsingle iteration of the gradient descent algorithm. The length of dotted and\ndashed arrows represent the local gradient of the w and w error curves,\n0 1\nrespectively, for that iteration. The plot on the right shows the overall path\ntaken to the global minimum of the error surface.\ndotted and dashed arrows represent the local gradients\nof the w and w error curves, respectively. The dashed\n0 1\narrow is longer than the dotted arrow reflecting the fact\nthat the local gradient of the w error curve is steeper than\n0\nthat of the w error curve. In each iteration, each of the\n1\nweights is updated in proportion to the gradient of their\nerror curve; so in the first iteration, the update for w is\n0\nlarger than for w and therefore the overall movement is\n1\ngreater across the valley than along the valley. The thick\nblack arrow illustrates the overall movement in the un-\nderlying weight space, resulting from the weight updates\nin this first iteration. Similarly, the middle plot illustrates\nthe error gradients and overall weight update for the next\niteration of gradient descent. The plot on the right shows\nthe complete path of descent taken by the search process\nfrom initial location to the global minimum (the lowest\npoint on the error surface).\n206 ChAPtER 6 It is relatively straightforward to map the weight up-\ndate rule over to training a single neuron. In this mapping,\nthe weight w is the bias term for a neuron, and the other\n0\nweights are associated with the other inputs to the neu-\nron. The derivation of the partial derivative of the SSE\nis dependent on the structure of the function that gen-\nerates y\u02c6. The more complex this function is, the more\ncomplex the partial derivative becomes. The fact that the\nfunction a neuron defines includes both a weighted sum-\nmation and an activation function means that the partial\nderivative of the SSE with respect to a weight in a neuron\nis more complex than the partial derivative given above.\nThe inclusion of the activation function within the neuron\nresults in an extra term in the partial derivative of the SSE.\nThis extra term is the derivative of the activation function\nwith respect to the output from the weighted summation\nfunction. The derivative of the activation function is with\nrespect to the output of the weighted summation function\nbecause this is the input that the activation function re-\nceives. The activation function does not receive the weight\ndirectly. Instead, the changes in the weight only affect\nthe output of the activation function indirectly through\nthe effect that these weight changes have on the output\nof the weighted summation. The main reason why the\nlogistic function was such a popular activation func-\ntion in neural networks for so long was that it has a very\nstraightforward derivative with respect to its inputs. The\nLEARnIng FunC tIons 207 gradient descent weight update rule for a neuron using the\nlogistic function is as follows:\n\uf8eb \uf8f6\n\uf8ec \uf8f7\n\uf8eb \uf8f6\n\uf8ec \uf8ec \uf8f7\uf8f7\n\uf8ec \uf8ec \uf8f7\uf8f7\n\uf8ec \uf8ec \uf8f7\uf8f7\nn\nwt+1 =wt +\uf8ec \u03b7\u00d7\u2211\uf8ec(yt \u2212y\u02c6t)\u00d7(y\u02c6t\u00d7(1\u2212y\u02c6t))\u00d7xtt \uf8f7\uf8f7\ni i \uf8ec \uf8ec j j (cid:31)j(cid:29)(cid:29)(cid:30)(cid:29)(cid:29)j(cid:28) j,i \uf8f7\uf8f7\nj=1\n\uf8ec \uf8ec derivativeofthe \uf8f7\uf8f7\n\uf8ec \uf8ec logissticfunction \uf8f7\uf8f7\n\uf8ec \uf8ec withrespecttothe \uf8f7\uf8f7\n(cid:31)(cid:29)\uf8ed (cid:29)(cid:29)(cid:29)(cid:29)(cid:29)w(cid:30)eigh(cid:29)ted(cid:29)(cid:29)sum(cid:29)mat(cid:29)ion(cid:29)(cid:29)(cid:28)\uf8f8\n\uf8ec \uf8f7\n\uf8ed error gradient for wi \uf8f8\nThe fact that the weight update rule includes the derivative\nof the activation function means that the weight update\nrule will change if the activation function of the neuron is\nchanged. However, this change will simply involve updat-\ning the derivative of the activation function; the overall\nstructure of the rule will remain the same.\nThis extended weight update rule means that the gra-\ndient descent algorithm can be used to train a single neu-\nron. It cannot, however, be used to train neural networks\nwith multiple layers of neurons because the definition of\nthe error gradient for a weight depends on the error of\nthe output of the function, the term y \u2212y\u02c6 . Although it\nj j\nis possible to calculate the error of the output of a neuron\nin the output layer of the network by directly comparing\n208 ChAPtER 6 the output with the expected output, it is not possible to\ncalculate this error term directly for the neurons in the\nhidden layer of the network, and as a result it is not pos-\nsible to calculate the error gradients for each weight. The\nbackpropagation algorithm is a solution to the problem of\ncalculating error gradients for the weights in the hidden\nlayers of the network.\nTraining a Neural Network Using Backpropagation\nThe term backpropagation has two different meanings.\nThe primary meaning is that it is an algorithm that can be\nused to calculate, for each neuron in a network, the sen-\nsitivity (gradient/rate-o f- change) of the error of the net-\nwork to changes in the weights. Once the error gradient\nfor a weight has been calculated, the weight can then be\nadjusted to reduce the overall error of the network using a\nweight update rule similar to the gradient descent weight\nupdate rule. In this sense, the backpropagation algorithm\nis a solution to the credit assignment problem, introduced\nin chapter 4. The second meaning of backpropagation is\nthat it is a complete algorithm for training a neural net-\nwork. This second meaning encompasses the first sense,\nbut also includes a learning rule that defines how the er-\nror gradients of the weights should be used to update the\nweights within the network. Consequently, the algorithm\nLEARnIng FunC tIons 209 described by this second meaning involves a two- step pro-\ncess: solve the credit assignment problem, and then use\nthe error gradients of the weights, calculated during credit\nassignment, to update the weights in the network. It is\nuseful to distinguish between these two meanings of back-\npropagation because there are a number of different learn-\ning rules that can be used to update the weights, once the\ncredit assignment problem has been resolved. The learn-\ning rule that is most commonly used with backpropaga-\ntion is the gradient descent algorithm introduced earlier.\nThe description of the backpropagation algorithm given\nhere focuses on the first meaning of backpropagation, that\nof the algorithm being a solution to the credit assignment\nproblem.\nBackpropagation: The Two- Stage Algorithm\nThe backpropagation algorithm begins by initializing all\nthe weights of the network using random values. Note\nthat even a randomly initialized network can still generate\nan output when an input is presented to the network, al-\nthough it is likely to be an output with a large error. Once\nthe network weights have been initialized, the network\ncan be trained by iteratively updating the weights so as\nto reduce the error of the network, where the error of the\nnetwork is calculated in terms of the difference between\nthe output generated by the network in response to an\ninput pattern, and the expected output for that input, as\n210 ChAPtER 6 defined in the training dataset. A crucial step in this itera-\ntive weight adjustment process involves solving the credit\nassignment problem, or, in other words, calculating the\nerror gradients for each weight in the network. The back-\npropagation algorithm solves this problem using a two-\nstage process. In first stage, known as the forward pass,\nan input pattern is presented to the network, and the re-\nsulting neuron activations flow forward through the net-\nwork until an output is generated. Figure 6.5 illustrates\nthe forward pass of the backpropagation algorithm. In\nthis figure, the weighted summation of inputs calculated\nat each neuron (e.g., z represents the weighted summa-\n1\ntion of inputs calculated for neuron 1) and the outputs (or\nactivations, e.g., a represents the activation for neuron 1)\n1\nof each neuron is shown. The reason for listing the z and\ni\na values for each neuron in this figure is to highlight the\ni\nfact that during the forward pass both of these values, for\neach neuron, are stored in memory. The reason they are\nstored in memory is that they are used in the backward\npass of the algorithm. The z value for a neuron is used to\ni\ncalculate the update to the weights on input connections\nto the neuron. The a value for a neuron is used to calculate\ni\nthe update to the weights on the output connections from\na neuron. The specifics of how these values are used in the\nbackward pass will be described below.\nThe second stage, known as the backward pass, be-\ngins by calculating an error gradient for each neuron in\nLEARnIng FunC tIons 211 the output layer. These error gradients represent the sen-\nsitivity of the network error to changes in the weighted\nsummation calculation of the neuron, and they are often\ndenoted by the shorthand notation \u03b4 (pronounced delta)\nwith a subscript indicating the neuron. For example, \u03b4 is\nk\nthe gradient of the network error with respect to small\nchanges in the weighted summation calculation of the\nneuron k. It is important to recognize that there are two\ndifferent error gradients calculated in the backpropaga-\ntion algorithm:\n1. The first is the \u03b4 value for each neuron. The \u03b4 for each\nneuron is the rate of change of the error of the network\nwith respect to changes in the weighted summation\ncalculation of the neuron. There is one \u03b4 for each\nneuron. It is these \u03b4 error gradients that the algorithm\nbackpropagates.\n2. The second is the error gradient of the network with\nrespect to changes in the weights of the network. There\nis one of these error gradients for each weight in the\nnetwork. These are the error gradients that are used\nto update the weights in the network. However, it is\nnecessary to first calculate the \u03b4 term for each neuron\n(using backpropagation) in order to calculate the error\ngradients for the weights.\n212 ChAPtER 6 Note there is only a single \u03b4 per neuron, but there may be\nmany weights associated with that neuron, so the \u03b4 term\nfor a neuron may be used in the calculation of multiple\nweight error gradients.\nOnce the \u03b4s for the output neurons have been calcu-\nlated, the \u03b4s for the neurons in the last hidden layer are\nthen calculated. This is done by assigning a portion of the\n\u03b4 from each output neuron to each hidden neuron that is\ndirectly connected to it. This assignment of blame, from\noutput neuron to hidden neuron, is dependent on the\nweight of the connection between the neurons, and the\nactivation of the hidden neuron during the forward pass\n(this is why the activations are recorded in memory dur-\ning the forward pass). Once the blame assignment, from\nthe output layer, has been completed, the \u03b4 for each neu-\nron in the last hidden layer is calculated by summing the\nportions of the \u03b4s assigned to the neuron from all of the\noutput neurons it connects to. The same process of blame\nassignment and summing is then repeated to propagate\nthe error gradient back from the last layer of hidden neu-\nrons to the neurons in the second last layer, and so on,\nback to the input layer. It is this backward propagation of\n\u03b4s through the network that gives the algorithm its name.\nAt the end of this backward pass there is a \u03b4 calculated\nfor each neuron in the network (i.e., the credit assignment\nproblem has been solved) and these \u03b4s can then be used\nto update the weights in the network (using, for example,\nLEARnIng FunC tIons 213 z1 1 a1\nz5 5 a5 z8 8 a8\nz2 2 a2 z11 11 a11\nz6 6 a6 z9 9 a9\nz3 3 a3 z12 12 a12\nz7 7 a7 z10 10 a10\nz4 4 a4\nForward pass: activations flow from inputs to outputs\nFigure 6.5 The forward pass of the backpropagation algorithm.\nthe gradient descent algorithm introduced earlier). Figure\n6.6 illustrates the backward pass of the backpropagation\nalgorithm. In this figure, the \u03b4s get smaller and smaller as\nthe backpropagation process gets further from the output\nlayer. This reflects the vanishing gradient problem dis-\ncussed in chapter 4 that slows down the learning rate of\nthe early layers of the network.\nIn summary, the main steps within each iteration of\nthe backpropagation algorithm are as follows:\n1. Present an input to the network and allow the neuron\nactivations to flow forward through the network until an\noutput is generated. Record both the weighted sum and\nthe activation of each neuron.\n214 ChAPtER 6 \u03b41 1\n\u03b45 5 \u03b4 8 8\n\u03b4\n\u03b42 2 11 11\n\u03b46 6 \u03b4 9 9\n\u03b4\n\u03b43 3 12 12\n\u03b47 7 \u03b4 10 10\n4\n\u03b44\nBackward pass: error gradients (\u03b4s) flow from outputs to inputs\nFigure 6.6 The backward pass of the backpropagation algorithm.\n2. Calculate a \u03b4 (delta) error gradient for each neuron in\nthe output layer.\n3. Backpropagate the \u03b4 error gradients to obtain a \u03b4\n(delta) error gradient for each neuron in the network.\n4. Use the \u03b4 error gradients and a weight update\nalgorithm, such as gradient descent, to calculate the error\ngradients for the weights and use these to update the\nweights in the network.\nThe algorithm continues iterating through these steps\nuntil the error of the network is reduced (or converged) to\nan acceptable level.\nLEARnIng FunC tIons 215 Backpropagation: Backpropagating the \u03b4 s\nA \u03b4 term of a neuron describes the error gradient for the\nnetwork with respect to changes in the weighted summa-\ntion of inputs calculated by the neuron. To help make this\nmore concrete, figure 6.7 (top) breaks open the processing\nstages within a neuron k and uses the term z to denote\nk\nthe result of the weighted summation within the neuron.\nThe neuron in this figure receives inputs (or activations)\nfrom three other neurons (h,i,j), and z is the weighted\nk\nsum of these activations. The output of the neuron, a , is\nk\nthen calculated by passing z through a nonlinear activa-\nk\ntion function, \u03d5, such as the logistic function. Using this\nnotation a \u03b4 for a neuron k is the rate of change of the\nerror of the network with respect to small changes in the\nvalue of z . Mathematically, this term is the partial deriva-\nk\ntive of the networks error with respect to z :\nk\n\u2202Error\n\u03b4 =\nk\n\u2202z\nk\nNo matter where in a network a neuron is located\n(output layer or hidden layer), the \u03b4 for the neuron is cal-\nculated as the product of two terms:\n1. the rate of change of the network error in response to\nchanges in the neuron\u2019s activation (output): \u2202E/\u2202a ;\nk\n216 ChAPtER 6 Figure 6.7 Top: the forward propagation of activations through the\nweighted sum and activation function of a neuron. Middle: The calculation of\nthe \u03b4 term for an output neuron (t is the expected activation for the neuron\nk\nand a is the actual activation). Bottom: The calculation of the \u03b4 term for a\nk\nhidden neuron. This figure is loosely inspired by figure 5.2 and figure 5.3 in\nReed and Marks II 1999.\nLEARnIng FunC tIons 217 2. the rate of change of the activation of the neuron with\nrespect to changes in the weighted sum of inputs to the\nneuron: \u2202a /\u2202z .\nk k\n\u2202E \u2202a\n\u03b4 = \u00d7 k\nk\n\u2202a \u2202z\nk k\nFigure 6.7 (middle) illustrates how this product is cal-\nculated for neurons in the output layer of a network. The\nfirst step is to calculate the rate of change of the error of\nthe network with respect to the output of the neuron, the\nterm \u2202E/\u2202a . Intuitively, the larger the difference between\nk\nthe activation of a neuron, a , and the expected activation,\nk\nt , the faster the error can be changed by changing the\nk\nactivation of the neuron. So the rate of change of the error\nof the network with respect to changes in the activation of\nan output neuron k can be calculated by subtracting the\nneuron\u2019s activation (a ) from the expected activation (t ):\nk k\n\u2202E\n=t \u2212a\nk k\n\u2202a\nk\nThis term connects the error of the network to the out-\nput of the neuron. The neuron\u2019s \u03b4, however, is the rate\nof change of the error with respect to the input to the\nactivation function (z ), not the output of that function\nk\n(a ). Consequently, in order to calculate the \u03b4 for the\nk\n218 ChAPtER 6 neuron, the \u2202E/\u2202a value must be propagated back\nk\nthrough the activation function to connect it to the in-\nput to the activation function. This is done by multiplying\n\u2202E/\u2202a by the rate of change of the activation function\nk\nwith respect to the input value to the function, z . In fig-\nk\nure 6.7, the rate of change of the activation function with\nrespect to its input is denoted by the term: \u2202a /\u2202z . This\nk k\nterm is calculated by plugging the value z (stored from\nk\nthe forward pass through the network) into the equation\nof the derivative of the activation function with respect\nto z . For example, the derivative of the logistic function\nk\nwith respect to its input is:\n\u2202logistic(z)\n=logistic(z)\u00d7(1\u2212logistic(z))\n\u2202z\nFigure 6.81 plots this function and shows that plugging a\nz value into this equation will result in a value between\nk\n0 and 0.25. For example, figure 6.8 shows that if z =0\nk\nthen \u2202a /\u2202z =0.25. This is why the weighted summa-\nk k\ntion value for each neuron (z ) is stored during the for-\nk\nward pass of the algorithm.\nThe fact that the calculation of a neuron\u2019s \u03b4 involves\na product that includes the derivative of the neuron\u2019s ac-\ntivation function makes it necessary to be able to take the\nderivative of the neuron\u2019s activation function. It is not\npossible to take the derivative of a threshold activation\nLEARnIng FunC tIons 219 0.1\nLogistic(z)\n\u2202 logistic(z)\n\u2202 z 8.0\n)z(noitavitcA 6.0\n4.0\nmax = 0.25\n2.0\nsaturated = 0 saturated = 0 0.0\n\u221210 \u22125 0 5 10\nz\nFigure 6.8 Plots of the logistic function and the derivative of the logistic\nfunction.\nfunction because there is a discontinuity in the function\nat the threshold. As a result, the backpropagation algo-\nrithm does not work for networks composed of neurons\nthat use threshold activation functions. This is one of the\nreasons why neural networks moved away from threshold\nactivation and started to use the logistic and tanh activa-\ntion functions. The logistic and tanh functions both have\nvery simple derivatives and this made them particularly\nsuitable to backpropagation.\nFigure 6.7 (bottom) illustrates how the \u03b4 for a neu-\nron in a hidden layer is calculated. This involves the same\n220 ChAPtER 6 product of terms as was used for neurons in the output\nlayer. The difference is that the calculation of the \u2202E/\u2202a\nk\nis more complex for hidden units. For hidden neurons, it is\nnot possible to directly connect the output of the neuron\nwith the error of a network. The output of a hidden neu-\nron only indirectly affects the overall error of the network\nthrough the variations that it causes in the downstream\nneurons that receive the output as input, and the magni-\ntude of these variations is dependent on the weight each\nof these downstream neurons applies to the output. Fur-\nthermore, this indirect effect on the network error is in\nturn dependent on the sensitivity of the network error to\nthese later neurons, that is, their \u03b4 values. Consequently,\nthe sensitivity of the network error to the output of a hid-\nden neuron can be calculated as a weighted sum of the \u03b4\nvalues of the neurons immediately downstream of the\nneuron:\n\u2202E N\n=\u2211w \u00d7\u03b4\nk,i i\n\u2202a\nk i=1\nAs a result, the error terms (the \u03b4 values) for all the down-\nstream neurons to which a neuron\u2019s output is passed in\nthe forward pass must be calculated before the \u2202E/\u2202a for\nk\nneuron k can be calculated. This, however, is not a prob-\nlem because in the backward pass the algorithm is working\nbackward through the network and will have calculated\nLEARnIng FunC tIons 221 the \u03b4 terms for the downstream neurons before it reaches\nneuron k.\nFor hidden neurons, the other term in the \u03b4 prod-\nuct, \u2202a /\u2202z , is calculated in the same way as it is calcu-\nk k\nlated for output neurons: the z value for the neuron (the\nk\nweighted summation of inputs, stored during the forward\npass through the network) is plugged into the derivative\nof the neuron\u2019s activation function with respect to z .\nk\nBackpropagation: Updating the Weights\nThe fundamental principle of the backpropagation algo-\nrithm in adjusting the weights in a network is that each\nweight in a network should be updated in proportion to the\nsensitivity of the overall error of the network to changes\nin that weight. The intuition is that if the overall error of\nthe network is not affected by a change in a weight, then\nthe error of the network is independent of that weight,\nand, therefore, the weight did not contribute to the error.\nThe sensitivity of the network error to a change in an in-\ndividual weight is measured in terms of the rate of change\nof the network error in response to changes in that weight.\nThe overall error of a network is a function with mul-\ntiple inputs: both the inputs to the network and all the\nweights in the network. So, the rate of change of the er-\nror of a network in response to changes in a given net-\nwork weight is calculated by taking the partial derivative\nof the network error with respect to that weight. In the\n222 ChAPtER 6 The fundamental\nprinciple of the\nbackpropagation\nalgorithm in adjusting\nthe weights in a network\nis that each weight in\na network should be\nupdated in proportion\nto the sensitivity of the\noverall error of the\nnetwork to changes\nin that weight. backpropagation algorithm, the partial derivative of the\nnetwork error for a given weight is calculated using the\nchain rule. Using the chain rule, the partial derivative of\nthe network error with respect a weight w on the con-\nj,k\nnection between a neuron j and a neuron k is calculated\nas the product of two terms:\n1. the first term describes the rate of change of the\nweighted sum of inputs in neuron k with respect to\nchanges in the weight \u2202z /\u2202w ;\nk j,k\n2. and the second term describes the rate of change of\nthe network error in response to changes in the weighted\nsum of inputs calculated by the neuron k. (This second\nterm is the \u03b4 for neuron k.)\nk\nFigure 6.9 shows how the product of these two terms\nconnects a weight to the output error of the network.\nThe figure shows the processing of the last two neurons\n(k and l) in a network with a single path of activation.\nNeuron k receives a single input a and the output from\nj\nneuron k is the sole input to neuron l. The output of neu-\nron l is the output of the network. There are two weights\nin this portion of the network, w and w .\nj,k k,l\nThe calculations shown in figure 6.9 appear compli-\ncated because they contain a number of different compo-\nnents. However, as we will see, by stepping through these\ncalculations, each of the individual elements is actually\n224 ChAPtER 6 Neuron k Neuron l\na j w j,k (cid:31) a j \u00d7w j,k z k \u03d5(z k) w k,l (cid:31) a k \u00d7w k,l z l \u03d5(z l) Output\n\u2202zl \u03b4\n\u2202wk,l \u00d7 l\n\u2202Error\n\u2202wk,l\n\u2202zk \u03b4\n\u2202wj,k \u00d7 k\n\u2202Error\n\u2202wj,k\nFigure 6.9 An illustration of how the product of derivatives connects\nweights in the network to the error of the network.\neasy to calculate; it\u2019s just keeping track of all the different\nelements that poses a difficulty.\nFocusing on w , this weight is applied to an input of\nk,l\nthe output neuron of the network. There are two stages\nof processing between this weight and the network out-\nput (and error): the first is the weighted sum calculated\nin neuron l; the second is the nonlinear function applied\nto this weighted sum by the activation function of neuron\nl. Working backward from the output, the \u03b4 term is calcu-\nl\nlated using the calculation shown in the middle figure of\nfigure 6.7: the difference between the target activation for\nthe neuron and the actual activation is calculated and is\nmultiplied by the partial derivative of the neuron\u2019s activa-\ntion function with respect to its input (the weighted sum\nz ), \u2202a /\u2202z . Assuming that the activation function used\nk l l\nLEARnIng FunC tIons 225 by neuron l is the logistic function, the term \u2202a /\u2202z is\nl l\ncalculated by plugging in the value z (stored during the\nl\nforward pass of the algorithm) into the derivation of the\nlogistic function:\n\u2202a \u2202logistic(z )\nl = l =logistic(z )\u00d7(1\u2212logistic(z ))\nl l\n\u2202z \u2202z\nl l\nSo the calculation of \u03b4 under the assumption that neuron\nl\nl uses a logistic function is:\n\u03b4 =logistic(z )\u00d7(1\u2212logistic(z ))\u00d7(t \u2212a )\nl l l l l\nThe \u03b4 term connects the error of the network to the\nl\ninput to the activation function (the weighted sum z ).\nl\nHowever, we wish to connect the error of the network back\nto the weight w . This is done by multiplying the \u03b4 term\nk,l l\nby the partial derivative of the weighted summation func-\ntion with respect to weight w : \u2202z /\u2202w . This partial\nk,l l k,l\nderivative describes how the output of the weighted sum\nfunction z changes as the weight w changes. The fact\nl k,l\nthat the weighted summation function is a linear function\nof weights and activations means that in the partial de-\nrivative with respect to a particular weight all the terms in\nthe function that do not involve the specific weight go to\nzero (are considered constants) and the partial derivative\n226 ChAPtER 6 simplifies to just the input associated with that weight, in\nthis instance input a .\nk\n\u2202z\nl =a\nk\n\u2202w\nk,l\nThis is why the activations for each neuron in the network\nare stored in the forward pass. Taken together these two\nterms, \u2202z /\u2202w and \u03b4 , connect the weight w to the\nl k,l l k,l\nnetwork error by first connecting the weight to z , and\nl\nthen connecting z to the activation of the neuron, and\nl\nthereby to the network error. So, the error gradient of\nthe network with respect to changes in weight w is\nk,l\ncalculated as:\n\u2202Error \u2202z\n= l \u00d7\u03b4 =a \u00d7\u03b4\nl k l\n\u2202w \u2202w\nk,l k,l\nThe other weight in the figure 6.9 network, w , is\nk,l\ndeeper in the network, and, consequently, there are more\nprocessing steps between it and the network output (and\nerror). The \u03b4 term for neuron k is calculated, through\nbackpropagation (as shown at the bottom of figure 6.7),\nusing the following product of terms:\n\u2202a\n\u03b4 = k \u00d7(w \u00d7\u03b4 )\nk k,l l\n\u2202z\nk\nLEARnIng FunC tIons 227 Assuming the activation function used by neuron k is the\nlogistic function, then the term \u2202a /\u2202z is calculated in\nk k\na similar way to \u2202a /\u2202z : the value z is plugged into the\nl l k\nequation for the derivative of the logistic function. So,\nwritten out in long form the calculation of \u03b4 is:\nk\n\u03b4 =logistic(z )\u00d7(1\u2212logistic(z ))\u00d7(w \u00d7\u03b4 )\nk k k k,l l\nHowever, in order to connect the weight w with the error\nj,k\nof the network, the term \u03b4 must be multiplied by the par-\nk\ntial derivative of the weighted summation function with\nrespect to the weight: \u2202z /\u2202w . As described above, the\nk j,k\npartial derivative of a weighted sum function with respect\nto a weight reduces to the input associated with the weight\nw (i.e., a); and the gradient of the networks error with\nj,k j\nrespect to the hidden weight w is calculated by multi-\nj,k\nplying a by \u03b4 . Consequently, the product of the terms\nj k\n(\u2202z /\u2202w and \u03b4 ) forms a chain connecting the weight\nk j,k k\nw to the network error. For completeness, the product\nj,k\nof terms for w , assuming logistic activation functions in\nj,k\nthe neurons, is:\n\u2202Error \u2202z\n= k \u00d7\u03b4 =a \u00d7\u03b4\nk j k\n\u2202w \u2202w\nj,k j,k\nAlthough this discussion has been framed in the con-\ntext of a very simple network with only a single path of\n228 ChAPtER 6 connections, it generalizes to more complex networks be-\ncause the calculation of the \u03b4 terms for hidden units already\nconsiders the multiple connections emanating from a neu-\nron. Once the gradient of the network error with respect\nto a weight has been calculated (\u2202Error/w =\u03b4 \u00d7a ),\nj,k k j\nthe weight can be adjusted so as to reduce the weight of\nthe network using the gradient descent weight update\nrule. Here is the weight update rule, specified using the\nnotation from backpropagation, for the weight on the con-\nnection between neuron j and neuron k during iteration\nt of the algorithm:\nwt+1 =wt +(\u03b7\u00d7\u03b4 \u00d7a )\nj,k j,k k j\nFinally, an important caveat on training neural net-\nworks with backpropagation and gradient descent is that\nthe error surface of a neural network is much more com-\nplex than that of a linear models. Figure 6.3 illustrated the\nerror surface of a linear model as a smooth convex bowl\nwith a single global minimum (a single best set of weights).\nHowever, the error surface of a neural network is more like\na mountain range with multiple valleys and peaks. This is\nbecause each of the neurons in a network includes a non-\nlinear function in its mapping of inputs to outputs, and so\nthe function implemented by the network is a nonlinear\nfunction. Including a nonlinearity within the neurons of\na network increases the expressive power of the network\nLEARnIng FunC tIons 229 in terms of its ability to learn more complex functions.\nHowever, the price paid for this is that the error surface\nbecomes more complex and the gradient descent algo-\nrithm is no longer guaranteed to find the set of weights\nthat define the global minimum on the error surface; in-\nstead it may get stuck within a minima (local minimum).\nFortunately, however, backpropagation and gradient de-\nscent can still often find sets of weights that define useful\nmodels, although searching for useful models may require\nrunning the training process multiple times to explore dif-\nferent parts of the error surface landscape.\n230 ChAPtER 6 7\nTHE FUTURE OF DEEP LEARNING\nOn March 27, 2019, Yoshua Bengio, Geoffrey Hinton, and\nYann LeCun jointly received the ACM A.M. Turing award.\nThe award recognized the contributions they have made\nto deep learning becoming the key technology driving the\nmodern artificial intelligence revolution. Often described\nas the \u201cNobel Prize for Computing,\u201d the ACM A.M Tur-\ning award carries a $1 million prize. Sometimes working\ntogether, and at other times working independently or in\ncollaboration with others, these three researchers have,\nover a number of decades of work, made numerous contri-\nbutions to deep learning, ranging from the popularization\nof backpropagation in the 1980s, to the development of\nconvolutional neural networks, word embeddings, atten-\ntion mechanisms in networks, and generative adversarial\nnetworks (to list just some examples). The announcement\nof the award noted the astonishing recent breakthroughs that deep learning has led to in computer vision, robot-\nics, speech recognition, and natural language processing,\nas well as the profound impact that these technologies\nare having on society, with billions of people now using\ndeep learning based artificial intelligence on a daily basis\nthrough smart phones applications. The announcement\nalso highlighted how deep learning has provided scien-\ntists with powerful new tools that are resulting in scien-\ntific breakthroughs in areas as diverse as medicine and\nastronomy. The awarding of this prize to these research-\ners reflects the importance of deep learning to modern\nscience and society. The transformative effects of deep\nlearning on technology is set to increase over the com-\ning decades with the development and adoption of deep\nlearning continuing to be driven by the virtuous cycle of\never larger datasets, the development of new algorithms,\nand improved hardware. These trends are not stopping,\nand how the deep learning community responds to them\nwill drive growth and innovations within the field over the\ncoming years.\nBig Data Driving Algorithmic Innovations\nChapter 1 introduced the different types of machine learn-\ning: supervised, unsupervised, and reinforcement learn-\ning. Most of this book has focused on supervised learning,\n232 ChAPtER 7 primarily because it is the most popular form of machine\nlearning. However, a difficulty with supervised learning\nis that it can cost a lot of money and time to annotate\nthe dataset with the necessary target labels. As datasets\ncontinue to grow, the data annotation cost is becoming\na barrier to the development of new applications. The\nImageNet dataset1 provides a useful example of the scale\nof the annotation task involved in deep learning projects.\nThis data was released in 2010, and is the basis for the Ima-\ngeNet Large- Scale Visual Recognition Challenge (ILSVRC).\nThis is the challenge that the AlexNet CNN won in 2012\nand the ResNet system won in 2015. As was discussed in\nchapter 4, AlexNet winning the 2012 ILSVRC challenge\ngenerated a lot of excitement about deep learning mod-\nels. However, the AlexNet win would not have been pos-\nsible without the creation of the ImageNet dataset. This\ndataset contains more than fourteen million images that\nhave been manually annotated to indicate which objects\nare present in each image; and more than one million of\nthe images have actually been annotated with the bound-\ning boxes of the objects in the image. Annotating data at\nthis scale required a significant research effort and budget,\nand was achieved using crowdsourcing platforms. It is not\nfeasible to create annotated datasets of this size for every\napplication.\nOne response to this annotation challenge has\nbeen a growing interest in unsupervised learning. The\nthE FutuRE oF dEEP LEARnIng 233 As datasets continue\nto grow, the data\nannotation cost is\nbecoming a barrier to\nthe development of new\napplications. autoencoder models used in Hinton\u2019s pretraining (see\nchapter 4) are one neural network approach to unsuper-\nvised learning, and in recent years different types of au-\ntoencoders have been proposed. Another approach to this\nproblem is to train generative models. Generative models\nattempt to learn the distribution of the data (or, to model\nthe process that generated the data). Similar to autoen-\ncoders, generative models are often used to learn a useful\nrepresentation of the data prior to training a supervised\nmodel. Generative adversarial networks (GANs) are an ap-\nproach to training generative models that has received a\nlot of attention in recent years (Goodfellow et al. 2014). A\nGAN consists of two neural networks, a generative model\nand a discriminative model, and a sample of real data. The\nmodels are trained in an adversarial manner. The task of\nthe discriminative model is to learn to discriminate be-\ntween real data sampled from the dataset, and fake data\nthat has been synthesized by the generator. The task of\nthe generator is to learn to synthesize fake data that can\nfool the discriminative model. Generative models trained\nusing a GAN can learn to synthesize fake images that\nmimic an artistic style (Elgammal et al. 2017), and also to\nsynthesize medical images along with lesion annotations\n(Frid- Adar et al. 2018). Learning to synthesize medical\nimages, along with the segmentation of the lesions in\nthe synthesized image, opens the possibility of automati-\ncally generating massive labeled datasets that can be used\nthE FutuRE oF dEEP LEARnIng 235 for supervised learning. A more worrying application of\nGANs is the use of these networks to generate deep fakes:\na deep fake is a fake video of a person doing something\nthey never did that is created by swapping their face into a\nvideo of someone else. Deep fakes are very hard to detect,\nand have been used maliciously on a number of occasions\nto embarrass public figures, or to spread fake news stories.\nAnother solution to the data labeling bottleneck is that\nrather than training a new model from scratch for each\nnew application, we rather repurpose models that have\nbeen trained on a similar task. Transfer learning is the ma-\nchine learning challenge of using information (or repre-\nsentations) learned on one task to aid learning on another\ntask. For transfer learning to work, the two tasks should\nbe from related domains. Image processing is an example\nof a domain where transfer learning is often used to speed\nup the training of models across different tasks. Transfer\nlearning is appropriate for image processing tasks because\nlow- level visual features, such as edges, are relatively stable\nand useful across nearly all visual categories. Furthermore,\nthe fact that CNN models learn a hierarchy of visual fea-\nture, with the early layers in CNN learning functions that\ndetect these low- level visual features in the input, makes it\npossible to repurpose the early layers of pretrained CNNs\nacross multiple image processing projects. For example,\nimagine a scenario where a project requires an image clas-\nsification model that can identify objects from specialized\n236 ChAPtER 7 categories for which there are no samples in general image\ndatasets, such as ImageNet. Rather than training a new\nCNN model from scratch, it is now relatively standard to\nfirst download a state- of- the- art model (such as the Mi-\ncrosoft ResNet model) that has been trained on ImageNet,\nthen replace the later layers of the model with a new set\nof layers, and finally to train this new hybrid-m odel on\na relatively small dataset that has been labeled with the\nappropriate categories for the project. The later layers of\nthe state- of- the- art (general) model are replaced because\nthese layers contain the functions that combine the low-\nlevel features into the task specific categories the model\nwas originally trained to identify. The fact that the early\nlayers of the model have already been trained to identify\nthe low- level visual features speeds up the training and re-\nduces the amount of data needed to train the new project\nspecific model.\nThe increased interest in unsupervised learning, gen-\nerative models, and transfer learning can all be understood\nas a response to the challenge of annotating increasingly\nlarge datasets.\nThe Emergence of New Models\nThe rate of emergence of new deep learning models is ac-\ncelerating every year. A recent example is capsule networks\nthE FutuRE oF dEEP LEARnIng 237 (Hinton et al. 2018; Sabour et al. 2017). Capsule networks\nare designed to address some of the limitations of CNNs.\nOne problem with CNNs, sometimes known as the Picasso\nproblem, is the fact that a CNN ignores the precise spatial\nrelationships between high-l evel components within an\nobject\u2019s structure. What this means in practice is that a\nCNN that has been trained to identify faces may learn to\nidentify the shapes of eyes, the nose, and the mouth, but\nwill not learn the required spatial relationships between\nthese parts. Consequently, the network can be fooled by\nan image that contains these body parts, even if they are\nnot in the correct relative position to each other. This\nproblem arises because of the pooling layers in CNNs that\ndiscard positional information.\nAt the core of capsule networks is the intuition that\nthe human brain learns to identify object types in a view-\npoint invariant manner. Essentially, for each object type\nthere is an object class that has a number of instantiation\nparameters. The object class encodes information such as\nthe relative relationship of different object parts to each\nother. The instantiation parameters control how the ab-\nstract description of an object type can be mapped to the\nspecific instance of the object that is currently in view (for\nexample, its pose, scale, etc.).\nA capsule is a set of neurons that learns to identify\nwhether a specific type of object or object part is present\nat a particular location in an image. A capsule outputs an\n238 ChAPtER 7 activity vector that represents the instantiation parame-\nters of the object instance, if one is present at the relevant\nlocation. Capsules are embedded within convolutional\nlayers. However, capsule networks replace the pooling\nprocess, which often defines the interface between convo-\nlutional layers, with a process called dynamic routing. The\nidea behind dynamic routing is that each capsule in one\nlayer in the network learns to predict which capsule in the\nnext layer is the most relevant capsule for it to forward its\noutput vector to.\nAt the time or writing, capsule networks have the state-\nof- the- art performance on the MNIST handwritten digit\nrecognition dataset that the original CNNs were trained\non. However, by today\u2019s standards, this is a relatively small\ndataset, and capsule networks have not been scaled to\nlarger datasets. This is partly because the dynamic rout-\ning process slows down the training of capsule networks.\nHowever, if capsule networks are successfully scaled, then\nthey may introduce an important new form of model that\nextends the ability of neural networks to analyze images\nin a manner much closer to the way humans do.\nAnother recent model that has garnered a lot of in-\nterest is the transformer model (Vaswani et al. 2017).\nThe transformer model is an example of a growing trend\nin deep learning where models are designed to have so-\nphisticated internal attention mechanisms that enable a\nmodel to dynamically select subsets of the input to focus\nthE FutuRE oF dEEP LEARnIng 239 on when generating an output. The transformer model\nhas achieved state- of- the- art performance on machine\ntranslation for some language pairs, and in the future this\narchitecture may replace the encoder-d ecoder architecture\ndescribed in chapter 5. The BERT (Bidirectional Encoder\nRepresentations from Transformers) model has built on\nthe Transformer architecture (Devlin et al. 2018). The\nBERT development is particularly interesting because at\nits core is the idea of transfer learning (as discussed above\nin relation to the data annotation bottleneck). The basic\napproach to creating a natural language processing model\nwith BERT is to pretrain a model for a given language us-\ning a large unlabeled dataset (the fact that the dataset is\nunlabeled means that it is relatively cheap to create). This\npretrained model can then be used as the basis to create a\nmodels for specific tasks for the language (such as senti-\nment classification or question answering) by fine- tuning\nthe pretrained model using supervised learning and a\nrelatively small annotated dataset. The success of BERT\nhas shown this approach to be tractable and effective in\ndeveloping state- of- the- art natural language processing\nsystems.\nNew Forms of Hardware\nToday\u2019s deep learning is powered by graphics processing\nunits (GPUs): specialized hardware that is optimized to\n240 ChAPtER 7 do fast matrix multiplications. The adoption, in the late\n2000s, of commodity GPUs to speed up neural network\ntraining was a key factor in many of the breakthroughs\nthat built momentum behind deep learning. In the last\nten years, hardware manufacturers have recognized the\nimportance of the deep learning market and have devel-\noped and released hardware specifically designed for deep\nlearning, and which supports deep learning libraries, such\nas TensorFlow and PyTorch. As datasets and networks\ncontinue to grow in size, the demand for faster hardware\ncontinues. At the same time, however, there is a grow-\ning recognition of the energy costs associated with deep\nlearning, and people are beginning to look for hardware\nsolutions that have a reduced energy footprint.\nNeuromorphic computing emerged in the late 1980s\nfrom the work of Carver Mead.2 A neuromorphic chip is\ncomposed of a very- large- scale integrated (VLSI) circuit,\nconnecting potentially millions of low-p ower units known\nas spiking neurons. Compared with the artificial neurons\nused in standard deep learning systems, the design of a\nspiking neuron is closer to the behavior of biological neu-\nrons. In particular, a spiking neuron does not fire in re-\nsponse to the set of input activations propagated to it at a\nparticular time point. Instead, a spiking neuron maintains\nan internal state (or activation potential) that changes\nthrough time as it receives activation pulses. The activa-\ntion potential increases when new activations are received,\nthE FutuRE oF dEEP LEARnIng 241 and decays through time in the absence of incoming ac-\ntivations. The neuron fires when its activation potential\nsurpasses a specific threshold. Due to the temporal decay\nof the neuron\u2019s activation potential, a spiking neuron only\nfires if it receives the requisite number of input activations\nwithin a time window (a spiking pattern). One advantage\nof this temporal based processing is that spiking neurons\ndo not fire on every propagation cycle, and this reduces\nthe amount of energy the network consumes.\nIn comparison with traditional CPU design, neuro-\nmorphic chips have a number of distinctive characteristics,\nincluding:\n1. Basic building blocks: traditional CPUs are built using\ntransistor based logic gates (e.g., AND, OR, NAND gates),\nwhereas neuromorphic chips are built using spiking\nneurons.\n2. Neuromorphic chips have an analog aspect to them:\nin a traditional digital computer, information is sent in\nhigh- low electrical bursts in sync with a central clock; in\na neuromorphic chip, information is sent as patterns of\nhigh- low signals that vary through time.\n3. Architecture: the architecture of traditional CPUs\nis based on the von Neumann architecture, which is\nintrinsically centralized with all the information passing\nthrough the CPU. A neuromorphic chip is designed to\n242 ChAPtER 7 allow massive parallelism of information flow between\nthe spiking neurons. Spiking neurons communicate\ndirectly with each other rather than via a central\ninformation processing hub.\n4. Information representation is distributed through\ntime: the information signals propagated through a\nneuromorphic chip use a distributed representation,\nsimilar to the distributed representations discussed in\nchapter 4, with the distinction that in a neuromorphic\nchip these representations are also distributed through\ntime. Distributed representations are more robust to\ninformation loss than local representations, and this is\na useful property when passing information between\nhundreds of thousands, or millions, of components,\nsome of which are likely to fail.\nCurrently there are a number of major research proj-\nects focused on neuromorphic computing. For example,\nin 2013 the European Commission allocated one billion\neuros in funding to the ten- year Human Brain Project.3\nThis project directly employs more than five hundred sci-\nentists, and involves research from more than a hundred\nresearch centers across Europe. One of the projects key ob-\njectives is the development of neuromorphic computing\nplatforms capable of running a simulation of a complete\nhuman brain. A number of commercial neuromorphic\nthE FutuRE oF dEEP LEARnIng 243 chips have also been developed. In 2014, IBM launched\nthe TrueNorth chip, which contained just over a million\nneurons that are connected together by over 286 million\nsynapses. This chip uses approximately 1/10,000th the\npower of a conventional microprocessor. In 2018, Intel\nLabs announced the Loihi (pronounced low- ee- hee) neu-\nromorphic chip. The Loihi chip has 131,072 neurons con-\nnected together by 130,000,000 synapses. Neuromorphic\ncomputing has the potential to revolutionize deep learn-\ning; however, it still faces a number of challenges, not least\nof which is the challenge of developing the algorithms and\nsoftware patterns for programming this scale of massively\nparallel hardware.\nFinally, on a slightly longer time horizon, quantum\ncomputing is another stream of hardware research that\nhas the potential to revolutionize deep learning. Quantum\ncomputing chips are already in existence; for example, In-\ntel has created a 49-q ubit quantum test chip, code named\nTangle Lake. A qubit is the quantum equivalent of a binary\ndigit (bit) in traditional computing. A qubit can store more\nthan one bit of information; however, it is estimated that\nit will require a system with one million or more qubits\nbefore quantum computing will be useful for commercial\npurposes. The current time estimate for scaling quantum\nchips to this level is around seven years.\n244 ChAPtER 7 The Challenge of Interpretability\nMachine learning, and deep learning, are fundamentally\nabout making data- driven decisions. Although deep learn-\ning provides a powerful set of algorithms and techniques\nto train models that can compete (and in some cases out-\nperform) humans on a range of decision- making tasks,\nthere are many situations where a decision by itself is not\nsufficient. Frequently, it is necessary to provide not only\na decision but also the reasoning behind a decision. This\nis particularly true when the decision affects a person, be\nit a medical diagnosis or a credit assessment. This concern\nis reflected in privacy and ethics regulations in relation to\nthe use of personal data and algorithmic decision-m aking\npertaining to individuals. For example, Recital 714 of the\nGeneral Data Protection Regulations (GDPR) states that\nindividuals, affected by a decision made by an automated\ndecision- making process, have the right to an explanation\nwith regards to how the decision was reached.\nDifferent machine learning models provide different\nlevels of interpretability with regard to how they reach a\nspecific decision. Deep learning models, however, are pos-\nsibly the least interpretable. At one level of description,\na deep learning model is quite simple: it is composed of\nsimple processing units (neurons) that are connected to-\ngether into a network. However, the scale of the networks\n(in terms of the number of neurons and the connections\nthE FutuRE oF dEEP LEARnIng 245 between them), the distributed nature of the represen-\ntations, and the successive transformations of the input\ndata as the information flows deeper into the network,\nmakes it incredibly difficult to interpret, understand, and\ntherefore explain, how the network is using an input to\nmake a decision.\nThe legal status of the right to explanation within\nGDPR is currently vague, and the specific implications\nof it for machine learning and deep learning will need to\nbe worked out in the courts. This example does, however,\nhighlight the societal need for a better understanding of\nhow deep learning models use data. The ability to inter-\npret and understand the inner workings of a deep learn-\ning model is also important from a technical perspective.\nFor example, understanding how a model uses data can\nreveal if a model has an unwanted bias in how it makes its\ndecisions, and also reveal the corner cases that the model\nwill fail on. The deep learning and the broader artificial\nintelligence research communities are already responding\nto this challenge. Currently, there are a number of proj-\nects and conferences focused on topics such as explainable\nartificial intelligence, and human interpretability in ma-\nchine learning.\nChis Olah and his colleagues summarize the main\ntechniques currently used to examine the inner workings\nof deep learning models as: feature visualization, attribu-\ntion, and dimensionality reduction (Olah et al. 2018). One\n246 ChAPtER 7 way to understand how a network processes information is\nto understand what inputs trigger particular behaviors in\na network, such as a neuron firing. Understanding the spe-\ncific inputs that trigger the activation of a neuron enables\nus to understand what the neuron has learned to detect in\nthe input. The goal of feature visualization is to generate\nand visualize inputs that cause a specific activity within a\nnetwork. It turns out that optimization techniques, such\na backpropogation, can be used to generate these inputs.\nThe process starts with a random generated input and the\ninput is then iteratively updated until the target behavior\nis triggered. Once the required necessary input has been\nisolated, it can then be visualized in order to provide a bet-\nter understanding of what the network is detecting in the\ninput when it responds in a particular way. Attribution fo-\ncuses on explaining the relationship between neurons, for\nexample, how the output of a neuron in one layer of the\nnetwork contributes to the overall output of the network.\nThis can be done by generating a saliency (or heat- map)\nfor the neurons in a network that captures how much\nweight the network puts on the output of a neuron when\nmaking a particular decision. Finally, much of the activity\nwithin a deep learning network is based on the processing\nof high- dimensional vectors. Visualizing data enables us\nto use our powerful visual cortex to interpret the data and\nthe relationships within the data. However, it is very dif-\nficult to visualize data that has a dimensionality greater\nthE FutuRE oF dEEP LEARnIng 247 than three. Consequently, visualization techniques that\nare able to systematically reduce the dimensionality of\nhigh- dimensional data and visualize the results are incred-\nibly useful tools for interpreting the flow of information\nwithin a deep network. t- SNE5 is a well-k nown technique\nthat visualizes high- dimensional data by projecting each\ndatapoint into a two- or three- dimensional map (van der\nMaaten and Hinton 2008). Research on interpreting deep\nlearning networks is still in its infancy, but in the com-\ning years, for both societal and technical reasons, this re-\nsearch is likely to become a more central concern to the\nbroader deep learning community.\nFinal Thoughts\nDeep learning is ideally suited for applications involving\nlarge datasets of high-d imensional data. Consequently,\ndeep learning is likely to make a significant contribution\nto some of the major scientific challenges of our age. In\nthe last two decades, breakthroughs in biological se-\nquencing technology have made it possible to generate\nhigh- precision DNA sequences. This genetic data has the\npotential to be the foundation for the next generation of\npersonalized precision medicine. At the same time, inter-\nnational research projects, such as the Large Hadron Col-\nlider and Earth orbit telescopes, generate huge amounts\n248 ChAPtER 7 One way to understand\nhow a network\nprocesses information\nis to understand what\ninputs trigger particular\nbehaviors in a network,\nsuch as a neuron firing. of data on a daily basis. Analyzing this data can help us to\nunderstand the physics of our universe at the smallest and\nthe biggest scales. In response to this flood of data, scien-\ntists are, in ever increasing numbers, turning to machine\nlearning and deep learning to enable them to analyze\nthis data.\nAt a more mundane level, however, deep learning al-\nready directly affects our lives. It is likely, that for the last\nfew years, you have unknowingly been using deep learning\nmodels on a daily basis. A deep learning model is prob-\nably being invoked every time you use an internet search\nengine, a machine translation system, a face recognition\nsystem on your camera or social media website, or use a\nspeech interface to a smart device. What is potentially\nmore worrying is that the trail of data and metadata that\nyou leave as you move through the online world is also\nbeing processed and analzsed using deep learning models.\nThis is why it is so important to understand what deep\nlearning is, how it works, what is it capable of, and its cur-\nrent limitations.\n250 ChAPtER 7 GLOSSARY\nActivation Function\nA function that takes as input the result of the weighted sum of the inputs to\na neuron and applies a nonlinear mapping to this weighted sum. Including an\nactivation function within the neurons of a network enables the network to\nlearn a nonlinear mapping. Examples of commonly used activation functions\ninclude: logistic, tanh, and ReLU.\nArtificial Intelligence\nThe field of research that is focused on developing computational systems\nthat can perform tasks and activities normally considered to require human\nintelligence.\nBackpropagation\nBackpropagation is an algorithm used to train a neural network with hidden\nlayers of neurons. During training, the weights in a network are iteratively\nupdated to reduce the error of the network. In order to update the weights\non the links coming into a specific neuron in a network, it is necessary to first\ncalculate an estimate of the contribution of the output of that neuron to the\noverall error of the network. The backpropagation algorithm is a solution to\ncalculating these estimates for each neuron in the network. Once these errors\nestimates have been calculated for each neuron, the weights of the neurons\ncan be updated using an optimization algorithm such as gradient descent.\nBackpropagation works in two phases: a forward pass and a backward pass. In\nthe forward pass, an example is presented to the network and the overall error\nof the network is calculated at the output layer of the network by comparing\nthe output of the network with the expected output for the example specified\nin the dataset. In the backward pass, the error of the network is shared back\nthrough the network with each neuron receiving a portion of blame for the\nerror in proportion to the sensitivity of the error to changes in the output\nof that neuron. The process of sharing back the errors through the network\nis known as backpropagating the errors and this is where the algorithm gets\nits name. Convolutional Neural Network\nA convolutional neural network is a network that has at least one convolu-\ntional layer in it. A convolution layer is composed of a set of neurons that share\nthe same set of weights and whose combined receptive fields cover an entire\ninput. The union of the outputs of such a set of neurons is known as a fea-\nture map. In many convolutional neural networks, features maps are passed\nthrough a ReLU activation layer and then a pooling layer.\nDataset\nA collection of instances with each instances described in terms of a set of\nfeatures. In its most basic form, a dataset is organized in an n \u00d7 m matrix,\nwhere n is the number of instances (rows) and m is the number of features\n(columns).\nDeep Learning\nDeep learning is the subfield of machine learning that designs and evaluates\ntraining algorithms and architectures for modern neural network models. A\ndeep neural network is a network that has multiple (e.g., >2) layers of hidden\nunits (or neurons).\nFeedforward Network\nA feedforward network is a neural network where all the connections in the\nnetwork point forward to the neurons in subsequent layer. In other words,\nthere are no links backward from the output of a neuron to the input of a\nneuron in an earlier layer.\nFunction\nA function is a deterministic mapping from a set of input values to one or more\noutput values. In the context of machine learning, the term function is often\nused interchangeably with the term model.\nGradient Descent\nGradient descent is an optimization algorithm for finding a function with the\nminimum error with respect to modeling the patterns in a dataset. In the\ncontext of training a neural network, gradient descent is used to find the set\nof weights for a neuron that minimizes the error of the output of the neuron.\nThe gradient the algorithm descends is the error gradient of the neuron as its\nweights are updated. The algorithm is frequently used in conjunction with\nbackpropagation to train neural networks with hidden layers of neurons.\n252 gLossARy GPU (Graphical Processing Unit)\nSpecialized hardware that is optimized for fast matrix multiplication. Origi-\nnally designed to increase the speed in graphics rendering but also found to\nspeed up the training of neural networks.\nLSTM (Long Short- Term Memory)\nA network designed to address the problem of vanishing gradients in recurrent\nneural networks. The network is composed of a cell block where activations\nflow through from one time- step to the next and a set of gates on the cell\nblock that control the flow of these activations. The gates are implemented\nusing layers of sigmoid and tanh activation functions. The standard LSTM\narchitecture has three such gates: the forget gate, the update gate, and the\noutput gate.\nMachine Learning (ML)\nThe field of computer science research that focuses on developing and evalu-\nating algorithms that enable computers to learn from experience. Generally\nthe concept of experience is represented as a dataset of historic events, and\nlearning involves identifying and extracting useful patterns from a data-\nset. A machine learning algorithm takes a dataset as input and returns a\nmodel that encodes the patterns the algorithm extracted (or learned) from\nthe data.\nMachine Learning Algorithm\nA process that analyzes a dataset and returns as model (i.e., an instan-\ntiation of a function as a computer program) that matches the patterns in\nthe data.\nModel\nIn machine learning, a model is a computer program that encodes the patterns\nthe machine learning algorithm has extracted from a dataset. There are many\ndifferent types of machine learning models; however, deep learning is focused\non creating neural network models with multiple layers of hidden neurons. A\nmodel is created (or trained) by running a machine learning algorithm on a\ndataset. Once the model has been trained, it can then be used to analyze new\ninstances; the term inference is sometimes used to describe the process of ana-\nlyzing a new instance using a trained model. In the context of machine learn-\ning, the terms model and function are often used interchangeably: a model is\nan instantiation of a function as a computer program.\ngLossARy 253 Neuromorphic Computing\nNeuromorphic chips are composed of very large sets of spiking neurons archi-\ntecture that are connected in a massively parallel manner.\nNeural Network\nA machine learning model that is implemented as a network of simple infor-\nmation processing units called neurons. It is possible to create a variety of\ndifferent types of neural networks by modifying the connections between the\nneurons in the network. Examples of popular types of neural networks in-\nclude: feedforward, convolutional, and recurrent networks.\nNeuron\nIn the context of deep learning (as opposed to brain science), a neuron is a\nsimple information processing algorithm that takes a number of numeric val-\nues as input and maps these values to a high- or low- output activation. This\nmapping is typically implemented by first multiplying each input value by a\nweight, then summing the results of these multiplications, and finally passing\nthe results of the weighted summation through an activation function.\nOverfitting\nOverfitting a dataset occurs if the model returned by a machine learning algo-\nrithm is so complex that it is able to model small variations in the data caused\nby the noise in the data sample.\nRecurrent Neural Network\nA recurrent neural network has a single layer of hidden neurons, the output\nof which is fed back into this layer with the next input. This feedback (or re-\ncurrence) within the network gives the network a memory that enables it to\nprocess each input within the context of what it has previously processed.\nRecurrent neural networks are ideally suited to processing sequential or time-\nseries data.\nReinforcement Learning\nThe goal of reinforcement learning is to enable an agent to learn a policy on\nhow it should act in a given environment. A policy is a function that maps\nfrom an agent\u2019s current observations of its environment and its own internal\nstate to an action. Typically used for online control tasks such as robot control\nand game playing.\n254 gLossARy ReLU Unit\nA ReLU unit is a neuron that uses a rectified linear function as its activation\nfunction.\nSupervised Learning\nA form of machine learning where the goal is to learn a function that maps\nfrom a set of input attributes for an instance to an accurate estimate of the\nmissing value for the target attribute of the same instance.\nTarget Attribute\nIn supervised machine learning, a target attribute is the attribute that the\nmodel is trained to estimate the value of.\nUnderfitting\nUnderfitting a dataset occurs if the model returned by a machine learning\nalgorithm is too simplistic to capture the real complexity of the relationship\nbetween the inputs and outputs in a domain.\nUnsupervised Learning\nA form of machine learning where the goal is to identify regularities, such as\nclusters of similar instances, in the data. Unlike supervised learning, there is\nno target attribute in an unsupervised learning task.\nVanishing Gradient\nThe vanishing gradient problem describes the fact that as more layers are\nadded to a network it takes longer to train the network. This problem is caused\nby the fact that when a neural network is trained using backpropagation and\ngradient descent, the updating of the weights on links coming into a neuron\nin the network is dependent on the gradient (or sensitivity) of the network\nerror with respect to the output of the neuron. Using backpropagation, the\nprocess of sharing back the error gradients through a neuron involves a se-\nquence of multiplications, often by values less than one. As a result, as the\nerror gradient is passed back through the network, the error gradient tends\nto get smaller and smaller (i.e., vanish). As a direct consequence of this, the\nupdates to weights in the early layers of the network are very small and the\nneurons in these layers take a long time to train.\ngLossARy 255  NOTES\nChapter 1\n1. https://deepmind.com/research/alphago/.\n2. The Elo rating system is a method for calculating the skill level of\nplayers in zero- sum games, such as Chess. It is named after its inventor,\nArpad Elo.\n3. Noise in data refers to corrupt or incorrect data. Noise in data can been\ncaused by broken sensors, or mistakes in data entry, and so on.\n4. By domain we mean the problem or task that we are trying to solve using\nmachine learning. For example, it could be spam filtering, house prices predic-\ntion, or automatically classifying X- rays.\n5. There are some scenarios where more complex dataset representations are\nrequired. For example, for time- series data, a dataset may require a three-\ndimensional representation, composed of a series of two- dimensional matri-\nces, each describing the state of the system at a point in time, linked together\nthrough time. The term tensor generalizes the concept of a matrix to higher\ndimensions.\nChapter 2\n1. It turns out that the relationship between annual income and happiness\nis linear up to a point, but that once your annual income goes beyond this\npoint more money won\u2019t make you happier. A study by Kahneman and Deaton\n(2010) found that in the US the general cutoff, after which increases in income\nno longer increase emotional well- being, was around $75,000.\n2. This is the same dataset that appears in table 1.1 in chapter 1; it is repeated\nhere for convenience.\nChapter 3\n1. The origin is the location in a coordinate system where the axes cross. In a\ntwo- dimensional coordinate system, it is where the x-a xis and y- axis cross\u2014 in\nother words, it is the location at coordinates x=0, y=0.\n2. In chapter 2, we used the same approach to merge the intercept parameter\nof the linear model into the weights of the model.\n3. To highlight this column organization the weights have been indexed\ncolumn- row, rather than row- column. 4. For further discussion on the size and growth of networks, see page 23 of\nGoodfellow et al. 2016.\nChapter 4\n1. Figures 3.6 and 3.7 show the linear (straight line) decision boundary of\nneuron that uses a threshold activation function.\n2. This illustration of the use of associative memory for pattern completion\nand error correction is inspired from an example in chapter 42 of MacKay\n2003.\n3. For example, Paul Werbos\u2019s 1974 PhD thesis is credited with being the first\npublication to describe the use of backpropagation of errors in the training of\nartificial neural networks (Werbos 1974).\n4. The Hopfield network architecture, introduced at the start of this sec-\ntion, also included recurrent connections (feedback loops between neurons).\nHowever, the design of the Hopfield architecture is such that a Hopfield net-\nwork cannot process sequences. Consequently, it is not considered a full RNN\narchitecture.\n5. I originally came across this Churchland quote in Marcus 2003 (p. 25).\n6. Critique of paper \u201cDeep Learning Conspiracy\u201d (Nature 521, p. 436), cri-\ntique posted by J\u00fcrgen Schmidhuber, June 2015, available at: http://people\n.idsia.ch/~juergen/deep-learning-conspiracy.html.\n7. There are a number of other ways that autoencoders can be constrained to\npreclude the possibility that the network will learn an uninformative identity\nmapping from inputs to outputs; for example, noise can be injected into the in-\nput patterns and the network can be trained to reconstruct the un- noisy data.\nAlternatively, the units in the hidden (or encoding) layer can be restricted\nto have binary values. Indeed, Hinton and his colleagues originally used net-\nworks called Restricted Boltzman Machines (RBMs) in their initial pretraining\nwork, which used binary units in the encoding layer.\n8. The number of layers trained during pretraining is a hyperparameter\nthat is set based on the intuition of the data scientist and trial- and- error\nexperimentation.\n9. As early as 1971, Alexey Ivakhnenko\u2019s GMDH method had been shown to\nbe able to train a deep network (up to eight layers), but this method had been\nlargely overlooked by the research community.\n10. Glorot initialization is also known as Xavier initialization. Both of these\nnames are references to one of the authors (Xavier Glorot) of the first paper\nthat introduced this initialization procedure: Xavier Glorot and Yoshua Bengio,\n\u201cUnderstanding the Difficulty of Training Deep Feedforward Neural Networks,\u201d\n258 notEs in Proceedings of the 13th International Conference on Artificial Intelligence and\nStatistics (AISTATS), 2010, pp. 249\u2013 256.\n11. Glorot initialization can also be defined as sampling the weights from\na Gaussian distribution with a mean of 0 and standard deviation set to the\nsquare root of 2 divided by n + n . However, both of these definitions of\nj j+1\nGlorot initialization have the same goal of ensuring a similar variance in acti-\nvations and gradients across the layers in a network.\n12. https://developer.nvidia.com/cuda-zone.\nChapter 5\n1. The explanation of LSTM units presented here is inspired by an excellent\nblog post by Christopher Olah, which explains LSTMs clearly and in detail; post\navailable at: http://colah.github.io/posts/2015\u201308-Understanding-LSTMs/.\n2. A sigmoid function is in fact a special case of the logistic function, and for\nthe purposes of this discussion the distinction is not relevant.\n3. If, for example, sigmoid units with an output range of 0 to 1 were used\nthen activations could only be either maintained or increased at each update\nand eventually the cell state would become saturated with maximum values.\nChapter 6\n1. This figure also appears in chapter 4 but it is repeated here for convenience.\nChapter 7\n1. http://www.image-net.org.\n2. https://en.wikipedia.org/wiki/Carver_Mead.\n3. https://www.humanbrainproject.eu/en/.\n4. Recitals are a non- legally binding section of a regulation that seeks to clar-\nify the meaning of the legal text.\n5. Laurens van der Maaten and Geoffrey Hinton, \u201cVisualizing Data using\nt- SNE,\u201d Journal of Machine Learning Research 9 (2008): 2579\u2013 2605.\nnotEs 259  REFERENCES\nAizenberg, I. N., N. N. Aizenberg, and J. Vandewalles. 2000. Multi-V alued and\nUniversal Binary Neurons: Theory, Learning and Applications. Springer.\nChellapilla, K., S. Puri, and Patrice Simard. 2006. \u201cHigh Performance Convo-\nlutional Neural Networks for Document Processing.\u201d In Tenth International\nWorkshop on Frontiers in Handwriting Recognition.\nChurchland, P. M. 1996. The Engine of Reason, the Seat of the Soul: A Philosophi-\ncal Journey into the Brain. MIT Press.\nDechter, R. 1986. \u201cLearning While Searching in Constraint- Satisfaction-\nProblems.\u201d In Proceedings of the Fifth National Conference on Artificial\nIntelligence (AAAI- 86), pp. 178\u2013 183.\nDevlin, J., M. W. Chang, K. Lee, and K. Toutanova. 2018. \u201cBert: Pre- training\nof deep bidirectional transformers for language understanding.\u201d arXiv pre-\nprint arXiv:1810.04805.\nElgammal, A., B. Liu, M. Elhoseiny, and M. Mazzone. 2017. \u201cCAN: Creative\nAdversarial Networks, Generating \u2018Art\u2019 by Learning about Styles and Deviat-\ning from Style Norms.\u201d arXiv:1706.07068.\nElman, J. L. 1990. \u201cFinding Structure in Time.\u201d Cogn. Sci. 14: 179\u2013 211.\nFrid- Adar, M., I. Diamant, E. Klang, M. Amitai, J. Goldberger, and H. Greens-\npan. 2018. \u201cGAN- based Synthetic Medical Image Augmentation for Increased\nCNN Performance in Liver Lesion Classification.\u201d arXiv:1803.01229.\nFukushima, K. 1980. \u201cNeocognitron: A self- organizing neural network model\nfor a mechanism of pattern recognition unaffected by shift in position.\u201d Biol.\nCybern. 36: 193\u2013 202.\nGlorot, X., and Y. Bengio. 2010. \u201cUnderstanding the Difficulty of Train-\ning Deep Feedforward Neural Networks.\u201d In Proceedings of the Thirteenth\nInternational Conference on Artificial Intelligence and Statistics (AISTATS),\npp. 249\u20132 56.\nGlorot, X., A. Bordes, and Y. Bengio. 2011. \u201cDeep Sparse Rectifier Neural Net-\nworks.\u201d In Proceedings of the Fourteenth International Conference on Artificial\nIntelligence and Statistics (AISTATS), pp. 315\u2013 323. Goodfellow, I., Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.\nGoodfellow, I., J. Pouget- Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A.\nCourville, and J. Bengio. 2014. \u201cGenerative Adversarial Nets.\u201d In Advances in\nNeural Information Processing Systems 27: 2672\u2013 2680.\nHe, K., X. Zhang, S. Ren, and J. Sun. 2016. \u201cDeep Residual Learning for Image\nRecognition.\u201d In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE, pp. 770\u2013 778. https://doi.org/10.1109/CVPR.2016.90.\nHebb, D. O. 1949. The Organization of Behavior: A Neuropsychological Theory.\nJohn Wiley & Sons.\nHerculano- Houzel, S. 2009. \u201cThe Human Brain in Numbers: A Linearly\nScaled- up Primate Brain.\u201d Front. Hum. Neurosci. 3. https://doi.org/10.3389/\nneuro.09.031.2009.\nHinton, G. E., S. Sabour, and N. Frosst. 2018. \u201cMatrix Capsules with EM\nRouting.\u201d In Proceedings of the 7th International Conference on Learning Rep-\nresentations (ICLR).\nHochreiter, S. 1991. Untersuchungen zu dynamischen neuronalen Netzen\n(Diploma). Technische Universit\u00e4t M\u00fcnchen.\nHochreiter, S., Schmidhuber, J. 1997. \u201cLong Short- Term Memory.\u201d Neural\nComput. 9: 1735\u2013 1780.\nHopfield, J. J. 1982. \u201cNeural Networks and Physical Systems with Emergent\nCollective Computational Abilities.\u201d Proc. Natl. Acad. Sci. 79: 2554\u2013 2558.\nhttps://doi.org/10.1073/pnas.79.8.2554.\nHubel, D. H., and T. N. Wiesel. 1962. \u201cReceptive Fields, Binocular Interaction\nand Functional Architecture in the Cat\u2019s Visual Cortex.\u201d J. Physiol. Lond. 160:\n106\u20131 54.\nHubel, D. H., and T. N. Wiesel. 1965. \u201cReceptive Fields and Function Architec-\nture in Two Nonstriate Visual Areas (18 and 19) of the Cat.\u201d J. Neurophysiol.\n28: 229\u20132 89.\nIvakhnenko, A. G. 1971. \u201cPolynomial Theory of Complex Systems.\u201d IEEE\nTrans. Syst. Man Cybern. 4: 364\u2013 378.\nKelleher, J. D., and B. Tierney. 2018. Data Science. MIT Press.\nKrizhevsky, A., I. Sutskever, and G. E. Hinton. 2012. \u201cImagenet Classification\nwith Deep Convolutional Neural Networks.\u201d In Advances in Neural Information\nProcessing Systems, pp. 1097\u2013 1105.\n262 REFEREnCEs LeCun, Y. 1989. Generalization and Network Design Strategies (Technical\nReport No. CRG- TR- 89- 4). University of Toronto Connectionist Research\nGroup.\nMaas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. \u201cRectifier Nonlinearities Im-\nprove Neural Network Acoustic Models.\u201d In Proceedings of the Thirteenth Inter-\nnational Conference on Machine Learning (ICML) Workshop on Deep Learning\nfor Audio, Speech and Language Processing, p. 3.\nMacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.\nCambridge University Press.\nMarcus, G.F. 2003. The Algebraic Mind: Integrating Connectionism and Cognitive\nScience. MIT Press.\nMcCulloch, W. S., and W. Pitts. 1943. \u201cA Logical Calculus of the Ideas Imma-\nnent in Nervous Activity.\u201d Bull. Math. Biophys. 5: 115\u2013 133.\nMikolov, T., K. Chen, G. Corrado, and J. Dean. 2013. \u201cEfficient Estimation of\nWord Representations in Vector Space.\u201d arXiv:1301.3781.\nMinsky, M., and S. Papert. 1969. Perceptrons. MIT Press.\nMnih, V., K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,\nand M. Riedmiller. 2013. \u201cPlaying Atari with Deep Reinforcement Learning.\u201d\nArXiv13125602 Cs.\nNilsson, N. J. 1965. Learning Machines: Foundations of Trainable Pattern-\nClassifying Systems, Series in Systems Science. McGraw-H ill.\nOh, K.- S., and K. Jung. 2004. \u201cGPU Implementation of Neural Networks.\u201d\nPattern Recognit. 36: 1311\u2013 1314.\nOlah, C., A. Satyanarayan, I. Johnson, S. Carter, S. Ludwig, K. Ye, and A.\nMordvintsev. 2018. \u201cThe Building Blocks of Interpretability.\u201d Distill. https://\ndoi.org/10.23915/distill.00010.\nReagen, B., R. Adolf, P. Whatmough, G.- Y. Wei, and D. Brooks. 2017. \u201cDeep\nLearning for Computer Architects.\u201d Synth. Lect. Comput. Archit. 12: 1\u2013 123.\nhttps://doi.org/10.2200/S00783ED1V01Y201706CAC041.\nReed, R. D., and R. J. Marks II. 1999. Neural Smithing: Supervised Learning in\nFeedforward Artificial Neural Networks. MIT Press.\nRosenblatt, F. 1960. On the Convergence of Reinforcement Procedures in\nSimple Perceptrons (Project PARA). (Report No. VG- 1196- G- 4). Cornell Aero-\nnautical Laboratory, Inc., Buffalo, NY.\nREFEREnCEs 263 Rosenblatt, F. 1962. Principles of Neurodynamics: Perceptrons and the Theory of\nBrain Mechanisms. Spartan Books.\nRosenblatt, Frank, 1958. \u201cThe Perceptron: A Probabilistic Model for Infor-\nmation Storage and Organization in the Brain.\u201d Psychol. Rev. 65: 386\u2013 408.\nhttps://doi.org/10.1037/h0042519.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986a. \u201cLearning Internal\nRepresentations by Error Propagation.\u201d In D. E. Rumelhart, J. L. McClelland,\nand PDP Research Group, eds. Parallel Distributed Processing: Explorations in\nthe Microstructure of Cognition, Vol. 1. MIT Press, pp. 318\u2013 362.\nRumelhart, D.E., J. L. McClelland, PDP Research Group, eds. 1986b. Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, Vol. 1:\nFoundations. MIT Press.\nRumelhart, D.E., J. L. McClelland, PDP Research Group, eds. 1986c. Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, Vol. 2:\nPsychological and Biological Models. MIT Press.\nSabour, S., N. Frosst, and G. E. Hinton. 2017. \u201cDynamic Routing Between\nCapsules.\u201d In Proceedings of the 31st Conference on Neural Information Proc-\nessing (NIPS). pp. 3856\u2013 3866.\nSchmidhuber, J. 2015. \u201cDeep Learning in Neural Networks: An Overview.\u201d\nNeural Netw. 61: 85\u2013 117.\nSteinkraus, D., Patrice Simard, and I. Buck. 2005. \u201cUsing GPUs for Machine\nLearning Algorithms.\u201d In Eighth International Conference on Document Analy-\nsis and Recognition (ICDAR\u201905). IEEE. https://doi.org/10.1109/ICDAR.2005\n.251.\nSutskever, I., O. Vinyals, and Q. V. Le. 2014. \u201cSequence to Sequence Learning\nwith Neural Networks.\u201d In Advances in Neural Information Processing Systems\n(NIPS), pp. 3104\u2013 3112.\nTaigman, Y., M. Yang, M. Ranzato, and L. Wolf. 2014. \u201cDeepFace: Closing\nthe Gap to Human- Level Performance in Face Verification.\u201d Presented at the\nProceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 1701\u2013 1708.\nvan der Maaten, L., and G. E. Hinton. 2008. \u201cVisualizing Data Using t- SNE.\u201d J.\nMach. Learn. Res. 9, 2579\u2013 2605.\n264 REFEREnCEs Vaswani, A., N. Shazer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kai-\nser, and I. Polosukhin. 2017. \u201cAttention Is All You Need.\u201d In Proceedings of\nthe 31st Conference on Neural Information Processing (NIPS), pp. 5998\u2013 6008.\nWerbos, P. 1974. \u201cBeyond Regression: New Tools for Prediction and Analysis\nin the Behavioral Sciences.\u201d PhD diss., Harvard University.\nWidrow, B., and M.E. Hoff. 1960. Adaptive Switching Circuits (Technical\nReport No. 1553- 1). Stanford Electronics Laboratories, Stanford University,\nStanford, California.\nXu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\nBengio, Y. 2015. \u201cShow, Attend and Tell: Neural Image Caption Generation\nwith Visual Attention.\u201d In Proceedings of the 32nd International Conference on\nMachine Learning, Proceedings of Machine Learning Research. PMLR, pp. 2048\u2013\n2057.\nREFEREnCEs 265  FURTHER READINGS\nBooks on Deep Learning and Neural Networks\nCharniak, Eugene. 2018. Introduction to Deep Learning. MIT Press.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning.\nMIT Press.\nHagan, Martin T., Howard B. Demuth, Mark Hudson Beale, and Orlando De\nJes\u00fas. 2014. Neural Network Design. 2nd ed.\nReagen, Brandon, Robert Adolf, Paul Whatmough, Gu- Yeon Wei, and David\nBrooks. 2017. \u201cDeep Learning for Computer Architects.\u201d Synthesis Lectures on\nComputer Architecture 12 (4): 1\u2013 123.\nSejnowski, Terrence J. 2018. The Deep Learning Revolution. MIT Press.\nOnline Resources\nNielsen, Michael A. 2015. Neural Networks and Deep Learning. Determination\nPress. Available at: http://neuralnetworksanddeeplearning.com.\nDistill (an open access journal with many articles on deep learning and\nmachine learning). Available at: https://distill.pub.\nOverview Journal Articles\nLeCun, Yann, Yoshua Bengio, and Geoffrey E. Hintron. 2015. \u201cDeep Learn-\ning.\u201d Nature 521: 436\u2013 444.\nSchmidhuber, J\u00fcrgen. 2015. \u201cDeep Learning in Neural Networks: An Over-\nview.\u201d Neural Networks 61: 85\u2013 117.  INDEX\nActivation functions. See also specific Attribution, 247\nfunctions Autoencoders, 144, 145\u2013 148\nbackpropagation and, 127 Axon, 65\u2013 66\ncommon characteristics, 79\ndefined, 251 Backpropagation\nderivative plot, 220 in deep learning history, 102,\nelement- wise application, 96 125\u2013 129\nhistory of, 158 defined, 251\nnecessity of, 77\u2013 80, 82 error gradients, 150\u2013 151\nneural network, 62 learning rule, 210\nneuron, 70\u2013 79, 127, 150\u2013 151 meanings of, 209\u2013 210\n\u03c6 notation, 96 ReLUs, 152\nshape of, 76, 79 RNNs, 175\u2013 176\nweight adjustments, 207 training neural networks, 138,\nActivation space, 59\u2013 61 209\u2013 210\nAdaptive linear neuron (ADALINE) the \u03b4 s, 216\u2013 222\nnetwork, 116\u2013 117 Backpropagation algorithm\nAizenberg, I. N., 143 background, 125\u2013 126\nAlexNet, 102, 138, 169\u2013 170, 233 backward pass, 126, 211\u2013 213, 215\nAlgorithms, 7\u2013 8. See also blame assignment, 126, 213\u2013 214\nBackpropagation algorithm; credit assignment problem,\nGradient descent algorithm; solving the, 125, 186, 209\u2013 210\nLeast mean squares (LSM) described, 209\u2013 210\nalgorithm; Machine learning error gradients, 211\u2013 213\n(ML) algorithm error propagation, 128\u2013 129\nAlphaGo (Deep Mind), 2, 4 forward pass, 126, 211, 214\nAND function, 119\u2013 119, 133 iteration steps, 213\u2013 214\nArtificial intelligence threshold activation function\nbackground, 4, 6\u2013 8 in, 127\nchallenges, 246 training function, 127, 186\ndefined, 251 two- stage, 126, 210\u2013 215\nmachine learning and, 4, 6 weight adjustments, 126\u2013 127,\nrelationships, 6, 10 222\u2013 230\nAssumptions, encoded, 18, 21 Backward pass, 126, 211\u2013 213, 215 Baidu, 1 Compute unified device architecture\nBengio, Yoshua, 231 (CUDA), 154\nBias Connectionism, 124, 129\u20131 41,\nin artificial neurons, 88 156\u2013 157\ninductive, 17\u2013 22 Connection weights, 70\npermissive, 20 Consumer devices, 37\npreference, 19\u2013 20 Convolutional layer, 168\u2013 170\nrestriction, 19 Convolutional neural network\nBias term, 88\u2013 92 (CNN)\nBidirectional Encoder architecture, 182\nRepresentations from convolution operation, 165\nTransformers (BERT) model, in deep learning history, 133\u2013 143\n240 defined, 252\nBig data design goal, 160\ndeep learning\u2019s impact on, 35 feature maps, 165\u2013 166, 168\ndriving algorithmic innovation, functions, 160\n232\u2013 237 kernels, 165, 168\u2013 169\nemergence of, 23 limitations, 237\u2013 238\nBiological neurons, 241 output, 165\nBlame assignment, 123, 126, pooling function, 166, 168\u2013 170,\n213\u2013 214 238\u2013 239\nBMI example, 32 processing stages, 163\u2013 164, 168\nBox, George, 40 receptive field, 162\u2013 166\nBrain, human, 65\u20136 7, 238 training, 153\ntranslation invariance, 161\u2013 163\nCandidate functions, 25\u2013 26, 28 visual feature detection, 160\u2013 163,\nCapsule networks, 237\u2013 239 168\u2013 170\nCars, self- driving, 1 Convolutional neural network\nCells, LSTM, 177\u2013 178, 180 (CNN) models\nChain rule, 128 pooling function, 238\u2013 239\nChellapilla, K., 154 transfer learning, 236\u2013 237\nChess, 2\u2013 4 visual feature detection, 236,\nChurchland, P. M., 140 238\nCivil liberties, 37, 245 Convolution mask, 165\nComplex cells, 135\u2013 136 Credit assignment problem, defined,\nComplex models, 62 123\nComputer games. See Game playing Credit assignment problem, solving\nComputer power, growth of, the. See also Loan decision\n153\u2013 155 model\n270 IndEX algorithms in, 7 growth, 241, 248\nbackpropagation algorithm, 125, high- dimensional, 35\n186, 209\u2013 210 historic, creating, 30\ndataset example, 6, 7, 27, 49, 51, large, 22\u2013 23, 35\n53 in machine learning, 6\u2013 7\nfunctions in the, 8, 10 modeling, 194\u2013 196\nmodeling, 27, 46, 48\u2013 55, 73 parameters, modifying to fit the\nweight adjustments, 51, 71, model, 49\u2013 54\n210\u2013 211 simplest form, 6\u2013 7, 24, 26\nCybernetics, 102 single input- output, 187\u2013 188\nDataset sizes, increases in, 153\u2013 155,\nData 233\u2013 235\nanalyzing for customer Dechter, Rina, 143\nsegmentation, 28 Decision boundaries, two- input\nclustering, 28\u2013 29 neurons, 84\u2013 91\nextracting accurate functions Decision- making\nfrom, 14 automated, GDPR rights,\nlearning patterns from, algorithm 245\u2013 246\nfor, 185 data- driven, 1, 3, 4\u2013 5\nneural network training on, 122, intuitive, 4, 22\n185 Decision space, 59\u2013 60. See also\nnoise in, 16, 20 Activation space\noverfitting/underfitting, 20, 22 Decoder, 142, 182\u2013 183\npersonal, protections for, 37, DeepBlue, 3\n245\u2013 246 DeepFace, 23\nunderfitting, 77\u2013 78 Deep fakes, 235\u2013 236\nData annotation costs, 233 Deep learning\nData-d riven decisions, enabling, 3 benefits, 248\u2013 250\nData labeling bottleneck, 144, 236 data- driven decision making, 1,\nDataset analysis algorithms, 7\u2013 8 3, 4\u2013 5\nDataset design, 25, 32, 34\u2013 35 defined, 252\nDatasets development drivers, 232\nannotated, 233\u2013 235 emergence of, 23\ncredit assignment problem, era of, 143\u2013 144\nsolving the, 6, 7, 27, 49, 51, 53 examples, 1\u2013 2\ndefined, 252 power of, 183\nerror of a model on, 190\u2013 191 relationships, 6, 10\nfeature selection, tradeoffs in, success, factors in, 32\u2013 35\n24\u2013 25 summary overview, 36\u2013 37\nIndEX 271 Deep learning (cont.) perceptrons, multilayer, 124\nterm use, 143 perceptron training model, 103,\nusefulness, 4, 248 105\u2013 113, 116\nusers of, 1\u2013 2 periods in, 101\nDeep learning, future of ReLU activation functions, 148,\nbig data driving algorithmic 150\u2013 152\ninnovation, 232\u2013 237 RNNs, 103, 133\u2013 143, 173\ninterpretability, challenge of, seq2seq, 103, 142\n244\u2013 248 summary overview, 155\u2013 158\nnew hardware, 240\u2013 244 themes within, 101\u2013 102\nnew models, emergence of, threshold logic units, 103,\n237\u2013 240 104\u2013 105\nsummary overview, 248, 250 timeline, 103\nDeep learning, history of vanishing gradients, 103,\nbackpropagation, 103, 125\u2013 129 125\u2013 129\nCNNs, 102 virtuous cycle, 153\u2013 155\ncomputer power, growth of, weight initialization, 148,\n153\u2013 155 150\u2013 152\nconnectionism, 124, 129\u2013 133 XOR problem, 103, 116\u2013 123\ndataset sizes, increases in, Deep learning architectures. See\n153\u2013 155 Capsule network; Convolutional\ndeep learning era, 103, 143\u2013 144 neural network (CNN);\nElman network, 103, 139\u2013 140 Generative adversarial network\nGlorot initialization, 103, 148, (GAN); Long short- term\n150 memory (LSTM) network;\nGPUs, 103, 153 Recurrent neural network\nHebb\u2019s postulate, 103, 104\u2013 105 (RNN); Transformer model\nlayer- wise pretraining, 103 Deep learning- GPU relation, 97\nlayer- wise pretraining using Deep learning models\nautoencoders, 145\u2013 148 feature learning function, 36\u2013 37\nLMS algorithm, 103, 123 new, emergence of, 237\u2013 240\nlocal vs. distributed training, 31\nrepresentations, 129\u2013 133 usefulness, 156\nLSTM algorithm, 103, 113\u2013 116 Deep learning networks\nMcCulloch & Pitts model, 103, components, 68\n104 defined, 39, 68\nneocognitron, 103 neuron hidden layers in,\nnetwork architectures, 133\u2013 143, 67\u2013 68summary overview,\n173 98\u2013 100\n272 IndEX training, 97, 127\u2013 129, 147, 150, Feature map, 165\u2013 166, 168\n170 Feature selection, 32\nDeepMind, 2, 31 Feature vector, 622\nDelta rule, 114, 204 Feature visualization, 246\u2013 248\n\u03b4 s, backpropagating the, 216\u2013 222 Feedforward network\nDendrite, 65\u2013 66 defined, 252\nDense layer, 169\u2013 170 dense layer, 168\u2013 169\nDimensionality reduction, 247 fully connected, 133\u2013 134, 169\nDiscriminative models, 235 neuron inputs and outputs, 92\ndistributed representation, standard, 92, 169\n129\u2013 132, 142, 243 training, 134, 151\nDivide-a nd conquer strategy, 10, Filter vector, 179\n79\u2013 82 Fitness functions, 26\u2013 27\nDNA sequencing, 248 Forget gate, 177\u2013 178\nDot product operation, 87\u2013 88 Forward pass, 126, 211, 214\nFukushima, Kunihiko, 136\u2013 137\nEarth orbit telescopes, 248 Fully connected networks, 133\u2013 134\nElman, Jeffrey Locke, 139\u2013 140 Functions. See also specific functions\nElman network, 103, 139\u2013 141 defined, 4, 14, 252\nElo rating, 3 encoded, 12\nEncoder, 142, 182\u2013 183 equation of a line to define a,\nEncoder- decoder architecture, 18\n182\u2013 183, 244 examples, 15, 21\nError, calculating, 190\u2013 191 if- then- else rules, 19\nError curves, 197\u2013 198 in machine learning, 7\u2013 8\nError gradients, 211\u2013 211 mathematical model vs., 40\nError signals, 128\u2013 129 models vs., 13\nError surface, 192, 193, 194\u20131 96, nonlinear as activation function,\n198 77\nEthics regulation, 245 partial derivatives, 199\u2013 200\nrate of change, 199\nFacebook, 1, 23, 156 representing, 8\nFace recognition simpler, 19\nCNNs for, 160\u2013 163, 168\u2013 169, 238 template structure defining,\nspatially invariant, 136 18\u2013 19\ntransfer learning for, 236\nFace recognition function, 15 Game playing, 2\u2013 4, 29, 31\nFace- recognition software, 23, 35, Gates, LSTM networks, 177\u2013 178\n156 Gene prediction function, 15\nIndEX 273 General Data Protection Regulations Happiness- income example, 41\u2013 43\n(GDPR), 245\u2013 246 Hardware energy costs, 241\nGenerative adversarial networks Healthcare sector, 1\n(GANs), 235 Hebb, Donald O., 104\u2013 105\nGenerative models, 235 Hebb\u2019s postulate, 103, 104\u2013 105\nGeometric spaces, 59\u2013 63 Hiker example of gradient descent,\nGlorot, X., 148 196\u2013 197\nGlorot initialization, 103, 148, 150 Hinge activation function, 73\nGo, 2\u2013 4 Hinton, Geoffrey E., 125, 144, 231,\nGoogle, 1, 30, 156 235\nGradient descent, 260 Hochreiter, Sepp, 128, 141\nGradient descent algorithm Hoff, Marcian, 113\u2013 114, 116\ncomponents, 197 Hopfield, John, 124\ndefined, 252 Hopfield network, 103, 124\u2013 125\ndescending error surfaces, 203, Hubel, D. H., 134\u2013 137\n205\u2013 206 Human Brain Project, 243\nerror curves, 197\u2013 198, 205\u2013 206 Hyperparameter, 80, 100\ngoal of, 197\nhiker example, 196\u2013 197 IBM, 244\ninitial model, creating, 194, 196 If- then- else rules, 19\nsimplifying factors, 200 ill- posed problem, 16\u2013 17\nsummary, 204\u2013 205 Image captioning systems,\ntraining function, 185\u2013 186, 208 automatic, 182\nweight updates, 51, 53\u2013 56, Image map, 170\n197\u2013 208 ImageNet, 233, 236\u2013 237\nGraphical processing unit (GPU) ImageNet Large- Scale Visual\naccelerating training, 92\u2013 98 recognition Challenge (ILSVRC),\nadoption of, 240\u2013 241 138, 169\u2013 170, 233\nin deep learning history, 103, Image processing, 134\u2013 138,\n153\u2013 154 236\u2013 237. See also Face\ndefined, 253 recognition\nmanufacturing, 98 Image recognition, 136\nGreedy layer- wise pretraining, 144, Income- happiness relation, 41\u2013 43\n147 Inductive bias, 17\u2013 22\nGroup method for data handling Inference, 12, 14\u2013 15, 20, 29\n(GMDH) network, 103, 122 Information flows\ninterpreting, 247\nHandwritten digit recognition, 160, neural networks, 68, 70\n239 RNNs, 139, 171, 173\n274 IndEX Information processing intercept- slope changing a,\nneurons, artificial, 70\u2013 77 189\u2013 190\nunderstanding, 246\u2013 247 mapping function, 187\u2013 189\nInput gate, 177\u2013 178 Linear activation function. See\nInput- output mapping, 10\u2013 11 also Rectified linear activation\nInput space function\nloan decision model, 57\u2013 58, in deep learning history, 73\n83\u2013 84 equation of a line representing,\ntwo- input neurons, 84, 85, 86 188\u2013 189\nInput vector, 62 Linearly separable functions,\nIntel Labs, 244 117\u2013 119\nIntercept, 43, 46, 188\u2013 189 Linear models\nInterpretability, challenge of, combining, 54\u2013 57, 62\n244\u2013 248 credit solvency example, 44\u2013 48,\nIntuition, 4, 22 49, 54\u2013 60, 62\u2013 63\nIvakhenko, Alexey, 122 error variation, 192, 193, 194\nincome- happiness relation, 41\u2013 43\nJung, K., 153 learning weights in, 49\u2013 54\nmodeling nonlinear relations,\nKasparov, Gary, 3 77\u2013 78\nKe Jie, 2 with multiple inputs, 44\u2013 46\nKernels, 165, 168\u2013 169 parameter setting, 46, 48\u2013 49,\n61\u2013 62\nLanguage processing, 142, 240 summary overview, 61\u2013 63\nLarge Hadron Collider, 248 templates, 41\u2013 44\nLayer- wise pretraining, 103, Loan decision model. See also Credit\n144\u2013 148 assignment problem\nLearning. See specific types of coordinate spaces, 59\nLearning rate (\u019e), 110\u2013 112, dataset example, 6, 7\n204 input space, 57\u2013 58, 83\u2013 84\nLeast mean squares (LSM) two- input, 83\u2013 84\nalgorithm, 103, 113\u2013 116, 123, weights, adjusting, 84, 107\u2013 108\n185, 204 Localist representation, 129,\nLeast mean squares (LSM) rule, 131\u2013 132, 243\n123 \u201cLogical Calculus of the Ideas\nLeCun, Yann, 138, 161, 166, 231 Immanent in Nervous Activity,\nLine A\u201d (McCulloch & Pitts), 104\nbest fit, 187 Logistic activation function, 152\nequation of a, 18, 41\u2013 43, 188\u2013 190 Logistic units, 75, 80, 235\nIndEX 275 Loihi chip, 247 candidate functions, 23, 25\u2013 26,\nLong short- term memory (LSTM), 28\n103, 141\u2013 142, 253 data, 23\u2013 25\nLong short- term memory (LSTM) fitness functions, 26\u2013 27\nnetwork cells, 177\u2013 178, 180 fitness measures, 24\nLong short- term memory (LSTM) Machine translation, 15, 35, 142,\nnetworks, 177\u2013 178, 181\u2013 183 181\u2013 182\nMapping\nMacHack-6 (MIT), 3 deterministic, 7\nMachine learning (ML) nonlinear, 76, 78, 79\nartificial intelligence and, 4, 6 Mathematical model, 40\nbenefits, 248\u2013 250 Matrix multiplication, 72\ndefined, 253 max pooling, 166\ndifficulty factors in, 16\u2013 17 McCulloch, Walter, 103\u2013 104\nfeature selection and design, 32, Mead, Carver, 241\n34 Medical images, synthesizing, 235\nfunctions, 10\u2013 11 Memory. See also Long short- term\ngoal of, 8 memory (LSTM)\nreinforcement, 29\u2013 31 associative, 148\u2013 125\nrelationships, 6, 10 forward pass stored in, 211\nin situ, 30 RNN, 139, 170\u2013 177\nsummary overview, 36\u2013 37 Microsoft, 1\nsupervised, 27\u2013 31 Microsoft Research, 170\ntraining model, 12\u2013 14 Mikolov, Tomas, 181\nunderstanding, 6\u2013 9, 10\u2013 11 Minsky, Marvin, 116\u2013 120, 122\nMachine learning (ML) algorithm MIT, 3\nassumptions, 18, 21 MNIST handwritten digit\nbias in, 17\u2013 22 recognition dataset, 239\ndefined, 10, 253 Mobile phones, 1\nill- posed problems, solving, 17 Model parameters, 48\u2013 54, 9\nsources of information to select Models\nthe best function, 17\u2013 18 complex, 56, 62\nsuccess criterion, 21\u2013 22 defined, 12, 253\ntemplate structure defining, equation of a line defining, 41\u2013 44\n18\u2013 19 fixed, 14\nMachine learning (ML) models, functions vs., 13\n28\u2013 30, 143 geometric spaces, 57\u2013 61\nMachine learning (ML) success real- world correspondence, 40\u2013 41\nfactors templates, 40\u2013 43\n276 IndEX training, 12\u2013 14 tailoring, 152\nusefulness, 40 weighted sum calculations, 80\u2013 82\nvariables in, 40\u2013 41 Neural network model\nbias in, 22\nNatural language processing (NLP), data, overfitting vs. underfitting,\n181\u2013 182 22\nNeocognitron, 103, 136 datasets, suitability to large,\nNetwork architectures 22\u2013 23\nconvolutional neural, 133\u2013 143 function, 13, 185\nin deep learning history, 133\u2013 143 training, 23, 80, 185\nencoder- decoder, 240 Neural network training\nrecurrent neural, 133\u2013 143 accelerating using GPUs, 92\u2013 98\nNetwork error, 210\u2013 213, 222\u2013 225 backpropagation for, 209\u2013 210\nNeural machine translation, 156 on data, 122, 193\nNeural network deep neural networks, 127\u2013 128,\nactivation function, 62 185\u2013 186\nartificial, 67\u2013 68, 70 hardware to speed up, 153\u2013 154\ncompositional nature, 99 with multiple layers, 120, 208\nconnection weights, 70 Neural network training model, 23,\ndefined, 65, 262 82, 185\ndepth, 97 Neuromorphic computing, 241\u2013 244,\ndesigning, 157\u2013 158 254\nfunctions, 78\u2013 79 Neurons\ngeometric spaces, 57\u2013 61 activation function, 61\u2013 62, 71\u2013 77,\ngraphic representation, 95, 96 127\nhuman brain, analogy to the, 67 artificial, 70\u2013 77, 91\ninformation flows, 68, 70 changing parameters effect on\nlearning functions, 10 behavior, 82\u2013 91\nlearning nonlinear mapping, 79 defined, 76, 254\nmatrix representation, 95, 96, 98 feedforward network, 92\nmodeling relationships, 78\u2013 79 function, 8, 56\nneurons in complex models, hidden layers, 69\n56\u2013 57 human brain, 65\u2013 67\nparameters, 82\u2013 83, 99\u2013 100 information processing, 70\npower of, 67, 79 input- output mapping, 8, 70\u2013 71\nschematic, 10 parameters, 82\nsimple, topological illustration, 68 receptive fields, 134\u2013 137, 162\nsize, growth in, 97\u2013 98 sensing, 68, 70\nstructure, 8\u2013 9, 67\u2013 68 sequence of operations, 71\u2013 77\nIndEX 277 Neurons (cont.) Perceptron training model, 105\u2013 113,\nstructure, 65\u2013 66 116\nthreshold functions, 62 Permissive bias, 20\nweight- output relation, 82 Personal data protections, 245\nNeurons, two- input \u03c6 symbol, 72, 94\ndecision boundaries, 84, 85, 90, Picasso problem, 237\u2013 238\n91 Pitts, Walter, 103\u2013 105\ninput space, 84, 85, 86 Planar models, 44\nloan decision model equivalence, Pooling function, 166, 168\u2013 172,\n84 238\u2013 239\nNilsson, N. J., 102 Positive linear activation function,\nNoise in data, 20 73\nNonlinear activation function, 165 Preference bias, 19\u2013 20\nNonlinear models, 77\u2013 78 Pretraining, term use, 146\nNVIDIA, 154 Privacy rights, 37, 245\nProblems, ill- posed, 16\u2013 17\nOh, K.- S., 153 Problem solving, neural networks,\nOlah, Chis, 246 79\nOptimization algorithm, 197 PyTorch, 241\nOR function, 117\u2013 118, 133\nOrganization of Behavior, The (Hebb), Quantum computing, 244\n104 Qubit, 244\nOutput gate, 177\u2013 178 Quetelet, Adolphe, 33\nOutput vector, 180\u2013 181\nOverfitting, 20, 22, 254 Reasoning, inductive, 17\nReceptive field, 134\u2013 137, 162\u2013 168\nParallel Distributed Processing Recital 69, 245\n(PDP), 125 120\u2013 124, 126 Rectified linear activation function,\nPapert, Seymour, 117, 119\u2013 122 73, 74, 165\u2013 166\nPerceptron Rectified linear units (ReLUs), 80,\nin deep learning history, 103 255\nmultilayer, 124 Rectifier activation function, 79, 80\nsingle layer, limitations of, 117, Recurrent neural network (RNN)\n119, 122\u2013 123 constructing a, 180\nPerceptron convergence theorem, in deep learning history, 133\u2013 143\n112 defined, 254\nPerceptron learning rule, 185 depth, 171\u2013 172\nPerceptrons (Minsky & Papert), functions, 170\n116\u2013 117 hidden layers, 170\u2013 177\n278 IndEX information flows, 171, 173 Support vector machines (SVMs),\nlayer connections, 175\u2013 176 143\nmemory buffer, 170\u2013 177\nstructure, 173 Tangle Lake chip, 244\nunrolled through time, 174 Tanh activation function, 73, 74, 76,\nvanishing gradient problem in, 79, 127, 150\u2013 151\n141, 175 Tanh layer, 180\nReinforcement learning, 29\u2013 31, Tan units, 179\u2013 180\n254 Target attributes, 27\u2013 28, 255\nRepresentation learning, 132 Templates, 18\u2013 19\nRepresentations, localist vs. TensorFlow, 241\ndistributed, 129\u2013 133 Threshold activation function,\nResNet, 170, 233, 237 73\u2013 75, 78\u2013 80, 83, 127\nRestriction bias, 19 Threshold logic units, 103\u2013 105\nRobot control, 30 Training model, 12\u2013 14, 31, 82\nRosenblatt, Frank, 106\u2013 113, 116 Transfer learning, 236\u2013 237\nRumelhart, D. E., 125 Transformer model, 239\u2013 240\nTrueNorth chip (IBM), 243\u2013 244\nSaliency, 247 T- SNE, 248\nSchmidhuber, J\u00fcrgen, 127, 141 Tuning phase, 145\u2013 147\nSedol, Lee, 2 Two- input neurons. See Neurons,\nSentence generation, 139\u2013 140, two- input\n181\u2013 182 Two- stage backpropagation\nseq2seq, 103, 181 algorithm, 126, 210\u2013 215\nseq2seq architecture, 142\nSequential data, 170 Underfitting, 22, 77\u2013 78, 255\nSimple cells, 135\u2013 136 Units, 79. See also Neurons\nSimplicity, 19 Unsupervised learning, 28\u2013 30, 233,\nSkip- connections, 170 237, 255\nSlope parameter, 43, 188\u2013 189 Update vector, 179\nSpam filtering, 15, 21\nSpeech recognition, 1, 15 Vanishing gradient problem\nSpiking neurons, 241\u2013 242 in deep learning history, 103,\nSteinkraus, D., 154 125\u2013 129, 143\n\u2211 symbol, 45, 72 defined, 129, 255\nSum of squared errors (SSE), Elman network, 139\n190\u2013 192, 193, 194\u2013 203 LSTM networks, 177\nSupervised learning, 27\u2013 30, overcoming the, 147\u2013 148\n232\u2013 233, 255 in RNNs, 141, 176\nIndEX 279 Variables in models, 40\u2013 41 Widrow- Hoff learning rule, 114, 204\nVectors, 62, 86\u2013 88 Wiesel, T. N., 134\u2013 137\nVery- large- scale integrated (VLSI) Williams, R. J., 125\ncircuit, 241 word2vec models, 180\u2013 181\nVirtuous cycle, 153\u2013 155\nVisual cortex experiments, 134\u2013 136 XOR function, 103, 119, 133\nVisual feature detection XOR problem, 103, 116\u2013 123\nCNNs for, 160\u2013 163, 168\u2013 169, 238\nspatially invariant, 136\ntransfer learning for, 236\nVisual feature detection function,\n15, 236, 238\nVisual feature detection software,\n15, 23, 35, 156\nVisualization techniques, 246\u2013 248\nWeight adjustment\nactivation functions and, 207\nbackpropagation algorithm,\n126\u2013 127, 222\u2013 230\ncredit assignment problem, 123,\n210\u2013 211\nWeight initialization, 148, 153, 155\nWeighted sum, 46, 47, 48, 61\u2013 64, 71\nWeighted sum calculations\nbias term in, 88\nneural networks, 80\u2013 82\nin a neuron, 82\nneuron layer, 92\u2013 97\nWeighted summation function, 98\nWeights\nerror gradients, adjusting, 209,\n211\nupdating, 53\u2013 54\nWeight space, 58\u2013 60, 192, 193, 194\nWeight update rule, 197\u2013 208\nWeight update strategy, 108\u2013 112\nWeight vector, 86\u2013 89\nWidrow, Bernard, 113\u2013 114, 116\n280 IndEX   The MIT Press Essential Knowledge Series\nAuctions, Timothy P. Hubbard and Harry J. Paarsch\nThe Book, Amaranth Borsuk\nCarbon Capture, Howard J. Herzog\nCloud Computing, Nayan B. Ruparelia\nComputational Thinking, Peter J. Denning and Matti Tedre\nComputing: A Concise History, Paul E. Ceruzzi\nThe Conscious Mind, Zoltan E. Torey\nCrowdsourcing, Daren C. Brabham\nData Science, John D. Kelleher and Brendan Tierney\nDeep Learning, John D. Kelleher\nExtremism, J. M. Berger\nFood, Fabio Parasecoli\nFree Will, Mark Balaguer\nThe Future, Nick Montfort\nGPS, Paul E. Ceruzzi\nHaptics, Lynette A. Jones\nInformation and Society, Michael Buckland\nInformation and the Modern Corporation, James W. Cortada\nIntellectual Property Strategy, John Palfrey\nThe Internet of Things, Samuel Greengard\nMachine Learning: The New AI, Ethem Alpaydin\nMachine Translation, Thierry Poibeau\nMemes in Digital Culture, Limor Shifman\nMetadata, Jeffrey Pomerantz\nThe Mind\u2013 Body Problem, Jonathan Westphal\nMOOCs, Jonathan Haber\nNeuroplasticity, Moheb Costandi\nNihilism, Nolen Gertz\nOpen Access, Peter Suber\nParadox, Margaret Cuonzo\nPhoto Authentication, Hany Farid\nPost-T ruth, Lee McIntyre\nRobots, John Jordan\nSchool Choice, David R. Garcia\nSelf- Tracking, Gina Neff and Dawn Nafus\nSexual Consent, Milena Popova\nSpaceflight, Michael J. Neufeld\nSustainability, Kent E. Portney\nSynesthesia, Richard E. Cytowic\nThe Technological Singularity, Murray Shanahan\n3D Printing, John Jordan\nUnderstanding Beliefs, Nils J. Nilsson\nWaves, Frederic Raichlen John d. kELLEhER is a Professor of Computer Science and the Academic\nLeader of the Information, Communication and Entertainment (ICE) research\ninstitute at the Technological University Dublin (TU Dublin). He has over\ntwenty years\u2019 experience in research and teaching in the fields of artificial\nintelligence, natural language processing, and machine learning. He has pub-\nlished more than a hundred academic articles in these fields, and two MIT\nPress books: Data Science (2018) and Fundamentals of Machine Learning for Pre-\ndictive Data Analytics (2015). His research is supported by the ADAPT Research\nCentre (https://www.adaptcentre.ie), which is funded by Science Foundation\nIreland (Grant 13/RC/2106) and is co- funded by the European Regional De-\nvelopment fund, and by PRECISE4Q project (https://precise4q.eu), which is\nfunded through the European Union\u2019s Horizon 2020 research and innovation\nprogram under grant agreement No. 777107.",
    "data/library/DS_MIT.txt": "  DATA SCIENCE The MIT Press Essential Knowledge Series\nAuctions, Timothy P. Hubbard and Harry J. Paarsch\nThe Book, Amaranth Borsuk\nCloud Computing, Nayan Ruparelia\nComputing: A Concise History, Paul E. Ceruzzi\nThe Conscious Mind, Zoltan L. Torey\nCrowdsourcing, Daren C. Brabham\nData Science, John D. Kelleher and Brendan Tierney\nFree Will, Mark Balaguer\nThe Future, Nick Montfort\nInformation and Society, Michael Buckland\nInformation and the Modern Corporation, James W. Cortada\nIntellectual Property Strategy, John Palfrey\nThe Internet of Things, Samuel Greengard\nMachine Learning: The New AI, Ethem Alpaydin\nMachine Translation, Thierry Poibeau\nMemes in Digital Culture, Limor Shifman\nMetadata, Jeffrey Pomerantz\nThe Mind\u2013Body Problem, Jonathan Westphal\nMOOCs, Jonathan Haber\nNeuroplasticity, Moheb Costandi\nOpen Access, Peter Suber\nParadox, Margaret Cuonzo\nPost-Truth, Lee McIntyre\nRobots, John Jordan\nSelf-Tracking, Gina Neff and Dawn Nafus\nSustainability, Kent E. Portney\nSynesthesia, Richard E. Cytowic\nThe Technological Singularity, Murray Shanahan\nUnderstanding Beliefs, Nils J. Nilsson\nWaves, Frederic Raichlen DATA SCIENCE\nJOHN D. KELLEHER\nAND BRENDAN TIERNEY\nThe MIT Press | Cambridge, Massachusetts | London, England \u00a9 2018 Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by\nany electronic or mechanical means (including photocopying, recording, or\ninformation storage and retrieval) without permission in writing from the\npublisher.\nThis book was set in Chaparral Pro by Toppan Best-set Premedia Limited.\nPrinted and bound in the United States of America.\nLibrary of Congress Cataloging-in-Publication Data\nNames: Kelleher, John D., 1974- author. | Tierney, Brendan, 1970- author.\nTitle: Data science / John D. Kelleher and Brendan Tierney.\nDescription: Cambridge, MA : The MIT Press, [2018] | Series: The MIT Press\nessential knowledge series | Includes bibliographical references and index.\nIdentifiers: LCCN 2017043665 | ISBN 9780262535434 (pbk. : alk. paper)\nSubjects: LCSH: Big data. | Machine learning. | Data mining. | Quantitative\nresearch.\nClassification: LCC QA76.9.B45 K45 2018 | DDC 005.7--dc23 LC record\navailable at https://lccn.loc.gov/2017043665\n10 9 8 7 6 5 4 3 2 1 CONTENTS\nSeries Foreword vii\nPreface ix\nAcknowledgments xiii\n1 What Is Data Science? 1\n2 What Are Data, and What Is a Data Set? 39\n3 A Data Science Ecosystem 69\n4 Machine Learning 101 97\n5 Standard Data Science Tasks 151\n6 Privacy and Ethics 181\n7 Future Trends and Principles of Success 219\nGlossary 239\nNotes 247\nFurther Readings 251\nReferences 253\nIndex 261  SERIES FOREWORD\nThe MIT Press Essential Knowledge series offers acces-\nsible, concise, beautifully produced pocket-size books on\ntopics of current interest. Written by leading thinkers, the\nbooks in this series deliver expert overviews of subjects\nthat range from the cultural and the historical to the sci-\nentific and the technical.\nIn today\u2019s era of instant information gratification, we\nhave ready access to opinions, rationalizations, and super-\nficial descriptions. Much harder to come by is the founda-\ntional knowledge that informs a principled understanding\nof the world. Essential Knowledge books fill that need.\nSynthesizing specialized subject matter for nonspecialists\nand engaging critical topics through fundamentals, each\nof these compact volumes offers readers a point of access\nto complex ideas.\nBruce Tidor\nProfessor of Biological Engineering and Computer Science\nMassachusetts Institute of Technology  PREFACE\nThe goal of data science is to improve decision making by\nbasing decisions on insights extracted from large data sets.\nAs a field of activity, data science encompasses a set of\nprinciples, problem definitions, algorithms, and processes\nfor extracting nonobvious and useful patterns from large\ndata sets. It is closely related to the fields of data mining\nand machine learning, but it is broader in scope. Today,\ndata science drives decision making in nearly all parts of\nmodern societies. Some of the ways that data science may\naffect your daily life include determining which advertise-\nments are presented to you online; which movies, books,\nand friend connections are recommended to you; which\nemails are filtered into your spam folder; what offers you\nreceive when you renew your cell phone service; the cost of\nyour health insurance premium; the sequencing and tim-\ning of traffic lights in your area; how the drugs you may\nneed were designed; and which locations in your city the\npolice are targeting.\nThe growth in use of data science across our societies\nis driven by the emergence of big data and social media,\nthe speedup in computing power, the massive reduction\nin the cost of computer memory, and the development of\nmore powerful methods for data analysis and modeling,\nsuch as deep learning. Together these factors mean that it has never been easier for organizations to gather, store,\nand process data. At the same time, these technical inno-\nvations and the broader application of data science means\nthat the ethical challenges related to the use of data and\nindividual privacy have never been more pressing. The aim\nof this book is to provide an introduction to data science\nthat covers the essential elements of the field at a depth\nthat provides a principled understanding of the field.\nChapter 1 introduces the field of data science and pro-\nvides a brief history of how it has developed and evolved.\nIt also examines why data science is important today and\nsome of the factors that are driving its adoption. The\nchapter finishes by reviewing and debunking some of the\nmyths associated with data science. Chapter 2 introduces\nfundamental concepts relating to data. It also describes\nthe standard stages in a data science project: business un-\nderstanding, data understanding, data preparation, mod-\neling, evaluation, and deployment. Chapter 3 focuses on\ndata infrastructure and the challenges posed by big data\nand the integration of data from multiple sources. One\naspect of a typical data infrastructure that can be chal-\nlenging is that data in databases and data warehouses of-\nten reside on servers different from the servers used for\ndata analysis. As a consequence, when large data sets are\nhandled, a surprisingly large amount of time can be spent\nmoving data between the servers a database or data ware-\nhouse are living on and the servers used for data analysis\nx Preface and machine learning. Chapter 3 begins by describing a\ntypical data science infrastructure for an organization and\nsome of the emerging solutions to the challenge of mov-\ning large data sets within a data infrastructure, which in-\nclude the use of in-database machine learning, the use of\nHadoop for data storage and processing, and the develop-\nment of hybrid database systems that seamlessly combine\ntraditional database software and Hadoop-like solutions.\nThe chapter concludes by highlighting some of the chal-\nlenges in integrating data from across an organization into\na unified representation that is suitable for machine learn-\ning. Chapter 4 introduces the field of machine learning\nand explains some of the most popular machine-learning\nalgorithms and models, including neural networks, deep\nlearning, and decision-tree models. Chapter 5 focuses on\nlinking machine-learning expertise with real-world prob-\nlems by reviewing a range of standard business problems\nand describing how they can be solved by machine-learning\nsolutions. Chapter 6 reviews the ethical implications of\ndata science, recent developments in data regulation,\nand some of the new computational approaches to pre-\nserving the privacy of individuals within the data science\nprocess. Finally, chapter 7 describes some of the areas\nwhere data science will have a significant impact in the\nnear future and sets out some of the principles that are\nimportant in determining whether a data science project\nwill succeed.\nPreface xi  ACKNOWLEDGMENTS\nJohn and Brendan thank Paul McElroy and Brian Leahy for\nreading and commenting on early drafts. They also thank\nthe two anonymous reviewers who provided detailed and\nhelpful feedback on the manuscript and the staff at the\nMIT Press for their support and guidance.\nJohn thanks his family and friends for their sup-\nport and encouragement during the preparation of this\nbook and dedicates this book to his father, John Bernard\nKelleher, in recognition of his love and friendship.\nBrendan thanks Grace, Daniel, and Eleanor for their\nconstant support while he was writing yet another book\n(his fourth), juggling the day jobs, and traveling.  1\nWHAT IS DATA SCIENCE?\nData science encompasses a set of principles, problem\ndefinitions, algorithms, and processes for extracting non-\nobvious and useful patterns from large data sets. Many\nof the elements of data science have been developed in\nrelated fields such as machine learning and data mining.\nIn fact, the terms data science, machine learning, and data\nmining are often used interchangeably. The commonality\nacross these disciplines is a focus on improving decision\nmaking through the analysis of data. However, although\ndata science borrows from these other fields, it is broader\nin scope. Machine learning (ML) focuses on the design\nand evaluation of algorithms for extracting patterns from\ndata. Data mining generally deals with the analysis of\nstructured data and often implies an emphasis on com-\nmercial applications. Data science takes all of these consid-\nerations into account but also takes up other challenges, such as the capturing, cleaning, and transforming of\nunstructured social media and web data; the use of big-\ndata technologies to store and process big, unstructured\ndata sets; and questions related to data ethics and\nregulation.\nUsing data science, we can extract different types of\npatterns. For example, we might want to extract patterns\nthat help us to identify groups of customers exhibiting\nsimilar behavior and tastes. In business jargon, this task\nis known as customer segmentation, and in data science\nterminology it is called clustering. Alternatively, we might\nwant to extract a pattern that identifies products that are\nfrequently bought together, a process called association-\nrule mining. Or we might want to extract patterns that\nidentify strange or abnormal events, such as fraudulent\ninsurance claims, a process known as anomaly or outlier\ndetection. Finally, we might want to identify patterns that\nhelp us to classify things. For example, the following rule\nillustrates what a classification pattern extracted from\nan email data set might look like: If an email contains the\nphrase \u201cMake money easily,\u201d it is likely to be a spam email.\nIdentifying these types of classification rules is known as\nprediction. The word prediction might seem an odd choice\nbecause the rule doesn\u2019t predict what will happen in the\nfuture: the email already is or isn\u2019t a spam email. So it\nis best to think of prediction patterns as predicting the\nmissing value of an attribute rather than as predicting\n2 Chapter 1 If a human expert can\neasily create a pattern\nin his or her own\nmind, it is generally\nnot worth the time and\neffort of using data\nscience to \u201cdiscover\u201d it. the future. In this example, we are predicting whether the\nemail classification attribute should have the value \u201cspam\u201d\nor not.\nAlthough we can use data science to extract differ-\nent types of patterns, we always want the patterns to be\nboth nonobvious and useful. The example email classifica-\ntion rule given in the previous paragraph is so simple and\nobvious that if it were the only rule extracted by a data\nscience process, we would be disappointed. For example,\nthis email classification rule checks only one attribute of\nan email: Does the email contain the phrase \u201cmake money\neasily\u201d? If a human expert can easily create a pattern in\nhis or her own mind, it is generally not worth the time\nand effort of using data science to \u201cdiscover\u201d it. In general,\ndata science becomes useful when we have a large number\nof data examples and when the patterns are too complex\nfor humans to discover and extract manually. As a lower\nbound, we can take a large number of data examples to\nbe defined as more than a human expert can check easily.\nWith regard to the complexity of the patterns, again, we\ncan define it relative to human abilities. We humans are\nreasonably good at defining rules that check one, two, or\neven three attributes (also commonly referred to as fea-\ntures or variables), but when we go higher than three attri-\nbutes, we can start to struggle to handle the interactions\nbetween them. By contrast, data science is often applied in\ncontexts where we want to look for patterns among tens,\n4 Chapter 1 hundreds, thousands, and, in extreme cases, millions of\nattributes.\nThe patterns that we extract using data science are\nuseful only if they give us insight into the problem that\nenables us to do something to help solve the problem. The\nphrase actionable insight is sometimes used in this context\nto describe what we want the extracted patterns to give us.\nThe term insight highlights that the pattern should give\nus relevant information about the problem that isn\u2019t ob-\nvious. The term actionable highlights that the insight we\nget should also be something that we have the capacity to\nuse in some way. For example, imagine we are working for\na cell phone company that is trying to solve a customer\nchurn problem\u2014that is, too many customers are switching\nto other companies. One way data science might be used to\naddress this problem is to extract patterns from the data\nabout previous customers that allow us to identify current\ncustomers who are churn risks and then contact these cus-\ntomers and try to persuade them to stay with us. A pattern\nthat enables us to identify likely churn customers is useful\nto us only if (a) the patterns identify the customers early\nenough that we have enough time to contact them before\nthey churn and (b) our company is able to put a team in\nplace to contact them. Both of these things are required in\norder for the company to be able to act on the insight the\npatterns give us.\nWhat Is Data sCIenCe? 5 A Brief History of Data Science\nThe term data science has a specific history dating back\nto the 1990s. However, the fields that it draws upon have\na much longer history. One thread in this longer history\nis the history of data collection; another is the history of\ndata analysis. In this section, we review the main develop-\nments in these threads and describe how and why they\nconverged into the field of data science. Of necessity, this\nreview introduces new terminology as we describe and\nname the important technical innovations as they arose.\nFor each new term, we provide a brief explanation of its\nmeaning; we return to many of these terms later in the\nbook and provide a more detailed explanation of them.\nWe begin with a history of data collection, then give a his-\ntory of data analysis, and, finally, cover the development\nof data science.\nA History of Data Gathering\nThe earliest methods for recording data may have been\nnotches on sticks to mark the passing of the days or poles\nstuck in the ground to mark sunrise on the solstices. With\nthe development of writing, however, our ability to re-\ncord our experiences and the events in our world vastly\nincreased the amount of data we collected. The earliest\nform of writing developed in Mesopotamia around 3200\nBC and was used for commercial record keeping. This type\n6 Chapter 1 of record keeping captures what is known as transactional\ndata. Transactional data include event information such\nas the sale of an item, the issuing of an invoice, the deliv-\nery of goods, credit card payment, insurance claims, and\nso on. Nontransactional data, such as demographic data,\nalso have a long history. The earliest-known censuses took\nplace in pharaonic Egypt around 3000 BC. The reason that\nearly states put so much effort and resources into large\ndata-collection operations was that these states needed to\nraise taxes and armies, thus proving Benjamin Franklin\u2019s\nclaim that there are only two things certain in life: death\nand taxes.\nIn the past 150 years, the development of the elec-\ntronic sensor, the digitization of data, and the invention\nof the computer have contributed to a massive increase\nin the amount of data that are collected and stored. A\nmilestone in data collection and storage occurred in 1970\nwhen Edgar F. Codd published a paper explaining the re-\nlational data model, which was revolutionary in terms of\nsetting out how data were (at the time) stored, indexed,\nand retrieved from databases. The relational data model\nenabled users to extract data from a database using simple\nqueries that defined what data the user wanted without\nrequiring the user to worry about the underlying structure\nof the data or where they were physically stored. Codd\u2019s\npaper provided the foundation for modern databases and\nthe development of structured query language (SQL), an\nWhat Is Data sCIenCe? 7 international standard for defining database queries. Re-\nlational databases store data in tables with a structure of\none row per instance and one column per attribute. This\nstructure is ideal for storing data because it can be decom-\nposed into natural attributes.\nDatabases are the natural technology to use for storing\nand retrieving structured transactional or operational data\n(i.e., the type of data generated by a company\u2019s day-to-day\noperations). However, as companies have become larger\nand more automated, the amount and variety of data\ngenerated by different parts of these companies have dra-\nmatically increased. In the 1990s, companies realized that\nalthough they were accumulating tremendous amounts\nof data, they were repeatedly running into difficulties in\nanalyzing those data. Part of the problem was that the\ndata were often stored in numerous separate databases\nwithin the one organization. Another difficulty was that\ndatabases were optimized for storage and retrieval of data,\nactivities characterized by high volumes of simple opera-\ntions, such as SELECT, INSERT, UPDATE, and DELETE. In\norder to analyze their data, these companies needed tech-\nnology that was able to bring together and reconcile the\ndata from disparate databases and that facilitated more\ncomplex analytical data operations. This business chal-\nlenge led to the development of data warehouses. In a data\nwarehouse, data are taken from across the organization\n8 Chapter 1 and integrated, thereby providing a more comprehensive\ndata set for analysis.\nOver the past couple of decades, our devices have be-\ncome mobile and networked, and many of us now spend\nmany hours online every day using social technologies,\ncomputer games, media platforms, and web search en-\ngines. These changes in technology and how we live have\nhad a dramatic impact on the amount of data collected.\nIt is estimated that the amount of data collected over the\nfive millennia since the invention of writing up to 2003\nis about 5 exabytes. Since 2013, humans generate and\nstore this same amount of data every day. However, it is\nnot only the amount of data collected that has grown dra-\nmatically but also the variety of data. Just consider the\nfollowing list of online data sources: emails, blogs, pho-\ntos, tweets, likes, shares, web searches, video uploads,\nonline purchases, podcasts. And if we consider the meta-\ndata (data describing the structure and properties of the\nraw data) of these events, we can begin to understand the\nmeaning of the term big data. Big data are often defined in\nterms of the three Vs: the extreme volume of data, the va-\nriety of the data types, and the velocity at which the data\nmust be processed.\nThe advent of big data has driven the development\nof a range of new database technologies. This new gen-\neration of databases is often referred to as \u201cNoSQL da-\ntabases.\u201d They typically have a simpler data model than\nWhat Is Data sCIenCe? 9 traditional relational databases. A NoSQL database stores\ndata as objects with attributes, using an object notation\nlanguage such as the JavaScript Object Notation (JSON).\nThe advantage of using an object representation of data\n(in contrast to a relational table-based model) is that the\nset of attributes for each object is encapsulated within the\nobject, which results in a flexible representation. For ex-\nample, it may be that one of the objects in the database,\ncompared to other objects, has only a subset of attributes.\nBy contrast, in the standard tabular data structure used\nby a relational database, all the data points should have\nthe same set of attributes (i.e., columns). This flexibility in\nobject representation is important in contexts where the\ndata cannot (due to variety or type) naturally be decom-\nposed into a set of structured attributes. For example, it\ncan be difficult to define the set of attributes that should\nbe used to represent free text (such as tweets) or images.\nHowever, although this representational flexibility allows\nus to capture and store data in a variety of formats, these\ndata still have to be extracted into a structured format be-\nfore any analysis can be performed on them.\nThe existence of big data has also led to the develop-\nment of new data-processing frameworks. When you are\ndealing with large volumes of data at high speeds, it can\nbe useful from a computational and speed perspective to\ndistribute the data across multiple servers, process que-\nries by calculating partial results of a query on each server,\n10 Chapter 1 and then merge these results to generate the response to\nthe query. This is the approach taken by the MapReduce\nframework on Hadoop. In the MapReduce framework, the\ndata and queries are mapped onto (or distributed across)\nmultiple servers, and the partial results calculated on each\nserver are then reduced (merged) together.\nA History of Data Analysis\nStatistics is the branch of science that deals with the col-\nlection and analysis of data. The term statistics originally\nreferred to the collection and analysis of data about the\nstate, such as demographics data or economic data. How-\never, over time the type of data that statistical analysis\nwas applied to broadened so that today statistics is used\nto analyze all types of data. The simplest form of statisti-\ncal analysis of data is the summarization of a data set in\nterms of summary (descriptive) statistics (including mea-\nsures of a central tendency, such as the arithmetic mean, or\nmeasures of variation, such as the range). However, in the\nseventeenth and eighteenth centuries the work of people\nsuch as Gerolamo Cardano, Blaise Pascal, Jakob Bernoulli,\nAbraham de Moivre, Thomas Bayes, and Richard Price\nlaid the foundations of probability theory, and through\nthe nineteenth century many statisticians began to use\nprobability distributions as part of their analytic tool kit.\nThese new developments in mathematics enabled statis-\nticians to move beyond descriptive statistics and to start\nWhat Is Data sCIenCe? 11 doing statistical learning. Pierre Simon de Laplace and Carl\nFriedrich Gauss are two of the most important and famous\nnineteenth-century mathematicians, and both made im-\nportant contributions to statistical learning and modern\ndata science. Laplace took the intuitions of Thomas Bayes\nand Richard Price and developed them into the first ver-\nsion of what we now call Bayes\u2019 Rule. Gauss, in his search\nfor the missing dwarf planet Ceres, developed the method\nof least squares, which enables us to find the best model\nthat fits a data set such that the error in the fit minimizes\nthe sum of squared differences between the data points in\nthe data set and the model. The method of least squares\nprovided the foundation for statistical learning methods\nsuch as linear regression and logistic regression as well as the\ndevelopment of artificial neural network models in artifi-\ncial intelligence (we will return to least squares, regression\nanalysis, and neural networks in chapter 4).\nBetween 1780 and 1820, around the same time that\nLaplace and Gauss were making their contributions to\nstatistical learning, a Scottish engineer named William\nPlayfair was inventing statistical graphics and laying the\nfoundations for modern data visualization and exploratory\ndata analysis. Playfair invented the line chart and area chart\nfor time-series data, the bar chart to illustrate compari-\nsons between quantities of different categories, and the\npie chart to illustrate proportions within a set. The advan-\ntage of visualizing quantitative data is that it allows us to\n12 Chapter 1 use our powerful visual abilities to summarize, compare,\nand interpret data. Admittedly, it is difficult to visualize\nlarge (many data points) or complex (many attributes)\ndata sets, but data visualization is still an important part\nof data science. In particular, it is useful in helping data sci-\nentists explore and understand the data they are working\nwith. Visualizations can also be useful to communicate the\nresults of a data science project. Since Playfair\u2019s time, the\nvariety of data-visualization graphics has steadily grown,\nand today there is research ongoing into the development\nof novel approaches to visualize large, multidimensional\ndata sets. A recent development is the t-distributed stochas-\ntic neighbor embedding (t-SNE) algorithm, which is a use-\nful technique for reducing high-dimensional data down to\ntwo or three dimensions, thereby facilitating the visualiza-\ntion of those data.\nThe developments in probability theory and statis-\ntics continued into the twentieth century. Karl Pearson\ndeveloped modern hypothesis testing, and R. A. Fisher\ndeveloped statistical methods for multivariate analysis\nand introduced the idea of maximum likelihood estimate\ninto statistical inference as a method to draw conclusions\nbased on the relative probability of events. The work of\nAlan Turing in the Second World War led to the inven-\ntion of the electronic computer, which had a dramatic\nimpact on statistics because it enabled much more com-\nplex statistical calculations. Throughout the 1940s and\nWhat Is Data sCIenCe? 13 subsequent decades, a number of important computa-\ntional models were developed that are still widely used in\ndata science. In 1943, Warren McCulloch and Walter Pitts\nproposed the first mathematical model of a neural net-\nwork. In 1948, Claude Shannon published \u201cA Mathemati-\ncal Theory of Communication\u201d and by doing so founded\ninformation theory. In 1951, Evelyn Fix and Joseph Hodges\nproposed a model for discriminatory analysis (what would\nnow be called a classification or pattern-recognition prob-\nlem) that became the basis for modern nearest-neighbor\nmodels. These postwar developments culminated in 1956\nin the establishment of the field of artificial intelligence\nat a workshop in Dartmouth College. Even at this early\nstage in the development of artificial intelligence, the\nterm machine learning was beginning to be used to de-\nscribe programs that gave a computer the ability to learn\nfrom data. In the mid-1960s, three important contribu-\ntions to ML were made. In 1965, Nils Nilsson\u2019s book titled\nLearning Machines showed how neural networks could be\nused to learn linear models for classification. The follow-\ning year, 1966, Earl B. Hunt, Janet Marin, and Philip J.\nStone developed the concept-learning system frame-\nwork, which was the progenitor of an important family\nof ML algorithms that induced decision-tree models from\ndata in a top-down fashion. Around the same time, a\nnumber of independent researchers developed and pub-\nlished early versions of the k-means clustering algorithm,\n14 Chapter 1 now the standard algorithm used for data (customer)\nsegmentation.\nThe field of ML is at the core of modern data science\nbecause it provides algorithms that are able to automati-\ncally analyze large data sets to extract potentially interest-\ning and useful patterns. Machine learning has continued\nto develop and innovate right up to the present day. Some\nof the most important developments include ensemble\nmodels, where predictions are made using a set (or com-\nmittee) of models, with each model voting on each query,\nand deep-learning neural networks, which have multiple\n(i.e., more than three) layers of neurons. These deeper lay-\ners in the network are able to discover and learn complex\nattribute representations (composed of multiple, interact-\ning input attributes that have been processed by earlier\nlayers), which in turn enable the network to learn patterns\nthat generalize across the input data. Because of their abil-\nity to learn complex attributes, deep-learning networks\nare particularly suitable to high-dimensional data and so\nhave revolutionized a number of fields, including machine\nvision and natural-language processing.\nAs we discussed in our review of database history, the\nearly 1970s marked the beginning of modern database\ntechnology with Edgar F. Codd\u2019s relational data model and\nthe subsequent explosion of data generation and storage\nthat led to the development of data warehousing in the\n1990s and more recently to the phenomenon of big data.\nWhat Is Data sCIenCe? 15 However, well before the emergence of big data, in fact by\nthe late 1980s and early 1990s, the need for a field of re-\nsearch specifically targeting the analysis of these large data\nsets was apparent. It was around this time that the term\ndata mining started to be used in the database communi-\nties. As we have already discussed, one response to this\nneed was the development of data warehouses. However,\nother database researchers responded by reaching out\nto other research fields, and in 1989 Gregory Piatetsky-\nShapiro organized the first workshop on knowledge dis-\ncovery in databases (KDD). The announcement of the first\nKDD workshop neatly sums how the workshop focused on\na multidisciplinary approach to the problem of analyzing\nlarge databases:\nKnowledge discovery in databases poses many\ninteresting problems, especially when databases\nare large. Such databases are usually accompanied\nby substantial domain knowledge which can\nsignificantly facilitate discovery. Access to large\ndatabases is expensive\u2014hence the need for sampling\nand other statistical methods. Finally, knowledge\ndiscovery in databases can benefit from many\navailable tools and techniques from several different\nfields including expert systems, machine learning,\nintelligent databases, knowledge acquisition, and\nstatistics.1\n16 Chapter 1 In fact, the terms knowledge discovery in databases and\ndata mining describe the same concept, the distinction\nbeing that data mining is more prevalent in the business\ncommunities and KDD more prevalent in academic com-\nmunities. Today, these terms are often used interchange-\nably,2 and many of the top academic venues use both.\nIndeed, the premier academic conference in the field is the\nInternational Conference on Knowledge Discovery and\nData Mining.\nThe Emergence and Evolution of Data Science\nThe term data science came to prominence in the late 1990s\nin discussions relating to the need for statisticians to join\nwith computer scientists to bring mathematical rigor to\nthe computational analysis of large data sets. In 1997,\nC. F. Jeff Wu\u2019s public lecture \u201cStatistics = Data Science?\u201d\nhighlighted a number of promising trends for statistics,\nincluding the availability of large/complex data sets in\nmassive databases and the growing use of computational\nalgorithms and models. He concluded the lecture by call-\ning for statistics to be renamed \u201cdata science.\u201d\nIn 2001, William S. Cleveland published an action plan\nfor creating a university department in the field of data\nscience (Cleveland 2001). The plan emphasizes the need\nfor data science to be a partnership between mathematics\nand computer science. It also emphasizes the need for data\nscience to be understood as a multidisciplinary endeavor\nWhat Is Data sCIenCe? 17 and for data scientists to learn how to work and engage\nwith subject-matter experts. In the same year, Leo Brei-\nman published \u201cStatistical Modeling: The Two Cultures\u201d\n(2001). In this paper, Breiman characterizes the tradi-\ntional approach to statistics as a data-modeling culture\nthat views the primary goal of data analysis as identifying\nthe (hidden) stochastic data model (e.g., linear regression)\nthat explains how the data were generated. He contrasts\nthis culture with the algorithmic-modeling culture that\nfocuses on using computer algorithms to create prediction\nmodels that are accurate (rather than explanatory in terms\nof how the data was generated). Breiman\u2019s distinction be-\ntween a statistical focus on models that explain the data\nversus an algorithmic focus on models that can accurately\npredict the data highlights a core difference between stat-\nisticians and ML researchers. The debate between these\napproaches is still ongoing within statistics (see, for ex-\nample, Shmueli 2010). In general, today most data science\nprojects are more aligned with the ML approach of build-\ning accurate prediction models and less concerned with\nthe statistical focus on explaining the data. So although\ndata science became prominent in discussions relating\nto statistics and still borrows methods and models from\nstatistics, it has over time developed its own distinct ap-\nproach to data analysis.\nSince 2001, the concept of data science has broad-\nened well beyond that of a redefinition of statistics. For\n18 Chapter 1 example, over the past 10 years there has been a tremen-\ndous growth in the amount of the data generated by online\nactivity (online retail, social media, and online entertain-\nment). Gathering and preparing these data for use in data\nscience projects has resulted in the need for data scientists\nto develop the programming and hacking skills to scrape,\nmerge, and clean data (sometimes unstructured data)\nfrom external web sources. Also, the emergence of big data\nhas meant that data scientists need to be able to work with\nbig-data technologies, such as Hadoop. In fact, today the\nrole of a data scientist has become so broad that there is an\nongoing debate regarding how to define the expertise and\nskills required to carry out this role.3 It is, however, pos-\nsible to list the expertise and skills that most people would\nagree are relevant to the role, which are shown in figure 1.\nIt is difficult for an individual to master all of these areas,\nand, indeed, most data scientists usually have in-depth\nknowledge and real expertise in just a subset of them.\nHowever, it is important to understand and be aware of\neach area\u2019s contribution to a data science project.\nData scientists should have some domain exper-\ntise. Most data science projects begin with a real-world,\ndomain-specific problem and the need to design a data-\ndriven solution to this problem. As a result, it is important\nfor a data scientist to have enough domain expertise that\nthey understand the problem, why it is important, and\nhow a data science solution to the problem might fit into\nWhat Is Data sCIenCe? 19 Figure 1 A skills-set desideratum for a data scientist.\n20 Chapter 1 an organization\u2019s processes. This domain expertise guides\nthe data scientist as she works toward identifying an op-\ntimized solution. It also enables her to engage with real\ndomain experts in a meaningful way so that she can illicit\nand understand relevant knowledge about the underly-\ning problem. Also, having some experience of the project\ndomain allows the data scientist to bring her experiences\nfrom working on similar projects in the same and related\ndomains to bear on defining the project focus and scope.\nData are at the center of all data science projects.\nHowever, the fact that an organization has access to data\ndoes not mean that it can legally or should ethically use\nthe data. In most jurisdictions, there is antidiscrimina-\ntion and personal-data-protection legislation that regu-\nlates and controls the use of data usage. As a result, a data\nscientist needs to understand these regulations and also,\nmore broadly, to have an ethical understanding of the im-\nplications of his work if he is to use data legally and ap-\npropriately. We return to this topic in chapter 6, where we\ndiscuss the legal regulations on data usage and the ethical\nquestions related to data science.\nIn most organizations, a significant portion of the\ndata will come from the databases in the organization.\nFurthermore, as the data architecture of an organization\ngrows, data science projects will start incorporating data\nfrom a variety of other data sources, which are commonly\nreferred to as \u201cbig-data sources.\u201d The data in these data\nWhat Is Data sCIenCe? 21 sources can exist in a variety of different formats, gener-\nally a database of some form\u2014relational, NoSQL, or Ha-\ndoop. All of the data in these various databases and data\nsources will need to be integrated, cleansed, transformed,\nnormalized, and so on. These tasks go by many names,\nsuch as extraction, transformation, and load, \u201cdata mung-\ning,\u201d \u201cdata wrangling,\u201d \u201cdata fusion,\u201d \u201cdata crunching,\u201d and\nso on. Like source data, the data generated from data sci-\nence activities also need to be stored and managed. Again,\na database is the typical storage location for the data gen-\nerated by these activities because they can then be easily\ndistributed and shared with different parts of the organi-\nzation. As a consequence, data scientists need to have the\nskills to interface with and manipulate data in databases.\nA range of computer science skills and tools allows\ndata scientists to work with big data and to process it\ninto new, meaningful information. High-performance\ncomputing (HPC) involves aggregating computing power\nto deliver higher performance than one can get from a\nstand-alone computer. Many data science projects work\nwith a very large data set and ML algorithms that are com-\nputationally expensive. In these situations, having the\nskills required to access and use HPC resources is impor-\ntant. Beyond HPC, we have already mentioned the need\nfor data scientists to be able to scrap, clean, and integrate\nweb data as well as handle and process unstructured text\nand images. Furthermore, a data scientist may also end up\n22 Chapter 1 writing in-house applications to perform a specific task or\naltering an existing application to tune it to the data and\ndomain being processed. Finally, computer science skills\nare also required to be able to understand and develop the\nML models and integrate them into the production or ana-\nlytic or back-end applications in an organization.\nPresenting data in a graphical format makes it much\neasier to see and understand what is happening with the\ndata. Data visualization applies to all phases of the data\nscience process. When data are inspected in tabular form,\nit is easy to miss things such as outliers or trends in dis-\ntributions or subtle changes in the data through time.\nHowever, when data are presented in the correct graphical\nform, these aspects of the data can pop out. Data visualiza-\ntion is an important and growing field, and we recommend\ntwo books, The Visual Display of Quantitative Information\nby Edward Tufte (2001) and Show Me the Numbers: Design-\ning Tables and Graphs to Enlighten by Stephen Few (2012)\nas excellent introductions to the principles and techniques\nof effective data visualization.\nMethods from statistics and probability are used\nthroughout the data science process, from the initial\ngathering and investigation of the data right through\nto the comparing of the results of different models and\nanalyses produced during the project. Machine learning\ninvolves using a variety of advanced statistical and com-\nputing techniques to process data to find patterns. The\nWhat Is Data sCIenCe? 23 data scientist who is involved in the applied aspects of\nML does not have to write his own versions of ML algo-\nrithms. By understanding the ML algorithms, what they\ncan be used for, what the results they generate mean, and\nwhat type of data particular algorithms can be run on, the\ndata scientist can consider the ML algorithms as a gray\nbox. This allows him to concentrate on the applied aspects\nof data science and to test the various ML algorithms to\nsee which ones work best for the scenario and data he is\nconcerned with.\nFinally, a key aspect of being a successful data scien-\ntist is being able to communicate the story in the data.\nThis story might uncover the insight that the analysis of\nthe data has revealed or how the models created during a\nproject fit into an organization\u2019s processes and the likely\nimpact they will have on the organization\u2019s functioning.\nThere is no point executing a brilliant data science proj-\nect unless the outputs from it are used and the results are\ncommunicated in such a way that colleagues with a non-\ntechnical background can understand them and have con-\nfidence in them.\nWhere Is Data Science Used?\nData science drives decision making in nearly all parts of\nmodern societies. In this section, we describe three case\n24 Chapter 1 studies that illustrate the impact of data science: consumer\ncompanies using data science for sales and marketing;\ngovernments using data science to improve health, crimi-\nnal justice, and urban planning; and professional sporting\nfranchises using data science in player recruitment.\nData Science in Sales and Marketing\nWalmart has access to large data sets about its customers\u2019\npreferences by using point-of-sale systems, by tracking\ncustomer behavior on the Walmart website, and by track-\ning social media commentary about Walmart and its prod-\nucts. For more than a decade, Walmart has been using data\nscience to optimize the stock levels in stores, a well-known\nexample being when it restocked strawberry Pop-Tarts\nin stores in the path of Hurricane Francis in 2004 based\non an analysis of sales data preceding Hurricane Char-\nley, which had struck a few weeks earlier. More recently,\nWalmart has used data science to drive its retail revenues\nin terms of introducing new products based on analyzing\nsocial media trends, analyzing credit card activity to make\nproduct recommendations to customers, and optimizing\nand personalizing customers\u2019 online experience on the\nWalmart website. Walmart attributes an increase of 10 to\n15 percent in online sales to data science optimizations\n(DeZyre 2015).\nThe equivalent of up-selling and cross-selling in the\nonline world is the \u201crecommender system.\u201d If you have\nWhat Is Data sCIenCe? 25 watched a movie on Netflix or purchased an item on Ama-\nzon, you will know that these websites use the data they\ncollect to provide suggestions for what you should watch\nor buy next. These recommender systems can be designed\nto guide you in different ways: some guide you toward\nblockbusters and best sellers, whereas others guide you\ntoward niche items that are specific to your tastes. Chris\nAnderson\u2019s book The Long Tail (2008) argues that as pro-\nduction and distribution get less expensive, markets shift\nfrom selling large amounts of a small number of hit items\nto selling smaller amounts of a larger number of niche\nitems. This trade-off between driving sales of hit or niche\nproducts is a fundamental design decision for a recom-\nmender system and affects the data science algorithms\nused to implement these systems.\nGovernments Using Data Science\nIn recent years, governments have recognized the advan-\ntages of adopting data science. In 2015, for example, the\nUS government appointed Dr. D. J. Patil as the first chief\ndata scientist. Some of the largest data science initiatives\nspearheaded by the US government have been in health.\nData science is at the core of the Cancer Moonshot4 and\nPrecision Medicine Initiatives. The Precision Medicine\nInitiative combines human genome sequencing and\ndata science to design drugs for individual patients. One\npart of the initiative is the All of Us program,5 which is\n26 Chapter 1 gathering environment, lifestyle, and biological data from\nmore than one million volunteers to create the world\u2019s\nbiggest data sets for precision medicine. Data science is\nalso revolutionizing how we organize our cities: it is used\nto track, analyze, and control environmental, energy, and\ntransport systems and to inform long-term urban plan-\nning (Kitchin 2014a). We return to health and smart cit-\nies in chapter 7 when we discuss how data science will\nbecome even more important in our lives over the coming\ndecades.\nThe US government\u2019s Police Data Initiative6 focuses\non using data science to help police departments under-\nstand the needs of their communities. Data science is\nalso being used to predict crime hot spots and recidivism.\nHowever, civil liberty groups have criticized some of the\nuses of data science in criminal justice. In chapter 6, we\ndiscuss the privacy and ethics questions raised by data\nscience, and one of the interesting factors in this discus-\nsion is that the opinions people have in relation to per-\nsonal privacy and data science vary from one domain to\nthe next. Many people who are happy for their personal\ndata to be used for publicly funded medical research have\nvery different opinions when it comes to the use of per-\nsonal data for policing and criminal justice. In chapter 6,\nwe also discuss the use of personal data and data science\nin determining life, health, car, home, and travel insurance\npremiums.\nWhat Is Data sCIenCe? 27 Data Science in Professional Sports\nThe movie Moneyball (Bennett Miller, 2011), starring Brad\nPitt, showcases the growing use of data science in mod-\nern sports. The movie is based on the book of the same\ntitle (Lewis 2004), which tells the true story of how the\nOakland A\u2019s baseball team used data science to improve\nits player recruitment. The team\u2019s management identified\nthat a player\u2019s on-base percentage and slugging percent-\nage statistics were more informative indicators of offen-\nsive success than the statistics traditionally emphasized\nin baseball, such as a player\u2019s batting average. This insight\nenabled the Oakland A\u2019s to recruit a roster of undervalued\nplayers and outperform its budget. The Oakland A\u2019s suc-\ncess with data science has revolutionized baseball, with\nmost other baseball teams now integrating similar data-\ndriven strategies into their recruitment processes.\nThe moneyball story is a very clear example of how\ndata science can give an organization an advantage in a\ncompetitive market space. However, from a pure data sci-\nence perspective perhaps the most important aspect of\nthe moneyball story is that it highlights that sometimes\nthe primary value of data science is the identification of\ninformative attributes. A common belief is that the value\nof data science is in the models created through the pro-\ncess. However, once we know the important attributes\nin a domain, it is very easy to create data-driven models.\nThe key to success is getting the right data and finding\n28 Chapter 1 The key to success is\ngetting the right data\nand finding the right\nattributes. the right attributes. In Freakonomics: A Rogue Economist\nExplores the Hidden Side of Everything, Steven D. Levitt and\nStephen Dubner illustrate the importance of this observa-\ntion across a wide range of problems. As they put it, the\nkey to understanding modern life is \u201cknowing what to\nmeasure and how to measure it\u201d (2009, 14). Using data\nscience, we can uncover the important patterns in a data\nset, and these patterns can reveal the important attributes\nin the domain. The reason why data science is used in\nso many domains is that it doesn\u2019t matter what the prob-\nlem domain is: if the right data are available and the\nproblem can be clearly defined, then data science can help.\nWhy Now?\nA number of factors have contributed to the recent growth\nof data science. As we have already touched upon, the emer-\ngence of big data has been driven by the relative ease with\nwhich organizations can gather data. Be it through point-\nof-sales transaction records, clicks on online platforms,\nsocial media posts, apps on smart phones, or myriad other\nchannels, companies can now build much richer profiles of\nindividual customers. Another factor is the commoditiza-\ntion of data storage with economies of scale, making it less\nexpensive than ever before to store data. There has also\nbeen tremendous growth in computer power. Graphics\n30 Chapter 1 cards and graphical processing units (GPUs) were origi-\nnally developed to do fast graphics rendering for computer\ngames. The distinctive feature of GPUs is that they can\ncarry out fast matrix multiplications. However, matrix\nmultiplications are useful not only for graphics rendering\nbut also for ML. In recent years, GPUs have been adapted\nand optimized for ML use, which has contributed to large\nspeedups in data processing and model training. User-\nfriendly data science tools have also become available and\nlowered the barriers to entry into data science. Taken to-\ngether, these developments mean that it has never been\neasier to collect, store, and process data.\nIn the past 10 years there have also been major ad-\nvances in ML. In particular, deep learning has emerged\nand has revolutionized how computers can process lan-\nguage and image data. The term deep learning describes\na family of neural network models with multiple lay-\ners of units in the network. Neural networks have been\naround since the 1940s, but they work best with large,\ncomplex data sets and take a great deal of computing\nresources to train. So the emergence of deep learning is\nconnected with growth in big data and computing power.\nIt is not an exaggeration to describe the impact of deep\nlearning across a range of domains as nothing less than\nextraordinary.\nDeepMind\u2019s computer program AlphaGo7 is an ex-\ncellent example of how deep learning has transformed a\nWhat Is Data sCIenCe? 31 field of research. Go is a board game that originated in\nChina 3,000 years ago. The rules of Go are much simpler\nthan chess; players take turns placing pieces on a board\nwith the goal of capturing their opponent\u2019s pieces or sur-\nrounding empty territory. However, the simplicity of the\nrules and the fact that Go uses a larger board means that\nthere are many more possible board configurations in Go\nthen there are in chess. In fact, there are more possible\nboard configurations for Go than there are atoms in the\nuniverse. This makes Go much more difficult than chess\nfor computers because of its much larger search space\nand difficulty in evaluating each of these possible board\nconfigurations. The DeepMind team used deep-learning\nmodels to enable AlphaGo to evaluate board configura-\ntions and to select the next move to make. The result was\nthat AlphaGo became the first computer program to beat\na professional Go player, and in March 2016 AlphaGo beat\nLed Sedol, the 18-time Go world champion, in a match\nwatched by more than 200 million people worldwide. To\nput the impact of deep learning on Go in context, as re-\ncently as 2009 the best Go computer program in the world\nwas rated at the low end of advanced amateur; seven\nyears later AlphaGo beat the world champion. In 2016,\nan article describing the deep-learning algorithms behind\nAlphaGo was published in the world\u2019s most prestigious ac-\nademic science journal, Nature (Silver, Huang, Maddison,\net al. 2016).\n32 Chapter 1 Deep learning has also had a massive impact on a\nrange of high-profile consumer technologies. Facebook\nnow uses deep learning for face recognition and to ana-\nlyze text in order to advertise directly to individuals based\non their online conversations. Both Google and Baidu\nuse deep learning for image recognition, captioning and\nsearch, and machine translation. Apple\u2019s virtual assistant\nSiri, Amazon\u2019s Alexa, Microsoft\u2019s Cortana, and Samsung\u2019s\nBixby use speech recognition based on deep learning.\nHuawei is currently developing a virtual assistant for the\nChinese market, and it, too, will use deep-learning speech\nrecognition. In chapter 4, \u201cMachine Learning 101,\u201d we de-\nscribe neural networks and deep learning in more detail.\nHowever, although deep learning is an important techni-\ncal development, perhaps what is most significant about\nit in terms of the growth of data science is the increased\nawareness of the capabilities and benefits of data science\nand organization buy-in that has resulted from these high-\nprofile success stories.\nMyths about Data Science\nData science has many advantages for modern organiza-\ntions, but there is also a great deal of hype around it, so we\nshould understand what its limitations are. One of the big-\ngest myths is the belief that data science is an autonomous\nWhat Is Data sCIenCe? 33 process that we can let loose on our data to find the answers\nto our problems. In reality, data science requires skilled\nhuman oversight throughout the different stages of the\nprocess. Humans analysts are needed to frame the prob-\nlem, to design and prepare the data, to select which ML\nalgorithms are most appropriate, to critically interpret the\nresults of the analysis, and to plan the appropriate action\nto take based on the insight(s) the analysis has revealed.\nWithout skilled human oversight, a data science project\nwill fail to meet its targets. The best data science outcomes\noccur when human expertise and computer power work\ntogether, as Gordon Linoff and Michael Berry put it: \u201cData\nmining lets computers do what they do best\u2014dig through\nlots of data. This, in turn, lets people do what people do\nbest, which is to set up the problem and understand the\nresults\u201d (2011, 3).\nThe widespread and growing use of data science\nmeans that today the biggest data science challenge for\nmany organizations is locating qualified human analysts\nand hiring them. Human talent in data science is at a\npremium, and sourcing this talent is currently the main\nbottleneck in the adoption of data science. To put this\ntalent shortfall in context, in 2011 a McKinsey Global\nInstitute report projected a shortfall in the United States\nof between 140,000 and 190,000 people with data sci-\nence and analytics skills and an even larger shortfall of\n1.5 million managers with the ability to understand data\n34 Chapter 1 science and analytics processes at a level that will enable\nthem to interrogate and interpret the results of data sci-\nence appropriately (Manyika, Chui, Brown, et al. 2011).\nFive years on, in their 2016 report, the institute remained\nconvinced that data science has huge untapped value po-\ntential across an expanding range of applications but that\nthe talent shortfall will remain, with a predicted shortfall\nof 250,000 data scientists in the near term (Henke, Bug-\nhin, Chui, et al. 2016).\nThe second big myth of data science is that every data\nscience project needs big data and needs to use deep learn-\ning. In general, having more data helps, but having the\nright data is the more important requirement. Data sci-\nence projects are frequently carried out in organizations\nthat have significantly less resources in terms of data\nand computing power than Google, Baidu, or Microsoft.\nExamples indicative of the scale of smaller data science\nprojects include claim prediction in an insurance company\nthat processes around 100 claims a month; student drop-\nout prediction for a university with less than 10,000 stu-\ndents; membership dropout prediction for a union with\nseveral thousand members. So an organization doesn\u2019t\nneed to be handling terabytes of data or to have mas-\nsive computing resources at its disposal to benefit from\ndata science.\nA third data science myth is that modern data sci-\nence software is easy to use, and so data science is easy to\nWhat Is Data sCIenCe? 35 do. It is true that data science software has become more\nuser-friendly. However, this ease of use can hide the fact\nthat doing data science properly requires both appropri-\nate domain knowledge and the expertise regarding the\nproperties of the data and the assumptions underpin-\nning the different ML algorithms. In fact, it has never\nbeen easier to do data science badly. Like everything else\nin life, if you don\u2019t understand what you are doing when\nyou do data science, you are going to make mistakes. The\ndanger with data science is that people can be intimidated\nby the technology and believe whatever results the soft-\nware presents to them. They may, however, have unwit-\ntingly framed the problem in the wrong way, entered the\nwrong data, or used analysis techniques with inappro-\npriate assumptions. So the results the software presents\nare likely to be the answer to the wrong question or to\nbe based on the wrong data or the outcome of the wrong\ncalculation.\nThe last myth about data science we want to mention\nhere is the belief that data science pays for itself quickly.\nThe truth of this belief depends on the context of the or-\nganization. Adopting data science can require significant\ninvestment in terms of developing data infrastructure\nand hiring staff with data science expertise. Furthermore,\ndata science will not give positive results on every project.\nSometimes there is no hidden gem of insight in the data,\n36 Chapter 1 and sometimes the organization is not in a position to act\non the insight the analysis has revealed. However, in con-\ntexts where there is a well-understood business problem\nand the appropriate data and human expertise are avail-\nable, then data science can (often) provide the actionable\ninsight that gives an organization the competitive advan-\ntage it needs to succeed.\nWhat Is Data sCIenCe? 37  2\nWHAT ARE DATA,\nAND WHAT IS A DATA SET?\nAs its name suggests, data science is fundamentally de-\npendent on data. In its most basic form, a datum or a piece\nof information is an abstraction of a real-world entity\n(person, object, or event). The terms variable, feature, and\nattribute are often used interchangeably to denote an in-\ndividual abstraction. Each entity is typically described by a\nnumber of attributes. For example, a book might have the\nfollowing attributes: author, title, topic, genre, publisher,\nprice, date published, word count, number of chapters,\nnumber of pages, edition, ISBN, and so on.\nA data set consists of the data relating to a collection\nof entities, with each entity described in terms of a set of\nattributes. In its most basic form,1 a data set is organized\nin an n * m data matrix called the analytics record, where n\nis the number of entities (rows) and m is the number of at-\ntributes (columns). In data science, the terms data set and analytics record are often used interchangeably, with the\nanalytics record being a particular representation of a data\nset. Table 1 illustrates an analytics record for a data set of\nclassic books. Each row in the table describes one book.\nThe terms instance, example, entity, object, case, individual,\nand record are used in data science literature to refer to\na row. So a data set contains a set of instances, and each\ninstance is described by a set of attributes.\nThe construction of the analytics record is a prerequi-\nsite of doing data science. In fact, the majority of the time\nand effort in data science projects is spent on creating,\ncleaning, and updating the analytics record. The analytics\nrecord is often constructed by merging information from\nmany different sources: data may have to be extracted\nfrom multiple databases, data warehouses, or computer\nfiles in different formats (e.g., spreadsheets or csv files) or\nscraped from the web or social media streams.\nTable 1 A Data Set of Classic Books\nID Title Author Year Cover Edition Price\n1 Emma Austen 1815 Paperback 20th $5.75\n2 Dracula Stoker 1897 Hardback 15th $12.00\n3 Ivanhoe Scott 1820 Hardback 8th $25.00\n4 Kidnapped Stevenson 1886 Paperback 11th $5.00\n40 Chapter 2 Four books are listed in the data set in table 1. Ex-\ncluding the ID attribute\u2014which is simply a label for each\nrow and hence is not useful for analysis\u2014each book is\ndescribed using six attributes: title, author, year, cover,\nedition, and price. We could have included many more at-\ntributes for each book, but, as is typical of data science\nprojects, we needed to make a choice when we were de-\nsigning the data set. In this instance, we were constrained\nby the size of the page and the number of attributes we\ncould fit onto it. In most data science projects, however,\nthe constraints relate to what attributes we can actually\ngather and what attributes we believe, based on our do-\nmain knowledge, are relevant to the problem we are trying\nto solve. Including extra attributes in a data set does not\ncome without cost. First, there is the extra time and effort\nin collecting and quality checking the attribute informa-\ntion for each instance in the data set and integrating these\ndata into the analytics record. Second, including irrelevant\nor redundant attributes can have a negative effect on the\nperformance of many of the algorithms used to analyze\ndata. Including many attributes in a data set increases the\nprobability that an algorithm will find irrelevant or spuri-\nous patterns in the data that appear to be statistically sig-\nnificant only because of the particular sample of instances\nin the data set. The problem of how to choose the correct\nattribute(s) is a challenge faced by all data science projects,\nand sometimes it comes down to an iterative process of\nWhat are Data, anD What Is a Data set? 41 trial-and-error experiments where each iteration checks\nthe results achieved using different subsets of attributes.\nThere are many different types of attributes, and for\neach attribute type different sorts of analysis are appro-\npriate. So understanding and recognizing different attri-\nbute types is a fundamental skill for a data scientist. The\nstandard types are numeric, nominal, and ordinal. Numeric\nattributes describe measurable quantities that are repre-\nsented using integer or real values. Numeric attributes can\nbe measured on either an interval scale or a ratio scale. In-\nterval attributes are measured on a scale with a fixed but\narbitrary interval and arbitrary origin\u2014for example, date\nand time measurements. It is appropriate to apply order-\ning and subtraction operations to interval attributes, but\nother arithmetic operations (such as multiplication and\ndivision) are not appropriate. Ratio scales are similar to\ninterval scales, but the scale of measurement possesses a\ntrue-zero origin. A value of zero indicates that none of the\nquantity is being measured. A consequence of a ratio scale\nhaving a true-zero origin is that we can describe a value\non a ratio scale as being a multiple (or ratio) of another\nvalue. Temperature is a useful example for distinguishing\nbetween interval and ratio scales.2 A temperature mea-\nsurement on the Celsius or Fahrenheit scale is an interval\nmeasurement because a 0 value on either of these scales\ndoes not indicate zero heat. So although we can compute\ndifferences between temperatures on these scales and\n42 Chapter 2 compare these differences, we cannot say that a temper-\nature of 20\u00b0 Celsius is twice as warm as 10\u00b0 Celsius. By\ncontrast, a temperature measurement in Kelvins is on a\nratio scale because 0 K (absolute zero) is the temperature\nat which all thermal motion ceases. Other common exam-\nples of ratio-scale measurements include money quanti-\nties, weight, height, and marks on an exam paper (scale\n0\u2013100). In table 1, the \u201cyear\u201d attribute is an example of\nan interval-scale attribute, and the \u201cprice\u201d attribute is an\nexample of a ratio-scale attribute.\nNominal (also known as categorical) attributes take\nvalues from a finite set. These values are names (hence\n\u201cnominal\u201d) for categories, classes, or states of things. Ex-\namples of nominal attributes include marital status (sin-\ngle, married, divorced) and beer type (ale, pale ale, pils,\nporter, stout, etc.). A binary attribute is a special case of\na nominal attribute where the set of possible values is re-\nstricted to just two values. For example, we might have\nthe binary attribute \u201cspam,\u201d which describes whether an\nemail is spam (true) or not spam (false), or the binary at-\ntribute \u201csmoker,\u201d which describes whether an individual is\na smoker (true) or not (false). Nominal attributes cannot\nhave ordering or arithmetic operations applied to them.\nNote that a nominal attribute may be sorted alphabeti-\ncally, but alphabetizing is a distinct operation from order-\ning. In table 1, \u201cauthor\u201d and \u201ctitle\u201d are examples of nominal\nattributes.\nWhat are Data, anD What Is a Data set? 43 Ordinal attributes are similar to nominal attributes,\nwith the difference that it is possible to apply a rank order\nover the categories of ordinal attributes. For example, an at-\ntribute describing the response to a survey question might\ntake values from the domain \u201cstrongly dislike, dislike, neu-\ntral, like, and strongly like.\u201d There is a natural ordering over\nthese values from \u201cstrongly dislike\u201d to \u201cstrongly like\u201d (or\nvice versa depending on the convention being used). How-\never, an important feature of ordinal data is that there is\nno notion of equal distance between these values. For ex-\nample, the cognitive distance between \u201cdislike\u201d and \u201cneu-\ntral\u201d may be different from the distance between \u201clike\u201d and\n\u201cstrongly like.\u201d As a result, it is not appropriate to apply\narithmetic operations (such as averaging) on ordinal at-\ntributes. In table 1, the \u201cedition\u201d attribute is an example of\nan ordinal attribute. The distinction between nominal and\nordinal data is not always clear-cut. For example, consider\nan attribute that describes the weather and that can take\nthe values \u201csunny,\u201d \u201crainy,\u201d \u201covercast.\u201d One person might\nview this attribute as being nominal, with no natural order\nover the values, whereas another person might argue that\nthe attribute is ordinal, with \u201covercast\u201d being treated as\nan intermediate value between \u201csunny\u201d and \u201crainy\u201d (Hall,\nWitten, and Frank 2011).\nThe data type of an attribute (numeric, ordinal, nomi-\nnal) affects the methods we can use to analyze and under-\nstand the data, including both the basic statistics we can\n44 Chapter 2 The data type of an\nattribute (numeric,\nordinal, nominal) affects\nthe methods we can\nuse to analyze and\nunderstand the data. use to describe the distribution of values that an attribute\ntakes and the more complex algorithms we use to iden-\ntify the patterns of relationships between attributes. At\nthe most basic level of analysis, numeric attributes allow\narithmetic operations, and the typical statistical analysis\napplied to numeric attributes is to measure the central\ntendency (using the mean value of the attribute) and the\ndispersion of the attributes values (using the variance or\nstandard deviation statistics). However, it does not make\nsense to apply arithmetic operations to nominal or ordi-\nnal attributes. So the basic analysis of these types of at-\ntributes involves counting the number of times each of the\nvalues occurs in the data set or calculating the proportion\nof occurrence of each value or both.\nData are generated through a process of abstraction,\nso any data are the result of human decisions and choices.\nFor every abstraction, somebody (or some set of people)\nwill have made choices with regard to what to abstract\nfrom and what categories or measurements to use in the\nabstracted representation. The implication is that data are\nnever an objective description of the world. They are in-\nstead always partial and biased. As Alfred Korzybski has\nobserved, \u201cA map is not the territory it represents, but,\nif correct, it has a similar structure to the territory which\naccounts for its usefulness\u201d (1996, 58).\nIn other words, the data we use for data science are\nnot a perfect representation of the real-world entities and\n46 Chapter 2 processes we are trying to understand, but if we are careful\nin how we design and gather the data that we use, then the\nresults of our analysis will provide useful insights into our\nreal-world problems. The moneyball story given in chapter\n1 is a great example of how the determinant of success\nin many data science projects is figuring out the correct\nabstractions (attributes) to use for a given domain. Recall\nthat the key to the moneyball story was that the Oakland\nA\u2019s figured out that a player\u2019s on-base percentage and slug-\nging percentage are better attributes to use to predict a\nplayer\u2019s offensive success than traditional baseball sta-\ntistics such as batting average. Using different attributes\nto describe players gave the Oakland A\u2019s a different and\nbetter model of baseball than the other teams had, which\nenabled it to identify undervalued players and to compete\nwith larger franchises using a smaller budget.\nThe moneyball story illustrates that the old computer\nscience adage \u201cgarbage in, garbage out\u201d is true for data\nscience: if the inputs to a computational process are in-\ncorrect, then the outputs from the process will be incor-\nrect. Indeed, two characteristics of data science cannot\nbe overemphasized: (a) for data science to be successful,\nwe need to pay a great deal of attention to how we create\nour data (in terms of both the choices we make in design-\ning the data abstractions and the quality of the data cap-\ntured by our abstraction processes), and (b) we also need\nto \u201csense check\u201d the results of a data science process\u2014that\nWhat are Data, anD What Is a Data set? 47 is, we need to understand that just because the computer\nidentifies a pattern in the data this doesn\u2019t mean that it is\nidentifying a real insight in the processes we are trying to\nanalyze; the pattern may simply be based on the biases in\nour data design and capture.\nPerspectives on Data\nOther than type of data (numeric, nominal, and ordinal),\na number of other useful distinctions can be made re-\ngarding data. One such distinction is between structured\nand unstructured data. Structured data are data that can\nbe stored in a table, and every instance in the table has\nthe same structure (i.e., set of attributes). As an example,\nconsider the demographic data for a population, where\neach row in the table describes one person and consists of\nthe same set of demographic attributes (name, age, date\nof birth, address, gender, education level, job status, etc.).\nStructured data can be easily stored, organized, searched,\nreordered, and merged with other structured data. It is\nrelatively easy to apply data science to structured data be-\ncause, by definition, it is already in a format that is suit-\nable for integration into an analytics record. Unstructured\ndata are data where each instance in the data set may have\nits own internal structure, and this structure is not neces-\nsarily the same in every instance. For example, imagine a\n48 Chapter 2 data set of webpages, with each webpage having a struc-\nture but this structure differing from one webpage to an-\nother. Unstructured data are much more common than\nstructured data. For example, collections of human text\n(emails, tweets, text messages, posts, novels, etc.) can be\nconsidered unstructured data, as can collections of sound,\nimage, music, video, and multimedia files. The variation in\nthe structure between the different elements means that\nit is difficult to analyze unstructured data in its raw form.\nWe can often extract structured data from unstructured\ndata using techniques from artificial intelligence (such as\nnatural-language processing and ML), digital signal pro-\ncessing, and computer vision. However, implementing\nand testing these data-transformation processes is expen-\nsive and time-consuming and can add significant financial\noverhead and time delays to a data science project.\nSometimes attributes are raw abstractions from an\nevent or object\u2014for example, a person\u2019s height, the num-\nber of words in an email, the temperature in a room, the\ntime or location of an event. But data can also be derived\nfrom other pieces of data. Consider the average salary in\na company or the variance in the temperature of a room\nacross a period of time. In both of these examples, the\nresulting data are derived from an original set of data by\napplying a function to the original raw data (individual sal-\naries or temperature readings). It is frequently the case that\nthe real value of a data science project is the identification\nWhat are Data, anD What Is a Data set? 49 It is frequently the case\nthat the real value of a\ndata science project is\nthe identification of\none or more important\nderived attributes that\nprovide insight into a\nproblem. of one or more important derived attributes that provide\ninsight into a problem. Imagine we are trying to get a bet-\nter understanding of obesity within a population, and we\nare trying to understand the attributes of an individual\nthat identify him as being obese. We would begin by exam-\nining the raw attributes of individuals, such as their height\nand weight, but after studying the problem for some time\nwe might end up designing a more informative derived\nattribute such as the Body Mass Index (BMI). BMI is the\nratio of a person\u2019s mass and height. Recognizing that the\ninteraction between the raw attributes \u201cmass\u201d and \u201cheight\u201d\nprovides more information about obesity then either of\nthese two attributes can when examined independently\nwill help us to identify people in the population who are at\nrisk of obesity. Obviously, BMI is a simple example that we\nuse here to illustrate the importance of derived attributes.\nBut consider situations where the insight into the problem\nis given through multiple derived attributes, where each\nattribute involves two (or potentially more) additional\nattributes. It is in contexts where multiple attributes in-\nteract together that data science provides us with real\nbenefits because the algorithms we use can, in some cases,\nlearn the derived attributes from the raw data.\nThere are generally two terms for gathered raw data:\ncaptured data and exhaust data (Kitchin 2014a). Captured\ndata are collected through a direct measurement or obser-\nvation that is designed to gather the data. For example,\nWhat are Data, anD What Is a Data set? 51 the primary purpose of surveys and experiments is to\ngather specific data on a particular topic of interest. By\ncontrast, exhaust data are a by-product of a process whose\nprimary purpose is something other than data capture.\nFor example, the primary purpose of many social media\ntechnologies is to enable users to connect with other peo-\nple. However, for every image shared, blog posted, tweet\nretweeted, or post liked, a range of exhaust data is gener-\nated: who shared, who viewed, what device was used, what\ntime of day, which device was used, how many people\nviewed/liked/retweeted, and so on. Similarly, the primary\npurpose of the Amazon website is to enable users to make\npurchases from the site. However, each purchase gener-\nates volumes of exhaust data: what items the user put into\nher basket, how long she stayed on the site, what other\nitems she viewed, and so on.\nOne of the most common types of exhaust data is\nmetadata\u2014that is, data that describe other data. When\nEdward Snowden released documents about the US Na-\ntional Security Agency\u2019s surveillance program PRISM, he\nrevealed that the agency was collecting a large amount of\nmetadata about people\u2019s phone calls. This meant that the\nagency was not actually recording the content of peoples\nphone calls (it was not doing wiretapping) but rather col-\nlecting the data about the calls, such as when the call was\nmade, who the recipient was, how long the call lasted, and\nso on (Pomerantz 2015). This type of data gathering may\nnot appear ominous, but the MetaPhone study carried\n52 Chapter 2 out at Stanford highlighted the types of sensitive insights\nthat phone-call metadata can reveal about an individual\n(Mayer and Mutchler 2014). The fact that many organiza-\ntions have very specific purposes makes it relatively easy\nto infer sensitive information about a person based on his\nphone calls to these organizations. For example, some of\nthe people in the MetaPhone study made calls to Alcohol-\nics Anonymous, divorce lawyers, and medical clinics spe-\ncializing in sexually transmitted diseases. Patterns in calls\ncan also be revealing. The pattern analysis from the study\nshowed how patterns of calls reveal potentially very sensi-\ntive information:\nParticipant A communicated with multiple local\nneurology groups, a specialty pharmacy, a rare\ncondition management service, and a hotline for a\npharmaceutical used solely to treat relapsing multiple\nsclerosis. \u2026 In a span of three weeks, Participant D\ncontacted a home improvement store, locksmiths,\na hydroponics dealer, and a head shop. (Mayer and\nMutchler 2014)\nData science has traditionally focused on captured\ndata. However, as the MetaPhone study shows, exhaust\ndata can be used to reveal hidden insight into situations.\nIn recent years, exhaust data have become more and more\nuseful, particularly in the realm of customer engagement,\nwhere the linking of different exhaust data sets has the\nWhat are Data, anD What Is a Data set? 53 potential to provide a business with a richer profile of indi-\nvidual customers, thereby enabling the business to target\nits services and marketing to certain customers. In fact,\none of the factors driving the growth in data science in\nbusiness today is the recognition of the value of exhaust\ndata and the potential that data science has to unlock this\nvalue for businesses.\nData Accumulates, Wisdom Doesn\u2019t!\nThe goal of data science is to use data to get insight and un-\nderstanding. The Bible urges us to attain understanding by\nseeking wisdom: \u201cwisdom is the principal thing, therefore\nget wisdom and with all thy getting get understanding\u201d\n(Proverbs 4:7 [King James]). This advice is reasonable, but\nit does beg the question of how one should go about seek-\ning wisdom. The following lines from T. S. Eliot\u2019s poem\n\u201cChoruses from The Rock\u201d describes a hierarchy of wis-\ndom, knowledge, and information:\nWhere is the wisdom we have lost in knowledge?\nWhere is the knowledge we have lost in information?\n(Eliot 1934, 96)\nEliot\u2019s hierarchy mirrors the standard model of the\nstructural relationships between wisdom, knowledge,\n54 Chapter 2 information, and data known as the DIKW pyramid (see\nfigure 2). In the DIKW pyramid, data precedes informa-\ntion, which precedes knowledge, which precedes wisdom.\nAlthough the order of the layers in the hierarchy are gener-\nally agreed upon, the distinctions between the layers and\nthe processes required to move from one layer to the next\nare often contested. Broadly speaking, however,\nFigure 2 The DIKW pyramid (adapted from Kitchin 2014a).\nWhat are Data, anD What Is a Data set? 55 \u2022 Data are created through abstractions or measurements\ntaken from the world.\n\u2022 Information is data that have been processed, structured,\nor contextualized so that it is meaningful to humans.\n\u2022 Knowledge is information that has been interpreted and\nunderstood by a human so that she can act on it if required.\n\u2022 Wisdom is acting on knowledge in an appropriate way.\nThe activities in the data science process can also be\nrepresented using a similar pyramid hierarchy where the\nwidth of the pyramid represents the amount of data be-\ning processed at each level and where the higher the layer\nin the pyramid, the more informative the results of the\nactivities are for decision making. Figure 3 illustrates the\nhierarchy of data science activities from data capture and\ngeneration through data preprocessing and aggregation,\ndata understanding and exploration, pattern discovery\nand model creation using ML, and decision support using\ndata-driven models deployed in the business context.\nThe CRISP-DM Process\nMany people and companies regularly put forward sug-\ngestions on the best process to follow to climb the data\nscience pyramid. The most commonly used process is the\n56 Chapter 2 Figure 3 Data science pyramid (adapted from Han, Kamber, and Pei 2011).\nCross Industry Standard Process for Data Mining (CRISP-\nDM). In fact, the CRISP-DM has regularly been in the\nnumber-one spot in various industry surveys for a number\nof years. The primary advantage of CRISP-DM, the main\nreason why it is so widely used, is that it is designed to\nbe independent of any software, vendor, or data-analysis\ntechnique.\nWhat are Data, anD What Is a Data set? 57 CRISP-DM was originally developed by a consortium\nof organizations consisting of leading data science ven-\ndors, end users, consultancy companies, and researchers.\nThe original CRISP-DM project was sponsored in part by\nthe European Commission under the ESPRIT Program,\nand the process was first presented at a workshop in\n1999. Since then, a number of attempts have been made\nto update the process, but the original version is still pre-\ndominantly in use. For many years, there was a dedicated\nwebsite for CRISP-DM, but in recent years this website is\nno longer available, and on occasion you might get redi-\nrected to the SPSS website by IBM, which was one of the\noriginal contributors to the project. The original consor-\ntium published a detailed (76-page) but readable step-by-\nstep guide to the process that is freely available online (see\nChapman et al. 1999), but the structure and major tasks of\nthe process can be summarized in a few pages.\nThe CRISP-DM life cycle consists of six stages: busi-\nness understanding, data understanding, data preparation,\nmodeling, evaluation, and deployment, as shown in figure 4.\nData are at the center of all data science activities, and that\nis why the CRISP-DM diagram has data at its center. The\narrows between the stages indicate the typical direction of\nthe process. The process is semistructured, which means\nthat a data scientist doesn\u2019t always move through these\nsix stages in a linear fashion. Depending on the outcome\nof a particular stage, a data scientist may go back to one of\n58 Chapter 2 Business Data\nunderstanding understanding\nData\npreparation\nDeployment Data\nModeling\nEvaluation\nFigure 4 The CRISP-DM life cycle (based on figure 2 in Chapman, Clinton,\nKerber, et al. 1999).\nWhat are Data, anD What Is a Data set? 59 the previous stages, redo the current stage, or move on to\nthe next stage.\nIn the first two stages, business understanding and\ndata understanding, the data scientist is trying to define\nthe goals of the project by understanding the business\nneeds and the data that the business has available to it.\nIn the early stages of a project, a data scientist will often\niterate between focusing on the business and exploring\nwhat data are available. This iteration typically involves\nidentifying a business problem and then exploring if the\nappropriate data are available to develop a data-driven so-\nlution to the problem. If the data are available, the project\ncan proceed; if not, the data scientist will have to identify\nan alternative problem to tackle. During this stage of a\nproject, a data scientist will spend a great deal of time in\nmeetings with colleagues in the business-focused depart-\nments (e.g., sales, marketing, operations) to understand\ntheir problems and with the database administrators to\nget an understanding of what data are available.\nOnce the data scientist has clearly defined a busi-\nness problem and is happy that the appropriate data are\navailable, she moves on to the next phase of the CRISP-\nDM: data preparation. The focus of the data-preparation\nstage is the creation of a data set that can be used for the\ndata analysis. In general, creating this data set involves\nintegrating data sources from a number of databases.\nWhen an organization has a data warehouse, this data\n60 Chapter 2 integration can be relatively straightforward. Once a data\nset has been created, the quality of the data needs to be\nchecked and fixed. Typical data-quality problems include\noutliers and missing values. Checking the quality of the\ndata is very important because errors in the data can have\na serious effect on the performance of the data-analysis\nalgorithms.\nThe next stage of CRISP-DM is the modeling stage.\nThis is the stage where automatic algorithms are used to\nextract useful patterns from the data and to create models\nthat encode these patterns. Machine learning is the field\nof computer science that focuses on the design of these al-\ngorithms. In the modeling stage, a data scientist will nor-\nmally use a number of different ML algorithms to train\na number of different models on the data set. A model is\ntrained on a data set by running an ML algorithm on the\ndata set so as to identify useful patterns in the data and to\nreturn a model that encodes these patterns. In some cases\nan ML algorithm works by fitting a template model struc-\nture to a data set by setting the parameters of the template\nto good values for that data set (e.g., fitting a linear regres-\nsion or neural network model to a data set). In other cases\nan ML algorithm builds a model in a piecewise fashion (e.g.\ngrowing a decision tree one node at a time beginning at the\nroot node of the tree). In most data science projects it is\na model generated by an ML algorithm that is ultimately\nthe software that is deployed by an organization to help it\nWhat are Data, anD What Is a Data set? 61 solve the problem the data science project is addressing.\nEach model is trained by a different type of ML algorithm,\nand each algorithm looks for different types of patterns\nin the data. At this stage in the project, the data scientist\ntypically doesn\u2019t know which patterns are the best ones to\nlook for in the data, so in this context it makes sense to\nexperiment with a number of different algorithms and see\nwhich algorithm returns the most accurate models when\nrun on the data set. In chapter 4 we will introduce ML al-\ngorithms and models in much more detail and explain how\nto create a test plan to evaluate model accuracy.\nIn the majority of data science projects, the initial\nmodel test results will uncover problems in the data. These\ndata errors sometimes come to light when the data scien-\ntist investigates why the performance of a model is lower\nthan expected or notices that maybe the model\u2019s perfor-\nmance is suspiciously good. Or by examining the structure\nof the models, the data scientist may find that the model is\nreliant on attributes that she would not expect, and as a re-\nsult she revisits the data to check that these attributes are\ncorrectly encoded. It is thus not uncommon for a project to\ngo through several rounds of these two stages of the pro-\ncess: modeling, data preparation; modeling, data prepara-\ntion; and so on. For example, Dan Steinberg and his team\nreported that during one data science project, they rebuilt\ntheir data set 10 times over a six-week period, and in week\nfive, having gone through a number of iterations of data\n62 Chapter 2 cleaning and preparation, they uncovered a major error in\nthe data (Steinberg 2013). If this error had not been iden-\ntified and fixed, the project would not have succeeded.\nThe last two stages of the CRISP-DM process, evalu-\nation and deployment, are focused on how the models fit\nthe business and its processes. The tests run during the\nmodeling stage are focused purely on the accuracy of the\nmodels for the data set. The evaluation phase involves\nassessing the models in the broader context defined by\nthe business needs. Does a model meet the business ob-\njectives of the process? Is there any business reason why\na model is inadequate? At this point in the process, it is\nalso useful for the data scientist to do a general quality-\nassurance review on the project activities: Was anything\nmissed? Could anything have been done better? Based on\nthe general assessment of the models, the main decision\nmade during the evaluation phase is whether any of the\nmodels should be deployed in the business or another it-\neration of the CRISP-DM process is required to create ad-\nequate models. Assuming the evaluation process approves\na model or models, the project moves into the final stage\nof the process: deployment. The deployment phase in-\nvolves examining how to deploy the selected models into\nthe business environment. This involves planning how\nto integrate the models into the organization\u2019s techni-\ncal infrastructure and business processes. The best mod-\nels are the ones that fit smoothly into current practices.\nWhat are Data, anD What Is a Data set? 63 Models that fit current practices have a natural set of users\nwho have a clearly defined problem that the model helps\nthem to solve. Another aspect of deployment is putting\na plan in place to periodically review the performance of\nthe model.\nThe outer circle of the CRISP-DM diagram (figure 4)\nhighlights how the whole process is iterative. The itera-\ntive nature of data science projects is perhaps the aspect of\nthese projects that is most often overlooked in discussions\nof data science. After a project has developed and deployed\na model, the model should be regularly reviewed to check\nthat it still fits the business\u2019s needs and that it hasn\u2019t be-\ncome obsolete. There are many reasons why a data-driven\nmodel can become obsolete: the business\u2019s needs might\nhave changed; the process the model emulates and pro-\nvides insight into might have changed (for example, cus-\ntomer behavior changes, spam email changes, etc.); or the\ndata streams the model uses might have changed (for ex-\nample, a sensor that feeds information into a model may\nhave been updated, and the new version of the sensor pro-\nvides slightly different readings, causing the model to be\nless accurate). The frequency of this review is dependent\non how quickly the business ecosystem and the data that\nthe model uses evolve. Constant monitoring is needed to\ndetermine the best time to go through the process again.\nThis is what the outer circle of the CRISP-DM process\nshown in figure 4 represents. For example, depending on\n64 Chapter 2 the data, the business question, and the domain, you may\nhave go through this iterative process on a yearly, quar-\nterly, monthly, weekly, or even daily basis. Figure 5 gives a\nsummary of the different stages of the data science project\nprocess and the major tasks involved in each phase.\nA frequent mistake that many inexperienced data sci-\nentists make is to focus their efforts on the modeling stage\nof the CRISP-DM and to rush through the other stages.\nThey may think that the really important deliverable from\na project is the model, so the data scientist should devote\nmost of his time to building and finessing the model. How-\never, data science veterans will spend more time on ensur-\ning that the project has a clearly defined focus and that it\nhas the right data. For a data science project to succeed, a\ndata scientist needs to have a clear understanding of the\nbusiness need that the project is trying to solve. So the\nbusiness understanding stage of the process is really im-\nportant. With regard to getting the right data for a project,\na survey of data scientists in 2016 found that 79 percent\nof their time is spent on data preparation. The time spent\nacross the major tasks in the project was distributed as fol-\nlows: collecting data sets, 19 percent; cleaning and orga-\nnizing data, 60 percent; building training sets, 3 percent;\nmining data for patterns, 9 percent; refining algorithms,\n4 percent; and performing other tasks, 5 percent (Crowd-\nFlower 2016). The 79 percent figure for preparation comes\nfrom summing the time spent on collecting, cleaning, and\nWhat are Data, anD What Is a Data set? 65 tnemyolpeD niatniam\nyolpeD nalp rotinoM nalp ecudorP lanif troper weiveR tcejorp\ndna\n.)9991\nnoitaulavE etaulavE stluser weiveR ssecorp enimreteD spets\ntxen .la\nte\n,rebreK\n,notnilC\ngniledoM tceleS gniledom euqinhcet etareneG tset ngised dliuB ledom ssessA ledom\n,nampahC\nni\n3\nerugif\nataD noitaraperp tceleS atad naelC atad tcurtsnoC atad etargetnI atad tamroF atad\nno\ndesab(\nsksat\ngnidnatsrednu\ndna\nataD tcelloC laitini atad ebircseD atad erolpxE atad yfireV atad ytilauq\nsegats\nMD-PSIRC\nssenisuB gnidnatsrednu enimreteD ssenisub sevitcejbo ssessA noitautis enimreteD ecneics slaog ecudorP tcejorp nalp ehT\natad 5\nerugiF organizing the data. That around 80 percent of project\ntime is spent on gathering and preparing data has been\na consistent finding in industry surveys for a number of\nyears. Sometimes this finding surprises people because\nthey imagine data scientists spend their time building\ncomplex models to extract insight from the data. But the\nsimple truth is that no matter how good your data analysis\nis, it won\u2019t identify useful patterns unless it is applied to\nthe right data.\nWhat are Data, anD What Is a Data set? 67  3\nA DATA SCIENCE ECOSYSTEM\nThe set of technologies used to do data science varies\nacross organizations. The larger the organization or the\ngreater the amount of data being processed or both,\nthe greater the complexity of the technology ecosystem\nsupporting the data science activities. In most cases, this\necosystem contains tools and components from a number\nof different software suppliers, processing data in many\ndifferent formats. There is a spectrum of approaches from\nwhich an organization can select when developing its own\ndata science ecosystem. At one end of the spectrum, the\norganization may decide to invest in a commercial inte-\ngrated tool set. At the other end, it might build up a be-\nspoke ecosystem by integrating a set of open-source tools\nand languages. In between these two extremes, some\nsoftware suppliers provide solutions that consist of a mix-\nture of commercial products and open-source products. However, although the particular mix of tools will vary\nfrom one organization to the next, there is a commonality\nin terms of the components that are present in most data\nscience architectures.\nFigure 6 gives a high-level overview of a typical data\narchitecture. This architecture is not just for big-data\nenvironments, but for data environments of all sizes. In\nthis diagram, the three main areas consist of data sources,\nwhere all the data in an organization are generated; data\nstorage, where the data are stored and processed; and ap-\nplications, where the data are shared with consumers of\nthese data.\nAll organizations have applications that generate\nand capture data about customers, transactions, and op-\nerational data on everything to do with how the organiza-\ntion operates. Such data sources and applications include\ncustomer management, orders, manufacturing, delivery,\ninvoicing, banking, finance, customer-relationship man-\nagement (CRM), call center, enterprise resource planning\n(ERP) applications, and so on. These types of applications\nare commonly referred to as online transaction processing\n(OLTP) systems. For many data science projects, the data\nfrom these applications will be used to form the initial in-\nput data set for the ML algorithms. Over time, the volume\nof data captured by the various applications in the orga-\nnization grows ever larger and the organization will start\nto branch out to capture data that was ignored, wasn\u2019t\n70 Chapter 3 erugif\nsisylana ataD\n).cte poodah/golb/moc.skrownotroh//:sptth\nsnoitacilppa\ndegakcaP\n,setisbew a yb\nsecruos\nderipsni(\n,sgolbew\natad-giB ecneics\n,rosnes\ndne-kcaB sessecorp\natad\n,laicoS(\nrof\nerutcetihcra\n,3102\nsnoitacilppa .)hcihw-esu-ot-nehw-esuoheraw-atad-eht-dna-\nmotsuC )sesabatad atad-gib ,32\nPPM\nsecruos lirpA\nSDO ,PALO dna ,rettelswen\natad-llams\nlanoitidarT ,PTLO\nesuoheraw\nssenisuB scitylana ataD\nsisylana ,snoitacilppA( skrownotroH\nlacipyt\nSMBDR\nataD\nA\n6 eht\nerugiF\nsnoitacilppA egarots ataD secruos ataD morf\na Data SCienCe eCoS yStem 71 captured previously, or wasn\u2019t available previously. These\nnewer data are commonly referred to as \u201cbig-data sources\u201d\nbecause the volume of data that is captured is significantly\nhigher than the organization\u2019s main operational applica-\ntions. Some of the common big-data sources include\nnetwork traffic, logging data from various applications,\nsensor data, weblog data, social media data, website data,\nand so on. In traditional data sources, the data are typi-\ncally stored in a database. However, because the applica-\ntions associated with many of the newer big-data sources\nare not primarily designed to store data long term\u2014for\nexample, with streaming data\u2014the storage formats and\nstructures for this type of data vary from application to\napplication.\nAs the number of data sources increases, so does the\nchallenge of being able to use these data for analytics\nand for sharing them across the wider organization. The\ndata-storage layer, shown in figure 6, is typically used to\naddress the data sharing and data analytics across an or-\nganization. This layer is divided into two parts. The first\npart covers the typical data-sharing software used by most\norganizations. The most popular form of traditional data-\nintegration and storage software is a relational database\nmanagement system (RDBMS). These traditional sys-\ntems are often the backbone of the business intelligence\n(BI) solutions within an organization. A BI solution is a\nuser-friendly decision-support system that provides data\n72 Chapter 3 aggregating, integration, and reporting as well as analy-\nsis functionality. Depending on the maturity level of a BI\narchitecture, it can consist of anything from a basic copy\nof an operational application to an operational data store\n(ODS) to massively parallel processing (MPP) BI database\nsolutions and data warehouses.\nData warehousing is best understood as a process of\ndata aggregation and analysis with the goal of supporting\ndecision making. However, the focus of this process is the\ncreation of a well-designed and centralized data reposi-\ntory, and the term data warehouse is sometimes used to\ndenote this type of data repository. In this sense, a data\nwarehouse is a powerful resource for data science. From a\ndata science perspective, one of the major advantages of\nhaving a data warehouse in place is a much shorter proj-\nect time. The key ingredient in any data science process\nis data, so it is not surprising that in many data science\nprojects the majority of time and effort goes into find-\ning, aggregating, and cleaning the data prior to their\nanalysis. If a data warehouse is available in a company,\nthen the effort and time that go into data preparation on\nindividual data science projects is often significantly re-\nduced. However, it is possible to do data science without\na centralized data repository. Constructing a centralized\nrepository of data involves more than simply dumping\nthe data from multiple operational databases into a single\ndatabase.\na Data SCienCe eCoS yStem 73 Merging data from multiple databases often requires\nmuch complex manual work to resolve inconsistencies\nbetween the source databases. Extraction, transformation,\nand load (ETL) is the term used to describe the typical proc-\nesses and tools used to support the mapping, merging,\nand movement of data between databases. The typical op-\nerations carried out in a data warehouse are different from\nthe simple operations normally applied to a standard rela-\ntional data model database. The term online analytical pro-\ncessing (OLAP) is used to describe these operations. OLAP\noperations are generally focused on generating summaries\nof historic data and involve aggregating data from mul-\ntiple sources. For example, we might pose the following\nOLAP request (expressed here in English for readability):\n\u201cReport the sales of all stores by region and by quarter and\ncompare these figures to last year\u2019s figures.\u201d What this ex-\nample illustrates is that the result of an OLAP request of-\nten resembles what you would expect to see as a standard\nbusiness report. OLAP operations essentially enable users\nto slice, dice, and pivot the data in the warehouse and get\ndifferent views of these data. They work on a data repre-\nsentation called a data cube that is built on top of the data\nwarehouse. A data cube has a fixed, predefined set of di-\nmensions in which each dimension represents a particular\ncharacteristic of the data. The required data-cube dimen-\nsions for the example OLAP request given earlier would\nbe sales by stores, sales by region, and sales by quarter. The\n74 Chapter 3 primary advantage of using a data cube with a fixed set of\ndimensions is that it speeds up the response time of OLAP\noperations. Also, because the set of data-cube dimensions\nis preprogrammed into the OLAP system, the system can\nprovide user-friendly graphical user interfaces for defining\nOLAP requests. However, the data-cube representation\nalso restricts the types of analysis that can be done using\nOLAP to the set of queries that can be generated using the\npredefined dimensions. By comparison, SQL provides a\nmore flexible query interface. Also, although OLAP sys-\ntems are useful for data exploration and reporting, they\ndon\u2019t enable data modeling or the automatic extraction of\npatterns from the data. Once the data from across an orga-\nnization has been aggregated and analyzed within the BI\nsystem, this analysis can then be used as input to a range\nof consumers in the applications layer of figure 6.\nThe second part of the data-storage layer deals with\nmanaging the data produced by an organization\u2019s big-data\nsources. In this architecture, the Hadoop platform is used\nfor the storage and analytics of these big data. Hadoop\nis an open-source framework developed by the Apache\nSoftware Foundation that is designed for the processing\nof big data. It uses distributed storage and processing\nacross clusters of commodity servers. Applying the Ma-\npReduce programming model, it speeds up the processing\nof queries on large data sets. MapReduce implements the\nsplit-apply-combine strategy: (a) a large data set is split up\na Data SCienCe eCoS yStem 75 into separate chunks, and each chunk is stored on a dif-\nferent node in the cluster; (b) a query is then applied to\nall the chunks in parallel; and (c) the result of the query\nis then calculated by combining the results generated on\nthe different chunks. Over the past couple of years, how-\never, the Hadoop platform is also being used as an exten-\nsion of an enterprise\u2019s data warehouse. Data warehouses\noriginally would store three years of data, but now data\nwarehouses can store more than 10 years of data, and this\nnumber keeps increasing. As the amount of data in a data\nwarehouse increases, however, the storage and processing\nrequirements of the database and server also have to in-\ncrease. This requirement can have a significant cost im-\nplication. An alternative is to move some of the older data\nin a data warehouse for storage into a Hadoop cluster. For\nexample, the data warehouse would store the most recent\ndata, say three years\u2019 worth of data, which frequently need\nto be available for quick analysis and presentation, while\nthe older data and the less frequently used data are stored\non Hadoop. Most of the enterprise-level databases have\nfeatures that connect the data warehouse with Hadoop,\nallowing a data scientist, using SQL, to query the data in\nboth places as if they all are located in one environment.\nHer query could involve accessing some data in the data-\nwarehouse database and some of the data in Hadoop.\nThe query processing will be automatically divided into\ntwo distinct parts, each running independently, and the\n76 Chapter 3 results will be automatically combined and integrated be-\nfore being presented back to the data scientist.\nData analysis is associated with both sections of the\ndata-storage layer in figure 6. Data analysis can occur on\nthe data in each section of the data layer, and the results\nfrom data analysis can be shared between each section\nwhile additional data analysis is being performed. The data\nfrom traditional sources frequently are relatively clean and\ninformation dense compared to the data captured from\nbig-data sources. However, the volume and real-time na-\nture of many big-data sources means that the effort in-\nvolved in preparing and analyzing these big-data sources\ncan be repaid in terms of additional insights not available\nthrough the data coming from traditional sources. A vari-\nety of data-analysis techniques developed across a number\nof different fields of research (including natural-language\nprocessing, computer vision, and ML) can be used to\ntransform unstructured, low-density, low-value big data\ninto high-density and high-value data. These high-value\ndata can then be integrated with the other high-value data\nfrom traditional sources for further data analysis. The de-\nscription given in this chapter and illustrated in figure 6\nis the typical architecture of the data science ecosystem.\nIt is suitable for most organizations, both small and large.\nHowever, as an organization scales in size, so too will the\ncomplexity of its data science ecosystem. For example,\nsmaller-scale organizations may not require the Hadoop\na Data SCienCe eCoS yStem 77 component, but for very large organizations the Hadoop\ncomponent will become very important.\nMoving the Algorithms to the Data\nThe traditional approach to data analysis involves the ex-\ntraction of data from various databases, integrating the\ndata, cleaning the data, subsetting the data, and building\npredictive models. Once the prediction models have been\ncreated they can be applied to the new data. Recall from\nchapter 1 that a prediction model predicts the missing\nvalue of an attribute: a spam filter is a prediction model\nthat predicts whether the classification attribute of an\nemail should have the value of \u201cspam\u201d or not. Applying the\npredictive models to the instances in new data to generate\nthe missing values is known as \u201cscoring the data.\u201d Then\nthe final results, after scoring new data, may be loaded\nback into a database so that these new data can be used\nas part of some workflow, reporting dashboard, or some\nother company assessment practice. Figure 7 illustrates\nthat much of the data processing involved in data prepa-\nration and analysis is located on a server that is separate\nfrom the databases and the data warehouse. Therefore, a\nsignificant amount of time can be spent just moving the\ndata out of the database and moving the results back into\nthe database.\n78 Chapter 3 Database or\nData data warehouse Loading\nextraction results\nAnalytics server\nExtract Integrate Prepare Build models\ndata data data and score data\nTime\nFigure 7 The traditional process for building predictive models and\nscoring data.\nAn experiment run at the Dublin Institute of Tech-\nnology on building a linear-regression model supplies an\nexample of the time involved in each part of the process.\nApproximately 70 to 80 percent of the time is taken with\nextracting and preparing the data; the remaining time is\nspent on building the models. For scoring data, approxi-\nmately 90 percent of the time is taken with extracting\nthe data and saving the scored data set back into the da-\ntabase; only 10 percent of the time is spent on actually\nscoring. These results are based on data sets consisting of\nanywhere from 50,000 records up to 1.5 million records.\na Data SCienCe eCoS yStem 79 A significant amount of\ntime can be spent just\nmoving the data out\nof the database and\nmoving the results back\ninto the database. Most enterprise database vendors have recognized the\ntime savings that would be available if time did not have\nto be spent on moving data and have responded to this\nproblem by incorporating data-analysis functionality and\nML algorithms into their database engines. The following\nsections explore how ML algorithms have been integrated\ninto modern databases, how data storage works in the big-\ndata world of Hadoop, and how using a combination of\nthese two approaches allows organizations to easily work\nwith all their data using SQL as a common language for\naccessing, analyzing, and performing ML and predictive\nanalytics in real time.\nThe Traditional Database or the Modern Traditional\nDatabase\nDatabase vendors continuously invest in developing the\nscalability, performance, security, and functionality of\ntheir databases. Modern databases are far more advanced\nthan traditional relational databases. They can store and\nquery data in variety of different formats. In addition\nto the traditional relational formats, it is also possible\nto define object types, store documents, and store and\nquery JSON objects, spatial data, and so on. Most mod-\nern databases also come with a large number of statisti-\ncal functions, so that some have an equivalent number\nof statistical functions as most statistical applications.\nFor example, the Oracle Database comes with more than\na Data SCienCe eCoS yStem 81 300 different statistical functions and the SQL language\nbuilt into it. These statistical functions cover the major-\nity of the statistical analyses needed by data science proj-\nects and include most if not all the statistical functions\navailable in other tools and languages, such as R. Using\nthe statistical functionality that is available in the data-\nbases in an organization may allow data analytics to be\nperformed in a more efficient and scalable manner using\nSQL. Furthermore, most leading database vendors (in-\ncluding Oracle, Microsoft, IBM, and EnterpriseDB) have\nintegrated many ML algorithms into their databases, and\nthese algorithms can be run using SQL. ML that is built\ninto the database engine and is accessible using SQL is\nknown as in-database machine learning. In-database ML\ncan lead to quicker development of models and quicker\ndeployment of models and results to applications and\nanalytic dashboards. The idea behind the in-database\nML algorithms is captured in the following directive:\n\u201cMove the algorithms to the data instead of the data to the\nalgorithms.\u201d\nThe main advantages of using the in-database ML\nalgorithms are:\n\u2022 No data movement. Some data science products re-\nquire the data to be exported from the database and\nconverted to a specialized format for input to the ML\nalgorithms. With in-database ML, no data movement or\n82 Chapter 3 conversion is needed. This makes the entire process less\ncomplex, less time-consuming, and less error prone.\n\u2022 Faster performance. With analytical operations per-\nformed in the database and with no data movement, it is\npossible to utilize the computing capabilities of the data-\nbase server, delivering performance up to 100 times faster\nthan the traditional approach. Most database servers have\nhigh specifications, with many central processing units\n(CPUs) and efficient memory management to process data\nsets containing more than one billion records.\n\u2022 High security. The database provides controlled and\nauditable access to the data in the database, accelerating\nthe data scientist\u2019s productivity while maintaining data\nsecurity. Also, in-database ML avoids the physical secu-\nrity risks inherent in extracting and downloading data to\nalternative analytics servers. The traditional process, in\ncontrast, results in the creation of many copies (and po-\ntentially different versions) of data sets in separate silos\nacross the organization.\n\u2022 Scalability. A database can easily scale the analytics\nas the data volume increases if the ML algorithms are\nbrought into the database. The database software is de-\nsigned to manage large volumes of data efficiently, utiliz-\ning the multiple CPUs and memory on the server to allow\nthe ML algorithms to run in parallel. Databases are also\nvery efficient at processing large data sets that do not fit\na Data SCienCe eCoS yStem 83 easily into memory. Databases have more than 40 years of\ndevelopment work behind them to ensure that they can\nprocess datasets quickly.\n\u2022 Real-time deployment and environments. The models\nthat are developed using the in-database ML algorithms\ncan be immediately deployed and used in real-time envi-\nronments. This allows the integration of the models into\neveryday applications, providing real-time predictions to\nend users and customers.\n\u2022 Production deployment. Models developed using stand-\nalone ML software may have to be recoded into other\nprogramming languages before they can be deployed\ninto enterprise applications. This is not the case with in-\ndatabase ML. SQL is the language of the database; it can\nbe used and called by any programming language and data\nscience tool. It is then a simple task to incorporate the in-\ndatabase models into production applications.\nMany organizations are exploiting the benefits of\nin-database ML. They range from small and medium or-\nganizations to large, big-data-type organizations. Some\nexamples of organizations that use in-database ML tech-\nnologies are:\n\u2022 Fiserv, an American provider of financial services and\nfraud detection and analysis. Fiserv migrated from using\n84 Chapter 3 multiple vendors for data storage and ML to using just\nthe ML capabilities in its database. By using in-database\nML, the time used for creating/updating and deploying a\nfraud-detection model went from nearly a week to just a\nfew hours.\n\u2022 84.51\u00b0 (formally Dunnhumby USA), a customer science\ncompany. 84.51\u00b0 used many different analytic products to\ncreate its various customer models. It typically would spend\nmore than 318 hours each month moving data from its da-\ntabase to its ML tools and back again, plus an additional 67\nhours a month to create models. When it switched to using\nthe ML algorithms in its database, there was no more need\nfor data movement. The data stayed in the database. The\ncompany immediately saved more than 318 hours of time\nper month. Because it was using its database as a compute\nengine, it was able to scale its analytics, and the time taken\nto generate or update its ML models went from more than\n67 hours to one hour per month. This gave the company\na saving of sixteen days each month. It is now able to get\nsignificantly quicker results and can now provide its cus-\ntomers with results much sooner after they have made a\npurchase.\n\u2022 Wargaming, the creators of World of Tanks and many\nother games. Wargaming uses in-database ML to model\nand predict how to interact with their more than 120 mil-\nlion customers.\na Data SCienCe eCoS yStem 85 Big Data Infrastructure\nAlthough the traditional (modern) database is incredibly\nefficient at processing transactional data, in the age of big\ndata new infrastructure is required to manage all the other\nforms of data and for longer-term storage of the data. The\nmodern traditional database can cope with data volumes\nup to a few petabytes, but for this scale of data, traditional\ndatabase solutions may become prohibitively expensive.\nThis cost issue is commonly referred to as vertical scaling.\nIn the traditional data paradigm, the more data an or-\nganization has to store and process within a reasonable\namount of time, the larger the database server required\nand in turn the greater the cost for server configuration\nand database licensing. Organizations may be able to in-\ngest and query one billion records on a daily/weekly bases\nusing traditional databases, but for this scale of processing\nthey may need to invest more than $100,000 just purchas-\ning the required hardware.\nHadoop is an open-source platform developed and\nreleased by the Apache Software Foundation. It is a well-\nproven platform for ingesting and storing large volumes of\ndata in an efficient manner and can be much less expensive\nthan the traditional database approach. In Hadoop, the\ndata are divided up and partitioned in a variety of ways,\nand these partitions or portions of data are spread across\nthe nodes of the Hadoop cluster. The various analytic tools\nthat work with Hadoop process the data that reside on each\n86 Chapter 3 of the nodes (in some instances these data can be memory\nresident), thus allowing for speedy processing of the data\nbecause the analytics is performed in parallel across the\nnodes. No data extraction or ETL process is needed. The\ndata are analyzed where they are stored.\nAlthough Hadoop is the best known big-data proc-\nessing framework, it is by no means the only one. Other\nbig-data processing frameworks include Storm, Spark, and\nFlink. All of these frameworks are part of the Apache soft-\nware foundation projects. The difference between these\nframeworks lies in the fact that Hadoop is primarily de-\nsigned for batch processing of data. Batch processing is ap-\npropriate where the dataset is static during the processing\nand where the results of the processing are not required\nimmediately (or at least are not particularly time sensi-\ntive). The Storm framework is designed for processing\nstreams of data. In stream processing each element in the\nstream is processed as it enters the system, and conse-\nquently the processing operations are defined to work on\neach individual element in the stream rather than on the\nentire data set. For example, where a batch process might\nreturn an average over a data set of values, a stream process\nwill return an individual label or value for each element in\nthe stream (such as calculating a sentiment score for each\ntweet in a Twitter stream). Storm is designed for real-time\nprocessing of data and according to the Storm website,1 it\nhas been benchmarked at processing over a million tuples\na Data SCienCe eCoS yStem 87 per second per node. Spark and Flink are both hybrid (batch\nand stream) processing frameworks. Spark is a fundamen-\ntally a batch processing framework, similar to Hadoop, but\nalso has some stream processing capabilities whereas Flink\nis a stream processing framework that can also be used\nfor batch processing. Although these big-data processing\nframeworks provide data scientists with a choice of tools\nto meet the specific big-data requirements of their project\nusing these frameworks can have the drawback that the\nmodern data scientist now has to analyze data in two dif-\nferent locations, in the traditional modern databases and\nin the big-data storage. The next section looks at how this\nparticular issue is being addressed.\nThe Hybrid Database World\nIf an organization does not have data of the size and scale\nthat require a Hadoop solution, then it will require only\ntraditional database software to manage its data. How-\never, some of the literature argues that the data-storage\nand processing tools available in the Hadoop world will\nreplace the more traditional databases. It is very difficult\nto see this happening, and more recently there has been\nmuch discussion about having a more balanced approach\nto managing data in what is called the \u201chybrid database\nworld.\u201d The hybrid database world is where traditional da-\ntabases and the Hadoop world coexist.\nIn the hybrid database world, the organization\u2019s da-\ntabases and Hadoop-stored data are connected and work\n88 Chapter 3 The hybrid database\nautomatically balances\nthe location of the data\nbased on the frequency\nof access and the type\nof data science being\nperformed. together, allowing the efficient processing, sharing, and\nanalysis of the data. Figure 8 shows a traditional data\nwarehouse, but instead of all the data being stored in the\ndatabase or the data warehouse, the majority of the data\nis moved to Hadoop. A connection is created between the\ndatabase and Hadoop, which allows the data scientist to\nquery the data as if they all are in one location. The data\nscientist does not need to query the portion of data that\nis in the database warehouse and then in a separate step\nquery the portion that is stored in Hadoop. He can query\nthe data as he always has done, and the solution will iden-\ntify what parts of the query need to be run in each loca-\ntion. The results of the query arrived at in each location\nwill be merged together and presented to him. Similarly,\nas the data warehouse grows, some the older data will not\nbe queried as frequently. The hybrid database solution\nautomatically moves the less frequently used data to the\nHadoop environment and the more frequently used data\nto the warehouse. The hybrid database automatically bal-\nances the location of the data based on the frequency of\naccess and the type of data science being performed.\nOne of the advantages of this hybrid solution is that\nthe data scientist still uses SQL to query the data. He does\nnot have to learn another data-query language or have to\nuse a variety of different tools. Based on current trends,\nthe main database vendors, data-integration solution\nvendors, and all cloud data-storage vendors will have solu-\ntions similar to this hybrid one in the near future.\n90 Chapter 3 Users and applications seamlessly access data\nfrom databases/data warehouses and Hadoop\n10%\nVirtual\n(90%) 90%\nVirtual\n(90%)\nData off-loaded\nand virtualized automatically\nRDBMS\nAnalytics off-loaded\nand results merged\nFigure 8 Databases, data warehousing, and Hadoop working together\n(inspired by a figure in the Gluent data platform white paper, 2017, https://\ngluent.com/wp-content/uploads/2017/09/Gluent-Overview.pdf).\na Data SCienCe eCoS yStem 91 Data Preparation and Integration\nData integration involves taking the data from different\ndata sources and merging them to give a unified view of\nthe data from across the organization. A good example of\nsuch integration occurs with medical records. Ideally, every\nperson would have one health record, and every hospital,\nmedical facility, and general practice would use the same\npatient identifier or same units of measures, the same\ngrading system, and so on. Unfortunately, nearly every\nhospital has its own independent patient-management\nsystem, as does each of the medical labs within the hos-\npital. Think of the challenges in finding a patient\u2019s record\nand assigning the correct results to the correct patient.\nAnd these are the challenges faced by just one hospital. In\nscenarios where multiple hospitals share patient data, the\nproblem of integration becomes significant. It is because\nof these kind of challenges that the first three CRISP-DM\nstages take up to 70 to 80 percent of the total data science\nproject time, with the majority of this time being allocated\nto data integration.\nIntegrating data from multiple data sources is difficult\neven when the data are structured. However, when some\nof the newer big-data sources are involved, where semi- or\nunstructured data are the norm, then the cost of integrat-\ning the data and managing the architecture can become\nsignificant. An illustrative example of the challenges of\n92 Chapter 3 data integration is customer data. Customer data can re-\nside in many different applications (and the applications\u2019\ncorresponding databases). Each application will contain a\nslightly different piece of customer data. For example, the\ninternal data sources might contain the customer credit\nrating, customer sales, payments, call-center contact in-\nformation, and so on. Additional data about the customer\nmay also be available from external data sources. In this\ncontext, creating an integrated view of a customer re-\nquires the data from each of these sources to be extracted\nand integrated.\nThe typical data-integration process will involve a\nnumber of different stages, consisting of extracting, clean-\ning, standardizing, transforming, and finally integrating\nto create a single unified version of the data. Extracting\ndata from multiple data sources can be challenging be-\ncause many data sources can be accessed only by using an\ninterface particular to that data source. As a consequence,\ndata scientists need to have a broad skill set to be able to\ninteract with each of the data sources in order to obtain\nthe data.\nOnce data have been extracted from a data source, the\nquality of the data needs to be checked. Data cleaning is a\nprocess that detects, cleans, or removes corrupt or inaccu-\nrate data from the extracted data. For example, customer\naddress information may have to be cleaned in order to\nconvert it into a standardized format. In addition, there\na Data SCienCe eCoS yStem 93 may be duplicate data in the data sources, in which case it\nis necessary to identify the correct customer record that\nshould be used and to remove all the other records from\nthe data sets. It is important to ensure that the values used\nin a data set are consistent. For example, one source appli-\ncation might use numeric values to represent a customer\ncredit rating, but another might have a mixture of nu-\nmeric and character values. In such a scenario, a decision\nregarding what value to use is needed, and then the other\nrepresentations should be mapped into the standardized\nrepresentation. For example, imagine one of the attri-\nbutes in the data set is a customer\u2019s shoe size. Custom-\ners can buy shoes from various regions around the world,\nbut the numbering system used for shoe sizes in Europe,\nthe United States, the United Kingdom, and other coun-\ntries are slightly different. Prior to doing data analysis and\nmodeling, these data values need to be standardized.\nData transformation involves the changing or com-\nbining of the data from one value to another. A wide vari-\nety of techniques can be used during this step and include\ndata smoothing, binning, and normalization as well as\nwriting custom code to perform a particular transforma-\ntion. A common example of data transformation is with\nprocessing a customer\u2019s age. In many data science tasks,\nprecisely distinguishing between customer ages is not\nparticularly helpful. The difference between a 42-year-\nold customer and a 43-year-old customer is generally not\n94 Chapter 3 significant, although differentiating between a 42-year-\nold customer and a 52-year-old customer may be in-\nformative. As a consequence, a customer\u2019s age is often\ntransformed from a raw age into a general age range. This\nprocess of converting ages into age ranges is an example of\na data-transformation technique called binning. Although\nbinning is relatively straightforward from a technical per-\nspective, the challenge here is to identify the most appro-\npriate range thresholds to apply during binning. Applying\nthe wrong thresholds may obscure important distinctions\nin the data. Finding appropriate thresholds, however, may\nrequire domain specific knowledge or a process of trial-\nand-error experimentation.\nThe final step in data integration involves creating the\ndata that are used as input to the ML algorithms. This data\nis known as the analytics base table.\nCreating the Analytics Base Table\nThe most important step in creating the analytics base\ntable is the selection of the attributes that will be included\nin the analysis. The selection is based on domain knowl-\nedge and on an analysis of the relationships between attri-\nbutes. Consider, for example, a scenario where the analysis\nis focused on customers of a service. In this scenario, some\nof the frequently used domain concepts that will inform\na Data SCienCe eCoS yStem 95 the design and selection of attributes include customer\ncontract details, demographics, usage, changes in usage,\nspecial usage, life-cycle phase, network links, and so on.\nFurthermore, attributes that are found to have a high cor-\nrelation with other attributes are likely to be redundant,\nand so one of the correlated attributes should be excluded.\nRemoving redundant features can result in simpler mod-\nels which are easier to understand, and also reduces the\nlikelihood of an ML algorithm returning a model that is\nfitted to spurious patterns in the data. The set of attri-\nbutes selected for inclusion define what is known as the\nanalytics record. An analytics record typically includes both\nraw and derived attributes. Each instance in the analytics\nbase table is represented by one analytics record, so the set\nof attributes included in the analytics record defines the\nrepresentation of the instances the analysis will be carried\nout on.\nAfter the analytics record has been designed, a set of\nrecords needs to extracted and aggregated to create a data\nset for analysis. When these records have been created and\nstored\u2014for example, in a database\u2014this data set is com-\nmonly referred to as the analytics base table. The analytics\nbase table is the data set that is used as input to the ML\nalgorithms. The next chapter introduces the field of ML\nand describes some of the most popular ML algorithms\nused in data science.\n96 Chapter 3 4\nMACHINE LEARNING 101\nData science is best understood as a partnership between\na data scientist and a computer. In chapter 2, we described\nthe process the data scientist follows: the CRISP-DM life\ncycle. CRISP-DM defines a sequence of decisions the data\nscientist has to make and the activities he should engage in\nto inform and implement these decisions. In CRISP-DM,\nthe major tasks for a data scientist are to define the prob-\nlem, design the data set, prepare the data, decide on the\ntype of data analysis to apply, and evaluate and interpret\nthe results of the data analysis. What the computer brings\nto this partnership is the ability to process data and search\nfor patterns in the data. Machine learning is the field of\nstudy that develops the algorithms that the computers fol-\nlow in order to identify and extract patterns from data. ML\nalgorithms and techniques are applied primarily during the modeling stage of CRISP-DM. ML involves a two-step\nprocess.\nFirst, an ML algorithm is applied to a data set to iden-\ntify useful patterns in the data. These patterns can be\nrepresented in a number of different ways. We describe\nsome popular representations later in this chapter, but\nthey include decision trees, regression models, and neural\nnetworks. These representations of patterns are known\nas \u201cmodels,\u201d which is why this stage of the CRISP-DM life\ncycle is known at the \u201cmodeling stage.\u201d Simply put, ML al-\ngorithms create models from data, and each algorithm is\ndesigned to create models using a particular representa-\ntion (neural network or decision tree or other).\nSecond, once a model has been created, it is used for\nanalysis. In some cases, the structure of the model is what\nis important. A model structure can reveal what the im-\nportant attributes are in a domain. For example, in a medi-\ncal domain we might apply an ML algorithm to a data set\nof stroke patients and use the structure of the model to\nidentify the factors that have a strong association with\nstroke. In other cases, a model is used to label or classify\nnew examples. For instance, the primary purpose of a\nspam-filter model is to label new emails as either spam or\nnot spam rather than to reveal the defining attributes of\nspam email.\n98 Chapter 4 Supervised versus Unsupervised Learning\nThe majority of ML algorithms can be classified as either\nsupervised learning or unsupervised learning. The goal of\nsupervised learning is to learn a function that maps from\nthe values of the attributes describing an instance to the\nvalue of another attribute, known as the target attribute,\nof that instance. For example, when supervised learning\nis used to train a spam filter, the algorithm attempts to\nlearn a function that maps from the attributes describing\nan email to a value (spam/not spam) for the target attri-\nbute; the function the algorithm learns is the spam-filter\nmodel returned by the algorithm. So in this context the\npattern that the algorithm is looking for in the data is the\nfunction that maps from the values of the input attributes\nto the values of the target attribute, and the model that\nthe algorithm returns is a computer program that imple-\nments this function. Supervised learning works by search-\ning through lots of different functions to find the function\nthat best maps between the inputs and output. However,\nfor any data set of reasonable complexity there are so many\ncombinations of inputs and possible mappings to outputs\nthat an algorithm cannot try all possible functions. As a\nconsequence, each ML algorithm is designed to look at or\nprefer certain types of functions during its search. These\npreferences are known as the algorithm\u2019s learning bias. The\nreal challenge in using ML is to find the algorithm whose\nMaChine Learning 101 99 learning bias is the best match for a particular data set.\nGenerally, this task involves experiments with a number\nof different algorithms to find out which one works best\non that data set.\nSupervised learning is \u201csupervised\u201d because each of\nthe instances in the data set lists both the input values\nand the output (target) value for each instance. So the\nlearning algorithm can guide its search for the best func-\ntion by checking how each function it tries matches with\nthe data set, and at the same time the data set acts as a\nsupervisor for the learning process by providing feedback.\nObviously, for supervised learning to take place, each in-\nstance in the data set must be labeled with the value of the\ntarget attribute. Often, however, the reason a target attri-\nbute is interesting is that it is not easy to directly measure,\nand therefore it is not possible to easily create a data set\nof labeled instances. In such scenarios, a great deal of time\nand effort is required to create a data set with the tar-\nget values before a model can be trained using supervised\nlearning.\nIn unsupervised learning, there is no target attribute.\nAs a consequence, unsupervised-learning algorithms can\nbe used without investing the time and effort in labeling\nthe instances of the data set with a target attribute. How-\never, not having a target attribute also means that learn-\ning becomes more difficult: instead of the specific problem\nof searching for a mapping from inputs to output that\n100 Chapter 4 The real challenge in\nusing ML is to find\nthe algorithm whose\nlearning bias is the best\nmatch for a particular\ndata set. matches the data, the algorithm has the more general task\nof looking for regularities in the data. The most common\ntype of unsupervised learning is cluster analysis, where the\nalgorithm looks for clusters of instances that are more sim-\nilar to each other than they are to other instances in the\ndata. These clustering algorithms often begin by guessing\na set of clusters and then iteratively updating the clusters\n(dropping instances from one cluster and adding them to\nanother) so as to increase both the within-cluster similar-\nity and the diversity across clusters.\nA challenge for clustering is figuring out how to mea-\nsure similarity. If all the attributes in a data set are numeric\nand have similar ranges, then it probably makes sense just\nto calculate the Euclidean distance (better known as the\nstraight-line distance) between the instances (or rows).\nRows that are close together in the Euclidean space are\nthen treated as similar. A number of factors, however, can\nmake the calculation of similarity between rows complex.\nIn some data sets, different numeric attributes have dif-\nferent ranges, with the result that a variation in row values\nin one attribute may not be as significant as a variation of\na similar magnitude in another attribute. In these cases,\nthe attributes should be normalized so that they all have\nthe same range. Another complicating factor in calculating\nsimilarity is that things can be deemed similar in many\ndifferent ways. Some attributes are sometimes more im-\nportant than other attributes, so it might make sense to\n102 Chapter 4 weight some attributes in the distance calculations, or it\nmay be that the data set includes nonnumeric data. These\nmore complex scenarios may require the design of bespoke\nsimilarity metrics for the clustering algorithm to use.\nUnsupervised learning can be illustrated with a con-\ncrete example. Imagine we are interested in analyzing the\ncauses of Type 2 diabetes in white American adult males.\nWe would begin by constructing a data set, with each row\nrepresenting one person and each column representing\nan attribute that we believe are relevant for the study. For\nthis example, we will include the following attributes: an\nindividual\u2019s height in meters and weight in kilos, the num-\nber of minutes he exercises per week, his shoe size, and\nthe likelihood that he will develop diabetes expressed as\na percentage based on a number of clinical tests and life-\nstyle surveys. Table 2 illustrates a snippet from this data\nTable 2 Diabetes Study Data Set\nID Height Weight Shoe Size Exercise Diabetes\n(meters) (kilograms) (minutes (% likelihood)\nper week)\n1 1.70 70 5 130 0.05\n2 1.77 88 9 80 0.11\n3 1.85 112 11 0 0.18\n\u2026\nMaChine Learning 101 103 set. Obviously, other attributes could be included\u2014for\nexample, a person\u2019s age\u2014and some attributes could be\nremoved\u2014for example, shoe size, which wouldn\u2019t be par-\nticularly relevant in determining whether someone will\ndevelop diabetes. As we discussed in chapter 2, the choice\nof which attributes to include and exclude from a data set\nis a key task in data science, but for the purposes of this\ndiscussion we will work with the data set as is.\nAn unsupervised clustering algorithm will look for\ngroups of rows that are more similar to each other than\nthey are to the other rows in the data. Each of these groups\nof similar rows defines a cluster of similar instances. For\ninstance, an algorithm can identify causes of a disease or\ndisease comorbidities (diseases that occur together) by\nlooking for attribute values that are relatively frequent\nwithin a cluster. The simple idea of looking for clusters of\nsimilar rows is very powerful and has applications across\nmany areas of life. Another application of clustering rows\nis making product recommendations to customers. If a\ncustomer liked a book, song, or movie, then he may enjoy\nanother book, song, or movie from the same cluster.\nLearning Prediction Models\nPrediction is the task of estimating the value of a target\nattribute for a given instance based on the values of other\n104 Chapter 4 attributes (or input attributes) for that instance. It is the\nproblem that supervised ML algorithms solve: they gener-\nate prediction models. The spam-filter example we used\nto illustrate supervised learning is also applicable here: we\nuse supervised learning to train a spam-filter model, and\nthe spam-filter model is a prediction model. The typical\nuse case of a prediction model is to estimate the target at-\ntribute for new instances that are not in the training data\nset. Continuing our spam example, we train our spam fil-\nter (prediction model) on a data set of old emails and then\nuse the model to predict whether new emails are spam or\nnot spam. Prediction problems are possibly the most pop-\nular type of problem that ML is used for, so the rest of this\nchapter focuses on prediction as the case study for intro-\nducing ML. We begin our introduction to prediction mod-\nels with a concept fundamental to prediction: correlation\nanalysis. Then we explain how supervised ML algorithms\nwork to create different types of popular prediction mod-\nels, including linear-regression models, neural network\nmodels, and decision trees.\nCorrelations Are Not Causations, but Some Are Useful\nA correlation describes the strength of association between\ntwo attributes.1 In a general sense, a correlation can de-\nscribe any type of association between two attributes.\nThe term correlation also has a specific statistical mean-\ning, in which it is often used as shorthand for \u201cPearson\nMaChine Learning 101 105 correlation.\u201d A Pearson correlation measures the strength\nof a linear relationship between two numeric attributes. It\nranges in value from \u22121 to +1. The letter r is used to denote\nthe Pearson value or coefficient between two attributes.\nA coefficient of r = 0 indicates that the two attributes are\nnot correlated. A coefficient of r = +1 indicates that the\ntwo attributes have a perfect positive correlation, mean-\ning that every change in one attribute is accompanied by\nan equivalent change in the other attribute in the same\ndirection. A coefficient of r = \u22121 indicates that the two at-\ntributes have a perfect negative correlation, meaning that\nevery change in one attribute is accompanied by the oppo-\nsite change in the other attribute. The general guidelines\nfor interpreting Pearson coefficients are that a value of r \u2248\n\u00b10.7 indicates a strong linear relationship between the at-\ntributes, r \u2248 \u00b10.5 indicates a moderate linear relationship,\nr \u2248 \u00b10.3 indicates a weak relationship, and r \u2248 0 indicates\nno relationship between the attributes.\nIn the case of the diabetes study, from our knowl-\nedge of how humans are physically made we would expect\nthat there will be relationships between some of the at-\ntributes listed in table 2. For example, it is generally the\ncase that the taller someone is, the larger her shoe size is.\nWe would also expect that the more someone exercises,\nthe lighter she will be, with the caveat that a tall person\nis likely to be heavier than a shorter person who exercises\nthe same amount. We would also expect that there will be\n106 Chapter 4 no obvious relationship between someone\u2019s shoe size and\nthe amount she exercises. Figure 9 presents three scat-\nterplots that illustrate how these intuitions are reflected\nin the data. The scatterplot at the top shows how the data\nspread out if the plotting is based on shoe size and height.\nThere is a clear pattern in this scatterplot: the data go\nfrom the bottom-left corner to the top-right corner, indi-\ncating the relationship that as people get taller (or as we\nmove to the right on the x axis), they also tend to wear\nlarger shoes (we move up on the y axis). A pattern of data\ngenerally going from bottom left to top right in a scat-\nterplot is indicative of a positive correlation between the\ntwo attributes. If we compute the Pearson correlation be-\ntween shoe size and height, the correlation coefficient is\nr = 0.898, indicating a strong positive correlation between\nthis pair of attributes. The middle scatterplot shows how\nthe data spread out when we plot weight and exercise.\nHere the general pattern is in the opposite direction,\nfrom top left to bottom right, indicating a negative cor-\nrelation: the more people exercise, the lighter they are.\nThe Pearson correlation coefficient for this pair of attri-\nbutes is r = \u22120.710, indicating a strong negative correla-\ntion. The final scatterplot, at the bottom, plots exercise\nand shoe size. The data are relatively randomly distrib-\nuted in this plot, and the Pearson correlation coefficient\nfor this pair of attributes is r = \u22120.272, indicating no\nreal correlation.\nMaChine Learning 101 107 Figure 9 Scatterplots of shoe size and height, weight and exercise, and shoe\nsize and exercise. The fact that the definition of a statistical Pearson cor-\nrelation is between two attributes might appear to limit\nthe application of statistical correlation to data analysis\nto just pairs of attributes. Fortunately, however, we can\ncircumvent this problem by using functions over sets of at-\ntributes. In chapter 2, we introduced BMI as a function of\na person\u2019s weight and height. Specifically, it is the ratio of\nhis weight (in kilograms) divided by his height (in meters)\nsquared. BMI was invented in the nineteenth century by\na Belgian mathematician, Adolphe Quetelet, and is used\nto categorize individuals as underweight, normal weight,\noverweight, or obese. The ratio of weight and height is\nused because BMI is designed to have a similar value for\npeople who are in the same category (underweight, nor-\nmal weight, overweight, or obese) irrespective of their\nheight. We know that weight and height are positively cor-\nrelated (generally, the taller someone is, the heavier he is),\nso by dividing weight by height, we control for the effect\nof height on weight. We divide by the square of the height\nbecause people get wider as they get taller, so squaring the\nheight is an attempt to account for a person\u2019s total vol-\nume in the calculation. Two aspects of BMI are interest-\ning for our discussion about correlation between multiple\nattributes. First, BMI is a function that takes a number\nof attributes as input and maps them to a new value. In\neffect, this mapping creates a new derived (as opposed to\nraw) attribute in the data. Second, because a person\u2019s BMI\nMaChine Learning 101 109 is a single numeric value, we can calculate the correlation\nbetween it and other attributes.\nIn our case study of the causes of Type 2 diabetes in\nwhite American adult males, we are interested in iden-\ntifying if any of the attributes have a strong correlation\nwith the target attribute describing a person\u2019s likelihood\nof developing diabetes. Figure 10 presents three more\nscatterplots, each plotting the correlation between the\ntarget attribute (diabetes) and another attribute: height,\nweight, and BMI. In the scatterplot of height and diabetes,\nthere doesn\u2019t appear to be any particular pattern in the\ndata indicating that there is no real correlation between\nthese two attributes (the Pearson coefficient is r = \u22120.277).\nThe middle scatterplot shows the distribution of the data\nplotted using weight and diabetes. The spread of the data\nindicates a positive correlation between these two attri-\nbutes: the more someone weighs, the more likely she is to\ndevelop diabetes (the Pearson coefficient is r = 0.655). The\nbottom scatterplot shows the data set plotted using BMI\nand diabetes. The pattern in this scatterplot is similar to\nthe middle scatterplot: the data spread from bottom left\nto top right, indicating a positive correlation. In this scat-\nterplot, however, the instances are more tightly packed\ntogether, indicating that the correlation between BMI and\ndiabetes is stronger than the correlation between weight\nand diabetes. In fact, the Pearson coefficient for diabetes\nand BMI for this data set is r = 0.877.\n110 Chapter 4 Figure 10 Scatterplots of the likelihood of diabetes with respect to height,\nweight, and BMI. The BMI example illustrates that it is possible to cre-\nate a new derived attribute by defining a function that\ntakes multiple attributes as input. It also shows that it\nis possible to calculate a Pearson correlation between\nthis derived attribute and another attribute in the data\nset. Furthermore, a derived attribute can actually have a\nhigher correlation with a target attribute than any of the\nattributes used to generate the derived attribute have\nwith the target. One way of understanding why BMI has a\nhigher correlation with the diabetes attribute compared to\nthe correlation for either height or weight is that the likeli-\nhood of someone developing diabetes is dependent on the\ninteraction between height and weight, and the BMI at-\ntribute models this interaction appropriately for diabetes.\nClinicians are interested in people\u2019s BMI because it gives\nthem more information about the likelihood of someone\ndeveloping Type 2 diabetes than either just the person\u2019s\nheight or just his weight does independently.\nWe have already noted that attribute selection is a key\ntask in data science. So is attribute design. Designing a\nderived attribute that has a strong correlation with an at-\ntribute we are interested in is often where the real value of\ndata science is found. Once you know the correct attributes\nto use to represent the data, you are able to build accurate\nmodels relatively quickly. Uncovering and designing the\nright attributes is the difficult part. In the case of BMI, a\nhuman designed this derived attribute in the nineteenth\n112 Chapter 4 century. However, ML algorithms can learn interactions\nbetween attributes and create useful derived attributes\nby searching through different combinations of attributes\nand checking the correlation between these combinations\nand the target attribute. This is why ML is useful in con-\ntexts where many weak interacting attributes contribute\nto the process we are trying to understand.\nIdentifying an attribute (raw or derived) that has a\nhigh correlation with a target attribute is useful because\nthe correlated attribute may give us insight into the pro-\ncess that causes the phenomenon the target attribute rep-\nresents: the fact that BMI is strongly correlated with the\nlikelihood of a person\u2019s developing diabetes indicates that\nit is not weight by itself that contributes to a person\u2019s de-\nveloping diabetes but whether that person is overweight.\nAlso, if an input attribute is highly correlated with a target\nattribute, it is likely to be a useful input into the prediction\nmodel. Similar to correlation analysis, prediction involves\nanalyzing the relationships between attributes. In order to\nbe able to map from the values of a set of input attributes\nto a target attribute, there must be a correlation between\nthe input attributes (or some derived function over them)\nand the target attribute. If this correlation does not exist\n(or cannot be found by the algorithm), then the input at-\ntributes are irrelevant for the prediction problem, and the\nbest a model can do is to ignore those inputs and always\npredict the central tendency of that target2 in the data set.\nMaChine Learning 101 113 Conversely, if a strong correlation does exist between in-\nput attributes and the target, then it is likely that an ML\nalgorithm will be able to generate a very accurate predic-\ntion model.\nLinear Regression\nWhen a data set is composed of numeric attributes, then\nprediction models based on regression are frequently\nused. Regression analysis estimates the expected (or aver-\nage) value of a numeric target attribute when all the input\nattributes are fixed. The first step in a regression analysis\nis to hypothesize the structure of the relationship between\nthe input attributes and the target. Then a parameterized\nmathematical model of the hypothesized relationship is\ndefined. This parameterized model is called a regression\nfunction. You can think of a regression function as a ma-\nchine that converts inputs to an output value and of the\nparameters as the settings that control the behavior of a\nmachine. A regression function may have multiple param-\neters, and the focus of regression analysis is to find the\ncorrect settings for these parameters.\nIt is possible to hypothesize and model many different\ntypes of relationships between attributes using regression\nanalysis. In principle, the only constraint on the structure\nof the relationship that can be modeled is the ability to\ndefine the appropriate regression function. In some do-\nmains, there may be strong theoretical reasons to assert a\n114 Chapter 4 particular type of relationship, but in the absence of this\ntype of domain theory it is good practice to begin by as-\nsuming the simplest form of relationship\u2014namely, a lin-\near relationship\u2014and then, if need be, progress to model\nmore complex relationships. One reason for starting with\na linear relationship is that linear-regression functions are\nrelatively easy to interpret. The other reason is the com-\nmonsense notion that keeping things as simple as possible\nis generally a good idea.\nWhen a linear relationship is assumed, the regression\nanalysis is called linear regression. The simplest application\nof linear regression is modeling the relationship between\ntwo attributes: an input attribute X and a target attribute\nY. In this simple linear-regression problem, the regression\nfunction has the following form:\nY =\u03c9 +\u03c9 X\n0 1\nThis regression function is just the equation of a\nline (often written as y = mx + c) that is familiar to most\npeople from high school geometry.3 The variables \u03c9 and\n0\n\u03c9 are the parameters of the regression function. Modi-\n1\nfying these parameters changes how the function maps\nfrom the input X to the output Y. The parameter \u03c9 is the\n0\ny-intercept (or c in high school geometry) that specifies\nwhere the line crosses the vertical y axis when X is equal to\nzero. The parameter \u03c9 defines the slope of the line (i.e., it\n1\nis equivalent to m in the high school version).\nMaChine Learning 101 115 In regression analysis, the parameters of a regression\nfunction are initially unknown. Setting the parameters of\na regression function is equivalent to searching for the line\nthat best fits the data. The strategy for setting these pa-\nrameters begins by guessing parameters values and then\niteratively updating the parameters so as to reduce the\noverall error of the function on the data set. The overall\nerror is calculated in three steps:\n1. The function is applied to the data set, and for each\ninstance in the data set it estimates the value of the target\nattribute.\n2. The error of the function for each instance is calculated\nby subtracting the estimated value of the target attribute\nfrom the actual value of the target attribute.\n3. The error of the function for each instance is squared,\nand then these squared values are summed.\nThe error of the function for each instance is squared\nin step 3 so that the error in the instances where the func-\ntion overestimates the target doesn\u2019t cancel out with the\nerror when it underestimates. Squaring the error makes\nthe error positive in both cases. This measure of error is\nknown as the sum of squared errors (SSE), and the strategy\nof fitting a linear function by searching for the parameters\nthat minimize the SSE is known as least squares. The SSE\nis defined as\n116 Chapter 4 n\nSSE=\u2211(target \u2212prediction)2\ni i\ni=i\nwhere the data set contains n instances, target is the value\ni\nof the target attribute for instance i in the data set, and\nprediction is the estimate of the target by function for the\ni\nsame instance.\nTo create a linear-regression prediction model that\nestimates the likelihood of an individual\u2019s developing\ndiabetes with respect to his BMI, we replace X with the\nBMI attribute, Y with the diabetes attribute, and apply the\nleast-squares algorithm to find the best-fit line for the dia-\nbetes data set. Figure 11a illustrates this best-fit line and\nwhere it lies relative to the instances in the data set. In\nfigure 11b, the dashed lines show the error (or residual)\nfor each instance for this line. Using the least-squares ap-\nproach, the best-fit line is the line that minimizes the sum\nof the squared residuals. The equation for this line is\nDiabetes=\u22127.38431+0.55593\u2217BMI.\nThe slope parameter value \u03c9 = 0.55593 indicates that\n1\nfor each increase of one unit in BMI, the model increases\nthe estimated likelihood of a person developing diabetes\nby a little more than half a percent. In order to predict\nthe likelihood of a person\u2019s developing diabetes, we sim-\nply input his BMI into the model. For example, when BMI\nequals 20, the model returns a prediction of a 3.73 percent\nMaChine Learning 101 117 a.\nsetebaiD\nBMI\nb.\nsetebaiD\nBMI\nFigure 11 (a) The best-fit regression line for the model \u201cDiabetes =\n\u22127.38431 + 0.55593 BMI.\u201d (b) The dashed vertical lines illustrate the residual\nfor each instance.\n118 Chapter 4 likelihood for the diabetes attribute, and when BMI equals\n21, the model predicts a 4.29 percent likelihood.4\nUnder the hood, a linear-regression model fitted\nusing the least-squares method is actually calculating a\nweighted average over the instances. In fact, the intercept\nparameter value \u03c9 = \u22127.38431 ensures that the best-fit\n0\nline goes through the point defined by the average BMI\nvalue and average diabetes value for the data set. If the av-\nerage BMI value in the data set (BMI = 24.0932) is entered,\nthe model estimates a 4.29 percent likelihood for the dia-\nbetes attribute, which is the average value for diabetes in\nthe data set.\nThe weighting of the instances is based on the dis-\ntance of the instance from the line: the farther an instance\nis away from the line, the larger the residual for that in-\nstance, and the algorithm will weight that instance by\nthe residual squared. One consequence of this weighting\nis that instances that have extreme values (outliers) can\nhave a disproportionately large impact on the line-fitting\nprocess, resulting in the line being dragged away from the\nother instances. Thus, it is important to check for outliers\nin a data set prior to fitting a line to the data set (or, in\nother words, training a linear regression function on the\ndata set) using the least squares algorithm.\nLinear-regression models can be extended to take mul-\ntiple inputs. A new parameter is added to the model for\neach new input attribute, and the equation for the model\nMaChine Learning 101 119 is updated to include the result of multiplying the new at-\ntribute by the new parameter within the summation. For\nexample, to extend the model to include the exercise and\nweight attributes as input, the structure of the regression\nfunction becomes\nDiabetes=\u03c9 +\u03c9BMI+\u03c9Exercise+\u03c9Weight.\n0 1 2 3\nIn statistics, a regression function that maps from\nmultiple inputs to a single output in this way is known\nas a multiple linear regression function. The structure of a\nmulti-input regression function is the basis for a range of\nML algorithms, including neural networks.\nCorrelation and regression are similar concepts inso-\nfar as both are techniques that focus on the relationship\nacross columns in the data set. Correlation is focused\non exploring whether a relationship exists between two\nattributes, and regression is focused on modeling an as-\nsumed relationship between attributes with the purpose\nof being able to estimate the value of one target attribute\ngiven the values of one or more input attributes. In the\nspecific cases of Pearson correlation and linear regression,\na Pearson correlation measures the degree to which two\nattributes have a linear relationship, and linear regression\ntrained using least squares is a process to find the best-fit\nline that predicts the value of one attribute given the value\nof another.\n120 Chapter 4 Neural Networks and Deep Learning\nA neural network consists of a set of neurons that are con-\nnected together. A neuron takes a set of numeric values as\ninput and maps them to a single output value. At its core, a\nneuron is simply a multi-input linear-regression function.\nThe only significant difference between the two is that in\na neuron the output of the multi-input linear-regression\nfunction is passed through another function that is called\nthe activation function.\nThese activation functions apply a nonlinear map-\nping to the output of the multi-input linear-regression\nfunction. Two commonly used activation functions are\nthe logistic function and tanh function (see figure 12). Both\nfunctions take a single value x as input; in a neuron, this x\nvalue is the output from the multi-input linear-regression\nfunction the neuron has applied to its inputs. Also, both\nfunctions use Euler\u2019s number e, which is approximately\nequal to 2.71828182. These functions are sometimes\ncalled squashing functions because they take any value be-\ntween plus infinity and minus infinity and map it into a\nsmall, predefined range. The output range of the logistic\nfunction is 0 to 1, and the tanh function is \u22121 to 1. As a\nconsequence, the outputs of a neuron that uses a logistic\nfunction as its activation function are always between 0\nand 1. The fact that both the logistic and tanh functions\napply nonlinear mappings is clear in the S shape of the\ncurves. The reason for introducing a nonlinear mapping\nMaChine Learning 101 121 0.1\n5.0\n1\nlogistic(x)=\n1+e\u2212x )x(noitavitcA\n0.0\n5.0\u2212\ne2x\u22121\ntanh(x)=\ne2x+1\nlogistic(x)\ntanh(x) 0.1\u2212\n\u221210 \u22125 0 5 10\nx\nFigure 12 Mapping the logistic and tanh functions as applied to the input x.\ninto a neuron is that one of the limitations of a multi-\ninput linear-regression function is that the function is by\ndefinition linear, and if all the neurons within a network\nimplement only linear mappings, then the overall network\nis also limited to learning a linear functions. However, in-\ntroducing a nonlinear activation function in the neurons\n122 Chapter 4 of a network allows the network to learn more complex\n(nonlinear) functions.\nIt is worth emphasizing that each neuron in a neural\nnetwork is doing a very simple set of operations:\n1. Multiplying each input by a weight.\n2. Adding together the results of the multiplications.\n3. Pushing this result through an activation function.\nOperations 1 and 2 are just the calculation of a multi-\ninput regression function over the inputs, and operation 3\nis the application of the activation function.\nAll the connections between the neurons in a neural\nnetwork are directed and have a weight associated with\nthem. The weight on a connection coming into a neuron is\nthe weight that the neuron applies to the input it receives\non that connection when it is calculating the multi-input\nregression function over its inputs. Figure 13 illustrates\nthe topological structure of a simple neural network. The\nsquares on the left side of the figure, labeled A and B, rep-\nresent locations in memory that we use to present input\ndata to the network. No processing or transformation of\ndata is carried out at these locations. You can think of\nthese nodes as input or sensing neurons, whose output\nactivation is set to the value of the input.5 The circles in\nfigure 13 (labeled C, D, E, and F) represent the neurons\nMaChine Learning 101 123 C\nWA,C\nWB,C\nA WC,F\nWA,D\nWD,F\nWB,D D F Output\nWE,F\nWA,E\nB\nWB,E\nE\nFigure 13 A simple neural network.\nin the network. It is often useful to think of the neurons\nin a network as organized into layers. This network has\nthree layers of neurons: the input layer contains A and B;\none hidden layer contains C, D, and E; and the output layer\ncontains F. The term hidden layer describes the fact that\nthe neurons in a layer are in neither the input layer nor the\noutput layer, so in this sense they are hidden from view.\nThe arrows connecting the neurons in the network\nrepresent the flow of information through the network.\nTechnically, this particular network is a feed-forward neu-\nral network because there are no loops in the network: all\n124 Chapter 4 the connections point forward from the input toward the\noutput. Also, this network is considered fully connected\nbecause each neuron is connected to all the neurons in the\nnext layer in the network. It is possible to create many dif-\nferent types of neural networks by changing the number\nof layers, the number of neurons in each layer, the type\nof activation functions used, the direction of the connec-\ntions between layers, and other parameters. In fact, much\nof the work involved in developing a neural network for\na particular task involves experimenting to find the best\nnetwork layout for that task.\nThe labels on each arrow represent the weight that the\nnode at the end of the arrow applies to the information\npassed along that connection. For example, the arrow con-\nnecting C with F indicates that the output from C is passed\nas an input to F, and F will apply the weight W to the\nC,F\ninput from C.\nIf we assume that the neurons in the network in figure\n13 use a tanh activation function, then we can define the\ncalculation carried out in neuron F of the network as\nOutput=tanh(\u03c9 C+\u03c9 D+\u03c9 E)\nC,F D,F E,F\nThe mathematical definition of the processing carried\nout in neuron F shows that the final output of the network\nis calculated using a composition of a set of functions. The\nphrase \u201ccomposing functions\u201d just means that the output\nMaChine Learning 101 125 of one function is used as input to another function. In\nthis case, the outputs of neurons C, D, and E are used as in-\nputs to neuron F, so the function implemented by F com-\nposes the functions implemented by C, D, and E.\nFigure 14 makes this description of neural networks\nmore concrete, illustrating a neural network that takes a\nperson\u2019s body-fat percentage and VO max (a measure of\n2\nthe maximum amount of oxygen that a person can use in a\nminute) as input and calculates a fitness level for the that\nperson.6 Each neuron in the middle layer of the network\ncalculates a function based on the body-fat percentage and\nVO max: f (), f (), and f (). Each of these functions mod-\n2 1 2 3\nels the interaction between the inputs in a different way.\nThese functions essentially represent new attributes that\nare derived from the raw inputs to the network. They are\nsimilar to the BMI attribute described earlier, which was\ncalculated as a function of weight and height. Sometimes\nit is possible to interpret what the output of a neuron in\nthe network represents insofar as it is possible to provide\na domain-theoretic description of what the derived attri-\nbute represents and to understand why this derived attri-\nbute is useful to the network. Often, however, the derived\nattribute calculated by a neuron will not have a symbolic\nmeaning for humans. These attributes are instead captur-\ning interactions between the other attributes that the net-\nwork has found to be useful. The final node in the network,\nf , calculates another function\u2014over the outputs of f (),\n4 1\n126 Chapter 4 f1()\nFat%\nf2() f4() Fitness\nVO2\nf3()\nFigure 14 A neural network that predicts a person\u2019s fitness level.\nf (), and f ()\u2014the output of which is the fitness prediction\n2 3\nreturned by the network. Again, this function may not be\nmeaningful to humans beyond the fact that it defines an\ninteraction the network has found to have a high correla-\ntion with the target attribute.\nTraining a neural network involves finding the correct\nweights for the connections in the network. To understand\nhow to train a network, it is useful to begin by thinking\nabout how to train the weights for a single neuron in the\noutput layer of the network. Assume that we have a train-\ning data set that has both inputs and target output for\nMaChine Learning 101 127 each instance. Also, assume that the connections coming\ninto the neuron already have weights assigned to them.\nIf we take an instance from the data set and present the\nvalues of the input attributes for this instance to the net-\nwork, the neuron will output a prediction for the target.\nBy subtracting this prediction from the value for the target\nin the data set, we can measure the neuron\u2019s error on that\ninstance. Using some basic calculus, it is possible to derive\na rule to update the weights on the connections coming\ninto a neuron given a measure of the neuron\u2019s output error\nso as to reduce the neuron\u2019s error. The precise definition\nof this rule will vary depending on the activation function\nused by the neuron because the activation function affects\nthe derivative used in the derivation of the rule. But we\ncan give the following intuitive explanation of how the\nweight-update rule works:\n1. If the error is 0, then we should not change the weights\non the inputs.\n2. If the error is positive, we will decrease the error if we\nincrease the neuron\u2019s output, so we must increase the\nweights for all the connections where the input is positive\nand decrease the weights for the connections where the\ninput is negative.\n3. If the error is negative, we will decrease the error if we\ndecrease the neuron\u2019s output, so we must decrease the\n128 Chapter 4 weights for all the connections where the input is posi-\ntive and increase the weights for the connections where\nthe input is negative.\nThe difficulty in training a neural network is that the\nweight-update rule requires an estimate of the error at a\nneuron, and although it is straightforward to calculate the\nerror for each neuron in the output layer of the network,\nit is difficult to calculate the error for the neurons in the\nearlier layers. The standard way to train a neural network\nis to use an algorithm called the backpropagation algorithm\nto calculate the error for each neuron in the network and\nthen use the weight-update rule to modify the weights in\nthe network.7 The backpropagation algorithm is a super-\nvised ML algorithm, so it assumes a training data set that\nhas both inputs and the target output for each instance.\nThe training starts by assigning random weights to each\nof the connections in the network. The algorithm then it-\neratively updates the weights in the network by showing\ntraining instances from the data set to the network and\nupdating the network weights until the network is work-\ning as expected. The algorithm\u2019s name comes from the fact\nthat after each training instance is presented to the net-\nwork, the algorithm passes (or backpropagates) the error\nof the network back through the network starting at the\noutput layer and at each layer in the network calculates the\nerror for the neurons in that layer before sharing this error\nMaChine Learning 101 129 back to the neurons in the preceding layer. The main steps\nin the algorithm are as follows:\n1. Calculate the error for the neurons in the output layer\nand use the weight-update rule to update the weights\ncoming into these neurons.\n2. Share the error calculated at a neuron with each of the\nneurons in the preceding layer that is connected to that\nneuron in proportion to the weight of the connection\nbetween the two neurons.\n3. For each neuron in the preceding layer, calculate the\noverall error of the network that the neuron is responsi-\nble for by summing the errors that have been backpropa-\ngated to it and use the result of this error summation to\nupdate the weights on the connections coming into this\nneuron.\n4. Work back through the rest of the layers in the network\nby repeating steps 2 and 3 until the weights between the\ninput neurons and the first layer of hidden neurons have\nbeen updated.\nIn backpropagation, the weight updates for each neu-\nrons are scaled to reduce but not to eliminate the neuron\u2019s\nerror in the training instance. The reason for this is that\nthe goal of training the network is to enable it to generalize\nto new instances that are not in the training data rather\n130 Chapter 4 than to memorize the training data. So each set of weight\nupdates nudges the network toward a set of weights that\nare generally better over the whole data set, and over many\niterations the network converges on a set of weights that\ncaptures the general distribution of the data rather than\nthe specifics of the training instances. In some versions of\nbackpropagation, the weights are updated after a number\nof instances (or batch of instances) have been presented\nto the network rather than after each training instance.\nThe only adjustment required in these versions is that the\nalgorithm uses the average error of the network on a batch\nas the measure of error at the output layer for the weight-\nupdate process.\nOne of the most exciting technical developments in\nthe past 10 years has been the emergence of deep learn-\ning. Deep-learning networks are simply neural networks\nthat have multiple8 layers of hidden units; in other words,\nthey are deep in terms of the number of hidden layers\nthey have. The neural network in figure 15 has five lay-\ners: one input layer on the left containing three neurons,\nthree hidden layers (the black circles), and one output\nlayer on the right containing two neurons. This network\nillustrates that it is possible to have a different number of\nneurons in each layer: the input layer has three neurons;\nthe first hidden layer has five; each of the next two hid-\nden layers has four; and the output layer has two. This\nnetwork also shows that it is possible to have multiple\nMaChine Learning 101 131 Figure 15 A deep neural network.\nneurons in the output layer. Using multiple output neu-\nrons is useful if the target is a nominal or ordinal data type\nthat has distinct levels. In these scenarios, the network is\nset up so that there is one output neuron for each level,\nand the network is trained so that for each input only one\nof the output neurons outputs a high activation (denoting\nthe predicted target level).\nAs in the previous networks we have looked at, the\none shown in figure 15 is a fully connected, feed-forward\nnetwork. However, not all networks are fully connected,\nfeed-forward networks. In fact, myriad network topolo-\ngies have been developed. For example, recurrent neural\n132 Chapter 4 networks (RNNs) introduce loops in the network topol-\nogy: the output of a neuron for one input is fed back into\nthe neuron during the processing of the next input. This\nloop gives the network a memory that enables it to pro-\ncess each input in the context of the previous inputs it has\nprocessed. As a consequence, RNNs are suitable for pro-\ncessing sequential data such as language.9 Another well-\nknown deep neural network architecture is a convolutional\nneural network (CNN). CNNs were originally designed for\nuse with image data (Le Cun 1989). A desirable character-\nistic of an image-recognition network is that it should be\nable to recognize if a visual feature has occurred in an im-\nage irrespective of where in the image it has occurred. For\nexample, if a network is doing face recognition, it needs\nto be able to recognize the shape of an eye whether the\neye is in the top-right corner of the image or in the center\nof the image. CNNs achieve this by having groups of neu-\nrons that share the same set of weights on their inputs.\nIn this context, think of a set of input weights as defining\na function that returns true if a particular visual feature\noccurs in the set of pixels that are passed into the func-\ntion. This means that each group of neurons that share\ntheir weights learns to identify a particular visual feature,\nand each neuron in the group acts as a detector for that\nfeature. In a CNN, the neurons within each group are ar-\nranged so that each neuron examines a different location\nin the image, and the group covers the entire image. As a\nMaChine Learning 101 133 consequence, if the visual feature the group detects occurs\nanywhere in the image, one of the neurons in the group will\nidentify it.\nThe power of deep neural networks comes from the\nfact that they can automatically learn useful attributes,\nsuch as the feature detectors in CNNs. In fact, deep learn-\ning is sometimes known as representation learning because\nthese deep networks are essentially learning a new rep-\nresentation of the input data that is better at predicting\nthe target output attribute than the original raw input is.\nEach neuron in a network defines a function that maps\nthe values coming into the neuron into a new output at-\ntribute. So a neuron in the first layer of a network might\nlearn a function that maps the raw input values (such as\nweight and height) into an attribute that is more useful\nthan individual input values (such as BMI). However, the\noutput of this neuron, along with the outputs of its sister\nneurons in the first layer, is then fed into the neurons in\nthe second layer, and these second-layer neurons try to\nlearn functions that map the outputs of the first layer into\nnew and yet more useful representations. This process of\nmapping inputs to new attributes and feeding these new\nattributes as inputs to new functions continues through-\nout the network, and as a network gets deeper, it can learn\nmore and more complex mappings from raw inputs to new\nattribute representations. It is the ability to automatically\nlearn complex mappings of input data to useful attribute\n134 Chapter 4 representations that has made deep-learning models so\naccurate in tasks with high-dimensional inputs (such as\nimage and text processing).\nIt has been known for a long time that making neural\nnetworks deeper allows the network to learn more com-\nplex mappings of data. The reason that deep learning has\nnot really taken off until the past few years, however, is\nthat the standard combination of using a random-weight\ninitialization followed by the backpropagation algorithm\ndoesn\u2019t work well with deep networks. One problem with\nthe backpropagation algorithm is that the error gets\nshared out as the process goes back through the layers, so\nthat in a deep network by the time the algorithm reaches\nthe early layers of the network, the error estimates are not\nthat useful anymore.10 As a result, the layers in the early\nparts of the network don\u2019t learn useful transformations\nfor the data. In the past few years, however, researchers\nhave developed new types of neurons and adaptations to\nthe backpropagation algorithm that deal with this prob-\nlem. It has also been found that being careful with how\nthe network weights are initialized is also helpful. Two\nother factors that formerly made training deep networks\ndifficult were that it takes a great deal of computing power\nto train a neural network, and neural networks work best\nwhen there is a great deal of training data. However, as\nwe have already discussed, in recent years significant in-\ncreases in the availability of computing power and large\nMaChine Learning 101 135 data sets have made the training of deep networks more\nfeasible.\nDecision Trees\nLinear regression and neural networks work best with\nnumeric inputs. If the input attributes in a data set are\nprimarily nominal or ordinal, however, then other ML al-\ngorithms and models, such as decision trees, may be more\nappropriate.\nA decision tree encodes a set of if then, else rules in\na tree structure. Figure 16 illustrates a decision tree for\ndeciding whether an email is spam or not. Rectangles with\nrounded corners represent tests on attributes, and the\nsquare nodes indicate decision, or classification, nodes.\nThis tree encodes the following rules: if the email is from\nUnknown\nsender?\nFalse True\nSuspicious\nSpam\nwords?\nFalse True\nNot\nSpam\nspam\nFigure 16 A decision tree for determining whether an email is spam or not.\n136 Chapter 4 an unknown sender, then it is spam; if it isn\u2019t from an un-\nknown sender but contains suspicious words, then it is spam;\nif it is neither from an unknown sender nor contains suspi-\ncious words, then it is not spam. In a decision tree, the deci-\nsion for an instance is made by starting at the top of the\ntree and navigating down through the tree by applying a\nsequence of attribute tests to the instance. Each node in\nthe tree specifies one attribute to test, and the process\ndescends the tree node by node by choosing the branch\nfrom the current node with the label matching the value\nof the test attribute of the instance. The final decision is\nthe label of the terminating (or leaf) node that the instance\ndescends to.\nEach path in a decision tree, from root to leaf, defines\na classification rule composed of a sequence of tests. The\ngoal of a decision-tree-learning algorithm is to find a set\nof classification rules that divide the training data set into\nsets of instances that have the same value for the target\nattribute. The idea is that if a classification rule can sepa-\nrate out from a data set a subset of instances that have the\nsame target value, and if this classification rule is true for a\nnew example (i.e., the example goes down that path in the\ntree), then it is likely that the correct prediction for this\nnew example is the target value shared by all the training\ninstances that fit this rule.\nThe progenitor of most modern ML algorithms for\ndecision-tree learning is the ID3 algorithm (Quinlan 1986).\nMaChine Learning 101 137 ID3 builds a decision tree in a recursive, depth-first man-\nner, adding one node at a time, starting with the root\nnode. It begins by selecting an attribute to test at the root\nnode. A branch is grown from the root for each value in\nthe domain of this test attribute and is labeled with that\nvalue. For example, a node with a binary test attribute will\nhave two branches descending from it. The data set is then\ndivided up: each instance in the data set is pushed down\nthe branch and given a label that matches the value of the\ntest attribute for the instance. ID3 then grows each branch\nusing the same process used to create the root node: select\na test attribute, add a node with branches, split the data\nby funneling the instances down the relevant branches.\nThis process continues until all the instances on a branch\nhave the same value for the target attribute, in which case\na terminating node is added to the tree and labeled with\nthe target attribute value shared by all the instances on\nthe branch.11\nID3 chooses the attribute to test at each node in the\ntree so as to minimize the number of tests required to\ncreate pure sets (i.e., sets of instances that have the same\nvalue for the target attribute). One way to measure the pu-\nrity of a set is to use Claude Shannon\u2019s entropy metric. The\nminimum possible entropy for a set is zero, and a pure set\nhas an entropy of zero. The numeric value of the maxi-\nmum possible entropy for a set depends on the size of the\nset and the number of different types of elements that can\n138 Chapter 4 be in the set. A set will have maximum entropy when all\nthe elements in it are of different types.12 ID3 selects the\nattribute to test at a node to be the attribute that results\nin the lowest-weighted entropy after splitting the data set\nat the node using this attribute. The weighted entropy\nfor an attribute is calculated by (1) splitting the data set\nusing the attribute; (2) calculating the entropy of the re-\nsulting sets; (3) weighting each of these entropies by the\nfraction of data that is in the set; and (4) then summing\nthe results.\nTable 3 lists a data set of emails in which each email\nis described by a number of attributes and whether it is a\nspam email or not. The \u201cattachment\u201d attribute is true for\nemails that have an attachment and false otherwise (in this\nsample of emails, none of the emails has an attachment).\nTable 3 A Data Set of Emails: Spam or Not Spam?\nAttachment Suspicious Unknown Spam\nWords Sender\nFalse False True True\nFalse False True True\nFalse True False True\nFalse False False False\nFalse False False False\nMaChine Learning 101 139 The \u201csuspicious words\u201d attribute is true if the email con-\ntains one or more words on a predefined list of suspicious\nwords. The \u201cunknown sender\u201d attribute is true if the\nsender of the email is not in the recipient\u2019s address book.\nThis is the data set that was used to train the decision tree\nshown in figure 16. In this data set, the attributes \u201cattach-\nment,\u201d \u201csuspicious words,\u201d and \u201cunknown sender\u201d are the\ninput attributes and the \u201cspam\u201d attribute is the target. The\n\u201cunknown sender\u201d attribute splits the data set into purer\nsets more than any of the other attributes does (one set\ncontaining instances where \u201cSpam = True\u201d and another set\nin which \u201cSpam = False\u201d for the majority of instances). As\na consequence, \u201cunknown sender\u201d is put at the root node\n(see figure 17). After this initial split, all of the instances\nUnknown\nsender?\nFalse True\nSuspicious\nAttachment words Spam Attachment Suspicious Spam\nwords\nFalse True True\nFalse False True\nFalse False False\nFalse False True\nFalse False False\nFigure 17 Creating the root node in the tree.\n140 Chapter 4 on the right branch have the same target value. However,\nthe instances on the left branch have different values\nfor the target. Splitting the instances on the left branch\nusing the \u201csuspicious words\u201d attribute results in two pure\nsets: one where \u201cSpam = False\u201d and another where \u201cSpam\n= True.\u201d So \u201csuspicious words\u201d is selected as the test attri-\nbute for a new node on the left branch (see figure 18). At\nthis point, the data subsets at the end of each branch are\npure, so the algorithm finishes and returns the decision\ntree shown in figure 16.\nUnknown\nSender?\nFalse True\nSuspicious\nAttachment Spam\nWords\nSuspicious\nWords? False False True\nFalse False True\nFalse True\nAttachmentSpam\nAttachmentSpam\nFalse False\nFalse True\nFalse False\nFigure 18 Adding the second node to the tree.\nMaChine Learning 101 141 One of the strengths of decision trees is that they are\nsimple to understand. Also it is possible to create very\naccurate models based on decision trees. For example, a\nrandom-forest model is composed of a set of decision trees,\nwhere each tree is trained on a random subsample of the\ntraining data, and the prediction returned by the model\nfor an individual query is the majority prediction across all\nthe trees in the forest. Although decision trees work well\nwith both nominal and ordinal data, they struggle with\nnumeric data. In a decision tree, a separate branch de-\nscends from each node for each value in the domain of the\nattribute tested at the node. Numeric attributes, however,\nhave an infinite number of values in their domains, with\nthe implication that a tree would need an infinite number\nof branches. One solution to this problem is to transform\nnumeric attributes into ordinal attributes, although doing\nso requires the definition of appropriate thresholds, which\ncan also be difficult.\nFinally, because a decision-tree-learning algorithm\nrepeatedly divides a data set as a tree becomes large, it\nbecomes more sensitive to noise (such as mislabeled in-\nstances). The subset of examples on each branch becomes\nsmaller, and so the data sample each classification rule is\nbased on becomes smaller. The smaller the data sample\nused to define a classification rule, the more sensitive to\nnoise the rule becomes. As a consequence, it is a good idea\nto keep decision trees shallow. One approach is to stop the\ngrowth of a branch when the number of instances on the\n142 Chapter 4 branch is still less than a predefined threshold (e.g., 20\ninstances). Other approaches allow the tree to grow and\nthen prune the tree back. These approaches typically use\nstatistical tests or the performance of the model on a set of\ninstances specifically chosen for this task to identify splits\nnear the bottom of the tree that should be removed.\nBias in Data Science\nThe goal of ML is to create models that encode appropriate\ngeneralizations from data sets. Two major factors contrib-\nute to the generalization (or model) that an ML algorithm\nwill generate from a data set. The first is the data set the\nalgorithm is run on. If the data set is not representative of\nthe population, then the model the algorithm generates\nwon\u2019t be accurate. For example, earlier we developed a re-\ngression model that predicted the likelihood that an indi-\nvidual will develop Type 2 diabetes based on his BMI. This\nmodel was generated from a data set of American white\nmales. As a consequence, this model is unlikely to be accu-\nrate if used to predict the likelihood of diabetes for females\nor for males of different race or ethnic backgrounds. The\nterm sample bias describes how the process used to select\na data set can introduce biases into later analysis, be it a\nstatistical analysis or the generation of predictive models\nusing ML.\nMaChine Learning 101 143 The second factor that affects the model generated\nfrom a data set is the choice of ML algorithm. There are\nmany different ML algorithms, and each one encodes a\ndifferent way to generalize from a data set. The type of\ngeneralization an algorithm encodes is known as the learn-\ning bias (or sometimes the modeling or selection bias) of the\nalgorithm. For example, a linear-regression algorithm en-\ncodes a linear generalization from the data and as a result\nignores nonlinear relationships that may fit the data more\nclosely. Bias is normally understood as a bad thing. For\nexample, the sampling bias is a bias that a data scientist\nwill try to avoid. However, without a learning bias there\ncan be no learning, and the algorithm will only be able to\nmemorize the data.\nHowever, because ML algorithms are biased to look\nfor different types of patterns, and because there is no one\nlearning bias across all situations, there is no one best ML\nalgorithm. In fact, a theorem known as the \u201cno free lunch\ntheorem\u201d (Wolpert and Macready 1997) states that there\nis no one best ML algorithm that on average outperforms\nall other algorithms across all possible data sets. So the\nmodeling phase of the CRISP-DM process normally in-\nvolves building multiple models using different algorithms\nand comparing the models to identify which algorithm\ngenerates the best model. In effect, these experiments are\ntesting which learning bias on average produces the best\nmodels for the given data set and task.\n144 Chapter 4 Evaluating Models: Generalization Not Memorization\nOnce a data scientist has selected a set of ML algorithms\nto experiment with on a data set, the next major task is to\ncreate a test plan for how the models generated by these\nalgorithms will be evaluated. The goal of the test plan is to\nensure that the evaluation provides realistic estimates of\nmodel performance on unseen data. A prediction model\nthat simply memorizes a data set is unlikely to do a good\njob at estimating values for new examples. One problem\nwith just memorizing data is that most data sets will con-\ntain noise. So a prediction model that merely memorizes\ndata is also memorizing the noise in the data. Another\nproblem with just memorizing the data is that it reduces\nthe prediction process to a table lookup and leaves un-\nsolved the problem of how to generalize from the training\ndata to new examples that aren\u2019t in the table.\nOne part of the test plan relates to how the data set\nis used to train and test the models. The data set has to\nbe used for two different purposes. The first is to find\nwhich algorithm generates the best models. The second\nis to estimate the generalization performance of the best\nmodel\u2014that is, how well the model is likely to do on un-\nseen data. The golden rule for evaluating models is that\nmodels should never be tested on the same data they were\ntrained on. Using the same data for training and test-\ning models is equivalent to giving a class of students the\nMaChine Learning 101 145 The golden rule for\nevaluating models is\nthat models should\nnever be tested on the\nsame data they were\ntrained on. questions on an exam the night before the test is held. The\nstudents will of course do very well in the test, and their\nscores will not reflect their real proficiency with the gen-\neral course material. So, too, with ML models: if a model\nis evaluated on the same data that it is trained on, the re-\nsults of the evaluation will be optimistic compared to the\nmodel\u2019s real performance. The standard process for ensur-\ning that the models aren\u2019t able to peek at the test data dur-\ning training is to split the data into three parts: a training\nset, a validation set, and a test set. The proportions of the\nsplit will vary between projects, but splits of 50:20:30 and\n40:20:40 are common. The size of the data set is a key fac-\ntor in determining the splits: generally, the larger the data\nset, the larger the test set. The training set is used to train\nan initial set of models. The validation set is then used\nto compare the performance of these models on unseen\ndata. Comparing the performance of these initial models\non the validation set enables us to determine which algo-\nrithm generated the best model. Once the best algorithm\nhas been selected, the training and validation set can be\nmerged back together into a larger training set, and this\ndata set is fed into the best algorithm to create the final\nmodel. It is crucial that the test set is not used during the\nprocess to select the best algorithm, nor should it be used\nto train this final model. If these caveats are followed, then\nthe test set can be used to estimate the generalization per-\nformance of this final model on unseen data.\nMaChine Learning 101 147 The other major component of the test plan is to\nchoose the appropriate evaluation metrics to use during\nthe testing. In general, models are evaluated based on\nhow often the outputs of the model match the outputs\nlisted in the test set. If the target attribute is a numeric\nvalue, then the sum of squared errors is one way to mea-\nsure the accuracy of a model on the test set. If the target\nattribute is nominal or ordinal, then the simplest way to\nestimate the model accuracy is to calculate the proportion\nof examples of the test set the model got correct. How-\never, in some contexts it is important to include an error\nanalysis within the evaluation. For example, if a model\nis used in a medical diagnosis setting, it is much more\nserious if the model diagnoses an ill patient as healthy\nthan if it diagnoses a healthy patient as ill. Diagnosing\nan ill patient as healthy may result in the patient being\nsent home without receiving appropriate medical atten-\ntion, but if a model diagnoses a healthy patient as ill,\nthis error is likely to be discovered through later test-\ning the patient will receive. So the evaluation metric\nused to evaluate these types of models should weight\none type of error more than the other when estimating\nmodel performance. Once the test plan has been cre-\nated, the data scientist can begin training and evaluating\nmodels.\n148 Chapter 4 Summary\nThis chapter started by noting that data science is a part-\nnership between a data scientist and a computer. Machine\nlearning provides a set of algorithms that generate models\nfrom a large data set. However, whether these models are\nuseful will depend on the data scientist\u2019s expertise. For a\ndata science project to succeed, the data set should be rep-\nresentative of the domain and should include relevant at-\ntributes. The data scientist should evaluate a range of ML\nalgorithms to find which one generates the best models.\nThe model-evaluation process should follow the golden\nrule that a model should never be evaluated on the data\nit was trained on.\nCurrently in most data science projects, the primary\ncriterion for selecting which model to use is model accu-\nracy. However, in the near future, data usage and privacy\nregulations may affect the selection of ML algorithms.\nFor example, the General Data Protection Regulations\nwill come into force in the European Union on May 25,\n2018. We discuss these regulations in relation to data us-\nage in chapter 6, but for now we just want to point out that\nsome articles in the regulations may appear to mandate a\n\u201cright to explanation\u201d in relation to automated decision\nprocesses.13 A potential implication of such a right is that\nusing models, such a neural networks, that are difficult to\ninterpret for decisions relating to individuals may become\nMaChine Learning 101 149 problematic. In such circumstances, the transparency and\nease of explanation of some models, such as decision trees,\nmay make the use of these models more appropriate.\nFinally, the world changes, and models don\u2019t. Implicit\nin the ML process of data set construction, model training,\nand model evaluation is the assumption that the future\nwill be the same as the past. This assumption is known\nas the stationarity assumption: the processes or behaviors\nthat are being modeled are stationary through time (i.e.,\nthey don\u2019t change). Data sets are intrinsically historic in\nthe sense that data are representations of observations\nthat were made in the past. So, in effect, ML algorithms\nsearch through the past for patterns that might general-\nize to the future. Obviously, this assumption doesn\u2019t al-\nways hold. Data scientists use the term concept drift to\ndescribe how a process or behavior can change, or drift, as\ntime passes. This is why models go out of date and need to\nbe retrained and why the CRISP-DM process includes the\nouter circle shown in figure 4 to emphasize that data sci-\nence is iterative. Processes need to put in place postmodel\ndeployment to ensure that a model has not gone stale, and\nwhen it has, it should be retrained. The majority of these\ndecisions cannot be automated and require human insight\nand knowledge. A computer will answer the question it is\nposed, but unless care is taken, it is very easy to pose the\nwrong question.\n150 Chapter 4 5\nSTANDARD DATA SCIENCE TASKS\nOne of the most important skills for a data scientist is the\nability to frame a real-world problem as a standard data\nscience task. Most data science projects can be classified as\nbelonging to one of four general classes of task:\n\u2022 Clustering (or segmentation)\n\u2022 Anomaly (or outlier) detection\n\u2022 Association-rule mining\n\u2022 Prediction (including the subproblems of classification\nand regression)\nUnderstanding which task a project is targeting can\nhelp with many project decisions. For example, training\na prediction model requires that each of the instances in\nthe data set include the value of the target attribute. So knowing that the project is doing prediction gives guidance\n(through requirements) in terms of data set design. Un-\nderstanding the task also informs which ML algorithm(s)\nto use. Although there are a large number of ML algo-\nrithms, each algorithm is designed for a particular data-\nmining task. For example, ML algorithms that generate\ndecision-tree models are designed primarily for prediction\ntasks. There is a many-to-one relationship between ML al-\ngorithms and a task, so knowing the task doesn\u2019t tell you\nexactly which algorithm to use, but it does define a set of\nalgorithms that are designed for the task. Because the data\nscience task affects both the data set design and the selec-\ntion of ML algorithms, the decision regarding which task\nthe project will target has to be made early on in the proj-\nect life cycle, ideally during the business-understanding\nphase of the CRISP-DM life cycle. To provide a better un-\nderstanding of each of these tasks, this chapter describes\nhow some standard business problems map to tasks.\nWho Are Our Customers? (Clustering)\nOne of the most frequent application areas of data science\nin business is to support marketing and sales campaigns.\nDesigning a targeted marketing campaign requires an un-\nderstanding of the target customer. Most businesses have\na diverse range of customers with a variety of needs, so\n152 Chapter 5 using a one-size-fits-all approach is likely to fail with a\nlarge segment of a customer base. A better approach is to\ntry to identify a number of customer personas or customer\nprofiles, each of which relates to a significant segment of\nthe customer base, and then to design targeted marketing\ncampaigns for each persona. These personas can be created\nusing domain expertise, but it is generally a good idea to\nbase the personas on the data that the business has about\nits customers. Human intuition about customers can of-\nten miss important nonobvious segments or not provide\nthe level of granularity that is required for nuanced mar-\nketing. For example, Meta S. Brown (2014) reports how\nin one data science project the well-known stereotype\nsoccer mom (a suburban homemaker who spends a great\ndeal of time driving her children to soccer or other sports\npractice) didn\u2019t resonate with a customer base. However,\nusing a data-driven clustering process identified more fo-\ncused personas, such as mothers working full-time outside\nthe home with young children in daycare and mothers who\nwork part-time with high-school-age children and women\ninterested in food and health and who do not have children.\nThese customer personas define clearer targets for mar-\nketing campaigns and may highlight previously unknown\nsegments in the customer base.\nThe standard data science approach to this type of\nanalysis is to frame the problem as a clustering task. Clus-\ntering involves sorting the instances in a data set into\nStandard data SCienCe t aSkS 153 Human intuition\nabout customers can\noften miss important\nnonobvious segments\nor not provide the level\nof granularity that is\nrequired for nuanced\nmarketing. subgroups containing similar instances. Usually clustering\nrequires an analyst to first decide on the number of sub-\ngroups she would like identified in the data. This decision\nmay be based on domain knowledge or informed by proj-\nect goals. A clustering algorithm is then run on the data\nwith the desired number of subgroups input as one of the\nalgorithms parameters. The algorithm then creates that\nnumber of subgroups by grouping instances based on the\nsimilarity of their attribute values. Once the algorithm has\ncreated the clusters, a human domain expert reviews the\nclusters to interpret whether they are meaningful. In the\ncontext of designing a marketing campaign, this review\ninvolves checking whether the groups reflect sensible cus-\ntomer personas or identifies new personas not previously\nconsidered.\nThe range of attributes that can be used to describe\ncustomers for clustering is vast, but some typical examples\ninclude demographic information (age, gender, etc.), loca-\ntion (ZIP code, rural or urban address, etc.), transactional\ninformation (e.g., what products or services they have pur-\nchased), the revenue the company generates from them,\nhow long have they been customers, if they are a mem-\nber of a loyalty-card scheme, whether they ever returned\na product or made a complaint about a service, and so on.\nAs is true of all data science projects, one of the biggest\nchallenges with clustering is to decide which attributes to\ninclude and which to exclude so as to get the best results.\nStandard data SCienCe t aSkS 155 Making this decision on attribute selection will involve it-\nerations of experiments and human analysis of the results\nof each iteration.\nThe best-known ML algorithm for clustering is the\nk-means algorithm. The k in the name signals that the\nalgorithm looks for k clusters in the data. The value of k\nis predefined and is often set through a process of trial-\nand-error experimentation with different values of k. The\nk-means algorithm assumes that all the attributes describ-\ning the customers in the data set are numeric. If the data\nset contains nonnumeric attributes, then these attributes\nneed to be mapped to numeric values in order to use k-\nmeans, or the algorithm will need to be amended to handle\nthese nonnumeric values. The algorithm treats each cus-\ntomer as a point in a point cloud (or scatterplot), where\nthe customer\u2019s position is determined by the attribute val-\nues in her profile. The goal of the algorithm is to find the\nposition of each cluster\u2019s center in the point cloud. There\nare k clusters, so there are k cluster centers (or means)\u2014\nhence the name of the algorithm.\nThe k-means algorithm begins by selecting k instances\nto act as initial cluster centers. Current best practice is to\nuse an algorithm called \u201ck-means++\u201d to select the initial\ncluster centers. The rationale behind k-means++ is that it\nis a good idea to spread out the initial cluster centers as\nmuch as possible. So in k-means++ the first cluster cen-\nter is set by randomly selecting one of the instances in\n156 Chapter 5 As is true of all data\nscience projects, one of\nthe biggest challenges\nwith clustering is to\ndecide which attributes\nto include and which to\nexclude so as to get the\nbest results. the data set. The second and subsequent cluster centers\nare set by selecting an instance from the data set with\nthe probability that an instance selected is proportional\nto the squared distance from the closest existing cluster\ncenter. Once all k cluster centers have been initialized, the\nalgorithm works by iterating through a two-step process:\nfirst, assigning each instance to the nearest cluster center,\nand then, second, updating the cluster center to be in the\nmiddle of the instances assigned to it. In the first itera-\ntion the instances are assigned to the nearest cluster cen-\nter returned by the k-means++ algorithm, and then these\ncluster centers are moved so that they are positioned at\nthe center of instances assigned to them. Moving the clus-\nter centers is likely to move them closer to some instances\nand farther away from other instances (including farther\naway from some instances assigned to the cluster center).\nThe instances are then reassigned, again to the closest up-\ndated cluster center. Some instances will remain assigned\nto the same cluster center, and others may be reassigned\nto a new cluster center. This process of instance assign-\nment and center updating continues until no instances are\nassigned to a new cluster center during an iteration. The\nk-means algorithm is nondeterministic, meaning that dif-\nferent starting positions for the cluster centers will likely\nproduce different clusters. As a result, the algorithm is\ntypically run several times, and the results of these differ-\nent runs are then compared to see which clusters appear\n158 Chapter 5 most sensible given the data scientist\u2019s domain knowledge\nand understanding.\nWhen a set of clusters for customer personas has been\ndeemed to be useful, the clusters are often given names\nto reflect the main characteristics of the cluster persona.\nEach cluster center defines a different customer persona,\nwith the persona description generated from the attri-\nbute values of the associated cluster center. The k-means\nalgorithm is not required to return equal-size clusters,\nand, in fact, it is likely to return different-size clusters.\nThe sizes of the clusters can be useful, though, because\nthey can help to guide marketing. For example, the clus-\ntering process may reveal small, focused clusters of cus-\ntomers that current marketing campaigns are missing. Or\nan alternative strategy might be to focus on clusters that\ncontain customers that generate a great deal of revenue.\nWhatever marketing strategy is adopted, understanding\nthe segments within a customer base is the prerequisite\nto marketing success.\nOne of the advantages of clustering as an analytics\napproach is that it can be applied to most types of data.\nBecause of its versatility, clustering is often used as a data-\nexploration tool during the data-understanding stage of\nmany data science projects. Also, clustering is also useful\nacross a wide range of domains. For example, it has been\nused to analyze students in a given course in order to iden-\ntify groups of students who need extra support or prefer\nStandard data SCienCe t aSkS 159 different learning approaches. It has also been used to\nidentify groups of similar documents in a corpus, and in\nscience it has been used in bio-informatics to analyze gene\nsequences in microarray analysis.\nIs This Fraud? (Anomaly Detection)\nAnomaly detection or outlier analysis involves searching\nfor and identifying instances that do not conform to the\ntypical data in a data set. These nonconforming cases are\noften referred to as anomalies or outliers. Anomaly detec-\ntion is often used in analyzing financial transactions in\norder to identify potential fraudulent activities and to trig-\nger investigations. For example, anomaly detection might\nuncover fraudulent credit card transactions by identifying\ntransactions that have occurred in an unusual location or\nthat involve an unusually large amount compared to other\ntransactions on a particular credit card.\nThe first approach that most companies typically use\nfor anomaly detection is to manually define a number of\nrules based on domain expertise that help with identify-\ning anomalous events. This rule set is often defined in\nSQL or in another language and is run against the data\nin the business databases or data warehouse. Some pro-\ngramming languages have begun to include specific com-\nmands to facilitate the coding of these types of rules. For\n160 Chapter 5 example, database implementations of SQL now includes\na MATCH_RECOGNIZE function to facilitate pattern\nmatching in data. A common pattern in credit card fraud\nis that when a credit card gets stolen, the thief first checks\nthat the card is working by purchasing a small item on the\ncard, and then if that transaction goes through, the thief\nas quickly as possible follows that purchase with the pur-\nchase of an expensive item before the card is canceled. The\nMATCH_RECOGNIZE function in SQL enables database\nprogrammers to write scripts that identify sequences of\ntransactions on a credit card that fit this pattern and ei-\nther block the card automatically or trigger a warning to\nthe credit-card company. Over time, as more anomalous\ntransactions are identified\u2014for example, by customers re-\nporting fraudulent transactions\u2014the set of rules identify-\ning anomalous transactions is expanded to handle these\nnew instances.\nThe main drawback with a rule-based approach to\nanomaly detection is that defining rules in this way means\nthat anomalous events can be identified only after they\nhave occurred and have come to the company\u2019s atten-\ntion. Ideally, most organizations would like to be able to\nidentify anomalies when they first happen or if they have\nhappened but have not been reported. In some ways,\nanomaly detection is the opposite of clustering: the goal\nof clustering is to identify groups of similar instances,\nwhereas the goal of anomaly detection is to find instances\nStandard data SCienCe t aSkS 161 that are dissimilar to the rest of the data in the data set.\nBy this intuition, clustering can also be used to automati-\ncally identify anomalies. There are two approaches to us-\ning clustering for anomaly detection. The first is that the\nnormal data will be clustered together, and the anomalous\nrecords will be in separate clusters. The clusters containing\nthe anomalous records will be small and so will be clearly\ndistinct from the large clusters for the main body of the\nrecords. The second approach is to measure the distance\nbetween each instance and the center of the cluster. The\nfarther away the instance is from the center of the clus-\nter, the more likely it is to be anomalous and thus to need\ninvestigation.\nAnother approach to anomaly detection is to train a\nprediction model, such as a decision tree, to classify in-\nstances as anomalous or not. However, training such a\nmodel normally requires a training data set that contains\nboth anomalous records and normal records. Also, it is not\nenough to have just a few instances of anomalous records;\nin order to train a normal prediction model, the data set\nneeds to contain a reasonable number of instances from\neach class. Ideally, the data set should be balanced; in a\nbinary-outcome case, balance would imply a 50:50 split in\nthe data. In general, acquiring this type of training data for\nanomaly detection is not feasible: by definition, anomalies\nare rare events, occurring maybe in 1 to 2 percent or less of\nthe data. This data constraint precludes the use of normal,\n162 Chapter 5 off-the-shelf prediction models. There are, however, ML\nalgorithms known as one-class classifiers that are designed\nto deal with the type of imbalanced data that are typical of\nanomaly-detection data sets.\nThe one-class support-vector machine (SVM) algorithm\nis a well-known one-class classifier. In general terms, the\none-class SVM algorithm examines the data as one unit\n(i.e., a single class) and identifies the core characteristics\nand expected behavior of the instances. The algorithm\nwill then indicate how similar or dissimilar each instance\nis from the core characteristics and expected behavior.\nThis information can then be used to identify instances\nthat warrant further investigation (i.e., the anomalous re-\ncords). The more dissimilar an instance is, the more likely\nthat it should be investigated.\nThe fact that anomalies are rare means that they can\nbe easy to miss and difficult to identify. As a result, data\nscientists often combine a number of different models to\ndetect anomalies. The idea is that different models will\ncapture different types of anomalies. In general, these\nmodels are used to supplement the known rules within the\nbusiness that already define various types of anomalous\nactivity. The different models are integrated together into\na decision-management solution that enables the predic-\ntions from each of the models to feed into a decision of\nthe final predicted outcome. For example, if a transaction\nis identified as fraudulent by only one out of four models,\nStandard data SCienCe t aSkS 163 the decision system may decide that it isn\u2019t a true case\nof fraud, and the transaction can be ignored. Conversely,\nhowever, if three or four out of the four models have iden-\ntified the transaction as possible fraud, then the transac-\ntion would be flagged for a data scientist to investigate.\nAnomaly detection can be applied to many problem\ndomains beyond credit card fraud. More generally, it\nis used by clearinghouses to identify financial transac-\ntions that require further investigation as potentially\nfraudulent or as cases of money laundering. It is used in\ninsurance-claims analysis to identify claims that are not\nin keeping with a company\u2019s typical claims. In cybersecu-\nrity, it is used to identify network intrusions by detecting\npossible hacking or untypical behavior by employees. In\nthe medical domain, identifying anomalies in medical re-\ncords can be useful for diagnosing disease and in studying\ntreatments and their effects on the body. Finally, with the\nproliferation of sensors and the increasing usage of Inter-\nnet of Things technology, anomaly detection will play an\nimportant role in monitoring data and alerting us when\nabnormal sensor events occur and action is required.\nDo You Want Fries with That? (Association-Rule Mining)\nA standard strategy in sales is cross-selling, or suggest-\ning to customers who are buying products that they may\n164 Chapter 5 also want to purchase other related or complementary\nproducts. The idea is to increase the customers\u2019 overall\nspending by getting them to purchase more products and\nat the same time to improve customer service by remind-\ning customers of products they probably wanted to buy\nbut may have forgotten to do so. The classic example of\nthe cross-sell is when a waiter in a hamburger restaurant\nasks a customer who has just ordered a hamburger, \u201cDo\nyou want fries with that?\u201d Supermarkets and retailer busi-\nnesses know that shoppers purchase products in groups,\nand they use this information to set up cross-selling op-\nportunities. For example, supermarket customers who\nbuy hot dogs are also likely to purchase ketchup and beer.\nUsing this type of information, a store can plan the lay-\nout of the products. Locating hot dogs, ketchup, and beer\nnear each other in the store helps customers to collect this\ngroup of items quickly and may also boost the store sales\nbecause customers who are purchasing hot dogs might\nsee and purchase the ketchup and beer that they forgot\nthey needed. Understanding these types of associations\nbetween products is the basis of all cross-selling.\nAssociation-rule mining is an unsupervised-data-anal-\nysis technique that looks to find groups of items that fre-\nquently co-occur together. The classic case of association\nmining is market-basket analysis, wherein retail companies\ntry to identify sets of items that are purchased together,\nsuch as hot dogs, ketchup, and beer. To do this type of\nStandard data SCienCe t aSkS 165 data analysis, a business keeps track of the set (or basket)\nof items that each customer bought during each visit to\nthe store. Each row in the data set describes one basket of\ngoods purchased by a particular customer on a particular\nvisit to the store. So the attributes in the data set are the\nproducts the store sells. Given these data, association-rule\nmining looks for items that co-occur within each basket\nof goods. Unlike clustering and anomaly detection, which\nfocus on identifying similarities or differences between\ninstances (or rows) in a data set, association-rule mining\nfocuses on looking at relationships between attributes\n(or columns) in a data set. In a general sense, it looks\nfor correlations\u2014measured as co-occurrences\u2014between\nproducts. Using association-rule mining, a business can\nstart to answer questions about its customers\u2019 behaviors\nby looking for patterns that may exist in the data. Ques-\ntions that market-basket analysis can be used to answer\ninclude: Did a marketing campaign work? Have this custom-\ner\u2019s buying patterns changed? Has the customer had a major\nlife event? Does the product location affect buying behavior?\nWho should we target with our new product?\nThe Apriori algorithm is the main algorithm used to\nproduce the association rules. It has a two-step process:\n1. Find all combinations of items in a set of transactions\nthat occur with a specified minimum frequency. These\ncombinations are called frequent itemsets.\n166 Chapter 5 2. Generate rules that express the probable co-occurrence\nof items within frequent itemsets. The Apriori algorithm\ncalculates the probability of an item being present in a\nfrequent itemset given that another item or items are\npresent.\nThe Apriori algorithm generates association rules that\nexpress probabilistic relationships between items in fre-\nquent itemsets. An association rule is of the form \u201cIF ante-\ncedent, THEN consequent.\u201d It states that an item or group\nof items, the antecedent, implies the presence of another\nitem in the same basket of goods, the consequent, with\nsome probability. For example, a rule derived from a fre-\nquent itemset containing A, B, and C might state that if A\nand B are included in a transaction, then C is likely to also\nbe included:\nIF {hot-dogs, ketchup}, THEN {beer}.\nThis rule indicates that customers who are buying\nhot dogs and ketchup are also likely to buy beer. A frequent\nexample of the power of association-rule mining is the\nbeer-diapers example that describes how an unknown US\nsupermarket in the 1980s used an early computer sys-\ntem to analyze its checkout data and identified an un-\nusual association between diapers and beer in customer\npurchases. The theory developed to understand this rule\nStandard data SCienCe t aSkS 167 was that families with young children were preparing for\nthe weekend and knew that they would need diapers and\nwould have to socialize at home. The store placed the two\nitems near each other, and sales soared. The beer-and-dia-\npers story has been debunked as apocryphal, but it is still\na useful example of the potential benefits of association-\nrule mining for retail businesses.\nTwo main statistical measures are linked with associa-\ntion rules: support and confidence. The support percentage\nof an association rule\u2014or the ratio of transactions that\ninclude both the antecedent and consequent to the total\nnumber of transactions\u2014indicates how frequently the\nitems in the rule occur together. The confidence percent-\nage of an association rule\u2014or the ratio of the number of\ntransactions that include both the antecedent and con-\nsequent to the number of transactions that includes the\nantecedent\u2014is the conditional probability that the con-\nsequent will occur given the occurrence of the antecedent.\nSo, for example, a confidence of 75 percent for the asso-\nciation rule relating hot dogs and ketchup with beer would\nindicate that in 75 percent of cases where customers pur-\nchased both hot dogs and ketchup, they also purchased beer.\nThe support score of a rule simply records the percentage\nof baskets in the data set where the rule holds. For exam-\nple, a support of 5 percent indicates that 5 percent of all\nthe baskets in the data set contain all three items in the\nrule \u201chot dogs, ketchup, and beer.\u201d\n168 Chapter 5 Even a small data set can result in the generation of\na large number of association rules. In order to control\nthe complexity of the analysis of these rules, it is usual\nto prune the generated rule set to include only rules that\nhave both a high support and a high confidence. Rules that\ndon\u2019t have high support or confidence are not interesting\neither because the rule covers only a very small percent-\nage of baskets (low support) or because the relationship\nbetween the items in the antecedent and the consequent is\nlow (low confidence). Rules that are trivial or inexplicable\nshould also be pruned. Trivial rules represent associations\nthat are obvious and well known to anyone who under-\nstands the business domain. An inexplicable rule repre-\nsents associations that are so strange that it is difficult to\nunderstand how to convert the rule into a useful action\nfor the company. It is likely that an inexplicable rule is the\nresult of an odd data sample (i.e., the rule represents a spu-\nrious correlation). Once the rule set has been pruned, the\ndata scientist can then analyze the remaining rules to un-\nderstand what products are associated with each other and\napply this new information in the organization. Organiza-\ntions will typically use this new information to determine\nstore layout or to perform some targeted marketing cam-\npaigns to their customers. These campaigns can involve\nupdates to their websites to include recommended prod-\nucts, in-store advertisements, direct mailings, the cross-\nselling of other products by check-out staff, and so on.\nStandard data SCienCe t aSkS 169 Association mining becomes more powerful if the bas-\nkets of items are connected to demographic data about the\ncustomer. This is why so many retailers run loyalty-card\nschemes because such schemes allow them not only to\nconnect different baskets of goods to the same customer\nthrough time but also to connect baskets of goods to the\ncustomer\u2019s demographics. Including this demographic in-\nformation in the association analysis enables the analysis\nto be focused on particular demographics, which can fur-\nther help marketing and targeted advertising. For exam-\nple, demographic-based association rules can be used with\nnew customers, for whom the company has no buying-\nhabit information but does have demographic informa-\ntion. An example of an association rule augmented with\ndemographic information might be\nIF gender(male) and age(< 35) and {hot-dogs, ketchup},\nTHEN {beer}.\n[Support = 2%, Confidence = 90%.]\nThe standard application area for association-rule\nmining focuses on what products are in the shopping bas-\nket and what products are not in the shopping basket. This\nassumes that the products are purchased in one visit to\nthe store or website. This kind of scenario will probably\nwork in most retail and other related scenarios. However,\n170 Chapter 5 association-rule mining is also useful in a range of domains\noutside of retail. For example, in the telecommunications\nindustry, applying association-rule mining to customer\nusage helps telecommunications companies to design how\nto bundle different services together into packages. In the\ninsurance industry, association-rule mining is used to see\nif there are associations between products and claims. In\nthe medical domain, it is used to check if there are inter-\nactions between existing and new treatments and medi-\ncines. And in banking and financial services, it is used to\nsee what products customers typically have and whether\nthese products can be applied to new or existing custom-\ners. Association-rule mining can also be used to analyze\npurchasing behavior over a period of time. For example,\ncustomers tend to buy product X and Y today, and in three\nmonths\u2019 time they buy product Z. This time period can\nbe considered a shopping basket, although it is one that\nspans three months. Applying association-rule mining to\nthis kind of temporally defined basket expands the appli-\ncations areas of association-rule mining to include main-\ntenance schedules, the replacement of parts, service calls,\nfinancial products, and so on.\nChurn or No Churn, That Is the Question (Classification)\nA standard business task in customer-relationship man-\nagement is to estimate the likelihood that an individual\ncustomer will take an action. The term propensity modeling\nStandard data SCienCe t aSkS 171 is used to describe this task because the goal is to model an\nindividual\u2019s propensity to do something. This action could\nbe anything from responding to marketing to default-\ning on a loan or leaving a service. The ability to identify\ncustomers who are likely to leave a service is particularly\nimportant to cell phone service companies. It costs a cell\nphone service company a substantial amount of money to\nattract new customers. In fact, it is estimated that it gener-\nally costs five to six times more to attract a new customer\nthan it does to retain an established one (Verbeke et al.\n2011). As a result, many cell phone service companies are\nvery keen to retain their current customers. However, they\nalso want to minimize costs. So although it would be easy\nto retain customers by simply giving all customers reduced\nrates and great phone upgrades, this is not a realistic op-\ntion. Instead, they want to target the offers they give their\ncustomers to just those customers who are likely to leave\nin the near future. If they can identify a customer who\nis about to leave a service and persuade that customer to\nstay, perhaps by offering her an upgrade or a new billing\npackage, then they can save the difference between the\nprice of the enticement they gave the customer and the\ncost of attracting a new customer.\nThe term customer churn is used to describe the pro-\ncess of customers leaving one service and joining another.\nSo the problem of predicting which customers are likely\nto leave in the near future is known as churn prediction. As\n172 Chapter 5 the name suggests, this is a prediction task. The predic-\ntion task is to classify a customer as being a churn risk\nor not. Many companies are using this kind of analysis to\npredict churn customers in the telecommunications, utili-\nties, banking, insurance, and other industries. A growing\narea that companies are focusing on is the prediction of\nstaff turnover or staff churn: which staff are likely to leave\nthe company within a certain time period.\nWhen a prediction model returns a label or category\nfor an input, it is known as a classification model. Training\na classification model requires historic data, where each\ninstance is labeled to indicate whether the target event\nhas happened for that instance. For example, customer-\nchurn classification requires a data set in which each cus-\ntomer (one row per customer) is assigned a label indicating\nwhether he or she has churned. The data set will include\nan attribute, known as the target attribute, that lists this\nlabel for each customer. In some instances, assigning a\nchurn label to a customer record is a relatively straightfor-\nward task. For example, the customer may have contacted\nthe organization and explicitly canceled his subscription\nor contract. However, in other cases the churn event may\nnot be explicitly signaled. For example, not all cell phone\ncustomers have a monthly contract. Some customers have\na pay-as-you-go (or prepay) contract in which they top up\ntheir account at irregular intervals when they need more\nphone credit. Defining whether a customer with this type\nStandard data SCienCe t aSkS 173 of contract has churned can be difficult: Has a customer\nwho hasn\u2019t made a call in two weeks churned, or is it neces-\nsary for a customer to have a zero balance and no activity\nfor three weeks before she is considered to have churned?\nOnce the churn event has been defined from a business\nperspective, it is then necessary to implement this defini-\ntion in code in order to assign a target label to each cus-\ntomer in the data set.\nAnother complicating factor in constructing the train-\ning data set for a churn-prediction model is that time lags\nneed to be taken into account. The goal of churn prediction\nis to model the propensity (or likelihood) that a customer\nwill churn at some point in the future. As a consequence,\nthis type of model has a temporal dimension that needs to\nbe considered during the creation of the data set. The set of\nattributes in a propensity-model data set are drawn from\ntwo separate time periods: the observation period and the\noutcome period. The observation period is when the values\nof the input attributes are calculated. The outcome period\nis when the target attribute is calculated. The business\ngoal of creating a customer-churn model is to enable the\nbusiness to carry out some sort of intervention before the\ncustomer churns\u2014in other words, to entice the customer\nto stay with the service. This means that the prediction\nabout the customer churning must be made sometime in\nadvance of the customer\u2019s actually leaving the service. The\nlength of this period is the length of the outcome period,\n174 Chapter 5 and the prediction that the churn model returns is actually\nthat a customer will churn within this outcome period. For\nexample, the model might be trained to predict that the\ncustomer will churn within one month or two months, de-\npending on the speed of the business process to carry out\nthe intervention.\nDefining the outcome period affects what data should\nbe used as input to the model. If the model is designed\nto predict that a customer will churn within two months\nfrom the day the model is run on that customer\u2019s record,\nthen when the model is being trained, the input attri-\nbutes that describe the historic customers who have al-\nready churned should be calculated using only the data\nthat were available about those customers two months\nprior to their leaving the service. The input attributes\ndescribing currently active customers should similarly be\ncalculated with the data available about these customers\u2019\nactivity two months earlier. Creating the data set in this\nway ensures that all the instances in the data set, including\nboth churned and active customers, describe the custom-\ners at the time in their individual customer journeys that\nthe model is being designed to make a prediction about\nthem: in this example, two months before they churn\nor stay.\nNearly all customer-propensity models will use attri-\nbutes describing the customer\u2019s demographic information\nas input: age, gender, occupation, and so on. In scenarios\nStandard data SCienCe t aSkS 175 relating to an ongoing service, they are also likely to in-\nclude attributes describing the customer\u2019s position in the\ncustomer life cycle: coming on board, standing still midcycle,\napproaching end of a contract. There are also likely to be\nattributes that are specific to the industry. For example,\ntypical attributes used in telecommunication industry\ncustomer-churn models include the customer\u2019s average\nbill, changes in billing amount, average usage, staying\nwithin or generally exceeding plan minutes, the ratio of\ncalls within the network to those outside the network, and\npotentially the type of phone used.1 However, the specific\nattributes used in each model will vary from one project to\nthe next. Gordon Linoff and Michael Berry (2011) report\nthat in one churn-prediction project in South Korea, the\nresearchers found it useful to include an attribute that de-\nscribed the churn rate associated with a customer\u2019s phone\n(i.e., What percentage of customers with this particular\nphone churned during the observation period?). However,\nwhen they went to build a similar customer-churn model\nin Canada, the handset/churn-rate attribute was useless.\nThe difference was that in South Korea the cell phone ser-\nvice company offered large discounts on new phones to\nnew customers, whereas in Canada the same discounts\nwere offered to both existing and new customers. The\noverall effect was that in South Korea phones going out\nof date drove customer churn; people were incentivized to\nleave one operator for another in order to avail themselves\n176 Chapter 5 of discounts, but in Canada this incentive to leave did not\nexist.\nOnce a labeled data set has been created, the major\nstage in creating a classification model is to use an ML algo-\nrithm to build the classification model. During modeling,\nit is good practice to experiment with a number of differ-\nent ML algorithms to find out which algorithm works best\non the data set. Once the final model has been selected,\nthe likely accuracy of the predictions of this model on new\ninstances is estimated by testing it on a subset of the data\nset that was not used during the model-training phase. If\na model is deemed accurate enough and suitable for the\nbusiness need, the model is then deployed and applied to\nnew data either in a batch process or in real time. A really\nimportant part of deploying the model is ensuring that\nthe appropriate business processes and resources are put\nin place so that the model is used effectively. There is no\npoint in creating a customer-churn model unless there is\na process whereby the model\u2019s predictions result in trig-\ngering customer interventions so that the business retains\ncustomers.\nIn addition to predicting the classification label, pre-\ndiction models can also give a measure of how confident\nthe model is in the prediction. This measure is called the\nprediction probability and will have a value between 0 and\n1. The higher the value, the more likely the prediction\nis correct. The prediction-probability value can be used\nStandard data SCienCe t aSkS 177 to prioritize which customers to focus on. For example,\nin customer-churn prediction the organization wants to\nconcentrate on the customers who are most likely to leave.\nBy using the prediction probability and sorting the churn-\ners based on this value, a business can focus on the key\ncustomers (those most likely to leave) first before mov-\ning on to customers with a lower prediction-probability\nscore.\nHow Much Will It Cost? (Regression)\nPrice prediction is the task of estimating the price that a\nproduct will cost at a particular point in time. The product\ncould be a car, a house, a barrel of oil, a stock, or a medical\nprocedure. Having a good estimate of what something will\ncost is obviously valuable to anyone who is considering\nbuying the item. The accuracy of a price-prediction model\nis domain dependent. For example, due to the variability\nin the stock market, predicting the price of a stock tomor-\nrow is very difficult. By comparison, it may be easier to\npredict the price of a house at an auction because the vari-\nation in house prices fluctuates much more slowly than\nstocks.\nThe fact that price prediction involves estimating the\nvalue of a continuous attribute means that it is treated as\na regression problem. A regression problem is structurally\n178 Chapter 5 very similar to a classification problem; in both cases, the\ndata science solution involves building a model that can\npredict the missing value of an attribute given a set of in-\nput attributes. The only difference is that classification in-\nvolves estimating the value of a categorical attribute and\nregression involves estimating the value of a continuous\nattribute. Regression analysis requires a data set where\nthe value of the target attribute for each of the historic in-\nstances is listed. The multi-input linear-regression model\nintroduced in chapter 4 illustrated the basic structure of a\nregression model, with most other regression models be-\ning variants of this approach. The basic structure of a re-\ngression model for price prediction is the same no matter\nwhat product it is applied to; all that varies are the name\nand number of the attributes. For example, to predict the\nprice of a house, the input would include attributes such\nas the size of the house, the number of rooms, the number\nof floors, the average house price in the area, the average\nhouse size in the area, and so on. By comparison, to predict\nthe price of a car, the attributes would include the age of\nthe car, the number of miles on the odometer, the engine\nsize, the make of the car, the number of doors, and so on.\nIn each case, given the appropriate data, the regression al-\ngorithm works out how each of the attributes contributes\nto the final price.\nAs has been the case with all the examples given\nthroughout this chapter, the application example of using\nStandard data SCienCe t aSkS 179 a regression model for price prediction is illustrative only\nof the type of problem that it is appropriate to frame as\na regression-modeling task. Regression prediction can be\nused in a wide variety of other real-world problems. Typi-\ncal regression-prediction problems include calculating\nprofit, value and volume of sales, sizes, demand, distances,\nand dosage.\n180 Chapter 5 6\nPRIVACY AND ETHICS\nThe biggest unknown facing data science today is how\nsocieties will choose to answer a new version of the old\nquestion regarding how best to balance the freedoms and\nprivacy of individuals and minorities against the security\nand interests of society. In the context of data science, this\nold question is framed as follows: What do we as a soci-\nety view are reasonable ways to gather and use the data\nrelating to individuals in contexts as diverse as fighting\nterrorism, improving medicine, supporting public-policy\nresearch, fighting crime, detecting fraud, assessing credit\nrisk, providing insurance underwriting, and advertising to\ntargeted groups?\nThe promise of data science is that it provides a way\nto understand the world through data. In the current\nera of big data, this promise is very tantalizing, and, in-\ndeed, a number of arguments can be used to support the development and adoption of data-driven infrastructure\nand technologies. One common argument relates to im-\nproving efficiency, effectiveness, and competiveness\u2014an\nargument that, at least in the business context, is backed\nby some academic research. For example, a study involv-\ning 179 large publicly traded firms in 2011 showed that\nthe more data driven a firm\u2019s decision making is, the more\nproductive the firm is: \u201cWe find that firms that adopt DDD\n[data-driven decision making] have output and productiv-\nity that is 5\u20136% higher than what would be expected given\ntheir other investments and information technology us-\nage\u201d (Brynjolfsson, Hitt, and Kim 2011, 1).\nAnother argument for increased adoption of data sci-\nence technologies and practices relates to securitization.\nFor a long time, governments have used the argument\nthat surveillance improves security. And since the terror-\nist attacks in the United States on September 11, 2001,\nas well as with each subsequent terrorist attack through-\nout the world, the argument has gained traction. Indeed,\nit was frequently used in the public debate caused by Ed-\nward Snowden\u2019s revelations about the US National Secu-\nrity Agency\u2019s PRISM surveillance program and the data it\nroutinely gathered on US citizens. A stark example of the\npower of this argument is the agency\u2019s US$1.7 billion in-\nvestment in a data center in Bluffdale, Utah, that has the\nability to store huge amounts of intercepted communica-\ntions (Carroll 2013).\n182 Chapter 6 At the same time, however, societies, governments,\nand business are struggling to understand the long-term\nimplications of data science in a big-data world. Given the\nrapid development of technologies around data gather-\ning, data storage, and data analysis, it is not surprising\nthat the legal frameworks in place and the broader ethical\ndiscussions around data, in particular the question of in-\ndividual privacy, are running behind these advances. Not-\nwithstanding this difficulty, basic legal principles around\ndata collection and usage are important to understand\nand are nearly always applicable. Also, the ethical debate\naround data usage and privacy has highlighted some wor-\nrying trends that we as individuals and citizens should be\naware of.\nCommercial Interests versus Individual Privacy\nData science can be framed as making the world a more\nprosperous and secure place to live. But these same argu-\nments can be used by very different organizations that\nhave very distinct agendas. For example, contrast calls by\ncivil liberties groups for government to be more open and\ntransparent in the gathering, use, and availability of data\nin the hope of empowering citizens to hold these same\ngovernments to account with similar calls from busi-\nness communities who hope to use these data to increase\nprivaCy and ethiCs 183 their profits (Kitchin 2014a). In truth, data science is a\ndouble-edged sword. It can be used to improve our lives\nthrough more efficient government, improved medicine\nand health care, less-expensive insurance, smarter cities,\nreduced crime, and many more ways. At the same time,\nhowever, it can also be used to spy on our private lives, to\ntarget us with unwanted advertising, and to control our\nbehavior both overtly and covertly (the fear of surveillance\ncan affect us as much as the surveillance itself does).\nThe contradictory aspects of data science can often\nbe apparent in the same applications. For example, the\nuse of data science in health insurance underwriting uses\nthird-party marketing data sets that contain informa-\ntion such as purchasing habits, web search history, along\nwith hundreds of other attributes relating to people\u2019s life-\nstyles (Batty, Tripathi, Kroll, et al. 2010). The use of these\nthird-party data is troublesome because it may trigger\nself-disciplining, wherein people avoid certain activities,\nsuch as visiting extreme-sports websites, for fear of in-\ncurring higher insurance premiums (Mayer-Sch\u00f6nberger\nand Cukier 2014). However, the justification for the use\nof these data is that it acts as a proxy for more invasive\nand expensive information sources, such as blood tests,\nand in the long term will reduce costs and premiums and\nthereby increase the number of people with health insur-\nance (Batty, Tripathi, Kroll, et al. 2010).\n184 Chapter 6 The fault lines in the debate between the commercial\nbenefits and ethical considerations of using data science\nare apparent in the discussions around the use of per-\nsonal data for targeted marketing. From a business ad-\nvertising perspective, the incentive to use personal data\nis that there is a relationship between personalizing mar-\nketing, services, and products, on the one hand, and the\neffectiveness of the marketing, on the other. It has been\nshown that the use of personal social network data\u2014\nsuch as identifying consumers who are connected to prior\ncustomers\u2014increases the effectiveness of a direct-mail\nmarketing campaign for a telecommunications service\nby three to five times compared to traditional marketing\napproaches (Hill, Provost, and Volinsky 2006). Similar\nclaims have been made about the effectiveness of data-\ndriven personalization of online marketing. For example,\na study of online cost and effectiveness of online targeted\nadvertising in the United States in 2010 compared run-of-\nthe-network marketing (when an advertising campaign is\npushed out across a range of websites without specific tar-\ngeting of users or sites) with behavioral targeting1 (Beales\n2010). The study found that behavioral marketing was\nboth more expensive (2.68 times more) but also more ef-\nfective, with a conversion rate more than twice that of run-\nof-the-network marketing. Another well-known study on\nthe effectiveness of data-driven online advertising was\nconducted by researchers from the University of Toronto\nprivaCy and ethiCs 185 and MIT (Goldfarb and Tucker 2011). They used the enact-\nment of a privacy-protection bill in the European Union\n(EU)2 that limited the ability of advertising companies to\ntrack users\u2019 online behavior in order to compare the effec-\ntiveness of online advertising under the new restrictions\n(i.e., in the EU) and the effectiveness online advertising\nnot under the new restrictions (i.e., in the United States\nand other non-EU countries). The study found that online\nadvertising was significantly less effective under the new\nrestrictions, with a reported drop of 65 percent in study\nparticipants\u2019 recorded purchasing intent. The results of\nthis study have been contested (see, for example, Mayer\nand Mitchell 2012), but the study has been used to sup-\nport the argument that the more data that are available\nabout an individual, the more effective the advertising\nthat is directed to that individual will be. Proponents of\ndata-driven targeted marketing frame this argument as a\nwin\u2013win for both the advertiser and the consumer, claim-\ning that advertisers lower marketing costs by reducing\nwasted advertising and achieve better conversions rates,\nand consumers get more relevant advertising.\nThis utopian perspective on the use of personal data\nfor targeted marketing is at best based on a selective un-\nderstanding of the problem. Probably one of the most wor-\nrying stories related to targeted advertising was reported\nin the New York Times in 2012 and involves the American\ndiscount retail store Target (Duhigg 2012). It is well known\n186 Chapter 6 in marketing that one of the times in a person\u2019s life when\nhis or her shopping habits change radically is at the con-\nception and birth of a child. Because of this radical change,\nmarketers see pregnancy as an opportunity to shift a\nperson\u2019s shopping habits and brand loyalties, and many\nretailers use publicly available birth records to trigger per-\nsonalized marketing for new parents, sending them offers\nrelating to baby products. In order to get a competitive ad-\nvantage, Target wanted to identify pregnant customers at\nan early stage (ideally during the second trimester) with-\nout the mother-to-be voluntarily telling Target that she\nwas pregnant.3 This insight would enable Target to begin\nits personalized marketing before other retailers knew the\nbaby was on the way. To achieve this goal, Target initiated\na data science project with the aim of predicting whether a\ncustomer was pregnant based on an analysis of her shop-\nping habits. The starting point for the project was to ana-\nlyze the shopping habits of women who had signed up for\nTarget\u2019s baby-shower registry. The analysis revealed that\nexpectant mothers tended to purchase larger quantities of\nunscented lotion at the beginning of the second trimes-\nter as well as certain dietary supplements throughout the\nfirst 20 weeks of pregnancy. Based on this analysis, Target\ncreated a data-driven model that used around 25 products\nand indictors and assigned each customer a \u201cpregnancy-\nprediction\u201d score. The success, for want of a better word, of\nthis model was made very apparent when a man turned up\nprivaCy and ethiCs 187 at a Target store to complain about the fact that his high-\nschool-age daughter had been mailed coupons for baby\nclothes and cribs. He accused Target of trying to encour-\nage his daughter to get pregnant. However, over the sub-\nsequent days it transpired that the man\u2019s daughter was in\nfact pregnant but hadn\u2019t told anyone. Target\u2019s pregnancy-\nprediction model was able to identify a pregnant high\nschool student and act on this information before she had\nchosen to tell her family.\nEthical Implications of Data Science: Profiling and\nDiscrimination\nThe story about Target identifying a pregnant high school\nstudent without her consent or knowledge highlights how\ndata science can be used for social profiling not only of\nindividuals but also of minority groups in society. In his\nbook The Daily You: How the New Advertising Industry Is De-\nfining Your Identity and Your Worth (2013), Joseph Turow\ndiscusses how marketers use digital profiling to categorize\npeople as either targets or waste and then use these cat-\negories to personalize the offers and promotions directed\nto individual consumers: \u201cthose considered waste are ig-\nnored or shunted to other products that marketers deem\nmore relevant to their tastes or income\u201d (11). This person-\nalization can result in preferential treatment for some and\n188 Chapter 6 Personalization can\nresult in preferential\ntreatment for some\nand marginalization\nof others. marginalization of others. A clear example of this discrim-\nination is differential pricing on websites, wherein some\ncustomers are charged more than other customers for the\nsame product based on their customer profiles (Clifford\n2012).\nThese profiles are constructed by integrating data\nfrom a number of different noisy and partial data sources,\nso the profiles can often be misleading about an individual.\nWhat is worse is that these marketing profiles are treated\nas products and are often sold to other companies, with\nthe result that a negative marketing assessment of an indi-\nvidual can follow that individual across many domains. We\nhave already discussed the use of marketing data sets in in-\nsurance underwriting (Batty, Tripathi, Kroll, et al. 2010),\nbut these profiles can also make their way into credit-risk\nassessments and many other decision processes that af-\nfect people\u2019s lives. Two aspects of these marketing pro-\nfiles make them particularly problematic. First, they are a\nblack box, and, second, they are persistent. The black-box\nnature of these profiles is apparent when one considers\nthat it is difficult for an individual to know what data are\nrecorded about them, where and when the data were re-\ncorded, and how the decision processes that use these data\nwork. As a result, if an individual ends up on a no-fly list or\na credit blacklist, it is \u201cdifficult to determine the grounds\nfor discrimination and to challenge them\u201d (Kitchin 2014a,\n177). What is more, in the modern world data are often\nstored for a long time. So data recorded about an event in\n190 Chapter 6 an individual\u2019s life persists long after an event. As Turow\nwarns, \u201cTurning individual profiles into individual evalu-\nations is what happens when a profile becomes a reputa-\ntion\u201d (2013, 6).\nFurthermore, unless used very carefully, data science\ncan actually perpetuate and reinforce prejudice. An argu-\nment is sometimes made that data science is objective:\nit is based on numbers, so it doesn\u2019t encode or have the\nprejudicial views that affect human decisions. The truth\nis that data science algorithms perform in an amoral\nmanner more than in an objective manner. Data science\nextracts patterns in data; however, if the data encode a\nprejudicial relationship in society, then the algorithm is\nlikely to identify this pattern and base its outputs on the\npattern. Indeed, the more consistent a prejudice is in a\nsociety, the stronger that prejudicial pattern will appear\nin the data about that society, and the more likely a data\nscience algorithm will extract and replicate that pattern of\nprejudice. For example, a study carried out by academic re-\nsearchers on the Google Online Advertising system found\nthat the system showed an ad relating to a high-paying\njob more frequently to participants whose Google profile\nidentified them as male compared to participants whose\nprofile identified them as female (Datta, Tschantz, and\nDatta 2015).\nThe fact that data science algorithms can reinforce\nprejudice is particularly troublesome when data science\nis applied to policing. Predictive Policing, or PredPol,4 is\nprivaCy and ethiCs 191 a data science tool designed to predict when and where\na crime is most likely to occur. When deployed in a city,\nPredPol generates a daily report listing a number of hot\nspots on a map (small areas 500 feet by 500 feet) where the\nsystem believes crimes are likely to occur and tags each hot\nspot with the police shift during which the system believes\nthe crime will occur. Police departments in both the United\nStates and the United Kingdom have deployed PredPol.\nThe idea behind this type of intelligent-policing system is\nthat policing resources can be efficiently deployed. On the\nsurface, this seems like a sensible application of data sci-\nence, potentially resulting in efficient targeting of crime\nand reducing policing costs. However, questions have\nbeen raised about the accuracy of PredPol and the effec-\ntiveness of similar predictive-policing initiatives (Hunt,\nSaunders, and Hollywood 2014; Oakland Privacy Work-\ning Group 2015; Harkness 2016). The potential for these\ntypes of systems to encode racial or class-based profiling\nin policing has also been noted (Baldridge 2015). The de-\nployment of police resources based on historic data can re-\nsult in a higher police presence in certain areas\u2014typically\neconomically disadvantaged areas\u2014which in turn results\nin higher levels of reported crime in these areas. In other\nwords, the prediction of crime becomes a self-fulfilling\nprophesy. The result of this cycle is that some locations\nwill be disproportionately targeted by police surveillance,\ncausing a breakdown in trust between the people who live\n192 Chapter 6 Unless used very\ncarefully, data science\ncan actually perpetuate\nand reinforce prejudice. in those communities and policing institutions (Harkness\n2016).\nAnother example of data-driven policing is the Stra-\ntegic Subjects List (SSL) used by the Chicago Police De-\npartment in an attempt to reduce gun crime. The list was\nfirst created in 2013, and at that time it listed 426 peo-\nple who were estimated to be at a very high risk of gun\nviolence. In an attempt to proactively prevent gun crime,\nthe Chicago Police Department contacted all the people\non the SSL to warn them that they were under surveil-\nlance. Some of the people on the list were very surprised\nto be included on it because although they did have crimi-\nnal records for minor offenses, they had no violence on\ntheir records (Gorner 2013). One question to ask about\nthis type of data gathering to prevent crime is, How ac-\ncurate is the technology? A recent study found that the\npeople on the SSL for 2013 were \u201cnot more or less likely to\nbecome a victim of a homicide or shooting than the com-\nparison group\u201d (Saunders, Hunt, and Hollywood 2016).\nHowever, this study also found that individuals on the list\nwere more likely to be arrested for a shooting incident,\nalthough it did point out that this greater likelihood could\nhave been created by the fact that these individuals were\non the list, which resulted in increasing police officers\u2019\nawareness of these individuals (Saunders, Hunt, and Hol-\nlywood 2016). Responding to this study, the Chicago Police\nDepartment stated that it regularly updated the algorithm\n194 Chapter 6 used to compile the SSL and that the effectiveness of the\nSSL had improved since 2013 (Rhee 2016). Another ques-\ntion about data-driven crime-prevention lists is, How\ndoes an individual end up on the list? The 2013 version\nof the SSL appears to have been compiled using, among\nother attributes of an individual, an analysis of his or her\nsocial network, including the arrest and shooting histo-\nries of his or her acquaintances (Dokoupil 2013; Gorner\n2013). On the one hand, the idea of using social network\nanalysis makes sense, but it opens up the very real prob-\nlem of guilt by association. One problem with this type\nof approach is that it can be difficult to define precisely\nwhat an association between two individuals entails. Is\nliving on the same street enough to be an association?\nFurthermore, in the United States, where the vast major-\nity of inmates in prison are African American and Latino\nmales, allowing predictive-policing algorithms to use the\nconcept of association as an input is likely to result in pre-\ndictions targeting mainly young men of color (Baldridge\n2015).\nThe anticipatory nature of predictive policing means\nthat individuals may be treated differently not because of\nwhat they have done but because of data-driven inferences\nabout what they might do. As a result, these types of sys-\ntems may reinforce discriminatory practices by replicating\nthe patterns in historic data and may create self-fulfilling\nprophecies.\nprivaCy and ethiCs 195 Ethical Implications of Data Science: Creating a\nPanopticon\nIf you spend time absorbing some of the commercial\nboosterism that surrounds data science, you get a sense\nthat any problem can be solved using data science technol-\nogy given enough of the right data. This marketing of the\npower of data science feeds into a view that a data-driven\napproach to governance is the best way to address complex\nsocial problems, such as crime, poverty, poor education,\nand poor public health: all we need to do to solve these\nproblems is to put sensors into our societies to track ev-\nerything, merge all the data, and run the algorithms to\ngenerate the key insights that provide the solution.\nWhen this argument is accepted, two processes are\noften intensified. The first is that society becomes more\ntechnocratic in nature, and aspects of life begin to be\nregulated by data-driven systems. Examples of this type\nof technological regulation already exist\u2014for example, in\nsome jurisdictions data science is currently used in parole\nhearings (Berk and Bleich 2013) and sentencing (Barry-\nJester, Casselman, and Goldstein 2015). For an example\noutside of the judicial system, consider how smart-city\ntechnologies regulate traffic flows through cities with\nalgorithms dynamically deciding which traffic flow gets\npriority at a junction at different times of day (Kitchin\n2014b). A by-product of this technocratic regulation is the\nproliferation of the sensors that support the automated\n196 Chapter 6 regulating systems. The second process is \u201ccontrol creep,\u201d\nwherein data gathered for one purpose is repurposed and\nused to regulate in another way (Innes 2001). For exam-\nple, road cameras that were installed in London with the\nprimary purpose of regulating congestion and implement-\ning congestion charges (the London congestion charge is\na daily charge for driving a vehicle within London dur-\ning peak times) have been repurposed for security tasks\n(Dodge and Kitchin 2007). Other examples of control\ncreep include a technology called ShotSpotter that con-\nsists of a city-wide network of microphones designed to\nidentify gunshots and report the locations of them but\nthat also records conversations, some of which were used\nto achieve criminal convictions (Weissman 2015), and the\nuse of in-car navigation systems to monitor and fine rental\ncar drivers who drive out of state (Elliott 2004; Kitchin\n2014a).\nAn aspect of control creep is the drive to merge data\nfrom different sources so as to provide a more complete\npicture of a society and thereby potentially unlock deeper\ninsights into the problems in the system. There are often\ngood reasons for the repurposing of data. Indeed, calls\nare frequently made for data held by different branches\nof government to be merged for legitimate purposes\u2014for\nexample, to support health research and for the conve-\nnience of the state and its citizens. From a civil liberties\nperspective, however, these trends are very concerning.\nprivaCy and ethiCs 197 Heightened surveillance, the integration of data from\nmultiple sources, control creep, and anticipatory gover-\nnance (such as the predictive-policing programs) may re-\nsult in a society where an individual may be treated with\nsuspicion simply because a sequence of unrelated inno-\ncent actions or encounters matches a pattern deemed sus-\npicious by a data-driven regulatory system. Living in this\ntype of a society would change each of us from free citizens\ninto inmates in Bentham\u2019s Panopticon,5 constantly self-\ndisciplining our behaviors for fear of what inferences may\nbe drawn from them. The distinction between individuals\nwho believe and act as though they are free of surveillance\nand individuals who self-discipline out of fear that they\ninhabit a Panopticon is the primary difference between a\nfree society and a totalitarian state.\n\u00c1 la recherche du privacy perdu\nAs individuals engage with and move through techni-\ncally modern societies, they have no choice but to leave\na data trail behind them. In the real world, the prolifera-\ntion of video surveillance means that location data can be\ngathered about an individual whenever she appears on a\nstreet or in a shop or car park, and the proliferation of\ncell phones means that many people can be tracked via\ntheir phones. Other examples of real-world data gathering\n198 Chapter 6 include the recording of credit card purchases, the use of\nloyalty schemes in supermarkets, the tracking of with-\ndrawals from ATMs, and the tracking of cell phone calls\nmade. In the online world, data are gathered about in-\ndividuals when they visit or log in to websites; send an\nemail; engage in online shopping; rate a date, restaurant,\nor store; use an e-book reader; watch a lecture in a massive\nopen online course; or like or post something on a social\nmedia site. To put into perspective the amount of data that\nare gathered on the average individual in a technologically\nmodern society, a report from the Dutch Data Protection\nAuthority in 2009 estimated that the average Dutch citi-\nzen was included in 250 to 500 databases, with this figure\nrising to 1,000 databases for more socially active people\n(Koops 2011). Taken together, the data points relating to\nan individual define that person\u2019s digital footprint.\nThe data in a digital footprint can be gathered in two\ncontexts that are problematic from a privacy perspective.\nFirst, data can be collected about an individual without\nhis knowledge or awareness. Second, in some contexts an\nindividual may choose to share data about himself and\nhis opinions but may have little or no knowledge of or\ncontrol over how these data are used or how they will be\nshared with and repurposed by third parties. The terms\ndata shadow and data footprint6 are used to distinguish\nthese two contexts of data gathering: an individual\u2019s data\nshadow comprises the data gathered about an individual\nprivaCy and ethiCs 199 without her knowledge, consent, or awareness, and an in-\ndividual\u2019s data footprint consists of the pieces of data that\nshe knowingly makes public (Koops 2011).\nThe collection of data about an individual without\nher knowledge or consent is of course worrying. However,\nthe power of modern data science techniques to uncover\nhidden patterns in data coupled with the integration and\nrepurposing of data from several sources means that even\ndata collected with an individual\u2019s knowledge and consent\nin one context can have negative effects on that individual\nthat are impossible for them to predict. Today, with the\nuse of modern data science techniques, very personal in-\nformation that we may not want to be made public and\nchoose not to share can still be reliably inferred from seem-\ningly unrelated data we willingly post on social media.\nFor example, many people are willing to like something\non Facebook because they want to demonstrate support\nto a friend. However, by simply using the items that an\nindividual has liked on Facebook, data-driven models can\naccurately predict that person\u2019s sexual orientation, politi-\ncal and religious views, intelligence and personality traits,\nand use of addictive substances such as alcohol, drugs, and\ncigarettes; they can even determine whether that person\u2019s\nparents stayed together until he or she was 21 years old\n(Kosinski, Stillwell, and Graepel 2013). The out-of-context\nlinkages made in these models is demonstrated by how lik-\ning a human rights campaign was found to be predictive of\n200 Chapter 6 homosexuality (both male and female) and by how liking\nHondas was found to be predictive of not smoking (Kosin-\nski, Stillwell, and Graepel 2013).\nComputational Approaches to Preserving Privacy\nIn recent years, there has been a growing interest in com-\nputational approaches to preserving individual privacy\nthroughout a data-analysis process. Two of the best-known\napproaches are differential privacy and federated learning.\nDifferential privacy is a mathematical approach to the\nproblem of learning useful information about a popula-\ntion while at the same time learning nothing about the in-\ndividuals within the population. Differential privacy uses\na particular definition of privacy: the privacy of an indi-\nvidual has not been compromised by the inclusion of his\nor her data in the data-analysis process if the conclusions\nreached by the analysis would have been the same inde-\npendent of whether the individual\u2019s data were included or\nnot. A number of processes can be used to implement dif-\nferential privacy. At the core of these processes is the idea\nof injecting noise either into the data-collection process\nor into the responses to database queries. The noise pro-\ntects the privacy of individuals but can be removed from\nthe data at an aggregate level so that useful population-\nlevel statistics can be calculated. A useful example of a\nprivaCy and ethiCs 201 procedure for injecting noise into data that provides an\nintuitive explanation of how differential privacy processes\ncan work is the randomized-response technique. The use\ncase for this technique is a survey that includes a sensitive\nyes/no question (e.g., relating to law breaking, health con-\nditions, etc.). Survey respondents are instructed to answer\nthe sensitive question using the following procedure:\n1. Flip a coin and keep the result of the coin flip secret.\n2. If tails, respond \u201cYes.\u201d\n3. If heads, respond truthfully.\nHalf the respondents will get tails and respond \u201cYes\u201d;\nthe other half will respond truthfully. Therefore, the true\nnumber of \u201cNo\u201d respondents in the total population is (ap-\nproximately) twice the number of \u201cNo\u201d responses (the coin\nis fair and selects randomly, so the distribution of yes/no\nresponses among the respondents who got tails should\nmirror the number of respondents who answered truth-\nfully). Given the true count for \u201cNo,\u201d we can calculate the\ntrue count for \u201cYes.\u201d However, although we now have an\naccurate count for the population regarding the sensitive\n\u201cYes\u201d condition, it is not possible to identify for which\nof the \u201cYes\u201d respondents the sensitive condition actually\nholds. There is a trade-off between the amount of noise\ninjected into data and the usefulness of the data for data\n202 Chapter 6 analysis. Differential privacy addresses this trade-off by\nproviding estimates of the amount of noise required given\nfactors such as the distribution of data within the data-\nbase, the type of database query that is being processed,\nand the number of queries through which we wish to guar-\nantee an individual\u2019s privacy. Cynthia Dwork and Aaron\nRoth (2014) provide an introduction to differential pri-\nvacy and an overview of several approaches to implement-\ning differential privacy. Differential-privacy techniques\nare now being deployed in a number of consumer prod-\nucts. For example, Apple uses differential privacy in iOS\n10 to protect the privacy of individual users while at the\nsame time learning usage patterns to improve predictive\ntext in the messaging application and to improve search\nfunctionality.\nIn some scenarios, the data being used in a data sci-\nence project are coming from multiple disparate sources.\nFor example, multiple hospitals may be contributing to\na single research project, or a company is collecting data\nfrom a large number of users of a cell phone application.\nRather than centralizing these data into a single data re-\npository and doing the analysis on the combined data, an\nalternative approach is to train different models on the\nsubsets of the data at the different data sources (i.e., at\nthe individual hospitals or on the phones of each individ-\nual user) and then to merge the separately trained models.\nGoogle uses this federated-learning approach to improve\nprivaCy and ethiCs 203 The truth is that data\nscience algorithms\nperform in an amoral\nmanner more than in\nan objective manner. the query suggestions made by the Google keyboard\non Android (McMahan and Ramage 2017). In Google\u2019s\nfederated-learning framework, the mobile device initially\nhas a copy of the current application loaded. As the user\nuses the application, the application data for that user are\ncollected on his phone and used by a learning algorithm\nthat is local to the phone to update the local version of\nthe model. This local update of the model is then uploaded\nto the cloud, where it is averaged with the model updates\nuploaded from other user phones. The core model is then\nupdated using this average. With the use of this process,\nthe core model can be improved, and individual users\u2019\nprivacy can at the same time be protected to the extent\nthat only the model updates are shared\u2014not the users\u2019\nusage data.\nLegal Frameworks for Regulating Data Use and\nProtecting Privacy\nThere is variation across jurisdictions in the laws relating\nto privacy protection and permissible data usage. How-\never, two core pillars are present across most democratic\njurisdictions: antidiscrimination legislation and personal-\ndata-protection legislation.\nIn most jurisdictions, antidiscrimination legisla-\ntion forbids discrimination based on any of the following\nprivaCy and ethiCs 205 grounds: disability, age, sex, race, ethnicity, nationality,\nsexual orientation, and religious or political opinion. In\nthe United States, the Civil Rights Act of 19647 prohib-\nits discrimination based on color, race, sex, religion, or\nnationality. Later legislation has extended this list; for\nexample, the Americans with Disabilities Act of 19908 ex-\ntended protection to people against discrimination based\non disabilities. Similar legalization is in place in many\nother jurisdictions. For example, the Charter of Funda-\nmental Rights of the European Union prohibits discrimi-\nnation based on any grounds, including race, color, ethnic\nor social origin, genetic features, sex, age, birth, disability,\nsexual orientation, religion or belief, property, member-\nship in a national minority, and political or any other opin-\nion (Charter 2000).\nA similar situation of variation and overlap exists\nwith respect to privacy legislation across different juris-\ndictions. In the United States, the Fair Information Prac-\ntice Principles (1973)9 have provided the basis for much\nof the subsequent privacy legislation in that jurisdiction.\nIn the EU, the Data Protection Directive (Council of the\nEuropean Union and European Parliament 1995) is the\nbasis for much of that jurisdiction\u2019s privacy legislation.\nThe General Data Protection Regulations (Council of the\nEuropean Union and European Parliament 2016) expand\non the data protection principles in the Data Protection\nDirective and provide consistent and legally enforceable\ndata protection regulations across all EU member states.\n206 Chapter 6 However, the most broadly accepted principles relating to\npersonal privacy and data are the Guidelines on the Pro-\ntection of Privacy and Transborder Flows of Personal Data\npublished by the Organisation for Economic Co-operation\nand Development (OECD 1980). Within these guidelines,\npersonal data are defined as records relating to an identifi-\nable individual, known as the data subject. The guidelines\ndefine eight (overlapping) principles that are designed to\nprotect a data subject\u2019s privacy:\n1. Collection Limitation Principle: Personal data should\nonly be obtained lawfully and with the knowledge and con-\nsent of the data subject.\n2. Data Quality Principle: Any personal data that are col-\nlected should be relevant to the purpose for which they are\nused; they should be accurate, complete, and up to date.\n3. Purpose Specification Principle: At or before the time\nthat personal data are collected, the data subject should be\ninformed of the purpose for which the data will be used.\nFurthermore, although changes of purpose are permis-\nsible, they should not be introduced arbitrarily (new pur-\nposes must be compatible with the original purpose) and\nshould be specified to the data subject.\n4. Use Limitation Principle: The use of personal data is\nlimited to the purpose that the data subject has been in-\nformed of, and the data should not be disclosed to third\nprivaCy and ethiCs 207 parties without the data subject\u2019s consent or by authority\nof law.\n5. Safety Safeguards Principle: Personal data should be\nprotected by security safeguards against deletion, theft,\ndisclosure, modification, or unauthorized use.\n6. Openness Principle: A data subject should be able to\nacquire information with reasonable ease regarding the\ncollection, storage, and use of his or her personal data.\n7. Individual Participation Principle: A data subject has\nthe right to access and challenge personal data.\n8. Accountability Principle: A data controller is account-\nable for complying with the principles.\nMany countries, including the EU and the United\nStates, endorse the OECD guidelines. Indeed, the data\nprotection principles in the EU General Data Protection\nRegulations can be broadly traced back to the OECD guide-\nlines. The General Data Protection Regulations apply to\nthe collection, storage, transfer and processing of personal\ndata relating to EU citizens within the EU and has impli-\ncations for the flows of this data outside of the EU. Cur-\nrently, several countries are developing data protection\nlaws similar to and consistent with the General Data Pro-\ntection Regulations.\n208 Chapter 6 Toward an Ethical Data Science\nIt is well known that despite the legal frameworks that are\nin place, nation-states frequently collect personal data on\ntheir citizens and foreign nationals without these people\u2019s\nknowledge, often in the name of security and intelligence.\nExamples include the US National Security Agency\u2019s\nPRISM program; the UK Government Communications\nHeadquarters\u2019 Tempora program (Shubber 2013); and the\nRussian government\u2019s System for Operative Investigative\nActivities (Soldatov and Borogan 2012). These programs\naffect the public\u2019s perception of governments and use of\nmodern communication technologies. The results of the\nPew survey \u201cAmericans\u2019 Privacy Strategies Post-Snowden\u201d\nin 2015 indicated that 87 percent of respondents were\naware of government surveillance of phone and Internet\ncommunications, and among those who were aware of\nthese programs 61 percent stated that they were losing\nconfidence that these programs served the public inter-\nest, and 25 percent reported that they had changed how\nthey used technologies in response to learning about these\nprograms (Rainie and Madden 2015). Similar results have\nbeen reported in European surveys, with more than half\nof Europeans aware of large-scale data collection by gov-\nernment agencies and most respondents stating that this\ntype of surveillance had a negative impact on their trust\nwith respect to how their online personal data are used\n(Eurobarometer 2015).\nprivaCy and ethiCs 209 At the same time, many private companies avoid the\nregulations around personal data and privacy by claiming\nto use derived, aggregated, or anonymized data. By re-\npackaging data in these ways, companies claim that the\ndata are no longer personal data, which, they argue, per-\nmits them to gather data without an individual\u2019s aware-\nness or consent and without having a clear immediate\npurpose for the data; to hold the data for long periods of\ntime; and to repurpose the data or sell the data when a\ncommercial opportunity arises. Many advocates of the\ncommercial opportunities of data science and big data\nargue that the real commercial value of data comes from\ntheir reuse or \u201coptional value\u201d (Mayer-Sch\u00f6nberger and\nCukier 2014). The advocates of data reuse highlight two\ntechnical innovations that make data gathering and stor-\nage a sensible business strategy: first, today data can be\ngathered passively with little or no effort or awareness on\nthe part of the individuals being tracked; and, second, data\nstorage has become relatively inexpensive. In this context,\nit makes commercial sense to record and store data in case\nfuture (potentially unforeseeable) commercial opportuni-\nties make it valuable.\nThe modern commercial practices of hoarding, repur-\nposing, and selling data are completely at odds with the\npurpose specification and use-limitation principles of the\nOECD guidelines. Furthermore, the collection-limitation\nprinciple is undermined whenever a company presents\n210 Chapter 6 a privacy agreement to a consumer that is designed to\nbe unreadable or reserves the right for the company to\nmodify the agreement without further consultation or\nnotification or both. Whenever this happens, the process\nof notification and granting of consent is turned into a\nmeaningless box-ticking exercise. Similar to the public\nopinion about government surveillance in the name of\nsecurity, public opinion is quite negative toward com-\nmercial websites\u2019 gathering and repurposing of personal\ndata. Again using American and European surveys as our\nlitmus test for wider public opinion, a survey of Ameri-\ncan Internet users in 2012 found that 62 percent of adults\nsurveyed stated that they did not know how to limit the\ninformation collected about them by websites, and 68 per-\ncent stated that they did not like the practice of targeted\nadvertising because they did not like their online behavior\ntracked and analyzed (Purcell, Brenner, and Rainie 2012).\nA recent survey of European citizens found similar results:\n69 percent of respondents felt that the collection of their\ndata should require their explicit approval, but only 18 per-\ncent of respondents actually fully read privacy statements.\nFurthermore, 67 percent of respondents stated that they\ndon\u2019t read privacy statements because they found them\ntoo long, and 38 percent stated that they found them un-\nclear or too difficult to understand. The survey also found\nthat 69 percent of respondents were concerned about\ntheir information being used for different purposes from\nprivaCy and ethiCs 211 the one it was collected for, and 53 percent of respondents\nwere uncomfortable with Internet companies using their\npersonal information to tailor advertising (Eurobarom-\neter 2015).\nSo at the moment public opinion is broadly negative\ntoward both government surveillance and Internet com-\npanies\u2019 gathering, storing, and analyzing of personnel\ndata. Today, most commentators agree that data-privacy\nlegislation needs to be updated and that changes are hap-\npening. In 2012, both the EU and the United States pub-\nlished reviews and updates relating to data-protection\nand privacy policies (European Commission 2012; Federal\nTrade Commission 2012; Kitchin 2014a, 173). In 2013,\nthe OECD guidelines were extended to include, among\nother updates, more details in relation to implementing\nthe accountability principle. In particular, the new guide-\nlines define the data controller\u2019s responsibilities to have\na privacy-management program in place and to define\nclearly what such a program entails and how it should be\nframed in terms of risk management in relation to per-\nsonal data (OECD 2013). In 2014, a Spanish citizen, Mario\nCosteja Gonzalez, won a case in the EU Court of Justice\nagainst Google (C-131/12 [2014]) asserting his right to be\nforgotten. The court held that an individual could request,\nunder certain conditions, an Internet search engine to re-\nmove links to webpages that resulted from searches on the\nindividual\u2019s name. The grounds for such a request included\nthat the data are inaccurate or out of date or that the data\n212 Chapter 6 had been kept for longer than was necessary for historical,\nstatistical, or scientific purposes. This ruling has major\nimplications for all Internet search engines but may also\nhave implications for other big-data hoarders. For ex-\nample, it is not clear at present what the implications are\nfor social media sites such as Facebook and Twitter (Marr\n2015). The concept of the right to be forgotten has been\nasserted in other jurisdictions. For example, the Califor-\nnia \u201ceraser\u201d law asserts a minor\u2019s right to have material\nhe has posted on an Internet or mobile service removed\nat his request. The law also prohibits Internet, online, or\ncell phone service companies from compiling personal\ndata relating to a minor for the purposes of targeted ad-\nvertising or allowing a third party to do so.10 As a final\nexample of the changes taking place, in 2016 the EU-US\nPrivacy Shield was signed and adopted (European Com-\nmission 2016). Its focus is on harmonizing data-privacy\nobligations across the two jurisdictions. Its purpose is to\nstrengthen the data-protection rights for EU citizens in the\ncontext where their data have been moved outside of the\nEU. This agreement imposed stronger obligations on com-\nmercial companies with regard to transparency of data us-\nage, strong oversight mechanisms and possible sanctions,\nas well as limitations and oversight mechanisms for public\nauthorities in recording or accessing personal data. How-\never, at the time of writing, the strength and effectiveness\nof the EU-US Privacy Shield is being tested in a legal case\nin the Irish courts. The reason why the Irish legal system\nprivaCy and ethiCs 213 is at the center of this debate is that many of the large\nUS multinational Internet companies (Google, Facebook,\nTwitter, etc.) have their European, Middle East, and Africa\nheadquarters in Ireland. As a result, the data-protection\ncommissioner for Ireland is responsible for enforcing EU\nregulations on transnational data transfers made by these\ncompanies. Recent history illustrates that it is possible\nfor legal cases to result in significant and swift changes\nin the regulation of how personnel data are handled. In\nfact, the EU-US Privacy Shield is a direct consequence of a\nsuit filed by Max Schrems, an Austrian lawyer and privacy\nactivist, against Facebook. The outcome of Schrems\u2019s case\nin 2015 was to invalidate the existing EU-US Safe Harbor\nagreement with immediate effect, and the EU-US Privacy\nShield was developed as an emergency response to this\noutcome. Compared to the original Safe Harbor agree-\nment, the Privacy Shield has strengthened EU citizens\u2019\ndata-privacy rights (O\u2019Rourke and Kerr 2017), and it may\nwell be that any new framework would further strengthen\nthese rights. For example, the EU General Data Protection\nRegulations will provide legally enforceable data protec-\ntion to EU citizens from May 2018.\nFrom a data science perspective, these examples illus-\ntrate that the regulations around data privacy and protec-\ntion are in flux. Admittedly, the examples listed here are\nfrom the US and EU contexts, but they are indicative of\nbroader trends in relation to privacy and data regulation.\n214 Chapter 6 It is very difficult to predict how these changes will play\nout in the long term. A range of vested interests exist in\nthis domain: consider the differing agendas of big Inter-\nnet, advertising and insurances companies, intelligence\nagencies, policing authorities, governments, medical and\nsocial science research, and civil liberties groups. Each of\nthese different sectors of society has differing goals and\nneeds with regard to data usage and consequently has dif-\nferent views on how data-privacy regulation should be\nshaped. Furthermore, we as individuals will probably have\nshifting views depending on the perspective we adopt.\nFor example, we might be quite happy for our personnel\ndata to be shared and reused in the context of medical re-\nsearch. However, as the public-opinion surveys in Europe\nand the United States have reported, many of us have res-\nervations about data gathering, reuse, and sharing in the\ncontext of targeted advertising. Broadly speaking, there\nare two themes in the discourse around the future of data\nprivacy. One view argues for the strengthening of regu-\nlations relating to the gathering of personal data and in\nsome cases empowering individuals to control how their\ndata are gathered, shared, and used. The other view argues\nfor deregulation in relation to the gathering of data but\nalso for stronger laws to redress the misuse of personnel\ndata. With so many different stakeholders and perspec-\ntives, there are no easy or obvious answers to the ques-\ntions posed about privacy and data. It is likely that the\nprivaCy and ethiCs 215 eventual solutions that are developed will be defined on a\nsector-by-sector basis and consist of compromises negoti-\nated between the relevant stakeholders.\nIn such a fluid context, it is best to act conservatively\nand ethically. As we work on developing new data science\nsolutions to business problems, we should consider ethi-\ncal questions in relation to personal data. There are good\nbusiness reasons to do so. First, acting ethically and trans-\nparently with personal data will ensure that a business will\nhave good relationships with its customers. Inappropriate\npractices around personal data can cause a business severe\nreputational damage and cause its customer to move to\ncompetitors (Buytendijk and Heiser 2013). Second, there\nis a risk that as data integration, reuse, profiling, and tar-\ngeting intensify, public opinion will harden around data\nprivacy in the coming years, which will lead to more-\nstringent regulations. Consciously acting transparently\nand ethically is the best way to ensure that the data science\nsolutions we develop do not run afoul of current regula-\ntions or of the regulations that may come into existence in\nthe coming years.\nAphra Kerr (2017) reports a case from 2015 that illus-\ntrates how not taking ethical considerations into account\ncan have serious consequences for technology developers\nand vendors. The case resulted in the US Federal Trade\nCommission fining app game developers and publishers\nunder the Children\u2019s Online Privacy Protection Act. The\n216 Chapter 6 developers had integrated third-party advertising into\ntheir free-to-play games. Integrating third-party advertis-\ning is standard practice in the free-to-play business model,\nbut the problem arose because the games were designed\nfor children younger than 13. As a result, in sharing their\nusers\u2019 data with advertising networks, the developers\nwhere in fact also sharing data relating to children and as\na result violated the Children\u2019s Online Privacy Protection\nAct. Also, in one instance the developers failed to inform\nthe advertising networks that the apps were for children.\nAs a result, it was possible that inappropriate advertising\ncould be shown to children, and in this instance the Federal\nTrade Commission ruled that the game publishers were re-\nsponsible for ensuring that age-appropriate content and\nadvertising were supplied to the game-playing children.\nThere has been an increasing number of these types of\ncases in recent years, and a number of organizations, in-\ncluding the Federal Trade Commission (2012), have called\nfor businesses to adopt the principles of privacy by design\n(Cavoukian 2013). These principles were developed in the\n1990s and have become a globally recognized framework\nfor the protection of privacy. They advocate that protect-\ning privacy should be the default mode of operation for\nthe design of technology and information systems. To fol-\nlow these principles requires a designer to consciously and\nproactively seek to embed privacy considerations into the\ndesign of technologies, organizational practices, and net-\nworked system architectures.\nprivaCy and ethiCs 217 Although the arguments of ethical data science are\nclear, it is not always easy to act ethically. One way to make\nthe challenge of ethical data science more concrete is to\nimagine you are working for a company as a data scientist\non a business-critical project. In analyzing the data, you\nhave identified a number of interacting attributes that\ntogether are a proxy for race (or some other personal at-\ntribute, such as religion, gender, etc.). You know that le-\ngally you can\u2019t use the race attribute in your model, but\nyou believe that these proxy attributes would enable you\nto circumvent the antidiscrimination legislation. You also\nbelieve that including these attributes in the model will\nmake your model work, although you are naturally con-\ncerned that this successful outcome may be because the\nmodel will learn to reinforce discrimination that is already\npresent in the system. Ask yourself: \u201cWhat do I do?\u201d\n218 Chapter 6 7\nFUTURE TRENDS AND PRINCIPLES\nOF SUCCESS\nAn obvious trend in modern societies is the proliferation\nof systems that can sense and react to the world: smart\nphones, smart homes, self-driving cars, and smart cities.\nThis proliferation of smart devices and sensors presents\nchallenges to our privacy, but it is also driving the growth\nof big data and the development of new technology para-\ndigms, such as the Internet of Things. In this context, data\nscience will have a growing impact across many areas of\nour lives. However, there are two areas where data science\nwill lead to significant developments in the coming decade:\npersonal medicine and the development of smart cities.\nMedical Data Science\nIn recent years, the medical industry has been looking\nat and adopting data science and predictive analytics. Doctors have traditionally had to rely on their experiences\nand instincts when diagnosing a condition or deciding on\nwhat the next treatment might be. The evidence-based\nmedicine and precision-medicine movement argue that\nmedical decisions should be based on data, ideally linking\nthe best available data to an individual patient\u2019s predica-\nment and preferences. For example, in the case of precision\nmedicine, fast genome-sequencing technology means that\nit is now feasible to analyze the genomes of patients with\nrare diseases in order to identify mutations that cause the\ndisease so as to design and select appropriate therapies\nspecific to that individual. Another factor driving data sci-\nence in medicine is the cost of health care. Data science,\nin particular predictive analytics, can be used to automate\nsome health care processes. For example, predictive ana-\nlytics has been used to decide when antibiotics and other\nmedicines should be administrated to babies and adults,\nand it is widely reported that many lives have been saved\nbecause of this approach.\nMedical sensors worn or ingested by the patient or\nimplanted are being developed to continuously monitor\na patient\u2019s vital signs and behaviors and how his or her\norgans are functioning throughout the day. These data\nare continuously gathered and fed back to a centralized\nmonitoring server. It is here at the monitoring server that\nhealth care professionals access the data being generated\nby all the patients, assess their conditions, understand\n220 Chapter 7 what effects the treatment is having, and compare each\npatient\u2019s results to those of other patients with similar\nconditions to inform them regarding what should happen\nnext in each patient\u2019s treatment regime. Medical science\nis using the data generated by these sensors and integrat-\ning it with additional data from the various parts of the\nmedical profession and the pharmaceutical industry to\ndetermine the effects of current and new medicines. Per-\nsonalized treatment programs are being developed based\non the type of patient, his condition, and how his body\nresponds to various medicines. In addition, this new type\nof medical data science is now feeding into new research\non medicines and their interactions, the design of more\nefficient and detailed monitoring systems, and the uncov-\nering of greater insights from clinical trials.\nSmart Cities\nVarious cities around the world are adopting new tech-\nnology to be able to gather and use the data generated by\ntheir citizens in order to better manage the cities\u2019 orga-\nnizations, utilities, and services. There are three core en-\nablers of this trend: data science, big data, and the Internet\nof Things. The name \u201cInternet of Things\u201d describes the\ninternetworking of physical devices and sensors so that\nthese devices can share information. This may sound\nFuture trends and prinCiples oF suCCess 221 mundane, but it has the benefit that we can now remotely\ncontrol smart devices (such as our home if it is properly\nconfigured) and opens the possibility that networked\nmachine-to-machine communication will enable smart\nenvironments to autonomously predict and react to our\nneeds (for example, there are now commercially available\nsmart refrigerators that can warn you when food is about\nto spoil and allows you to order fresh milk through your\nsmart phone).\nSmart-city projects integrate real-time data from\nmany different data sources into a single data hub, where\nthey are analyzed and used to inform management and\nplanning decisions. Some smart-city projects involve\nbuilding brand-new cities that are smart from the ground\nup. Both Masdar City in the United Arab Emirates and\nSongdo City in South Korea are brand-new cities that have\nbeen built with the smart technology at their core and a\nfocus on being eco-friendly and energy efficient. However,\nmost smart-city projects involve the retrofitting of exist-\ning cities with new sensor networks and data-processing\ncenters. For example, in the SmartSantander project in\nSpain,1 more than 12,000 networked sensors have been\ninstalled across the city to measure temperature, noise,\nambient lighting, carbon monoxide levels, and parking.\nSmart-city projects often focus on developing energy ef-\nficiency, planning and routing traffic, and planning utility\nservices to match population needs and growth.\n222 Chapter 7 Japan has embraced the smart-city concept with a par-\nticular focus on reducing energy usage. The Tokyo Electric\nPower Company (TEPC) has installed more than 10 million\nsmart meters across homes in the TEPC service area.2 At\nthe same time, TEPC is developing and rolling out smart-\nphone applications that enable customers to track the\nelectricity used in their homes in real time and to change\ntheir electricity contract. These smart-phone applications\nalso enable the TEPC to send each customer personalized\nenergy-saving advice. Outside of the home, smart-city\ntechnology can be used to reduce energy usage through\nintelligent street lighting. The Glasgow Future Cities\nDemonstrator is piloting street lighting that switches on\nand off depending on whether people are present. Energy\nefficiency is also a top priority for all new buildings, par-\nticularly for large local government and commercial build-\nings. These buildings\u2019 energy efficiency can be optimized\nby automatically managing climate controls through a\ncombination of sensor technology, big data, and data sci-\nence. An extra benefit of these smart-building monitor-\ning systems is that they can monitor for levels of pollution\nand air quality and can activate the necessary controls and\nwarnings in real time.\nTransport is another area where cities are using data\nscience. Many cities have implemented traffic-monitoring\nand management systems. These systems use real-time\ndata to control the flow of traffic through the city. For\nFuture trends and prinCiples oF suCCess 223 example, they can control traffic-light sequences in real\ntime, in some cases to give priority to public-transport\nvehicles. Data on city transport networks are also useful\nfor planning public transport. Cities are examining the\nroutes, schedules, and vehicle management to ensure that\nservices support the maximum number of people and to\nreduce the costs associated with delivering the transport\nservices. In addition to modeling the public network, data\nscience is also being used to monitor official city vehicles to\nensure their optimal usage. Such projects combine traffic\nconditions (collected by sensors along the road network,\nat traffic lights, etc.), the type of task being performed,\nand other conditions to optimize route planning, and dy-\nnamic route adjustments are fed to the vehicles with live\nupdates and changes to their routes.\nBeyond energy usage and transport, data science is be-\ning used to improve the provision of utility services and\nto implement longer-term planning of infrastructure proj-\nects. The efficient provision of utility services is constantly\nbeing monitored based on current usage and projected\nusages, and the monitoring takes into account previous\nusage in similar conditions. Utility companies are using\ndata science in a number of ways. One way is monitoring\nthe delivery network for the utility: the supply, the qual-\nity of the supply, any network issues, areas that require\nhigher-than-expected usage, automated rerouting of the\nsupply, and any anomalies in the network. Another way\n224 Chapter 7 that utility companies are using data science is in monitor-\ning their customers. They are looking for unusual usage\nthat might indicate some criminality (for example, a grow\nhouse), customers who may have altered the equipment\nand meters for the building where they live, and custom-\ners who are most likely to default on their payments. Data\nscience is also being used in examining the best way to al-\nlocate housing and associated services in city planning.\nModels of population growth are built to forecast into the\nfuture, and based on various simulations the city planners\ncan estimate when and where certain support services,\nsuch as high schools, are needed.\nData Science Project Principles: Why Projects Succeed\nor Fail\nA data science project sometimes fails insofar as it doesn\u2019t\ndeliver what was hoped for because it gets bogged down\nin some technical or political issues, does not deliver use-\nful results, and, more typically, is run once (or a couple\nof times) but never run again. Just like Leo Tolstoy\u2019s\nhappy families,3 the success of a data science project is de-\npendent on a number of factors. Successful data science\nprojects need focus, good-quality data, the right people,\nthe willingness to experiment with multiple models, in-\ntegration into the business information technology (IT)\nFuture trends and prinCiples oF suCCess 225 architecture and processes, buy-in from senior manage-\nment, and an organization\u2019s recognition that because the\nworld changes, models go out of date and need to be re-\nbuilt semiregularly. Failure in any of these areas is likely to\nresult in a failed project. This section details the common\nfactors that determine the success of data science projects\nas well as the typical reasons why data science projects fail.\nFocus\nEvery successful data science project begins by clearly\ndefining the problem that the project will help solve. In\nmany ways, this step is just common sense: it is difficult\nfor a project to be successful unless it has a clear goal.\nHaving a well-defined goal informs the decisions regard-\ning which data to use, what ML algorithms to use, how to\nevaluate the results, how the analysis and models will be\nused and deployed, and when the optimal time might be\nto go through the process again to update the analysis and\nmodels.\nData\nA well-defined question can be used to define what data\nare needed for the project. Having a clear understanding of\nwhat data are needed helps to direct the project to where\nthese required data are located. It also helps with defining\nwhat data are currently unavailable and hence identifies\nsome additional projects that can look at capturing and\n226 Chapter 7 Every successful data\nscience project begins\nby clearly defining\nthe problem that the\nproject will help solve. making available these data. It is important, however, to\nensure that the data used are good-quality data. Organi-\nzations may have applications that are poorly designed, a\nvery poor data model, and staff who are not trained cor-\nrectly to ensure that good data get entered. In fact, myriad\nfactors can lead to bad-quality data in systems. Indeed, the\nneed for good-quality data is so important that some orga-\nnizations have hired people to constantly inspect the data,\nassess the quality of the data, and then feed back ideas on\nhow to improve the quality of the data captured by the\napplications and by the people inputting the data. With-\nout good-quality data, it is very difficult for a data science\nproject to succeed.\nWhen the required data are sourced, it is always im-\nportant to check what data are being captured and used\nacross an organization. Unfortunately, the approach to\nsourcing data taken by some data science projects is to\nlook at what data are available in the transactional data-\nbases (and other data sources) and then to integrate and\nclean these data before going on to data exploration and\nanalysis. This approach completely ignores the BI team\nand any data warehouse that might exist. In many or-\nganizations, the BI and data-warehouse team is already\ngathering, cleaning, transforming, and integrating the\norganization\u2019s data into one central repository. If a data\nwarehouse already exists, then it probably contains all\nor most of the data required by a project. Therefore, a\ndata warehouse can save a significant amount of time on\n228 Chapter 7 integrating and cleaning the data. It will also have much\nmore data than the current transactional databases con-\ntain. If the data warehouse is used, it is possible to go back\na number of years, build predictive models using the his-\ntoric data, roll these models through various time periods,\nand then measure each model\u2019s level of predictive accu-\nracy. This process allows for the monitoring of changes in\nthe data and how they affect the models. In addition, it\nis possible to monitor variations in the models that are\nproduced by ML algorithms and how the models evolve\nover time. Following this kind of approach facilitates the\ndemonstration of how the models work and behave over\na number of years and helps with building up the cus-\ntomer\u2019s confidence in what is being done and what can\nbe achieved. For example, in one project where five years\nof historical data were available in the data warehouse, it\nwas possible to demonstrate that the company could have\nsaved US$40 million or more over that time period. If the\ndata warehouse had not been available or used, then it\nwould not have been possible to demonstrate this conclu-\nsion. Finally, when a project is using personal data it is\nessential to ensure that the use of this data is in line with\nthe relevant antidiscrimination and privacy regulations.\nPeople\nA successful data science project often involves a team\nof people with a blend of data science competencies and\nskills. In most organizations, a variety of people in existing\nFuture trends and prinCiples oF suCCess 229 A successful data science\nproject often involves a\nteam of people with a\nblend of data science\ncompetencies and skills. roles can and should contribute to data science projects:\npeople working with databases, people who work with the\nETL process, people who perform data integration, proj-\nect managers, business analysts, domain experts, and so\non. But organizations often still need to hire data science\nspecialists\u2014that is, people with the skills to work with big\ndata, to apply ML, and to frame real-world problems in\nterms of data-driven solutions. Successful data scientists\nare willing and able to work and communicate with the\nmanagement team, end users, and all involved to show\nand explain what and how data science can support their\nwork. It is difficult to find people who have both the re-\nquired technical skill set and the ability to communicate\nand work with people across an organization. However,\nthis blend is crucial to the success of data science projects\nin most organizations.\nModels\nIt is import to experiment with a variety of ML algorithms\nto discover which works best with the data sets. All too\noften in the literature, examples are given of cases where\nonly one ML algorithm was used. Maybe the authors are\ndiscussing the algorithm that worked best for them or that\nis their favorite. Currently there is a great deal of inter-\nest in the use of neural networks and deep learning. Many\nother algorithms can be used, however, and these alterna-\ntives should be considered and tested. Furthermore, for\nFuture trends and prinCiples oF suCCess 231 data science projects based in the EU, the General Data\nProtection Regulations, which go into effect in April\n2018, may become a factor in determining the selection\nof algorithms and model. A potential side effect of these\nregulations is that an individual\u2019s \u201cright to explanation\u201d in\nrelation to automated decision processes that affect them\nmay limit the use in some domains of complex models that\nare difficult to interpret and explain (such as deep neural\nnetwork models).\nIntegration with the Business\nWhen the goal of a data science project is being defined,\nit is vital also to define how the outputs and results of\nthe project will be deployed within the organization\u2019s IT\narchitecture and business processes. Doing so involves\nidentifying where and how the model is to be integrated\nwithin existing systems and how the generated results will\nbe used by the system end users or if the results will be fed\ninto another process. The more automated this process is,\nthe quicker the organization can respond to its customers\u2019\nchanging profile, thereby reducing costs and increasing\npotential profits. For example, if a customer-risk model is\nbuilt for the loan process in a bank, it should be built into\nthe front-end system that captures the loan application\nby the customer. That way, when the bank employee is en-\ntering the loan application, she can be given live feedback\nby the model. The employee can then use this live feedback\n232 Chapter 7 to address any issues with the customer. Another example\nis fraud detection. It can take four to six weeks to identify\na potential fraud case that needs investigation. By using\ndata science and building it into transaction-monitoring\nsystems, organizations can now detect potential fraud\ncases in near real time. By automating and integrating\ndata-driven models, quicker response times are achieved,\nand actions can be taken at the right time. If the outputs\nand models created by a project are not integrated into the\nbusiness processes, then these outputs will not be used,\nand, ultimately, the project will fail.\nBuy-in\nFor most projects in most organizations, support by se-\nnior management is crucial to the success of many data\nscience projects. However, most senior IT managers are\nvery focused on the here and now: keeping the lights on,\nmaking sure their day-to-day applications are up and run-\nning, making sure the backups and recovery processes\nare in place (and tested), and so on. Successful data sci-\nence projects are sponsored by senior business manag-\ners (rather than by an IT manager) because the former\nare focused not on the technology but on the processes\ninvolved in the data science project and how the outputs\nof the data science project can be used to the organiza-\ntion\u2019s advantage. The more focused a project sponsor is\non these factors, the more successful the project will be.\nFuture trends and prinCiples oF suCCess 233 For an organization to\nreap long-term benefits,\nit needs to build its\ncapacity to execute data\nscience projects often\nand to use the outputs\nof these projects. He or she will then act as the key to informing the rest\nof the organization about the project and selling it to\nthem. But even when data science has a senior manager\nas an internal champion, a data science strategy can still\nfail in the long term if the initial data science project is\ntreated as a box-ticking exercise. The organization should\nnot view data science as a one-off project. For an orga-\nnization to reap long-term benefits, it needs to build its\ncapacity to execute data science projects often and to use\nthe outputs of these projects. It takes long-term commit-\nment from senior management to view data science as\na strategy.\nIteration\nMost data science projects will need to be updated and\nrefreshed on a semiregular basis. For each new update\nor iteration, new data can be added, new updates can be\nadded, maybe new algorithms can be used, and so on. The\nfrequency of these iterations will vary from project to proj-\nect; it could be daily or quarterly or biannually or annually.\nChecks should be built into the productionalized data sci-\nence outputs to detect when models need updating (see\nKelleher, Mac Namee, and D\u2019Arcy 2015 for an explanation\nof how to use a stability index to identify when a model\nshould be updated).\nFuture trends and prinCiples oF suCCess 235 Final Thoughts\nHumans have always abstracted from the world and tried\nto understand it by identifying patterns in their experi-\nences of it. Data science is the latest incarnation of this\npattern-seeking behavior. However, although data science\nhas a long history, the breadth of its impact on modern life\nis without precedent. In modern societies, the words preci-\nsion, smart, targeted, and personalized are often indicative\nof data science projects: precision medicine, precision polic-\ning, precision agriculture, smart cities, smart transport, tar-\ngeted advertising, personalized entertainment. The common\nfactor across all these areas of human life is that decisions\nhave to be made: What treatment should we use for this\npatient? Where should we allocate our policing resources?\nHow much fertilizer should we spread? How many high\nschools do we need to build in the next four years? Who\nshould we send this advertisement to? What movie or\nbook should we recommend to this person? The power\nof data science to help with decision making is driving its\nadoption. Done well, data science can provide actionable\ninsight that leads to better decisions and ultimately better\noutcomes.\nData science, in its modern guise, is driven by big data,\ncomputer power, and human ingenuity from a number of\nfields of scientific endeavor (from data mining and data-\nbase research to machine learning). This book has tried to\n236 Chapter 7 provide an overview of the fundamental ideas and concepts\nrequired to understand data science. The CRISP-DM proj-\nect life cycle makes the data science process explicit and\nprovides a structure for the data science journey from data\nto wisdom: understand the problem, prepare the data, use\nML to extract patterns and create models, use the models\nto get actionable insight. The book also touches on some\nof the ethical concerns relating to individual privacy in a\ndata science world. People have genuine and well-founded\nconcerns that data science has the potential to be used by\ngovernments and vested interests to manipulate our be-\nhaviors and police our actions. We, as individuals, need\nto develop informed opinions about what type of a data\nworld we want to live in and to think about the laws we\nwant our societies to develop in order to steer the use of\ndata science in appropriate directions. Despite the ethical\nconcerns we may have around data science, the genie is\nalready very much out of the bottle: data science is hav-\ning and will continue to have significant effects on our\ndaily lives. When used appropriately, it has the potential\nto improve our lives. But if we want the organizations we\nwork with, the communities we live in, and the families\nwe share our lives with to benefit from data science, we\nneed to understand and explore what data science is, how\nit works, and what it can (and can\u2019t) do. We hope this book\nhas given you the essential foundations you need to go on\nthis journey.\nFuture trends and prinCiples oF suCCess 237  GLOSSARY\nAnalytics Base Table\nA table in which each row contains the data relating to a specific instance\nand each column describes the values of a particular attribute for each in-\nstance. These data are the basic input to data-mining and machine-learning\nalgorithms.\nAnomaly Detection\nSearching for and identifying examples of atypical data in a data set. These\nnonconforming cases are often referred to as anomalies or outliers. This process\nis often used in analyzing financial transactions to identify potential fraudu-\nlent activities and to trigger investigations.\nAssociation-Rule Mining\nAn unsupervised data-analysis technique that looks to find groups of items\nthat frequently co-occur together. The classic use case is market-basket analy-\nsis, where retail companies try to identify sets of items that are purchased\ntogether, such as the hot dogs, ketchup, and beer.\nAttribute\nEach instance in a data set is described by a number of attributes (also known\nas features or variables). An attribute captures one piece of information relating\nto an instance. An attribute can be either raw or derived.\nBackpropagation\nThe backpropagation algorithm is an ML algorithm used to train neural net-\nworks. The algorithm calculates for each neuron in a network the contribution\nthe neuron makes to the error of the network. Using this error calculation for\neach neuron it is possible to update the weights on the inputs to each neuron\nso as to reduce the overall error of the network. The backpropagation algo-\nrithm is so named because it works in a two stage process. In the first stage an\ninstance is input to the network and the information flows forward through\nthe network until the network generates a prediction for that instance. In\nthe second stage the error of the network on that instance is calculated by\ncomparing the network's prediction to the correct output for that instance (as specified by the training data) and then this error is then shared back (or\nbackpropagated) through the neurons in the network on a layer by layer basis\nbeginning at the output layer.\nBig Data\nBig data are often defined in terms of the three Vs: the extreme volume of\ndata, the variety of the data types, and the velocity at which the data must\nbe processed.\nCaptured Data\nData that are captured through a direct measurement process that is designed\nto gather the data. Contrast with exhaust data.\nClassification\nThe task of predicting a value for a target attribute of an instance based on\nthe values of a set of input attributes, where the target attribute is a nominal\nor ordinal data type.\nClustering\nIdentifying groups of similar instances in a data set.\nCorrelation\nThe strength of association between two attributes.\nCross Industry Standard Process for Data Mining (CRISP-DM)\nThe CRISP-DM defines a standard life cycle for a data-mining project. The life\ncycle is often adopted for data science projects.\nData In its most basic form, a piece of data is an abstraction (or measure-\nment) from a real-world entity (person, object, or event).\nData Analysis\nAny process for extracting useful information from data. Types of data\nanalysis include data visualization, summary statistics, correlation analysis,\nand modeling using machine learning.\nDatabase\nA central repository of data. The most common database structure is a rela-\ntional database, which stores data in tables with a structure of one row per\n240 Glossary instance and one column per attribute. This representation is ideal for storing\ndata with a clear structure that can be decomposed into natural attributes.\nData Mining\nThe process of extracting useful patterns from a data set to solve a well-defined\nproblem. CRISP-DM defines the standard life cycle for a data-mining project.\nClosely related to data science but in general not as broad in scope.\nData Science\nAn emerging field that integrates a set of problem definitions, algorithms, and\nprocesses that can be used to analyze data so as to extract actionable insight\nfrom (large) data sets. Closely related to the field of data mining but broader\nin scope and concern. Deals with both structured and unstructured (big) data\nand encompasses principles from a range of fields, including machine learn-\ning, statistics, data ethics and regulation, and high-performance computing.\nData Set\nA collection of data relating to a set of instances, with each instance described\nin terms of a set of attributes. In its most basic form, a data set is organized in\nan n * m matrix, where n is the number of instances (rows) and m is the number\nof attributes (columns).\nData Warehouse\nA centralized repository containing data from a range of sources across an or-\nganization. The data are structured to support summary reports from the ag-\ngregated data. Online analytical processing (OLAP) is the term used to describe\nthe typical operations on a data warehouse.\nDecision Tree\nA type of prediction model that encodes if-then-else rules in a tree structure.\nEach node in the tree defines one attribute to test, and a path from the root\nnode to a terminating leaf node defines a sequence of tests that an instance\nmust pass for the label of the terminating node to be predicted for that\ninstance.\nDeep Learning\nA deep-learning model is a neural network that has multiple (more than two)\nlayers of hidden units (or neurons). Deep networks are deep in terms of the\nnumber of layers of neurons in the network. Today many deep networks have\nGlossary 241 tens to hundreds of layers. The power of deep-learning models comes from the\nability of the neurons in the later layers to learn useful attributes derived from\nattributes that were themselves learned by the neurons in the earlier layers.\nDerived Attribute\nAn attribute whose value is generated by applying a function to other data\nrather than a direct measurement taken from the entity. An attribute that\ndescribes an average value in a population is an example of a derived attribute.\nContrast with raw attribute.\nDIKW Pyramid\nA model of the structural relationships between data, information, knowledge,\nand wisdom. In the DIKW pyramid, data precedes information, which precedes\nknowledge, which precedes wisdom.\nExhaust Data\nData that are a by-product of a process whose primary purpose is some-\nthing other than data capture. For example, for every image shared, tweeted,\nretweeted, or liked, a range of exhaust data is generated: who shared, who\nviewed, what device was used, what time of day, and so on. Contrast with\ncaptured data.\nExtraction, Transformation, and Load (ETL)\nETL is the term used to describe the typical processes and tools used to support\nthe mapping, merging, and movement of data between databases.\nHadoop\nHadoop is an open-source framework developed by the Apache Software Foun-\ndation that is designed for the processing of big data. It uses distributed stor-\nage and processing across clusters of commodity hardware.\nHigh-Performance Computing (HPC)\nThe field of HPC focuses on designing and implementing frameworks to con-\nnect large number of computers together so that the resulting computer clus-\nter can store and process large amounts of data efficiently.\nIn-Database Machine Learning\nUsing machine-learning algorithms that are built into the database solution.\nThe benefit of in-database machine learning is that it reduces the time spent\non moving data in and out of databases for analysis.\n242 Glossary Instance\nEach row in a data set contains the information relating to one instance (also\nknown as an example, entity, case, or record).\nInternet of Things\nThe internetworking of physical devices and sensors so that these devices can\nshare information. Includes the field of machine-to-machine communication,\nwhich develops systems that enable machines not only to share informa-\ntion but also to react to this information and trigger actions without human\ninvolvement.\nLinear Regression\nWhen a linear relationship is assumed in a regression analysis, the analysis is\ncalled linear regression. A popular type of prediction model used to estimate the\nvalue of a numeric target attribute based on a set of numeric input attributes.\nMachine Learning (ML)\nThe field of computer science research that focuses on developing and evalu-\nating algorithms that can extract useful patterns from data sets. A machine-\nlearning algorithm takes a data set as input and returns a model that encodes\nthe patterns the algorithm extracted from the data.\nMassively Parallel Processing Database (MPP)\nIn an MPP database, data is partitioned across multiple servers, and each\nserver can process the data on that server locally and independently.\nMetadata\nData describing the structures and properties of other data\u2014for example, a\ntime stamp that describes when a piece of data was collected. Metadata are one\nof the most common types of exhaust data.\nModel\nIn the context of machine learning, a model is a representation of a pattern\nextracted using machine learning from a data set. Consequently, models are\ntrained, fitted to a data set, or created by running a machine learning algo-\nrithm on a data set. Popular model representations include decision trees and\nneural networks. A prediction model defines a mapping (or function) from a\nset of input attributes to a value for a target attribute. Once a model has been\ncreated, it can then be applied to new instances from the domain. For example,\nGlossary 243 in order to train a spam filter model, we would apply a machine learning al-\ngorithm to a data set of historic emails that have been labeled as spam or not\nspam. Once the model has been trained it can be used to label (or filter) new\nemails that were not in the original data set.\nNeural Network\nA type of machine-learning model that is implemented as a network of simple\nprocessing units called neurons. It is possible to create a variety of different\ntypes of neural networks by modifying the topology of the neurons in the\nnetwork. A feed-forward, fully connected neural network is a very common\ntype of network that can be trained using backpropagation.\nNeuron\nA neuron takes a number of input values (or activations) as input and\nmaps these values to a single output activation. This mapping is typically\nimplemented by applying a multi-input linear-regression function to the\ninputs and then pushing the result of this regression function through a\nnonlinear activation function, such as the logistic or tanh function.\nOnline Analytical Processing (OLAP)\nOLAP operations generate summaries of historic data and aggregate data from\nmultiple sources. OLAP operations are designed to generate report-type sum-\nmaries and enable users to slice, dice, and pivot data in a data warehouse using\na predefined set of dimensions on the data, such as sales by stores, sale by\nquarter, and so on. Contrast with Online Transaction Processing (OLTP).\nOnline Transaction Processing (OLTP)\nOLTPs are designed for short online data transactions (such as INSERT, DE-\nLETE, UPDATE, etc.) with an emphasis on fast query processing and main-\ntaining data integrity in a multi-access environment. Contrast with OLAP\nsystems, which are designed for more complex operations on historic data.\nOperational Data Store (ODS)\nAn ODS system integrates operational or transactional data from multiple sys-\ntems to support operational reporting.\nPrediction\nIn the context of data science and machine learning, the task of estimating\nthe value of a target attribute for a given instance based on the values of other\nattributes (or input attributes) for that instance.\n244 Glossary Raw Attribute\nAn abstraction from an entity that is a direct measurement taken from the\nentity\u2014for example, a person\u2019s height. Contrast with derived attribute.\nRegression Analysis\nEstimates the expected (or average) value of a numeric target attribute when\nall the input attribute values are fixed. Regression analysis assumes a param-\neterized mathematical model of the hypothesized relationship between the\ninputs and output known as a regression function. A regression function may\nhave multiple parameters, and the focus of regression analysis is to find the\ncorrect settings for these parameters.\nRelational Database Management System (RDBMS)\nDatabase management systems based on Edgar F. Codd\u2019s relational data model.\nRelational databases store data in collection of tables where each table has a\nstructure of one row per instance and one column per attribute. Links between\ntables can be created by having key attributes appear in multiple tables. This\nstructure is suited for SQL queries which define operations on the data in the\ntables.\nSmart City\nSmart-city projects generally try to integrate real-time data from many differ-\nent data sources into a single data hub, where they are analyzed and used to\ninform city-management and planning decisions.\nStructured Data\nData that can be stored in a table. Every instance in the table has the same set\nof attributes. Contrast with unstructured data.\nStructured Query Language (SQL)\nAn international standard for defining database queries.\nSupervised Learning\nA form of machine learning in which the goal is to learn a function that maps\nfrom a set of input attribute values for an instance to an estimate of the miss-\ning value for the target attribute of the same instance.\nTarget Attribute\nIn a prediction task, the attribute that the prediction model is trained to es-\ntimate the value of.\nGlossary 245 Transactional Data\nEvent information, such as the sale of an item, the issuing of an invoice, the\ndelivery of goods, credit card payment, and so on.\nUnstructured Data\nA type of data where each instance in the data set may have its own internal\nstructure; that is, the structure is not necessarily the same in every instance.\nFor example, text data are often unstructured and require a sequence of op-\nerations to be applied to them in order to extract a structured representation\nfor each instance.\nUnsupervised Learning\nA form of machine learning in which the goal is to identify regularities in the\ndata. These regularities may include clusters of similar instances within the\ndata or regularities between attributes. In contrast to supervised learning, in\nunsupervised learning no target attribute is defined in the data set.\n246 Glossary NOTES\nChapter 1\n1. Quote taken from the call for participation sent out for the KDD workshop\nin 1989.\n2. Some practitioners do distinguish between data mining and KDD by\nviewing data mining as a subfield of KDD or a particular approach to KDD.\n3. For a recent review of this debate, see Battle of the Data Science Venn\nDiagrams (Taylor 2016).\n4. For more on the Cancer Moonshot Initiative, see https://www.cancer.gov/\nresearch/key-initiatives.\n5. For more on the All of Us program in the Precision Medicine Initiative, see\nhttps://allofus.nih.gov.\n6. For more on the Police Data Initiative, see https://www.policedatainitiative\n.org.\n7. For more on AlphaGo, see https://deepmind.com/research/alphago.\nChapter 2\n1. Although many data sets can be described as a flat n * m matrix, in some\nscenarios the data set is more complex: for example, if a data set describes\nthe evolution of multiple attributes through time, then each time point\nin the data set will be represented by a two-dimensional flat n * m matrix,\nlisting the state of the attributes at that point in time, but the overall data\nset will be three dimensional, where time is used to link the two-dimensional\nsnapshots. In these contexts, the term tensor is sometimes used to generalize\nthe matrix concept to higher dimensions.\n2. This example is inspired by an example in Han, Kamber, and Pei 2011.\nChapter 3\n1. See Storm website, at http://storm.apache.org.\nChapter 4\n1. This subheading, Correlations Are Not Causations, but Some Are Useful, is\ninspired by George E. P. Box\u2019s (1979) observation, \u201cEssentially, all models are\nwrong, but some are useful.\u201d\n2. For a numeric target, the average is the most common measure of central\ntendency, and for nominal or ordinal data the mode (or most frequently occur-\nring value is the most common measure of central tendency). 3. We are using a more complex notation here involving \u03c9 and \u03c9 because a\n0 1\nfew paragraphs later we expand this function to include more than one input\nattribute, so the subscripted variables are useful notations when dealing with\nmultiple inputs.\n4. A note of caution: the numeric values reported here should be taken as\nillustrative only and not interpreted as definitive estimates of the relationship\nbetween BMI and likelihood of diabetes.\n5. In general, neural networks work best when the inputs have similar ranges.\nIf there are large differences in the ranges of input attributes, the attributes\nwith the much larger values tend to dominate the processing of the network.\nTo avoid this, it is best to normalize the input attributes so that they all have\nsimilar ranges.\n6. For the sake of simplicity, we have not included the weights on the connec-\ntions in figures 14 and 15.\n7. Technically, the backpropagation algorithm uses the chain rule from calcu-\nlus to calculate the derivative of the error of the network with respect to each\nweight for each neuron in the network, but for this discussion we will pass over\nthis distinction between the error and the derivative of the error for the sake of\nclarity in explaining the essential idea behind the backpropagation algorithm.\n8. No agreed minimum number of hidden layers is required for a network to\nbe considered \u201cdeep,\u201d but some people would argue that even two layers are\nenough to be deep. Many deep networks have tens of layers, but some net-\nworks can have hundreds or even thousands of layers.\n9. For an accessible introduction to RNNs and their natural-language process-\ning, see Kelleher 2016.\n10. Technically, the decrease in error estimates is known as the vanishing-\ngradient problem because the gradient over the error surface disappears as the\nalgorithm works back through the network.\n11. The algorithm also terminates on two corner cases: a branch ends up with\nno instances after the data set is split up, or all the input attributes have al-\nready been used at nodes between the root node and the branch. In both cases,\na terminating node is added and is labeled with the majority value of the target\nattribute at the parent node of the branch.\n12. For an introduction to entropy and its use in decision-tree algorithms, see\nKelleher, Mac Namee, and D\u2019Arcy 2015 on information-based learning.\n13. See Burt 2017 for an introduction to the debate on the \u201cright to\nexplanation.\u201d\n248 Notes to Chapter 4 Chapter 5\n1. A customer-churn case study in Kelleher, Mac Namee, and D\u2019Arcy 2015\nprovides a longer discussion of the design of attributes in propensity\nmodels.\nChapter 6\n1. Behavioral targeting uses data from users\u2019 online activities\u2014sites visited,\nclicks made, time spent on a site, and so on\u2014and predictive modeling to select\nthe ads shown to the user.\n2. The EU Privacy and Electronic Communications Directive (2002/58/EC).\n3. For example, some expectant women explicitly tell retailers that they\nare pregnant by registering for promotional new-mother programs at the\nstores.\n4. For more on PredPol, see http://www.predpol.com.\n5. A Panopticon is an eighteenth-century design by Jeremy Bentham for in-\nstitutional buildings, such as prisons and psychiatric hospitals. The defining\ncharacteristic of a Panopticon was that the staff could observe the inmates\nwithout the inmates\u2019 knowledge. The underlying idea of this design was that\nthe inmates were forced to act as though they were being watched at all times.\n6. As distinct from digital footprint.\n7. Civil Rights Act of 1964, Pub. L. 88-352, 78 Stat. 241, at https://www.gpo\n.gov/fdsys/pkg/STATUTE-78/pdf/STATUTE-78-Pg241.pdf.\n8. Americans with Disabilities Act of 1990, Pub. L. 101-336, 104 Stat. 327,\nat https://www.gpo.gov/fdsys/pkg/STATUTE-104/pdf/STATUTE-104-Pg327\n.pdf.\n9. The Fair Information Practice Principles are available at https://www.dhs\n.gov/publication/fair-information-practice-principles-fipps.\n10. Senate of California, SB-568 Privacy: Internet: Minors, Business and\nProfessions Code, Relating to the Internet, vol. division 8, chap. 22.1 (com-\nmencing with sec. 22580) (2013), at https://leginfo.legislature.ca.gov/faces/\nbillNavClient.xhtml?bill_id=201320140SB568.\nChapter 7\n1. For more on the SmartSantander project in Spain, see http://\nsmartsantander.eu.\n2. For more on the TEPC\u2019s projects, see http://www.tepco.co.jp/en/press/\ncorp-com/release/2015/1254972_6844.html.\nNotes to Chapters 6\u20137 249 3. Leo Tolstoy\u2019s book Anna Karenina (1877) begins: \u201cAll happy families are\nalike; each unhappy family is unhappy in its own way.\u201d Tolstoy\u2019s idea is that\nto be happy, a family must be successful in a range of areas (love, finance,\nhealth, in-laws), but failure in any of these areas will result in unhappiness.\nSo all happy families are the same because they are successful in all areas,\nbut unhappy families can be unhappy for many different combinations of\nreasons.\n250 Notes to Chapter 7 FURTHER READINGS\nAbout Data and Big Data\nDavenport, Thomas H. Big Data at Work: Dispelling the Myths, Uncovering the\nOpportunities. Cambridge, MA: Harvard Business Review, 2014.\nHarkness, Timandra. Big Data: Does Size Matter? New York: Bloomsbury Sigma,\n2016.\nKitchin, Rob. The Data Revolution: Big Data, Open Data, Data Infrastructures, and\nTheir Consequences. Los Angeles: Sage, 2014.\nMayer-Sch\u00f6nberger, Viktor, and Kenneth Cukier. Big Data: A Revolution That\nWill Transform How We Live, Work, and Think. Boston: Eamon Dolan/Mariner\nBooks, 2014.\nPomerantz, Jeffrey. Metadata. Cambridge, MA: MIT Press, 2015.\nRudder, Christian. Dataclysm: Who We Are (When We Think No One\u2019s Looking).\nNew York: Broadway Books, 2014.\nAbout Data Science, Data Mining, and Machine Learning\nKelleher, John D., Brian Mac Namee, and Aoife D\u2019Arcy. Fundamentals\nof Machine Learning for Predictive Data Analytics. Cambridge, MA: MIT Press,\n2015.\nLinoff, Gordon S., and Michael J. A. Berry. Data Mining Techniques: For Mar-\nketing, Sales, and Customer Relationship Management. Indianapolis, IN: Wiley,\n2011.\nProvost, Foster, and Tom Fawcett. Data Science for Business: What You Need to\nKnow about Data Mining and Data-Analytic Thinking. Sebastopol, CA: O\u2019Reilly\nMedia, 2013.\nAbout Privacy, Ethics, and Advertising\nDwork, Cynthia, and Aaron Roth. 2014. \u201cThe Algorithmic Foundations of Dif-\nferential Privacy.\u201d Foundations and Trends\u00ae in Theoretical Computer Science 9\n(3\u20134): 211\u2013407. Nissenbaum, Helen. Privacy in Context: Technology, Policy, and the Integrity of\nSocial Life. Stanford, CA: Stanford Law Books, 2009.\nSolove, Daniel J. Nothing to Hide: The False Tradeoff between Privacy and Security.\nNew Haven, CT: Yale University Press, 2013.\nTurow, Joseph. The Daily You: How the New Advertising Industry Is Defining Your\nIdentity and Your Worth. New Haven, CT: Yale University Press, 2013.\n252 Further readings REFERENCES\nAnderson, Chris. 2008. The Long Tail: Why the Future of Business Is Selling Less\nof More. Rev. ed. New York: Hachette Books.\nBaldridge, Jason. 2015. \u201cMachine Learning and Human Bias: An Uneasy Pair.\u201d\nTechCrunch, August 2. http://social.techcrunch.com/2015/08/02/machine\n-learning-and-human-bias-an-uneasy-pair.\nBarry-Jester, Anna Maria, Ben Casselman, and Dana Goldstein. 2015.\n\u201cShould Prison Sentences Be Based on Crimes That Haven\u2019t Been Commit-\nted Yet?\u201d FiveThirtyEight, August 4. https://fivethirtyeight.com/features/\nprison-reform-risk-assessment.\nBatty, Mike, Arun Tripathi, Alice Kroll, Peter Wu Cheng-sheng, David Moore,\nChris Stehno, Lucas Lau, Jim Guszcza, and Mitch Katcher. 2010. \u201cPredictive\nModeling for Life Insurance: Ways Life Insurers Can Participate in the Busi-\nness Analytics Revolution.\u201d Society of Actuaries. https://www.soa.org/files/\npdf/research-pred-mod-life-batty.pdf.\nBeales, Howard. 2010. \u201cThe Value of Behavioral Targeting.\u201d Network\nAdvertising Initiative. http://www.networkadvertising.org/pdfs/Beales_NA\nI_Study.pdf.\nBerk, Richard A., and Justin Bleich. 2013. \u201cStatistical Procedures for Forecast-\ning Criminal Behavior.\u201d Criminology & Public Policy 12 (3): 513\u2013544.\nBox, George E. P. 1979. \u201cRobustness in the Strategy of Scientific Model Build-\ning,\u201d in Robustness in Statistics, ed. R. L. Launer and G. N. Wilkinson, 201\u2013236.\nNew York: Academic Press.\nBreiman, Leo. 2001. \u201cStatistical Modeling: The Two Cultures (with Com-\nments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231.\ndoi:10.1214/ss/1009213726.\nBrown, Meta S. 2014. Data Mining for Dummies. New York: Wiley. http://\nwww.wiley.com/WileyCDA/WileyTitle/productCd-1118893174,subjectCd\n-STB0.html.\nBrynjolfsson, Erik, Lorin M. Hitt, and Heekyung Hellen Kim. 2011. \u201cStrength in\nNumbers: How Does Data-Driven Decisionmaking Affect Firm Performance?\u201d SSRN Scholarly Paper ID 1819486. Social Science Research Network, Roches-\nter, NY. https://papers.ssrn.com/abstract=1819486.\nBurt, Andrew. 2017. \u201cIs There a \u2018Right to Explanation\u2019 for Machine Learning\nin the GDPR?\u201d https://iapp.org/news/a/is-there-a-right-to-explanation-for\n-machine-learning-in-the-gdpr.\nBuytendijk, Frank, and Jay Heiser. 2013. \u201cConfronting the Privacy and Ethical\nRisks of Big Data.\u201d Financial Times, September 24. https://www.ft.com/content/\n105e30a4-2549-11e3-b349-00144feab7de.\nCarroll, Rory. 2013. \u201cWelcome to Utah, the NSA\u2019s Desert Home for Eaves-\ndropping on America.\u201d Guardian, June 14. https://www.theguardian.com/\nworld/2013/jun/14/nsa-utah-data-facility.\nCavoukian, Ann. 2013. \u201cPrivacy by Design: The 7 Foundation Principles\n(Primer).\u201d Information and Privacy Commissioner, Ontario, Canada. https://\nwww.ipc.on.ca/wp-content/uploads/2013/09/pbd-primer.pdf.\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas\nReinartz, Colin Shearer, and Rudiger Wirth. 1999. \u201cCRISP-DM 1.0: Step-by-\nStep Data Mining Guide.\u201d ftp://ftp.software.ibm.com/software/analytics/\nspss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf.\nCharter of Fundamental Rights of the European Union. 2000. Official Journal\nof the European Communities C (364): 1\u201322.\nCleveland, William S. 2001. \u201cData Science: An Action Plan for Expanding the\nTechnical Areas of the Field of Statistics.\u201d International Statistical Review 69 (1):\n21\u201326. doi:10.1111/j.1751-5823.2001.tb00477.x.\nClifford, Stephanie. 2012. \u201cSupermarkets Try Customizing Prices for\nShoppers.\u201d New York Times, August 9. http://www.nytimes.com/2012/08/10/\nbusiness/supermarkets-try-customizing-prices-for-shoppers.html.\nCouncil of the European Union and European Parliament. 1995. \u201c95/46/EC\nof the European Parliament and of the Council of 24 October 1995 on the Pro-\ntection of Individuals with Regard to the Processing of Personal Data and on\nthe Free Movement of Such Data.\u201d Official Journal of the European Community\nL 281:38-1995): 31\u201350.\nCouncil of the European Union and European Parliament. 2016. \u201cGeneral\nData Protection Regulation of the European Council and Parliament.\u201d Official\n254 RefeRences Journal of the European Union L 119: 1\u20132016. http://ec.europa.eu/justice/\ndata-protection/reform/files/regulation_oj_en.pdf.\nCrowdFlower. 2016. 2016 Data Science Report. http://visit.crowdflower.com/\nrs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf.\nDatta, Amit, Michael Carl Tschantz, and Anupam Datta. 2015. \u201cAutomated\nExperiments on Ad Privacy Settings.\u201d Proceedings on Privacy Enhancing\nTechnologies 2015 (1): 92\u2013112.\nDeZyre. 2015. \u201cHow Big Data Analysis Helped Increase Walmart\u2019s Sales Turn-\nover.\u201d May 23. https://www.dezyre.com/article/how-big-data-analysis-helped\n-increase-walmarts-sales-turnover/109.\nDodge, Martin, and Rob Kitchin. 2007. \u201cThe Automatic Management of\nDrivers and Driving Spaces.\u201d Geoforum 38 (2): 264\u2013275.\nDokoupil, Tony. 2013. \u201c\u2018Small World of Murder\u2019: As Homicides Drop,\nChicago Police Focus on Social Networks of Gangs.\u201d NBC News, December 17.\nhttp://www.nbcnews.com/news/other/small-world-murder-homicides-drop\n-chicago-police-focus-social-networks-f2D11758025.\nDuhigg, Charles. 2012. \u201cHow Companies Learn Your Secrets.\u201d New York\nTimes, February 16. http://www.nytimes.com/2012/02/19/magazine/\nshopping-habits.html.\nDwork, Cynthia, and Aaron Roth. 2014. \u201cThe Algorithmic Foundations of\nDifferential Privacy.\u201d Foundations and Trends\u00ae in Theoretical Computer Science\n9 (3\u20134): 211\u2013407.\nEliot, T. S. 1934 [1952]. \u201cChoruses from \u2018The Rock.\u2019\u201d In T. S. Eliot: The Complete\nPoems and Plays\u20141909\u20131950. San Diego: Harcourt, Brace and Co.\nElliott, Christopher. 2004. \u201cBUSINESS TRAVEL; Some Rental Cars Are Keep-\ning Tabs on the Drivers.\u201d New York Times, January 13. http://www.nytimes\n.com/2004/01/13/business/business-travel-some-rental-cars-are-keeping\n-tabs-on-the-drivers.html.\nEurobarometer. 2015. \u201cData Protection.\u201d Special Eurobarometer 431. http://\nec.europa.eu/COMMFrontOffice/publicopinion/index.cfm/Survey/index#p\n=1&instruments=SPECIAL.\nEuropean Commission. 2012. \u201cCommission Proposes a Comprehensive Reform\nof the Data Protection Rules\u2014European Commission.\u201d January 25. http://\nec.europa.eu/justice/newsroom/data-protection/news/120125_en.htm.\nRefeRences 255 European Commission. 2016. \u201cThe EU-U.S. Privacy Shield.\u201d December 7.\nhttp://ec.europa.eu/justice/data-protection/international-transfers/eu-us\n-privacy-shield/index_en.htm.\nFederal Trade Commission. 2012. Protecting Consumer Privacy in an Era of\nRapid Change. Washington, DC: Federal Trade Commission. https://www\n.ftc.gov/sites/default/files/documents/reports/federal-trade-commission\n-report-protecting-consumer-privacy-era-rapid-change-recommendations/12\n0326privacyreport.pdf.\nFew, Stephen. 2012. Show Me the Numbers: Designing Tables and Graphs to\nEnlighten. 2nd ed. Burlingame, CA: Analytics Press.\nGoldfarb, Avi, and Catherine E. Tucker. 2011. Online Advertising, Behavioral\nTargeting, and Privacy. Communications of the ACM 54 (5): 25\u201327.\nGorner, Jeremy. 2013. \u201cChicago Police Use Heat List as Strategy to Prevent\nViolence.\u201d Chicago Tribune, August 21. http://articles.chicagotribune.com/\n2013-08-21/news/ct-met-heat-list-20130821_1_chicago-police-commander\n-andrew-papachristos-heat-list.\nHall, Mark, Ian Witten, and Eibe Frank. 2011. Data Mining: Practical Machine\nLearning Tools and Techniques. Amsterdam: Morgan Kaufmann.\nHan, Jiawei, Micheline Kamber, and Jian Pei. 2011. Data Mining: Concepts and\nTechniques. 3rd ed. Haryana, India: Morgan Kaufmann.\nHarkness, Timandra. 2016. Big Data: Does Size Matter? New York: Bloomsbury\nSigma.\nHenke, Nicolaus, Jacques Bughin, Michael Chui, James Manyika, Tamim\nSaleh, and Bill Wiseman. 2016. The Age of Analytics: Competing in a Data-\nDriven World. Chicago: McKinsey Global Institute. http://www.mckinsey.com/\nbusiness-functions/mckinsey-analytics/our-insights/the-age-of-analytics\n-competing-in-a-data-driven-world.\nHill, Shawndra, Foster Provost, and Chris Volinsky. 2006. Network-Based\nMarketing: Identifying Likely Adopters via Consumer Networks. Statistical\nScience 21 (2): 256\u2013276. doi:10.1214/088342306000000222.\nHunt, Priscillia, Jessica Saunders, and John S. Hollywood. 2014. Evaluation\nof the Shreveport Predictive Policing Experiment. Santa Monica, CA: Rand Corpo-\nration. http://www.rand.org/pubs/research_reports/RR531.\n256 RefeRences Innes, Martin. 2001. Control Creep. Sociological Research Online 6 (3). https://\nideas.repec.org/a/sro/srosro/2001-45-2.html.\nKelleher, John D. 2016. \u201cFundamentals of Machine Learning for Neural\nMachine Translation.\u201d In Proceedings of the European Translation Forum, 1\u201315.\nBrussels: European Commission Directorate-General for Translation. https://\ntinyurl.com/RecurrentNeuralNetworks.\nKelleher, John D., Brian Mac Namee, and Aoife D\u2019Arcy. 2015. Fundamentals\nof Machine Learning for Predictive Data Analytics. Cambridge, MA: MIT Press.\nKerr, Aphra. 2017. Global Games: Production, Circulation, and Policy in the Net-\nworked Era. New York: Routledge.\nKitchin, Rob. 2014a. The Data Revolution: Big Data, Open Data, Data Infrastruc-\ntures, and Their Consequences. Los Angeles: Sage.\nKitchin, Rob. 2014b. \u201cThe Real-Time City? Big Data and Smart Urbanism.\u201d\nGeoJournal 79 (1): 1\u201314. doi:10.1007/s10708-013-9516-8.\nKoops, Bert-Jaap. 2011. \u201cForgetting Footprints, Shunning Shadows: A Criti-\ncal Analysis of the \u2018Right to Be Forgotten\u2019 in Big Data Practice.\u201d Tilburg Law\nSchool Legal Studies Research Paper no. 08/2012. SCRIPTed 8 (3): 229\u201356.\ndoi:10.2139/ssrn.1986719.\nKorzybski, Alfred. 1996. \u201cOn Structure.\u201d In Science and Sanity: An Introduc-\ntion to Non-Aristotelian Systems and General Semantics, CD-ROM, ed. Charlotte\nSchuchardt-Read. Englewood, NJ: Institute of General Semantics. http://\nesgs.free.fr/uk/art/sands.htm.\nKosinski, Michal, David Stillwell, and Thore Graepel. 2013. \u201cPrivate Traits and\nAttributes Are Predictable from Digital Records of Human Behavior.\u201d Proceed-\nings of the National Academy of Sciences of the United States of America 110 (15):\n5802\u20135805. doi:10.1073/pnas.1218772110.\nLe Cun, Yann. 1989. Generalization and Network Design Strategies. Technical\nReport CRG-TR-89\u20134. Toronto: University of Toronto Connectionist Research\nGroup.\nLevitt, Steven D., and Stephen J. Dubner. 2009. Freakonomics: A Rogue Economist\nExplores the Hidden Side of Everything. New York: William Morrow Paperbacks.\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair Game. New York:\nNorton.\nRefeRences 257 Linoff, Gordon S., and Michael J.A. Berry. 2011. Data Mining Techniques: For\nMarketing, Sales, and Customer Relationship Management. Indianapolis, IN:\nWiley.\nManyika, James, Michael Chui, Brad Brown, Jacques Bughin, Richard Dobbs,\nCharles Roxburgh, and Angela Hung Byers. 2011. Big Data: The Next Fron-\ntier for Innovation, Competition, and Productivity. Chicago: McKinsey Global\nInstitute. http://www.mckinsey.com/business-functions/digital-mckinsey/\nour-insights/big-data-the-next-frontier-for-innovation.\nMarr, Bernard. 2015. Big Data: Using SMART Big Data, Analytics, and Metrics to\nMake Better Decisions and Improve Performance. Chichester, UK: Wiley.\nMayer, J. R., and J. C. Mitchell. 2012. \u201cThird-Party Web Tracking: Policy and\nTechnology.\u201d In 2012 IEEE Symposium on Security and Privacy, 413\u201327. Piscat-\naway, NJ: IEEE. doi:10.1109/SP.2012.47.\nMayer, Jonathan, and Patrick Mutchler. 2014. \u201cMetaPhone: The Sensitivity of\nTelephone Metadata.\u201d Web Policy, March 12. http://webpolicy.org/2014/03/12/\nmetaphone-the-sensitivity-of-telephone-metadata.\nMayer-Sch\u00f6nberger, Viktor, and Kenneth Cukier. 2014. Big Data: A Revolution\nThat Will Transform How We Live, Work, and Think. Reprint. Boston: Eamon\nDolan/Mariner Books.\nMcMahan, Brendan, and Daniel Ramage. 2017. \u201cFederated Learning: Collab-\norative Machine Learning without Centralized Training Data.\u201d Google Research\nBlog, April. https://research.googleblog.com/2017/04/federated-learning\n-collaborative.html.\nNilsson, Nils. 1965. Learning Machines: Foundations of Trainable Pattern-\nClassifying Systems. New York: McGraw-Hill.\nOakland Privacy Working Group. 2015. \u201cPredPol: An Open Letter to the\nOakland City Council.\u201d June 25. https://www.indybay.org/newsitems/2015/\n06/25/18773987.php.\nOrganisation for Economic Co-operation and Development (OECD). 1980.\nGuidelines on the Protection of Privacy and Transborder Flows of Personal\nData. Paris: OECD. https://www.oecd.org/sti/ieconomy/oecdguidelinesonthe\nprotectionofprivacyandtransborderflowsofpersonaldata.htm.\n258 RefeRences Organisation for Economic Co-operation and Development (OECD). 2013.\n2013 OECD Privacy Guidelines. Paris: OECD. https://www.oecd.org/internet/\nieconomy/privacy-guidelines.htm.\nO\u2019Rourke, Crist\u00edn, and Aphra Kerr. 2017. \u201cPrivacy Schield for Whom? Key Ac-\ntors and Privacy Discourse on Twitter and in Newspapers.\u201d In \u201cRedesigning or\nRedefining Privacy?,\u201d special issue of Westminster Papers in Communication and\nCulture 12 (3): 21\u201336. doi:http://doi.org/ 10.16997/wpcc.264.\nPomerantz, Jeffrey. 2015. Metadata. Cambridge, MA: MIT Press. https://\nmitpress.mit.edu/books/metadata-0.\nPurcell, Kristen, Joanna Brenner, and Lee Rainie. 2012. \u201cSearch Engine Use\n2012.\u201d Pew Research Center, March 9. http://www.pewinternet.org/2012/\n03/09/main-findings-11/.\nQuinlan, J. R. 1986. \u201cInduction of Decision Trees.\u201d Machine Learning 1 (1):\n81\u2013106. doi:10.1023/A:1022643204877.\nRainie, Lee, and Mary Madden. 2015. \u201cAmericans\u2019 Privacy Strategies Post-\nSnowden.\u201d Pew Research Center, March. http://www.pewinternet.org/files/\n2015/03/PI_AmericansPrivacyStrategies_0316151.pdf.\nRhee, Nissa. 2016. \u201cStudy Casts Doubt on Chicago Police\u2019s Secretive \u2018Heat\nList.\u2019\u201d Chicago Magazine, August 17. http://www.chicagomag.com/city-life/\nAugust-2016/Chicago-Police-Data/.\nSaunders, Jessica, Priscillia Hunt, and John S. Hollywood. 2016. \u201cPredictions\nPut into Practice: A Quasi-Experimental Evaluation of Chicago\u2019s Predictive\nPolicing Pilot.\u201d Journal of Experimental Criminology 12 (3): 347\u2013371.\ndoi:10.1007/s11292-016-9272-0.\nShmueli, Galit. 2010. \u201cTo Explain or to Predict?\u201d Statistical Science 25 (3):\n289\u2013310. doi:10.1214/10-STS330.\nShubber, Kadhim. 2013. \u201cA Simple Guide to GCHQ\u2019s Internet Surveillance\nProgramme Tempora.\u201d WIRED UK, July 24. http://www.wired.co.uk/article/\ngchq-tempora-101.\nSilver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,\nGeorge van den Driessche, Julian Schrittwieser, et al. 2016. \u201cMastering the\nGame of Go with Deep Neural Networks and Tree Search.\u201d Nature 529 (7587):\n484\u2013489. doi:10.1038/nature16961.\nRefeRences 259 Soldatov, Andrei, and Irina Borogan. 2012. \u201cIn Ex-Soviet States, Russian Spy\nTech Still Watches You.\u201d WIRED, December 21. https://www.wired.com/2012/\n12/russias-hand.\nSteinberg, Dan. 2013. \u201cHow Much Time Needs to Be Spent Preparing Data\nfor Analysis?\u201d http://info.salford-systems.com/blog/bid/299181/How-Much\n-Time-Needs-to-be-Spent-Preparing-Data-for-Analysis.\nTaylor, David. 2016. \u201cBattle of the Data Science Venn Diagrams.\u201d KDnug-\ngets, October. http://www.kdnuggets.com/2016/10/battle-data-science-venn\n-diagrams.html.\nTufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed.\nCheshire, CT: Graphics Press.\nTurow, Joseph. 2013. The Daily You: How the New Advertising Industry Is Defin-\ning Your Identity and Your Worth. New Haven, CT: Yale University Press.\nVerbeke, Wouter, David Martens, Christophe Mues, and Bart Baesens. 2011.\n\u201cBuilding Comprehensible Customer Churn Prediction Models with Ad-\nvanced Rule Induction Techniques.\u201d Expert Systems with Applications 38 (3):\n2354\u20132364.\nWeissman, Cale Gutherie. 2015. \u201cThe NYPD\u2019s Newest Technology May Be Record-\ning Conversations.\u201d Business Insider, March 26. http://uk.businessinsider.com/\nthe-nypds-newest-technology-may-be-recording-conversations-2015-3.\nWolpert, D. H., and W. G. Macready. 1997. \u201cNo Free Lunch Theorems for\nOptimization.\u201d IEEE Transactions on Evolutionary Computation 1 (1): 67\u201382.\ndoi:10.1109/4235.585893.\n260 RefeRences INDEX\nAdvertising, 170, 181, 184, 185, nominal, 42\u201348, 132, 136, 142,\n186, 188, 191, 211, 213\u2013217, 148\n236 numeric, 42\u201348, 94, 102\u2013104,\nAlgorithm 106, 110, 115, 121, 136, 138,\nApriori, 166\u2013171 142, 148, 156\nbackpropagation, 129\u2013131 ordinal, 42\u201348, 132, 136, 142, 148\nID3, 137\u2013142 raw, 51, 109, 245\nk-means, 14, 156\u2013159 target (see Machine learning)\nleast squares, 12, 116\u2013120\nAlphaGo, 31\u201332 Backpropagation. See Algorithm\nAnalytics record, 39\u201341, 48, 96 Bias, 46\u201348, 143\u2013144\nAnomaly detection, 160\u2013164, 166, learning bias, 99\u2013101, 144\n239 sample bias, 143\nAntidiscrimination legislation, 21, Big data, 2, 9\u201310, 15\u201316, 19, 21\u201322,\n205\u2013206, 218 30\u201331, 35, 70\u201372, 75, 77, 84,\nArtificial intelligence, 12, 14, 49 86\u201392, 181, 183, 210, 213, 219,\nAssociation-rule mining, 151, 221, 223, 231, 236, 240\n164\u2013171, 239 Body Mass Index (BMI), 51,\nAttribute, 239 109\u2013113, 117\u2013120, 126, 134,\nbinary, 43 143\nderived, 49\u201351, 96, 109, 112\u2013113, Business intelligence, 72\u201373, 75,\n126, 242 228 Children\u2019s Online Privacy Protection recurrent neural networks,\nAct, 216\u2013217 132\u2013133\nChurn, 5, 171\u2013178 representation learning, 134\nCluster analysis, 2, 14, 102\u2013104, Decision tree. See Machine learning\n151\u2013159, 161\u2013162, 166, 240 Differential privacy, 201\u2013203\nCorrelation, 96, 105\u2013114, 120, 127, DIKW pyramid, 55, 242\n166, 169, 240\nCRISP-DM, 56\u201367, 92, 97\u201398, 144, Entropy, 138\u2013139\n150, 152, 237, 240 Extraction, Transformation, and\nLoad (ETL), 21, 74, 87, 231,\nData 242\ncaptured, 51, 53, 240\nexhaust, 51\u201354, 242 Feature. See Attribute\nmetadata, 9, 52\u201354, 243 Federated learning, 201, 203, 205\noperational, 8, 70 Flink, 87\u201388\nstructured, 1, 8, 10, 48, 92, 245\ntransactional, 7\u20138, 86, 155, General Data Protection\n228\u2013229, 244 Regulations, 149, 206, 208,\nunstructured, 2, 19, 22, 48\u201349, 77, 214\u2013215, 232\n92, 244\nDatabase Hadoop, 11, 19, 22, 71, 75\u201378, 80,\nhybrid, 88\u201391 86\u201391, 242\nNoSQL, 9\u201310, 22, High-performance computing\nRelational Database Management (HPC), 22, 242\nSystem (RDBMS), 72, 91, 245\nrelational data model, 7, 15, In-database machine learning,\n74 82\u201385, 242\nData cube, 74\u201375 Internet of Things (IoT), 164, 219,\nData mining, 1, 16\u201317, 34, 236, 221\u2013222, 243\n241\nData visualization, 12\u201313, 23 Knowledge discovery in databases\nData warehouse, 8, 16, 40, 60, 71, (KDD), 16\u201317\n73\u201376, 78\u201379, 90\u201391, 160,\n228\u2013229, 241 Linear regression. See Machine\nData wrangling, 22 learning\nDeep learning, 15, 31\u201333, 35, 121,\n131\u2013135, 231, 241\u2013242 Machine learning, 1, 14\u201316, 23\u201325,\nconvolutional neural networks, 33, 61\u201362, 97\u2013150, 236,\n133\u2013134 243\n262 Index classification, 2, 4, 14, 78, Moneyball, 28, 47\n136\u2013137, 142, 151, 171\u2013179, Massively Parallel Processing (MPP),\n240 71, 73, 243\nconcept drift, 150\ndecision trees, 14, 61, 98, 105, Neural networks. See Deep learning;\n136\u2013143, 150, 152, 162, Machine learning\n241\nevaluation, 145\u2013148 OECD, 207, 208, 210, 212\nlearning bias (see Bias) Online Analytical Processing\nlinear regression, 12, 18, 61, 79, (OLAP), 71, 74\u201375, 244\n98, 105, 114\u2013123, 143\u2013144, Online Transaction Processing\n136, 243 (OLTP), 70\u201371, 244\nno free lunch theorem, 144 Operational Data Store (ODS), 73,\nneural networks, 12, 14\u201315, 31, 244\n33, 61, 98, 105, 120, 121\u2013136, Outliers, 23, 61, 119, 160\n149, 231\u2013232, 244 Outlier detection. See Anomaly\nprediction, 2, 15, 18, 35, 78, detection\n84, 104\u2013105, 113\u2013114, 117,\n127\u2013128, 137, 142, 145, Precision medicine, 26\u201327,\n151\u2013152, 162\u2013163, 172\u2013180, 219\u2013221, 236\n187\u2013188, 192, 195, Predictive Policing (PredPol),\n244 191\u2013195, 198\nrandom-forest model, 142 Privacy by design, 217\nstationarity assumption, 150 Privacy shield, 213\u2013214\nsum of squared errors, 116\u2013117, Propensity modeling, 171\u2013172\n148\nsupervised, 99\u2013100, 105, 129, RDBMS. See Database\n245 Recommender system, 25\u201326\ntarget attribute, 99\u2013100, 104\u2013105, Regression, 12, 98, 114\u2013116,\n110, 112\u2013117, 120, 127, 120, 143, 151, 178\u2013180,\n137\u2013138, 148, 151, 173\u2013174, 245\n179, 245 Relational data model. See\nunsupervised, 99\u2013104, 165, Database\n246 Right to be forgotten, 212\u2013213\nMapReduce, 11, 75\u201376\nMarket-basket analysis, 165\u2013171 Segmentation. See Cluster analysis\nMarketing, 25\u201326, 54, 60, 152\u2013155, Smart City, 27, 196, 219, 221\u2013223,\n159, 166, 169\u2013170, 172, 236, 245\n184\u2013187, 190, 196 Spark, 87\u201388\nIndex 263 Storm, 87\nStructured Query Language (SQL),\n7, 75\u201376, 81\u201382, 84, 90,\n160\u2013161, 245\nTarget, pregnancy prediction,\n187\u2013188\nTarget attribute. See Machine\nlearning\nVariable. See Attribute\n264 Index  JOHN D. KELLEHER is a Professor of Computer Science and the Academic\nLeader of the Information, Communication, and Entertainment Research In-\nstitute at the Dublin Institute of Technology. His research is supported by\nthe ADAPT Centre, which is funded by Science Foundation Ireland (Grant 13/\nRC/2106) and co-funded by the European Regional Development fund. He is\nthe coauthor of Fundamentals of Machine Learning for Predictive Data Analytics\n(MIT Press).\nBRENDAN TIERNEY is a lecturer in the School of Computing at the Dublin\nInstitute of Technology, an Oracle ACE director, and the author of a number\nof books on data mining using Oracle technology.",
    "data/library/Deep_Learning.txt": "",
    "data/library/Distributed_Systems.txt": "",
    "data/library/Elon_Musk.txt": "  DEDICATION\nFor Mum and Dad. Thanks for Everything. CONTENTS\nDEDICATION\n1 ELON\u2019S WORLD\n2 AFRICA\n3 CANADA\n4 ELON\u2019S FIRST START-UP\n5 PAYPAL MAFIA BOSS\n6 MICE IN SPACE\nPHOTOGRAPHIC INSERT\n7 ALL ELECTRIC\n8 PAIN, SUFFERING, AND SURVIVAL\n9 LIFTOFF\n10 THE REVENGE OF THE ELECTRIC CAR\n11 THE UNIFIED FIELD THEORY OF ELON MUSK\nEPILOGUE\nAPPENDIX 1\nAPPENDIX 2\nAPPENDIX 3\nACKNOWLEDGMENTS\nNOTES\nABOUT THE AUTHOR\nALSO BY ASHLEE VANCE\nCREDITS\nCOPYRIGHT\nABOUT THE PUBLISHER  1 ELON\u2019S WORLD\nD\nO YOU THINK I\u2019M INSANE?\u201d\nThis question came from Elon Musk near the very end of a long dinner\nwe shared at a high-end seafood restaurant in Silicon Valley. I\u2019d gotten to\nthe restaurant first and settled down with a gin and tonic, knowing Musk\nwould\u2014as ever\u2014be late. After about fifteen minutes, Musk showed up\nwearing leather shoes, designer jeans, and a plaid dress shirt. Musk stands\nsix foot one but ask anyone who knows him and they\u2019ll confirm that he\nseems much bigger than that. He\u2019s absurdly broad-shouldered, sturdy, and\nthick. You\u2019d figure he would use this frame to his advantage and perform an\nalpha-male strut when entering a room. Instead, he tends to be almost\nsheepish. It\u2019s head tilted slightly down while walking, a quick handshake\nhello after reaching the table, and then butt in seat. From there, Musk needs\na few minutes before he warms up and looks at ease.\nMusk asked me to dinner for a negotiation of sorts. Eighteen months\nearlier, I\u2019d informed him of my plans to write a book about him, and he\u2019d\ninformed me of his plans not to cooperate. His rejection stung but thrust me\ninto dogged reporter mode. If I had to do this book without him, so be it.\nPlenty of people had left Musk\u2019s companies, Tesla Motors and SpaceX, and\nwould talk, and I already knew a lot of his friends. The interviews followed\none after another, month after month, and two hundred or so people into the\nprocess, I heard from Musk once again. He called me at home and declared\nthat things could go one of two ways: he could make my life very difficult\nor he could help with the project after all. He\u2019d be willing to cooperate if he\ncould read the book before it went to publication, and could add footnotes\nthroughout it. He would not meddle with my text, but he wanted the chance\nto set the record straight in spots that he deemed factually inaccurate. I\nunderstood where this was coming from. Musk wanted a measure of control\nover his life\u2019s story. He\u2019s also wired like a scientist and suffers mental\nanguish at the sight of a factual error. A mistake on a printed page would\ngnaw at his soul\u2014forever. While I could understand his perspective, I could not let him read the book, for professional, personal, and practical reasons.\nMusk has his version of the truth, and it\u2019s not always the version of the truth\nthat the rest of the world shares. He\u2019s prone to verbose answers to even the\nsimplest of questions as well, and the thought of thirty-page footnotes\nseemed all too real. Still, we agreed to have dinner, chat all this out, and see\nwhere it left us.\nOur conversation began with a discussion of public-relations people.\nMusk burns through PR staffers notoriously fast, and Tesla was in the\nprocess of hunting for a new communications chief. \u201cWho is the best PR\nperson in the world?\u201d he asked in a very Muskian fashion. Then we talked\nabout mutual acquaintances, Howard Hughes, and the Tesla factory. When\nthe waiter stopped by to take our order, Musk asked for suggestions that\nwould work with his low-carb diet. He settled on chunks of fried lobster\nsoaked in black squid ink. The negotiation hadn\u2019t begun, and Musk was\nalready dishing. He opened up about the major fear keeping him up at night:\nnamely that Google\u2019s cofounder and CEO Larry Page might well have been\nbuilding a fleet of artificial-intelligence-enhanced robots capable of\ndestroying mankind. \u201cI\u2019m really worried about this,\u201d Musk said. It didn\u2019t\nmake Musk feel any better that he and Page were very close friends and that\nhe felt Page was fundamentally a well-intentioned person and not Dr. Evil.\nIn fact, that was sort of the problem. Page\u2019s nice-guy nature left him\nassuming that the machines would forever do our bidding. \u201cI\u2019m not as\noptimistic,\u201d Musk said. \u201cHe could produce something evil by accident.\u201d As\nthe food arrived, Musk consumed it. That is, he didn\u2019t eat it as much as he\nmade it disappear rapidly with a few gargantuan bites. Desperate to keep\nMusk happy and chatting, I handed him a big chunk of steak from my plate.\nThe plan worked . . . for all of ninety seconds. Meat. Hunk. Gone.\nIt took awhile to get Musk off the artificial intelligence doom-and-\ngloom talk and to the subject at hand. Then, as we drifted toward the book,\nMusk started to feel me out, probing exactly why it was that I wanted to\nwrite about him and calculating my intentions. When the moment presented\nitself, I moved in and seized the conversation. Some adrenaline released and\nmixed with the gin, and I launched into what was meant to be a forty-five-\nminute sermon about all the reasons Musk should let me burrow deep into\nhis life and do so while getting exactly none of the controls he wanted in\nreturn. The speech revolved around the inherent limitations of footnotes,\nMusk coming off like a control freak and my journalistic integrity being compromised. To my great surprise, Musk cut me off after a couple of\nminutes and simply said, \u201cOkay.\u201d One thing that Musk holds in the highest\nregard is resolve, and he respects people who continue on after being told\nno. Dozens of other journalists had asked him to help with a book before,\nbut I\u2019d been the only annoying asshole who continued on after Musk\u2019s\ninitial rejection, and he seemed to like that.\nThe dinner wound down with pleasant conversation and Musk laying\nwaste to the low-carb diet. A waiter showed up with a giant yellow cotton\ncandy desert sculpture, and Musk dug into it, ripping off handfuls of the\nsugary fluff. It was settled. Musk granted me access to the executives at his\ncompanies, his friends, and his family. He would meet me for dinner once a\nmonth for as long as it took. For the first time, Musk would let a reporter\nsee the inner workings of his world. Two and a half hours after we started,\nMusk put his hands on the table, made a move to get up, and then paused,\nlocked eyes with me, and busted out that incredible question: \u201cDo you think\nI\u2019m insane?\u201d The oddity of the moment left me speechless for a beat, while\nmy every synapse fired trying to figure out if this was some sort of riddle,\nand, if so, how it should be answered artfully. It was only after I\u2019d spent lots\nof time with Musk that I realized the question was more for him than me.\nNothing I said would have mattered. Musk was stopping one last time and\nwondering aloud if I could be trusted and then looking into my eyes to\nmake his judgment. A split second later, we shook hands and Musk drove\noff in a red Tesla Model S sedan.\nANY STUDY OF ELON MUSK must begin at the headquarters of SpaceX,\nin Hawthorne, California\u2014a suburb of Los Angeles located a few miles\nfrom Los Angeles International Airport. It\u2019s there that visitors will find two\ngiant posters of Mars hanging side by side on the wall leading up to Musk\u2019s\ncubicle. The poster to the left depicts Mars as it is today\u2014a cold, barren red\norb. The poster on the right shows a Mars with a humongous green\nlandmass surrounded by oceans. The planet has been heated up and\ntransformed to suit humans. Musk fully intends to try and make this happen.\nTurning humans into space colonizers is his stated life\u2019s purpose. \u201cI would\nlike to die thinking that humanity has a bright future,\u201d he said. \u201cIf we can\nsolve sustainable energy and be well on our way to becoming a\nmultiplanetary species with a self-sustaining civilization on another planet\n\u2014to cope with a worst-case scenario happening and extinguishing human consciousness\u2014then,\u201d and here he paused for a moment, \u201cI think that\nwould be really good.\u201d\nIf some of the things that Musk says and does sound absurd, that\u2019s\nbecause on one level they very much are. On this occasion, for example,\nMusk\u2019s assistant had just handed him some cookies-and-cream ice cream\nwith sprinkles on top, and he then talked earnestly about saving humanity\nwhile a blotch of the dessert hung from his lower lip.\nMusk\u2019s ready willingness to tackle impossible things has turned him\ninto a deity in Silicon Valley, where fellow CEOs like Page speak of him in\nreverential awe, and budding entrepreneurs strive \u201cto be like Elon\u201d just as\nthey had been striving in years past to mimic Steve Jobs. Silicon Valley,\nthough, operates within a warped version of reality, and outside the confines\nof its shared fantasy, Musk often comes off as a much more polarizing\nfigure. He\u2019s the guy with the electric cars, solar panels, and rockets\npeddling false hope. Forget Steve Jobs. Musk is a sci-fi version of P. T.\nBarnum who has gotten extraordinarily rich by preying on people\u2019s fear and\nself-hatred. Buy a Tesla. Forget about the mess you\u2019ve made of the planet\nfor a while.\nI\u2019d long been a subscriber to this latter camp. Musk had struck me as a\nwell-intentioned dreamer\u2014a card-carrying member of Silicon Valley\u2019s\ntechno-utopian club. This group tends to be a mix of Ayn Rand devotees\nand engineer absolutists who see their hyperlogical worldviews as the\nAnswer for everyone. If we\u2019d just get out of their way, they\u2019d fix all our\nproblems. One day, soon enough, we\u2019ll be able to download our brains to a\ncomputer, relax, and let their algorithms take care of everything. Much of\ntheir ambition proves inspiring and their works helpful. But the techno-\nutopians do get tiresome with their platitudes and their ability to prattle on\nfor hours without saying much of substance. More disconcerting is their\nunderlying message that humans are flawed and our humanity is an\nannoying burden that needs to be dealt with in due course. When I\u2019d caught\nMusk at Silicon Valley events, his highfalutin talk often sounded straight\nout of the techno-utopian playbook. And, most annoyingly, his world-\nsaving companies didn\u2019t even seem to be doing all that well.\nYet, in the early part of 2012, the cynics like me had to take notice of\nwhat Musk was actually accomplishing. His once-beleaguered companies\nwere succeeding at unprecedented things. SpaceX flew a supply capsule to\nthe International Space Station and brought it safely back to Earth. Tesla Motors delivered the Model S, a beautiful, all-electric sedan that took the\nautomotive industry\u2019s breath away and slapped Detroit sober. These two\nfeats elevated Musk to the rarest heights among business titans. Only Steve\nJobs could claim similar achievements in two such different industries,\nsometimes putting out a new Apple product and a blockbuster Pixar movie\nin the same year. And yet, Musk was not done. He was also the chairman\nand largest shareholder of SolarCity, a booming solar energy company\npoised to file for an initial public offering. Musk had somehow delivered\nthe biggest advances the space, automotive, and energy industries had seen\nin decades in what felt like one fell swoop.\nIt was in 2012 that I decided to see what Musk was like firsthand and to\nwrite a cover story about him for Bloomberg Businessweek. At this point in\nMusk\u2019s life, everything ran through his assistant/loyal appendage Mary\nBeth Brown. She invited me to visit what I\u2019ve come to refer to as Musk\nLand.\nAnyone arriving at Musk Land for the first time will have the same\nhead-scratching experience. You\u2019re told to park at One Rocket Road in\nHawthorne, where SpaceX has its HQ. It seems impossible that anything\ngood could call Hawthorne home. It\u2019s a bleak part of Los Angeles County\nin which groupings of rundown houses, run-down shops, and run-down\neateries surround huge, industrial complexes that appear to have been built\nduring some kind of architectural Boring Rectangle movement. Did Elon\nMusk really stick his company in the middle of this dreck? Then, okay,\nthings start to make more sense when you see one 550,000-square-foot\nrectangle painted an ostentatious hue of \u201cUnity of Body, Soul, and Mind\u201d\nwhite. This is the main SpaceX building.\nIt was only after going through the front doors of SpaceX that the\ngrandeur of what this man had done became apparent. Musk had built an\nhonest-to-God rocket factory in the middle of Los Angeles. And this factory\nwas not making one rocket at a time. No. It was making many rockets\u2014\nfrom scratch. The factory was a giant, shared work area. Near the back were\nmassive delivery bays that allowed for the arrival of hunks of metal, which\nwere transported to two-story-high welding machines. Over to one side\nwere technicians in white coats making motherboards, radios, and other\nelectronics. Other people were in a special, airtight glass chamber, building\nthe capsules that rockets would take to the Space Station. Tattooed men in\nbandanas were blasting Van Halen and threading wires around rocket engines. There were completed bodies of rockets lined up one after the\nother ready to be placed on trucks. Still more rockets, in another part of the\nbuilding, awaited coats of white paint. It was difficult to take in the entire\nfactory at once. There were hundreds of bodies in constant motion whirring\naround a variety of bizarre machines.\nThis is just building number one of Musk Land. SpaceX had acquired\nseveral buildings that used to be part of a Boeing factory, which made the\nfuselages for 747s. One of these buildings has a curved roof and looks like\nan airplane hangar. It serves as the research, development, and design studio\nfor Tesla. This is where the company came up with the look for the Model S\nsedan and its follow-on, the Model X SUV. In the parking lot outside the\nstudio, Tesla has built one of its recharging stations where Los Angeles\ndrivers can top up with electricity for free. The charging center is easy\nenough to spot because Musk has installed a white and red obelisk branded\nwith the Tesla logo that sits in the middle of an infinity pool.\nIt was in my first interview with Musk, which took place at the design\nstudio, that I began to get a sense of how he talked and operated. He\u2019s a\nconfident guy, but does not always do a good job of displaying this. On\ninitial encounter, Musk can come off as shy and borderline awkward. His\nSouth African accent remains present but fading, and the charm of it is not\nenough to offset the halting nature of Musk\u2019s speech pattern. Like many an\nengineer or physicist, Musk will pause while fishing around for exact\nphrasing, and he\u2019ll often go rumbling down an esoteric, scientific rabbit\nhole without providing any helping hands or simplified explanations along\nthe way. Musk expects you to keep up. None of this is off-putting. Musk, in\nfact, will toss out plenty of jokes and can be downright charming. It\u2019s just\nthat there\u2019s a sense of purpose and pressure hanging over any conversation\nwith the man. Musk doesn\u2019t really shoot the shit. (It would end up taking\nabout thirty hours of interviews for Musk to really loosen up and let me into\na different, deeper level of his psyche and personality.)\nMost high-profile CEOs have handlers all around them. Musk mostly\nmoves about Musk Land on his own. This is not the guy who slinks into the\nrestaurant. It\u2019s the guy who owns the joint and strides about with authority.\nMusk and I talked, as he made his way around the design studio\u2019s main\nfloor, inspecting prototype parts and vehicles. At each station, employees\nrushed up to Musk and disgorged information. He listened intently,\nprocessed it, and nodded when satisfied. The people moved away and Musk moved to the next information dump. At one point, Tesla\u2019s design chief,\nFranz von Holzhausen, wanted Musk\u2019s take on some new tires and rims that\nhad come in for the Model S and on the seating arrangements for the Model\nX. They spoke, and then they went into a back room where executives from\na seller of high-end graphics software had prepared a presentation for Musk.\nThey wanted to show off new 3-D rendering technology that would allow\nTesla to tweak the finish of a virtual Model S and see in great detail how\nthings like shadows and streetlights played off the car\u2019s body. Tesla\u2019s\nengineers really wanted the computing systems and needed Musk\u2019s sign-off.\nThe men did their best to sell Musk on the idea while the sound of drills and\ngiant industrial fans drowned out their shtick. Musk, wearing leather shoes,\ndesigner jeans, and a black T-shirt, which is essentially his work uniform,\nhad to don 3-D goggles for the demonstration and seemed unmoved. He\ntold them he\u2019d think about it and then walked toward the source of the\nloudest noise\u2014a workshop deep in the design studio where Tesla engineers\nwere building the scaffolding for the thirty-foot decorative towers that go\noutside the charging stations. \u201cThat thing looks like it could survive a\nCategory Five hurricane,\u201d Musk said. \u201cLet\u2019s thin it up a bit.\u201d Musk and I\neventually hop into his car\u2014a black Model S\u2014and zip back to the main\nSpaceX building. \u201cI think there are probably too many smart people\npursuing Internet stuff, finance, and law,\u201d Musk said on the way. \u201cThat is\npart of the reason why we haven\u2019t seen as much innovation.\u201d\nMUSK LAND WAS A REVELATION.\nI\u2019d come to Silicon Valley in 2000 and ended up living in the Tenderloin\nneighborhood of San Francisco. It\u2019s the one part of the city that locals will\nimplore you to avoid. Without trying very hard, you can find someone\npulling down his pants and pooping in between parked cars or encounter\nsome deranged sort bashing his head into the side of a bus stop. At dive bars\nnear the local strip clubs, transvestites hit on curious businessmen and\ndrunks fall asleep on couches and soil themselves as part of their lazy\nSunday ritual. It\u2019s the gritty, knife-stabby part of San Francisco and turned\nout to be a great place to watch the dotcom dream die.\nSan Francisco has an enduring history with greed. It became a city on\nthe back of the gold rush, and not even a catastrophic earthquake could slow\nSan Francisco\u2019s economic lust for long. Don\u2019t let the granola vibes fool\nyou. Booms and busts are the rhythm of this place. And, in 2000, San Francisco had been overtaken by the boom of all booms and consumed by\navarice. It was a wonderful time to be alive with just about the entire\npopulace giving in to a fantasy\u2014a get-rich-quick, Internet madness. The\npulses of energy from this shared delusion were palpable, producing a\nconstant buzz that vibrated across the city. And here I was in the center of\nthe most depraved part of San Francisco, watching just how high and low\npeople get when consumed by excess.\nStories tracking the insanity of business in these times are well-known.\nYou no longer had to make something that other people wanted to buy in\norder to start a booming company. You just had to have an idea for some\nsort of Internet thing and announce it to the world in order for eager\ninvestors to fund your thought experiment. The whole goal was to make as\nmuch money as possible in the shortest amount of time because everyone\nknew on at least a subconscious level that reality had to set in eventually.\nValley denizens took very literally the clich\u00e9 of working as hard as you\nplay. People in their twenties, thirties, forties, and fifties were expected to\npull all-nighters. Cubicles were turned into temporary homes, and personal\nhygiene was abandoned. Oddly enough, making Nothing appear to be\nSomething took a lot of work. But when the time to decompress arrived,\nthere were plenty of options for total debauchery. The hot companies and\nmedia powers of the time seemed locked in a struggle to outdo each other\nwith ever-fancier parties. Old-line companies trying to look \u201cwith it\u201d would\nregularly buy space at a concert venue and then order up some dancers,\nacrobats, open bars, and the Barenaked Ladies. Young technologists would\nshow up to pound their free Jack and Cokes and snort their cocaine in porta-\npotties. Greed and self-interest were the only things that made any sense\nback then.\nWhile the good times have been well chronicled, the subsequent bad\ntimes have been\u2014unsurprisingly\u2014ignored. It\u2019s more fun to reminiscence\non irrational exuberance than the mess that gets left behind.\nLet it be said for the record, then, that the implosion of the get-rich-\nquick Internet fantasy left San Francisco and Silicon Valley in a deep\ndepression. The endless parties ended. The prostitutes no longer roamed the\nstreets of the Tenderloin at 6 A.M. offering pre-commute love. (\u201cCome on,\nhoney. It\u2019s better than coffee!\u201d) Instead of the Barenaked Ladies, you got\nthe occasional Neil Diamond tribute band at a trade show, some free T-\nshirts, and a lump of shame. The technology industry had no idea what to do with itself. The dumb\nventure capitalists who had been taken during the bubble didn\u2019t want to\nlook any dumber, so they stopped funding new ventures altogether.\nEntrepreneurs\u2019 big ideas were replaced by the smallest of notions. It was as\nif Silicon Valley had entered rehab en masse. It sounds melodramatic, but\nit\u2019s true. A populace of millions of clever people came to believe that they\nwere inventing the future. Then . . . poof! Playing it safe suddenly became\nthe fashionable thing to do.\nThe evidence of this malaise is in the companies and ideas formed\nduring this period. Google had appeared and really started to thrive around\n2002, but it was an outlier. Between Google and Apple\u2019s introduction of the\niPhone in 2007, there\u2019s a wasteland of ho-hum companies. And the hot new\nthings that were just starting out\u2014Facebook and Twitter\u2014certainly did not\nlook like their predecessors\u2014Hewlett-Packard, Intel, Sun Microsystems\u2014\nthat made physical products and employed tens of thousands of people in\nthe process. In the years that followed, the goal went from taking huge risks\nto create new industries and grand new ideas, to chasing easier money by\nentertaining consumers and pumping out simple apps and advertisements.\n\u201cThe best minds of my generation are thinking about how to make people\nclick ads,\u201d Jeff Hammerbacher, an early Facebook engineer, told me. \u201cThat\nsucks.\u201d Silicon Valley began to look an awful lot like Hollywood.\nMeanwhile, the consumers it served had turned inward, obsessed with their\nvirtual lives.\nOne of the first people to suggest that this lull in innovation could signal\na much larger problem was Jonathan Huebner, a physicist who works at the\nPentagon\u2019s Naval Air Warfare Center in China Lake, California. Huebner is\nthe Leave It to Beaver version of a merchant of death. Middle-aged, thin,\nand balding, he likes to wear a dirt-inspired ensemble of khaki pants, a\nbrown-striped shirt, and a canvas khaki jacket. He has designed weapons\nsystems since 1985, gaining direct insight into the latest and greatest\ntechnology around materials, energy, and software. Following the dot-com\nbust, he became miffed at the ho-hum nature of the supposed innovations\ncrossing his desk. In 2005, Huebner delivered a paper, \u201cA Possible\nDeclining Trend in Worldwide Innovation,\u201d which was either an indictment\nof Silicon Valley or at least an ominous warning.\nHuebner opted to use a tree metaphor to describe what he saw as the\nstate of innovation. Man has already climbed past the trunk of the tree and gone out on its major limbs, mining most of the really big, game-changing\nideas\u2014the wheel, electricity, the airplane, the telephone, the transistor. Now\nwe\u2019re left dangling near the end of the branches at the top of the tree and\nmostly just refining past inventions. To back up his point in the paper,\nHuebner showed that the frequency of life-changing inventions had started\nto slow. He also used data to prove that the number of patents filed per\nperson had declined over time. \u201cI think the probability of us discovering\nanother top-one-hundred-type invention gets smaller and smaller,\u201d Huebner\ntold me in an interview. \u201cInnovation is a finite resource.\u201d\nHuebner predicted that it would take people about five years to catch on\nto his thinking, and this forecast proved almost exactly right. Around 2010,\nPeter Thiel, the PayPal cofounder and early Facebook investor, began\npromoting the idea that the technology industry had let people down. \u201cWe\nwanted flying cars, instead we got 140 characters\u201d became the tagline of his\nventure capital firm Founders Fund. In an essay called \u201cWhat Happened to\nthe Future,\u201d Thiel and his cohorts described how Twitter, its 140-character\nmessages, and similar inventions have let the public down. He argued that\nscience fiction, which once celebrated the future, has turned dystopian\nbecause people no longer have an optimistic view of technology\u2019s ability to\nchange the world.\nI\u2019d subscribed to a lot of this type of thinking until that first visit to\nMusk Land. While Musk had been anything but shy about what he was up\nto, few people outside of his companies got to see the factories, the R&D\ncenters, the machine shops, and to witness the scope of what he was doing\nfirsthand. Here was a guy who had taken much of the Silicon Valley ethic\nbehind moving quickly and running organizations free of bureaucratic\nhierarchies and applied it to improving big, fantastic machines and chasing\nthings that had the potential to be the real breakthroughs we\u2019d been missing.\nBy rights, Musk should have been part of the malaise. He jumped right\ninto dot-com mania in 1995, when, fresh out of college, he founded a\ncompany called Zip2\u2014a primitive Google Maps meets Yelp. That first\nventure ended up a big, quick hit. Compaq bought Zip2 in 1999 for $307\nmillion. Musk made $22 million from the deal and poured almost all of it\ninto his next venture, a start-up that would morph into PayPal. As the\nlargest shareholder in PayPal, Musk became fantastically well-to-do when\neBay acquired the company for $1.5 billion in 2002. Instead of hanging around Silicon Valley and falling into the same funk\nas his peers, however, Musk decamped to Los Angeles. The conventional\nwisdom of the time said to take a deep breath and wait for the next big thing\nto arrive in due course. Musk rejected that logic by throwing $100 million\ninto SpaceX, $70 million into Tesla, and $10 million into SolarCity. Short\nof building an actual money-crushing machine, Musk could not have picked\na faster way to destroy his fortune. He became a one-man, ultra-risk-taking\nventure capital shop and doubled down on making super-complex physical\ngoods in two of the most expensive places in the world, Los Angeles and\nSilicon Valley. Whenever possible, Musk\u2019s companies would make things\nfrom scratch and try to rethink much that the aerospace, automotive, and\nsolar industries had accepted as convention.\nWith SpaceX, Musk is battling the giants of the U.S. military-industrial\ncomplex, including Lockheed Martin and Boeing. He\u2019s also battling nations\n\u2014most notably Russia and China. SpaceX has made a name for itself as the\nlow-cost supplier in the industry. But that, in and of itself, is not really good\nenough to win. The space business requires dealing with a mess of politics,\nback-scratching, and protectionism that undermines the fundamentals of\ncapitalism. Steve Jobs faced similar forces when he went up against the\nrecording industry to bring the iPod and iTunes to market. The crotchety\nLuddites in the music industry were a pleasure to deal with compared to\nMusk\u2019s foes who build weapons and countries for a living. SpaceX has been\ntesting reusable rockets that can carry payloads to space and land back on\nEarth, on their launchpads, with precision. If the company can perfect this\ntechnology, it will deal a devastating blow to all of its competitors and\nalmost assuredly push some mainstays of the rocket industry out of business\nwhile establishing the United States as the world leader for taking cargo and\nhumans to space. It\u2019s a threat that Musk figures has earned him plenty of\nfierce enemies. \u201cThe list of people that would not mind if I was gone is\ngrowing,\u201d Musk said. \u201cMy family fears that the Russians will assassinate\nme.\u201d\nWith Tesla Motors, Musk has tried to revamp the way cars are\nmanufactured and sold, while building out a worldwide fuel distribution\nnetwork at the same time. Instead of hybrids, which in Musk lingo are\nsuboptimal compromises, Tesla strives to make all-electric cars that people\nlust after and that push the limits of technology. Tesla does not sell these\ncars through dealers; it sells them on the Web and in Apple-like galleries located in high-end shopping centers. Tesla also does not anticipate making\nlots of money from servicing its vehicles, since electric cars do not require\nthe oil changes and other maintenance procedures of traditional cars. The\ndirect sales model embraced by Tesla stands as a major affront to car\ndealers used to haggling with their customers and making their profits from\nexorbitant maintenance fees. Tesla\u2019s recharging stations now run alongside\nmany of the major highways in the United States, Europe, and Asia and can\nadd hundreds of miles of oomph back to a car in about twenty minutes.\nThese so-called supercharging stations are solar-powered, and Tesla owners\npay nothing to refuel. While much of America\u2019s infrastructure decays,\nMusk is building a futuristic end-to-end transportation system that would\nallow the United States to leapfrog the rest of the world. Musk\u2019s vision, and,\nof late, execution seem to combine the best of Henry Ford and John D.\nRockefeller.\nWith SolarCity, Musk has funded the largest installer and financer of\nsolar panels for consumers and businesses. Musk helped come up with the\nidea for SolarCity and serves as its chairman, while his cousins Lyndon and\nPeter Rive run the company. SolarCity has managed to undercut dozens of\nutilities and become a large utility in its own right. During a time in which\nclean-tech businesses have gone bankrupt with alarming regularity, Musk\nhas built two of the most successful clean-tech companies in the world. The\nMusk Co. empire of factories, tens of thousands of workers, and industrial\nmight has incumbents on the run and has turned Musk into one of the\nrichest men in the world, with a net worth around $10 billion.\nThe visit to Musk Land started to make a few things clear about how\nMusk had pulled all this off. While the \u201cputting man on Mars\u201d talk can\nstrike some people as loopy, it gave Musk a unique rallying cry for his\ncompanies. It\u2019s the sweeping goal that forms a unifying principle over\neverything he does. Employees at all three companies are well aware of this\nand well aware that they\u2019re trying to achieve the impossible day in and day\nout. When Musk sets unrealistic goals, verbally abuses employees, and\nworks them to the bone, it\u2019s understood to be\u2014on some level\u2014part of the\nMars agenda. Some employees love him for this. Others loathe him but\nremain oddly loyal out of respect for his drive and mission. What Musk has\ndeveloped that so many of the entrepreneurs in Silicon Valley lack is a\nmeaningful worldview. He\u2019s the possessed genius on the grandest quest\nanyone has ever concocted. He\u2019s less a CEO chasing riches than a general marshaling troops to secure victory. Where Mark Zuckerberg wants to help\nyou share baby photos, Musk wants to . . . well . . . save the human race\nfrom self-imposed or accidental annihilation.\nThe life that Musk has created to manage all of these endeavors is\npreposterous. A typical week starts at his mansion in Bel Air. On Monday,\nhe works the entire day at SpaceX. On Tuesday, he begins at SpaceX, then\nhops onto his jet and flies to Silicon Valley. He spends a couple of days\nworking at Tesla, which has its offices in Palo Alto and factory in Fremont.\nMusk does not own a home in Northern California and ends up staying at\nthe luxe Rosewood hotel or at friends\u2019 houses. To arrange the stays with\nfriends, Musk\u2019s assistant will send an e-mail asking, \u201cRoom for one?\u201d and if\nthe friend says, \u201cYes,\u201d Musk turns up at the door late at night. Most often he\nstays in a guest room, but he\u2019s also been known to crash on the couch after\nwinding down with some video games. Then it\u2019s back to Los Angeles and\nSpaceX on Thursday. He shares custody of his five young boys\u2014twins and\ntriplets\u2014with his ex-wife, Justine, and has them four days a week. Each\nyear, Musk tabulates the amount of flight time he endures per week to help\nhim get a sense of just how out of hand things are getting. Asked how he\nsurvives this schedule, Musk said, \u201cI had a tough childhood, so maybe that\nwas helpful.\u201d\nDuring one visit to Musk Land, he had to squeeze our interview in\nbefore heading off for a camping trip at Crater Lake National Park in\nOregon. It was almost 8 P.M. on a Friday, so Musk would soon be piling his\nboys and nannies into his private jet and then meeting drivers who would\ntake him to his friends at the campsite; the friends would then help the\nMusk clan unpack and complete their pitch-black arrival. There would be a\nbit of hiking over the weekend. Then the relaxation would end. Musk would\nfly with the boys back to Los Angeles on Sunday afternoon. Then, he would\ntake off on his own that evening for New York. Sleep. Hit the morning talk\nshows on Monday. Meetings. E-mail. Sleep. Fly back to Los Angeles\nTuesday morning. Work at SpaceX. Fly to San Jose Tuesday afternoon to\nvisit the Tesla Motors factory. Fly to Washington, D.C., that night and see\nPresident Obama. Fly back to Los Angeles Wednesday night. Spend a\ncouple of days working at SpaceX. Then go to a weekend conference held\nby Google\u2019s chairman, Eric Schmidt, in Yellowstone. At this time, Musk\nhad just split from his second wife, the actress Talulah Riley, and was trying\nto calculate if he could mix a personal life into all of this. \u201cI think the time allocated to the businesses and the kids is going fine,\u201d Musk said. \u201cI would\nlike to allocate more time to dating, though. I need to find a girlfriend.\nThat\u2019s why I need to carve out just a little more time. I think maybe even\nanother five to ten\u2014how much time does a woman want a week? Maybe\nten hours? That\u2019s kind of the minimum? I don\u2019t know.\u201d\nMusk rarely finds time to decompress, but when he does, the festivities\nare just as dramatic as the rest of his life. On his thirtieth birthday, Musk\nrented out a castle in England for about twenty people. From 2 A.M. until 6\nA.M., they played a variation of hide-and-seek called sardines in which one\nperson runs off and hides and everyone else looks for him. Another party\noccurred in Paris. Musk, his brother, and cousins found themselves awake\nat midnight and decided to bicycle through the city until 6 A.M. They slept\nall day and then boarded the Orient Express in the evening. Once again,\nthey stayed up all night. The Lucent Dossier Experience\u2014an avant-garde\ngroup of performers\u2014were on the luxurious train, performing palm\nreadings and acrobatics. When the train arrived in Venice the next day,\nMusk\u2019s family had dinner and then hung out on the patio of their hotel\noverlooking the Grand Canal until 9 A.M. Musk loves costume parties as\nwell, and turned up at one dressed like a knight and using a parasol to duel a\nmidget wearing a Darth Vader costume.\nFor one of his most recent birthdays, Musk invited fifty people to a\ncastle\u2014or at least the United States\u2019 best approximation of a castle\u2014in\nTarrytown, New York. This party had a Japanese steampunk theme, which\nis sort of like a sci-fi lover\u2019s wet dream\u2014a mix of corsets, leather, and\nmachine worship. Musk dressed as a samurai.\nThe festivities included a performance of The Mikado, a Victorian\ncomic opera by Gilbert and Sullivan set in Japan, at a small theater in the\nheart of town. \u201cI am not sure the Americans got it,\u201d said Riley, whom Musk\nremarried after his ten-hour-a-week dating plan failed. The Americans and\neveryone else did enjoy what followed. Back at the castle, Musk donned a\nblindfold, got pushed up against a wall, and held balloons in each hand and\nanother between his legs. The knife thrower then went to work. \u201cI\u2019d seen\nhim before, but did worry that maybe he could have an off day,\u201d Musk said.\n\u201cStill, I thought, he would maybe hit one gonad but not both.\u201d The\nonlookers were stunned and frightened for Musk\u2019s safety. \u201cThat was\nbizarre,\u201d said Bill Lee, a technology investor and one of Musk\u2019s good\nfriends. \u201cBut Elon believes in the science of things.\u201d One of the world\u2019s top sumo wrestlers showed up at the party along with some of his compatriots.\nA ring had been set up at the castle, and Musk faced off against the\nchampion. \u201cHe was three hundred and fifty pounds, and they were not\njiggly pounds,\u201d Musk said. \u201cI went full adrenaline rush and managed to lift\nthe guy off the ground. He let me win that first round and then beat me. I\nthink my back is still screwed up.\u201d\nRiley turned planning these types of parties for Musk into an art. She\nmet Musk back in 2008, when his companies were collapsing. She watched\nhim lose his entire fortune and get ridiculed by the press. She knows that\nthe sting of these years remains and has combined with the other traumas in\nMusk\u2019s life\u2014the tragic loss of an infant son and a brutal upbringing in\nSouth Africa\u2014to create a tortured soul. Riley has gone to great lengths to\nmake sure Musk\u2019s escapes from work and this past leave him feeling\nrefreshed if not healed. \u201cI try to think of fun things he has not done before\nwhere he can relax,\u201d Riley said. \u201cWe\u2019re trying to make up for his miserable\nchildhood now.\u201d\nGenuine as Riley\u2019s efforts might have been, they were not entirely\neffective. Not long after the Sumo party, I found Musk back at work at the\nTesla headquarters in Palo Alto. It was a Saturday, and the parking lot was\nfull of cars. Inside of the Tesla offices, hundreds of young men were at\nwork\u2014some of them designing car parts on computers and others\nconducting experiments with electronics equipment on their desks. Musk\u2019s\nuproarious laugh would erupt every few minutes and carry through the\nentire floor. When Musk came into the meeting room where I\u2019d been\nwaiting, I noted how impressive it was for so many people to turn up on a\nSaturday. Musk saw the situation in a different light, complaining that fewer\nand fewer people had been working weekends of late. \u201cWe\u2019ve grown\nfucking soft,\u201d Musk replied. \u201cI was just going to send out an e-mail. We\u2019re\nfucking soft.\u201d (A word of warning: There\u2019s going to be a lot of \u201cfuck\u201d in\nthis book. Musk adores the word, and so do most of the people in his inner\ncircle.)\nThis kind of declaration seems to fit with our impressions of other\nvisionaries. It\u2019s not hard to imagine Howard Hughes or Steve Jobs\nchastising their workforce in a similar way. Building things\u2014especially big\nthings\u2014is a messy business. In the two decades Musk has spent creating\ncompanies, he\u2019s left behind a trail of people who either adore or despise\nhim. During the course of my reporting, these people lined up to give me their take on Musk and the gory details of how he and his businesses\noperate.\nMy dinners with Musk and periodic trips to Musk Land revealed a\ndifferent set of possible truths about the man. He\u2019s set about building\nsomething that has the potential to be much grander than anything Hughes\nor Jobs produced. Musk has taken industries like aerospace and automotive\nthat America seemed to have given up on and recast them as something new\nand fantastic. At the heart of this transformation are Musk\u2019s skills as a\nsoftware maker and his ability to apply them to machines. He\u2019s merged\natoms and bits in ways that few people thought possible, and the results\nhave been spectacular. It\u2019s true enough that Musk has yet to have a\nconsumer hit on the order of the iPhone or to touch more than one billion\npeople like Facebook. For the moment, he\u2019s still making rich people\u2019s toys,\nand his budding empire could be an exploded rocket or massive Tesla recall\naway from collapse. On the other hand, Musk\u2019s companies have already\naccomplished far more than his loudest detractors thought possible, and the\npromise of what\u2019s to come has to leave hardened types feeling optimistic\nduring their weaker moments. \u201cTo me, Elon is the shining example of how\nSilicon Valley might be able to reinvent itself and be more relevant than\nchasing these quick IPOs and focusing on getting incremental products\nout,\u201d said Edward Jung, a famed software engineer and inventor. \u201cThose\nthings are important, but they are not enough. We need to look at different\nmodels of how to do things that are longer term in nature and where the\ntechnology is more integrated.\u201d The integration mentioned by Jung\u2014the\nharmonious melding of software, electronics, advanced materials, and\ncomputing horsepower\u2014appears to be Musk\u2019s gift. Squint ever so slightly,\nand it looks like Musk could be using his skills to pave the way toward an\nage of astonishing machines and science fiction dreams made manifest.\nIn that sense, Musk comes off much more like Thomas Edison than\nHoward Hughes. He\u2019s an inventor, celebrity businessman, and industrialist\nable to take big ideas and turn them into big products. He\u2019s employing\nthousands of people to forge metal in American factories at a time when this\nwas thought to be impossible. Born in South Africa, Musk now looks like\nAmerica\u2019s most innovative industrialist and outlandish thinker and the\nperson most likely to set Silicon Valley on a more ambitious course.\nBecause of Musk, Americans could wake up in ten years with the most\nmodern highway in the world: a transit system run by thousands of solar- powered charging stations and traversed by electric cars. By that time,\nSpaceX may well be sending up rockets every day, taking people and things\nto dozens of habitats and making preparations for longer treks to Mars.\nThese advances are simultaneously difficult to fathom and seemingly\ninevitable if Musk can simply buy enough time to make them work. As his\nex-wife, Justine, put it, \u201cHe does what he wants, and he is relentless about\nit. It\u2019s Elon\u2019s world, and the rest of us live in it.\u201d 2 AFRICA\nT\nHE PUBLIC FIRST MET ELON REEVE MUSK IN 1984. The South\nAfrican trade publication PC and Office Technology published the source\ncode to a video game Musk had designed. Called Blastar, the science-\nfiction-inspired space game required 167 lines of instructions to run. This\nwas back in the day when early computer users were required to type out\ncommands to make their machines do much of anything. In that context,\nMusk\u2019s game did not shine as a marvel of computer science but it certainly\nsurpassed what most twelve-year-olds were kicking out at the time. Its\ncoverage in the magazine netted Musk five hundred dollars and provided\nsome early hints about his character. The Blastar spread on page 69 of the\nmagazine shows that the young man wanted to go by the sci-fi-author-\nsounding name E. R. Musk and that he already had visions of grand\nconquests dancing in his head. The brief explainer states, \u201cIn this game you\nhave to destroy an alien space freighter, which is carrying deadly Hydrogen\nBombs and Status Beam Machines. This game makes good use of sprites\nand animation, and in this sense makes the listing worth reading.\u201d (As of\nthis writing, not even the Internet knows what \u201cstatus beam machines\u201d are.)\nA boy fantasizing about space and battles between good and evil is\nanything but amazing. A boy who takes these fantasies seriously is more\nremarkable. Such was the case with the young Elon Musk. By the middle of\nhis teenage years, Musk had blended fantasy and reality to the point that\nthey were hard to separate in his mind. Musk came to see man\u2019s fate in the\nuniverse as a personal obligation. If that meant pursuing cleaner energy\ntechnology or building spaceships to extend the human species\u2019s reach, then\nso be it. Musk would find a way to make these things happen. \u201cMaybe I\nread too many comics as a kid,\u201d Musk said. \u201cIn the comics, it always seems\nlike they are trying to save the world. It seemed like one should try to make\nthe world a better place because the inverse makes no sense.\u201d\nAt around age fourteen, Musk had a full-on existential crisis. He tried to\ndeal with it like many gifted adolescents do, turning to religious and philosophical texts. Musk sampled a handful of ideologies and then ended\nup more or less back where he had started, embracing the sci-fi lessons\nfound in one of the most influential books in his life: The Hitchhiker\u2019s\nGuide to the Galaxy, by Douglas Adams. \u201cHe points out that one of the\nreally tough things is figuring out what questions to ask,\u201d Musk said. \u201cOnce\nyou figure out the question, then the answer is relatively easy. I came to the\nconclusion that really we should aspire to increase the scope and scale of\nhuman consciousness in order to better understand what questions to ask.\u201d\nThe teenage Musk then arrived at his ultralogical mission statement. \u201cThe\nonly thing that makes sense to do is strive for greater collective\nenlightenment,\u201d he said.\nIt\u2019s easy enough to spot some of the underpinnings of Musk\u2019s search for\npurpose. Born in 1971, he grew up in Pretoria, a large city in the\nnortheastern part of South Africa, just an hour\u2019s drive from Johannesburg.\nThe specter of apartheid was present throughout his childhood, as South\nAfrica frequently boiled over with tension and violence. Blacks and whites\nclashed, as did blacks of different tribes. Musk turned four years old just\ndays after the Soweto Uprising, in which hundreds of black students died\nwhile protesting decrees of the white government. For years South Africa\nfaced sanctions imposed by other nations due to its racist policies. Musk\nhad the luxury of traveling abroad during his childhood and would have\ngotten a flavor for how outsiders viewed South Africa.\nBut what had even more of an impact on Musk\u2019s personality was the\nwhite Afrikaner culture so prevalent in Pretoria and the surrounding areas.\nHypermasculine behavior was celebrated and tough jocks were revered.\nWhile Musk enjoyed a level of privilege, he lived as an outsider whose\nreserved personality and geeky inclinations ran against the prevailing\nattitudes of the time. His notion that something about the world had gone\nawry received constant reinforcement, and Musk, almost from his earliest\ndays, plotted an escape from his surroundings and dreamed of a place that\nwould allow his personality and dreams to flourish. He saw America in its\nmost clich\u00e9d form, as the land of opportunity and the most likely stage for\nmaking the realization of his dreams possible. This is how it came to pass\nthat a lonesome, gawky South African boy who talked with the utmost\nsincerity about pursuing \u201ccollective enlightenment\u201d ended up as America\u2019s\nmost adventurous industrialist. When Musk did finally reach the United States in his twenties, it\nmarked a return to his ancestral roots. Family trees suggest that ancestors\nbearing the Swiss German surname of Haldeman on the maternal side of\nMusk\u2019s family left Europe for New York during the Revolutionary War.\nFrom New York, they spread out to the prairies of the Midwest\u2014Illinois\nand Minnesota, in particular. \u201cWe had people that fought on both sides of\nthe Civil War apparently and were a family of farmers,\u201d said Scott\nHaldeman, Musk\u2019s uncle and the unofficial family historian.\nThroughout his childhood, boys teased Musk because of his unusual\nname. He earned the first part of it from his great-grandfather John Elon\nHaldeman, who was born in 18721 and grew up in Illinois before heading to\nMinnesota. There he would meet his wife, Almeda Jane Norman, who was\nfive years younger. By 1902, the couple had settled down in a log cabin in\nthe central Minnesota town of Pequot and given birth to their son Joshua\nNorman Haldeman, Musk\u2019s grandfather. He would grow up to become an\neccentric and exceptional man and a model for Musk.*\nJoshua Norman Haldeman is described as an athletic, self-reliant boy. In\n1907, his family moved to the prairies of Saskatchewan, and his father died\nshortly thereafter when Joshua was just seven, leaving the boy to help run\nthe house. He took to the wide-open land and picked up bronco horseback\nriding, boxing, and wrestling. Haldeman would break in horses for local\nfarmers, often hurting himself in the process, and he organized one of\nCanada\u2019s first rodeos. Family pictures show Joshua dressed in a decorative\npair of chaps demonstrating his rope-spinning skills. As a teenager,\nHaldeman left home to get a degree from the Palmer School of Chiropractic\nin Iowa and then returned to Saskatchewan to become a farmer.\nWhen the depression hit in the 1930s, Haldeman fell into a financial\ncrisis. He could not afford to keep up with bank loans on his equipment and\nhad five thousand acres of land seized. \u201cFrom then on, Dad didn\u2019t believe in\nbanks or holding on to money,\u201d said Scott Haldeman, who would go on to\nreceive his chiropractic degree from the same school as his father and\nbecome one of the world\u2019s top spinal pain experts. After losing the farm\naround 1934, Haldeman lived something of a nomadic existence that his\ngrandson would replicate in Canada decades later. Standing six feet, three\ninches, he did odd jobs as a construction worker and rodeo performer before\nsettling down as a chiropractor.* By 1948, Haldeman had married a Canadian dance studio instructor,\nWinnifred Josephine Fletcher, or Wyn, and built a booming chiropractic\npractice. That year, the family, which already included a son and a daughter,\nwelcomed twin daughters Kaye and Maye, Musk\u2019s mother. The children\nlived in a three-story, twenty-room house that included a dance studio to let\nWyn keep teaching students. Ever in search of something new to do,\nHaldeman had picked up flying and bought his own plane. The family\ngained some measure of notoriety as people heard about Haldeman and his\nwife packing their kids into the back of the single-engine craft and heading\noff on excursions all around North America. Haldeman would often show\nup at political and chiropractic meetings in the plane and later wrote a book\nwith his wife called The Flying Haldemans: Pity the Poor Private Pilot.\nHaldeman seemed to have everything going for him when, in 1950, he\ndecided to give it all away. The doctor-cum-politician had long railed\nagainst government interference in the lives of individuals and had come to\nsee the Canadian bureaucracy as too meddlesome. A man who forbade\nswearing, smoking, Coca-Cola, and refined flour at his house, Haldeman\ncontended that the moral character of Canada had started to decline.\nHaldeman also possessed an enduring lust for adventure. And so, over the\ncourse of a few months, the family sold their house and dance and\nchiropractic practices and decided to move to South Africa\u2014a place\nHaldeman had never been. Scott Haldeman remembers helping his father\ndisassemble the family\u2019s Bellanca Cruisair (1948) airplane and put it into\ncrates before shipping it to Africa. Once in South Africa, the family rebuilt\nthe plane and used it to scour the country for a nice place to live, ultimately\nsettling on Pretoria, where Haldeman set up a new chiropractic practice.\nThe family\u2019s spirit for adventure seemed to know no bounds. In 1952,\nJoshua and Wyn made a 22,000-mile round-trip journey in their plane,\nflying up through Africa to Scotland and Norway. Wyn served as the\nnavigator and, though not a licensed pilot, would sometimes take over the\nflying duties. The couple topped this effort in 1954, flying 30,000 miles to\nAustralia and back. Newspapers reported on the couple\u2019s trip, and they\u2019re\nbelieved to be the only private pilots to get from Africa to Australia in a\nsingle-engine plane.*\nWhen not up in the air, the Haldemans were out in the bush going on\ngreat, monthlong expeditions to find the Lost City of the Kalahari Desert, a\nsupposed abandoned city in southern Africa. A family photo from one of these excursions shows the five children in the middle of the African bush.\nThey have gathered around a large metal pot being warmed by the embers\nof a campfire. The children look relaxed as they sit in folding chairs, legs\ncrossed and reading books. Behind them is the ruby-red Bellanca plane, a\ntent, and a car. The tranquility of the scene belies how dangerous these trips\nwere. During one incident, the family\u2019s truck hit a tree stump and forced the\nbumper through the radiator. Stuck in the middle of nowhere with no means\nof communication, Joshua worked for three days to fix the truck, while the\nfamily hunted for food. At other times, hyenas and leopards would circle\nthe campfire at night, and, one morning, the family woke to find a lion three\nfeet away from their main table. Joshua grabbed the first object he could\nfind\u2014a lamp\u2014waved it, and told the lion to go away. And it did.*\nThe Haldemans had a laissez-faire approach to raising their children,\nwhich would extend over the generations to Musk. Their kids were never\npunished, as Joshua believed they would intuit their way to proper behavior.\nWhen mom and dad went off on their tremendous flights, the kids were left\nat home. Scott Haldeman can\u2019t remember his father setting foot at his\nschool a single time even though his son was captain of the rugby team and\na prefect. \u201cTo him, that was all just anticipated,\u201d said Scott Haldeman. \u201cWe\nwere left with the impression that we were capable of anything. You just\nhave to make a decision and do it. In that sense, my father would be very\nproud of Elon.\u201d\nHaldeman died in 1974 at the age of seventy-two. He\u2019d been doing\npractice landings in his plane and didn\u2019t see a wire attached to a pair of\npoles. The wire caught the plane\u2019s wheels and flipped the craft, and\nHaldeman broke his neck. Elon was a toddler at the time. But throughout\nhis childhood, Elon heard many stories about his grandfather\u2019s exploits and\nsat through countless slide shows that documented his travels and trips\nthrough the bush. \u201cMy grandmother told these tales of how they almost died\nseveral times along their journeys,\u201d Musk said. \u201cThey were flying in a plane\nwith literally no instruments\u2014not even a radio, and they had road maps\ninstead of aerial maps, and some of those weren\u2019t even correct. My\ngrandfather had this desire for adventure, exploration doing crazy things.\u201d\nElon buys into the idea that his unusual tolerance for risk may well have\nbeen inherited directly from his grandfather. Many years after the last slide\nshow, Elon tried to find and purchase the red Bellanca plane but could not\nlocate it. Maye Musk, Elon\u2019s mother, grew up idolizing her parents. In her youth,\nshe was considered a nerd. She liked math and science and did well at the\ncoursework. By the age of fifteen, however, people had taken notice of\nsome of her other attributes. Maye was gorgeous. Tall with ash-blond hair,\nMaye had the high cheekbones and angular features that would make her\nstand out anywhere. A friend of the family ran a modeling school, and\nMaye took some courses. On the weekends, she did runway shows,\nmagazine shoots, occasionally showed up at a senator\u2019s or ambassador\u2019s\nhome for an event, and ended up as a finalist for Miss South Africa. (Maye\nhas continued to model into her sixties, appearing on the covers of\nmagazines like New York and Elle and in Beyonc\u00e9\u2019s music videos.)\nMaye and Elon\u2019s father, Errol Musk, grew up in the same neighborhood.\nThey met for the first time when Maye, born in 1948, was about eleven.\nErrol was the cool kid to Maye\u2019s nerd but had a crush on her for years. \u201cHe\nfell in love with me because of my legs and my teeth,\u201d said Maye. The two\nwould date on and off throughout their time at university. And, according to\nMaye, Errol spent about seven years as a relentless suitor seeking her hand\nin marriage and eventually breaking her will. \u201cHe just never stopped\nproposing,\u201d she said.\nTheir marriage was complicated from the start. Maye became pregnant\nduring the couple\u2019s honeymoon and gave birth to Elon on June 28, 1971,\nnine months and two days after her wedding day. While they may not have\nenjoyed marital bliss, the couple carved out a decent life for themselves in\nPretoria. Errol worked as a mechanical and electrical engineer and handled\nlarge projects such as office buildings, retail complexes, residential\nsubdivisions, and an air force base, while Maye set up a practice as a\ndietician. A bit more than a year after Elon\u2019s birth came his brother Kimbal,\nand soon thereafter came their sister Tosca.\nElon exhibited all the traits of a curious, energetic tot. He picked things\nup easily, and Maye, like many mothers do, pegged her son as brilliant and\nprecocious. \u201cHe seemed to understand things quicker than the other kids,\u201d\nshe said. The perplexing thing was that Elon seemed to drift off into a\ntrance at times. People spoke to him, but nothing got through when he had a\ncertain, distant look in his eyes. This happened so often that Elon\u2019s parents\nand doctors thought he might be deaf. \u201cSometimes, he just didn\u2019t hear you,\u201d\nsaid Maye. Doctors ran a series of tests on Elon, and elected to remove his\nadenoid glands, which can improve hearing in children. \u201cWell, it didn\u2019t change,\u201d said Maye. Elon\u2019s condition had far more to do with the wiring of\nhis mind than how his auditory system functioned. \u201cHe goes into his brain,\nand then you just see he is in another world,\u201d Maye said. \u201cHe still does that.\nNow I just leave him be because I know he is designing a new rocket or\nsomething.\u201d\nOther children did not respond well to these dreamlike states. You could\ndo jumping jacks right beside Musk or yell at him, and he would not even\nnotice. He kept right on thinking, and those around him judged that he was\neither rude or really weird. \u201cI do think Elon was always a little different but\nin a nerdy way,\u201d Maye said. \u201cIt didn\u2019t endear him to his peers.\u201d\nFor Musk, these pensive moments were wonderful. At five and six, he\nhad found a way to block out the world and dedicate all of his concentration\nto a single task. Part of this ability stemmed from the very visual way in\nwhich Musk\u2019s mind worked. He could see images in his mind\u2019s eye with a\nclarity and detail that we might associate today with an engineering drawing\nproduced by computer software. \u201cIt seems as though the part of the brain\nthat\u2019s usually reserved for visual processing\u2014the part that is used to\nprocess images coming in from my eyes\u2014gets taken over by internal\nthought processes,\u201d Musk said. \u201cI can\u2019t do this as much now because there\nare so many things demanding my attention but, as a kid, it happened a lot.\nThat large part of your brain that\u2019s used to handle incoming images gets\nused for internal thinking.\u201d Computers split their hardest jobs between two\ntypes of chips. There are graphics chips that deal with processing the\nimages produced by a television show stream or video game and\ncomputational chips that handle general purpose tasks and mathematical\noperations. Over time, Musk has ended up thinking that his brain has the\nequivalent of a graphics chip. It allows him to see things out in the world,\nreplicate them in his mind, and imagine how they might change or behave\nwhen interacting with other objects. \u201cFor images and numbers, I can\nprocess their interrelationships and algorithmic relationships,\u201d Musk said.\n\u201cAcceleration, momentum, kinetic energy\u2014how those sorts of things will\nbe affected by objects comes through very vividly.\u201d\nThe most striking part of Elon\u2019s character as a young boy was his\ncompulsion to read. From a very young age, he seemed to have a book in\nhis hands at all times. \u201cIt was not unusual for him to read ten hours a day,\u201d\nsaid Kimbal. \u201cIf it was the weekend, he could go through two books in a\nday.\u201d The family went on numerous shopping excursions in which they realized mid-trip that Elon had gone missing. Maye or Kimbal would pop\ninto the nearest bookstore and find Elon somewhere near the back sitting on\nthe floor and reading in one of his trancelike states.\nAs Elon got older, he would take himself to the bookstore when school\nended at 2 P.M. and stay there until about 6 P.M., when his parents returned\nhome from work. He plowed through fiction books and then comics and\nthen nonfiction titles. \u201cSometimes they kicked me out of the store, but\nusually not,\u201d Elon said. He listed The Lord of the Rings, Isaac Asimov\u2019s\nFoundation series, and Robert Heinlein\u2019s The Moon Is a Harsh Mistress as\nsome of his favorites, alongside The Hitchhiker\u2019s Guide to the Galaxy. \u201cAt\none point, I ran out of books to read at the school library and the\nneighborhood library,\u201d Musk said. \u201cThis is maybe the third or fourth grade.\nI tried to convince the librarian to order books for me. So then, I started to\nread the Encyclopaedia Britannica. That was so helpful. You don\u2019t know\nwhat you don\u2019t know. You realize there are all these things out there.\u201d\nElon, in fact, churned through two sets of encyclopedias\u2014a feat that did\nlittle to help him make friends. The boy had a photographic memory, and\nthe encyclopedias turned him into a fact factory. He came off as a classic\nknow-it-all. At the dinner table, Tosca would wonder aloud about the\ndistance from Earth to the Moon. Elon would spit out the exact\nmeasurement at perigee and apogee. \u201cIf we had a question, Tosca would\nalways say, \u2018Just ask genius boy,\u2019\u201d Maye said. \u201cWe could ask him about\nanything. He just remembered it.\u201d Elon cemented his bookworm reputation\nthrough his clumsy ways. \u201cHe\u2019s not very sporty,\u201d said Maye.\nMaye tells the story of Elon playing outside one night with his siblings\nand cousins. When one of them complained of being frightened by the dark,\nElon pointed out that \u201cdark is merely the absence of light,\u201d which did little\nto reassure the scared child. As a youngster, Elon\u2019s constant yearning to\ncorrect people and his abrasive manner put off other kids and added to his\nfeelings of isolation. Elon genuinely thought that people would be happy to\nhear about the flaws in their thinking. \u201cKids don\u2019t like answers like that,\u201d\nsaid Maye. \u201cThey would say, \u2018Elon, we are not playing with you anymore.\u2019\nI felt very sad as a mother because I think he wanted friends. Kimbal and\nTosca would bring home friends, and Elon wouldn\u2019t, and he would want to\nplay with them. But he was awkward, you know.\u201d Maye urged Kimbal and\nTosca to include Elon. They responded as kids will. \u201cBut Mom, he\u2019s not\nfun.\u201d As he got older, however, Elon would have strong, affectionate attachments to his siblings and cousins\u2014his mother\u2019s sister\u2019s sons. Though\nhe kept to himself at school, Elon had an outgoing nature with members of\nhis family and eventually took on the role of elder and chief instigator\namong them.\nFor a while, life inside the Musk household was quite good. The family\nowned one of the biggest houses in Pretoria thanks to the success of Errol\u2019s\nengineering business. There\u2019s a portrait of the three Musk children taken\nwhen Elon was about eight years old that shows three blond, fit children\nsitting next to each other on a brick porch with Pretoria\u2019s famous purple\njacaranda trees in the background. Elon has large, rounded cheeks and a\nbroad smile.\nThen, not long after the photo was taken, the family fell apart. His\nparents separated and divorced within the year. Maye moved with the kids\nto the family\u2019s holiday home in Durban, on South Africa\u2019s eastern coast.\nAfter a couple of years of this arrangement, Elon decided he wanted to live\nwith his father. \u201cMy father seemed sort of sad and lonely, and my mom had\nthree kids, and he didn\u2019t have any,\u201d Musk said. \u201cIt seemed unfair.\u201d Some\nmembers of Musk\u2019s family have bought into this idea that Elon\u2019s logical\nnature propelled him, while others claim that his father\u2019s mother, Cora,\nexerted a lot of pressure on the boy. \u201cI could not understand why he would\nleave this happy home I made for him\u2014this really happy home,\u201d said\nMaye. \u201cBut Elon is his own person.\u201d Justine Musk, Elon\u2019s ex-wife and the\nmother of his five boys, theorized that Elon identified more with the alpha\nmale of the house and wasn\u2019t bothered by the emotional aspect of the\ndecision. \u201cI don\u2019t think he was particularly close with either parent,\u201d Justine\nsaid, while describing the Musk clan overall as being cool and the opposite\nof doting. Kimbal later opted to live with Errol as well, saying simply that\nby nature a son wants to live with his father.\nWhenever the topic of Errol arrives, members of Elon\u2019s family clam up.\nThey\u2019re in agreement that he is not a pleasant man to be around but have\ndeclined to elaborate. Errol has since been remarried, and Elon has two,\nyounger half sisters of whom he\u2019s quite protective. Elon and his siblings\nseem determined not to bad-mouth Errol publicly, so as not to upset the\nsisters.\nThe basics are as follows: Errol\u2019s side of the family has deep South\nAfrican roots. The Musk clan can trace its presence in the country back\nabout two hundred years and claim an entry in Pretoria\u2019s first phone book. Errol\u2019s father, Walter Henry James Musk, was an army sergeant. \u201cI\nremember him almost never talking,\u201d Elon said. \u201cHe would just drink\nwhiskey and be grumpy and was very good at doing crossword puzzles.\u201d\nCora Amelia Musk, Errol\u2019s mother, was born in England to a family famed\nfor its intellectual genes. She embraced both the spotlight and her\ngrandchildren. \u201cOur grandmother had this very dominant personality and\nwas quite an enterprising woman,\u201d said Kimbal. \u201cShe was a very big\ninfluence in our lives.\u201d Elon considered his relationship with Cora\u2014or\nNana, as he called her\u2014particularly tight. \u201cAfter the divorce, she took care\nof me quite a lot,\u201d he said. \u201cShe would pick me up from school, and I would\nhang out with her playing Scrabble and that type of thing.\u201d\nOn the surface, life at Errol\u2019s house seemed grand. He had plenty of\nbooks for Elon to read from cover to cover and money to buy a computer\nand other objects that Elon desired. Errol took his children on numerous\ntrips overseas. \u201cIt was an amazingly fun time,\u201d said Kimbal. \u201cI have a lot of\nfun memories from that.\u201d Errol also impressed the kids with his intellect\nand dealt out some practical lessons. \u201cHe was a talented engineer,\u201d Elon\nsaid. \u201cHe knew how every physical object worked.\u201d Both Elon and Kimbal\nwere required to go to the sites of Errol\u2019s engineering jobs and learn how to\nlay bricks, install plumbing, fit windows, and put in electrical wiring.\n\u201cThere were fun moments,\u201d Elon said.\nErrol was what Kimbal described as \u201cultra-present and very intense.\u201d\nHe would sit Elon and Kimbal down and lecture at them for three to four\nhours without the boys being able to respond. He seemed to delight in being\nhard on the boys and sucked the fun out of common childhood diversions.\nFrom time to time, Elon tried to convince his dad to move to America and\noften talked about his intentions to live in the United States later in life.\nErrol countered such dreams by trying to teach Elon a lesson. He sent the\nhousekeepers away and had Elon do all the chores to let him know what it\nwas like \u201cto play American.\u201d\nWhile Elon and Kimbal declined to provide an exact recounting, they\nclearly experienced something awful and profound during those years with\ntheir father. They both talk about having to endure some form of\npsychological torture. \u201cHe definitely has serious chemical stuff,\u201d said\nKimbal. \u201cWhich I am sure Elon and I have inherited. It was a very\nemotionally challenging upbringing, but it made us who we are today.\u201d\nMaye bristled when the subject of Errol came up. \u201cNobody gets along with him,\u201d she said. \u201cHe is not nice to anyone. I don\u2019t want to tell stories because\nthey are horrendous. You know, you just don\u2019t talk about it. There are kids\nand grandkids involved.\u201d\nWhen asked to chat about Elon, Errol responded via e-mail: \u201cElon was a\nvery independent and focused child at home with me. He loved computer\nscience before anyone even knew what it was in South Africa and his\nability was widely recognized by the time he was 12 years old. Elon and his\nbrother Kimbal\u2019s activities as children and young men were so many and\nvaried that it\u2019s difficult to name just one, as they travelled together with me\nextensively in S. Africa and the world at large, visiting all the continents\nregularly from the age of six onwards. Elon and his brother and sister were\nand continue to be exemplary, in every way a father could want. I\u2019m very\nproud of what Elon\u2019s accomplished.\u201d\nErrol copied Elon on this e-mail, and Elon warned me off corresponding\nwith his father, insisting that his father\u2019s take on past events could not be\ntrusted. \u201cHe is an odd duck,\u201d Musk said. But, when pressed for more\ninformation, Musk dodged. \u201cIt would certainly be accurate to say that I did\nnot have a good childhood,\u201d he said. \u201cIt may sound good. It was not absent\nof good, but it was not a happy childhood. It was like misery. He\u2019s good at\nmaking life miserable\u2014that\u2019s for sure. He can take any situation no matter\nhow good it is and make it bad. He\u2019s not a happy man. I don\u2019t know . . .\nfuck . . . I don\u2019t know how someone becomes like he is. It would just cause\ntoo much trouble to tell you any more.\u201d Elon and Justine have vowed that\ntheir children will not be allowed to meet Errol.\nWhen Elon was nearly ten years old, he saw a computer for the first\ntime, at the Sandton City Mall in Johannesburg. \u201cThere was an electronics\nstore that mostly did hi-fi-type stuff, but then, in one corner, they started\nstocking a few computers,\u201d Musk said. He felt awed right away\u2014\u201cIt was\nlike, \u2018Whoa. Holy shit!\u2019\u201d\u2014by this machine that could be programmed to do\na person\u2019s bidding. \u201cI had to have that and then hounded my father to get\nthe computer,\u201d Musk said. Soon he owned a Commodore VIC-20, a popular\nhome machine that went on sale in 1980. Elon\u2019s computer arrived with five\nkilobytes of memory and a workbook on the BASIC programming\nlanguage. \u201cIt was supposed to take like six months to get through all the\nlessons,\u201d Elon said. \u201cI just got super OCD on it and stayed up for three days\nwith no sleep and did the entire thing. It seemed like the most super-\ncompelling thing I had ever seen.\u201d Despite being an engineer, Musk\u2019s father was something of a Luddite and dismissive of the machine. Elon recounted\nthat \u201che said it was just for games and that you\u2019d never be able to do real\nengineering on it. I just said, \u2018Whatever.\u2019\u201d\nWhile bookish and into his new computer, Elon quite often led Kimbal\nand his cousins (Kaye\u2019s children) Russ, Lyndon, and Peter Rive on\nadventures. They dabbled one year in selling Easter eggs door-to-door in\nthe neighborhood. The eggs were not well decorated, but the boys still\nmarked them up a few hundred percent for their wealthy neighbors. Elon\nalso spearheaded their work with homemade explosives and rockets. South\nAfrica did not have the Estes rocket kits popular among hobbyists, so Elon\nwould create his own chemical compounds and put them inside of canisters.\n\u201cIt is remarkable how many things you can get to explode,\u201d Elon said.\n\u201cSaltpeter, sulfur, and charcoal are the basic ingredients for gunpowder, and\nthen if you combine a strong acid with a strong alkaline, that will generally\nrelease a lot of energy. Granulated chlorine with brake fluid\u2014that\u2019s quite\nimpressive. I\u2019m lucky I have all my fingers.\u201d When not handling\nexplosives, the boys put on layers of clothing and goggles and shot each\nother with pellet guns. Elon and Kimbal raced dirt bikes against each other\nin sandlots until Kimbal flew off his bike one day and hurtled into a barbed\nwire fence.\nAs the years went on, the cousins took their entrepreneurial pursuits\nmore seriously, even attempting at one point to start a video arcade. Without\nany parents knowing, the boys picked out a spot for their arcade, got a\nlease, and started navigating the permit process for their business.\nEventually, they had to get someone over eighteen to sign a legal document,\nand neither the Rives\u2019 father nor Errol would oblige. It would take a couple\nof decades, but Elon and the Rives would eventually go into business\ntogether.\nThe boys\u2019 most audacious exploits may have been their trips between\nPretoria and Johannesburg. During the 1980s, South Africa could be a\nterribly violent place, and the thirty-five-mile train trip linking Pretoria and\nJohannesburg stood out as one of the world\u2019s more dangerous rides. Kimbal\ncounted the train journeys as formative experiences for him and Elon.\n\u201cSouth Africa was not a happy-go-lucky place, and that has an impact on\nyou. We saw some really rough stuff. It was part of an atypical upbringing\n\u2014just this insane set of experiences that changes how you view risk. You don\u2019t grow up thinking getting a job is the hard part. That\u2019s not interesting\nenough.\u201d\nThe boys ranged in age from about thirteen to sixteen and chased a mix\nof parties and geeky exploits in Johannesburg. During one jaunt, they went\nto a Dungeons & Dragons tournament. \u201cThat was us being nerd master\nsupremes,\u201d Musk said. All of the boys were into the role-playing game,\nwhich requires someone to help set the mood for a contest by imagining and\nthen describing a scene. \u201cYou have entered a room, and there is a chest in\nthe corner. What will you do? . . . You open the chest. You\u2019ve sprung a trap.\nDozens of goblins are on the loose.\u201d Elon excelled at this Dungeon Master\nrole and had memorized the texts detailing the powers of monsters and\nother characters. \u201cUnder Elon\u2019s leadership, we played the role so well and\nwon the tournament,\u201d said Peter Rive. \u201cWinning requires this incredible\nimagination, and Elon really set the tone for keeping people captivated and\ninspired.\u201d\nThe Elon that his peers encountered at school was far less inspirational.\nThroughout middle and high school, Elon bounced around a couple of\ninstitutions. He spent the equivalent of eighth and ninth grades at Bryanston\nHigh School. One afternoon Elon and Kimbal were sitting at the top of a\nflight of concrete stairs eating when a boy decided to go after Elon. \u201cI was\nbasically hiding from this gang that was fucking hunting me down for God\nknows fucking why. I think I accidentally bumped this guy at assembly that\nmorning and he\u2019d taken some huge offense at that.\u201d The boy crept up\nbehind Musk, kicked him in the head, and then shoved him down the stairs.\nMusk tumbled down the entire flight, and a handful of boys pounced on\nhim, some of them kicking Musk in the side and the ringleader bashing his\nhead against the ground. \u201cThey were a bunch of fucking psychos,\u201d Musk\nsaid. \u201cI blacked out.\u201d Kimbal watched in horror and feared for Elon\u2019s life.\nHe rushed down the stairs to find Elon\u2019s face bloodied and swollen. \u201cHe\nlooked like someone who had just been in the boxing ring,\u201d Kimbal said.\nElon then went to the hospital. \u201cIt was about a week before I could get back\nto school,\u201d Musk said. (During a news conference in 2013, Elon disclosed\nthat he\u2019d had a nose job to deal with the lingering effects of this beating.)\nFor three or four years, Musk endured relentless hounding at the hands\nof these bullies. They went so far as to beat up a boy that Musk considered\nhis best friend until the child agreed to stop hanging out with Musk.\n\u201cMoreover, they got him\u2014they got my best fucking friend\u2014to lure me out of hiding so they could beat me up,\u201d Musk said. \u201cAnd that fucking hurt.\u201d\nWhile telling this part of the story, Musk\u2019s eyes welled up and his voice\nquivered. \u201cFor some reason, they decided that I was it, and they were going\nto go after me nonstop. That\u2019s what made growing up difficult. For a\nnumber of years, there was no respite. You get chased around by gangs at\nschool who tried to beat the shit out of me, and then I\u2019d come home, and it\nwould just be awful there as well. It was just like nonstop horrible.\u201d\nMusk spent the latter stages of his high school career at Pretoria Boys\nHigh School, where a growth spurt and the generally better behavior of the\nstudents made life more bearable. While a public school by definition,\nPretoria Boys has functioned more like a private school for the last hundred\nyears. It\u2019s the place you send a young man to get him ready to attend\nOxford or Cambridge.\nThe boys from Musk\u2019s class remember him as a likable, quiet,\nunspectacular student. \u201cThere were four or five boys that were considered\nthe very brightest,\u201d said Deon Prinsloo, who sat behind Elon in some\nclasses. \u201cElon was not one of them.\u201d Such comments were echoed by a half\ndozen boys who also noted that Musk\u2019s lack of interest in sports left him\nisolated in the midst of an athletics-obsessed culture. \u201cHonestly, there were\njust no signs that he was going to be a billionaire,\u201d said Gideon Fourie,\nanother classmate. \u201cHe was never in a leadership position at school. I was\nrather surprised to see what has happened to him.\u201d\nWhile Musk didn\u2019t have any close friends at school, his eccentric\ninterests did leave an impression. One boy\u2014Ted Wood\u2014remembered\nMusk bringing model rockets to school and blasting them off during breaks.\nThis was not the only hint of his aspirations. During a science-class debate,\nElon gained attention for railing against fossil fuels in favor of solar power\n\u2014an almost sacrilegious stance in a country devoted to mining the earth\u2019s\nnatural resources. \u201cHe always had firm views on things,\u201d said Wood.\nTerency Beney, a classmate who stayed in touch with Elon over the years,\nclaimed that Musk had started fantasizing about colonizing other planets in\nhigh school as well.\nIn another nod to the future, Elon and Kimbal were chatting during a\nclass break outdoors when Wood interrupted them and asked what they\nwere going on about. \u201cThey said, \u2018We are talking about whether there is a\nneed for branch banking in the financial industry and whether we will move to paperless banking.\u2019 I remember thinking that was such an absurd\ncomment to make. I said, \u2018Yeah, that\u2019s great.\u2019\u201d*\nWhile Musk might not have been among the academic elite in his class,\nhe was among a handful of students with the grades and self-professed\ninterest to be selected for an experimental computer program. Students were\nplucked out of a number of schools and brought together to learn the\nBASIC, COBOL, and Pascal programming languages. Musk continued to\naugment these technological leanings with his love of science fiction and\nfantasy and tried his hand at writing stories that involved dragons and\nsupernatural beings. \u201cI wanted to write something like Lord of the Rings,\u201d\nhe said.\nMaye viewed these high school years through a mother\u2019s eyes and\nrecounted plenty of tales of Musk performing spectacular academic feats.\nThe video game he wrote, she said, impressed much older, more\nexperienced techies. He aced math exams well beyond his years. And he\nhad that incredible memory. The only reason he did not outrank the other\nboys was a lack of interest in the work prescribed by the school.\nAs Musk saw it, \u201cI just look at it as \u2018What grades do I need to get where\nI want to go?\u2019 There were compulsory subjects like Afrikaans, and I just\ndidn\u2019t see the point of learning that. It seemed ridiculous. I\u2019d get a passing\ngrade and that was fine. Things like physics and computers\u2014I got the\nhighest grade you can get in those. There needs to be a reason for a grade.\nI\u2019d rather play video games, write software, and read books than try and get\nan A if there\u2019s no point in getting an A. I can remember failing subjects in\nlike fourth and fifth grade. Then, my mother\u2019s boyfriend told me I\u2019d be held\nback if I didn\u2019t pass. I didn\u2019t actually know you had to pass the subjects to\nmove to the next grade. I got the best grades in class after that.\u201d\nAt seventeen, Musk left South Africa for Canada. He has recounted this\njourney quite often in the press and typically leans on two descriptions of\nthe motivation for his flight. The short version is that Musk wanted to get to\nthe United States as quickly as possible and could use Canada as a pit stop\nvia his Canadian ancestry. The second go-to story that Musk relies on has\nmore of a social conscience. South Africa required military service at the\ntime. Musk wanted to avoid joining the military, he has said, because it\nwould have forced him to participate in the apartheid regime.\nWhat rarely gets mentioned is that Musk attended the University of\nPretoria for five months before heading off on his grand adventure. He began pursuing physics and engineering but put lackluster effort into the\nwork and soon dropped out of school. Musk characterized the time at\nuniversity as just something to do while he awaited his Canadian\ndocumentation. In addition to being an inconsequential part of his life,\nMusk lazing through school to avoid South Africa\u2019s required military\nservice rather undermines the tale of a brooding, adventurous youth that he\nlikes to tell, which is likely why the stint at the University of Pretoria never\nseems to come up.\nThere\u2019s no question, though, that Musk had been pining to get to the\nUnited States on a visceral level for a long time. Musk\u2019s early inclination\ntoward computers and technology had fostered an intense interest in Silicon\nValley, and his trips overseas had reinforced the idea that America was the\nplace to get things done. South Africa, by contrast, presented far less\nopportunity for an entrepreneurial soul. As Kimbal put it, \u201cSouth Africa\nwas like a prison for someone like Elon.\u201d\nMusk\u2019s opportunity to flee arrived with a change in the law that allowed\nMaye to pass her Canadian citizenship to her children. Musk immediately\nbegan researching how to complete the paperwork for this process. It took\nabout a year to receive the approvals from the Canadian government and to\nsecure a Canadian passport. \u201cThat\u2019s when Elon said, \u2018I\u2019m leaving for\nCanada,\u2019\u201d Maye said. In these pre-Internet days, Musk had to wait three\nagonizing weeks to get a plane ticket. Once it arrived, and without\nflinching, he left home for good. 3 CANADA\nM\nUSK\u2019S GREAT ESCAPE TO CANADA WAS NOT WELL\nTHOUGHT OUT. He knew of a great-uncle in Montreal, hopped on a flight\nand hoped for the best. Upon landing in June 1988, Musk found a pay\nphone and tried to use directory assistance to find his uncle. When that\ndidn\u2019t work, he called his mother collect. She had bad news. Maye had sent\na letter to the uncle before Musk left and received a reply while her son was\nin transit. The uncle had gone to Minnesota, meaning Musk had nowhere to\nstay. Bags in hand, Musk headed for a youth hostel.\nAfter spending a few days in Montreal exploring the city, Musk tried to\ncome up with a long-term plan. Maye had family scattered all across\nCanada, and Musk began reaching out to them. He bought a countrywide\nbus ticket that let him hop on and off as he pleased for one hundred dollars,\nand opted to head to Saskatchewan, the former home of his grandfather.\nAfter a 1,900-mile bus ride, he ended up in Swift Current, a town of fifteen\nthousand people. Musk called a second cousin out of the blue from the bus\nstation and hitched a ride to his house.\nMusk spent the next year working a series of odd jobs around Canada.\nHe tended vegetables and shoveled out grain bins at a cousin\u2019s farm located\nin the tiny town of Waldeck. Musk celebrated his eighteenth birthday there,\nsharing a cake with the family he\u2019d just met and a few strangers from the\nneighborhood. After that, he learned to cut logs with a chain saw in\nVancouver, British Columbia. The hardest job Musk took came after a visit\nto the unemployment office. He inquired about the job with the best wage,\nwhich turned out to be a gig cleaning the boiler room of a lumber mill for\neighteen dollars an hour. \u201cYou have to put on this hazmat suit and then\nshimmy through this little tunnel that you can barely fit in,\u201d Musk said.\n\u201cThen, you have a shovel and you take the sand and goop and other residue,\nwhich is still steaming hot, and you have to shovel it through the same hole\nyou came through. There is no escape. Someone else on the other side has\nto shovel it into a wheelbarrow. If you stay in there for more than thirty minutes, you get too hot and die.\u201d Thirty people started out at the beginning\nof the week. By the third day, five people were left. At the end of the week,\nit was just Musk and two other men doing the work.\nAs Musk made his way around Canada, his brother, sister, and mother\nwere figuring out how to get there as well.* When Kimbal and Elon\neventually reunited in Canada, their headstrong, playful natures bloomed.\nElon ended up enrolling at Queen\u2019s University in Kingston, Ontario, in\n1989. (He picked Queen\u2019s over the University of Waterloo because he felt\nthere were more good-looking women at Queen\u2019s.)2 Outside of his studies,\nElon would read the newspaper alongside Kimbal, and the two of them\nwould identify interesting people they would like to meet. They then took\nturns cold-calling these people to ask if they were available to have lunch.\nAmong the harassed was the head of marketing for the Toronto Blue Jays\nbaseball team, a business writer for the Globe and Mail, and a top executive\nat the Bank of Nova Scotia, Peter Nicholson. Nicholson remembered the\nboys\u2019 call well. \u201cI was not in the habit of getting out-of-the-blue requests,\u201d\nhe said. \u201cI was perfectly prepared to have lunch with a couple of kids that\nhad that kind of gumption.\u201d It took six months to get on Nicholson\u2019s\ncalendar, but, sure enough, the Musk brothers made a three-hour train ride\nand showed up on time.\nNicholson\u2019s first exposure to the Musk brothers left him with an\nimpression many would share. Both presented themselves well and were\npolite. Elon, though, clearly came off as the geekier, more awkward\ncounterpoint to the charismatic, personable Kimbal. \u201cI became more\nimpressed and fascinated as I talked to them,\u201d Nicholson said. \u201cThey were\nso determined.\u201d Nicholson ended up offering Elon a summer internship at\nthe bank and became his trusted advisor.\nNot long after their initial meeting, Elon invited Peter Nicholson\u2019s\ndaughter Christie to his birthday party. Christie showed up at Maye\u2019s\nToronto apartment with a jar of homemade lemon curd in hand and was\ngreeted by Elon and about fifteen other people. Elon had never met Christie\nbefore, but he went right up to her and led her to a couch. \u201cThen, I believe\nthe second sentence out of his mouth was \u2018I think a lot about electric cars,\u2019\u201d\nChristie said. \u201cAnd then he turned to me and said, \u2018Do you think about\nelectric cars?\u2019\u201d The conversation left Christie, who is now a science writer,\nwith the distinct impression that Musk was handsome, affable, and a\ntremendous nerd. \u201cFor whatever reason, I was so struck by that moment on the sofa,\u201d she said. \u201cYou could tell that this person was very different. He\ncaptivated me in that way.\u201d\nWith her angular features and blond hair, Christie fit Musk\u2019s type, and\nthe two stayed in touch during Musk\u2019s time in Canada. They never really\ndated, but Christie found Musk interesting enough to have lengthy\nconversations with him on the phone. \u201cOne night he told me, \u2018If there was a\nway that I could not eat, so I could work more, I would not eat. I wish there\nwas a way to get nutrients without sitting down for a meal.\u2019 The enormity\nof his work ethic at that age and his intensity jumped out. It seemed like one\nof the more unusual things I had ever heard.\u201d\nA deeper relationship during this stint in Canada arose between Musk\nand Justine Wilson, a fellow student at Queen\u2019s. Leggy with long, brown\nhair, Wilson radiated romance and sexual energy. Justine had already fallen\nin love with an older man and then ditched him to go to college. Her next\nconquest was meant to wear a leather jacket and be a damaged, James Dean\nsort. As fortune would have it, however, the clean-cut, posh-sounding Musk\nspotted Wilson on campus and went right to work trying to date her. \u201cShe\nlooked pretty great,\u201d Musk said. \u201cShe was also smart and this intellectual\nwith sort of an edge. She had a black belt in tae kwon do and was semi-\nbohemian and, you know, like the hot chick on campus.\u201d He made his first\nmove just outside of her dorm, where he pretended to have bumped into her\nby accident and then reminded her that they had met previously at a party.\nJustine, only one week into school, agreed to Musk\u2019s proposal of an ice\ncream date. When he arrived to pick up Wilson, Musk found a note on the\ndorm room door, notifying him that he\u2019d been stood up. \u201cIt said that she had\nto go study for an exam and couldn\u2019t make it and that she was sorry,\u201d Musk\nsaid. Musk then hunted down Justine\u2019s best friend and did some research,\nasking where Justine usually studied and what her favorite flavor of ice\ncream was. Later, as Justine hid in the student center studying Spanish,\nMusk appeared behind her with a couple of melting chocolate chip ice\ncream cones in hand.\nWilson had dreamed of having a torrid romance with a writer. \u201cI wanted\nto be Sylvia and Ted,\u201d she said. What she fell for instead was a relentless,\nambitious geek. The pair attended the same abnormal-psychology class and\ncompared their grades following an exam. Justine notched a 97, Musk a 98.\n\u201cHe went back to the professor, and talked his way into the two points he\nlost and got a hundred,\u201d Justine said. \u201cIt felt like we were always competing.\u201d Musk had a romantic side as well. One time he sent Wilson a\ndozen roses, each with its own note, and he also gifted Wilson a copy of\nThe Prophet filled with handwritten romantic musings. \u201cHe can sweep you\noff your feet,\u201d Justine said.\nDuring their university years, the two youngsters were off and on, with\nMusk having to work hard to keep the relationship going. \u201cShe was hip and\ndated the coolest guys and wasn\u2019t interested in Elon at all,\u201d Maye said. \u201cSo\nthat was hard on him.\u201d Musk pursued a couple of other girls, but kept\nreturning to Justine. Any time she acted cool toward him, Musk responded\nwith his usual show of force. \u201cHe would call very insistently,\u201d she said.\n\u201cYou always knew it was Elon because the phone would never stop ringing.\nThe man does not take no for an answer. You can\u2019t blow him off. I do think\nof him as the Terminator. He locks his gaze on to something and says, \u2018It\nshall be mine.\u2019 Bit by bit, he won me over.\u201d\nCollege suited Musk. He worked on being less of a know-it-all, while\nalso finding a group of people who respected his intellectual abilities. The\nuniversity students were less inclined to laugh off or deride his opinionated\ntakes on energy, space, and whatever else was captivating him at the\nmoment. Musk had found people who responded to his ambition rather than\nmocking it, and he fed on this environment.\nNavaid Farooq, a Canadian who grew up in Geneva, ended up in\nMusk\u2019s freshman-year dormitory in the fall of 1990. Both men were placed\nin the international section where a Canadian student would get paired with\na student from overseas. Musk sort of broke the system, since he technically\ncounted as a Canadian but knew almost nothing about his surroundings. \u201cI\nhad a roommate from Hong Kong, and he was a really nice guy,\u201d Musk\nsaid. \u201cHe religiously attended every lecture, which was helpful, since I went\nto the least number of classes possible.\u201d For a time, Musk sold computer\nparts and full PCs in the dorm to make some extra cash. \u201cI could build\nsomething to suit their needs like a tricked-out gaming machine or a simple\nword processor that cost less than what they could get in a store,\u201d Musk\nsaid. \u201cOr if their computer didn\u2019t boot properly or had a virus, I\u2019d fix it. I\ncould pretty much solve any problem.\u201d Farooq and Musk bonded over their\nbackgrounds living abroad and a shared interest in strategy board games. \u201cI\ndon\u2019t think he makes friends easily, but he is very loyal to those he has,\u201d\nFarooq said. When the video game Civilization was released, the college\nchums spent hours building their empire, much to the dismay of Farooq\u2019s girlfriend, who was forgotten in another room. \u201cElon could lose himself for\nhours on end,\u201d Farooq said. The students also relished their loner lifestyles.\n\u201cWe are the kinds of people that can be by ourselves at a party and not feel\nawkward,\u201d Farooq said. \u201cWe can think to ourselves and not feel socially\nweird about it.\u201d\nMusk was more ambitious in college than he\u2019d been in high school. He\nstudied business, competed in public speaking contests, and began to\ndisplay the brand of intensity and competitiveness that marks his behavior\ntoday. After one economics exam, Musk, Farooq, and some other students\nin class came back to the dorms and began comparing notes to try to\nascertain how well they did on the test. It soon became clear that Musk had\na firmer grasp on the material than anyone else. \u201cThis was a group of fairly\nhigh achievers, and Elon stood way outside of the bell curve,\u201d Farooq said.\nMusk\u2019s intensity has continued to be a constant in their long relationship.\n\u201cWhen Elon gets into something, he develops just this different level of\ninterest in it than other people. That is what differentiates Elon from the rest\nof humanity.\u201d\nIn 1992, having spent two years at Queen\u2019s, Musk transferred to the\nUniversity of Pennsylvania on a scholarship. Musk saw the Ivy League\nschool as possibly opening some additional doors and went off in pursuit of\ndual degrees\u2014first an economics degree from the Wharton School and then\na bachelor\u2019s degree in physics. Justine stayed at Queen\u2019s, pursuing her\ndream of becoming a writer, and maintained a long-distance relationship\nwith Musk. Now and again, she would visit him, and the two would\nsometimes head off to New York for a romantic weekend.\nMusk blossomed even more at Penn, and really started to feel\ncomfortable while hanging out with his fellow physics students. \u201cAt Penn,\nhe met people that thought like him,\u201d Maye said. \u201cThere were some nerds\nthere. He so enjoyed them. I remember going for lunch with them, and they\nwere talking physics things. They were saying, \u2018A plus B equals pi squared\u2019\nor whatever. They would laugh out loud. It was cool to see him so happy.\u201d\nOnce again, however, Musk did not make many friends among the broader\nschool body. It\u2019s difficult to find former students who remember him being\nthere at all. But he did make one very close friend named Adeo Ressi, who\nwould go on to be a Silicon Valley entrepreneur in his own right and is to\nthis day as tight with Elon as anyone. Ressi is a lanky guy well over six feet tall and possesses an eccentric air.\nHe was the artistic, colorful foil to the studious, more buttoned-up Musk.\nBoth of the young men were transfer students and ended up being placed in\nthe funky freshman dorm. The lackluster social scene did not live up to\nRessi\u2019s expectations, and he talked Musk into renting a large house off\ncampus. They got the ten-bedroom home relatively cheap, since it was a frat\nhouse that had gone unrented. During the week, Musk and Ressi would\nstudy, but as the weekend approached, Ressi, in particular, would transform\nthe house into a nightclub. He covered the windows with trash bags to make\nit pitch black inside and decorated the walls with bright paints and whatever\nobjects he could find. \u201cIt was a full-out, unlicensed speakeasy,\u201d Ressi said.\n\u201cWe would have as many as five hundred people. We would charge five\ndollars, and it would be pretty much all you could drink\u2014beer and Jell-O\nshots and other things.\u201d\nCome Friday night, the ground around the house would shake from the\nintensity of the bass being pumped out by Ressi\u2019s speakers. Maye visited\none of the parties and discovered that Ressi had hammered objects into the\nwalls and lacquered them with glow-in-the-dark paint. She ended up\nworking the door as the coat check/money taker and grabbed a pair of\nscissors for protection as the cash piled up in a shoe box.\nA second house had fourteen rooms. Musk, Ressi, and one other person\nlived there. They fashioned tables by laying plywood on top of used kegs\nand came up with other makeshift furniture ideas. Musk returned home one\nday to find that Ressi had nailed his desk to the wall and then painted it in\nDay-Glo colors. Musk retaliated by pulling his desk down, painting it black,\nand studying. \u201cI\u2019m like, \u2018Dude, that\u2019s installation art in our party house,\u2019\u201d\nsaid Ressi. Remind Musk of this incident and he\u2019ll respond matter-of-factly,\n\u201cIt was a desk.\u201d\nMusk will have the occasional vodka and Diet Coke, but he\u2019s not a big\ndrinker and does not really care for the taste of alcohol. \u201cSomebody had to\nstay sober during these parties,\u201d Musk said. \u201cI was paying my own way\nthrough college and could make an entire month\u2019s rent in one night. Adeo\nwas in charge of doing cool shit around the house, and I would run the\nparty.\u201d As Ressi put it, \u201cElon was the most straight-laced dude you have\never met. He never drank. He never did anything. Zero. Literally nothing.\u201d\nThe only time Ressi had to step in and moderate Musk\u2019s behavior came\nduring video game binges that could go on for days. Musk\u2019s longtime interest in solar power and in finding other new ways\nto harness energy expanded at Penn. In December 1994, he had to come up\nwith a business plan for one of his classes and ended up writing a paper\ntitled \u201cThe Importance of Being Solar.\u201d The document started with a bit of\nMusk\u2019s wry sense of humor. At the top of the page, he wrote: \u201cThe sun will\ncome out tomorrow. . . .\u201d\u2014Little Orphan Annie on the subject of renewable\nenergy. The paper went on to predict a rise in solar power technology based\non materials improvements and the construction of large-scale solar plants.\nMusk delved deeply into how solar cells work and the various compounds\nthat can make them more efficient. He concluded the paper with a drawing\nof the \u201cpower station of the future.\u201d It depicted a pair of giant solar arrays in\nspace\u2014each four kilometers in width\u2014sending their juice down to Earth\nvia microwave beams to a receiving antenna with a seven-kilometer\ndiameter. Musk received a 98 on what his professor deemed a \u201cvery\ninteresting and well written paper.\u201d\nA second paper talked about taking research documents and books and\nelectronically scanning them, performing optical character recognition, and\nputting all of the information in a single database\u2014much like a mix\nbetween today\u2019s Google Books and Google Scholar. And a third paper\ndwelled on another of Musk\u2019s favorite topics\u2014ultracapacitors. In the forty-\nfour-page document, Musk is plainly jubilant over the idea of a new form of\nenergy storage that would suit his future pursuits with cars, planes, and\nrockets. Pointing to the latest research coming out of a lab in Silicon Valley,\nhe wrote: \u201cThe end result represents the first new means of storing\nsignificant amounts of electrical energy since the development of the\nbattery and fuel cell. Furthermore, because the Ultracapacitor retains the\nbasic properties of a capacitor, it can deliver its energy over one hundred\ntimes faster than a battery of equivalent weight, and be recharged just as\nquickly.\u201d Musk received a 97 for this effort and praise for \u201ca very thorough\nanalysis\u201d with \u201cexcellent financials!\u201d\nThe remarks from the professor were spot-on. Musk\u2019s clear, concise\nwriting is the work of a logician, moving from one point to the next with\nprecision. What truly stood out, though, was Musk\u2019s ability to master\ndifficult physics concepts in the midst of actual business plans. Even then,\nhe showed an unusual knack for being able to perceive a path from a\nscientific advance to a for-profit enterprise. As Musk began to think more seriously about what he would do after\ncollege, he briefly considered getting into the videogame business. He\u2019d\nbeen obsessed with video games since his childhood and had held a gaming\ninternship. But he came to see them as not quite grand enough a pursuit. \u201cI\nreally like computer games, but then if I made really great computer games,\nhow much effect would that have on the world,\u201d he said. \u201cIt wouldn\u2019t have\na big effect. Even though I have an intrinsic love of video games, I couldn\u2019t\nbring myself to do that as a career.\u201d\nIn interviews, Musk often makes sure that people know he had some\ntruly big ideas on his mind during this period of his life. As he tells it, he\nwould daydream at Queen\u2019s and Penn and usually end up with the same\nconclusion: he viewed the Internet, renewable energy, and space as the three\nareas that would undergo significant change in the years to come and as the\nmarkets where he could make a big impact. He vowed to pursue projects in\nall three. \u201cI told all my ex-girlfriends and my ex-wife about these ideas,\u201d he\nsaid. \u201cIt probably sounded like super-crazy talk.\u201d\nMusk\u2019s insistence on explaining the early origins of his passion for\nelectric cars, solar energy, and rockets can come off as insecure. It feels as if\nMusk is trying to shape his life story in a forced way. But for Musk, the\ndistinction between stumbling into something and having intent is\nimportant. Musk has long wanted the world to know that he\u2019s different from\nthe run-of-the-mill entrepreneur in Silicon Valley. He wasn\u2019t just sniffing\nout trends, and he wasn\u2019t consumed by the idea of getting rich. He\u2019s been in\npursuit of a master plan all along. \u201cI really was thinking about this stuff in\ncollege,\u201d he said. \u201cIt is not some invented story after the fact. I don\u2019t want\nto seem like a Johnny-come-lately or that I\u2019m chasing a fad or just being\nopportunistic. I\u2019m not an investor. I like to make technologies real that I\nthink are important for the future and useful in some sort of way.\u201d 4 ELON\u2019S FIRST START-UP\nI\nN THE SUMMER OF 1994, Musk and his brother, Kimbal, took their\nfirst steps toward becoming honest-to-God Americans. They set off on a\nroad trip across the country.\nKimbal had been working as a franchisee for College Pro Painters and\ndone well for himself, running what amounted to a small business. He sold\noff his part of the franchise and pooled the money with what Musk had on\nhand to buy a beat-up 1970s BMW 320i. The brothers began their trip near\nSan Francisco in August, as temperatures in California soared. The first part\nof the drive took them down to Needles, a city in the Mojave Desert. There\nthey experienced the sweaty thrill of 120-degree weather in a car with no\nair-conditioning and learned to love pit stops at Carl\u2019s Jr. burger joints,\nwhere they spent hours recuperating in the cold.\nThe trip provided plenty of time for your typical twenty-something\nhijinks and raging capitalist daydreaming. The Web had just started to\nbecome accessible to the public thanks to the rise of directory sites like\nYahoo! and tools like Netscape\u2019s browser. The brothers were tuned in to the\nInternet and thought they might like to start a company together doing\nsomething on the Web. From California to Colorado, Wyoming, South\nDakota, and Illinois, they took turns driving, brainstorming, and talking shit\nbefore heading back east to get Musk to school that fall. The best idea to\narise from the journey was an online network for doctors. This wasn\u2019t\nmeant to be something as ambitious as electronic health records but more of\na system for physicians to exchange information and collaborate. \u201cIt\nseemed like the medical industry was one that could be disrupted,\u201d Kimbal\nsaid. \u201cI went to work on a business plan and the sales and marketing side of\nit later, but it didn\u2019t fly. We didn\u2019t love it.\u201d\nMusk had spent the earlier part of that summer in Silicon Valley,\nholding down a pair of internships. By day, he worked at Pinnacle Research\nInstitute. Based in Los Gatos, Pinnacle was a much-ballyhooed start-up\nwith a team of scientists exploring ways in which ultracapacitors could be used as a revolutionary fuel source in electric and hybrid vehicles. The\nwork also veered\u2014at least conceptually\u2014into more bizarre territory. Musk\ncould talk at length about how ultracapacitors might be used to build laser-\nbased sidearms in the tradition of Star Wars and just about any other\nfuturistic film. The laser guns would release rounds of enormous energy,\nand then the shooter would replace an ultracapacitor at the base of the gun,\nmuch like swapping out a clip of bullets, and start blasting away again.\nUltracapacitors also looked promising as the power supplies for missiles.\nThey were more resilient than batteries under the mechanical stresses of a\nlaunch and would hold a more consistent charge over long periods of time.\nMusk fell in love with the work at Pinnacle and began using it as the basis\nfor some of his business plan experiments at Penn and for his industrialist\nfantasies.\nIn the evenings, Musk headed to Rocket Science Games, a start-up\nbased in Palo Alto that wanted to create the most advanced video games\never made by moving them off cartridges and onto CDs that could hold\nmore information. The CDs would in theory allow them to bring\nHollywood-style storytelling and production quality to the games. A team\nof budding all-stars who were a mix of engineers and film people was\nassembled to pull off the work. Tony Fadell, who would later drive much of\nthe development of both the iPod and iPhone at Apple, worked at Rocket\nScience, as did the guys who developed the QuickTime multimedia\nsoftware for Apple. They also had people who worked on the original Star\nWars effects at Industrial Light & Magic and some who did games at\nLucasArts Entertainment. Rocket Science gave Musk a flavor for what\nSilicon Valley had to offer both from a talent and culture perspective. There\nwere people working at the office twenty-four hours a day, and they didn\u2019t\nthink it at all odd that Musk would turn up around 5 P.M. every evening to\nstart his second job. \u201cWe brought him in to write some very menial low-\nlevel code,\u201d said Peter Barrett, an Australian engineer who helped start the\ncompany. \u201cHe was completely unflappable. After a short while, I don\u2019t\nthink anyone was giving him any direction, and he ended up making what\nhe wanted to make.\u201d\nSpecifically, Musk had been asked to write the drivers that would let\njoysticks and mice communicate with various computers and games.\nDrivers are the same types of annoying files that you have to install to get a\nprinter or camera working with a home computer\u2014true grunt work. A self- taught programmer, Musk fancied himself quite good at coding and\nassigned himself to more ambitious jobs. \u201cI was basically trying to figure\nout how you could multitask stuff, so you could read video from a CD,\nwhile running a game at the same time,\u201d Musk said. \u201cAt the time, you could\ndo one or the other. It was this complicated bit of assembly programming.\u201d\nComplicated indeed. Musk had to issue commands that spoke directly to a\ncomputer\u2019s main microprocessor and fiddled with the most basic functions\nthat made the machine work. Bruce Leak, the former lead engineer behind\nApple\u2019s QuickTime, had overseen the hiring of Musk and marveled at his\nability to pull all-nighters. \u201cHe had boundless energy,\u201d Leak said. \u201cKids\nthese days have no idea about hardware or how stuff works, but he had a PC\nhacker background and was not afraid to just go figure things out.\u201d\nMusk found in Silicon Valley a wealth of the opportunity he\u2019d been\nseeking and a place equal to his ambitions. He would return two summers in\na row and then bolt west permanently after graduating with dual degrees\nfrom Penn. He initially intended to pursue a doctorate in materials science\nand physics at Stanford and to advance the work he\u2019d done at Pinnacle on\nultracapacitors. As the story goes, Musk dropped out of Stanford after two\ndays, finding the Internet\u2019s call irresistible. He talked Kimbal into moving\nto Silicon Valley as well, so they could conquer the Web together.\nThe first inklings of a viable Internet business had come to Musk during\nhis internships. A salesperson from the Yellow Pages had come into one of\nthe start-up offices. He tried to sell the idea of an online listing to\ncomplement the regular listing a company would have in the big, fat Yellow\nPages book. The salesman struggled with his pitch and clearly had little\ngrasp of what the Internet actually was or how someone would find a\nbusiness on it. The flimsy pitch got Musk thinking, and he reached out to\nKimbal, talking up the idea of helping businesses get online for the first\ntime.\n\u201cElon said, \u2018These guys don\u2019t know what they are talking about. Maybe\nthis is something we can do,\u2019\u201d Kimbal said. This was 1995, and the brothers\nwere about to form Global Link Information Network, a start-up that would\neventually be renamed Zip2. (For details on the controversy surrounding\nZip2\u2019s founding and Musk\u2019s academic record, see Appendix 1.)\nThe Zip2 idea was ingenious. Few small businesses in 1995 understood\nthe ramifications of the Internet. They had little idea how to get on it and\ndidn\u2019t really see the value in creating a website for their business or even in having a Yellow Pages\u2013like listing online. Musk and his brother hoped to\nconvince restaurants, clothing shops, hairdressers, and the like that the time\nhad come for them to make their presence known to the Web-surfing public.\nZip2 would create a searchable directory of businesses and tie this into\nmaps. Musk often explained the concept through pizza, saying that\neveryone deserved the right to know the location of their closest pizza\nparlor and the turn-by-turn directions to get there. This may seem obvious\ntoday\u2014think Yelp meets Google Maps\u2014but back then, not even stoners\nhad dreamed up such a service.\nThe Musk brothers brought Zip2 to life at 430 Sherman Avenue in Palo\nAlto. They rented a studio-apartment-sized office\u2014twenty feet by thirty\nfeet\u2014and acquired some basic furniture. The three-story building had its\nquirks. There were no elevators, and the toilets often backed up. \u201cIt was\nliterally a shitty place to work,\u201d said an early employee. To get a fast\nInternet connection, Musk struck a deal with Ray Girouard, an entrepreneur\nwho ran an Internet service provider operation from the floor below the\nZip2 offices. According to Girouard, Musk drilled a hole in the drywall near\nthe Zip2 door and then strung an Ethernet cable down the stairwell to the\nISP. \u201cThey were slow to pay a couple of times but never stiffed me on the\nbill,\u201d Girouard said.\nMusk did all of the original coding behind the service himself, while the\nmore amiable Kimbal looked to ramp up the door-to-door sales operation.\nMusk had acquired a cheap license to a database of business listings in the\nBay Area that would give a business\u2019s name and its address. He then\ncontacted Navteq, a company that had spent hundreds of millions of dollars\nto create digital maps and directions that could be used in early GPS\nnavigation-style devices, and struck a masterful bargain. \u201cWe called them\nup, and they gave us the technology for free,\u201d said Kimbal. Musk merged\nthe two databases together to get a rudimentary system up and running.\nOver time, Zip2\u2019s engineers had to augment this initial data haul with more\nmaps to cover areas outside of major metropolitan areas and to build custom\nturn-by-turn directions that would look good and work well on a home\ncomputer.\nErrol Musk gave his sons $28,000 to help them through this period, but\nthey were more or less broke after getting the office space, licensing\nsoftware, and buying some equipment. For the first three months of Zip2\u2019s\nlife, Musk and his brother lived at the office. They had a small closet where they kept their clothes and would shower at the YMCA. \u201cSometimes we ate\nfour meals a day at Jack in the Box,\u201d Kimbal said. \u201cIt was open twenty-four\nhours, which suited our work schedule. I got a smoothie one time, and there\nwas something in it. I just pulled it out and kept drinking. I haven\u2019t been\nable to eat there since, but I can still recite their menu.\u201d\nNext, the brothers rented a two-bedroom apartment. They didn\u2019t have\nthe money or the inclination to get furniture. So there were just a couple of\nmattresses on the floor. Musk somehow managed to convince a young\nSouth Korean engineer to come work at Zip2 as an intern in exchange for\nroom and board. \u201cThis poor kid thought he was coming over for a job at a\nbig company,\u201d Kimbal said. \u201cHe ended up living with us and had no idea\nwhat he was getting into.\u201d One day, the intern drove the Musks\u2019 battered\nBMW 320i to work, and a wheel came off en route. The axle dug into the\nstreet at the intersection of Page Mill Road and El Camino Real, and the\ngroove it carved out remained visible for years.\nZip2 may have been a go-go Internet enterprise aimed at the\nInformation Age, but getting it off the ground required old-fashioned door-\nto-door salesmanship. Businesses needed to be persuaded of the Web\u2019s\nbenefits and charmed into paying for the unknown. In late 1995, the Musk\nbrothers began making their first hires and assembling a motley sales team.\nJeff Heilman, a free-spirited twenty-year-old trying to figure out what to do\nwith his life, arrived as one of Zip2\u2019s first recruits. He\u2019d been watching TV\nlate one night with his dad and seen a Web address printed at the bottom of\nthe screen during a commercial. \u201cIt was for something dot-com,\u201d Heilman\nsaid. \u201cI remember sitting there and asking my dad what we were looking at.\nHe said he didn\u2019t know, either. That\u2019s when I realized I had to go find me\nsome Internet.\u201d Heilman spent a couple of weeks trying to chat up people\nwho could explain the Internet to him and then stumbled on a two-by-two-\ninch Zip2 job listing in the San Jose Mercury News. \u201cInternet Sales Apply\nHere!\u201d it read, and Heilman got the gig. A handful of other salespeople\njoined him and worked for commissions.\nMusk never seemed to leave the office. He slept, not unlike a dog, on a\nbeanbag next to his desk. \u201cAlmost every day, I\u2019d come in at seven thirty or\neight A.M., and he\u2019d be asleep right there on that bag,\u201d Heilman said.\n\u201cMaybe he showered on the weekends. I don\u2019t know.\u201d Musk asked those\nfirst employees of Zip2 to give him a kick when they arrived, and he\u2019d\nwake up and get back to work. While Musk did his possessed coder thing, Kimbal became the rah-rah sales leader. \u201cKimbal was the eternal optimist,\nand he was very, very uplifting,\u201d Heilman said. \u201cI had never met anyone\nquite like him.\u201d Kimbal sent Heilman to the high-end Stanford shopping\nmall and to University Avenue, the main drag in Palo Alto, to coax retailers\ninto signing up with Zip2, explaining that a sponsored listing would send a\ncompany to the top of search results. The big problem, of course, was that\nno one was buying. Week after week, Heilman knocked on doors and\nreturned to the office with very little to report in the way of good news. The\nnicest responses came from the people who told Heilman that advertising\non the Internet sounded like the dumbest thing they had ever heard of. Most\noften, the shop owners just told Heilman to leave and stop bothering them.\nWhen lunchtime came around, the Musks would reach into a cigar box\nwhere they kept some cash, take Heilman out, and get the depressing status\nreports on the sales.\nCraig Mohr, another early employee, gave up his job selling real estate\nto hawk Zip2\u2019s service. He decided to court auto dealerships because they\nusually spent lots of money on advertising. He told them about Zip2\u2019s main\nwebsite\u2014www.totalinfo.com\u2014and tried to convince them that demand was\nhigh to get a listing like www.totalinfo.com/toyotaofsiliconvalley. The\nservice did not always work when Mohr demonstrated it or it would load\nvery slowly, as was common back then. This forced him to talk the\ncustomers into imagining Zip2\u2019s potential. \u201cOne day I came back with\nabout nine hundred dollars in checks,\u201d Mohr said. \u201cI walked into the office\nand asked the guys what they wanted me to do with the money. Elon\nstopped pounding his keyboard, leaned out from behind his monitor, and\nsaid, \u2018No way, you\u2019ve got money.\u2019\u201d\nWhat kept the employees\u2019 spirits up were the continuous improvements\nMusk made with the Zip2 software. The service had morphed from a proof-\nof-concept to an actual product that could be used and demoed. Ever\nmarketing savvy, the Musk brothers tried to make their Web service seem\nmore important by giving it an imposing physical body. Musk built a huge\ncase around a standard PC and lugged the unit onto a base with wheels.\nWhen prospective investors would come by, Musk would put on a show and\nroll this massive machine out so that it appeared like Zip2 ran inside of a\nmini-supercomputer. \u201cThe investors thought that was impressive,\u201d Kimbal\nsaid. Heilman also noticed that the investors bought into Musk\u2019s slavish\ndevotion to the company. \u201cEven then, as essentially a college kid with zits, Elon had this drive that this thing\u2014whatever it was\u2014had to get done and\nthat if he didn\u2019t do it, he\u2019d miss his shot,\u201d Heilman said. \u201cI think that\u2019s what\nthe VCs saw\u2014that he was willing to stake his existence on building out this\nplatform.\u201d Musk actually said as much to one venture capitalist, informing\nhim, \u201cMy mentality is that of a samurai. I would rather commit seppuku\nthan fail.\u201d\nEarly on in the Zip2 venture, Musk acquired an important confidant,\nwho tempered some of these more dramatic impulses. Greg Kouri, a\nCanadian businessman in his mid-thirties, had met the Musks in Toronto\nand bought into the early Zip2 brainstorming. The boys had showed up at\nhis door one morning to inform Kouri that they intended to head to\nCalifornia to give the business a shot. Still in his red bathrobe, Kouri went\nback into the house, dug around for a couple of minutes, and came back\nwith a wad of $6,000. In early 1996, he moved to California and joined\nZip2 as a cofounder.\nKouri, who had done a number of real estate deals in the past and had\nactual business experience and skills at reading people, served as the adult\nsupervision at Zip2. The Canadian had a knack for calming Musk and\nended up becoming something of a mentor. \u201cReally smart people sometimes\ndon\u2019t understand that not everyone can keep up with them or go as fast,\u201d\nsaid Derek Proudian, a venture capitalist who would become Zip2\u2019s chief\nexecutive officer. \u201cGreg is one of the few people that Elon would listen to\nand had a way of putting things in context for him.\u201d Kouri also used to\nreferee fistfights between Elon and Kimbal, in the middle of the office.\n\u201cI don\u2019t get in fights with anyone else, but Elon and I don\u2019t have the\nability to reconcile a vision other than our own,\u201d Kimbal said. During a\nparticularly nasty scrap over a business decision, Elon ripped some skin off\nhis fist and had to go get a tetanus shot. Kouri put an end to the fights after\nthat. (Kouri died of a heart attack in 2012 at the age of fifty-one, having\nmade a fortune investing in Musk\u2019s companies. Musk attended his funeral.\n\u201cWe owe him a lot,\u201d said Kimbal.)\nIn early 1996, Zip2 underwent a massive change. The venture capital\nfirm Mohr Davidow Ventures had caught wind of a couple of South African\nboys trying to make a Yellow Pages for the Internet and met with the\nbrothers. Musk, while raw in his presentation skills, pitched the company\nwell enough, and the investors came away impressed with his energy. Mohr\nDavidow invested $3 million into the company.* With these funds in hand, the company officially changed its name from Global Link to Zip2\u2014the\nidea being zip to here, zip to there\u2014moved to a larger office at 390\nCambridge Avenue in Palo Alto, and began hiring talented engineers. Zip2\nalso shifted its business strategy. At the time, the company had built one of\nthe best direction systems on the Web. Zip2 would advance this technology\nand take it from focusing just on the Bay Area to having a national scope.\nThe company\u2019s main focus, however, would be an altogether new play.\nInstead of selling its service door-to-door, Zip2 would create a software\npackage that could be sold to newspapers, which would in turn build their\nown directories for real estate, auto dealers, and classifieds. The newspapers\nwere late understanding how the Internet would impact their businesses,\nand Zip2\u2019s software would give them a quick way of getting online without\nneeding to develop all their own technology from scratch. For its part, Zip2\ncould chase bigger prey and get a cut of a nationwide network of listings.\nThis transition of the business model and the company\u2019s makeup would\nbe a seminal moment in Musk\u2019s life. The venture capitalists pushed Musk\ninto the role of chief technology officer and hired Rich Sorkin as the\ncompany\u2019s CEO. Sorkin had worked at Creative Labs, a maker of audio\nequipment, and run the business development group at the company, where\nhe steered a number of investments in Internet start-ups. Zip2\u2019s investors\nsaw him as experienced and clued in to the Web. While Musk agreed to the\narrangement, he came to resent giving up control of Zip2. \u201cProbably the\nbiggest regret the whole time I worked with him was that he had made a\ndeal with the devil with Mohr Davidow,\u201d said Jim Ambras, the vice\npresident of engineering at Zip2. \u201cElon didn\u2019t have any operational\nresponsibilities, and he wanted to be CEO.\u201d\nAmbras had worked at Hewlett-Packard Labs and Silicon Graphics Inc.\nand exemplified the high-caliber talent Zip2 brought on after the first wave\nof money arrived. Silicon Graphics, a maker of high-end computers beloved\nby Hollywood, was the flashiest company of its day and had hoarded the\nelite geeks of Silicon Valley. And yet Ambras used the promise of Internet\nriches to poach a team of SGI\u2019s smartest engineers over to Zip2. \u201cOur\nattorneys got a letter from SGI saying that we were cherry-picking the very\nbest guys,\u201d Ambras said. \u201cElon thought that was fantastic.\u201d\nWhile Musk had exceled as a self-taught coder, his skills weren\u2019t nearly\nas polished as those of the new hires. They took one look at Zip2\u2019s code and\nbegan rewriting the vast majority of the software. Musk bristled at some of their changes, but the computer scientists needed just a fraction of the lines\nof code that Musk used to get their jobs done. They had a knack for\ndividing software projects into chunks that could be altered and refined\nwhereas Musk fell into the classic self-taught coder trap of writing what\ndevelopers call hairballs\u2014big, monolithic hunks of code that could go\nberserk for mysterious reasons. The engineers also brought a more refined\nworking structure and realistic deadlines to the engineering group. This was\na welcome change from Musk\u2019s approach, which had been to set overly\noptimistic deadlines and then try to get engineers to work nonstop for days\non end to meet the goals. \u201cIf you asked Elon how long it would take to do\nsomething, there was never anything in his mind that would take more than\nan hour,\u201d Ambras said. \u201cWe came to interpret an hour as really taking a day\nor two and if Elon ever did say something would take a day, we allowed for\na week or two weeks.\u201d\nStarting Zip2 and watching it grow imbued Musk with self-confidence.\nTerence Beney, one of Musk\u2019s high school friends, came to California for a\nvisit and noticed the change in Musk\u2019s character right away. He watched\nMusk confront a nasty landlord who had been giving his mother, who was\nrenting an apartment in town, a hard time. \u201cHe said, \u2018If you\u2019re going to\nbully someone, bully me.\u2019 It was startling to see him take over the situation.\nThe last time I had seen him he was this geeky, awkward kid who would\nsometimes lose his temper. He was the kid you would pick on to get a\nresponse. Now he was confident and in control.\u201d Musk also began\nconsciously trying to manage his criticism of others. \u201cElon is not someone\nwho would say, \u2018I feel you. I see your point of view,\u2019\u201d said Justine.\n\u201cBecause he doesn\u2019t have that \u2018I feel you\u2019 dimension there were things that\nseemed obvious to other people that weren\u2019t that obvious to him. He had to\nlearn that a twenty-something-year-old shouldn\u2019t really shoot down the\nplans of older, senior people and point out everything wrong with them. He\nlearned to modify his behavior in certain ways. I just think he comes at the\nworld through strategy and intellect.\u201d The personality tweaks worked with\nvarying degrees of success. Musk still tended to drive the young engineers\nmad with his work demands and blunt criticism. \u201cI remember being in a\nmeeting once brainstorming about a new product\u2014a new-car site,\u201d said\nDoris Downes, the creative director at Zip2. \u201cSomeone complained about a\ntechnical change that we wanted being impossible. Elon turned and said, \u2018I\ndon\u2019t really give a damn what you think,\u2019 and walked out of the meeting. For Elon, the word no does not exist, and he expects that attitude from\neveryone around him.\u201d Periodically, Musk let loose on the more senior\nexecutives as well. \u201cYou would see people come out of the meetings with\nthis disgusted look on their face,\u201d Mohr, the salesman, said. \u201cYou don\u2019t get\nto where Elon is now by always being a nice guy, and he was just so driven\nand sure of himself.\u201d\nAs Musk tried to come to terms with the changes the investors had\ninflicted on Zip2, he did enjoy some of the perks of having big-money\nbacking. The financiers helped the Musk brothers with their visas. They\nalso gave them $30,000 each to buy cars. Musk and Kimbal had traded in\ntheir dilapidated BMW for a dilapidated sedan that they spray-painted with\npolka dots. Kimbal upgraded from that to a BMW 3 Series, and Musk\nbought a Jaguar E-Type. \u201cIt kept breaking down, and would arrive at the\noffice on a flatbed,\u201d Kimbal said. \u201cBut Elon always thought big.\u201d*\nAs a bonding exercise one weekend, Musk, Ambras, a few other\nemployees and friends took off for a bike ride through the Saratoga Gap\ntrail in the Santa Cruz Mountains. Most of the riders had been training and\nwere accustomed to strenuous sessions and the summer\u2019s heat. They set up\nthe mountains at a furious pace. After an hour, Russ Rive, Musk\u2019s cousin,\nreached the top and proceeded to vomit. Right behind him were the rest of\nthe cyclists. Then, fifteen minutes later, Musk became visible to the group.\nHis face had turned purple, and sweat poured out of him, and he made it to\nthe top. \u201cI always think back to that ride. He wasn\u2019t close to being in the\ncondition needed for it,\u201d Ambras said. \u201cAnyone else would have quit or\nwalked up their bike. As I watched him climb that final hundred feet with\nsuffering all over his face, I thought, That\u2019s Elon. Do or die but don\u2019t give\nup.\u201d\nMusk continued to be a ball of energy around the office as well. Ahead\nof visits by venture capitalists and other investors, Musk would rally the\ntroops and instruct them all to get on the phone to create a buzzy\natmosphere. He also formed a video-game team to participate in\ncompetitions around Quake, a first-person-shooter game. \u201cWe competed in\none of the first nationwide tournaments,\u201d Musk said. \u201cWe came in second,\nand we would have come in first, but one of our top players\u2019 machine\ncrashed because he had pushed his graphics card too hard. We won a few\nthousand dollars.\u201d Zip2 had remarkable success courting newspapers. The New York Times,\nKnight Ridder, Hearst Corporation, and other media properties signed up to\nits service. Some of these companies contributed $50 million in additional\nfunding for Zip2. Services like Craigslist with its free online classifieds had\njust started to appear, and the newspapers needed some course of action.\n\u201cThe newspapers knew they were in trouble with the Internet, and the idea\nwas to sign up as many of them as possible,\u201d Ambras said. \u201cThey wanted\nclassifieds and listings for real estate, automotive, and entertainment and\ncould use us as a platform for all these online services.\u201d Zip2 acquired a\ntrademark for its \u201cWe Power the Press\u201d slogan and the influx of cash kept\nZip2 growing fast. Company headquarters were soon so crowded that one\ndesk ended up directly in front of the women\u2019s bathroom. In 1997, Zip2\nmoved into flashier, more spacious digs at 444 Castro Street in Mountain\nView.\nIt irritated Musk that Zip2 had become a behind-the-scenes player to the\nnewspapers. He believed the company could offer interesting services\ndirectly to consumers and encouraged the purchase of the domain name\ncity.com with the hopes of turning it into a consumer destination. But the\nlure of the media companies\u2019 money kept Sorkin and the board on a\nconservative path, and they decided to worry about a consumer push down\nthe road.\nIn April 1998, Zip2 announced a blockbuster move to double down on\nits strategy. It would merge with its main competitor CitySearch in a deal\nvalued at around $300 million. The new company would retain the\nCitySearch name, while Sorkin would head up the venture. On paper, the\nunion looked very much like a merger of equals. CitySearch had built up an\nextensive set of directories for cities around the country. It also appeared to\nhave strong sales and marketing teams that would complement the talented\nengineers at Zip2. The merger had been announced in the press and seemed\ninevitable.\nThe opinions on what happened next vary greatly. The logistics of the\nsituation required the two companies to go over each other\u2019s books and to\nfigure out which employees would be fired to avoid a duplication of roles.\nThis process raised some questions about how frank CitySearch had been\nwith its financials and rankled some executives at Zip2 who could see their\npositions being diminished or erased altogether at the new company. One\nfaction inside Zip2 argued that the deal should be abandoned, while Sorkin demanded that it go through. Musk, who had been an early advocate of the\ndeal, turned against it. In May 1998, the two companies canceled the\nmerger, and the press pounced, making a big deal of the chaotic bust-up.\nMusk urged Zip2\u2019s board to oust Sorkin and reinstate him as CEO of Zip2.\nThe board declined. Instead, Musk lost his chairman title, and Sorkin was\nreplaced by Derek Proudian, a venture capitalist with Mohr Davidow.\nSorkin considered Musk\u2019s behavior through the whole affair atrocious and\nlater pointed to the board\u2019s reaction and Musk\u2019s demotion as evidence that\nthey felt the same way. \u201cThere was a lot of backlash and finger-pointing,\u201d\nProudian said. \u201cElon wanted to be CEO, but I said, \u2018This is your first\ncompany. Let\u2019s find an acquirer and make some money, so you can do your\nsecond, third, and fourth company.\u2019\u201d\nWith the deal busted, Zip2 found itself in a predicament. It was losing\nmoney. Musk still wanted to go the consumer route, but Proudian feared\nthat would take too much capital. Microsoft had mounted a charge into the\nsame market, and start-ups with mapping, real estate, and automotive ideas\nmultiplied. The Zip2 engineers were deflated and worried that they might\nnot be able to outrun the competition. Then, in February 1999, the PC\nmaker Compaq Computer suddenly offered to pay $307 million in cash for\nZip2. \u201cIt was like pennies from heaven,\u201d said Ed Ho, a former Zip2\nexecutive. Zip2\u2019s board accepted the offer, and the company rented out a\nrestaurant in Palo Alto and threw a huge party. Mohr Davidow had made\nback twenty times its original investment, and Musk and Kimbal had come\naway with $22 million and $15 million, respectively. Musk never\nentertained the idea of sticking around at Compaq. \u201cAs soon as it was clear\nthe company would be sold, Elon was on to his next project,\u201d Proudian said.\nFrom that point on, Musk would fight to maintain control of his companies\nand stay CEO. \u201cWe were overwhelmed and just thought these guys must\nknow what they\u2019re doing,\u201d Kimbal said. \u201cBut they\u2019 didn\u2019t. There was no\nvision once they took over. They were investors, and we got on well with\nthem, but the vision had just disappeared from the company.\u201d\nYears later, after he had time to reflect on the Zip2 situation, Musk\nrealized that he could have handled some of the situations with employees\nbetter. \u201cI had never really run a team of any sort before,\u201d Musk said. \u201cI\u2019d\nnever been a sports captain or a captain of anything or managed a single\nperson. I had to think, Okay, what are the things that affect how a team\nfunctions. The first obvious assumption would be that other people will behave like you. But that\u2019s not true. Even if they would like to behave like\nyou, they don\u2019t necessarily have all the assumptions or information that you\nhave in your mind. So, if I know a certain set of things, and I talk to a\nreplica of myself but only communicate half the information, you can\u2019t\nexpect that the replica would come to the same conclusion. You have to put\nyourself in a position where you say, \u2018Well, how would this sound to them,\nknowing what they know?\u2019\u201d\nEmployees at Zip2 would go home at night, come back, and find that\nMusk had changed their work without talking to them, and Musk\u2019s\nconfrontational style did more harm than good. \u201cYeah, we had some very\ngood software engineers at Zip2, but I mean, I could code way better than\nthem. And I\u2019d just go in and fix their fucking code,\u201d Musk said. \u201cI would be\nfrustrated waiting for their stuff, so I\u2019m going to go and fix your code and\nnow it runs five times faster, you idiot. There was one guy who wrote a\nquantum mechanics equation, a quantum probability on the board, and he\ngot it wrong. I\u2019m like, \u2018How can you write that?\u2019 Then I corrected it for\nhim. He hated me after that. Eventually, I realized, Okay, I might have fixed\nthat thing but now I\u2019ve made the person unproductive. It just wasn\u2019t a good\nway to go about things.\u201d\nMusk, the dot-com striver, had been both lucky and good. He had a\ndecent idea, turned it into a real service, and came out of the dot-com tumult\nwith cash in his pockets, which was better than what many of his\ncompatriots could say. The process had been painful. Musk had yearned to\nbe a leader, but the people around him struggled to see how Musk as the\nCEO could work. As far as Musk was concerned, they were all wrong, and\nhe set out to prove his point with what would end up being even more\ndramatic results. 5 PAYPAL MAFIA BOSS\nT\nHE SALE OF ZIP2 INFUSED ELON MUSK WITH A NEW BRAND\nOF CONFIDENCE. Much like the video-game characters he adored, Musk\nhad leveled up. He had solved Silicon Valley and become what everyone at\nthe time wanted to be\u2014a dot-com millionaire. His next venture would need\nto live up to his rapidly inflating ambition. This left Musk searching for an\nindustry that had tons of money and inefficiencies that he and the Internet\ncould exploit. Musk began thinking back to his time as an intern at the Bank\nof Nova Scotia. His big takeaway from that job, that bankers are rich and\ndumb, now had the feel of a massive opportunity.\nDuring his time working for the head of strategy at the bank in the early\n1990s, Musk had been asked to take a look at the company\u2019s third-world\ndebt portfolio. This pool of money went by the depressing name of \u201cless-\ndeveloped country debt,\u201d and Bank of Nova Scotia had billions of dollars of\nit. Countries throughout South America and elsewhere had defaulted in the\nyears prior, forcing the bank to write down some of its debt value. Musk\u2019s\nboss wanted him to dig into the bank\u2019s holdings as a learning experiment\nand try to determine how much the debt was actually worth.\nWhile pursuing this project, Musk stumbled upon what seemed like an\nobvious business opportunity. The United States had tried to help reduce the\ndebt burden of a number of developing countries through so-called Brady\nbonds, in which the U.S. government basically backstopped the debt of\ncountries like Brazil and Argentina. Musk noticed an arbitrage play. \u201cI\ncalculated the backstop value, and it was something like fifty cents on the\ndollar, while the actual debt was trading at twenty-five cents,\u201d Musk said.\n\u201cThis was like the biggest opportunity ever, and nobody seemed to realize\nit.\u201d Musk tried to remain cool and calm as he rang Goldman Sachs, one of\nthe main traders in this market, and probed around about what he had seen.\nHe inquired as to how much Brazilian debt might be available at the 25-\ncents price. \u201cThe guy said, \u2018How much do you want?\u2019 and I came up with\nsome ridiculous number like ten billion dollars,\u201d Musk said. When the trader confirmed that was doable, Musk hung up the phone. \u201cI was thinking\nthat they had to be fucking crazy because you could double your money.\nEverything was backed by Uncle Sam. It was a no-brainer.\u201d\nMusk had spent the summer earning about fourteen dollars an hour and\ngetting chewed out for using the executive coffee machine, among other\nstatus infractions, and figured his moment to shine and make a big bonus\nhad arrived. He sprinted up to his boss\u2019s office and pitched the opportunity\nof a lifetime. \u201cYou can make billions of dollars for free,\u201d he said. His boss\ntold Musk to write up a report, which soon got passed up to the bank\u2019s\nCEO, who promptly rejected the proposal, saying the bank had been burned\non Brazilian and Argentinian debt before and didn\u2019t want to mess with it\nagain. \u201cI tried to tell them that\u2019s not the point,\u201d Musk said. \u201cThe point is\nthat it\u2019s fucking backed by Uncle Sam. It doesn\u2019t matter what the South\nAmericans do. You cannot lose unless you think the U.S. Treasury is going\nto default. But they still didn\u2019t do it, and I was stunned. Later in life, as I\ncompeted against the banks, I would think back to this moment, and it gave\nme confidence. All the bankers did was copy what everyone else did. If\neveryone else ran off a bloody cliff, they\u2019d run right off a cliff with them. If\nthere was a giant pile of gold sitting in the middle of the room and nobody\nwas picking it up, they wouldn\u2019t pick it up, either.\u201d\nIn the years that followed, Musk considered starting an Internet bank\nand discussed it openly during his internship at Pinnacle Research in 1995.\nThe youthful Musk lectured the scientists about the inevitable transition\ncoming in finance toward online systems, but they tried to talk him down,\nsaying that it would takes ages for Web security to be good enough to win\nover consumers. Musk, though, remained convinced that the finance\nindustry could do with a major upgrade and that he could have a big\ninfluence on banking with a relatively small investment. \u201cMoney is low\nbandwidth,\u201d he said, during a speech at Stanford University in 2003, to\ndescribe his thinking. \u201cYou don\u2019t need some sort of big infrastructure\nimprovement to do things with it. It\u2019s really just an entry in a database.\u201d\nThe actual plan that Musk concocted was beyond grandiose. As the\nresearchers at Pinnacle had pointed out, people were barely comfortable\nbuying books online. They might take their chances entering a credit card\nnumber but exposing just their bank accounts to the Web was out of the\nquestion to many. Pah. So what? Musk wanted to build a full-service\nfinancial institution online: a company that would have savings and checking accounts as well as brokerage services and insurance. The\ntechnology to build such a service was possible, but navigating the\nregulatory hell of creating an online bank from scratch looked like an\nintractable problem to optimists and an impossibility to more level heads.\nThis was not dishing out directions to a pizzeria or putting up a house\nlisting. It was dealing with people\u2019s finances, and there would be real\nrepercussions if the service did not work as billed.\nUndaunted, Musk kicked this new plan into action before Zip2 had even\nbeen sold. He chatted up some of the best engineers at the company to get a\nfeel for who might be willing to join him in another venture. Musk also\nbounced his ideas off some contacts he\u2019d made at the bank in Canada. In\nJanuary 1999, with Zip2\u2019s board seeking a buyer, Musk began to formalize\nhis banking plan. The deal with Compaq was announced the next month.\nAnd in March, Musk incorporated X.com, a finance start-up with a\npornographic-sounding name.\nIt had taken Musk less than a decade to go from being a Canadian\nbackpacker to becoming a multimillionaire at the age of twenty-seven. With\nhis $22 million, he moved from sharing an apartment with three roommates\nto buying an 1,800-square-foot condo and renovating it. He also bought a\n$1 million McLaren F1 sports car and a small prop plane and learned to fly.\nMusk embraced the newfound celebrity that he\u2019d earned as part of the dot-\ncom millionaire set. He let CNN show up at his apartment at 7 A.M. to film\nthe delivery of the car. A black eighteen-wheeler pulled up in front of\nMusk\u2019s place and then lowered the sleek, sliver vehicle onto the street,\nwhile Musk stood slack-jawed with his arms folded. \u201cThere are sixty-two\nMcLarens in the world, and I will own one of them,\u201d he told CNN. \u201cWow, I\ncan\u2019t believe it\u2019s actually here. That\u2019s pretty wild, man.\u201d\nCNN interspersed video of the car delivery with interviews with Musk.\nThe whole time he looked like a caricature of an engineer who had made it\nbig. Musk\u2019s hair had started thinning, and he had a closely cropped cut that\naccentuated his boyish face. He wore an all-too-big brown sport coat and\nchecked his cell phone from his lavish car, sitting next to his gorgeous\ngirlfriend, Justine, and he seemed spellbound by his life. Musk rolled out\none laughable rich-guy line after another, talking first about the Zip2 deal\n\u2014\u201cReceiving cash is cash. I mean, those are just a large number of Ben\nFranklins\u201d\u2014next about the awesomeness of his life\u2014\u201cThere it is,\ngentlemen, the fastest car in the world\u201d\u2014and then about his prodigious ambition\u2014\u201cI could go and buy one of the islands in the Bahamas and turn it\ninto my personal fiefdom, but I am much more interested in trying to build\nand create a new company.\u201d The camera crew followed Musk to the X.com\noffices, where his cocksure delivery led to another round of cringe-worthy\nstatements: \u201cI do not fit the picture of a banker,\u201d \u201cRaising fifty million\ndollars is a matter of making a series of phone calls, and the money is\nthere,\u201d \u201cI think X.com could absolutely be a multibillion-dollar bonanza.\u201d\nMusk purchased the McLaren from a seller in Florida, snatching the car\naway from Ralph Lauren, who had also inquired about buying it. Even very\nwealthy people like Lauren would tend to reserve something like a\nMcLaren for special events or the occasional Sunday drive. Not Musk. He\ndrove it all around Silicon Valley and parked it on the street by the X.com\noffices. His friends were horrified to see such a work of art covered with\nbird droppings or in the parking lot of a Safeway. One day, Musk e-mailed\nfellow McLaren owner Larry Ellison, the billionaire cofounder of the\nsoftware maker Oracle, out of the blue to see if he wanted to go race cars\naround a track for fun. Jim Clark, another billionaire who liked fast things,\ncaught wind of the proposal and told a friend that he needed to rush over to\nthe local Ferrari dealership to buy something that could compete. Musk had\njoined the big boys\u2019 club. \u201cElon was super-excited about all of this,\u201d said\nGeorge Zachary, a venture capitalist and close friend of Musk\u2019s. \u201cHe\nshowed me the correspondence with Larry.\u201d The next year, while driving\ndown Sand Hill Road to meet with an investor, Musk turned to a friend in\nthe car and said, \u201cWatch this.\u201d He floored the car, did a lane change, spun\nout, and hit an embankment, which started the car spinning in midair like a\nFrisbee. The windows and wheels were blown to smithereens, and the body\nof the car damaged. Musk again turned to his companion and said, \u201cThe\nfunny part is it wasn\u2019t insured.\u201d The two of them then thumbed a ride to the\nventure capitalist\u2019s office.\nTo his credit, Musk did not fully buy in to this playboy persona. He\nactually plowed the majority of the money he made from Zip2 into X.com.\nThere were practical reasons for this decision. Investors catch a break under\nthe tax law if they roll a windfall into a new venture within a couple of\nmonths. But even by Silicon Valley\u2019s high-risk standards, it was shocking to\nput so much of one\u2019s newfound wealth into something as iffy as an online\nbank. All told, Musk invested about $12 million into X.com, leaving him,\nafter taxes, with $4 million or so for personal use. \u201cThat\u2019s part of what separates Elon from mere mortals,\u201d said Ed Ho, the former Zip2 executive,\nwho went on to cofound X.com. \u201cHe\u2019s willing to take an insane amount of\npersonal risk. When you do a deal like that, it either pays off or you end up\nin a bus shelter somewhere.\u201d\nMusk\u2019s decision to invest so much money in X.com looks even more\nunusual in hindsight. Much of the point of being a dot-com success in 1999\nwas to prove yourself once, stash away your millions, and then use your\ncredentials to talk other people into betting their money on your next\nventure. Musk would certainly go on to rely on outside investors, but he put\nmajor skin in the game as well. So while Musk could be found on television\ntalking like the rest of the self-absorbed dot-com schmucks, he behaved\nmore like a throwback to Silicon Valley\u2019s earlier days, when the founders of\ncompanies like Intel were willing to take huge gambles on themselves.\nWhere Zip2 had been a neat, useful idea, X.com held the promise of\nfomenting a major revolution. Musk, for the first time, would be\nconfronting a deep-pocketed, entrenched industry head-on with the hopes of\nupending all of the incumbents. Musk also began to hone his trademark\nstyle of entering an ultracomplex business and not letting the fact that he\nknew very little about the industry\u2019s nuances bother him in the slightest. He\nhad an inkling that the bankers were doing finance all wrong and that he\ncould run the business better than everyone else. Musk\u2019s ego and\nconfidence had started heading toward the levels that would inspire some\nand leave others thinking of him as pompous and unscrupulous. The\ncreation of X.com would ultimately reveal a great deal about Musk\u2019s\ncreativity, relentless drive, confrontational style, and foibles as a leader.\nMusk would also get another taste of being pushed aside at his own\ncompany and the pain that accompanies a grand vision left unfulfilled.\nMusk assembled what looked like an all-star crew to start X.com. Ho\nhad worked at SGI and Zip2 as an engineer, and his peers marveled at his\ncoding and team-management skills. They were joined by a pair of\nCanadians with finance experience\u2014Harris Fricker and Christopher Payne.\nMusk had met Fricker during his time as an intern at the Bank of Nova\nScotia, and the two really hit it off. A Rhodes scholar, Fricker brought the\nknowledge of the banking world\u2019s mechanics that X.com would need.\nPayne was Fricker\u2019s friend from the Canadian finance community. All four\nmen were considered cofounders of the company, while Musk emerged as\nthe largest shareholder thanks to his hefty up-front investment. X.com began, like so many Silicon Valley operations, at a house where the\ncofounders began brainstorming, and then moved to more formal offices at\n394 University Avenue in Palo Alto.\nThe cofounders were aligned philosophically around the idea that the\nbanking industry had fallen behind the times. Visiting a branch bank to\nspeak with a teller seemed pretty archaic now that the Internet had arrived.\nThe rhetoric sounded good, and the four men were enthused. The only thing\nstopping them was reality. Musk had a modicum of banking experience and\nhad resorted to buying a book on the industry to help understand its inner\nworkings. The more the cofounders thought about their plan of attack, the\nmore they realized the regulatory issues blocking the creation of an online\nbank were insurmountable. \u201cAs four and five months went by, the onion\njust kept unwrapping,\u201d said Ho.*\nFrom the outset, there were personality clashes as well. Musk had\nbecome a budding superstar in Silicon Valley and had the press fawning\nover him. This didn\u2019t sit that well with Fricker, who\u2019d moved from Canada\nand pegged X.com as his chance to make a mark on the world as a banking\nwhiz. Fricker, according to numerous people, wanted to run X.com and do\nso in a more conventional manner. He found Musk\u2019s visionary statements to\nthe press about rethinking the entire banking industry silly since the\ncompany was struggling to build much of anything. \u201cWe were out\npromising the sun, moon, and the stars to the media,\u201d Fricker said. \u201cElon\nwould say that this is not a normal business environment, and you have to\nsuspend normal business thinking. He said, \u2018There is a happy-gas factory up\non the hill, and it\u2019s pumping stuff into the Valley.\u2019\u201d Fricker would not be the\nlast person to accuse Musk of overhyping products and playing the public,\nalthough whether this is a flaw or one of Musk\u2019s great talents as a\nbusinessman is up for debate.\nThe squabble between Fricker and Musk came to a quick, nasty end.\nJust five months after X.com had started, Fricker initiated a coup. \u201cHe said\neither he takes over as CEO or he\u2019s just going to take everyone from the\ncompany and create his own company,\u201d Musk said. \u201cI don\u2019t do well with\nblackmail. I said, \u2018You should go do that.\u2019 So he did.\u201d Musk tried to talk Ho\nand some of the other key engineers into staying, but they sided with\nFricker and left. Musk ended up with a shell of a company and a handful of\nloyal employees. \u201cAfter all that went down, I remember sitting with Elon in\nhis office,\u201d said Julie Ankenbrandt, an early X.com employee who stayed. \u201cThere were a million laws in place to block something like X.com from\nhappening, but Elon didn\u2019t care. He just looked at me and said, \u2018I guess we\nshould hire some more people.\u2019\u201d*\nMusk had been trying to raise funding for X.com and had been forced to\ngo to venture capitalists and confess that there wasn\u2019t much in the way of a\ncompany left. Mike Moritz, a famed investor from Sequoia Capital, backed\nthe company nonetheless, making a bet on Musk and little else. Musk hit\nthe streets of Silicon Valley once again and managed to attract engineers\nwith his rah-rah speeches about the future of Internet banking. Scott\nAnderson, a young computer scientist, started on August 1, 1999, just a few\ndays after the exodus, and bought right into the vision. \u201cYou look back, and\nit was total insanity,\u201d Anderson said. \u201cWe had what amounted to a\nHollywood movie set of a website. It barely got past the VCs.\u201d\nWeek by week, more engineers arrived and the vision became more real.\nThe company secured a banking license and a mutual fund license and\nformed a partnership with Barclays. By November, X.com\u2019s small software\nteam had created one of the world\u2019s first online banks complete with FDIC\ninsurance to back the bank accounts and three mutual funds for investors to\nchoose. Musk gave the engineers $100,000 of his own money to conduct\ntheir testing. On the night before Thanksgiving in 1999, X.com went live to\nthe public. \u201cI was there until two A.M.,\u201d Anderson said. \u201cThen, I went home\nto cook Thanksgiving dinner. Elon called me a few hours later and asked\nme to come into the office to relieve some of the other engineers. Elon\nstayed there forty-eight straight hours, making sure things worked.\u201d\nUnder Musk\u2019s direction, X.com tried out some radical banking\nconcepts. Customers received a $20 cash card just for signing up to use the\nservice and a $10 card for every person they referred. Musk did away with\nniggling fees and overdraft penalties. In a very modern twist, X.com also\nbuilt a person-to-person payment system in which you could send someone\nmoney just by plugging their e-mail address into the site. The whole idea\nwas to shift away from slow-moving banks with their mainframes taking\ndays to process payments and to create a kind of agile bank account where\nyou could move money around with a couple of clicks on a mouse or an e-\nmail. This was revolutionary stuff, and more than 200,000 people bought\ninto it and signed up for X.com within the first couple of months of\noperation. Soon enough, X.com had a major competitor. A couple of brainy kids\nnamed Max Levchin and Peter Thiel had been working on a payment\nsystem of their own at their start-up called Confinity. The duo actually\nrented their office space\u2014a glorified broom closet\u2014from X.com and were\ntrying to make it possible for owners of Palm Pilot handhelds to swap\nmoney via the infrared ports on the devices. Between X.com and Confinity,\nthe small office on University Avenue had turned into the frenzied epicenter\nof the Internet finance revolution. \u201cIt was this mass of adolescent men that\nworked so hard,\u201d Ankenbrandt said. \u201cIt stunk so badly in there. I can still\nsmell it\u2014leftover pizza, body odor, and sweat.\u201d\nThe pleasantries between X.com and Confinity came to an abrupt end.\nThe Confinity founders moved to an office down the street and, like X.com,\nbegan focusing their attention on Web and e-mail-based payments with their\nservice known as PayPal. The companies became locked in a heated battle\nto match each other\u2019s features and attract more users, knowing that whoever\ngot bigger faster would win. Tens of millions of dollars were spent on\npromotions, while millions more were lost battling hackers who had seized\nupon the services as new playgrounds for fraud. \u201cIt was like the Internet\nversion of making it rain at a strip club,\u201d said Jeremy Stoppelman, an\nX.com engineer who went on to become the CEO of Yelp. \u201cYou gave away\nmoney as fast as you could.\u201d\nThe race to win Internet payments gave Musk a chance to show off his\nquick thinking and work ethic. He kept devising plans to counter the\nadvantage PayPal had established on auction sites like eBay. And he rallied\nthe X.com employees to implement the tactics as fast as possible using\nbrute-force appeals to their competitive natures. \u201cThere really wasn\u2019t\nanything suave about him,\u201d Ankenbrandt said. \u201cWe all worked twenty hours\na day, and he worked twenty-three hours.\u201d\nIn March 2000, X.com and Confinity finally decided to stop trying to\nspend each other into oblivion and to join forces. Confinity had what looked\nlike the hottest product in PayPal but was paying out $100,000 a day in\nawards to new customers and didn\u2019t have the cash reserves to keep going.\nX.com, by contrast, still had plenty of cash reserves and the more\nsophisticated banking products. It took the lead in setting the merger terms,\nleaving Musk as the largest shareholder of the combined company, which\nwould be called X.com. Shortly after the deal closed, X.com raised $100 million from backers including Deutsche Bank and Goldman Sachs and\nboasted that it had more than one million customers.*\nThe two companies tried hard to mesh their cultures, with modest\nsuccess. Groups of employees from X.com tied their computer monitors to\ntheir desk chairs with power cords and rolled them down the street to the\nConfinity offices to work alongside their new colleagues. But the teams\ncould never quite see eye to eye. Musk kept championing the X.com brand,\nwhile most everyone else favored PayPal. More fights broke out over the\ndesign of the company\u2019s technology infrastructure. The Confinity team led\nby Levchin favored moving toward open-source software like Linux, while\nMusk championed Microsoft\u2019s data-center software as being more likely to\nkeep productivity high. This squabble may sound silly to outsiders, but it\nwas the equivalent of a religious war to the engineers, many of whom\nviewed Microsoft as a dated evil empire and Linux as the modern software\nof the people. Two months after the merger, Thiel resigned and Levchin\nthreatened to walk out over the technology rift. Musk was left to run a\nfractured company.\nThe technology issues X.com had been facing worsened as the\ncomputing systems failed to keep up with an exploding customer base.\nOnce a week, the company\u2019s website collapsed. Most of the engineers were\nordered to start work designing a new system, which distracted key\ntechnical personnel and left X.com vulnerable to fraud. \u201cWe were losing\nmoney hand over fist,\u201d said Stoppelman. As X.com became more popular\nand its transaction volume exploded, all of its problems worsened. There\nwas more fraud. There were more fees from banks and credit card\ncompanies. There was more competition from start-ups. X.com lacked a\ncohesive business model to offset the losses and turn a profit from the\nmoney it managed. Roelof Botha, the start-up\u2019s chief financial officer and\nnow a prominent venture capitalist at Sequoia, did not think Musk provided\nthe board with a true picture of X.com\u2019s issues. A growing number of other\npeople at the company questioned Musk\u2019s decision-making in the face of all\nthe crises.\nWhat followed was one of the nastiest coups in Silicon Valley\u2019s long,\nillustrious history of nasty coups. A small group of X.com employees\ngathered one night at Fanny & Alexander, a now-defunct bar in Palo Alto,\nand brainstormed about how to push out Musk. They decided to sell the\nboard on the idea of Thiel returning as CEO. Instead of confronting Musk directly with this plan, the conspirators decided to take action behind\nMusk\u2019s back.\nMusk and Justine had been married in January 2000 but had been too\nbusy for a honeymoon. Nine months later, in September, they planned to\nmix business and pleasure by going on a fund-raising trip and ending it with\na honeymoon in Sydney to catch the Olympics. As they boarded their flight\none night, X.com executives delivered letters of no confidence to X.com\u2019s\nboard. Some of the people loyal to Musk had sensed something was wrong,\nbut it was too late. \u201cI went to the office at ten thirty that night, and everyone\nwas there,\u201d Ankenbrandt said. \u201cI could not believe it. I am frantically trying\nto call Elon, but he\u2019s on a plane.\u201d By the time he landed, Musk had been\nreplaced by Thiel.\nWhen Musk finally heard what had happened, he hopped on the next\nplane back to Palo Alto. \u201cIt was shocking, but I will give Elon this\u2014I\nthought he handled it pretty well,\u201d Justine said. For a brief period, Musk\ntried to fight back. He urged the board to reconsider its decision. But when\nit became clear that the company had already moved on, Musk relented. \u201cI\ntalked to Moritz and a few others,\u201d Musk said. \u201cIt wasn\u2019t so much that I\nwanted to be CEO but more like, \u2018Hey, I think there are some pretty\nimportant things that need to happen, and if I\u2019m not CEO, I\u2019m not sure they\nare going to happen.\u2019 But then I talked to Max and Peter, and it seemed like\nthey would make these things happen. So then, I mean, it\u2019s not the end of\nthe world.\u201d\nMany of the X.com employees who had been with Musk since early on\nwere less than impressed by what had happened. \u201cI was floored by it and\nangry,\u201d said Stoppelman. \u201cElon was sort of a rock star in my view. I was\nvery vocal about how I thought it was bullshit. But I knew fundamentally\nthat the company was doing well. It was a rocket ship, and I wasn\u2019t going to\nleave.\u201d Stoppelman, then twenty-three, went into a conference room and\ntore into Thiel and Levchin. \u201cThey let me vent it all out, and their reaction\nwas part of the reason I stayed.\u201d Others remained embittered. \u201cIt was\nbackhanded and cowardly,\u201d said Branden Spikes, a Zip2 and X.com\nengineer. \u201cI would have been more behind it if Elon had been in the room.\u201d\nBy June 2001, Musk\u2019s influence on the company was fading quickly.\nThat month, Thiel rebranded X.com as PayPal. Musk rarely lets a slight go\nunpunished. Throughout this ordeal, however, he showed incredible\nrestraint. He embraced the role of being an advisor to the company and kept investing in it, increasing his stake as PayPal\u2019s largest shareholder. \u201cYou\nwould expect someone in Elon\u2019s position to be bitter and vindictive, but he\nwasn\u2019t,\u201d said Botha. \u201cHe supported Peter. He was a prince.\u201d\nThe next few months would end up being key for Musk\u2019s future. The\ndot-com joyride was coming to a quick end, and people wanted to try to\ncash out in any way possible. When executives from eBay began\napproaching PayPal about an acquisition, the inclination for most people\nwas to sell and sell fast. Musk and Moritz, though, urged the board to reject\na number of offers and hold out for more money. PayPal had revenue of\nabout $240 million per year, and looked like it might make it as an\nindependent company and go public. Musk and Moritz\u2019s resistance paid off\nand then some. In July 2002, eBay offered $1.5 billion for PayPal, and\nMusk and the rest of the board accepted the deal. Musk netted about $250\nmillion from the sale to eBay, or $180 million after taxes\u2014enough to make\nwhat would turn out to be his very wild dreams possible.\nThe PayPal episode was a mixed bag for Musk. His reputation as a\nleader suffered in the aftermath of the deal, and the media turned on him in\nearnest for the first time. Eric Jackson, an early Confinity employee, wrote\nThe PayPal Wars: Battles with eBay, the Media, the Mafia, and the Rest of\nPlanet Earth in 2004 and recounted the company\u2019s tumultuous journey. The\nbook painted Musk as an egomaniacal, stubborn jerk, making wrong\ndecisions at every turn, and portrayed Thiel and Levchin as heroic geniuses.\nValleywag, the technology industry gossip site, piled on as well and turned\nbashing Musk into one of its pet projects. The criticisms grew to the point\nthat people started wondering aloud whether or not Musk counted as a true\ncofounder of PayPal or had just ridden Thiel\u2019s coattails to a magical payday.\nThe tone of the book along with the blog posts goaded Musk in 2007 into\nwriting a 2,200-word e-mail to Valleywag meant to set the record straight\nwith his version of events.\nIn the e-mail, Musk let his literary flair loose and gave the public a\ndirect look at his combative side. He described Jackson as \u201ca sycophantic\njackass\u201d and \u201cone notch above an intern,\u201d who had little insight into the\nhigh-level goings-on at the company. \u201cSince Eric worships Peter, the\noutcome was obvious\u2014Peter sounds like Mel Gibson in Braveheart and my\nrole is somewhere between negligible and a bad seed,\u201d Musk wrote. Musk\nthen detailed seven reasons why he deserved cofounder status of PayPal,\nincluding his role as its largest shareholder, the hiring of a lot of the top talent, the creation of a number of the company\u2019s most successful business\nideas, and his time as CEO when the company went from sixty to several\nhundred employees.\nAlmost everyone I interviewed from the PayPal days leaned toward\nagreeing with Musk\u2019s overall assessment. They said that Jackson\u2019s account\nbordered on fantasy when it came to celebrating the Confinity team over\nMusk and the X.com team. \u201cThere are a lot of PayPal people that suffer\nfrom warped memories,\u201d said Botha.\nBut these same people reached another consensus, saying that Musk had\nmishandled the branding, technology infrastructure, and fraud situations. \u201cI\nthink it would have killed the company if Elon had stayed on as CEO for\nsix more months,\u201d said Botha. \u201cThe mistakes Elon was making at the time\nwere amplifying the risk of the business.\u201d (For more on Musk\u2019s take on the\nPayPal years, see Appendix 2.)\nThe suggestions that Musk did not count as a \u201ctrue\u201d cofounder of\nPayPal seem asinine in retrospect. Thiel, Levchin, and other PayPal\nexecutives have said as much in the years since the eBay deal closed. The\nonly useful thing such criticisms produced were the bombastic\ncounteroffensives from Musk, which revealed touches of insecurity and the\nseriousness with which Musk insists that the historical record reflect his\ntake on events. \u201cHe comes from the school of thought in the public relations\nworld that you let no inaccuracy go uncorrected,\u201d said Vince Sollitto, the\nformer communications chief at PayPal. \u201cIt sets a precedent, and you\nshould fight every out-of-place comma tooth and nail. He takes things very\npersonally and usually seeks war.\u201d\nThe stronger critique of Musk during this period of his life was that he\nhad succeeded to a large degree despite himself. Musk\u2019s traits as a\nconfrontational know-it-all and his abundant ego created deep, lasting\nfractures within his companies. While Musk consciously tried to temper his\nbehavior, these efforts were not enough to win over investors and more\nexperienced executives. At both Zip2 and PayPal, the companies\u2019 boards\ncame to the conclusion that Musk was not yet CEO material. It can also be\nargued that Musk had become a hyperbolic huckster, who overreached and\noversold his companies\u2019 technology. Musk\u2019s biggest detractors have made\nall of these arguments either in public or private and a half dozen or so of\nthem said far worse things to me about his character and actions, describing\nMusk as unethical in business and vicious with his personal attacks. Almost universally, these people were unwilling to go on the record with their\ncomments, claiming to be afraid Musk would pursue litigation against them\nor ruin their ability to do business.\nThese criticisms must be weighed against Musk\u2019s track record. He\ndemonstrated an innate ability to read people and technology trends at the\ninception of the consumer Web. While others tried to wrap their heads\naround the Internet\u2019s implications, Musk had already set off on a purposeful\nplan of attack. He envisioned many of the early pieces of technology\u2014\ndirectories, maps, sites that focused on vertical markets\u2014that would\nbecome mainstays on the Web. Then, just as people became comfortable\nwith buying things from Amazon.com and eBay, Musk made the great leap\nforward to full-fledged Internet banking. He would bring standard financial\ninstruments online and then modernize the industry with a host of new\nconcepts. He exhibited a deep insight into human nature that helped his\ncompanies pull off exceptional marketing, technology, and financial feats.\nMusk was already playing the entrepreneur game at the highest level and\nworking the press and investors like few others could. Did he hype things\nup and rub people the wrong way? Absolutely\u2014and with spectacular\nresults.\nBased in large part on Musk\u2019s guidance, PayPal survived the bursting of\nthe dot-com bubble, became the first blockbuster IPO after the 9/11 attacks,\nand then sold to eBay for an astronomical sum while the rest of the\ntechnology industry was mired in a dramatic downturn. It was nearly\nimpossible to survive let alone emerge as a winner in the midst of such a\nmess.\nPayPal also came to represent one of the greatest assemblages of\nbusiness and engineering talent in Silicon Valley history. Both Musk and\nThiel had a keen eye for young, brilliant engineers. The founders of start-\nups as varied as YouTube, Palantir Technologies, and Yelp all worked at\nPayPal. Another set of people\u2014including Reid Hoffman, Thiel, and Botha\n\u2014emerged as some of the technology industry\u2019s top investors. PayPal staff\npioneered techniques in fighting online fraud that have formed the basis of\nsoftware used by the CIA and FBI to track terrorists and of software used\nby the world\u2019s largest banks to combat crime. This collection of super-\nbright employees has become known as the PayPal Mafia\u2014more or less the\ncurrent ruling class of Silicon Valley\u2014and Musk is its most famous and\nsuccessful member. Hindsight also continues to favor Musk\u2019s unbridled vision over the more\ncautious pragmatism of executives at Zip2 and PayPal. Had it chased\nconsumers as Musk urged, Zip2 may have ended up as a blockbuster\nmapping and review service. As for PayPal, an argument can still be made\nthat the investors sold out too early and should have listened more to\nMusk\u2019s demands to remain independent. By 2014, PayPal had amassed 153\nmillion users and was valued at close to $32 billion as a stand-alone\ncompany. A flood of payment and banking start-ups have appeared as well\n\u2014Square, Stripe, and Simple, to name three among the S\u2019s\u2014that have\nlooked to fulfill much of the original X.com vision.\nIf X.com\u2019s board had been a bit more patient with Musk, there\u2019s good\nreason to believe he would have succeeded with delivery of the \u201conline\nbank to rule them all\u201d that he had set out to create. History has\ndemonstrated that while Musk\u2019s goals can sound absurd in the moment, he\ncertainly believes in them and, when given enough time, tends to achieve\nthem. \u201cHe always works from a different understanding of reality than the\nrest of us,\u201d Ankenbrandt said. \u201cHe is just different than the rest of us.\u201d\nWhile navigating the business tumult of Zip2 and PayPal, Musk found a\nmoment of peace in his personal life. He\u2019d spent years courting Justine\nWilson from afar, flying her out for visits on the weekends. For a long time,\nhis oppressive hours and his roommates put a crimp on the relationship. But\nthe Zip2 sale let Musk buy a place of his own and pay a bit more attention\nto Justine. Like any couple, they had their ups and downs, but that passion\nof young love remained. \u201cWe fought a lot, but when we weren\u2019t fighting,\nthere was a deep sense of compassion\u2014a bond,\u201d Justine said. The couple\nhad been sparring for a few days about phone calls Justine kept getting from\nan ex-boyfriend\u2014\u201cElon didn\u2019t like that\u201d\u2014and had a major spat while\nwalking near the X.com offices. \u201cI remember thinking it was a lot of drama,\nand that if I was going to put up with it, we might as well be married. I told\nhim he should just propose to me,\u201d Justine said. It took Musk a few minutes\nto cool down and then he did just that, proposing on the spot. A few days\nlater, a more chivalrous Musk returned to the sidewalk, got down on bended\nknee, and presented Justine with a ring.\nJustine knew all about Musk\u2019s grim childhood and the intense range of\nemotions he could exhibit. Her romantic sensibilities overrode any\ntrepidation she might have had about these parts of Musk\u2019s history and\ncharacter and centered instead on his strength. Musk often talked fondly about Alexander the Great, and Justine saw him as her own conquering\nhero. \u201cHe wasn\u2019t afraid of responsibility,\u201d she said. \u201cHe didn\u2019t run from\nthings. He wanted to get married and have kids early on.\u201d Musk also\nexuded a confidence and passion that made Justine think life with him\nwould always be okay. \u201cMoney is not his motivation, and, quite frankly, I\nthink it just happens for him,\u201d Justine said. \u201cIt\u2019s just there. He knows he can\ngenerate it.\u201d\nAt their wedding reception, Justine encountered the other side of the\nconquering hero. Musk pulled Justine close while they danced, and\ninformed her, \u201cI am the alpha in this relationship.\u201d3 Two months later,\nJustine signed a postnuptial financial agreement that would come back to\nhaunt her and entered into an enduring power struggle. She described the\nsituation years later in an article for Marie Claire, writing, \u201cHe was\nconstantly remarking on the ways he found me lacking. \u2018I am your wife,\u2019 I\ntold him repeatedly, \u2018not your employee.\u2019 \u2018If you were my employee,\u2019 he\nsaid just as often, \u2018I would fire you.\u2019\u201d\nThe newlyweds were not helped by the drama at X.com. They\u2019d put off\ntheir honeymoon and then had it derailed by the coup. It took until late\nDecember 2000 for things to calm down enough for Musk to take his first\nvacation in years. He arranged a two-week trip, with the first part taking\nplace in Brazil and the second in South Africa at a game reserve near the\nMozambique border. While in Africa, Musk contracted the most virulent\nversion of malaria\u2014falciparum malaria\u2014which accounts for the vast\nmajority of malaria deaths.\nMusk returned to California in January, which is when the illness took\nhold. He started to get sick and was bedridden for a few days before Justine\ntook him to a doctor who then ordered that Musk be rushed in an ambulance\nto Sequoia Hospital in Redwood City.* Doctors there misdiagnosed and\nmistreated his condition to the point that Musk was near death. \u201cThen, there\nhappened to be a guy visiting from another hospital who had seen a lot\nmore malaria cases,\u201d Musk said. He spied Musk\u2019s blood work in the lab and\nordered an immediate maximum dosage of doxycycline, an antibiotic. The\ndoctor told Musk that if he had turned up a day later, the medicine likely\nwould no longer have been effective.\nMusk spent ten agonizing days in the intensive care unit. The\nexperience shocked Justine. \u201cHe\u2019s built like a tank,\u201d she said. \u201cHe has a\nlevel of stamina and an ability to deal with levels of stress that I\u2019ve never seen in anyone else. To see him laid low like that in total misery was like a\nvisit to an alternate universe.\u201d It took Musk six months to recover. He lost\nforty-five pounds over the course of the illness and had a closet full of\nclothes that no longer fit. \u201cI came very close to dying,\u201d Musk said. \u201cThat\u2019s\nmy lesson for taking a vacation: vacations will kill you.\u201d 6 MICE IN SPACE\nE\nLON MUSK TURNED THIRTY IN JUNE 2001, and the birthday hit\nhim hard. \u201cI\u2019m no longer a child prodigy,\u201d he told Justine, only half joking.\nThat same month X.com officially changed its name to PayPal, providing a\nharsh reminder that the company had been ripped away from Musk and\ngiven to someone else to run. The start-up life, which Musk described as\nakin to \u201ceating glass and staring into the abyss,\u201d4 had gotten old and so had\nSilicon Valley. It felt like Musk was living inside a trade show where\neveryone worked in the technology industry and talked all the time about\nfunding, IPOs, and chasing big paydays. People liked to brag about the\ncrazy hours they worked, and Justine would just laugh, knowing Musk had\nlived a more extreme version of the Silicon Valley lifestyle than they could\nimagine. \u201cI had friends who complained that their husbands came home at\nseven or eight,\u201d she said. \u201cElon would come home at eleven and work some\nmore. People didn\u2019t always get the sacrifice he made in order to be where\nhe was.\u201d\nThe idea of escaping this incredibly lucrative rat race started to grow\nmore and more appealing. Musk\u2019s entire life had been about chasing a\nbigger stage, and Palo Alto seemed more like a stepping-stone than a final\ndestination. The couple decided to move south and begin their family and\nthe next chapter of their lives in Los Angeles.\n\u201cThere\u2019s an element to him that likes the style and the excitement and\ncolor of a place like L.A.,\u201d said Justine. \u201cElon likes to be where the action\nis.\u201d A small group of Musk\u2019s friends who felt similarly had also decamped\nto Los Angeles for what would be a wild couple of years.\nIt wasn\u2019t just Los Angeles\u2019s glitz and grandeur that attracted Musk. It\nwas also the call of space. After being pushed out of PayPal, Musk had\nstarted to revisit his childhood fantasies around rocket ships and space\ntravel and to think that he might have a greater calling than creating Internet\nservices. The changes in his attitude and thinking soon became obvious to\nhis friends, including a group of PayPal executives who had gathered in Las Vegas one weekend to celebrate the company\u2019s success. \u201cWe\u2019re all hanging\nout in this cabana at the Hard Rock Cafe, and Elon is there reading some\nobscure Soviet rocket manual that was all moldy and looked like it had been\nbought on eBay,\u201d said Kevin Hartz, an early PayPal investor. \u201cHe was\nstudying it and talking openly about space travel and changing the world.\u201d\nMusk had picked Los Angeles with intent. It gave him access to space\nor at least the space industry. Southern California\u2019s mild, consistent weather\nhad made it a favored city of the aeronautics industry since the 1920s, when\nthe Lockheed Aircraft Company set up shop in Hollywood. Howard\nHughes, the U.S. Air Force, NASA, Boeing, and myriad other people and\norganizations have performed much of their manufacturing and cutting-\nedge experimentation in and around Los Angeles. Today the city remains a\nmajor hub for the military\u2019s aeronautics work and commercial activity.\nWhile Musk didn\u2019t know exactly what he wanted to do in space, he realized\nthat just by being in Los Angeles he would be surrounded by the world\u2019s\ntop aeronautics thinkers. They could help him refine any ideas, and there\nwould be plenty of recruits to join his next venture.\nMusk\u2019s first interactions with the aeronautics community were with an\neclectic collection of space enthusiasts, members of a nonprofit group\ncalled the Mars Society. Dedicated to exploring and settling the Red Planet,\nthe Mars Society planned to hold a fund-raiser in mid-2001. The $500-per-\nplate event was to take place at the house of one of the well-off Mars\nSociety members, and invitations to the usual characters had been mailed\nout. What stunned Robert Zubrin, the head of the group, was the reply from\nsomeone named Elon Musk, whom no one could remember inviting. \u201cHe\ngave us a check for five thousand dollars,\u201d Zubrin said. \u201cThat made\neveryone take notice.\u201d Zubrin began researching Musk, determined he was\nrich, and invited him for coffee ahead of the dinner. \u201cI wanted to make sure\nhe knew the projects we had under way,\u201d Zubrin said. He proceeded to\nregale Musk with tales of the research center the society had built in the\nArctic to mimic the tough conditions of Mars and the experiments they had\nbeen running for something called the Translife Mission, in which there\nwould be a spinning capsule orbiting Earth that was piloted by a crew of\nmice. \u201cIt would spin to give them one-third gravity\u2014the same you would\nhave on Mars\u2014and they would live there and reproduce,\u201d Zubrin told\nMusk. When it was time for dinner, Zubrin placed Musk at the VIP table next\nto himself, the director and space buff James Cameron, and Carol Stoker, a\nplanetary scientist for NASA with a deep interest in Mars. \u201cElon is so\nyouthful-looking and at that time he looked like a little boy,\u201d Stoker said.\n\u201cCameron was chatting him up right away to invest in his next movie, and\nZubrin was trying to get him to make a big donation to the Mars Society.\u201d\nIn return for being hounded for cash, Musk probed about for ideas and\ncontacts. Stoker\u2019s husband was an aerospace engineer at NASA working on\na concept for an airplane that would glide over Mars looking for water.\nMusk loved that. \u201cHe was much more intense than some of the other\nmillionaires,\u201d Zubrin said. \u201cHe didn\u2019t know a lot about space, but he had a\nscientific mind. He wanted to know exactly what was being planned in\nregards to Mars and what the significance would be.\u201d Musk took to the\nMars Society right away and joined its board of directors. He donated\nanother $100,000 to fund a research station in the desert as well.\nMusk\u2019s friends were not entirely sure what to make of his mental state.\nHe\u2019d lost a tremendous amount of weight fighting off malaria and looked\nalmost skeletal. With little prompting, Musk would start expounding on his\ndesire to do something meaningful with his life\u2014something lasting. His\nnext move had to be either in solar or in space. \u201cHe said, \u2018The logical thing\nto happen next is solar, but I can\u2019t figure out how to make any money out of\nit,\u2019\u201d said George Zachary, the investor and close friend of Musk\u2019s, recalling\na lunch date at the time. \u201cThen he started talking about space, and I thought\nhe meant office space like a real estate play.\u201d Musk had actually started\nthinking bigger than the Mars Society. Rather than send a few mice into\nEarth\u2019s orbit, Musk wanted to send them to Mars. Some very rough\ncalculations done at the time suggested that the journey would cost $15\nmillion. \u201cHe asked if I thought that was crazy,\u201d Zachary said. \u201cI asked, \u2018Do\nthe mice come back? Because, if they don\u2019t, yeah, most people will think\nthat\u2019s crazy.\u2019\u201d As it turned out, the mice were not only meant to go to Mars\nand come back but were also meant to procreate along the way, during a\njourney that would take months. Jeff Skoll, another one of Musk\u2019s friends\nwho made a fortune at eBay, pointed out that the fornicating mice would\nneed a hell of a lot of cheese and bought Musk a giant wheel of Le Brou\u00e8re,\na type of Gruy\u00e8re.\nMusk did not mind becoming the butt of cheese jokes. The more he\nthought about space, the more important its exploration seemed to him. He felt as if the public had lost some of its ambition and hope for the future.\nThe average person might see space exploration as a waste of time and\neffort and rib him for talking about the subject, but Musk thought about\ninterplanetary travel in a very earnest way. He wanted to inspire the masses\nand reinvigorate their passion for science, conquest, and the promise of\ntechnology.\nHis fears that mankind had lost much of its will to push the boundaries\nwere reinforced one day when Musk went to the NASA website. He\u2019d\nexpected to find a detailed plan for exploring Mars and instead found\nbupkis. \u201cAt first I thought, jeez, maybe I\u2019m just looking in the wrong\nplace,\u201d Musk once told Wired. \u201cWhy was there no plan, no schedule? There\nwas nothing. It seemed crazy.\u201d Musk believed that the very idea of America\nwas intertwined with humanity\u2019s desire to explore. He found it sad that the\nAmerican agency tasked with doing audacious things in space and\nexploring new frontiers as its mission seemed to have no serious interest in\ninvestigating Mars at all. The spirit of Manifest Destiny had been deflated\nor maybe even come to a depressing end, and hardly anyone seemed to\ncare.\nLike so many quests to revitalize America\u2019s soul and bring hope to all\nof mankind, Musk\u2019s journey began in a hotel conference room. By this\ntime, Musk had built up a decent network of contacts in the space industry,\nand he brought the best of them together at a series of salons\u2014sometimes at\nthe Renaissance hotel at the Los Angeles airport and sometimes at the\nSheraton hotel in Palo Alto. Musk had no formal business plan for these\npeople to debate. He mostly wanted them to help him develop the mice-to-\nMars idea or at least to come up with something comparable. Musk hoped\nto hit on a grand gesture for mankind\u2014some type of event that would\ncapture the world\u2019s attention, get people thinking about Mars again, and\nhave them reflect on man\u2019s potential. The scientists and luminaries at the\nmeetings were to figure out a spectacle that would be technically feasible at\na price tag of approximately $20 million. Musk resigned from his position\nas a director of the Mars Society and announced his own organization\u2014the\nLife to Mars Foundation.\nThe collection of talent attending these sessions in 2001 was impressive.\nScientists showed up from NASA\u2019s Jet Propulsion Laboratory, or JPL.\nJames Cameron appeared, lending some celebrity to the affair. Also\nattending was Michael Griffin, whose academic credentials were spectacular and included degrees in aerospace engineering, electrical\nengineering, civil engineering, and applied physics. Griffin had worked for\nthe CIA\u2019s venture capital arm called In-Q-Tel, at NASA, and at JPL and was\njust in the process of leaving Orbital Sciences Corporation, a maker of\nsatellites and spacecraft, where he had been chief technical officer and the\ngeneral manager of the space systems group. It could be argued that no one\non the planet knew more about the realities of getting things into space than\nGriffin, and he was working for Musk as space thinker in chief. (Four years\nlater, in 2005, Griffin took over as head of NASA.)\nThe experts were thrilled to have another rich guy appear who was\nwilling to fund something interesting in space. They happily debated the\nmerits and feasibility of sending up rodents and watching them hump. But,\nas the discussion wore on, a consensus started to build around pursuing a\ndifferent project\u2014something called \u201cMars Oasis.\u201d Under this plan, Musk\nwould buy a rocket and use it to shoot what amounted to a robotic\ngreenhouse to Mars. A group of researchers had already been working on a\nspace-ready growth chamber for plants. The idea was to modify their\nstructure, so that it could open up briefly and suck in some of the Martian\nregolith, or soil, and then use it to grow a plant, which would in turn\nproduce the first oxygen on Mars. Much to Musk\u2019s liking, this new plan\nseemed both ostentatious and feasible.\nMusk wanted the structure to have a window and a way to send a video\nfeedback to Earth, so that people could watch the plant grow. The group\nalso talked about sending out kits to students around the country who would\ngrow their own plants simultaneously and take notice, for example, that the\nMartian plant could grow twice as high as its Earth-bound counterpart in the\nsame amount of time. \u201cThis concept had been floating around in various\nforms for a while,\u201d said Dave Bearden, a space industry veteran who\nattended the meetings. \u201cIt would be, yes, there is life on Mars, and we put it\nthere. The hope was that it might turn on a light for thousands of kids that\nthis place is not that hostile. Then they might start thinking, Maybe we\nshould go there.\u201d Musk\u2019s enthusiasm for the idea started to inspire the\ngroup, many of whom had grown cynical about anything novel happening\nin space again. \u201cHe\u2019s a very smart, very driven guy with a huge ego,\u201d\nBearden said. \u201cAt one point someone mentioned that he might become Time\nmagazine\u2019s Man of the Year, and you could see him light up. He has this\nbelief that he is the guy who can change the world.\u201d The main thing troubling the space experts was Musk\u2019s budget.\nFollowing the salons, it seemed like Musk wanted to spend somewhere\nbetween $20 million and $30 million on the stunt, and everyone knew that\nthe cost of a rocket launch alone would eat up that money and then some.\n\u201cIn my mind, you needed two hundred million dollars to do it right,\u201d\nBearden said. \u201cBut people were reluctant to bring too much reality into the\nsituation too early and just get the whole idea killed.\u201d Then there were the\nimmense engineering challenges that would need solving. \u201cTo have a big\nwindow on this thing was a real thermal problem,\u201d Bearden said. \u201cYou\ncould not keep the container warm enough to keep anything alive.\u201d\nScooping Martian soil into the structure seemed not only hard to do\nphysically but also like a flat-out bad idea since the regolith would be toxic.\nFor a while, the scientists debated growing the plant in a nutrient-rich gel\ninstead, but that felt like cheating and like it might undermine the whole\npoint of the endeavor. Even the optimistic moments were awash in\nunknowns. One scientist found some very resilient mustard seeds and\nthought they could possibly survive a treated version of the Martian soil.\n\u201cThere was a pretty big downside if the plant didn\u2019t survive,\u201d Bearden said.\n\u201cYou have this dead garden on Mars that ends up giving off the opposite of\nthe intended effect.\u201d*\nMusk never flinched. He turned some of the volunteer thinkers into\nconsultants, and put them to work on the plant machine\u2019s design. He also\nplotted a trip to Russia to find out exactly how much a launch would cost.\nMusk intended to buy a refurbished intercontinental ballistic missile, or\nICBM, from the Russians and use that as his launch vehicle. For help with\nthis, Musk reached out to Jim Cantrell, an unusual fellow who had done a\nmix of classified and unclassified work for the United States and other\ngovernments. Among other claims to fame, Cantrell had been accused of\nespionage and placed under house arrest in 1996 by the Russians after a\nsatellite deal went awry. \u201cAfter a couple of weeks, Al Gore made some\ncalls, and it got worked out,\u201d Cantrell said. \u201cI didn\u2019t want anything to do\nwith the Russians again\u2014ever.\u201d Musk had other ideas.\nCantrell was driving his convertible on a hot July evening in Utah when\na call came in. \u201cThis guy in a funny accent said, \u2018I really need to talk to\nyou. I am a billionaire. I am going to start a space program.\u2019\u201d Cantrell could\nnot hear Musk well\u2014he thought his name was Ian Musk\u2014and said he\nwould call back once he got home. The two men didn\u2019t exactly trust each other at the outset. Musk refused to give Cantrell his cell phone number and\nmade the call from his fax machine. Cantrell found Musk both intriguing\nand all too eager. \u201cHe asked if there was an airport near me and if I could\nmeet the next day,\u201d Cantrell said. \u201cMy red flags started going off.\u201d Fearing\none of his enemies was trying to orchestrate an elaborate setup, Cantrell\ntold Musk to meet him at the Salt Lake City airport, where he would rent a\nconference room near the Delta lounge. \u201cI wanted him to meet me behind\nsecurity so he couldn\u2019t pack a gun,\u201d Cantrell said. When the meeting finally\ntook place, Musk and Cantrell hit it off. Musk rolled out his \u201chumans need\nto become a multiplanetary species\u201d speech, and Cantrell said that if Musk\nwas really serious, he\u2019d be willing to go to Russia\u2014again\u2014and help buy a\nrocket.\nIn late October 2001, Musk, Cantrell, and Adeo Ressi, Musk\u2019s friend\nfrom college, boarded a commercial flight to Moscow. Ressi had been\nplaying the role of Musk\u2019s guardian and trying to ascertain whether his best\nfriend had started to lose his mind. Compilation videos of rockets exploding\nwere made, and interventions were held with Musk\u2019s friends trying to talk\nhim out of wasting his money. While these measures failed, Adeo went\nalong to Russia to try to contain Musk as best as he could. \u201cAdeo would call\nme to the side and say, \u2018What Elon is doing is insane. A philanthropic\ngesture? That\u2019s crazy,\u2019\u201d Cantrell said. \u201cHe was seriously worried but was\ndown with the trip.\u201d And why not? The men were heading to Russia at the\nheight of its freewheeling post-Soviet days when rich guys could apparently\nbuy space missiles on the open market.\nTeam Musk would grow to include Mike Griffin, and meet with the\nRussians three times over a period of four months.* The group set up a few\nmeetings with companies like NPO Lavochkin, which had made probes\nintended for Mars and Venus for the Russian Federal Space Agency, and\nKosmotras, a commercial rocket launcher. The appointments all seemed to\ngo the same way, following Russian decorum. The Russians, who often skip\nbreakfast, would ask to meet around 11 A.M. at their offices for an early\nlunch. Then there would be small talk for an hour or more as the meeting\nattendees picked over a spread of sandwiches, sausages, and, of course,\nvodka. At some point during this process, Griffin usually started to lose his\npatience. \u201cHe suffers fools very poorly,\u201d Cantrell said. \u201cHe\u2019s looking\naround and wondering when we\u2019re going to get down to fucking business.\u201d\nThe answer was not soon. After lunch came a lengthy smoking and coffee- drinking period. Once all of the tables were cleared, the Russian in charge\nwould turn to Musk and ask, \u201cWhat is it you\u2019re interested in buying?\u201d The\nbig windup may not have bothered Musk as much if the Russians had taken\nhim more seriously. \u201cThey looked at us like we were not credible people,\u201d\nCantrell said. \u201cOne of their chief designers spit on me and Elon because he\nthought we were full of shit.\u201d\nThe most intense meeting occurred in an ornate, neglected,\nprerevolutionary building near downtown Moscow. The vodka shots started\n\u2014\u201cTo space!\u201d \u201cTo America!\u201d\u2014while Musk sat on $20 million, which he\nhoped would be enough to buy three ICBMs that could be retooled to go to\nspace. Buzzed from the vodka, Musk asked point-blank how much a missile\nwould cost. The reply: $8 million each. Musk countered, offering $8 million\nfor two. \u201cThey sat there and looked at him,\u201d Cantrell said. \u201cAnd said\nsomething like, \u2018Young boy. No.\u2019 They also intimated that he didn\u2019t have\nthe money.\u201d At this point, Musk had decided the Russians were either not\nserious about doing business or determined to part a dot-com millionaire\nfrom as much of his money as possible. He stormed out of the meeting.\nThe Team Musk mood could not have been worse. It was near the end\nof February 2002, and they went outside to hail a cab and drove straight to\nthe airport surrounded by the snow and dreck of the Moscow winter. Inside\nthe cab, no one talked. Musk had come to Russia filled with optimism about\nputting on a great show for mankind and was now leaving exasperated and\ndisappointed by human nature. The Russians were the only ones with\nrockets that could possibly fit within Musk\u2019s budget. \u201cIt was a long drive,\u201d\nCantrell said. \u201cWe sat there in silence looking at the Russian peasants\nshopping in the snow.\u201d The somber mood lingered all the way to the plane,\nuntil the drink cart arrived. \u201cYou always feel particularly good when the\nwheels lift off in Moscow,\u201d Cantrell said. \u201cIt\u2019s like, \u2018My God. I made it.\u2019\nSo, Griffin and I got drinks and clinked our glasses.\u201d Musk sat in the row in\nfront of them, typing on his computer. \u201cWe\u2019re thinking, Fucking nerd. What\ncan he be doing now?\u201d At which point Musk wheeled around and flashed a\nspreadsheet he\u2019d created. \u201cHey, guys,\u201d he said, \u201cI think we can build this\nrocket ourselves.\u201d\nGriffin and Cantrell had downed a couple of drinks by this time and\nwere too deflated to entertain a fantasy. They knew all too well the stories\nof gung-ho millionaires who thought they could conquer space only to lose\ntheir fortunes. Just the year before, Andrew Beal, a real estate and finance whiz in Texas, folded his aerospace company after having poured millions\ninto a massive test site. \u201cWe\u2019re thinking, Yeah, you and whose fucking\narmy,\u201d Cantrell said. \u201cBut, Elon says, \u2018No, I\u2019m serious. I have this\nspreadsheet.\u2019\u201d Musk passed his laptop over to Griffin and Cantrell, and they\nwere dumbfounded. The document detailed the costs of the materials\nneeded to build, assemble, and launch a rocket. According to Musk\u2019s\ncalculations, he could undercut existing launch companies by building a\nmodest-sized rocket that would cater to a part of the market that specialized\nin carrying smaller satellites and research payloads to space. The\nspreadsheet also laid out the hypothetical performance characteristics of the\nrocket in fairly impressive detail. \u201cI said, \u2018Elon, where did you get this?\u2019\u201d\nCantrell said.\nMusk had spent months studying the aerospace industry and the physics\nbehind it. From Cantrell and others, he\u2019d borrowed Rocket Propulsion\nElements, Fundamentals of Astrodynamics, and Aerothermodynamics of\nGas Turbine and Rocket Propulsion, along with several more seminal texts.\nMusk had reverted to his childhood state as a devourer of information and\nhad emerged from this meditative process with the realization that rockets\ncould and should be made much cheaper than what the Russians were\noffering. Forget the mice. Forget the plant with its own video feed growing\n\u2014or possibly dying\u2014on Mars. Musk would inspire people to think about\nexploring space again by making it cheaper to explore space.\nAs word traveled around the space community about Musk\u2019s plans,\nthere was a collective ho-hum. People like Zubrin had seen this show many\ntimes before. \u201cThere was a string of zillionaires that got sold a good story\nby an engineer,\u201d Zubrin said. \u201cCombine my brains and your money, and we\ncan build a rocket ship that will be profitable and open up the space frontier.\nThe techies usually ended up spending the rich guy\u2019s money for two years,\nand then the rich guy gets bored and shuts the thing down. With Elon,\neveryone gave a sigh and said, \u2018Oh well. He could have spent ten million\ndollars to send up the mice, but instead he\u2019ll spend hundreds of millions and\nprobably fail like all the others that proceeded him.\u2019\u201d\nWhile well aware of the risks tied to starting a rocket company, Musk\nhad at least one reason to think he might succeed where others had failed.\nThat reason\u2019s name was Tom Mueller.\nMueller grew up the son of a logger in the tidy Idaho town of St.\nMaries, where he developed a reputation as an oddball. While the rest of the kids were outside exploring the woods in winter, Mueller stayed warm in\nthe library reading books or watching Star Trek at his house. He also\ntinkered. Walking to grade school one day, Mueller discovered a smashed\nclock in an alley and turned it into a pet project. Each day, he fixed some\npart of the clock\u2014a gear, a spring\u2014until he got it working. A similar thing\nhappened with the family\u2019s lawn mower, which Mueller disassembled one\nafternoon on the front lawn for fun. \u201cMy dad came home and was so mad\nbecause he thought he\u2019d have to buy a new mower,\u201d Mueller said. \u201cBut I\nput it back together, and it ran.\u201d Mueller then got stuck on rockets. He\nstarted buying mail order kits and following the instructions to build small\nrockets. Rather quickly, Mueller graduated to constructing his own devices.\nAt the age of twelve, he crafted a mock-up space shuttle that could be\nattached to a rocket, sent up into the air, and then glide back to the ground.\nFor a science project a couple of years later, Mueller borrowed his dad\u2019s\noxyacetylene welding equipment to make a rocket engine prototype.\nMueller cooled the device by placing it upside down in a coffee can full of\nwater\u2014\u201cI could run it like that all day long\u201d\u2014and invented equally creative\nways to measure its performance. The machine was good enough for\nMueller to win a couple of regional science fair competitions and end up at\nan international event. \u201cThat\u2019s where I promptly got my ass kicked,\u201d\nMueller said.\nTall, lanky, and with a rectangular face, Mueller is an easygoing sort\nwho muddled through college for a bit, teaching his friends how to make\nsmoke bombs, and then eventually settled down and did well as a\nmechanical engineering student. Fresh out of college, he worked for Hughes\nAircraft on satellites\u2014\u201cIt wasn\u2019t rockets, but it was close\u201d\u2014and then went\nto TRW Space & Electronics. It was the latter half of the 1980s, and Ronald\nReagan\u2019s Star Wars program had the space gearheads dreaming about\nkinetic weapons and all sorts of mayhem. At TRW, Mueller experimented\nwith crazy types of propellants and oversaw the development of the\ncompany\u2019s TR-106 engine, a giant machine fueled by liquid oxygen and\nhydrogen. As a hobby, Mueller hung out with a couple hundred amateur\nrocketry buffs in the Reaction Research Society, a group formed in 1943 to\nencourage the building and firing of rockets. On the weekends, Mueller\ntraveled out to the Mojave Desert with the other RRS members to push the\nlimits of amateur machines. Mueller was one of the club\u2019s standouts, able to\nbuild things that actually worked, and could experiment with some of the more radical concepts that were quashed by his conservative bosses at\nTRW. His crowning achievement was an eighty-pound engine that could\nproduce thirteen thousand pounds of thrust and earned accolades as the\nworld\u2019s largest liquid-fuel rocket engine built by an amateur. \u201cI still keep\nthe rockets hanging in my garage,\u201d Mueller said.\nIn January 2002, Mueller was hanging out in the workshop of John\nGarvey, who had left a job at the aerospace company McDonnell Douglas\nto start building his own rockets. Garvey\u2019s facility was in Huntington\nBeach, where he rented an industrial space about the size of a six-car\ngarage. The two men were fiddling around with the eighty-pound engine\nwhen Garvey mentioned that a guy named Elon Musk might be stopping by.\nThe amateur rocketry scene is tight, and it was Cantrell who recommended\nthat Musk check out Garvey\u2019s workshop and see Mueller\u2019s designs. On a\nSunday, Musk arrived with a pregnant Justine, wearing a stylish black\nleather trench coat and looking like a high-paid assassin. Mueller had the\neighty-pound engine on his shoulder and was trying to bolt it to a support\nstructure when Musk began peppering him with questions. \u201cHe asked me\nhow much thrust it had,\u201d Mueller said. \u201cHe wanted to know if I had ever\nworked on anything bigger. I told him that yeah, I\u2019d worked on a 650,000-\npound thrust engine at TRW and knew every part of it.\u201d Mueller set the\nengine down and tried to keep up with Musk\u2019s interrogation. \u201cHow much\nwould that big engine cost?\u201d Musk asked. Mueller told him TRW built it for\nabout $12 million. Musk shot back, \u201cYeah, but how much could you really\ndo it for?\u201d\nMueller ended up chatting with Musk for hours. The next weekend,\nMueller invited Musk to his house to continue their discussion. Musk knew\nhe had found someone who really knew the ins and outs of making rockets.\nAfter that, Musk introduced Mueller to the rest of his roundtable of space\nexperts and their stealthy meetings. The caliber of the people impressed\nMueller, who had turned down past job offers from Beal and other budding\nspace magnates because of their borderline insane ideas. Musk, by contrast,\nseemed to know what he was doing, weeding out the naysayers meeting by\nmeeting and forming a crew of bright, committed engineers.\nMueller had helped Musk fill out that spreadsheet around the\nperformance and cost metrics of a new, low-cost rocket, and, along with the\nrest of Team Musk, had subsequently refined the idea. The rocket would not\ncarry truck-sized satellites like some of the monster rockets flown by Boeing, Lockheed, the Russians, and others countries. Instead, Musk\u2019s\nrocket would be aimed at the lower end of the satellite market, and it could\nend up as ideal for an emerging class of smaller payloads that capitalized on\nthe massive advances that had taken place in recent years in computing and\nelectronics technology. The rocket would cater directly to a theory in the\nspace industry that a whole new market might open for both commercial\nand research payloads if a company could drastically lower the price per\nlaunch and perform launches on a regular schedule. Musk relished the idea\nof being at the forefront of this trend and developing the workhorse of a\nnew era in space. Of course, all of this was theoretical\u2014and then, suddenly,\nit wasn\u2019t. PayPal had gone public in February with its shares shooting up 55\npercent, and Musk knew that eBay wanted to buy the company as well.\nWhile noodling on the rocket idea, Musk\u2019s net worth had increased from\ntens of millions to hundreds of millions. In April 2002, Musk fully\nabandoned the publicity-stunt idea and committed to building a commercial\nspace venture. He pulled aside Cantrell, Griffin, Mueller, and Chris\nThompson, an aerospace engineer at Boeing, and told the group, \u201cI want to\ndo this company. If you guys are in, let\u2019s do it.\u201d (Griffin wanted to join but\nended up declining when Musk rebuffed his request to live on the East\nCoast, and Cantrell only stuck around for a few months after this meeting,\nseeing the venture as too risky.)\nFounded in June 2002, Space Exploration Technologies came to life in\nhumble settings. Musk acquired an old warehouse at 1310 East Grand\nAvenue in El Segundo, a suburb of Los Angeles humming with the activity\nof the aerospace industry. The previous tenant of the 75,000-square-foot\nbuilding had done lots of shipping and had used the south side of the facility\nas a logistics depot, outfitting it with several receiving bays for delivery\ntrucks. This allowed Musk to drive his silver McLaren right into the\nbuilding. Beyond that the surroundings were sparse\u2014just a dusty floor and\na forty-foot-high ceiling with its wooden beams and insulation exposed and\nwhich curved at the top to give the place a hangarlike feel. The north side of\nthe building was an office space with cubicles and room for about fifty\npeople. During the first week of SpaceX\u2019s operations, delivery trucks\nshowed up packed full of Dell laptops and printers and folding tables that\nwould serve as the first desks. Musk walked over to one of the loading\ndocks, rolled up the door, and off-loaded the equipment himself. Musk had soon transformed the SpaceX office with what has become\nhis signature factory aesthetic: a glossy epoxy coating applied over concrete\non the floors, and a fresh coat of white paint slathered onto the walls. The\nwhite color scheme was intended to make the factory look clean and feel\ncheerful. Desks were interspersed around the factory so that Ivy League\ncomputer scientists and engineers designing the machines could sit with the\nwelders and machinists building the hardware. This approach stood as\nSpaceX\u2019s first major break with traditional aerospace companies that prefer\nto cordon different engineering groups off from each other and typically\nseparate engineers and machinists by thousands of miles by placing their\nfactories in locations where real estate and labor run cheap.\nAs the first dozen or so employees came to the offices, they were told\nthat SpaceX\u2019s mission would be to emerge as the \u201cSouthwest Airlines of\nSpace.\u201d SpaceX would build its own engines and then contract with\nsuppliers for the other components of the rocket. The company would gain\nan edge over the competition by building a better, cheaper engine and by\nfine-tuning the assembly process to make rockets faster and cheaper than\nanyone else. This vision included the construction of a type of mobile\nlaunch vehicle that could travel to various sites, take the rocket from a\nhorizontal to vertical position, and send it off to space\u2014no muss, no fuss.\nSpaceX was meant to get so good at this process that it could do multiple\nlaunches a month, make money off each one, and never need to become a\nhuge contractor dependent on government funds.\nSpaceX was to be America\u2019s attempt at a clean slate in the rocket\nbusiness, a modernized reset. Musk felt that the space industry had not\nreally evolved in about fifty years. The aerospace companies had little\ncompetition and tended to make supremely expensive products that\nachieved maximum performance. They were building a Ferrari for every\nlaunch, when it was possible that a Honda Accord might do the trick. Musk,\nby contrast, would apply some of the start-up techniques he\u2019d learned in\nSilicon Valley to run SpaceX lean and fast and capitalize on the huge\nadvances in computing power and materials that had taken place over the\npast couple of decades. As a private company, SpaceX would also avoid the\nwaste and cost overruns associated with government contractors. Musk\ndeclared that SpaceX\u2019s first rocket would be called the Falcon 1, a nod to\nStar Wars\u2019 Millennium Falcon and his role as the architect of an exciting\nfuture. At a time when the cost of sending a 550-pound payload started at $30 million, he promised that the Falcon 1 would be able to carry a 1,400-\npound payload for $6.9 million.\nBowing to his nature, Musk set an insanely ambitious timeline for all of\nthis. One of the earliest SpaceX presentations suggested that the company\nwould complete its first engine in May 2003, a second engine in June, the\nbody of the rocket in July, and have everything assembled in August. A\nlaunchpad would then be prepared by September, and the first launch would\ntake place in November 2003, or about fifteen months after the company\nstarted. A trip to Mars was naturally slated for somewhere near the end of\nthe decade. This was Musk the logical, na\u00efve optimist tabulating how long it\nshould take people physically to perform all of this work. It\u2019s the baseline\nhe expects of himself and one that his employees, with their human foibles,\nare in a never-ending struggle to match.\nAs space enthusiasts started to learn about the new company, they didn\u2019t\nreally obsess over whether Musk\u2019s delivery schedule sounded realistic or\nnot. They were just thrilled that someone had decided to take the cheap and\nfast approach. Some members of the military had already been promoting\nthe idea of giving the armed forces more aggressive space capabilities, or\nwhat they called \u201cresponsive space.\u201d If a conflict broke out, the military\nwanted the ability to respond with purpose-built satellites for that mission.\nThis would mean moving away from a model where it takes ten years to\nbuild and deploy a satellite for a specific job. Instead, the military desired\ncheaper, smaller satellites that could be reconfigured through software and\nsent up on short notice, almost like disposable satellites. \u201cIf we could pull\nthat off, it would be really game-changing,\u201d said Pete Worden, a retired air\nforce general, who met with Musk while serving as a consultant to the\nDefense Department. \u201cIt could make our response in space similar to what\nwe do on land, sea and in the air.\u201d Worden\u2019s job required him to look at\nradical technologies. While many of the people he encountered came off as\neccentric dreamers, Musk seemed grounded, knowledgeable, and capable.\n\u201cI talked to people building ray guns and things in their garages. It was\nclear that Elon was different. He was a visionary who really understood the\nrocket technology, and I was impressed with him.\u201d\nLike the military, scientists wanted cheap, quick access to space and the\nability to send up experiments and get data back on a regular basis. Some\ncompanies in the medical and consumer-goods industries were also interested in rides to space to study how a lack of gravity affected the\nproperties of their products.\nAs good as a cheap launch vehicle sounded, the odds of a private citizen\nbuilding one that worked were beyond remote. A quick search on YouTube\nfor \u201crocket explosions\u201d turns up thousands of compilation videos\ndocumenting U.S. and Soviet launch disasters that have occurred over the\ndecades. From 1957 to 1966, the United States alone tried to blast more\nthan 400 rockets into orbit and about 100 of them crashed and burned.5 The\nrockets used to transport things to space are mostly modified missiles\ndeveloped through all of this trial and error and funded by billions upon\nbillions of government dollars. SpaceX had the advantage of being able to\nlearn from this past work and having a few people on staff that had\noverseen rocket projects at companies like Boeing and TRW. That said, the\nstart-up did not have a budget that could support a string of explosions. At\nbest, SpaceX would have three or four shots at making the Falcon 1 work.\n\u201cPeople thought we were just crazy,\u201d Mueller said. \u201cAt TRW, I had an army\nof people and government funding. Now we were going to make a low-cost\nrocket from scratch with a small team. People just didn\u2019t think it could be\ndone.\u201d\nIn July 2002, Musk was gripped by the excitement of this daring\nenterprise, and eBay made its aggressive move to buy PayPal for $1.5\nbillion. This deal gave Musk some liquidity and supplied him with more\nthan $100 million to throw at SpaceX. With such a massive up-front\ninvestment, no one would be able to wrestle control of SpaceX away from\nMusk as they had done at Zip2 and PayPal. For the employees who had\nagreed to accompany Musk on this seemingly impossible journey, the\nwindfall provided at least a couple of years of job security. The acquisition\nalso upped Musk\u2019s profile and celebrity, which he could leverage to score\nmeetings with top government officials and to sway suppliers.\nAnd then all of a sudden none of this seemed to matter. Justine had\ngiven birth to a son\u2014Nevada Alexander Musk. He was ten weeks old\nwhen, just as the eBay deal was announced, he died. The Musks had tucked\nNevada in for a nap and placed the boy on his back as parents are taught to\ndo. When they returned to check on him, he was no longer breathing and\nhad suffered from what the doctors would term a sudden infant death\nsyndrome\u2013related incident. \u201cBy the time the paramedics resuscitated him,\nhe had been deprived of oxygen for so long that he was brain-dead,\u201d Justine wrote in her article for Marie Claire. \u201cHe spent three days on life support in\na hospital in Orange County before we made the decision to take him off it.\nI held him in my arms when he died. Elon made it clear that he did not want\nto talk about Nevada\u2019s death. I didn\u2019t understand this, just as he didn\u2019t\nunderstand why I grieved openly, which he regarded as \u2018emotionally\nmanipulative.\u2019 I buried my feelings instead, coping with Nevada\u2019s death by\nmaking my first visit to an IVF clinic less than two months later. Elon and I\nplanned to get pregnant again as swiftly as possible. Within the next five\nyears, I gave birth to twins, then triplets.\u201d Later, Justine chalked up Musk\u2019s\nreaction to a defense mechanism that he\u2019d learned from years of suffering\nas a kid. \u201cHe doesn\u2019t do well in dark places,\u201d she told Esquire magazine.\n\u201cHe\u2019s forward-moving, and I think it\u2019s a survival thing with him.\u201d\nMusk did open up to a couple of close friends and expressed the depth\nof his misery. But for the most part, Justine read her husband right. He\ndidn\u2019t see the value in grieving publicly. \u201cIt made me extremely sad to talk\nabout it,\u201d Musk said. \u201cI\u2019m not sure why I\u2019d want to talk about extremely\nsad events. It does no good for the future. If you\u2019ve got other kids and\nobligations, then wallowing in sadness does no good for anyone around\nyou. I\u2019m not sure what should be done in such situations.\u201d\nFollowing Nevada\u2019s death, Musk threw himself at SpaceX and rapidly\nexpanded the company\u2019s goals. His conversations with aerospace\ncontractors around possible work for SpaceX left Musk disenchanted. It\nsounded like they all charged a lot of money and worked slowly. The plan\nto integrate components made by these types of companies gave way to the\ndecision to make as much as practical right at SpaceX. \u201cWhile drawing\nupon the ideas of many prior launch vehicle programs from Apollo to the\nX-34/Fastrac, SpaceX is privately developing the entire Falcon rocket from\nthe ground up, including both engines, the turbo-pump, the cryogenic tank\nstructure and the guidance system,\u201d the company announced on its website.\n\u201cA ground up internal development increases difficulty and the required\ninvestment, but no other path will achieve the needed improvement in the\ncost of access to space.\u201d\nThe SpaceX executives Musk hired were an all-star crew. Mueller set to\nwork right away building the two engines\u2014Merlin and Kestrel, named after\ntwo types of falcons. Chris Thompson, a onetime marine who had managed\nthe production of the Delta and Titan rockets at Boeing, joined as the vice\npresident of operations. Tim Buzza also came from Boeing, where he\u2019d earned a reputation as one of the world\u2019s leading rocket testers. Steve\nJohnson, who had worked at JPL and at two commercial space companies,\nwas tapped as the senior mechanical engineer. The aerospace engineer Hans\nKoenigsmann came on to develop the avionics, guidance, and control\nsystems. Musk also recruited Gwynne Shotwell, an aerospace veteran who\nstarted as SpaceX\u2019s first salesperson and rose in the years that followed to\nbe president and Musk\u2019s right-hand woman.\nThese early days also marked the arrival of Mary Beth Brown, a now-\nlegendary character in the lore of both SpaceX and Tesla. Brown\u2014or MB,\nas everyone called her\u2014became Musk\u2019s loyal assistant, establishing a real-\nlife version of the relationship between Iron Man\u2019s Tony Stark and Pepper\nPotts. If Musk worked a twenty-hour day, so too did Brown. Over the years,\nshe brought Musk meals, set up his business appointments, arranged time\nwith his children, picked out his clothes, dealt with press requests, and\nwhen necessary yanked Musk out of meetings to keep him on schedule. She\nwould emerge as the only bridge between Musk and all of his interests and\nwas an invaluable asset to the companies\u2019 employees.\nBrown played a key role in developing SpaceX\u2019s early culture. She paid\nattention to small details like the office\u2019s red spaceship trash cans and\nhelped balance the vibe around the office. When it came to matters related\ndirectly to Musk, Brown put on her firm countenance and no-nonsense\nattitude. The rest of the time she usually had a warm, broad smile and a\ndisarming charm. \u201cIt was always, \u2018Oh, dear. How are you, dear?\u2019\u201d recalled\na SpaceX technician. Brown collected the weird e-mails that arrived for\nMusk and sent them out as \u201cKook of the Week\u201d missives to make people\nlaugh. One of the better entries included a pencil sketch of a lunar\nspacecraft that had a red spot on the page. The person who sent in the letter\nhad circled the spot on his own drawing and then written \u201cWhat is that?\nBlood?\u201d next to it. In other letters there were plans for a perpetual motion\nmachine and a proposal for a giant inflatable rabbit that could be used to\nplug oil spills. For a short time, Brown\u2019s duties extended to managing\nSpaceX\u2019s books and handling the flow of business in Musk\u2019s absence. \u201cShe\npretty much called the shots,\u201d the technician said. \u201cShe would say, \u2018This is\nwhat Elon would want.\u2019\u201d\nHer greatest gift, though, may have been reading Musk\u2019s moods. At\nboth SpaceX and Tesla, Brown placed her desk a few feet in front of\nMusk\u2019s, so that people had to pass her before having a meeting with him. If someone needed to request permission to buy a big-ticket item, they would\nstop for a moment in front of Brown and wait for a nod to go see Musk or\nthe shake-off to go away because Musk was having a bad day. This system\nof nods and shakes became particularly important during periods of\nromantic strife for Musk, when his nerves were on edge more than usual.\nThe rank-and-file engineers at SpaceX tended to be young, male\noverachievers. Musk would personally reach out to the aerospace\ndepartments of top colleges and inquire about the students who had finished\nwith the best marks on their exams. It was not unusual for him to call the\nstudents in their dorm rooms and recruit them over the phone. \u201cI thought it\nwas a prank call,\u201d said Michael Colonno, who heard from Musk while\nattending Stanford. \u201cI did not believe for a minute that he had a rocket\ncompany.\u201d Once the students looked Musk up on the Internet, selling them\non SpaceX was easy. For the first time in years if not decades, young\naeronautics whizzes who pined to explore space had a really exciting\ncompany to latch on to and a path toward designing a rocket or even\nbecoming an astronaut that did not require them to join a bureaucratic\ngovernment contractor. As word of SpaceX\u2019s ambitions spread, top\nengineers from Boeing, Lockheed Martin, and Orbital Sciences with a high\ntolerance for risk fled to the upstart, too.\nThroughout the first year at SpaceX, one or two new employees joined\nalmost every week. Kevin Brogan was employee No. 23 and came from\nTRW, where he\u2019d been used to various internal policies blocking him from\ndoing work. \u201cI called it the country club,\u201d he said. \u201cNobody did anything.\u201d\nBrogan started at SpaceX the day after his interview and was told to go\nhunting in the office for a computer to use. \u201cIt was go to Fry\u2019s and get\nwhatever you need and go to Staples and get a chair,\u201d Brogan said. He\nimmediately felt in over his head and would work for twelve hours, drive\nhome, sleep for ten hours, and then head right back to the factory. \u201cI was\nexhausted and out of shape mentally,\u201d he said. \u201cBut soon I loved it and got\ntotally hooked.\u201d\nOne of the first projects SpaceX decided to tackle was the construction\nof a gas generator, a machine not unlike a small rocket engine that produces\nhot gas. Mueller, Buzza, and a couple of young engineers assembled the\ngenerator in Los Angeles and then packed it into the back of a pickup truck\nand drove it out to Mojave, California, to test it. A desert town about one\nhundred miles from Los Angeles, Mojave had become a hub for aerospace companies like Scaled Composites and XCOR. A lot of the aerospace\nprojects were based out of the Mojave airport, where companies had their\nworkshops and sent up all manner of cutting-edge airplanes and rockets.\nThe SpaceX team fit right into this environment and borrowed a test stand\nfrom XCOR that was just about the perfect size to hold the gas generator.\nThe first ignition run took place at 11 A.M. and lasted ninety seconds. The\ngas generator worked, but it had let out a billowing black smoke cloud that\non this windless day parked right over the airport tower. The airport\nmanager came down to the test area and lit into Mueller and Buzza. The\nairport official and some of the guys from XCOR who had been helping out\nurged the SpaceX engineers to take it easy and wait until the next day to run\nanother test. Instead, Buzza a strong leader ready to put SpaceX\u2019s relentless\nethos into play, coordinated a couple of trucks to pick up more fuel, talked\nthe airport manager down, and got the test stand ready for another fire. In\nthe days that followed, SpaceX\u2019s engineers perfected a routine that let them\ndo multiple tests a day\u2014an unheard-of practice at the airport\u2014and had the\ngas generator tuned to their liking after two weeks of work.\nThey made a few more trips to Mojave and some other spots, including\na test stand at Edwards Air Force Base and another in Mississippi. While on\nthis countrywide rocketry tour, the SpaceX engineers came across a three-\nhundred-acre test site in McGregor, Texas, a small city near the center of\nthe state. They really liked this spot, and talked Musk into buying it. The\nnavy had tested rockets on the land years before and so too had Andrew\nBeal before his aerospace company collapsed. \u201cAfter Beal saw it was going\nto cost him $300 million to develop a rocket capable of sending sizeable\nsatellites into orbit, he called it quits, leaving behind a lot of useful\ninfrastructure for SpaceX, including a three-story concrete tripod with legs\nas big around as redwood tree trunks,\u201d wrote journalist Michael Belfiore in\nRocketeers, a book that captured the rise of a handful of private space\ncompanies.\nJeremy Hollman was one of the young engineers who soon found\nhimself living in Texas and customizing the test site to SpaceX\u2019s needs.\nHollman exemplified the kind of recruit Musk wanted: he\u2019d earned an\naerospace engineering degree from Iowa State University and a master\u2019s in\nastronautical engineering from the University of Southern California. He\u2019d\nspent a couple of years working as a test engineer at Boeing dealing with\njets, rockets, and spacecraft.* The stint at Boeing had left Hollman unimpressed with big aerospace.\nHis first day on the job came right as Boeing completed its merger with\nMcDonnell Douglas. The resultant mammoth government contractor held a\npicnic to boost morale but ended up failing at even this simple exercise.\n\u201cThe head of one of the departments gave a speech about it being one\ncompany with one vision and then added that the company was very cost\nconstrained,\u201d Hollman said. \u201cHe asked that everyone limit themselves to\none piece of chicken.\u201d Things didn\u2019t improve much from there. Every\nproject at Boeing felt large, cumbersome, and costly. So, when Musk came\nalong selling radical change, Hollman bit. \u201cI thought it was an opportunity I\ncould not pass up,\u201d he said. At twenty-three, Hollman was young, single,\nand willing to give up any semblance of having a life in favor of working at\nSpaceX nonstop, and he became Mueller\u2019s second in command.\nMueller had developed a pair of three-dimensional computer models of\nthe two engines he wanted to build. Merlin would be the engine for the first\nstage of the Falcon 1, which lifted it off the ground, and Kestrel would be\nthe smaller engine used to power the upper, second stage of the rocket and\nguide it in space. Together, Hollman and Mueller figured out which parts of\nthe engines SpaceX would build at the factory and which parts it would try\nto buy. For the purchased parts, Hollman had to head out to various\nmachine shops and get quotes and delivery dates for the hardware. Quite\noften, the machinists told Hollman that SpaceX\u2019s timelines were nuts.\nOthers were more accommodating and would try to bend an existing\nproduct to SpaceX\u2019s needs instead of building something from scratch.\nHollman also found that creativity got him a long way. He discovered, for\nexample, that changing the seals on some readily available car wash valves\nmade them good enough to be used with rocket fuel.\nAfter SpaceX completed its first engine at the factory in California,\nHollman loaded it and mounds of other equipment into a U-Haul trailer. He\nhitched the U-Haul to the back of a white Hummer H2 and drove four\nthousand pounds of gear* across Interstate 10 from Los Angeles to Texas\nand the test site. The arrival of the engine in Texas kicked off one of the\ngreat bonding exercises in SpaceX\u2019s history. Amid rattlesnakes, fire ants,\nisolation, and searing heat, the group led by Buzza and Mueller began the\nprocess of exploring every intricacy of the engines. It was a high-pressure\nslog full of explosions\u2014or what the engineers politely called \u201crapid\nunscheduled disassemblies\u201d\u2014that would determine whether a small band of engineers really could match the effort and skill of nation-states. The\nSpaceX employees christened the site in fitting fashion, downing a $1,200\nbottle of R\u00e9my Martin cognac out of paper cups and passing a sobriety test\non the drive back to the company apartments in the Hummer. From that\npoint on, the trek from California to the test site became known as the Texas\nCattle Haul. The SpaceX engineers would work for ten days straight, come\nback to California for a weekend, and then head back. To ease the burden of\ntravel, Musk sometimes let them use his private jet. \u201cIt carried six people,\u201d\nMueller said. \u201cWell, seven if someone sat in the toilet, which happened all\nthe time.\u201d\nWhile the navy and Beal had left some testing apparatus, SpaceX had to\nbuild a large amount of custom gear. One of the largest of these structures\nwas a horizontal test stand about 30 feet long, 15 feet wide, and 15 feet tall.\nThen there was the complementary vertical test stand that stood two stories\nhigh. When an engine needed to be fired, it would be fastened to one of the\ntest stands, outfitted with sensors to collect data, and monitored via several\ncameras. The engineers took shelter in a bunker protected on one side by a\ndirt embankment. If something went wrong, they would look at feeds from\nthe webcams or slowly lift one of the bunker\u2019s hatches to listen for any\nclues. The locals in town rarely complained about the noise, although the\nanimals on nearby farms seemed less impressed. \u201cCows have this natural\ndefense mechanism where they gather and start running in a circle,\u201d\nHollman said. \u201cEvery time we fired an engine, the cows scattered and then\ngot in that circle with the younger ones placed in the middle. We set up a\ncow cam to watch them.\u201d\nBoth Kestrel and Merlin came with challenges, and they were treated as\nalternating engineering exercises. \u201cWe would run Merlin until we ran out of\nhardware or did something bad,\u201d Mueller said. \u201cThen we\u2019d run Kestrel and\nthere was never a shortage of things to do.\u201d For months, the SpaceX\nengineers arrived at the site at 8 A.M. and spent twelve hours there working\non the engines before retiring to the Outback Steakhouse for dinner. Mueller\nhad a particular knack for looking over test data and spotting some place\nwhere the engine ran hot or cold or had another flaw. He would call\nCalifornia and prescribe hardware changes, and engineers would refashion\nparts and send them off to Texas. Often the workers in Texas modified parts\nthemselves using a mill and lathe that Mueller had brought out. \u201cKestrel\nstarted out as a real dog, and one of my proudest moments was taking it from terrible to great performance with stuff we bought online and did in\nthe machine shop,\u201d Mueller said. Some members of the Texas crew honed\ntheir skills to the point that they could build a test-worthy engine in three\ndays. These same people were required to be adept at software. They\u2019d pull\nan all-nighter building a turbo pump for the engine and then dig in the next\nnight to retool a suite of applications used to control the engines. Hollman\ndid this type of work all the time and was an all-star, but he was not alone\namong this group of young, nimble engineers who crossed disciplines out of\nnecessity and the spirit of adventure. \u201cThere was an almost addictive quality\nto the experience,\u201d Hollman said. \u201cYou\u2019re twenty-four or twenty-five, and\nthey\u2019re trusting you with so much. It was very empowering.\u201d\nTo get to space, the Merlin engine would need to burn for 180 seconds.\nThat seemed like an eternity for the engineers at the outset of their stint in\nTexas, when the engine would burn for only a half second before it conked\nout. Sometimes Merlin vibrated too much during the tests. Sometimes it\nresponded badly to a new material. Sometimes it cracked and needed major\npart upgrades, like moving from an aluminum manifold to a manifold made\nout of the more exotic Inconel, an alloy suited to extreme temperatures. On\none occasion, a fuel valve refused to open properly and caused the whole\nengine to blow up. Another test gone wrong ended up with the whole test\nstand burning down. It usually came to Buzza and Mueller to make the\nunpleasant call back to Musk and recap the day\u2019s foibles. \u201cElon had pretty\ngood patience,\u201d Mueller said. \u201cI remember one time we had two test stands\nrunning and blew up two things in one day. I told Elon we could put another\nengine on there, but I was really, really frustrated and just tired and mad and\nwas kinda short with Elon. I said, \u2018We can put another fucking thing on\nthere, but I\u2019ve blown up enough shit today.\u2019 He said, \u2018Okay, all right, that\u2019s\nfine. Just calm down. We\u2019ll do it again tomorrow.\u2019\u201d Coworkers in El\nSegundo later reported that Musk had been near tears during this call after\nhearing the frustration and agony in Mueller\u2019s voice.\nWhat Musk would not tolerate were excuses or the lack of a clear plan\nof attack. Hollman was one of many engineers who arrived at this\nrealization after facing one of Musk\u2019s trademark grillings. \u201cThe worst call\nwas the first one,\u201d Hollman said. \u201cSomething had gone wrong, and Elon\nasked me how long it would take to be operational again, and I didn\u2019t have\nan immediate answer. He said, \u2018You need to. This is important to the\ncompany. Everything is riding on this. Why don\u2019t you have an answer?\u2019 He kept hitting me with pointed, direct questions. I thought it was more\nimportant to let him know quickly what happened, but I learned it was more\nimportant to have all the information.\u201d\nFrom time to time, Musk participated in the testing process firsthand.\nOne of the more memorable examples of this came as SpaceX tried to\nperfect a cooling chamber for its engines. The company had bought several\nof these chambers at $75,000 a pop and needed to put them under pressure\nwith water to gauge their ability to handle stress. During the initial test, one\nof the pricey chambers cracked. Then the second one broke in the same\nplace. Musk ordered a third test, as the engineers looked on in horror. They\nthought the test might be putting the chamber under undue stress and that\nMusk was burning through essential equipment. When the third chamber\ncracked, Musk flew the hardware back to California, took it to the factory\nfloor, and, with the help of some engineers, started to fill the chambers with\nan epoxy to see if it would seal them. \u201cHe\u2019s not afraid to get his hands\ndirty,\u201d Mueller said. \u201cHe\u2019s out there with his nice Italian shoes and clothes\nand has epoxy all over him. They were there all night and tested it again and\nit broke anyway.\u201d Musk, clothes ruined, had decided the hardware was\nflawed, tested his hypothesis, and moved on quickly, asking the engineers to\ncome up with a new solution.\nThese incidents were all part of a trying but productive process. SpaceX\nhad developed the feeling of a small, tight-knit family up against the world.\nIn late 2002, the company had an empty warehouse. One year later, the\nfacility looked like a real rocket factory. Working Merlin engines were\narriving back from Texas, and being fed into an assembly line where\nmachinists could connect them to the main body, or first stage, of the\nrocket. More stations were set up to link the first stage with the upper stage\nof the rocket. Cranes were placed on the floor to handle the heavy lifting of\ncomponents, and blue metal transport tracks were positioned to guide the\nrocket\u2019s body through the factory from station to station. SpaceX had also\nstarted to build the fairing, or case, that protects payloads atop the rocket\nduring launch and then opens up like a clam in space to let out the cargo.\nSpaceX had picked up a customer as well. According to Musk, its first\nrocket would launch in \u201cearly 2004\u201d from Vandenberg Air Force Base,\ncarrying a satellite called TacSat-1 for the Department of Defense. With this\ngoal looming, twelve-hour days, six days a week were considered the norm,\nalthough many people worked longer than that for extended periods of time. Respites, as far as they existed, came around 8 P.M. on some weeknights\nwhen Musk would allow everyone to use their work computers to play first-\nperson-shooter video games like Quake III Arena and Counter-Strike\nagainst each other. At the appointed hour, the sound of guns loading would\ncascade throughout the office as close to twenty people armed themselves\nfor battle. Musk\u2014playing under the handle Random9\u2014often won the\ngames, talking trash and blasting away his employees without mercy. \u201cThe\nCEO is there shooting at us with rockets and plasma guns,\u201d said Colonno.\n\u201cWorse, he\u2019s almost alarmingly good at these games and has insanely fast\nreactions. He knew all the tricks and how to sneak up on people.\u201d\nThe pending launch ignited Musk\u2019s salesman instincts. He wanted to\nshow the public what his tireless workers had accomplished and drum up\nsome excitement around SpaceX. Musk decided to unveil a prototype of\nFalcon 1 to the public in December 2003. The company would haul the\nseven-story-high Falcon 1 across the country on a specially built rig and\nleave it\u2014and the SpaceX mobile launch system\u2014outside of the Federal\nAviation Administration\u2019s headquarters in Washington, D.C. An\naccompanying press conference would make it clear to Washington that a\nmodern, smarter, cheaper rocket maker had arrived.\nThis marketing song and dance didn\u2019t sound sensible to SpaceX\u2019s\nengineers. They were working more than one hundred hours per week to\nmake the actual rocket that SpaceX would need to be in business. Musk\nwanted them to do that and build a slick-looking mock-up. Engineers were\ncalled back from Texas and assigned another ulcer-inducing deadline to\ncraft this prop. \u201cIn my mind, it was a boondoggle,\u201d Hollman said. \u201cIt wasn\u2019t\nadvancing anything. In Elon\u2019s mind, it would get us a lot of backing from\nimportant people in the government.\u201d\nWhile making the prototype for the event, Hollman experienced the full\nspectrum of highs and lows that came with working for Musk. The engineer\nhad lost his regular glasses weeks earlier when they slipped off his face and\nfell down a flame duct at the Texas test site. Hollman had since made do by\nwearing an old pair of prescription safety glasses,* but they too were ruined\nwhen he scratched the lenses while trying to duck under an engine at the\nSpaceX factory. Without a spare moment to visit an optometrist, Hollman\nstarted to feel his sanity fray. The long hours, the scratch, the publicity stunt\n\u2014they were all too much. He vented about this in the factory one night, unaware that Musk stood\nnearby and could hear everything. Two hours later, Mary Beth Brown\nappeared with an appointment card to see a Lasik eye surgery specialist.\nWhen Hollman visited the doctor, he discovered that Musk had already\nagreed to pay for the surgery. \u201cElon can be very demanding, but he\u2019ll make\nsure the obstacles in your way are removed,\u201d Hollman said. Upon\nreflection, he also warmed to the long-term thinking behind Musk\u2019s\nWashington plan. \u201cI think he wanted to add an element of realism to\nSpaceX, and if you park a rocket in someone\u2019s front yard, it\u2019s hard to deny\nit,\u201d Hollman said.\nThe event in Washington ended up being well received, and just a few\nweeks after it took place, SpaceX made another astonishing announcement.\nDespite not having even flown a rocket yet, SpaceX revealed plans for a\nsecond rocket. Along with the Falcon 1, it would build the Falcon 5. Per the\nname, this rocket would have five engines and could carry more weight\u2014\n9,200 pounds\u2014to low orbit around Earth. Crucially, the Falcon 5 could also\ntheoretically reach the International Space Station for resupply missions\u2014a\ncapability that would open up SpaceX for some large NASA contracts. And,\nin a nod to Musk\u2019s obsession with safety, the rocket was said to be able to\ncomplete its missions even if three of the five engines failed, which was a\nlevel of added reliability that had not been seen in the market in decades.\nThe only way to keep up with all of this work was to do what SpaceX\nhad promised from the beginning: operate in the spirit of a Silicon Valley\nstart-up. Musk was always looking for brainy engineers who had not just\ndone well at school but had done something exceptional with their talents.\nWhen he found someone good, Musk was relentless in courting him or her\nto come to SpaceX. Bryan Gardner, for example, first met Musk at a space\nrave in the hangars at the Mojave airport and a short while later started\ntalking about a job. Gardner was having some of his academic work\nsponsored by Northrop Grumman. \u201cElon said, \u2018We\u2019ll buy them out,\u2019\u201d\nGardner said. \u201cSo, I e-mailed him my resume at two thirty A.M., and he\nreplied back in thirty minutes addressing everything I put in there point by\npoint. He said, \u2018When you interview make sure you can talk concretely\nabout what you do rather than use buzzwords.\u2019 It floored me that he would\ntake the time to do this.\u201d After being hired, Gardner was tasked with\nimproving the system for testing the valves on the Merlin engine. There\nwere dozens of valves, and it took three to five hours to manually test each one. Six months later, Gardner had built an automated system for testing the\nvalves in minutes. The testing machine tracked the valves individually, so\nthat an engineer in Texas could request what the metrics had been on a\nspecific part. \u201cI had been handed this redheaded stepchild that no one else\nwanted to deal with and established my engineering credibility,\u201d Gardner\nsaid.\nAs the new hires arrived, SpaceX moved beyond its original building to\nfill up several buildings in the El Segundo complex. The engineers were\nrunning demanding software and rendering large graphics files and needed\nhigh-speed connections between all of these offices. But SpaceX had\nneighbors who were blocking an initiative to connect all of its buildings via\nfiber optic lines. Instead of taking the time to haggle with the other\ncompanies for right of way, the IT chief Branden Spikes, who had worked\nwith Musk at Zip2 and PayPal, came up with a quicker, more devious\nsolution. A friend of his worked for the phone company and drew a diagram\nthat demonstrated a way to squeeze a networking cable safely between the\nelectricity, cable, and phone wires on a telephone pole. At 2 A.M., an off-the-\nbooks crew showed up with a cherry picker and ran fiber to the telephone\npoles and then ran cables straight to the SpaceX buildings. \u201cWe did that\nover a weekend instead of taking months to get permits,\u201d Spikes said.\n\u201cThere was always this feeling that we were facing a sort of insurmountable\nchallenge and that we had to band together to fight the good fight.\u201d\nSpaceX\u2019s landlord, Alex Lidow, chuckled when thinking back to all of the\nantics of Musk\u2019s team. \u201cI know they did a lot of hanky stuff at night,\u201d he\nsaid. \u201cThey were smart, needed to get things done, and didn\u2019t always have\ntime to wait for things like city permits.\u201d\nMusk never relented in asking his employees to do more and be better,\nwhether it was at the office or during extracurricular activities. Part of\nSpikes\u2019s duties included building custom gaming PCs for Musk\u2019s home that\npushed their computational power to the limits and needed to be cooled\nwith water running through a series of tubes inside the machines. When one\nof these gaming rigs kept breaking, Spikes figured out that Musk\u2019s mansion\nhad dirty power lines and had a second, dedicated power circuit built for the\ngaming room to correct the problem. Doing this favor bought Spikes no\nspecial treatment. \u201cSpaceX\u2019s mail server crashed one time, and Elon word\nfor word said, \u2018Don\u2019t ever fucking let that happen again,\u2019\u201d Spikes said. \u201cHe had a way of looking at you\u2014a glare\u2014and would keep looking at you until\nyou understood him.\u201d\nMusk had tried to find contractors that could keep up with SpaceX\u2019s\ncreativity and pace. Instead of always hitting up aerospace guys, for\nexample, he located suppliers with similar experience from different fields.\nEarly on, SpaceX needed someone to build the fuel tanks, essentially the\nmain body of the rocket, and Musk ended up in the Midwest talking to\ncompanies that had made large, metal agricultural tanks used in the dairy\nand food processing businesses. These suppliers also struggled to keep up\nwith SpaceX\u2019s schedule, and Musk found himself flying across the country\nto pay visits\u2014sometimes surprise ones\u2014on the contractors to check on\ntheir progress. One such inspection took place at a company in Wisconsin\ncalled Spincraft. Musk and a couple of SpaceX employees flew his jet\nacross the country and arrived late at night expecting to see a shift of\nworkers doing extra duty to get the fuel tanks completed. When Musk\ndiscovered that Spincraft was well behind schedule, he turned to a Spincraft\nemployee and informed him, \u201cYou\u2019re fucking us up the ass, and it doesn\u2019t\nfeel good.\u201d David Schmitz was a general manager at Spincraft and said\nMusk earned a reputation as a fearsome negotiator who did indeed follow\nup on things personally. \u201cIf Elon was not happy, you knew it,\u201d Schmitz said.\n\u201cThings could get nasty.\u201d In the months that followed that meeting, SpaceX\nincreased its internal welding capabilities so that it could make the fuel\ntanks in El Segundo and ditch Spincraft.\nAnother salesman flew down to SpaceX to sell the company on some\ntechnology infrastructure equipment. He was doing the standard\nrelationship-building exercise practiced by salespeople for centuries. Show\nup. Speak for a while. Feel each other out. Then, start doing business down\nthe road. Musk was having none of it. \u201cThe guy comes in, and Elon asks\nhim why they\u2019re meeting,\u201d Spikes said. \u201cHe said, \u2018To develop a\nrelationship.\u2019 Elon replied, \u2018Okay. Nice to meet you,\u2019 which basically\nmeant, \u2018Get the fuck out of my office.\u2019 This guy had spent four hours\ntraveling for what ended up as a two-minute meeting. Elon just has no\ntolerance for that kind of stuff.\u201d Musk could be equally brisk with\nemployees who were not hitting his standards. \u201cHe would often say, \u2018The\nlonger you wait to fire someone the longer it has been since you should\nhave fired them,\u2019\u201d Spikes said. Most of the SpaceX employees were thrilled to be part of the company\u2019s\nadventure and tried not to let Musk\u2019s grueling demands and harsh behavior\nget to them. But there were some moments where Musk went too far. The\nengineering corps flew into a collective rage every time they caught Musk\nin the press claiming to have designed the Falcon rocket more or less by\nhimself. Musk also hired a documentary crew to follow him around for a\nwhile. This audacious gesture really grated on the people toiling away in the\nSpaceX factory. They felt like Musk\u2019s ego had gotten the best of him and\nthat he was presenting SpaceX as the conqueror of the aerospace industry\nwhen the company had yet to launch successfully. Employees who made\ndetailed cases around what they saw as flaws in the Falcon 5 design or\npresented practical suggestions to get the Falcon 1 out the door more\nquickly were often ignored or worse. \u201cThe treatment of staff was not good\nfor long stretches of this era,\u201d said one engineer. \u201cMany good engineers,\nwho everyone beside \u2018management\u2019 felt were assets to the company, were\nforced out or simply fired outright after being blamed for things they hadn\u2019t\ndone. The kiss of death was proving Elon wrong about something.\u201d\nEarly 2004, when SpaceX had hoped to launch its rocket, came and\nwent. The Merlin engine that Mueller and his team had built appeared to be\namong the most efficient rocket engines ever made. It was just taking longer\nthan Musk had expected to pass tests needed to clear the engine for a\nlaunch. Finally, in the fall of 2004, the engines were burning consistently\nand meeting all their requirements. This meant that Mueller and his team\ncould breathe easy and that everyone else at SpaceX should prepare to\nsuffer. Mueller had spent SpaceX\u2019s entire existence as the \u201ccritical path\u201d\u2014\nthe person holding up the company from achieving its next steps\u2014working\nunder Musk\u2019s scrutiny. \u201cWith the engine ready, it was time for mass panic,\u201d\nMueller said. \u201cNo one else knew what it was like to be on critical path.\u201d\nLots of people soon found out, as major problems abounded. The\navionics, which included the electronics for the navigation, communication,\nand overall management of the rocket, turned into a nightmare. Seemingly\ntrivial things like getting a flash storage drive to talk to the rocket\u2019s main\ncomputer failed for undetectable reasons. The software needed to manage\nthe rocket also became a major burden. \u201cIt\u2019s like anything else where you\nfind out that the last ten percent is where all the integration happens and\nthings don\u2019t play together,\u201d Mueller said. \u201cThis process went on for six\nmonths.\u201d Finally, in May 2005, SpaceX transported the rocket 180 miles north to Vandenberg Air Force Base for a test fire and completed a five-\nsecond burn on the launchpad.\nLaunching from Vandenberg would have been very convenient for\nSpaceX. The site is close to Los Angeles and has several launchpads to pick\nfrom. SpaceX, though, became an unwelcome guest. The air force gave the\nnewcomer a cool welcome, and the people assigned to manage the launch\nsites did not go out of their way help SpaceX. Lockheed and Boeing, which\nfly $1 billion spy satellites for the military from Vandenberg, didn\u2019t care for\nSpaceX\u2019s presence, either\u2014in part because SpaceX represented a threat to\ntheir business and in part because this startup was mucking around near\ntheir precious cargo. As SpaceX started to move from the testing phase to\nthe launch, it was told to get in line. They would have to wait months to\nlaunch. \u201cEven though they said we could fly, it was clear that we would\nnot,\u201d said Gwynne Shotwell.\nSearching for a new site, Shotwell and Hans Koenigsmann put a\nMercator projection of the world up on the wall and looked for a name they\nrecognized along the equator, where the planet spins faster and gives\nrockets an added boost. The first name that jumped out was Kwajalein\nIsland\u2014or Kwaj\u2014the largest island in an atoll between Guam and Hawaii\nin the Pacific Ocean and part of the Republic of the Marshall Islands. This\nspot registered with Shotwell because the U.S. Army had used it for\ndecades as a missile test site. Shotwell looked up the name of a colonel at\nthe test site and sent him an e-mail, and three weeks later got a call back\nwith the army saying they would love to have SpaceX fly from the islands.\nIn June 2005, SpaceX\u2019s engineers began to fill containers with their\nequipment to ship them to Kwaj.\nAbout one hundred islands make up the Kwajalein Atoll. Many of them\nstretch for just a few hundred yards and are much longer than they are wide.\n\u201cFrom the air, the place looks like these beautiful beads on a string,\u201d said\nPete Worden, who visited the site in his capacity as a Defense Department\nconsultant. Most of the people in the area live on an island called Ebeye,\nwhile the U.S. military has taken over Kwajalein, the southernmost island,\nand turned it into part tropical paradise and part Dr. Evil\u2019s secret lair. The\nUnited States spent years lobbing its ICBMs from California at Kwaj and\nused the island to run experiments on its space weapons during the \u201cStar\nWars\u201d period. Laser beams would be aimed at Kwaj from space in a bid to\nsee if they were accurate and responsive enough to take out an ICBM hurtling toward the islands. The military presence resulted in a weird array\nof buildings including hulking, windowless trapezoidal concrete structures\nclearly conceived by someone who deals with death for a living.\nTo get to Kwaj, the SpaceX employees either flew on Musk\u2019s jet or took\ncommercial flights through Hawaii. The main accommodations were two-\nbedroom affairs on Kwajalein Island that looked more like dormitories than\nhotel rooms, with their military-issued dressers and desks. Any materials\nthat the engineers needed had to be flown in on Musk\u2019s plane or were more\noften brought by boat from Hawaii or the mainland United States. Each day,\nthe SpaceX crew gathered their gear and took a forty-five-minute boat ride\nto Omelek, a seven-acre, palm-tree-and vegetation-covered island that\nwould be transformed into their launchpad. Over the course of several\nmonths, a small team of people cleared the brush, poured concrete to\nsupport the launchpad, and converted a double-wide trailer into offices. The\nwork was grueling and took place in soul-sapping humidity under a sun\npowerful enough to burn the skin through a T-shirt. Eventually, some of the\nworkers preferred to spend the night on Omelek rather than make the\njourney through rough waters back to the main island. \u201cSome of the offices\nwere turned into bedrooms with mattresses and cots,\u201d Hollman said. \u201cThen\nwe shipped over a very nice refrigerator and a good grill and plumbed in a\nshower. We tried to make it less like camping and more like living.\u201d\nThe sun rose at 7 A.M. each day, and that\u2019s when the SpaceX team got to\nwork. A series of meetings would take place with people listing what\nneeded to get done, and debating solutions to lingering problems. As the\nlarge structures arrived, the workers placed the body of the rocket\nhorizontally in a makeshift hangar and spent hours melding together all of\nits parts. \u201cThere was always something to do,\u201d Hollman said. \u201cIf the engine\nwasn\u2019t a problem, then there was an avionics problem or a software\nproblem.\u201d By 7 P.M., the engineers wound down their work. \u201cOne or two\npeople would decide it was their night to cook, and they would make steak\nand potatoes and pasta,\u201d Hollman said. \u201cWe had a bunch of movies and a\nDVD player, and some of us did a lot of fishing off the docks.\u201d For many of\nthe engineers, this was both a torturous and magical experience. \u201cAt Boeing\nyou could be comfortable, but that wasn\u2019t going to happen at SpaceX,\u201d said\nWalter Sims, a SpaceX tech expert who found time to get certified to dive\nwhile on Kwaj. \u201cEvery person on that island was a fucking star, and they were always holding seminars on radios or the engine. It was such an\ninvigorating place.\u201d\nThe engineers were constantly baffled by what Musk would fund and\nwhat he wouldn\u2019t. Back at headquarters, someone would ask to buy a\n$200,000 machine or a pricey part that they deemed essential to Falcon 1\u2019s\nsuccess, and Musk would deny the request. And yet he was totally\ncomfortable paying a similar amount to put a shiny surface on the factory\nfloor to make it look nice. On Omelek, the workers wanted to pave a two-\nhundred-yard pathway between the hangar and the launchpad to make it\neasier to transport the rocket. Musk refused. This left the engineers moving\nthe rocket and its wheeled support structure in the fashion of the ancient\nEgyptians. They laid down a series of wooden planks and rolled the rocket\nacross them, grabbing the last piece of wood from the back and running it\nforward in a continuous cycle.\nThe whole situation was ludicrous. A start-up rocket company had\nended up in the middle of nowhere trying to pull off one of the most\ndifficult feats known to man, and, truth be told, only a handful of the\nSpaceX team had any idea how to make a launch happen. Time and again,\nthe rocket would get marched out to the launchpad and hoisted vertical for a\ncouple of days, while technical and safety checks would reveal a litany of\nnew problems. The engineers worked on the rocket for as long as they could\nbefore laying it horizontal and marching it back to the hangar to avoid\ndamage from the salty air. Teams that had worked separately for months\nback at the SpaceX factory\u2014propulsion, avionics, software\u2014were thrust\ntogether on the island and forced to become an interdisciplinary whole. The\nsum total was an extreme learning and bonding exercise that played like a\ncomedy of errors. \u201cIt was like Gilligan\u2019s Island except with rockets,\u201d\nHollman said.\nIn November 2005, about six months after they had first gotten to the\nisland, the SpaceX team felt ready to give launching a shot. Musk flew in\nwith his brother, Kimbal, and joined the majority of the SpaceX team in the\nbarracks on Kwaj. On November 26, a handful of people woke up at 3 A.M.\nand filled the rocket with liquid oxygen. They then scampered off to an\nisland about three miles away for protection, while the rest of the SpaceX\nteam monitored the launch systems from a control room twenty-six miles\naway on Kwaj. The military gave SpaceX a six-hour launch window.\nEveryone was hoping to see the first stage take off and reach about 6,850 miles per hour before giving way to the second stage, which would ignite\nup in the air and reach 17,000 miles per hour. But, while going through the\npre-launch checks, the engineers detected a major problem: a valve on a\nliquid oxygen tank would not close, and the LOX was boiling off into the\nair at 500 gallons per hour. SpaceX scrambled to fix the issue but lost too\nmuch of its fuel to launch before the window closed.\nWith that mission aborted, SpaceX ordered major LOX reinforcements\nfrom Hawaii and prepared for another attempt in mid-December. High\nwinds, faulty valves, and other errors thwarted that launch attempt. Before\nanother attempt could be made, SpaceX discovered on a Saturday night that\nthe rocket\u2019s power distribution systems had started malfunctioning and\nwould need new capacitors. On Sunday morning, the rocket was lowered\nand split into its two stages so that a technician could slide in and remove\nthe electrical boards. Someone found an electronics supplier that was open\non Sunday in Minnesota, and off a SpaceX employee flew to get some fresh\ncapacitors. By Monday he was in California and testing the parts at\nSpaceX\u2019s headquarters to make sure they passed various heat and vibration\nchecks, then on a plane again back to the islands. In under eighty hours, the\nelectronics had been returned in working order and installed in the rocket.\nThe dash to the United States and back showed that SpaceX\u2019s thirty-person\nteam had real pluck in the face of adversity and inspired everyone on the\nisland. A traditional three-hundred-person-strong aerospace launch crew\nwould never have tried to fix a rocket like that on the fly. But the energy,\nsmarts, and resourcefulness of the SpaceX team still could not overcome\ntheir inexperience or the difficult conditions. More problems arose and\nblocked any thoughts of a launch.\nFinally, on March 24, 2006, it was all systems go. The Falcon 1 stood\non its square launchpad and ignited. It soared into the sky, turning the island\nbelow it into a green spec amid a vast, blue expanse. In the control room,\nMusk paced as he watched the action, wearing shorts, flip-flops, and a T-\nshirt. Then, about twenty-five seconds in, it became clear that all was not\nwell. A fire broke out above the Merlin engine and suddenly this machine\nthat had been flying straight and true started to spin and then tumble\nuncontrollably back to Earth. The Falcon 1 ended up falling directly down\nonto the launch site. Most of the debris went into a reef 250 feet from the\nlaunchpad, and the satellite cargo smashed through SpaceX\u2019s machine shop\nroof and landed more or less intact on the floor. Some of the engineers put on their snorkeling and scuba gear and recovered the pieces, fitting all of\nthe rocket\u2019s remnants into two refrigerator-sized crates. \u201cIt is perhaps worth\nnoting that those launch companies that succeeded also took their lumps\nalong the way,\u201d Musk wrote in a postmortem. \u201cA friend of mine wrote to\nremind me that only 5 of the first 9 Pegasus launches succeeded; 3 of 5 for\nAriane; 9 of 20 for Atlas; 9 of 21 for Soyuz; and 9 of 18 for Proton. Having\nexperienced firsthand how hard it is to reach orbit, I have a lot of respect for\nthose that persevered to produce the vehicles that are mainstays of space\nlaunch today.\u201d Musk closed the letter writing, \u201cSpaceX is in this for the\nlong haul and, come hell or high water, we are going to make this work.\u201d\nMusk and other SpaceX executives blamed the crash on an unnamed\ntechnician. They said this technician had done some work on the rocket one\nday before the launch and failed to properly tighten a fitting on a fuel pipe,\nwhich caused the fitting to crack. The fitting in question was something\nbasic\u2014an aluminum b-nut that\u2019s often used to connect a pair of tubes. The\ntechnician was Hollman. In the aftermath of the rocket crash, Hollman flew\nto Los Angeles to confront Musk directly. He\u2019d spent years working day\nand night on the Falcon 1 and felt enraged that Musk had called out him and\nhis team in public. Hollman knew that he\u2019d fastened the b-nut correctly and\nthat observers from NASA had been looking over his shoulder to check the\nwork. When Hollman charged into SpaceX\u2019s headquarters with a head full\nof fury, Mary Beth Brown tried to calm him and stop him from seeing\nMusk. Hollman kept going anyway, and the two of them proceeded to have\na shouting match at Musk\u2019s cubicle.\nAfter all the debris was analyzed, it turned out that the b-nut had almost\ncertainly cracked due to corrosion from the months in Kwaj\u2019s salty\natmosphere. \u201cThe rocket was literally crusted with salt on one side, and you\nhad to scrape it off,\u201d Mueller said. \u201cBut we had done a static fire three days\nearlier, and everything was fine.\u201d SpaceX had tried to save about fifty\npounds of weight by using aluminum components instead of stainless steel.\nThompson, the former marine, had seen the aluminum parts work just fine\nin helicopters that sat on aircraft carriers, and Mueller had seen aircraft\nresting outside of Cape Canaveral for forty years with aluminum b-nuts in\nfine condition. Years later, a number of SpaceX\u2019s executives still agonize\nover the way Hollman and his team were treated. \u201cThey were our best guys,\nand they kind of got blamed to get an answer out to the world,\u201d Mueller\nsaid. \u201cThat was really bad. We found out later that it was dumb luck.\u201d* After the crash, there was a lot of drinking at a bar on the main island.\nMusk wanted to launch again within six months, but putting together a new\nmachine would again require an immense amount of work. SpaceX had\nsome pieces for the vehicle ready in El Segundo but certainly not a ready-\nto-fire rocket. As they downed drinks, the engineers vowed to take a more\ndisciplined approach with their next craft and to work better as a collective.\nWorden hoped the SpaceX engineers would raise their game as well. He\u2019d\nbeen observing them for the Defense Department and loved the energy of\nthe young engineers but not their methodology. \u201cIt was being done like a\nbunch of kids in Silicon Valley would do software,\u201d Worden said. \u201cThey\nwould stay up all night and try this and try that. I\u2019d seen hundreds of these\ntypes of operations, and it struck me that it wouldn\u2019t work.\u201d Leading up to\nthe first launch, Worden tried to caution Musk, sending a letter to him and\nthe director of DARPA, the research arm of the Defense Department, that\nmade his views clear. \u201cElon didn\u2019t react well. He said, \u2018What do you know?\nYou\u2019re just an astronomer,\u2019\u201d Worden said. But, after the rocket blew up,\nMusk recommended that Worden perform an investigation for the\ngovernment. \u201cI give Elon huge credit for that,\u201d Worden said.\nAlmost exactly a year later, SpaceX was ready to try another launch. On\nMarch 15, 2007, a successful test fire took place. Then, on March 21, the\nFalcon 1 finally behaved. From its launchpad surrounded by palm trees, the\nFalcon 1 surged up and toward space. It flew for a couple of minutes with\nengineers now and again reporting that the systems were \u201cnominal,\u201d or in\ngood shape. At three minutes into the flight, the first stage of the rocket\nseparated and fell back to Earth, and the Kestrel engine kicked in as planned\nto carry the second stage into orbit. Ecstatic cheers went out in the control\nroom. Next, at the four-minute mark, the fairing atop the rocket separated as\nplanned. \u201cIt was doing exactly what it was supposed to do,\u201d said Mueller. \u201cI\nwas sitting next to Elon and looked at him and said, \u2018We\u2019ve made it.\u2019 We\u2019re\nhugging and believe it\u2019s going to make it to orbit. Then, it starts to wiggle.\u201d\nFor more than five glorious minutes, the SpaceX engineers got to feel like\nthey had done everything right. A camera on board the Falcon 1 pointed\ndown and showed Earth getting smaller and smaller as the rocket made its\nway methodically into space. But then that wiggle that Mueller noticed\nturned into flailing, and the machine swooned, started to break apart, and\nthen blew up. This time the SpaceX engineers were quick to figure out what\nwent wrong. As the propellant was consumed, what was left started to move around the tank and slosh against the sides, much like wine spinning around\na glass. The sloshing propellant triggered the wobbling, and at one point it\nsloshed enough to leave an opening to the engine exposed. When the engine\nsucked in a big breath of air, it flamed out.\nThe failure was another crushing blow to SpaceX\u2019s engineers. Some of\nthem had spent close to two years shuffling back and forth between\nCalifornia, Hawaii, and Kwaj. By the time SpaceX could attempt another\nlaunch, it would be about four years after Musk\u2019s original target, and the\ncompany had been chewing through his Internet fortune at a worrying rate.\nMusk had vowed publicly that he would see this thing through to the end,\nbut people inside and outside the company were doing back-of-the-\nenvelope math and could tell that SpaceX likely could only afford one more\nattempt\u2014maybe two. To the extent that the financial situation unnerved\nMusk, he rarely if ever let it show to employees. \u201cElon did a great job of not\nburdening people with those worries,\u201d said Spikes. \u201cHe always\ncommunicated the importance of being lean and of success, but it was never\n\u2018if we fail, we\u2019re done for.\u2019 He was very optimistic.\u201d\nThe failures seemed to do little to curtail Musk\u2019s vision for the future or\nraise doubts about his capabilities. In the midst of the chaos, he took a tour\nof the islands with Worden. Musk began thinking aloud about how the\nislands could be unified into one landmass. He suggested that walls could\nbe built through the small channels between the islands, and the water could\nbe pumped out in the spirit of the manmade systems in the Netherlands.\nWorden, also known for his out-there ideas, was attracted to Musk\u2019s\nbravado. \u201cThat he is thinking of this stuff is kind of cool,\u201d Worden said.\n\u201cFrom that point on, he and I discussed settling Mars. It really impressed\nme that this is a guy that thinks big.\u201d PHOTOGRAPHIC INSERT\nThe Haldeman children had lots of downtime in the African bush while on wild adventures with their\nparents. \u00a9Maye Musk As a toddler, Musk would often drift off into his own world and ignore those around him. Doctors\ntheorized that he might be hard of hearing and had his adenoid glands removed. \u00a9Maye Musk\nMusk was a loner throughout grade school and suffered for years at the hands of bullies. \u00a9Maye\nMusk Musk\u2019s original video-game code for Blastar, the game he wrote as a twelve-year-old and published\nin a local magazine. \u00a9Maye Musk (From left to right:) Elon, Kimbal, and Tosca at their house in South Africa. All three children now\nlive in the United States. \u00a9Maye Musk Musk ran away on his own to Canada and ended up at Queen\u2019s University in Ontario, living in a\ndormitory for foreign students. \u00a9Maye Musk J. B. Straubel puts together one of Tesla Motors\u2019 early battery packs at his house. Photograph\ncourtesy of Tesla Motors A handful of engineers built the first Tesla Roadster in a Silicon Valley warehouse that they had\nturned into a garage workshop and research lab. Photograph courtesy of Tesla Motors\nMusk and Martin Eberhard prepare to take the early Roadster for a test-drive. The relationship\nbetween the two men would fall apart in the years to come. Photograph courtesy of Tesla Motors SpaceX built its rocket factory from the ground up in a Los Angeles warehouse to give birth to the\nFalcon 1 rocket. Photograph courtesy of SpaceX\nTom Mueller (far right, gray shirt) led the design, testing, and construction of SpaceX\u2019s engines.\nPhotograph courtesy of SpaceX SpaceX had to conduct its first flights from Kwajalein Atoll (or Kwaj) in the Marshall Islands. The\nisland experience was a difficult but ultimately fruitful adventure for the engineers. Photograph\ncourtesy of SpaceX\nSpaceX built a mobile mission-control trailer, and Musk and Mueller used it to monitor the later\nlaunches from Kwaj. Photograph courtesy of SpaceX Musk hired Franz von Holzhausen in 2008 to design the Tesla Model S. The two men speak almost\nevery day, as can be seen in this meeting in Musk\u2019s SpaceX cubicle. \u00a9Steve Jurvetson\nSpaceX\u2019s ambitions grew over the years to include the construction of the Dragon capsule, which\ncould take people to the International Space Station and beyond. \u00a9Steve Jurvetson Musk has long had a thing for robots and is always evaluating new machines for both the SpaceX and\nTesla factories. \u00a9Steve Jurvetson\nWhen SpaceX moved to a new factory in Hawthorne, California, it was able to scale out its assembly\nline and work on multiple rockets and capsules at the same time. \u00a9Steve Jurvetson SpaceX tests new engines and crafts at a site in McGregor, Texas. Here the company is testing a\nreusable rocket, code-named \u201cGrasshopper,\u201d that can land itself. Photograph courtesy of SpaceX\nMusk has a tradition of visiting Dairy Queen ahead of test flights in Texas, in this case with SpaceX\ninvestor and board member Steve Jurvetson (left) and fellow investor Randy Glein (right). \u00a9Steve\nJurvetson With a Dragon capsule hanging overhead, SpaceX employees peer into the company\u2019s mission\ncontrol center at the Hawthorne factory. Photograph courtesy of SpaceX\nGwynne Shotwell is Musk\u2019s right-hand woman at SpaceX and oversees the day-to-day operations of\nthe company, including monitoring a launch from mission control. Photograph courtesy of SpaceX Tesla took over the New United Motor Manufacturing Inc. (or NUMMI) car factory in Fremont,\nCalifornia, which is where workers produce the Model S sedan. Photograph courtesy of Tesla Motors Tesla began shipping the Model S sedan in 2012. The car ended up winning most of the automotive\nindustry\u2019s major awards. Photograph courtesy of Tesla Motors The Tesla Model S sedan with its electric motor (near the rear) and battery pack (bottom) exposed.\nPhotograph courtesy of Tesla Motors\nTesla\u2019s next car will be the Model X SUV with its signature \u201cfalcon-wing doors.\u201d Photograph\ncourtesy of Tesla Motors In 2013, Musk visited Cuba with Sean Penn (driving) and the investor Shervin Pishevar (back seat\nnext to Musk). They met with students and members of the Castro family, and tried to free an\nAmerican prisoner. \u00a9Shervin Pishevar Musk unveiled the Hyperloop in 2013. He proposed it as a new mode of transportation, and multiple\ngroups have now set to work on building it. Photograph courtesy of SpaceX\nIn 2014, Musk unveiled a radical new take on the space capsule\u2014the Dragon V2. It comes with a\ndrop-down touch-screen display and slick interior. Photograph courtesy of SpaceX The Dragon V2 will be able to return to Earth and land with pinpoint accuracy. Photograph courtesy\nof SpaceX Musk is a nonstop traveler. Here\u2019s a look at one year in his life via records obtained through a\nFreedom of Information Act request. Musk married, divorced, remarried, and then divorced the actress Talulah Riley. Photograph courtesy\nof Talulah Riley\nMusk and Riley relax at home in Los Angeles. Musk shares the home with his five young boys.\nPhotograph courtesy of Talulah Riley 7 ALL ELECTRIC\nJ\n. B. STRAUBEL HAS A TWO-INCH-LONG SCAR that cuts across the\nmiddle of his left cheek. He earned it in high school, during a chemistry\nclass experiment. Straubel whipped up the wrong concoction of chemicals,\nand the beaker he was holding exploded, throwing off shards of glass, one\nof which sliced through his face.\nThe wound lingers as a tinkerer\u2019s badge of honor. It arrived near the end\nof a childhood full of experimentation with chemicals and machines. Born\nin Wisconsin, Straubel constructed a large chemistry lab in the basement of\nhis family\u2019s home that included fume hoods and chemicals ordered,\nborrowed, or pilfered. At thirteen, Straubel found an old golf cart at the\ndump. He brought it back home and restored it to working condition, which\nrequired him to rebuild the electric motor. It seemed that Straubel was\nalways taking something apart, sprucing it up, and putting it back together.\nAll of this fit into the Straubel family\u2019s do-it-yourself traditions. In the late\n1890s Straubel\u2019s great-grandfather started the Straubel Machine Company,\nwhich built one of the first internal combustion engines in the United States\nand used it to power boats.\nStraubel\u2019s inquisitive spirit carried him west to Stanford University,\nwhere he enrolled in 1994 intending to become a physicist. After flying\nthrough the hardest courses he could take, Straubel concluded that majoring\nin physics would not be for him. The advanced courses were too theoretical,\nand Straubel liked to get his hands dirty. He developed his own major called\nenergy systems and engineering. \u201cI wanted to take software and electricity\nand use it to control energy,\u201d Straubel said. \u201cIt was computing combined\nwith power electronics. I collected all the things I love doing in one place.\u201d\nThere was no clean-technology movement at this time, but there were\ncompanies dabbling with new uses for solar power and electric vehicles.\nStraubel ended up hunting down these startups, hanging out in their garages\nand pestering the engineers. He began tinkering once again on his own as\nwell in the garage of a house he shared with a half dozen friends. Straubel bought a \u201cpiece of shit Porsche\u201d for $1,600 and turned it into an electric car.\nThis meant that Straubel had to create a controller to manage the electric\nmotor, build a charger from scratch, and write the software that made the\nentire machine work. The car set the world record for electric vehicle (EV)\nacceleration, traveling a quarter mile in 17.28 seconds. \u201cThe thing I took\naway was that the electronics were great, and you could get acceleration on\na shoestring budget, but the batteries sucked,\u201d Straubel said. \u201cIt had a thirty-\nmile range, so I learned firsthand about some of the limitations of electric\nvehicles.\u201d Straubel gave his car a hybrid boost, building a gasoline-powered\ncontraption that could be towed behind the Porsche and used to recharge the\nbatteries. It was good enough for Straubel to drive the four hundred miles\ndown to Los Angeles and back.\nBy 2002, Straubel was living in Los Angeles. He\u2019d gotten a master\u2019s\ndegree from Stanford and bounced around a couple of companies looking\nfor something that called out to him. He decided on Rosen Motors, which\nhad built one of the world\u2019s first hybrid vehicles\u2014a car that ran off a\nflywheel and a gas turbine and had electric motors to drive the wheel. After\nit folded, Straubel followed Harold Rosen, an engineer famed for inventing\nthe geostationary satellite, to create an electric plane. \u201cI\u2019m a pilot and love\nto fly, so this was perfect for me,\u201d Straubel said. \u201cThe idea was that it would\nstay aloft for two weeks at a time and hover over a specific spot. This was\nway before drones and all that.\u201d To help make ends meet, Straubel also\nworked nights and on the weekend doing electronics consulting for a start-\nup.\nIt was in the midst of toiling away on all these projects that Straubel\u2019s\nold buddies from the Stanford solar car team came to pay him a visit. A\ngroup of rogue engineers at Stanford had been working on solar cars for\nyears, building them in a World War II\u2013era Quonset hut full of toxic\nchemicals and black widows. Unlike today, when the university would jump\nat the chance to support such a project, Stanford tried to shut down this\ngroup of fringe freaks and geeks. The students proved very capable of doing\nthe work on their own and competed in cross-country solar-powered car\nraces. Straubel helped build the vehicles during his time at university and\neven after, forming relationships with the incoming crop of engineers. The\nteam had just raced 2,300 miles from Chicago to Los Angeles, and Straubel\noffered the strapped, exhausted kids a place to stay. About a half dozen\nstudents showed up at Straubel\u2019s place, took their first showers in many days, and then spread across his floor. As they chatted late into the night,\nStraubel and the solar team kept fixating on one topic. They realized that\nlithium ion batteries\u2014such as the ones in their car being fed by the sun\u2014\nhad gotten much better than most people realized. Many consumer\nelectronics devices like laptops were running on so-called 18650 lithium ion\nbatteries, which looked a lot like AA batteries and could be strung together.\n\u201cWe wondered what would happen if you put ten thousand of the battery\ncells together,\u201d Straubel said. \u201cWe did the math and figured you could go\nalmost one thousand miles. It was totally nerdy shit, and eventually\neveryone fell asleep, but the idea really stuck with me.\u201d\nSoon enough, Straubel was stalking the solar car crew, trying to talk\nthem into building an electric car based on the lithium ion batteries. He\nwould fly up to Palo Alto, spend the night sleeping in his plane, and then\nride a bicycle to the Stanford campus to make his sales pitch while helping\nwith their current projects. The design Straubel had come up with was a\nsuper-aerodynamic vehicle with 80 percent of its mass made up of the\nbatteries. It looked quite a bit like a torpedo on wheels. No one knew the\nexact details of Straubel\u2019s long-term vision for this thing, including\nStraubel. The plan seemed to be less about forming a car company than\nabout building a proof-of-concept vehicle just to get people thinking about\nthe power of the lithium ion batteries. With any luck, they would find a race\nto compete in.\nThe Stanford students agreed to join Straubel, if he could raise some\nmoney. He began going to trade shows handing out brochures about his idea\nand e-mailing just about anyone he could think of. \u201cI was shameless,\u201d he\nsaid. The only problem was that no one had any interest in what Straubel\nwas selling. Investors dealt him one rejection after another for months on\nend. Then, in the fall of 2003, Straubel met Elon Musk.\nHarold Rosen had set up a lunch with Musk at a seafood restaurant near\nthe SpaceX headquarters in Los Angeles and brought Straubel along to help\ntalk up the electric plane idea. When Musk didn\u2019t bite on that, Straubel\nannounced his electric car side project. The crazy idea struck an immediate\nchord with Musk, who had been thinking about electric vehicles for years.\nWhile Musk had mostly focused on using ultracapacitors for the vehicles,\nhe was thrilled and surprised to hear how far the lithium ion battery\ntechnology had progressed. \u201cEveryone else had told me I was nuts, but Elon\nloved the idea,\u201d Straubel said. \u201cHe said, \u2018Sure, I will give you some money.\u2019\u201d Musk promised Straubel $10,000 of the $100,000 he was seeking.\nOn the spot, Musk and Straubel formed a kinship that would survive more\nthan a decade of extreme highs and lows as they set out to do nothing less\nthan change the world.\nAfter the meeting with Musk, Straubel reached out to his friends at AC\nPropulsion. The Los Angeles\u2013based company started in 1992 and was the\nbleeding edge of electric vehicles, building everything from zippy midsize\npassenger jobs right on up to sports cars. Straubel really wanted to show\nMusk the tzero (from \u201ct-zero\u201d)\u2014the highest-end vehicle in AC Propulsion\u2019s\nstable. It was a type of kit car that had a fiberglass body sitting on top of a\nsteel frame and went from zero to 60 miles per hour in 4.9 seconds when\nfirst unveiled in 1997. Straubel had spent years hanging out with the AC\nPropulsion crew and asked Tom Gage, the company\u2019s president, to bring a\ntzero over for Musk to drive. Musk fell for the car. He saw its potential as a\nscreaming-fast machine that could shift the perception of electric cars from\nboring and plodding to something aspirational. For months Musk offered to\nfund an effort to transform the kit car into a commercial vehicle but got\nrebuffed time and again. \u201cIt was a proof of concept and needed to be made\nreal,\u201d Straubel said. \u201cI love the hell out of the AC Propulsion guys, but they\nwere sort of hopeless at business and refused to do it. They kept trying to\nsell Elon on this car called the eBox that looked like shit, didn\u2019t have good\nperformance, and was just uninspiring.\u201d While the meetings with AC\nPropulsion didn\u2019t result in a deal, they had solidified Musk\u2019s interest in\nbacking something well beyond Straubel\u2019s science project. In a late\nFebruary 2004 e-mail to Gage, Musk wrote, \u201cWhat I\u2019m going to do is figure\nout the best choice of a high performance base car and electric powertrain\nand go in that direction.\u201d\nUnbeknownst to Straubel, at about the same time, a couple of business\npartners in Northern California had also fallen in love with the idea of\nmaking a lithium ion battery powered car. Martin Eberhard and Marc\nTarpenning had founded NuvoMedia in 1997 to create one of the earliest\nelectronic book readers, called the Rocket eBook. The work at NuvoMedia\nhad given the men insight into cutting-edge consumer electronics and the\nhugely improved lithium ion batteries used to power laptops and other\nportable devices. While the Rocket eBook was too far ahead of its time and\nnot a major commercial success, it was innovative enough to attract the\nattention of Gemstar International Group, which owned TV Guide and some electronic programming guide technology. Gemstar paid $187 million to\nacquire NuvoMedia in March 2000. Spoils in hand, the cofounders stayed\nin touch after the deal. They both lived in Woodside, one of the wealthiest\ntowns in Silicon Valley, and chatted from time to time about what they\nshould tackle next. \u201cWe thought up some goofball things,\u201d said Tarpenning.\n\u201cThere was one plan for these fancy irrigation systems for farms and the\nhome based on smart water-sensing networks. But nothing really resonated,\nand we wanted something more important.\u201d\nEberhard was a supremely talented engineer with a do-gooder\u2019s social\nconscience. The United States\u2019 repeated conflicts in the Middle East\nbothered him, and like many other science-minded folks around 2000 he\nhad started to accept global warming as a reality. Eberhard began looking\nfor alternatives to gas-guzzling cars. He investigated the potential of\nhydrogen fuel cells but found them lacking. He also didn\u2019t see much point\nin leasing something like the EV1 electric car from General Motors. What\ndid catch Eberhard\u2019s interest, however, were the all-electric cars from AC\nPropulsion that he spied on the Internet. Eberhard went down to Los\nAngeles around 2001 to visit the AC Propulsion shop. \u201cThe place looked\nlike a ghost town and like they were going out of business,\u201d Eberhard said.\n\u201cI bailed them out with five hundred thousand dollars so that they could\nbuild one of their cars for me with lithium ion instead of lead acid\nbatteries.\u201d Eberhard too tried to goad AC Propulsion into being a\ncommercial enterprise rather than a hobby shop. When they rejected his\novertures, Eberhard decided to form his own company and see what the\nlithium ion batteries could really do.\nEberhard\u2019s journey began with him building a technical model of the\nelectric car on a spreadsheet. This let him tweak various components and\nsee how they might affect the vehicle\u2019s shape and performance. He could\nadjust the weight, number of batteries, resistance of the tires and body, and\nthen get back answers on how many batteries it would take to power the\nvarious designs. The models made it clear that SUVs, which were very\npopular at the time, and things like delivery trucks were unlikely\ncandidates. The technology seemed instead to favor a lighter-weight, high-\nend sports car, which would be fast, fun to drive, and have far better range\nthan most people would expect. These technical specifications\ncomplemented the findings of Tarpenning, who had been doing research\ninto a financial model for the car. The Toyota Prius had started to take off in California, and it was being purchased by wealthy eco-crusaders. \u201cWe also\nlearned that the average income for EV1 owners was around two hundred\nthousand dollars per year,\u201d Tarpenning said. People who used to go after the\nLexus, BMW, and Cadillac brands saw electric and hybrid cars as a\ndifferent kind of status symbol. The men figured they could build\nsomething for the $3 billion per year luxury auto market in the United\nStates that would let rich people have fun and feel good about themselves\ntoo. \u201cPeople pay for cool and sexy and an amazing zero-to-sixty time,\u201d\nTarpenning said.\nOn July 1, 2003, Eberhard and Tarpenning incorporated their new\ncompany. While at Disneyland a few months earlier on a date with his wife,\nEberhard had come up with the name Tesla Motors, both to pay homage to\nthe inventor and electric motor pioneer Nikola Tesla and because it sounded\ncool. The cofounders rented an office that had three desks and two small\nrooms in a decrepit 1960s building located at 845 Oak Grove Avenue in\nMenlo Park. The third desk was occupied a few months later by Ian Wright,\nan engineer who grew up on a farm in New Zealand. He was a neighbor of\nthe Tesla cofounders in Woodside, and had been working with them to hone\nhis pitch for a networking startup. When the start-up failed to raise any\nmoney from venture capitalists, Wright joined Tesla. As the three men\nbegan to tell some of their confidants of their plans, they were confronted\nwith universal derision. \u201cWe met a friend at this Woodside pub to tell her\nwhat we had finally decided to do and that it was going to be an electric\ncar,\u201d Tarpenning said. \u201cShe said, \u2018You have to be kidding me.\u2019\u201d\nAnyone who tries to build a car company in the United States is quickly\nreminded that the last successful start-up in the industry was Chrysler,\nfounded in 1925. Designing and building a car from the ground up comes\nwith plenty of challenges, but it\u2019s really getting the money and know-how\nto build lots of cars that has thwarted past efforts to get a new company\ngoing. The Tesla founders were aware of these realities. They figured that\nNikola Tesla had built an electric motor a century earlier and that creating a\ndrivetrain to take the power from the motor and send it to the wheels was\ndoable. The really frightening part of their enterprise would be building the\nfactory to make the car and its associated parts. But the more the Tesla guys\nresearched the industry, the more they realized that the big automakers\ndon\u2019t even really build their cars anymore. The days of Henry Ford having\nraw materials delivered to one end of his Michigan factory and then sending cars out the other end had long passed. \u201cBMW didn\u2019t make its windshields\nor upholstery or rearview mirrors,\u201d Tarpenning said. \u201cThe only thing the big\ncar companies had kept was internal combustion research, sales and\nmarketing, and the final assembly. We thought na\u00efvely that we could access\nall the same suppliers for our parts.\u201d\nThe plan the Tesla cofounders came up with was to license some\ntechnology from AC Propulsion around the tzero vehicle and to use the\nLotus Elise chassis for the body of their car. Lotus, the English carmaker,\nhad released the two-door Elise in 1996, and it certainly had the sleek,\nground-hugging appeal to make a statement to high-end car buyers. After\ntalking to a number of people in the car dealership business, the Tesla team\ndecided to avoid selling their cars through partners and sell direct. With\nthese basics of a plan in place, the three men went hunting for some venture\ncapital funding in January 2004.\nTo make things feel more real for the investors, the Tesla founders\nborrowed a tzero from AC Propulsion and drove it to the venture capital\ncorridor of Sand Hill Road. The car accelerated faster than a Ferrari, and\nthis translated into visceral excitement for the investors. The downside,\nthough, was that venture capitalists are not a terribly imaginative bunch,\nand they struggled to see past the crappy plastic finish of this glorified kit\ncar. The only venture capitalists that bit were Compass Technology Partners\nand SDL Ventures, and they didn\u2019t sound altogether thrilled. The lead\npartner at Compass had made out well on NuvoMedia and felt some loyalty\nto Eberhard and Tarpenning. \u201cHe said, \u2018This is stupid, but I have invested in\nevery automotive start-up for the last forty years, so why not,\u2019\u201d Tarpenning\nrecalled. Tesla still needed a lead investor who would pony up the bulk of\nthe $7 million needed to make what\u2019s known as a mule or a prototype\nvehicle. That would be their first milestone and give them something\nphysical to show off, which could aid a second round of funding.\nEberhard and Tarpenning had Elon Musk\u2019s name in the back of their\nheads as a possible lead investor from the outset. They had both seen him\nspeak a couple of years earlier at a Mars Society conference held at\nStanford where Musk had laid out his vision of sending mice into space,\nand they got the impression that he thought a bit differently and would be\nopen to the idea of an electric car. The idea to pitch Musk on Tesla Motors\nsolidified when Tom Gage from AC Propulsion called Eberhard and told\nhim that Musk was looking to fund something in the electric car arena. Eberhard and Wright flew down to Los Angeles and met with Musk on a\nFriday. That weekend, Musk peppered Tarpenning, who had been away on a\ntrip, with questions about the financial model. \u201cI just remember responding,\nresponding, and responding,\u201d Tarpenning said. \u201cThe following Monday,\nMartin and I flew down to meet him again, and he said, \u2018Okay, I\u2019m in.\u2019\u201d\nThe Tesla founders felt like they had lucked into the perfect investor.\nMusk had the engineering smarts to know what they were building. He also\nshared their larger goal of trying to end the United States\u2019 addiction to oil.\n\u201cYou need angel investors to have some belief, and it wasn\u2019t a purely\nfinancial transaction for him,\u201d Tarpenning said. \u201cHe wanted to change the\nenergy equation of the country.\u201d With an investment of $6.5 million, Musk\nhad become the largest shareholder of Tesla and the chairman of the\ncompany. Musk would later wield his position of strength well while\nbattling Eberhard for control of Tesla. \u201cIt was a mistake,\u201d Eberhard said. \u201cI\nwanted more investors. But, if I had to do it again, I would take his money.\nA bird in the hand, you know. We needed it.\u201d\nNot long after this meeting took place, Musk called Straubel and urged\nhim to meet with the Tesla team. Straubel heard that their offices in Menlo\nPark were about a half a mile from his house, and he was intrigued but very\nskeptical of their story. No one on the planet was more dialed into the\nelectric vehicle scene than Straubel, and he found it hard to believe that a\ncouple of guys had gotten this far along without word of their project\nreaching him. Nonetheless, Straubel stopped by the office for a meeting,\nand was hired right away in May 2004 at a salary of $95,000 per year. \u201cI\ntold them that I had been building the battery pack they need down the\nstreet with funding from Elon,\u201d Straubel said. \u201cWe agreed to join forces and\nformed this ragtag group.\u201d\nHad anyone from Detroit stopped by Tesla Motors at this point, they\nwould have ended up in hysterics. The sum total of the company\u2019s\nautomotive expertise was that a couple of the guys at Tesla really liked cars\nand another one had created a series of science fair projects based on\ntechnology that the automotive industry considered ridiculous. What\u2019s\nmore, the founding team had no intention of turning to Detroit for advice on\nhow to build a car company. No, Tesla would do what every other Silicon\nValley start-up had done before it, which was hire a bunch of young, hungry\nengineers and figure things out as they went along. Never mind that the Bay\nArea had no real history of this model ever having worked for something like a car and that building a complex, physical object had little in common\nwith writing a software application. What Tesla did have, ahead of anyone\nelse, was the realization that 18650 lithium ion batteries had gotten really\ngood and were going to keep getting better. Hopefully that coupled with\nsome effort and smarts would be enough.\nStraubel had a direct pipeline into the smart, energetic engineers at\nStanford and told them about Tesla. Gene Berdichevsky, one of the\nmembers of the solar-powered-car team, lit up the second he heard from\nStraubel. An undergraduate, Berdichevsky volunteered to quit school, work\nfor free, and sweep the floors at Tesla if that\u2019s what it took to get a job. The\nfounders were impressed with his spirit and hired Berdichevsky after one\nmeeting. This left Berdichevsky in the uncomfortable position of calling his\nRussian immigrant parents, a pair of nuclear submarine engineers, to tell\nthem that he was giving up on Stanford to join an electric car start-up. As\nemployee No. 7, he spent part of the workday in the Menlo Park office and\nthe rest in Straubel\u2019s living room designing three-dimensional models of the\ncar\u2019s powertrain on a computer and building battery pack prototypes in the\ngarage. \u201cOnly now do I realize how insane it was,\u201d Berdichevsky said.\nTesla soon needed to expand to accommodate its budding engineer army\nand to create a workshop that would help bring the Roadster, as they were\nnow calling the car, to life. They found a two-story industrial building in\nSan Carlos at 1050 Commercial Street. The 10,000-square-foot facility\nwasn\u2019t much, but it had room to build a research and development shop\ncapable of knocking out some prototype cars. There were a couple of large\nassembly bays on the ride side of the building and two large rollup doors\nbig enough for cars to drive in and out. Wright divided the open floor space\ninto segments\u2014motors, batteries, power electronics, and final assembly.\nThe left half of the building was an office space that had been modified in\nweird ways by the previous tenant, a plumbing supply company. The main\nconference room had a wet bar and a sink where the faucet was a swan\u2019s\nmouth, and the hot and cold knobs were wings. Berdichevsky painted the\noffice white on a Sunday night, and the next week the employees made a\nfield trip to IKEA to buy desks and hopped online to order their computers\nfrom Dell. As for tools, Tesla had a single Craftsman toolbox loaded with\nhammers, nails, and other carpentry basics. Musk would visit now and\nagain from Los Angeles and was unfazed by the conditions, having seen\nSpaceX grow up in similar surroundings. The original plan for producing a prototype vehicle sounded simple.\nTesla would take the AC Propulsion tzero powertrain and fit it into the\nLotus Elise body. The company had acquired a schematic for an electric\nmotor design and figured it could buy a transmission from a company in the\nUnited States or Europe and outsource any other parts from Asia. Tesla\u2019s\nengineers mostly needed to focus on developing the battery pack systems,\nwiring the car, and cutting and welding metal as needed to bring everything\ntogether. Engineers love to muck around with hardware, and the Tesla team\nthought of the Roadster as something akin to a car conversion project that\ncould be done with two or three mechanical engineers, and a few assembly\npeople.\nThe main team of prototype builders consisted of Straubel,\nBerdichevsky, and David Lyons, a very clever mechanical engineer and\nemployee No. 12. Lyons had about a decade of experience working for\nSilicon Valley companies and had met Straubel a few years before when the\ntwo men struck up a conversation at a 7-Eleven about an electric bike\nStraubel was riding. Lyons had helped Straubel pay bills by hiring him as a\nconsultant for a company building a device to measure people\u2019s core body\ntemperature. Straubel thought he could return the favor by bringing Lyons\non early to such an exciting project. Tesla would benefit in a big way as\nwell. As Berdichevsky put it, \u201cDave Lyons knew how to get shit done.\u201d\nThe engineers bought a blue lift for the car and set it up inside the\nbuilding. They also purchased some machine tools, hand tools, and\nfloodlights to work at night and started to turn the facility into a hotbed of\nR&D activity. Electrical engineers studied the Lotus\u2019s base-level software\nto figure out how it tied together the pedals, mechanical apparatus, and the\ndashboard gauges. The really advanced work took place with the battery\npack design. No one had ever tried to combine hundreds of lithium ion\nbatteries in parallel, so Tesla ended up at the cutting edge of the technology.\nThe engineers started trying to understand how heat would dissipate and\ncurrent flow would behave across seventy batteries by supergluing them\ntogether into groups called bricks. Then ten bricks would be placed\ntogether, and the engineers would test various types of air and liquid\ncooling mechanisms. When the Tesla team had developed a workable\nbattery pack, they stretched the yellow Lotus Elise chassis five inches and\nlowered the pack with a crane into the back of the car, where its engine\nwould normally be. These efforts began in earnest on October 18, 2004, and, rather remarkably, four months later, on January 27, 2005, an entirely\nnew kind of car had been built by eighteen people. It could even be driven\naround. Tesla had a board meeting that day, and Musk zipped about in the\ncar. He came away happy enough to keep investing. Musk put in $9 million\nmore as Tesla raised a $13 million funding round. The company now\nplanned to deliver the Roadster to consumers in early 2006.\nOnce they\u2019d finished building a second car a few months later, the\nengineers at Tesla decided they needed to face up to a massive potential\nflaw in their electric vehicle. On July 4, 2005, they were at Eberhard\u2019s\nhouse in Woodside celebrating Independence Day and figured it was as\ngood a moment as any to see what happened when the Roadster\u2019s batteries\ncaught on fire. Someone taped twenty of the batteries together, put a heating\nstrip wire into the bundle, and set it off. \u201cIt went up like a cluster of bottle\nrockets,\u201d Lyons said. Instead of twenty batteries, the Roadster would have\nclose to 7,000, and the thought of what an explosion at that scale would be\nlike horrified the engineers. One of the perks of an electric car was meant to\nbe that it moved people away from a flammable liquid like gasoline and the\nendless explosions that take place in an engine. Rich people were unlikely\nto pay a high price for something even more dangerous, and the early\nnightmare scenario for the employees at Tesla was that a rich, famous\nperson would get caught in a fire caused by the car. \u201cIt was one of those \u2018oh\nshit\u2019 moments,\u201d Lyons said. \u201cThat is when we really sobered up.\u201d\nTesla formed a six-person task force to deal with the battery issue. They\nwere pulled off all other work and given money to begin running\nexperiments. The first explosions started taking place at the Tesla\nheadquarters, where the engineers filmed them in slow motion. Once saner\nminds prevailed, Tesla moved its explosion research to a blast area behind\nan electrical substation maintained by the fire department. Blast by blast,\nthe engineers learned a great deal about the inner workings of the batteries.\nThey developed methods for arranging them in ways that would prevent\nfires spreading from one battery to the next and other techniques for\nstopping explosions altogether. Thousands of batteries exploded along the\nway, and the effort was worth it. It was still early days, for sure, but Tesla\nwas on the verge of inventing battery technology that would set it apart\nfrom rivals for years to come and would become one of the company\u2019s great\nadvantages. The early success at building two prototype cars, coupled with Tesla\u2019s\nengineering breakthroughs around the batteries and other technological\npieces, boosted the company\u2019s confidence. It was time to put Tesla\u2019s stamp\non the vehicle. \u201cThe original plan had been to do the bare minimum we\ncould get away with as far as making the car stylistically different from a\nLotus but electric,\u201d said Tarpenning. \u201cAlong the way, Elon and the rest of\nthe board said, \u2018You only get to do this once. It has to delight the customer,\nand the Lotus just isn\u2019t good enough to do that.\u2019\u201d\nThe Elise\u2019s chassis, or base frame, worked fine for Tesla\u2019s engineering\npurposes. But the body of the car had serious issues in both form and\nfunction. The door on the Elise was all of a foot tall, and you were meant to\neither jump into the car or fall into it, depending on your flexibility and/or\ndignity. The body also needed to be longer to accommodate Tesla\u2019s battery\npack and a trunk. And Tesla preferred to make the Roadster out of carbon\nfiber instead of fiberglass. On these design points, Musk had a lot of\nopinion and influence. He wanted a car that Justine could feel comfortable\ngetting into and that had some measure of practicality. Musk made these\nopinions clear when he visited Tesla for board meetings and design reviews.\nTesla hired a handful of designers to mock up new looks for the\nRoadster. After settling on a favorite, the company paid to build a quarter-\nscale model of the vehicle in January 2005 and then a full-scale model in\nApril. This process provided the Tesla executives with yet another\nrevelation of everything that went into making a car. \u201cThey wrap this shiny\nMylar material around the model and vacuum it, so that you can really see\nthe contours and shine and shadows,\u201d Tarpenning said. The silver model\nwas then turned into a digital rendering that the engineers could manipulate\non their computers. A British company took the digital file and used it to\ncreate a plastic version of the car called an \u201caero buck\u201d for aerodynamics\ntesting. \u201cThey put it on a boat and shipped it to us, and then we took it to\nBurning Man,\u201d Tarpenning said, referring to the annual drug-infused art\nfestival held in the Nevada desert.\nAbout a year later, after many tweaks and much work, Tesla had a\npencils-down moment. It was May 2006, and the company had grown to a\nhundred employees. This team built a black version of the Roadster known\nas EP1, or engineering prototype one. \u201cIt was saying, \u2018We now think we\nknow what we will build,\u2019\u201d Tarpenning said. \u201cYou can feel it. It\u2019s a real car,\nand it\u2019s very exciting.\u201d The arrival of the EP1 provided a great excuse to show existing investors what their money had bought and to ask for more\nfunds from a wider audience. The venture capitalists were impressed\nenough to overlook the fact that engineers sometimes had to manually fan\nthe car to cool it down in between test drives and were now starting to grasp\nTesla\u2019s long-term potential. Musk once again put money into Tesla\u2014$12\nmillion\u2014and a handful of other investors, including the venture capital firm\nDraper Fisher Jurvetson, VantagePoint Capital Partners, J.P. Morgan,\nCompass Technology Partners, Nick Pritzker, Larry Page, and Sergey Brin,\njoined the $40 million round.*\nIn July 2006, Tesla decided to tell the world what it had been up to. The\ncompany\u2019s engineers had built a red prototype\u2014EP2\u2014to complement the\nblack one, and they both went on display at an event in Santa Clara. The\npress flocked to the announcement and were quite taken with what they\nsaw. The Roadsters were gorgeous, two-seater convertibles that could go\nfrom zero to 60 in about four seconds. \u201cUntil today,\u201d Musk said at the\nevent, \u201call electric cars have sucked.\u201d6\nCelebrities like then-governor Arnold Schwarzenegger and former\nDisney CEO Michael Eisner showed up at the event, and many of them\ntook test rides in the Roadsters. The vehicles were so fragile that only\nStraubel and a couple of other trusted hands knew how to run them, and\nthey were swapped out every five minutes to avoid overheating. Tesla\nrevealed that each car would cost about $90,000 and had a range of 250\nmiles per charge. Thirty people, the company said, had committed to buying\na Roadster, including the Google cofounders Brin and Page and a handful of\nother technology billionaires. Musk promised that a cheaper car\u2014a four-\nseat, four-door model under $50,000, would arrive in about three years.\nAround the time of this event, Tesla made its debut in the New York\nTimes via a mini-profile on the company. Eberhard vowed\u2014optimistically\n\u2014to begin shipments of the Roadster in the middle of 2007, instead of early\n2006 as once planned, and laid out Tesla\u2019s strategy of starting with a high-\npriced, low-volume product and moving down to more affordable products\nover time, as underlying technology and manufacturing capabilities\nadvanced. Musk and Eberhard were big believers in this strategy, having\nseen it play out with a number of electronic devices. \u201cCellphones,\nrefrigerators, color TV\u2019s, they didn\u2019t start off by making a low-end product\nfor masses,\u201d Eberhard told the paper.7 \u201cThey were relatively expensive, for\npeople who could afford it.\u201d While the story was a coup for Tesla, Musk didn\u2019t appreciate being left out of the article entirely. \u201cWe tried to\nemphasize him, and told the reporter about him over and over again, but\nthey weren\u2019t interested in the board of the company,\u201d Tarpenning said.\n\u201cElon was furious. He was livid.\u201d\nYou could understand why Musk might want some of the shine of Tesla\nto rub off on him. The car had turned into a cause c\u00e9l\u00e8bre of the automotive\nworld. Electric vehicles tended to invoke religious overreactions from both\nthe pro and con camps, and the appearance of a good-looking, fast electric\ncar stoked everyone\u2019s passions. Tesla had also turned Silicon Valley into a\nreal threat, at least conceptually, to Detroit for the first time. The month\nafter the Santa Monica event was the Pebble Beach Concours d\u2019Elegance, a\nfamous showcase for exotic cars. Tesla had become such a topic of\nconversation that the organizers of the event begged to have a Roadster and\nwaived the usual display fees. Tesla set up a booth, and people showed up\nby the dozens writing $100,000 checks on the spot to pre-order their cars.\n\u201cThis was long before Kickstarter, and we just had not thought of trying to\ndo that,\u201d Tarpenning said. \u201cBut then we started getting millions of dollars at\nthese types of events.\u201d Venture capitalists, celebrities, and friends of Tesla\nemployees began trying to buy their way onto the waiting list. Some of\nSilicon Valley\u2019s wealthy elite went so far as to show up at the Tesla office\nand knock on the door, looking to buy a car. The entrepreneurs Konstantin\nOthmer and Bruce Leak, who had known Musk from his internship days at\nRocket Science Games, did just that one weekday and ended up getting a\npersonal tour of the car from Musk and Eberhard that stretched over a\ncouple of hours. \u201cAt the end we said, \u2018We\u2019ll take one,\u2019\u201d Othmer said. \u201cThey\nweren\u2019t actually allowed to sell cars yet, though, so we joined their club. It\ncost one hundred thousand dollars, but one of the benefits of membership\nwas that you\u2019d get a free car.\u201d\nAs Tesla switched from marketing back into R&D mode, it had some\ntrends working in its favor. Advances in computing had made it so that\nsmall car companies could sometimes punch at the same weight as the\ngiants of the industry. Years ago, automakers would have needed to make a\nfleet of cars for crash testing. Tesla could not afford to do that, and it didn\u2019t\nhave to. The third Roadster engineering prototype went to the same\ncollision testing facility used by large automakers, giving Tesla access to\ntop-of-the-line high-speed cameras and other imaging technology.\nThousands of other tests, though, were done by a third party that specialized in computer simulations and saved Tesla from building a fleet of crash\nvehicles. Tesla also had equal access to the big guys\u2019 durability tracks made\nout of cobblestones and concrete embedded with metal objects. It could\nreplicate 100,000 miles and ten years of wear at these facilities.\nQuite often, the Tesla engineers brought their Silicon Valley attitude to\nthe automakers\u2019 traditional stomping grounds. There\u2019s a break and traction\ntesting track in northern Sweden near the Arctic Circle where cars get tuned\non large plains of ice. It would be standard to run the car for three days or\nso, get the data, and return to company headquarters for many weeks of\nmeetings about how to adjust the car. The whole process of tuning a car can\ntake the entire winter. Tesla, by contrast, sent its engineers along with the\nRoadsters being tested and had them analyze the data on the spot. When\nsomething needed to be tweaked, the engineers would rewrite some code\nand send the car back on the ice. \u201cBMW would need to have a confab\nbetween three or four companies that would all blame each other for the\nproblem,\u201d Tarpenning said. \u201cWe just fixed it ourselves.\u201d Another testing\nprocedure required that the Roadsters go into a special cooling chamber to\ncheck how they would respond to frigid temperatures. Not wanting to pay\nthe exorbitant costs to use one of these chambers, the Tesla engineers opted\nto rent an ice cream delivery truck with a large refrigerated trailer. Someone\nwould drive a Roadster into the truck, and the engineers would don parkas\nand work on the car.\nEvery time Tesla interacted with Detroit it received a reminder of how\nthe once-great city had been separated from its own can-do culture. Tesla\ntried to lease a small office in Detroit. The costs were incredibly low\ncompared with space in Silicon Valley, but the city\u2019s bureaucracy made\ngetting just a basic office an ordeal. The building\u2019s owner wanted to see\nseven years of audited financials from Tesla, which was still a private\ncompany. Then the building owner wanted two years\u2019 worth of advanced\nrent. Tesla had about $50 million in the bank and could have bought the\nbuilding outright. \u201cIn Silicon Valley, you say you\u2019re backed by a venture\ncapitalist, and that\u2019s the end of the negotiation,\u201d Tarpenning said. \u201cBut\neverything was like that in Detroit. We\u2019d get FedEx boxes, and they\ncouldn\u2019t even decide who should sign for the package.\u201d\nThroughout these early years, the engineers credited Eberhard with\nmaking quick, crisp decisions. Rarely did Tesla get hung up overanalyzing a\nsituation. The company would pick a plan of attack, and when it failed at something, it failed fast and then tried a new approach. It was many of the\nchanges that Musk wanted that started to delay the Roadster. Musk kept\npushing for the car to be more comfortable, asking for alterations to the\nseats and the doors. He made the carbon-fiber body a priority, and he\npushed for electronic sensors on the doors so that the Roadster could be\nunlocked with the touch of a finger instead of a tug on a handle. Eberhard\ngroused that these features were slowing the company down, and many of\nthe engineers agreed. \u201cIt felt at times like Elon was this unreasonably\ndemanding overarching force,\u201d said Berdichevsky. \u201cThe company as a\nwhole was sympathetic to Martin because he was there all the time, and we\nall felt the car should ship sooner.\u201d\nBy the middle of 2007, Tesla had grown to 260 employees and seemed\nto be pulling off the impossible. It had produced the fastest, most beautiful\nelectric car the world had ever seen almost from thin air. All it had to do\nnext was build a lot of the cars\u2014a process that would end up almost\nbankrupting the company.\nThe greatest mistake Tesla\u2019s executives made in the early days were\nassumptions around the transmission system for the Roadster. The goal had\nalways been to get from zero to 60 mph as quickly as possible in the hopes\nthat the raw speed of the Roadster would attract a lot of attention and make\nit fun to drive. To do this, Tesla\u2019s engineers had decided on a two-speed\ntransmission, which is the underlying mechanism in the car for transferring\npower from the motor to the wheels. The first gear would take the car from\nzero to 60 mph in less than four seconds, and then the second gear would\ntake the car up to 130 mph. Tesla had hired Xtrac, a British company\nspecializing in transmission designs, to build this part and had every reason\nto believe that this would be one of the smoother bits of the Roadster\u2019s\njourney. \u201cPeople had been making transmissions since Robert Fulton built\nthe steam engine,\u201d said Bill Currie,8 a veteran Silicon Valley engineer and\nemployee No. 86 at Tesla. \u201cWe thought you would just order one. But the\nfirst one we had lasted forty seconds.\u201d The initial transmission could not\nhandle the big jump from the first to the second gear, and the fear was that\nthe second gear would engage at high speed and not be synchronized with\nthe motor properly, which would result in catastrophic damage to the car.\nLyons and the other engineers quickly set out to try to fix the issue.\nThey found a couple of other contractors to design replacements and again\nhoped that these longtime transmission experts would deliver something usable with relative ease. It soon became apparent, however, that the\ncontractors were not always putting their A team to work on this project for\na tiny start-up in Silicon Valley and that the new transmissions were no\nbetter than the first. During tests, Tesla found that the transmissions would\nsometimes break after 150 miles and that the mean time between failures\nwas about 2,000 miles. When a team from Detroit ran a root cause analysis\nof the transmission to find failures, they discovered fourteen separate issues\nthat could cause the system to break. Tesla had wanted to deliver the\nRoadster in November 2007, but the transmission issues lingered, and by\nthe time January 1, 2008, rolled around, the company had to once again\nstart from scratch, on a third transmission push.\nTesla also faced issues abroad. The company had decided to send a team\nof its youngest, most energetic engineers to Thailand to set up a battery\nfactory. Tesla partnered with an enthusiastic although not totally capable\nmanufacturing partner. The Tesla engineers had been told that they could fly\nover and manage the construction of a state-of-the-art battery factory.\nInstead of a factory, they found a concrete slab with posts holding up a roof.\nThe building was about a three-hour drive south from Bangkok, and had\nbeen left mostly open like many of the other factories because of the\nincredible heat. The other manufacturing operations dealt with making\nstoves, tires, and commodities that could withstand the elements. Tesla had\nsensitive batteries and electronics, and like parts of the Falcon 1, they\u2019d be\nchewed up by the salty, humid conditions. Eventually, Tesla\u2019s partner paid\nabout $75,000 to put in drywall, coat the floor, and create storage rooms\nwith temperature controls. Tesla\u2019s engineers ended up working maddening\nhours trying to train the Thai workers on how to handle the electronics\nproperly. The development of the battery technology, which had once\nmoved along at a rapid pace, slowed to a crawl.\nThe battery factory was one part of a supply chain that stretched across\nthe globe, adding cost and delays to the Roadster production. Body panels\nfor the car were to be made in France, while the motors were to come from\nTaiwan. Tesla planned on buying battery cells in China and shipping them\nto Thailand to turn the piece parts into battery packs. The battery packs,\nwhich had to be stored for a minimal amount of time to avoid degradation,\nwould then be taken to port and shipped to England, where they needed to\nclear customs. Tesla then planned for Lotus to build the body of the car,\nattach the battery packs, and ship the Roadsters by boat around Cape Horn to Los Angeles. In that scenario, Tesla would have paid for the bulk of the\ncar and had no chance to recognize revenue on the parts until six to nine\nmonths had passed. \u201cThe idea was to get to Asia, get things done fast and\ncheap, and make money on the car,\u201d said Forrest North, one of the\nengineers sent to Thailand. \u201cWhat we found out was that for really\ncomplicated things, you can do the work cheaper here and have less delays\nand less problems.\u201d When some new hires came on, they were horrified to\ndiscover just how haphazard Tesla\u2019s plan appeared. Ryan Popple, who had\nspent four years in the army and then gotten an MBA from Harvard, arrived\nat Tesla as a director of finance meant to prep the company to go public.\nAfter examining the company\u2019s books early in his tenure, Popple asked the\nmanufacturing and operations head exactly how he would get the car made.\n\u201cHe said, \u2018Well, we will decide we\u2019re going into production and then a\nmiracle is going to happen,\u2019\u201d Popple said.\nAs word of the manufacturing issues reached Musk, he became very\nconcerned about the way Eberhard had run the company and called in a\nfixer to address the situation. One of Tesla\u2019s investors was Valor Equity, a\nChicago-based investment firm that specialized in fine-tuning\nmanufacturing operations. The company had been drawn to Tesla\u2019s battery\nand powertrain technology and calculated that even if Tesla failed to sell\nmany cars, the big automakers would end up wanting to buy its intellectual\nproperty. To protect its investment, Valor sent in Tim Watkins, its managing\ndirector of operations, and he soon reached some horrific conclusions.\nWatkins is a Brit with degrees in industrial robotics and electrical\nengineering. He\u2019s built up a reputation as an ingenious solver of problems.\nWhile doing work in Switzerland, for example, Watkins found a way to get\naround the country\u2019s rigid labor laws that limit the hours employees can\nwork, by automating a metal stamping factory so that it could run twenty-\nfour hours per day instead of sixteen hours like the factories or rivals.\nWatkins is also known for keeping his ponytail in place with a black\nscrunchie, wearing a black leather jacket, and toting a black fanny pack\neverywhere he goes. The fanny pack has his passport, checkbook, earplugs,\nsunscreen, food, and an assortment of other necessities. \u201cIt\u2019s full of the\neveryday things I need to survive,\u201d said Watkins. \u201cIf I walk ten feet away\nfrom this thing, I sense it.\u201d While a bit eccentric, Watkins was thorough and\nspent weeks talking to employees and analyzing every part of Tesla\u2019s\nsupply chain to figure out how much it cost to make the Roadster. Tesla had done a decent job of keeping its employee costs down. It hired\nthe kid fresh out of Stanford for $45,000 rather than the proven guy who\nprobably didn\u2019t want to work that hard anyway for $120,000. But when it\ncame to equipment and materials, Tesla was a spending horror show. No\none liked using the company\u2019s software that tracked the bill of materials. So\nsome people used it, and some people didn\u2019t. Those that did use it often\nmade huge errors. They would take the cost of a part from the prototype\ncars and then estimate how much of a discount they expected when buying\nthat part in bulk, rather than actually negotiating to find a viable price. At\none point, the software declared that each Roadster should cost about\n$68,000, which would leave Tesla making about $30,000 per vehicle.\nEveryone knew the figure was wrong, but it got reported to the board\nanyway.\nAround the middle of 2007, Watkins came to Musk with his findings.\nMusk was prepared for a high figure but felt confident that the price of the\ncar would come down significantly over time as Tesla ironed out its\nmanufacturing process and increased its sales. \u201cThat\u2019s when Tim told me it\nwas really bad news,\u201d Musk said. It looked like each Roadster could cost up\nto $200,000 to make, and Tesla planned to sell the car for only around\n$85,000. \u201cEven in full production, they would have been like $170,000 or\nsomething insane,\u201d Musk said. \u201cOf course, it didn\u2019t much matter because\nabout a third of the cars didn\u2019t flat-out fucking work.\u201d\nEberhard made attempts to pull his team out of this mess. He\u2019d gone to\nsee a speech in which the famous venture capitalist John Doerr, who\nbecame a major investor in green technology companies, declared that he\nwould devote his time and money to trying to save the Earth from global\nwarming because he owed such an effort to his children. Eberhard promptly\nreturned to the Tesla building and ginned up a similar speech. In front of\nabout a hundred people, Eberhard had a picture of his young daughter\nprojected onto the wall of the main workshop. He asked the Tesla engineers\nwhy he had put that picture up. One of them guessed that it was because\npeople like his daughter would drive the car. To which Eberhard replied,\n\u201cNo. We are building this because by the time she is old enough to drive she\nwill know a car as something completely different to how we know it today,\njust like you don\u2019t think of a phone as a thing on the wall with a cord on it.\nIt\u2019s this future that depends on you.\u201d Eberhard then thanked some of the key\nengineers and called out their efforts in public. Many of the engineers had been pulling all-nighters on a regular basis and Eberhard\u2019s show boosted\nmorale. \u201cWe were all working ourselves to the point of exhaustion,\u201d said\nDavid Vespremi, a former Tesla spokesman. \u201cThen came this profound\nmoment where we were reminded that building the car was not about\ngetting to an IPO or selling it to a bunch of rich dudes but because it might\nchange what a car is.\u201d\nThese victories, though, were not enough to overcome the feeling\nshared by many of the Tesla engineers that Eberhard had reached the end of\nhis abilities as a CEO. The company veterans had always admired\nEberhard\u2019s engineering smarts and continued to do so. Eberhard, in fact,\nhad turned Tesla into a cult of engineering. Regrettably, other parts of the\ncompany had been neglected, and people doubted Eberhard\u2019s ability to take\nthe company from the R&D stage to production. The ridiculous cost of the\ncar, the transmission, the ineffective suppliers were crippling Tesla. And, as\nthe company started to miss its delivery dates, many of the once-fanatical\nconsumers who had made their large up-front payments turned on Tesla and\nEberhard. \u201cWe saw the writing on the wall,\u201d Lyons said. \u201cEveryone knew\nthat the person who starts a company is not necessarily the right person to\nlead it in the long term, but whenever that is the case, it\u2019s not easy.\u201d\nEberhard and Musk had battled for years over some of the design points\non the car. But for the most part, they had gotten along well enough. Neither\nman suffered fools. And they certainly shared many of the same visions for\nthe battery technology and what it could mean to the world. What their\nrelationship could not survive were the cost figures for the Roadster\nunearthed by Watkins. It looked to Musk as if Eberhard had grossly\nmismanaged the company by allowing the parts costs to soar so high. Then,\nas Musk saw it, Eberhard failed to disclose the severity of the situation to\nthe board. While on his way to give a talk to the Motor Press Guild in Los\nAngeles, Eberhard received a call from Musk and in a brief, uncomfortable\nchat learned that he would be replaced as CEO.\nIn August 2007, Tesla\u2019s board demoted Eberhard and named him\npresident of technology, which only exacerbated the company\u2019s issues.\n\u201cMartin was so bitter and disruptive,\u201d Straubel said. \u201cI remember him\nrunning around the office and sowing discontent, as we\u2019re trying to finish\nthe car and are running out of money and everything is at knife\u2019s edge.\u201d As\nEberhard saw it, other people at Tesla had foisted a wonky finance software\napplication on him that made it tricky to accurately track costs. He contended that the delays and cost increases were partly due to the requests\nof other members of the management team and that he\u2019d been up front with\nthe board about the issues. Beyond that, he thought Watkins had made the\nsituation out to be worse than it really was. Start-ups in Silicon Valley view\nmayhem as standard operating procedure. \u201cValor was used to dealing with\nolder companies,\u201d Eberhard said. \u201cThey found chaos and weren\u2019t used to it.\nThis was the chaos of a start-up.\u201d Eberhard had also already been asking\nTesla\u2019s board to replace him as CEO and find someone with more\nmanufacturing experience.\nA few months passed, and Eberhard remained pissed-off. Many of the\nTesla employees felt like they were caught in the middle of a divorce and\nhad to pick their parent\u2014Eberhard or Musk. By the time December arrived,\nthe situation was untenable, and Eberhard left the company altogether. Tesla\nsaid in a statement that Eberhard had been offered a position on its advisory\nboard, although he denied that. \u201cI am no longer with Tesla Motors\u2014neither\non its board of directors nor an employee of any sort,\u201d Eberhard said in a\nstatement at the time. \u201cI\u2019m not happy with the way I was treated.\u201d Musk\nsent a note to a Silicon Valley newspaper saying, \u201cI\u2019m sorry that it came to\nthis and wish it were not so. It was not a question of personality differences,\nas the decision to have Martin transition to an advisory role was unanimous\namong the board. Tesla has operational problems that need to be solved and\nif the board thought there was any way that Martin could be part of the\nsolution, then he would still be an employee of the company.\u201d9 These\nstatements were the start of a war that would drag on between the two men\nin public for years and that in many ways continues to the present day.\nAs 2007 played out, the problems mounted for Tesla. The carbon-fiber\nbody that looked so good turned out to be a huge pain to paint, and Tesla\nhad to cycle through a couple of companies to find one that could do the\nwork well. Sometimes there were faults in the battery pack. The motor\nshort-circuited now and again. The body panels had visible gaps. The\ncompany also had to face up to the reality that a two-speed transmission\nwas not going to happen. In order for the Roadster to achieve its flashy\nzero-to-60 times with a single-speed transmission, Tesla\u2019s engineers had to\nredesign the car\u2019s motor and inverter and shave off some weight. \u201cWe\nessentially had to do a complete reboot,\u201d Musk said. \u201cThat was terrible.\u201d\nAfter Eberhard was removed as CEO, Tesla\u2019s board tapped Michael\nMarks as its interim chief. Marks had run Flextronics, an enormous electronics supplier, and had deep experience with complex manufacturing\noperations and logistics issues. Marks began interrogating various groups at\nthe company to try to figure out their problems and to prioritize the issues\nplaguing the Roadster. He also put in some basic rules like making sure that\npeople all showed up at work at the same time to establish a baseline of\nproductivity\u2014a tricky ask in Silicon Valley\u2019s work anywhere, anytime\nculture. All of these moves were part of the Marks List, a 10-point, 100-day\nplan that included eliminating all faults in the battery packs, getting gaps\nbetween body parts to less than 40 mm, and booking a specified number of\nreservations. \u201cMartin had been falling apart and lacked a lot of the\ndiscipline key for a manager,\u201d Straubel said. \u201cMichael came in and\nevaluated the mess and was a bullshit filter. He didn\u2019t really have a dog in\nthe fight and could say, \u2018I don\u2019t care what you think or what you think. This\nis what we should do.\u2019\u201d For a while, Marks\u2019s strategy worked, and the\nengineers at Tesla could once again focus on building the Roadster rather\nthan on internal politics. But then Marks\u2019s vision for the company began to\ndiverge from Musk\u2019s.\nBy this time, Tesla had moved into a larger facility at 1050 Bing Street\nin San Carlos. The bigger building allowed Tesla to bring the battery work\nback in-house from Asia and for it to do some of the Roadster\nmanufacturing, alleviating the supply chain issues. Tesla was maturing as a\ncar company, although its wild-child start-up streak remained well intact.\nWhile strolling around the factory one day, Marks saw a Smart car from\nDaimler on a lift. Musk and Straubel had a small side project going on\naround the Smart car to see what it might be like as an electric vehicle.\n\u201cMichael didn\u2019t know about it, and he\u2019s like, \u2018Who is the CEO here?\u2019\u201d said\nLyons. (The work on the Smart car eventually led to Daimler buying a 10\npercent stake in Tesla.)\nMarks\u2019s inclination was to try to package Tesla as an asset that could be\nsold to a larger car company. It was a perfectly reasonable plan. While\nrunning Flextronics, Marks had overseen a vast, global supply chain and\nknew the difficulties of manufacturing intimately. Tesla must have looked\nborderline hopeless to him at this point. The company could not make its\none product well, was poised to hemorrhage money, and had missed a string\nof delivery deadlines and yet its engineers were still off doing side\nexperiments. Making Tesla look as pretty as possible for a suitor was the\nrational thing to do. In just about every other case, Marks would be thanked for his decisive\nplan of action and saving the company\u2019s investors from a big loss. But\nMusk had little interest in polishing up Tesla\u2019s assets for the highest bidder.\nHe\u2019d started the company to put a dent in the automotive industry and force\npeople to rethink electric cars. Instead of doing the fashionable Silicon\nValley thing of \u201cpivoting\u201d toward a new idea or plan, Musk would dig in\ndeeper. \u201cThe product was late and over budget and everything was wrong,\nbut Elon didn\u2019t want anything to do with those plans to either sell the whole\ncompany or lose control through a partnership,\u201d Straubel said. \u201cSo, Elon\ndecided to double down.\u201d\nOn December 3, 2007, Ze\u2019ev Drori replaced Marks as CEO. Drori had\nexperience in Silicon Valley starting a company that made computer\nmemory and selling it to the chipmaker Advanced Micro Devices. Drori\nwas not Musk\u2019s first pick\u2014a top choice had turned down the job because he\ndidn\u2019t want to move from the East Coast\u2014and did not inspire much\nenthusiasm from the Tesla employees. Drori had about fifteen years on the\nyoungest Tesla worker and no connection to this group bonded by suffering\nand toil. He came to be seen more as an executor of Musk\u2019s wishes than as\na commanding, independent CEO.\nMusk began making more public gestures to mitigate the bad press\naround Tesla. He issued statements and did interviews, promising that the\nRoadster would ship to customers in early 2008. He began talking up a car\ncode-named WhiteStar\u2014the Roadster had been code-named DarkStar\u2014that\nwould be a sedan possibly priced around $50,000, and a new factory to\nbuild the machine. \u201cGiven the recent management changes, some\nreassurances are in order regarding Tesla Motors\u2019 future plans,\u201d Musk wrote\nin a blog post. \u201cThe near term message is simple and unequivocal\u2014we are\ngoing to deliver a great sports car next year that customers will love driving.\n. . . My car, production VIN 1, is already off the production line in the UK\nand final preparations are being made for importation.\u201d Tesla held a series\nof town hall meetings with customers where it tried to fess up to its\nproblems in the open, and it started building some showrooms for its car.\nVince Sollitto, the former PayPal executive, visited the Menlo Park\nshowroom and found Musk complaining about the public relations issues\nbut clearly inspired by the product Tesla was building. \u201cHis demeanor\nchanged the moment we got to this display of the motor,\u201d Sollitto said.\nDressed in a leather jacket and slacks, Musk started talking about the motor\u2019s properties and then put on a performance worthy of a carnival\nstrongman by lifting the hundred-or-so-pound hunk of metal. \u201cHe picks this\nthing up and wedges it between his two palms,\u201d Sollitto said. \u201cHe\u2019s holding\nit, and he\u2019s shaking and beads of sweat are forming on his forehead. It\nwasn\u2019t so much a display of strength as a physical demonstration of the\nbeauty of the product.\u201d While the customers complained a lot about the\ndelays, they seemed to sense this passion from Musk and share his\nenthusiasm for the product. Only a handful of customers asked for their\nprepayments back.\nTesla employees soon got to witness the same Musk that SpaceX\nemployees had seen for years. When an issue like the Roadster\u2019s faulty\ncarbon-fiber body panels cropped up, Musk dealt with it directly. He flew to\nEngland in his jet to pick up some new manufacturing tools for the body\npanels and personally delivered them to a factory in France to ensure that\nthe Roadster stayed on its production schedule. The days of people being\nambiguous about the Roadster\u2019s manufacturing costs were gone as well.\n\u201cElon got fired up and said we were going to do this intense cost-down\nprogram,\u201d said Popple. \u201cHe gave a speech, saying we would work on\nSaturdays and Sundays and sleep under desks until it got done. Someone\npushed back from the table and argued that everyone had been working so\nhard just to get the car done, and they were ready for a break and to see\ntheir families. Elon said, \u2018I would tell those people they will get to see their\nfamilies a lot when we go bankrupt.\u2019 I was like, \u2018Wow,\u2019 but I got it. I had\ncome out of a military culture, and you just have to make your objective\nhappen.\u201d Employees were required to meet at 7 A.M. every Thursday\nmorning for bill-of-materials updates. They had to know the price of every\npart and have a cogent plan for getting parts cheaper. If the motor cost\n$6,500 a pop at the end of December, Musk wanted it to cost $3,800 by\nApril. The costs were plotted and analyzed each month. \u201cIf you started\nfalling behind, there was hell to pay,\u201d Popple said. \u201cEveryone could see it,\nand people lost their jobs when they didn\u2019t deliver. Elon has a mind that\u2019s a\nbit like a calculator. If you put a number on the projector that does not make\nsense, he will spot it. He doesn\u2019t miss details.\u201d Popple found Musk\u2019s style\naggressive, but he liked that Musk would listen to a well-argued, analytical\npoint and often change his mind if given a good enough reason. \u201cSome\npeople thought Elon was too tough or hot-tempered or tyrannical,\u201d Popple\nsaid. \u201cBut these were hard times, and those of us close to the operational realities of the company knew it. I appreciated that he didn\u2019t sugarcoat\nthings.\u201d\nOn the marketing front, Musk would run daily Google searches for\nnews stories about Tesla. If he saw a bad story, he ordered someone to \u201cfix\nit\u201d even though the Tesla public relations people could do little to sway the\nreporters. One employee missed an event to witness the birth of his child.\nMusk fired off an e-mail saying, \u201cThat is no excuse. I am extremely\ndisappointed. You need to figure out where your priorities are. We\u2019re\nchanging the world and changing history, and you either commit or you\ndon\u2019t.\u201d*\nMarketing people who made grammatical mistakes in e-mails were let\ngo, as were other people who hadn\u2019t done anything \u201cawesome\u201d in recent\nmemory. \u201cHe can be incredibly intimidating at times but doesn\u2019t have a real\nsense for just how imposing he can be,\u201d said one former Tesla executive.\n\u201cWe\u2019d have these meetings and take bets on who was going to get bloodied\nand bruised. If you told him that you made a particular choice because \u2018it\nwas the standard way things had always been done,\u2019 he\u2019d kick you out of a\nmeeting fast. He\u2019d say, \u2018I never want to hear that phrase again. What we\nhave to do is fucking hard and half-assing things won\u2019t be tolerated.\u2019 He\njust destroys you and, if you survive, he determines if he can trust you. He\nhas to understand that you\u2019re as crazy as he is.\u201d This ethos filtered through\nthe entire company, and everyone quickly understood that Musk meant\nbusiness.\nStraubel, while sometimes on the bad end of the critiques, welcomed\nMusk\u2019s hard-charging presence. The five years to get to this point had been\nan enjoyable slog for him. Straubel had transformed from a quiet, capable\nengineer who shuffled around Tesla\u2019s factory floor with his head down into\nthe most crucial member of the technical team. He knew more about the\nbatteries and the electric drivetrain than just about anyone else at the\ncompany. He also began developing a role as a go-between for employees\nand Musk. Straubel\u2019s engineering smarts and work ethic had earned Musk\u2019s\nrespect, and Straubel found that he could deliver difficult messages to Musk\non behalf of other employees. As he would do for years to come, Straubel\nalso proved willing to check his ego at the door. All that mattered was\ngetting the Roadster and the follow-on sedan to market to popularize\nelectric cars, and Musk looked like the best person to make that happen. Other employees had enjoyed the thrill of the engineering challenge\nover the past five years but were burnt-out beyond repair. Wright didn\u2019t\nbelieve that an electric car for the masses would ever take off. He left and\nstarted his own company dedicated to making electric versions of delivery\ntrucks. Berdichevsky had been a crucial, do-anything young engineer for\nmuch of Tesla\u2019s existence. Now that the company employed about three\nhundred people, he felt less effective and didn\u2019t relish the idea of suffering\nfor another five years to bring the sedan to market. He would leave Tesla,\nget a couple of degrees from Stanford, and cofound a start-up looking to\nmake a revolutionary new battery that could soon go into electric cars. With\nEberhard gone, Tarpenning found Tesla less fun. He didn\u2019t see eye to eye\nwith Drori and also shied away from the idea of frying his soul to get the\nsedan out. Lyons stuck around longer, which is a minor miracle. At various\npoints, he had led the development of most of the core technology behind\nthe Roadster, including the battery packs, the motor, the power electronics,\nand, yes, the transmission. This meant that for about five years Lyons had\nbeen among Tesla\u2019s most capable employees and the guy constantly in the\ndoghouse for being behind on something and thus holding the rest of the\ncompany up. He suffered through some of Musk\u2019s more colorful tirades\u2014\ndirected either at him or suppliers that had let Tesla down\u2014that included\ntalk of people\u2019s balls being chopped off and other violent or sexual acts.\nLyons also saw an exhausted, stressed-out Musk spit coffee across a\nconference room table because it was cold and then, without a pause,\ndemand that the employees work harder, do more, and mess up less. Like so\nmany people privy to these performances, Lyons came away with no\nillusions about Musk\u2019s personality but with the utmost respect for his vision\nand drive to execute. \u201cWorking at Tesla back then was like being Kurtz in\nApocalypse Now,\u201d Lyons said. \u201cDon\u2019t worry about the methods or if they\u2019re\nunsound. Just get the job done. It comes from Elon. He listens, asks good\nquestions, is fast on his feet, and gets to the bottom of things.\u201d\nTesla could survive the loss of some of these early hires. Its strong\nbrand had allowed the company to keep recruiting top talent, including\npeople from large automotive companies who knew how to get over the last\nset of challenges blocking the Roadster from reaching customers. But\nTesla\u2019s major issue no longer revolved around effort, engineering, or clever\nmarketing. Heading into 2008, the company was running out of money. The\nRoadster had cost about $140 million to develop, way over the $25 million originally estimated in the 2004 business plan. Under normal\ncircumstances, Tesla had probably done enough to raise more funds. These,\nhowever, were not normal times. The big automakers in the United States\nwere charging toward bankruptcy in the middle of the worst financial crisis\nsince the Great Depression. In the midst of all this, Musk needed to\nconvince Tesla\u2019s investors to fork over tens of millions of additional dollars,\nand those investors had to go to their constituents to lay out why this made\nany sense. As Musk put it, \u201cTry to imagine explaining that you\u2019re investing\nin an electric car company, and everything you read about the car company\nsounds like it is shit and doomed and it\u2019s a recession and no one is buying\ncars.\u201d All Musk had to do to dig Tesla out of this conundrum was lose his\nentire fortune and verge on a nervous breakdown. 8 PAIN, SUFFERING, AND SURVIVAL\nA\nS HE PREPARED TO BEGIN FILMING IRON MAN IN EARLY 2007,\nthe director Jon Favreau rented out a complex in Los Angeles that once\nbelonged to Hughes Aircraft, the aerospace and defense contractor started\nabout eighty years earlier by Howard Hughes. The facility had a series of\ninterlocking hangars and served as a production office for the movie. It also\nsupplied Robert Downey Jr., who was to play Iron Man and his human\ncreator Tony Stark, with a splash of inspiration. Downey felt nostalgic\nlooking at one of the larger hangars, which had fallen into a state of\ndisrepair. Not too long ago, that building had played host to the big ideas of\na big man who shook up industries and did things his own way.\nDowney heard some rumblings about a Hughes-like figure named Elon\nMusk who had constructed his own, modern-day industrial complex about\nten miles away. Instead of visualizing how life might have been for Hughes,\nDowney could perhaps get a taste of the real thing. He set off in March\n2007 for SpaceX\u2019s headquarters in El Segundo and wound up receiving a\npersonal tour from Musk. \u201cMy mind is not easily blown, but this place and\nthis guy were amazing,\u201d Downey said.\nTo Downey, the SpaceX facility looked like a giant, exotic hardware\nstore. Enthusiastic employees were zipping about, fiddling with an\nassortment of machines. Young white-collar engineers interacted with blue-\ncollar assembly line workers, and they all seemed to share a genuine\nexcitement for what they were doing. \u201cIt felt like a radical start-up\ncompany,\u201d Downey said. After the initial tour, Downey came away pleased\nthat the sets being hammered out at the Hughes factory did have parallels to\nthe SpaceX factory. \u201cThings didn\u2019t feel out of place,\u201d he said.\nBeyond the surroundings, Downey really wanted a peek inside Musk\u2019s\npsyche. The men walked, sat in Musk\u2019s office, and had lunch. Downey\nappreciated that Musk was not a foul-smelling, fidgety, coder whack job.\nWhat Downey picked up on instead were Musk\u2019s \u201caccessible eccentricities\u201d\nand the feeling that he was an unpretentious sort who could work alongside the people in the factory. Both Musk and Stark were the type of men,\naccording to Downey, who \u201chad seized an idea to live by and something to\ndedicate themselves to\u201d and were not going to waste a moment.\nWhen he returned to the Iron Man production office, Downey asked that\nFavreau be sure to place a Tesla Roadster in Tony Stark\u2019s workshop. On a\nsuperficial level, this would symbolize that Stark was so cool and connected\nthat he could get a Roadster before it even went on sale. On a deeper level,\nthe car was to be placed as the nearest object to Stark\u2019s desk so that it\nformed something of a bond between the actor, the character, and Musk.\n\u201cAfter meeting Elon and making him real to me, I felt like having his\npresence in the workshop,\u201d Downey said. \u201cThey became contemporaries.\nElon was someone Tony probably hung out with and partied with or more\nlikely they went on some weird jungle trek together to drink concoctions\nwith the shamans.\u201d\nAfter Iron Man came out, Favreau began talking up Musk\u2019s role as the\ninspiration for Downey\u2019s interpretation of Tony Stark. It was a stretch on\nmany levels. Musk is not exactly the type of guy who downs scotch in the\nback of a Humvee while part of a military convoy in Afghanistan. But the\npress lapped up the comparison, and Musk started to become more of a\npublic figure. People who sort of knew him as \u201cthat PayPal guy\u201d began to\nthink of him as the rich, eccentric businessman behind SpaceX and Tesla.\nMusk enjoyed his rising profile. It fed his ego and provided some fun.\nHe and Justine bought a house in Bel Air. Their neighbor to one side was\nQuincy Jones, the music producer, and their other neighbor was Joe Francis,\nthe infamous creator of the Girls Gone Wild videos. Musk and some former\nPayPal executives, having settled their differences, produced Thank You for\nSmoking and used Musk\u2019s jet in the movie. While not a hard-drinking\ncarouser, Musk took part in the Hollywood nightlife and its social scene.\n\u201cThere were just a lot of parties to go to,\u201d said Bill Lee, Musk\u2019s close\nfriend. \u201cElon was neighbors with two quasi-celebrities. Our friends were\nmaking movies and through this confluence of our networks, there was\nsomething to go out and do every night.\u201d In one interview, Musk calculated\nthat his life had become 10 percent playboy and 90 percent engineer.10 \u201cWe\nhad a domestic staff of five; during the day our home transformed into a\nworkplace,\u201d Justine wrote in magazine article. \u201cWe went to black-tie\nfundraisers and got the best tables at elite Hollywood nightclubs, with Paris\nHilton and Leonardo DiCaprio partying next to us. When Google cofounder Larry Page got married on Richard Branson\u2019s private Caribbean island, we\nwere there, hanging out in a villa with John Cusack and watching Bono\npose with swarms of adoring women outside the reception tent.\u201d\nJustine appeared to relish their status even more than Musk. A writer of\nfantasy fiction novels, she kept a blog detailing the couple\u2019s family life and\ntheir adventures on the town. In one entry, Justine had Musk saying that\nhe\u2019d prefer to sleep with Veronica than Betty from the Archie comics and\nthat he\u2019d like to visit a Chuck E. Cheese sometime. In another entry, she\nwrote about meeting Leonardo DiCaprio at a club and having him beg for a\nfree Tesla Roadster, only to be turned down. Justine handed out nicknames\nto oft-occurring characters in the blog, so Bill Lee became \u201cBill the Hotel\nGuy\u201d because he owns a hotel in the Dominican Republic, and Joe Francis\nappeared as \u201cNotorious Neighbor.\u201d It\u2019s hard to imagine Musk, who keeps to\nhimself, hanging out with someone as ostentatious as Francis, but the men\ngot along well. When Francis took over an amusement park for his birthday,\nMusk attended and then ended up partying at Francis\u2019s house. Justine wrote,\n\u201cE was there for a bit but admitted he also found it \u2018kind of lame\u2019\u2014he\u2019s\nbeen to a couple of parties at NN\u2019s house now and ends up feeling self-\nconscious, \u2018because it just seems like there are always these skeevy guys\nwandering around the house trolling for girls. I don\u2019t want to be seen as one\nof those guys.\u2019\u201d When Francis got ready to buy a Roadster, he stopped by\nthe Musks\u2019 house and handed over a yellow envelope with $100,000 in\ncash.\nFor a while, the blog provided a rare, welcome glimpse into the life of\nan unconventional CEO. Musk seemed charming. The public learned that\nhe bought Justine a nineteenth-century edition of Pride and Prejudice, that\nMusk\u2019s best friends gave him the nickname \u201cElonius,\u201d and that Musk likes\nto place one-dollar wagers on all manner of things\u2014Can you catch herpes\nfrom the Great Barrier Reef? Is it possible to balance two forks with a\ntoothpick?\u2014that he knows he will win. Justine told one story about Musk\ntraveling to Necker Island, in the British Virgin Islands, to hang out with\nTony Blair and Richard Branson. A photo of the three men appeared later in\nthe press that depicted Musk with a vacant stare. \u201cThis was E\u2019s I\u2019m-\nthinking-about-a-rocket-problem stance, which makes me pretty sure that\nhe had just gotten some kind of bothersome work-related e-mail, and was\nclearly oblivious to the fact that a picture was being taken at all,\u201d she wrote.\n\u201cThis is also the reason I get suck [sic] a kick out of it\u2014the spouse the camera caught is the exact spouse I encountered, say, last night en route to\nthe bathroom, standing in the hallway frowning with his arms folded.\u201d\nJustine letting the world into the couple\u2019s bathroom should have served as a\nwarning of things to come. Her blog would soon turn into one of Musk\u2019s\nworst nightmares.\nThe press had not run into a guy like Musk for a very long time. His\nshine as an Internet millionaire kept getting, well, shinier thanks to PayPal\u2019s\nongoing success. He also had an element of mystery. There was the weird\nname. And there was the willingness to spend vast sums of money on\nspaceships and electric cars, which came across as a combination of daring,\nflamboyant, and downright flabbergasting. \u201cElon Musk has been called\n\u2018part playboy, part space cowboy,\u2019 an image hardly dispelled by a car\ncollection that has boasted a Porsche 911 Turbo, 1967 Series 1 Jaguar, a\nHamann BMW M5 plus the aforementioned McLaren F1\u2014which he has\ndriven at up to 215mph on a private airstrip,\u201d a British reporter gushed in\n2007. \u201cThen there was the L39 Soviet military jet, which he sold after\nbecoming a father.\u201d The press had picked up on the fact that Musk tended to\ntalk a huge game and then struggle to deliver on his promises in time, but\nthey didn\u2019t much care. The game he talked was so much bigger than anyone\nelse\u2019s that reporters were comfortable giving Musk leeway. Tesla became\nthe darling of Silicon Valley\u2019s bloggers, who tracked its every move and\nwere breathless in their coverage. Similarly, reporters covering SpaceX\nwere overjoyed that a young, feisty company had arrived to needle Boeing,\nLockheed, and, to a large extent, NASA. All Musk had to do was eventually\nbring some of these wondrous things he\u2019d been funding to market.\nWhile Musk put on a good show for the public and press, he\u2019d started to\nget very worried about his businesses. SpaceX\u2019s second launch attempt had\nfailed, and the reports coming in from Tesla kept getting worse. Musk had\nstarted these two adventures with a fortune nearing $200 million and had\nchewed through more than half the money with little to show for it. As each\nTesla delay turned into a PR fiasco, the Musk glow dimmed. People in\nSilicon Valley began to gossip about Musk\u2019s money problems. Reporters\nwho months earlier had been heaping adulation on Musk turned on him.\nThe New York Times picked up on Tesla\u2019s transmission problems.\nAutomotive websites griped that the Roadster might never ship. By the end\nof 2007, things got downright nasty. Valleywag, Silicon Valley\u2019s gossip\nblog, began to take a particular interest in Musk. Owen Thomas, the site\u2019s lead writer, dug into the histories of Zip2 and PayPal and played up the\ntimes Musk was ousted as CEO to undermine some of his entrepreneurial\nstreet cred. Thomas then championed the premise that Musk was a master\nmanipulator who played fast and loose with other people\u2019s money. \u201cIt\u2019s\nwonderful that Musk has realized even a small part of his childhood\nfantasies,\u201d Thomas wrote. \u201cBut he risks destroying his dreams by refusing\nto reconcile them with reality.\u201d Valleywag anointed the Tesla Roadster as its\nNo. 1 fail of 2007 among technology companies.\nAs his businesses and public persona suffered, Musk\u2019s home life\ndegraded as well. His triplets\u2014Kai, Damian, and Saxon\u2014had arrived near\nthe end of 2006 and joined their brothers Griffin and Xavier. According to\nMusk, Justine suffered from postpartum depression following the birth of\nthe triplets. \u201cIn the spring of 2007, our marriage was having real issues,\u201d\nMusk said. \u201cIt was on the rocks.\u201d Justine\u2019s blog posts back up his\nsentiments. She described a much less romantic Musk and felt people\ntreated her as \u201can arm ornament who couldn\u2019t possibly have anything\ninteresting to say\u201d rather than as an author and her husband\u2019s equal. During\none trip to St. Barts, the Musks ended up sharing dinner with some wealthy,\ninfluential couples. When Justine let out her political views, one of the men\nat the table made a crack about her being so opinionated. \u201cE chuckled back,\npatted my hand the way you pat a child\u2019s,\u201d Justine wrote on her blog. From\nthat point on, Justine ordered Musk to introduce her as a published novelist\nand not just his wife and mother of his children. The results? \u201cE\u2019s way of\ndoing this throughout the rest of the trip: \u2018Justine wants me to tell you that\nshe\u2019s written novels,\u2019 which made people look at me like oh, that\u2019s just so\ncute and didn\u2019t really help my case.\u201d\nAs 2007 rolled into 2008, Musk\u2019s life became more tumultuous. Tesla\nbasically had to start over on much of the Roadster, and SpaceX still had\ndozens of people living in Kwajalein awaiting the next launch of the Falcon\n1. Both endeavors were vacuuming up Musk\u2019s money. He started selling off\nprized possessions like the McLaren to generate extra cash. Musk tended to\nshield employees from the gravity of his fiscal situation by always\nencouraging them to do their best work. At the same time, he personally\noversaw all significant purchases at both companies. Musk also trained\nemployees to make the right trade-offs between spending money and\nproductivity. This struck many of the SpaceX employees as a novel idea,\nsince they were used to traditional aerospace companies that had huge, multiyear government contracts and no day-to-day survival pressure. \u201cElon\nwould always be at work on Sunday, and we had some chats where he laid\nout his philosophy,\u201d said Kevin Brogan, the early SpaceX employee. \u201cHe\nwould say that everything we did was a function of our burn rate and that\nwe were burning through a hundred thousand dollars per day. It was this\nvery entrepreneurial, Silicon Valley way of thinking that none of the\naerospace engineers in Los Angeles were dialed into. Sometimes he\nwouldn\u2019t let you buy a part for two thousand dollars because he expected\nyou to find it cheaper or invent something cheaper. Other times, he\nwouldn\u2019t flinch at renting a plane for ninety thousand dollars to get\nsomething to Kwaj because it saved an entire workday, so it was worth it.\nHe would place this urgency that he expected the revenue in ten years to be\nten million dollars a day and that every day we were slower to achieve our\ngoals was a day of missing out on that money.\u201d\nMusk had become all consumed with Tesla and SpaceX out of necessity,\nand there can be no doubt that this exacerbated the tensions in his marriage.\nThe Musks had a team of nannies to help with their five children, but Elon\ncould not spend much time at home. He worked seven days a week and\nquite often split his time between Los Angeles and San Francisco. Justine\nneeded a change. During moments of self-reflection, she felt sickened,\nperceiving herself a trophy wife. Justine longed to be Elon\u2019s partner again\nand to feel some of that spark from their early days before life had turned so\ndazzling and demanding. It\u2019s not clear how much Musk let on to Justine\nabout his dwindling bank account. She has long maintained that Musk kept\nher in the dark about the family\u2019s financial arrangements. But some of\nMusk\u2019s closest friends did get a glimpse into the worsening financial\nsituation. In the first half of 2008, Antonio Gracias, the founder and CEO of\nValor Equity, met Musk for dinner. Gracias had been an investor in Tesla\nand had become one of Musk\u2019s closest friends and allies, and he could see\nMusk agonizing over his future. \u201cThings were starting to be difficult with\nJustine, but they were still together,\u201d Gracias said. \u201cDuring that dinner, Elon\nsaid, \u2018I will spend my last dollar on these companies. If we have to move\ninto Justine\u2019s parents\u2019 basement, we\u2019ll do it.\u2019\u201d\nThe option of moving in with Justine\u2019s parents expired on June 16,\n2008, when Musk filed for divorce. The couple did not disclose the\nsituation right away, although Justine left hints on her blog. In late June, she\nposted a quotation from Moby without any additional context: \u201cThere\u2019s no such thing as a well-adjusted public figure. If they were well adjusted they\nwouldn\u2019t try to be a public figure.\u201d The next entry had Justine house\nhunting for undisclosed reasons with Sharon Stone, and a couple of entries\nlater she talked about \u201ca major drama\u201d that she\u2019d been dealing with. In\nSeptember, Justine wrote her first blog post explicitly about the divorce,\nsaying, \u201cWe had a good run. We married young, took it as far as we could\nand now it is over.\u201d Valleywag naturally followed with a story about the\ndivorce and noted that Musk had been seen out with a twenty-something\nactress.\nThe media coverage and divorce freed Justine to write about her private\nlife in a much more liberated way. In the posts that followed, she gave her\naccount of how the marriage ended, her views on Musk\u2019s girlfriend and\nfuture second wife, and the inner workings of the divorce proceedings. For\nthe first time, the public had access to a deeply unpleasant portrayal of\nMusk and received some firsthand accounts\u2014albeit from an ex-wife\u2014of\nhis hardline behavior. The writing may have been biased, but it provided a\nwindow into how Musk operated. Here\u2019s one post about the lead-up to the\ndivorce and its rapid execution:\nDivorce, for me, was like the bomb you set off when all other\noptions have been exhausted. I had not yet given up on the\ndiplomacy option, which was why I hadn\u2019t already filed. We were\nstill in the early stages of marital counseling (three sessions total).\nElon, however, took matters into his own hands\u2014he tends to like to\ndo that\u2014when he gave me an ultimatum: \u201cEither we fix [the\nmarriage] today, or I will divorce you tomorrow.\u201d\nThat night, and again the next morning, he asked me what I\nwanted to do. I stated emphatically that I was not ready to unleash\nthe dogs of divorce; I suggested that \u201cwe\u201d hold off for at least\nanother week. Elon nodded, touched the top of my head, and left.\nLater that same morning I tried to make a purchase and discovered\nthat he had cut off my credit card, which is when I also knew that he\nhad gone ahead and filed (as it was, E did not tell me directly; he\nhad another person do it).\nFor Musk, each online missive from Justine created another public\nrelations crisis that added to the endless stream of issues faced by his companies. The image he\u2019d sculpted over the years appeared ready to\ncrumble alongside his businesses. It was a disaster scenario.\nSoon enough, the Musks had achieved celebrity divorce status.\nMainstream outlets joined Valleywag in poring over court filings tied to the\nbreakup, particularly as Justine fought for more money. During the PayPal\ndays, Justine had signed a postnuptial agreement and now argued that she\ndidn\u2019t really have the time or inclination to dig into the ramifications of the\npaperwork. Justine took to her blog in an entry titled \u201cgolddigger,\u201d and said\nshe was fighting for a divorce settlement that would include their house,\nalimony and child support, $6 million in cash, 10 percent of Musk\u2019s Tesla\nstock, 5 percent of Musk\u2019s SpaceX stock, and a Tesla Roadster. Justine also\nappeared on CNBC\u2019s show Divorce Wars and wrote an article for Marie\nClaire titled \u201c\u2018I Was a Starter Wife\u2019: Inside America\u2019s Messiest Divorce.\u201d\nThe public tended to side with Justine during all of this and couldn\u2019t\nquite figure out why a billionaire was fighting his wife\u2019s seemingly fair\nrequests. A major problem for Musk, of course, was that his assets were\nanything but liquid with most of his net worth being tied up in Tesla and\nSpaceX stock. The couple eventually settled with Justine getting the house,\n$2 million in cash (minus her legal fees), $80,000 a month in alimony and\nchild support for seventeen years, and a Tesla Roadster.*\nYears after the settlement, Justine still struggled to speak about her\nrelationship with Musk. During our interview, she broke down in tears\nseveral times and needed moments to compose her thoughts. Musk, she\nsaid, had hidden many things from her during their marriage and ultimately\ntreated her much like a business adversary to be conquered during the\ndivorce. \u201cWe were at war for a while, and when you go to war with Elon,\nit\u2019s pretty brutal,\u201d she said. Well after their marriage ended, Justine\ncontinued to blog about Musk. She wrote about Riley and provided\ncommentary on his parenting. One post gave Musk a hard time for banning\nstuffed animals from the house when their twins turned seven. Asked about\nthis, Justine said, \u201cElon is hard-core. He grew up in a tough culture and\ntough circumstances. He had to become very tough to not only thrive but to\nconquer the world. He doesn\u2019t want to raise soft overprivileged kids with no\ndirection.\u201d Comments like these seemed to indicate that Justine still\nadmired or at least understood Musk\u2019s strong will.*\nIn the weeks after he first filed for divorce in mid-June of 2008, Musk\ntumbled into a deep funk. Bill Lee started to worry about his friend\u2019s mental state and, as one of Musk\u2019s more free-spirited friends, wanted to do\nsomething to cheer him up. Now and again, Musk and Lee, an investor,\nwould take trips overseas and mix business and pleasure. The time was\nright for just such a journey, and they set off for London at the start of July.\nThe decompression program began poorly. Musk and Lee visited the\nheadquarters of Aston Martin to see the company\u2019s CEO and get a tour of\nhis factory. The executive treated Musk like an amateur car builder, talking\ndown to him and suggesting that he knew more about electric vehicles than\nanyone else on the planet. \u201cHe was a complete douche,\u201d as Lee put it, and\nthe men did their best to make a hasty exit back to central London. Along\nthe way, Musk had a nagging stomach pain turn severe. At the time, Lee\nwas married to Sarah Gore, the daughter of former vice president Al Gore,\nwho had been a medical student, and so he called her for advice. They\ndecided that Musk might be suffering from appendicitis, and Lee took him\nto a medical clinic in the middle of a shopping mall. When the tests came\nback negative, Lee set to work trying to goad Musk into a night on the\ntown. \u201cElon didn\u2019t want to go out, and I didn\u2019t really, either,\u201d Lee said. \u201cBut\nI was like, \u2018No, come on. We\u2019re all the way here.\u2019\u201d\nLee coaxed Musk into going to a club called Whisky Mist, in Mayfair.\nPeople had packed the small, high-end dance spot and Musk wanted to\nleave after ten minutes. The well-connected Lee texted a promoter friend of\nhis, who pulled some strings to get Musk escorted into the VIP area. The\npromoter then reached out to some of his prettiest friends, including a\ntwenty-two-year-old up-and-coming actress named Talulah Riley, and they\nsoon arrived at the club as well. Riley and her two gorgeous friends had\ncome from a charity gala and were in full-length, flowing gowns. \u201cTalulah\nwas in this huge Cinderella thing,\u201d Lee said. Musk and Riley were\nintroduced by people at the club, and he perked at the sight of her dazzling\nfigure.\nMusk and Riley sat at a table with their friends but immediately zeroed\nin on each other. Riley had just hit it big with her portrayal of Mary Bennet\nin Pride and Prejudice and thought of herself as quite the hotshot. The older\nMusk, meanwhile, took on the role of the soft-spoken, sweet engineer. He\nwhipped out his phone and displayed photos of the Falcon 1 and Roadster,\nalthough Riley thought he had just done some work on these projects and\ndidn\u2019t realize he ran the companies building the machines. \u201cI remember\nthinking that this guy probably didn\u2019t get to talk to young actresses a lot and that he seemed quite nervous,\u201d Riley said. \u201cI decided to be really nice to\nhim and give him a nice evening. Little did I know that he\u2019d spoken to a lot\nof pretty girls in his life.\u201d* The more Musk and Riley talked, the more Lee\negged them on. It was the first time in weeks that his friend appeared happy.\n\u201cHis stomach didn\u2019t hurt; he\u2019s not bummed, this is great,\u201d Lee said. Despite\nbeing dressed for a fairy tale, Riley didn\u2019t fall in love with Musk at first\nsight. But she did become more impressed and intrigued as the night went\non, particularly after the club promoter introduced Musk to a stunning\nmodel, and he politely said \u201cHello\u201d and then sat right back down with\nRiley. \u201cI figured he couldn\u2019t be all bad after that,\u201d said Riley, who then\nallowed Musk to place his hand on her knee. Musk asked Riley out to\ndinner the next night, and she accepted.\nWith her curvy figure, sultry eyes, and playful good-girl demeanor,\nRiley was a budding film star but didn\u2019t really act the part. She grew up in\nthe idyllic English countryside, went to a top school, and, until a week\nbefore she met Musk, had been living at home with her parents. After the\nnight at Whisky Mist, Riley called her family to tell them about the\ninteresting guy she had met who builds rockets and cars. Her father used to\nhead up the National Crime Squad and went straight to his computer to\nconduct a background check that illuminated Musk\u2019s resume as a married\ninternational playboy with five kids. Riley\u2019s father chided his daughter for\nbeing a fool, but she held out hope that Musk had an explanation and went\nto dinner with him anyway.\nMusk brought Lee to the dinner, and Riley brought her friend Tamsin\nEgerton, also a beautiful actress. Things were cooler throughout the meal as\nthe group dined in a depressingly empty restaurant. Riley waited to see\nwhat Musk would bring up on his own. Eventually, he did announce his five\nsons and his pending divorce. The confession proved enough to keep Riley\ninterested and curious about where things would lead. Following the meal,\nMusk and Riley broke off on their own. They went for a walk through Soho\nand then stopped at Cafe Boheme, where Riley, a lifelong teetotaler, sipped\nan apple juice. Musk kept Riley\u2019s attention, and the romance began in\nearnest.\nThe couple had lunch the next day and then went to the White Cube, a\nmodern art gallery, and then back to Musk\u2019s hotel room. Musk told Riley, a\nvirgin, that he wanted to show her his rockets. \u201cI was skeptical, but he did\nactually show me rocket videos,\u201d she said. Once Musk went back to the United States,* they kept in touch via e-mail for a couple of weeks, and\nthen Riley booked a flight to Los Angeles. \u201cI wasn\u2019t even thinking\ngirlfriend or anything like that,\u201d Riley said. \u201cI was just having fun.\u201d\nMusk had other ideas. Riley had been in California for just five days\nwhen he made his move as they lay in bed talking in a tiny room at the\nPeninsula hotel in Beverley Hills. \u201cHe said, \u2018I don\u2019t want you to leave. I\nwant you to marry me.\u2019 I think I laughed. Then, he said, \u2018No. I\u2019m serious.\nI\u2019m sorry I don\u2019t have a ring.\u2019 I said, \u2018We can shake on it if you like.\u2019 And\nwe did. I don\u2019t remember what I was thinking at the time, and all I can say\nis that I was twenty-two.\u201d\nRiley had been a model daughter up to that point, never giving her\nparents much of anything to worry about. She did well at school, had scored\nsome tremendous acting gigs, and had a soft, sweet personality that her\nfriends described as Snow White brought to life. But there she was on the\nhotel\u2019s balcony, informing her parents that she had agreed to marry a man\nfourteen years her senior, who had just filed for divorce from his first wife,\nhad five kids and two companies, and she didn\u2019t even see how she could\npossibly love him after knowing him for a matter of weeks. \u201cI think my\nmother had a nervous breakdown,\u201d Riley said. \u201cBut I had always been\nhighly romantic, and it actually didn\u2019t strike me as that strange.\u201d Riley flew\nback to England to gather her things, and her parents flew back with her to\nthe United States to meet Musk, who belatedly asked Riley\u2019s father for his\nblessing. Musk did not have his own house, which left the couple moving\ninto a home that belonged to Musk\u2019s friend the billionaire Jeff Skoll. \u201cI had\nbeen living there a week when this random guy walked in,\u201d Riley said. \u201cI\nsaid, \u2018Who are you?\u2019 He said, \u2018I am the homeowner. Who are you?\u2019 I told\nhim, and then he just walked out.\u201d Musk later proposed to Riley again on\nthe balcony of Skoll\u2019s house, unveiling a massive ring. (He has since bought\nher three engagement rings, including the giant first one, an everyday ring,\nand one designed by Musk that has a diamond surrounded by ten\nsapphires.) \u201cI remember him saying, \u2018Being with me was choosing the hard\npath.\u2019 I didn\u2019t quite understand at the time, but I do now. It\u2019s quite hard,\nquite the crazy ride.\u201d\nRiley experienced a baptism by fire. The whirlwind romance had given\nher the impression that she was engaged to a world conquering, jet-setting\nbillionaire. That was true in theory but a murkier proposition in practice. As\nlate July rolled around, Musk could see that he had just enough cash on hand to scrape through to the end of the year. Both SpaceX and Tesla would\nneed cash infusions at some point just to pay the employees, and it was\nunclear where that money would come from with the world\u2019s financial\nmarkets in disarray and investments being put on hold. If things had been\ngoing more smoothly at the companies, Musk could have felt more\nconfident about raising money, but they were not. \u201cHe would come home\nevery day, and there would be some calamity,\u201d Riley said. \u201cHe was under\nimmense pressure from all quarters. It was horrendous.\u201d\nSpaceX\u2019s third flight from Kwajalein jumped out as Musk\u2019s most\npressing concern. His team of engineers had remained camped out on the\nisland, preparing the Falcon 1 for another run. A typical company would\nfocus just on the task at hand. Not SpaceX. It had shipped the Falcon 1 to\nKwaj in April with one set of engineers and then put another group of\nengineers on a new project to develop the Falcon 9, a nine-engine rocket\nthat would take the place of the Falcon 5 and serve as a possible\nreplacement to the retiring space shuttle. SpaceX had yet to prove it could\nget to space successfully, but Musk kept positioning it to bid on big-ticket\nNASA contracts.*\nOn July 30, 2008, the Falcon 9 had a successful test fire in Texas with\nall nine of its engines lighting up and producing 850,000 pounds of thrust.\nThree days later, in Kwaj, SpaceX\u2019s engineers fueled up the Falcon 1 and\ncrossed their fingers. The rocket had an air force satellite as its payload,\nalong with a couple of experiments from NASA. All told, the cargo\nweighed 375 pounds.\nSpaceX had been making significant changes to its rocket since the last,\nfailed launch. A traditional aerospace company would not have wanted the\nadded risk, but Musk insisted that SpaceX push its technology forward\nwhile at the same time trying to make it work right. Among the biggest\nchanges for the Falcon 1 was a new version of the Merlin 1 engine that\nrelied on a tweaked cooling system.\nThe first launch attempt on August 2, 2008, aborted at T minus zero\nseconds. SpaceX regrouped and tried to launch again the same day. This\ntime everything seemed to be going well. The Falcon 1 soared into the sky\nand flew spectacularly without any indication of a problem. SpaceX\nemployees watching a webcast of the proceedings back in California let out\nhoots and whistles. Then, right at the moment when the first stage and\nsecond stage were to separate, there was a malfunction. An analysis after the fact would show that the new engines had delivered an unexpected\nthrust during the separation process that caused the first stage to bump up\ninto the second stage, damaging the top part of the rocket and its engine.*\nThe failed launch left many SpaceX employees shattered. \u201cIt was so\nprofound seeing the energy shift over the room in the course of thirty\nseconds,\u201d said Dolly Singh, a recruiter at SpaceX. \u201cIt was like the worst\nfucking day ever. You don\u2019t usually see grown-ups weeping, but there they\nwere. We were tired and broken emotionally.\u201d Musk addressed the workers\nright away and encouraged them to get back to work. \u201cHe said, \u2018Look. We\nare going to do this. It\u2019s going to be okay. Don\u2019t freak out,\u2019\u201d Singh recalled.\n\u201cIt was like magic. Everyone chilled out immediately and started to focus\non figuring out what just happened and how to fix it. It went from despair to\nhope and focus.\u201d Musk put up a positive front to the public as well. In a\nstatement, he said that SpaceX had another rocket waiting to attempt a\nfourth launch and a fifth launch planned shortly after that. \u201cI have also\ngiven the go-ahead to begin fabrication of flight six,\u201d he said. \u201cFalcon 9\ndevelopment will also continue unabated.\u201d\nIn reality, the third launch was a disaster with cascading consequences.\nSince the second stage of the rocket did not fire properly, SpaceX never got\na chance to see if it had really fixed the fuel-sloshing issues that had\nplagued the second flight. Many of the SpaceX engineers were confident\nthat they had solved this problem and were anxious to get to the fourth\nlaunch, believing that they had an easy answer for the recent thrust problem.\nFor Musk, the situation seemed graver. \u201cI was super depressed,\u201d Musk said.\n\u201cIf we hadn\u2019t solved the slush coupling problem on flight two, or there was\njust some random other thing that occurred\u2014say a mistake in the launch\nprocess or the manufacturing process unrelated to anything previous\u2014then\ngame over.\u201d SpaceX simply did not have enough money to try a fifth flight.\nHe\u2019d put $100 million into the company and had nothing to spare because\nof the issues at Tesla. \u201cFlight four was it,\u201d Musk said. If, however, SpaceX\ncould nail the fourth flight, it would instill confidence on the part of the\nU.S. government and possible commercial customers, paving the way for\nthe Falcon 9 and even more ambitious projects.\nLeading up to the third launch, Musk had been his usual ultra-involved\nself. Anyone at SpaceX who held the launch back went onto Musk\u2019s\ncritical-path shit list. Musk would hound the person responsible about the\ndelays but, typically, he would also do everything in his power to help solve problems. \u201cI was personally holding up the launch once and had to give\nElon twice-daily updates about what was going on,\u201d said Kevin Brogan.\n\u201cBut Elon would say, \u2018There are five hundred people at this company. What\ndo you need?\u2019\u201d One of the calls must have taken place while Musk courted\nRiley because Brogan remembered Musk phoning from the bathroom of a\nLondon club to find out how welding had gone on a large part of the rocket.\nMusk fielded another call in the middle of the night while sleeping next to\nRiley and had to whisper as he berated the engineers. \u201cHe\u2019s giving us the\npillow talk voice, so we all have to huddle around the speakerphone, while\nhe tells us, \u2018You guys need to get your shit together,\u2019\u201d Brogan said.\nWith the fourth launch, the demands and anticipation had ratcheted to\nthe point that people started making silly mistakes. Typically, the body of\nthe Falcon 1 rocket traveled to Kwaj via barge. This time Musk and the\nengineers were too excited and desperate to wait for the ocean journey.\nMusk rented a military cargo plane to fly the rocket body from Los Angeles\nto Hawaii and then on to Kwaj. This would have been a fine idea except the\nSpaceX engineers forgot to factor in what the pressurized plane would do to\nthe body of the rocket, which is less than an eighth of an inch thick. As the\nplane started its descent into Hawaii, everyone inside of it could hear\nstrange noises coming from the cargo hold. \u201cI looked back and could see\nthe stage crumpling,\u201d said Bulent Altan, the former head of avionics at\nSpaceX. \u201cI told the pilot to go up, and he did.\u201d The rocket had behaved\nmuch like an empty water bottle will on a plane, with the air pressure\npushing against the sides of the bottle and making it buckle. Altan\ncalculated that the SpaceX team on the plane had about thirty minutes to do\nsomething about the problem before they would need to land. They pulled\nout their pocketknives and cut away the shrink wrap that held the rocket\u2019s\nbody tight. Then they found a maintenance kit on the plane and used\nwrenches to open up some nuts on the rocket that would allow its internal\npressure to match that of the plane\u2019s. When the plane landed, the engineers\ndivvied up the duties of calling SpaceX\u2019s top executives to tell them about\nthe catastrophe. It was 3 A.M. Los Angeles time, and one of the executives\nvolunteered to deliver the horrific news to Musk. The thinking at the time\nwas that it would take three months to repair the damage. The body of the\nrocket had caved in in several places, baffles placed inside the fuel tank to\nstop the sloshing problem had broken, and an assortment of other issues had\nappeared. Musk ordered the team to continue on to Kwaj and sent in a reinforcement team with repair parts. Two weeks later, the rocket had been\nfixed inside of the makeshift hangar. \u201cIt was like being stuck in a foxhole\ntogether,\u201d Altan said. \u201cYou weren\u2019t going to quit and leave the person next\nto you behind. When it was all done, everyone felt amazing.\u201d\nThe fourth and possibly final launch for SpaceX took place on\nSeptember 28, 2008. The SpaceX employees had worked nonstop shifts\nunder agonizing pressure for six weeks to reach this day. Their pride as\nengineers and their hopes and dreams were on the line. \u201cThe people\nwatching back at the factory were trying their best not to throw up,\u201d said\nJames McLaury, a machinist at SpaceX. Despite their past flubs, the\nengineers on Kwaj were confident that this launch would work. Some of\nthese people had spent years on the island going through one of the more\nsurreal engineering exercises in human history. They had been separated\nfrom their families, assaulted by the heat, and exiled on their tiny launchpad\noutpost\u2014sometimes without much food\u2014for days on end as they waited\nfor the launch windows to open and dealt with the aborts that followed. So\nmuch of that pain and suffering and fear would be forgotten if this launch\nwent successfully.\nIn the late afternoon on the twenty-eighth, the SpaceX team raised the\nFalcon 1 into its launch position. Once again, it stood tall, looking like a\nbizarre artifact of an island tribe as palm trees swayed beside it and a\nsmattering of clouds crossed through the spectacular blue sky. By this time,\nSpaceX had upped its webcast game, turning each launch into a major\nproduction both for its employees and the public. Two SpaceX marketing\nexecutives spent twenty minutes before the launch going through all the\ntechnical ins and outs of the launch. The Falcon 1 was not carrying real\ncargo this time; neither the company nor the military wanted to see\nsomething else blow up or get lost at sea, so the rocket held a 360-pound\ndummy payload.\nThe fact that SpaceX had been reduced to launch theater did not faze the\nemployees or dampen their enthusiasm. As the rocket rumbled and then\nclimbed higher, the employees back at SpaceX headquarters let out raucous\ncheers. Each milestone that followed\u2014clearing the island, engine checks\ncoming back good\u2014was again met with whistles and shouts. As the first\nstage fell away, the second stage fired up about ninety seconds into the\nflight and the employees turned downright rapturous, filling the webcast\nwith their ecstatic hollering. \u201cPerfect,\u201d said one of the talking heads. The Kestrel engine glowed red and started its six-minute burn. \u201cWhen the\nsecond stage cleared, I could finally start breathing again and my knees\nstopped buckling,\u201d said McLaury.\nThe fairing opened up around the three-minute mark and fell back\ntoward Earth. And, finally, around nine minutes into its journey, the Falcon\n1 shut down just as planned and reached orbit, making it the first privately\nbuilt machine to accomplish such a feat. It took six years\u2014about four and\nhalf more than Musk had once planned\u2014and five hundred people to make\nthis miracle of modern science and business happen.\nEarlier in the day, Musk had tried to distract himself from the mounting\npressure by going to Disneyland with his brother Kimbal and their children.\nMusk then had to race back to make the 4 P.M. launch and walked into\nSpaceX\u2019s trailer control room about two minutes before blastoff. \u201cWhen the\nlaunch was successful, everyone burst into tears,\u201d Kimbal said. \u201cIt was one\nof the most emotional experiences I\u2019ve had.\u201d Musk left the control room\nand walked out to the factory floor, where he received a rock star\u2019s\nwelcome. \u201cWell, that was freaking awesome,\u201d he said. \u201cThere are a lot of\npeople who thought we couldn\u2019t do it\u2014a lot actually\u2014but as the saying\ngoes, \u2018the fourth time is the charm,\u2019 right? There are only a handful of\ncountries on Earth that have done this. It\u2019s normally a country thing, not a\ncompany thing. . . . My mind is kind of frazzled, so it\u2019s hard for me to say\nanything, but, man, this is definitely one of the greatest days in my life, and\nI think probably for most people here. We showed people we can do it. This\nis just the first step of many. . . . I am going to have a really great party\ntonight. I don\u2019t know about you guys.\u201d Mary Beth Brown then tapped Musk\non the shoulder and pulled him away to a meeting.\nThe afterglow of this mammoth victory faded soon after the party\nended, and the severity of SpaceX\u2019s financial hell became top of mind again\nfor Musk. SpaceX had the Falcon 9 efforts to support and had also\nimmediately green-lighted the construction of another machine\u2014the\nDragon capsule\u2014that would be used to take supplies, and one day humans,\nto the International Space Station. Historically, either project would cost\nmore than $1 billion to complete, but SpaceX would have to find a way to\nbuild both machines simultaneously for a fraction of the cost. The company\nhad dramatically increased the rate at which it hired employees and moved\ninto a much larger headquarters in Hawthorne, California. SpaceX had a\ncommercial flight booked to carry a satellite into orbit for the Malaysian government, but that launch and the payment for it would not arrive until\nthe middle of 2009. In the meantime, SpaceX simply struggled to make its\npayroll.\nThe press did not know the extent of Musk\u2019s financial woes, but they\nknew enough to turn detailing Tesla\u2019s precarious financial situation into a\nfavored pastime. A website called the Truth About Cars began a \u201cTesla\nDeath Watch\u201d in May 2008 and followed up with dozens of entries\nthroughout the year. The blog took special pleasure in rejecting the idea that\nMusk was a true founder of the company, presenting him as the moneyman\nand chairman who had more or less stolen Tesla from the genius engineer\nEberhard. When Eberhard started a blog detailing the pros and cons of\nbeing a Tesla customer, the auto site was all too happy to echo his gripes.\nTop Gear, a popular British television show, ripped the Roadster apart,\nmaking it look as if the car had run out of juice during a road test. \u201cPeople\njoke about the Tesla Death Watch and all that, but it was harsh,\u201d said\nKimbal Musk. \u201cOne day there were fifty articles about how Tesla will die.\u201d\nThen, in October 2008 (just a couple weeks after SpaceX\u2019s successful\nlaunch), Valleywag appeared on the scene again. First it ridiculed Musk for\nofficially taking over as CEO of Tesla and replacing Drori, on the grounds\nthat Musk had just lucked into his past successes. It followed that by\nprinting a tell-all e-mail from a Tesla employee. The report said that Tesla\nhad just gone through a round of layoffs, shut down its Detroit office, and\nhad only $9 million left in the bank. \u201cWe have over 1,200 reservations,\nwhich manes [sic] we\u2019ve taken multiples of tens of millions of cash from\nour customers and have spent them all,\u201d the Tesla employee wrote.\n\u201cMeanwhile, we only delivered less than 50 cars. I actually talked a close\nfriend of mine into putting down $60,000 for a Tesla Roadster. I cannot\nconscientiously be a bystander anymore and allow my company to deceive\nthe public and defraud our dear customers. Our customers and the general\npublic are the reason Tesla is so loved. The fact that they are being lied to is\njust wrong.\u201d*\nYes, Tesla deserved much of the negative attention. Musk, though, felt\nlike the 2008 climate with the hatred of bankers and the rich had turned him\ninto a particularly juicy target. \u201cI was just getting pistol-whipped,\u201d Musk\nsaid. \u201cThere was a lot of schadenfreude at the time, and it was bad on so\nmany levels. Justine was torturing me in the press. There were always all\nthese negative articles about Tesla, and the stories about SpaceX\u2019s third failure. It hurt really bad. You have these huge doubts that your life is not\nworking, your car is not working, you\u2019re going through a divorce and all of\nthose things. I felt like a pile of shit. I didn\u2019t think we would overcome it. I\nthought things were probably fucking doomed.\u201d\nWhen Musk ran through the calculations concerning SpaceX and Tesla,\nit occurred to him that only one company would likely even have a chance\nat survival. \u201cI could either pick SpaceX or Tesla or split the money I had\nleft between them,\u201d Musk said. \u201cThat was a tough decision. If I split the\nmoney, maybe both of them would die. If I gave the money to just one\ncompany, the probability of it surviving was greater, but then it would mean\ncertain death for the other company. I debated that over and over.\u201d While\nMusk meditated on this, the economy worsened quickly and so too did\nMusk\u2019s financial condition. As 2008 came to an end, Musk had run out of\nmoney.\nRiley began to see Musk\u2019s life as a Shakespearean tragedy. Sometimes\nMusk would open up to her about the issues, and other times he retreated\ninto himself. Riley spied on Musk while he read e-mail and watched him\ngrimace as the bad news poured in. \u201cYou\u2019d witness him having these\nconversations in his head,\u201d she said. \u201cIt\u2019s really hard to watch someone you\nlove struggle like that.\u201d Because of the long hours that he worked and his\neating habits, Musk\u2019s weight fluctuated wildly. Bags formed under his eyes,\nand his countenance started to resemble that of a shattered runner at the\nback end of an ultra-marathon. \u201cHe looked like death itself,\u201d Riley said. \u201cI\nremember thinking this guy would have a heart attack and die. He seemed\nlike a man on the brink.\u201d In the middle of the night, Musk would have\nnightmares and yell out. \u201cHe was in physical pain,\u201d Riley said. \u201cHe would\nclimb on me and start screaming while still asleep.\u201d The couple had to start\nborrowing hundreds of thousands of dollars from Musk\u2019s friend Skoll, and\nRiley\u2019s parents offered to remortgage their house. Musk no longer flew his\njet back and forth between Los Angles and Silicon Valley. He took\nSouthwest.\nBurning through about $4 million a month, Tesla needed to close\nanother major round of funding to get through 2008 and stay alive. Musk\nhad to lean on friends just to try to make payroll from week to week, as he\nnegotiated with investors. He sent impassioned pleas to anyone he could\nthink of who might be able to spare some money. Bill Lee invested $2\nmillion in Tesla, and Sergey Brin invested $500,000. \u201cA bunch of Tesla employees wrote checks to keep the company going,\u201d said Diarmuid\nO\u2019Connell, the vice president of business development at Tesla. \u201cThey\nturned into investments, but, at the time, it was twenty-five or fifty thousand\ndollars that you didn\u2019t expect to see again. It just seemed like holy shit, this\nthing is going to crater.\u201d Kimbal had lost most of his money during the\nrecession when his investments bottomed out but sold what he had left and\nput it into Tesla as well. \u201cI was close to bankruptcy,\u201d Kimbal said. Tesla had\nset the prepayments that customers made for the Roadsters aside, but Musk\nnow needed to use that money to keep the company going and soon those\nfunds were gone, too. These fiscal maneuvers worried Kimbal. \u201cI\u2019m sure\nElon would have found a way to make things right, but he definitely took\nrisks that seemed like they could have landed him in jail for using someone\nelse\u2019s money,\u201d he said.\nIn December 2008, Musk mounted simultaneous campaigns to try to\nsave his companies. He heard a rumor that NASA was on the verge of\nawarding a contract to resupply the space station. SpaceX\u2019s fourth launch\nhad put it in a position to receive some of this money, which was said to be\nin excess of $1 billion. Musk reached out through some back channels in\nWashington and found out that SpaceX might even be a front-runner for the\ndeal. Musk began doing everything in his power to assure people that the\ncompany could meet the challenge of getting a capsule to the ISS. As for\nTesla, Musk had to go to his existing investors and ask them to pony up for\nanother round of funding that needed to close by Christmas Eve to avoid\nbankruptcy. To give the investors some measure of confidence, Musk made\na last-ditch effort to raise all the personal funds he could and put them into\nthe company. He took out a loan from SpaceX, which NASA approved, and\nearmarked the money for Tesla. Musk went to the secondary markets to try\nto sell some of his shares in SolarCity. He also seized about $15 million that\ncame through when Dell acquired a data center software start-up called\nEverdream, founded by Musk\u2019s cousins, in which he had invested. \u201cIt was\nlike the fucking Matrix,\u201d Musk said, describing his financial maneuvers.\n\u201cThe Everdream deal really saved my butt.\u201d\nMusk had cobbled together $20 million, and asked Tesla\u2019s existing\ninvestors\u2014since no new investors materialized\u2014to match that figure. The\ninvestors agreed, and on December 3, 2008, they were in the process of\nfinalizing the paperwork for the funding round when Musk noticed a\nproblem. VantagePoint Capital Partners had signed all of the paperwork except for one crucial page. Musk phoned up Alan Salzman, VantagePoint\u2019s\ncofounder and managing partner, to ask about the situation. Salzman\ninformed Musk that the firm had a problem with the investment round\nbecause it undervalued Tesla. \u201cI said, \u2018I\u2019ve got an excellent solution then.\nTake my entire portion of the deal. I had a real hard time coming up with\nthe money. Based on the cash we have in the bank right now, we will\nbounce payroll next week. So unless you\u2019ve got another idea, can you either\njust participate as much as you\u2019d like, or allow the round to go through\nbecause otherwise we will be bankrupt.\u2019\u201d Salzman balked and told Musk to\ncome in the following week at 7 A.M. to present to VantagePoint\u2019s top brass.\nNot having a week of time to work with, Musk asked to come in the next\nday, and Salzman refused that offer, forcing Musk to continue taking on\nloans. \u201cThe only reason he wanted the meeting at his office was for me to\ncome on bended knee begging for money so he could say, \u2018No,\u2019\u201d Musk\ntheorized. \u201cWhat a fuckhead.\u201d\nVantagePoint declined to speak about this period, but Musk believed\nthat Salzman\u2019s tactics were part of a mission to bankrupt Tesla. Musk feared\nthat VantagePoint would oust him as CEO, recapitalize Tesla, and emerge as\nthe major owner of the carmaker. It could then sell Tesla to a Detroit\nautomaker or focus on selling electric drivetrains and battery packs instead\nof making cars. Such reasoning would have been quite practical from a\nbusiness standpoint but did not match up with Musk\u2019s goals for Tesla.\n\u201cVantagePoint was forcing that wisdom down the throat of an entrepreneur\nwho wanted to do something bigger and bolder,\u201d said Steve Jurvetson, a\npartner at Draper Fisher Jurvetson and Tesla investor. \u201cMaybe they\u2019re used\nto a CEO buckling, but Elon doesn\u2019t do that.\u201d Instead, Musk took another\nhuge risk. Tesla recharacterized the funding as a debt round rather than an\nequity round, knowing that VantagePoint could not interfere with a debt\ndeal. The tricky part of this strategy was that investors like Jurvetson who\nwanted to help Tesla were put in a bind because venture capital firms are\nnot structured to do debt deals, and convincing their backers to alter their\nnormal rules of engagement for a company that could very well go bankrupt\nin a matter of days would be a very tough ask. Knowing this, Musk bluffed.\nHe told the investors that he would take another loan from SpaceX and fund\nthe entire round\u2014all $40 million\u2014himself. The tactic worked. \u201cWhen you\nhave scarcity, it naturally reinforces greed and leads to more interest,\u201d\nJurvetson said. \u201cIt was also easier for us to go back to our firms and say, \u2018Here is the deal. Go or no go?\u2019\u201d The deal ended up closing on Christmas\nEve, hours before Tesla would have gone bankrupt. Musk had just a few\nhundred thousand dollars left and could not have made payroll the next day.\nMusk ultimately put in $12 million, and the investment firms put up the\nrest. As for Salzman, Musk said, \u201cHe should be ashamed of himself.\u201d\nAt SpaceX, Musk and the company\u2019s top executives had spent most of\nDecember in a state of fear. According to reports in the press, SpaceX, the\nonetime front-runner for the large NASA contract, had suddenly lost favor\nwith the space agency. Michael Griffin, who had once almost been a\ncofounder of SpaceX, was the head of NASA and had turned on Musk.\nGriffin did not care for Musk\u2019s aggressive business tactics, seeing him as\nborderline unethical. Others have suggested that Griffin ended up being\njealous of Musk and SpaceX.* On December 23, 2008, however, SpaceX\nreceived a shock. People inside NASA had backed SpaceX to become a\nsupplier for the ISS. The company received $1.6 billion as payment for\ntwelve flights to the space station. Staying with Kimbal in Boulder,\nColorado, for the holidays, Musk broke down in tears as the SpaceX and\nTesla transactions processed. \u201cI hadn\u2019t had an opportunity to buy a\nChristmas present for Talulah or anything,\u201d he said. \u201cI went running down\nthe fucking street in Boulder, and the only place that was open sold these\nshitty trinkets, and they were about to close. The best thing I could find\nwere these plastic monkeys with coconuts\u2014those \u2018see no evil, hear no evil\u2019\nmonkeys.\u201d\nFor Gracias, the Tesla and SpaceX investor and Musk\u2019s friend, the 2008\nperiod told him everything he would ever need to know about Musk\u2019s\ncharacter. He saw a man who arrived in the United States with nothing, who\nhad lost a child, who was being pilloried in the press by reporters and his\nex-wife and who verged on having his life\u2019s work destroyed. \u201cHe has the\nability to work harder and endure more stress than anyone I\u2019ve ever met,\u201d\nGracias said. \u201cWhat he went through in 2008 would have broken anyone\nelse. He didn\u2019t just survive. He kept working and stayed focused.\u201d That\nability to stay focused in the midst of a crisis stands as one of Musk\u2019s main\nadvantages over other executives and competitors. \u201cMost people who are\nunder that sort of pressure fray,\u201d Gracias said. \u201cTheir decisions go bad. Elon\ngets hyperrational. He\u2019s still able to make very clear, long-term decisions.\nThe harder it gets, the better he gets. Anyone who saw what he went through firsthand came away with more respect for the guy. I\u2019ve just never\nseen anything like his ability to take pain.\u201d 9 LIFTOFF\nT\nHE FALCON 9 HAS BECOME SPACEX\u2019S WORKHORSE. The rocket\nlooks\u2014let\u2019s face it\u2014like a giant white phallus. It stands 224.4 feet tall, is\n12 feet across, and weighs 1.1 million pounds. The rocket is powered by\nnine engines arranged in an \u201coctaweb\u201d pattern at its base with one engine in\nthe center and eight others encircling it. The engines connect to the first\nstage, or the main body of the rocket, which bears the blue SpaceX insignia\nand an American flag. The shorter second stage of the rocket sits on top of\nthe first and is the one that actually ends up doing things in space. It can be\noutfitted with a rounded container for carrying satellites or a capsule\ncapable of transporting humans. By design, there\u2019s nothing particularly\nflashy about the Falcon 9\u2019s outward appearance. It\u2019s the spaceship\nequivalent of an Apple laptop or a Braun kettle\u2014an elegant, purposeful\nmachine stripped of frivolity and waste.\nSpaceX sometimes uses Vandenberg Air Force Base in Southern\nCalifornia to send up these Falcon 9 rockets. Were it not owned by the\nmilitary, the base would be a resort. The Pacific Ocean runs for miles along\nits border, and its grounds have wide-open shrubby fields dotted by green\nhills. Nestled into one hilly spot just at the ocean\u2019s edge are a handful of\nlaunchpads. On launch days, the white Falcon 9 breaks up the blue and\ngreen landscape, pointing skyward and leaving no doubt about its\nintentions.\nAbout four hours before a launch, the Falcon 9 starts getting filled with\nan immense amount of liquid oxygen and rocket-grade kerosene. Some of\nthe liquid oxygen vents out of the rocket as it awaits launch and is kept so\ncold that it boils off on contact with the metal and air, forming white plumes\nthat stream down the rocket\u2019s sides. This gives the impression of the Falcon\n9 huffing and puffing as it limbers up before the journey. The engineers\ninside of SpaceX\u2019s mission control monitor these fuel systems and all\nmanner of other items. They chat back and forth through headsets and begin\ncycling through their launch checklist, consumed by what people in the business call \u201cgo fever\u201d as they move from one approval to the next. Ten\nminutes before launch, the humans step out of the way and leave the\nremaining processes up to automated machines. Everything goes quiet, and\nthe tension builds until right before the main event. That\u2019s when, out of\nnowhere, the Falcon 9 breaks the silence by letting out a loud gasp.\nA white latticed support structure pulls away from its body. The T-\nminus-ten-seconds countdown begins. Nothing much happens from ten\ndown to four. At the count of three, however, the engines ignite, and the\ncomputers conduct a last, oh-so-rapid, health check. Four enormous metal\nclamps hold the rocket down, as computing systems evaluate all nine\nengines and measure if there\u2019s sufficient downward force being produced.\nBy the time zero arrives, the rocket has decided that all is well enough to go\nthrough with its mission, and the clamps release. The rocket goes to war\nwith inertia, and then, with flames surrounding its base and snow-thick\nplumes of the liquid oxygen filling the air, it shoots up. Seeing something so\nlarge hold so straight and steady while suspended in midair is hard for the\nbrain to register. It is foreign, inexplicable. About twenty seconds after\nliftoff, the spectators placed safely a few miles away catch the first faceful\nof the Falcon 9\u2019s rumble. It\u2019s a distinct sound\u2014a sort of staccato crackling\nthat arises from chemicals whipped into a violent frenzy. Pant legs vibrate\nfrom shock waves produced by a stream of sonic booms coming out of the\nFalcon 9\u2019s exhaust. The white rocket climbs higher and higher with\nimpressive stamina. After about a minute, it\u2019s just a red spot in the sky, and\nthen\u2014poof\u2014it\u2019s gone. Only a cynical dullard could come away from\nwitnessing this feeling anything other than wonder at what man can\naccomplish.\nFor Elon Musk, this spectacle has turned into a familiar experience.\nSpaceX has metamorphosed from the joke of the aeronautics industry into\none of its most consistent operators. SpaceX sends a rocket up about once a\nmonth, carrying satellites for companies and nations and supplies to the\nInternational Space Station. Where the Falcon 1 blasting off from Kwajalein\nwas the work of a start-up, the Falcon 9 taking off from Vandenberg is the\nwork of an aerospace superpower. SpaceX can undercut its U.S.\ncompetitors\u2014Boeing, Lockheed Martin, Orbital Sciences\u2014on price by a\nridiculous margin. It also offers U.S. customers a peace of mind that its\nrivals can\u2019t. Where these competitors rely on Russian and other foreign\nsuppliers, SpaceX makes all of its machines from scratch in the United States. Because of its low costs, SpaceX has once again made the United\nStates a player in the worldwide commercial launch market. Its $60 million\nper launch cost is much less than what Europe and Japan charge and trumps\neven the relative bargains offered by the Russians and Chinese, who have\nthe added benefit of decades of sunk government investment into their\nspace programs as well as cheap labor.\nThe United States continues to take great pride in having Boeing\ncompete against Airbus and other foreign aircraft makers. For some reason,\nthough, government leaders and the public have been willing to concede\nmuch of the commercial launch market. It\u2019s a disheartening and\nshortsighted position. The total market for satellites, related services, and\nthe rocket launches needed to carry them to space has exploded over the\npast decade from about $60 billion per year to more than $200 billion.11 A\nnumber of countries pay to send up their own spy, communication, and\nweather satellites. Companies then turn to space for television, Internet,\nradio, weather, navigation, and imaging services. The machines in space\nsupply the fabric of modern life, and they\u2019re going to become more capable\nand interesting at a rapid pace. A whole new breed of satellite makers has\njust appeared on the scene with the ability to answer Google-like queries\nabout our planet. These satellites can zoom in on Iowa and determine when\ncornfields are at peak yields and ready to harvest, and they can count cars in\nWal-Mart parking lots throughout California to calculate shopping demand\nduring the holiday season. The start-ups making these types of innovative\nmachines must often turn to the Russians to get them into space, but\nSpaceX intends to change that.\nThe United States has remained competitive in the most lucrative parts\nof the space industry, building the actual satellites and complementary\nsystems and services to run them. Each year, the United States makes about\none-third of all satellites and takes about 60 percent of the global satellite\nrevenue. The majority of this revenue comes from business done with the\nU.S. government. China, Europe, and Russia account for almost all of the\nremaining satellite sales and launches. It\u2019s expected that China\u2019s role in the\nspace industry will increase, while Russia has vowed to spend $50 billion\non revitalizing its space program. This leaves the United States dealing with\ntwo of its least-favored nations in space matters and doing so without much\nleverage. Case in point: the retirement of the space shuttle made the United\nStates totally dependent on the Russians to get astronauts to the ISS. Russia gets to charge $70 million per person for the trip and to cut the United\nStates off as it sees fit during political rifts. At present, SpaceX looks like\nthe best hope of breaking this cycle and giving back to America its ability to\ntake people into space.\nSpaceX has become the free radical trying to upend everything about\nthis industry. It doesn\u2019t want to handle a few launches per year or to rely on\ngovernment contracts for survival. Musk\u2019s goal is to use manufacturing\nbreakthroughs and launchpad advances to create a drastic drop in the cost of\ngetting things to space. Most significant, he\u2019s been testing rockets that can\npush their payload to space and then return to Earth and land with supreme\naccuracy on a pad floating at sea or even their original launchpad. Instead of\nhaving its rockets break apart after crashing into the sea, SpaceX will use\nreverse thrusters to lower them down softly and reuse them. Within the next\nfew years, SpaceX expects to cut its price to at least one-tenth that of its\nrivals. Reusing its rockets will drive the bulk of this reduction and SpaceX\u2019s\ncompetitive advantage. Imagine one airline that flies the same plane over\nand over again, competing against others that dispose of their planes after\nevery flight.* Through its cost advantages, SpaceX hopes to take over the\nmajority of the world\u2019s commercial launches, and there\u2019s evidence that the\ncompany is on its way toward doing just that. To date, it has flown satellites\nfor Canadian, European, and Asian customers and completed about two\ndozen launches. Its public launch manifest stretches out for a number of\nyears, and SpaceX has more than fifty flights planned, which are all\ntogether worth more than $5 billion. The company remains privately owned\nwith Musk as the largest shareholder alongside outside investors including\nventure capital firms like the Founders Fund and Draper Fisher Jurvetson,\ngiving it a competitive ethos its rivals lack. Since getting past its near-death\nexperience in 2008, SpaceX has been profitable and is estimated to be\nworth $12 billion.\nZip2, PayPal, Tesla, SolarCity\u2014they are all expressions of Musk.\nSpaceX is Musk. Its foibles emanate directly from him, as do its successes.\nPart of this comes from Musk\u2019s maniacal attention to detail and\ninvolvement in every SpaceX endeavor. He\u2019s hands-on to a degree that\nwould make Hugh Hefner feel inadequate. Part of it stems from SpaceX\nbeing the apotheosis of the Cult of Musk. Employees fear Musk. They\nadore Musk. The give up their lives for Musk, and they usually do all of this\nsimultaneously. Musk\u2019s demanding management style can only flourish because of the\notherworldly\u2014in a literal sense\u2014aspirations of the company. While the rest\nof the aerospace industry has been content to keep sending what look like\nrelics from the 1960s into space, SpaceX has made a point of doing just the\nopposite. Its reusable rockets and reusable spaceships look like true twenty-\nfirst-century machines. The modernization of the equipment is not just for\nshow. It reflects SpaceX\u2019s constant push to advance its technology and\nchange the economics of the industry. Musk does not simply want to lower\nthe cost of deploying satellites and resupplying the space station. He wants\nto lower the cost of launches to the point that it becomes economical and\npractical to fly thousands upon thousands of supply trips to Mars and start a\ncolony. Musk wants to conquer the solar system, and, as it stands, there\u2019s\njust one company where you can work if that sort of quest gets you out of\nbed in the morning.\nIt seems unfathomable, but the rest of the space industry has made space\nboring. The Russians, who dominate much of the business of sending things\nand people to space, do so with decades-old equipment. The cramped Soyuz\ncapsule that takes people to the space station has mechanical knobs and\ncomputer screens that appear unchanged from its inaugural 1966 flight.\nCountries new to the space race have mimicked the antiquated Russian and\nAmerican equipment with maddening accuracy. When young people get\ninto the aerospace industry, they\u2019re forced to either laugh or cry at the state\nof the machines. Nothing sucks the fun out of working on a spaceship like\ncontrolling it with mechanisms last seen in a 1960s laundromat. And the\nactual work environment is as outmoded as the machines. Hotshot college\ngraduates have historically been forced to pick between a variety of slow-\nmoving military contractors and interesting but ineffectual start-ups.\nMusk has managed to take these negatives surrounding the aerospace\nbusiness and turn them into gains for SpaceX. He\u2019s presented the company\nas anything but another aerospace contractor. SpaceX is the hip, forward-\nthinking place that\u2019s brought the perks of Silicon Valley\u2014namely frozen\nyogurt, stock options, speedy decision making, and a flat corporate structure\n\u2014to a staid industry. People who know Musk well tend to describe him\nmore as a general than a CEO, and this is apt. He\u2019s built an engineering\narmy by having the pick of just about anyone in the business that SpaceX\nwants. The SpaceX hiring model places some emphasis on getting top marks at\ntop schools. But most of the attention goes toward spotting engineers who\nhave exhibited type A personality traits over the course of their lives. The\ncompany\u2019s recruiters look for people who might excel at robot-building\ncompetitions or who are car-racing hobbyists who have built unusual\nvehicles. The object is to find individuals who ooze passion, can work well\nas part of a team, and have real-world experience bending metal. \u201cEven if\nyou\u2019re someone who writes code for your job, you need to understand how\nmechanical things work,\u201d said Dolly Singh, who spent five years as the\nhead of talent acquisition at SpaceX. \u201cWe were looking for people that had\nbeen building things since they were little.\u201d\nSometimes these people walked through the front door. Other times,\nSingh relied on a handful of enterprising techniques to find them. She\nbecame famous for trawling through academic papers to find engineers with\nvery specific skills, cold-calling researchers at labs and plucking possessed\nengineers out of college. At trade shows and conferences, SpaceX recruiters\nwooed interesting candidates they had spotted with a cloak-and-dagger\nshtick. They would hand out blank envelopes that contained invitations to\nmeet at a specific time and place, usually a bar or restaurant near the event,\nfor an initial interview. The candidates that showed up would discover they\nwere among only a handful of people who been anointed out of all the\nconference attendees. They were immediately made to feel special and\ninspired.\nLike many tech companies, SpaceX subjects potential hires to a gauntlet\nof interviews and tests. Some of the interviews are easygoing chats in which\nboth parties get to feel each other out; others are filled with quizzes that can\nbe quite hard. Engineers tend to face the most rigorous interrogations,\nalthough business types and salesmen are made to suffer, too. Coders who\nexpect to pass through standard challenges have rude awakenings.\nCompanies will typically challenge software developers on the spot by\nasking them to solve problems that require a couple of dozen lines of code.\nThe standard SpaceX problem requires five hundred or more lines of code.\nAll potential employees who make their way to the end of the interview\nprocess then handle one more task. They\u2019re asked to write an essay for\nMusk about why they want to work at SpaceX.\nThe reward for solving the puzzles, acting clever in interviews, and\npenning up a good essay is a meeting with Musk. He interviewed almost every one of SpaceX\u2019s first one thousand hires, including the janitors and\ntechnicians, and has continued to interview the engineers as the company\u2019s\nworkforce swelled. Each employee receives a warning before going to meet\nwith Musk. The interview, he or she is told, could last anywhere from thirty\nseconds to fifteen minutes. Elon will likely keep on writing e-mails and\nworking during the initial part of the interview and not speak much. Don\u2019t\npanic. That\u2019s normal. Eventually, he will turn around in his chair to face\nyou. Even then, though, he might not make actual eye contact with you or\nfully acknowledge your presence. Don\u2019t panic. That\u2019s normal. In due course,\nhe will speak to you. From that point, the tales of engineers who have\ninterviewed with Musk run the gamut from torturous experiences to the\nsublime. He might ask one question or he might ask several. You can be\nsure, though, that he will roll out the Riddle: \u201cYou\u2019re standing on the\nsurface of the Earth. You walk one mile south, one mile west, and one mile\nnorth. You end up exactly where you started. Where are you?\u201d One answer\nto that is the North Pole, and most of the engineers get it right away. That\u2019s\nwhen Musk will follow with \u201cWhere else could you be?\u201d The other answer\nis somewhere close to the South Pole where, if you walk one mile south, the\ncircumference of the Earth becomes one mile. Fewer engineers get this\nanswer, and Musk will happily walk them through that riddle and others and\ncite any relevant equations during his explanations. He tends to care less\nabout whether or not the person gets the answer than about how they\ndescribe the problem and their approach to solving it.\nWhen speaking to potential recruits, Singh tried to energize them and be\nup front about the demands of SpaceX and of Musk at the same time. \u201cThe\nrecruiting pitch was SpaceX is special forces,\u201d she said. \u201cIf you want as\nhard as it gets, then great. If not, then you shouldn\u2019t come here.\u201d Once at\nSpaceX, the new employees found out very quickly if they were indeed up\nfor the challenge. Many of them would quit within the first few months\nbecause of the ninety-plus-hour workweeks. Others quit because they could\nnot handle just how direct Musk and the other executives were during\nmeetings. \u201cElon doesn\u2019t know about you and he hasn\u2019t thought through\nwhether or not something is going to hurt your feelings,\u201d Singh said. \u201cHe\njust knows what the fuck he wants done. People who did not normalize to\nhis communication style did not do well.\u201d\nThere\u2019s an impression that SpaceX suffers from incredibly high\nturnover, and the company has without question churned through a fair number of bodies. Many of the key executives who helped start the\ncompany, however, have hung on for a decade or more. Among the rank-\nand-file engineers, most people stay on for at least five years to have their\nstock options vest and to see their projects through. This is typical behavior\nfor any technology company. SpaceX and Musk also seem to inspire an\nunusual level of loyalty. Musk has managed to conjure up that Steve Jobs\u2013\nlike zeal among his troops. \u201cHis vision is so clear,\u201d Singh said. \u201cHe almost\nhypnotizes you. He gives you the crazy eye, and it\u2019s like, yes, we can get to\nMars.\u201d Take that a bit further and you arrive at a pleasure-pain,\nsadomasochistic vibe that comes with working for Musk. Numerous people\ninterviewed for this book decried the work hours, Musk\u2019s blunt style, and\nhis sometimes ludicrous expectations. Yet almost every person\u2014even those\nwho had been fired\u2014still worshipped Musk and talked about him in terms\nusually reserved for superheroes or deities.\nSpaceX\u2019s original headquarters in El Segundo were not quite up to the\ncompany\u2019s desired image as a place where the cool kids want to work. This\nis not a problem for SpaceX\u2019s new facility in Hawthorne. The building\u2019s\naddress is 1 Rocket Road, and it has the Hawthorne Municipal Airport and\nseveral tooling and manufacturing companies as neighbors. While the\nSpaceX building resembles the others in size and shape, its all-white color\nmakes it the obvious outlier. The structure looks like a gargantuan,\nrectangular glacier that\u2019s been planted in the midst of a particularly soulless\nportion of Los Angeles County\u2019s sprawl.\nVisitors to SpaceX have to walk past a security guard and through a\nsmall executive parking lot where Musk parks his black Model S, which\nflanks the building\u2019s entryway. The front doors are reflective and hide\nwhat\u2019s on the inside, which is more white. There are white walls in the\nfoyer, a funky white table in the waiting area, and a white check-in desk\nwith a pair of orchids sitting in white pots. After going through the\nregistration process, guests are given a name badge and led into the main\nSpaceX office space. Musk\u2019s cubicle\u2014a supersize unit\u2014sits to the right\nwhere he has a couple of celebratory Aviation Week magazine covers up on\nthe wall, pictures of his boys, next to a huge flat-screen monitor, and\nvarious knickknacks on his desk, including a boomerang, some books, a\nbottle of wine, and a giant samurai sword named Lady Vivamus, which\nMusk received when he won the Heinlein Prize, an award given for big\nachievements in commercial space. Hundreds of other people work in cubicles amid the big, wide-open area, most of them executives, engineers,\nsoftware developers, and salespeople tapping away on their computers. The\nconference rooms that surround their desks all have space-themed names\nlike Apollo or Wernher von Braun and little nameplates that explain the\nlabel\u2019s significance. The largest conference rooms have ultramodern chairs\n\u2014high-backed, sleek red jobs that surround large glass tables\u2014while\npanoramic photos of a Falcon 1 taking off from Kwaj or the Dragon capsule\ndocking with the ISS hang on the walls in the background.\nTake away the rocket swag and the samurai sword and this central part\nof the SpaceX office looks just like what you might find at your run-of-the-\nmill Silicon Valley headquarters. The same thing cannot be said for what\nvisitors encounter as they pass through a pair of double doors into the heart\nof the SpaceX factory.\nThe 550,000-square-foot factory floor is difficult to process at first\nglance. It\u2019s one continuous space with grayish epoxied floors, white walls,\nand white support columns. A small city\u2019s worth of stuff\u2014people,\nmachines, noise\u2014has been piled into this area. Just near the entryway, one\nof the Dragon capsules that has gone to the ISS and returned to Earth hangs\nfrom the ceiling with black burn marks running down its side. Just under the\ncapsule on the ground are a pair of the twenty-five-foot-long landing legs\nbuilt by SpaceX to let the Falcon rocket come to a gentle rest on the ground\nafter a flight so it can be flown again. To the left side of this entryway area\nthere\u2019s a kitchen, and to the right side there\u2019s the mission control room. It\u2019s\na closed-off area with expansive glass windows and fronted by wall-size\nscreens for tracking a rocket\u2019s progress. It has four rows of desks with about\nten computers each for the mission control staff. Step a bit farther into the\nfactory and there are a handful of industrial work areas separated from each\nother in the most informal of ways. In some spots there are blue lines on the\nfloor to mark off an area and in other spots blue workbenches have been\narranged in squares to cordon off the space. It\u2019s a common sight to have one\nof the Merlin engines raised up in the middle of one of these work areas\nwith a half dozen technicians wiring it up and tuning its bits and pieces.\nJust behind these workspaces is a glass-enclosed square big enough to\nfit two of the Dragon capsules. This is a clean room where people must\nwear lab coats and hairnets to fiddle with the capsules without\ncontaminating them. About forty feet to the left, there are several Falcon 9\nrockets lying next to each other horizontally that have been painted and await transport. There are some areas tucked in between all of this that have\nblue walls and appear to have been covered by fabric. These are top-secret\nzones where SpaceX might be working on a fanciful astronaut\u2019s outfit or\nrocket part that it has to hide from visitors and employees not tied to the\nprojects. There\u2019s a large area off to the side where SpaceX builds all of its\nelectronics, another area for creating specialized composite materials, and\nanother for making the bus-sized fairings that wrap around the satellites.\nHundreds of people move about at the same time through the factory\u2014a\nmix of gritty technicians with tattoos and bandanas, and young, white-collar\nengineers. The sweaty smell of kids who have just come off the playground\npermeates the building and hints at its nonstop activity.\nMusk has left his personal touches throughout the factory. There are\nsmall things like the data center that has been bathed in blue lights to give it\na sci-fi feel. The refrigerator-sized computers under the lights have been\nlabeled with big block letters to make it look like they were made by\nCyberdyne Systems, the fictional company from the Terminator movie\nfranchise. Near the elevators, Musk has placed a glowing life-size Iron Man\nfigure. Surely the factory\u2019s most Muskian element is the office space that\nhas been built smack-dab in its center. This is a three-story glass structure\nwith meeting rooms and desks that rises up between various welding and\nconstruction areas. It looks and feels bizarre to have a see-through office\ninside this hive of industry. Musk, though, wanted his engineers to watch\nwhat was going on with the machines at all times and to make sure they had\nto walk through the factory and talk to the technicians on the way to their\ndesks.\nThe factory is a temple devoted to what SpaceX sees as its major\nweapon in the rocket-building game, in-house manufacturing. SpaceX\nmanufactures between 80 percent and 90 percent of its rockets, engines,\nelectronics, and other parts. It\u2019s a strategy that flat-out dumbfounds\nSpaceX\u2019s competitors, like United Launch Alliance, or ULA, which openly\nbrags about depending on more than 1,200 suppliers to make its end\nproducts. (ULA, a partnership between Lockheed Martin and Boeing, sees\nitself as an engine of job creation rather than a model of inefficiency.)\nA typical aerospace company comes up with the list of parts that it\nneeds for a launch system and then hands off their design and specifications\nto myriad third parties who then actually build the hardware. SpaceX tends\nto buy as little as possible to save money and because it sees depending on suppliers\u2014especially foreign ones\u2014as a weakness. This approach comes\noff as excessive at first blush. Companies have made things like radios and\npower distribution units for decades. Reinventing the wheel for every\ncomputer and machine on a rocket could introduce more chances for error\nand, in general, be a waste of time. But for SpaceX, the strategy works. In\naddition to building its own engines, rocket bodies, and capsules, SpaceX\ndesigns its own motherboards and circuits, sensors to detect vibrations,\nflight computers, and solar panels. Just by streamlining a radio, for instance,\nSpaceX\u2019s engineers have found that they can reduce the weight of the\ndevice by about 20 percent. And the cost savings for a homemade radio are\ndramatic, dropping from between $50,000 to $100,000 for the industrial-\ngrade equipment used by aerospace companies to $5,000 for SpaceX\u2019s unit.\nIt\u2019s hard to believe these kinds of price differentials at first, but there are\ndozens if not hundreds of places where SpaceX has secured such savings.\nThe equipment at SpaceX tends to be built out of readily available\nconsumer electronics as opposed to \u201cspace grade\u201d equipment used by others\nin the industry. SpaceX has had to work for years to prove to NASA that\nstandard electronics have gotten good enough to compete with the more\nexpensive, specialized gear trusted in years past. \u201cTraditional aerospace has\nbeen doing things the same way for a very, very long time,\u201d said Drew\nEldeen, a former SpaceX engineer. \u201cThe biggest challenge was convincing\nNASA to give something new a try and building a paper trail that showed\nthe parts were high enough quality.\u201d To prove that it\u2019s making the right\nchoice to NASA and itself, SpaceX will sometimes load a rocket with both\nthe standard equipment and prototypes of its own design for testing during\nflight. Engineers then compare the performance characteristics of the\ndevices. Once a SpaceX design equals or outperforms the commercial\nproducts, it becomes the de facto hardware.\nThere have also been numerous times when SpaceX has done\npioneering work on advancing very complex hardware systems. A classic\nexample of this is one of the factory\u2019s weirder-looking contraptions, a two-\nstory machine designed to perform what\u2019s known as friction stir welding.\nThe machine allows SpaceX to automate the welding process for massive\nsheets of metal like the ones that make up the bodies of the Falcon rockets.\nAn arm takes one of the rocket\u2019s body panels, lines it up against another\nbody panel, and then joins them together with a weld that could run twenty\nfeet or more. Aerospace companies typically try to avoid welds whenever possible because they create weaknesses in the metal, and that\u2019s limited the\nsize of metal sheets they can use and forced other design constraints. From\nthe early days of SpaceX, Musk pushed the company to master friction stir\nwelding, in which a spinning head is smashed at high speeds into the join\nbetween two pieces of metal in a bid to make their crystalline structures\nmerge. It\u2019s as if you heated two sheets of aluminum foil and then joined\nthem by putting your thumb down on the seam and twisting the metal\ntogether. This type of welding tends to result in much stronger bonds than\ntraditional welds. Companies had performed friction stir welding before but\nnot on structures as large as a rocket\u2019s body or to the degree to which\nSpaceX has used the technique. As a result of its trials and errors, SpaceX\ncan now join large, thin sheets of metal and shave hundreds of pounds off\nthe weight of the Falcon rockets, as it\u2019s able to use lighter-weight alloys and\navoid using rivets, fasteners, and other support structures. Musk\u2019s\ncompetitors in the auto industry might soon need to do the same because\nSpaceX has transferred some of the equipment and techniques to Tesla. The\nhope is that Tesla will be able to make lighter, stronger cars.\nThe technology has proven so valuable that SpaceX\u2019s competitors have\nstarted to copy it and have tried to poach some of the company\u2019s experts in\nthe field. Blue Origin, Jeff Bezos\u2019s secretive rocket company, has been\nparticularly aggressive, hiring away Ray Miryekta, one of the world\u2019s\nforemost friction stir welding experts and igniting a major rift with Musk.\n\u201cBlue Origin does these surgical strikes on specialized talent* offering like\ndouble their salaries. I think it\u2019s unnecessary and a bit rude,\u201d Musk said.\nWithin SpaceX, Blue Origin is mockingly referred to as BO and at one\npoint the company created an e-mail filter to detect messages with \u201cblue\u201d\nand \u201corigin\u201d to block the poaching. The relationship between Musk and\nBezos has soured, and they no longer chat about their shared ambition of\ngetting to Mars. \u201cI do think Bezos has an insatiable desire to be King\nBezos,\u201d Musk said. \u201cHe has a relentless work ethic and wants to kill\neverything in e-commerce. But he\u2019s not the most fun guy, honestly.\u201d*\nIn the early days of SpaceX, Musk knew little about the machines and\namount of grunt work that goes into making rockets. He rebuffed requests\nto buy specialized tooling equipment, until the engineers could explain in\nclear terms why they needed certain things and until experience taught him\nbetter. Musk also had yet to master some of the management techniques for\nwhich he would become both famous and to some degree infamous. Musk\u2019s growth as a CEO and rocket expert occurred alongside\nSpaceX\u2019s maturation as a company. At the start of the Falcon 1 journey,\nMusk was a forceful software executive trying to learn some basic things\nabout a very different world. At Zip2 and PayPal, he felt comfortable\nstanding up for his positions and directing teams of coders. At SpaceX, he\nhad to pick things up on the job. Musk initially relied on textbooks to form\nthe bulk of his rocketry knowledge. But as SpaceX hired one brilliant\nperson after another, Musk realized he could tap into their stores of\nknowledge. He would trap an engineer in the SpaceX factory and set to\nwork grilling him about a type of valve or specialized material. \u201cI thought at\nfirst that he was challenging me to see if I knew my stuff,\u201d said Kevin\nBrogan, one of the early engineers. \u201cThen I realized he was trying to learn\nthings. He would quiz you until he learned ninety percent of what you\nknow.\u201d People who have spent significant time with Musk will attest to his\nabilities to absorb incredible quantities of information with near-flawless\nrecall. It\u2019s one of his most impressive and intimidating skills and seems to\nwork just as well in the present day as it did when he was a child\nvacuuming books into his brain. After a couple of years running SpaceX,\nMusk had turned into an aerospace expert on a level that few technology\nCEOs ever approach in their respective fields. \u201cHe was teaching us about\nthe value of time, and we were teaching him about rocketry,\u201d Brogan said.\nIn regards to time, Musk may well set more aggressive delivery targets\nfor very difficult-to-make products than any executive in history. Both his\nemployees and the public have found this to be one of the more jarring\naspects of Musk\u2019s character. \u201cElon has always been optimistic,\u201d Brogan\nsaid. \u201cThat\u2019s the nice word. He can be a downright liar about when things\nneed to get done. He will pick the most aggressive time schedule\nimaginable assuming everything goes right, and then accelerate it by\nassuming that everyone can work harder.\u201d\nMusk has been pilloried by the press for setting and then missing\nproduct delivery dates. It\u2019s one of the habits that got him in the most trouble\nas SpaceX and Tesla tried to bring their first products to market. Time and\nagain, Musk found himself making a public appearance where he had to\ncome up with a new batch of excuses for a delay. Reminded about the initial\n2003 target date to fly the Falcon 1, Musk acted shocked. \u201cAre you\nserious?\u201d he said. \u201cWe said that? Okay, that\u2019s ridiculous. I think I just didn\u2019t\nknow what the hell I was talking about. The only thing I had prior experience in was software, and, yeah, you can write a bunch of software\nand launch a website in a year. No problem. This isn\u2019t like software. It\ndoesn\u2019t work that way with rockets.\u201d Musk simply cannot help himself.\nHe\u2019s an optimist by nature, and it can feel like he makes calculations for\nhow long it will take to do something based on the idea that things will\nprogress without flaw at every step and that all the members of his team\nhave Muskian abilities and work ethics. As Brogan joked, Musk might\nforecast how long a software project will take by timing the amount of\nseconds needed physically to write a line of code and then extrapolating that\nout to match however many lines of code he expects the final piece of\nsoftware to be. It\u2019s an imperfect analogy but one that does not seem that far\noff from Musk\u2019s worldview. \u201cEverything he does is fast,\u201d Brogan said. \u201cHe\npees fast. It\u2019s like a fire hose\u2014three seconds and out. He\u2019s authentically in a\nhurry.\u201d\nAsked about his approach, Musk said,\nI certainly don\u2019t try to set impossible goals. I think impossible\ngoals are demotivating. You don\u2019t want to tell people to go through a\nwall by banging their head against it. I don\u2019t ever set intentionally\nimpossible goals. But I\u2019ve certainly always been optimistic on time\nframes. I\u2019m trying to recalibrate to be a little more realistic.\nI don\u2019t assume that it\u2019s just like 100 of me or something like that.\nI mean, in the case of the early SpaceX days, it would have been just\nthe lack of understanding of what it takes to develop a rocket. In that\ncase I was off by, say, 200 percent. I think future programs might be\noff by anywhere from like 25 percent to 50 percent as opposed to\n200 percent.\nSo, I think generally you do want to have a timeline where,\nbased on everything you know about, the schedule should be X, and\nyou execute towards that, but with the understanding that there will\nbe all sorts of things that you don\u2019t know about that you will\nencounter that will push the date beyond that. It doesn\u2019t mean that\nyou shouldn\u2019t have tried to aim for that date from the beginning\nbecause aiming for something else would have been an arbitrary\ntime increase.\nIt\u2019s different to say, \u201cWell, what do you promise people?\u201d\nBecause you want to try to promise people something that includes schedule margin. But in order to achieve the external promised\nschedule, you\u2019ve got to have an internal schedule that\u2019s more\naggressive than that. Sometimes you still miss the external schedule.\nSpaceX, by the way, is not alone here. Being late is par for the\ncourse in the aerospace industry. It\u2019s not a question of if it\u2019s late, it\u2019s\nhow late will the program be. I don\u2019t think an aerospace program\nhas been completed on time since bloody World War II.\nDealing with the epically aggressive schedules and Musk\u2019s expectations\nhas required SpaceX\u2019s engineers to develop a variety of survival techniques.\nMusk often asks for highly detailed proposals for how projects will be\naccomplished. The employees have learned never to break the time needed\nto accomplish something down into months or weeks. Musk wants day-by-\nday and hour-by-hour forecasts and sometimes even minute-by-minute\ncountdowns, and the fallout from missed schedules is severe. \u201cYou had to\nput in when you would go to the bathroom,\u201d Brogan said. \u201cI\u2019m like, \u2018Elon,\nsometimes people need to take a long dump.\u2019\u201d SpaceX\u2019s top managers work\ntogether to, in essence, create fake schedules that they know will please\nMusk but that are basically impossible to achieve. This would not be such a\nhorrible situation if the targets were kept internal. Musk, however, tends to\nquote these fake schedules to customers, unintentionally giving them false\nhope. Typically, it falls to Gwynne Shotwell, SpaceX\u2019s president, to clean\nup the resulting mess. She will either need to ring up a customer to give\nthem a more realistic timeline or concoct a litany of excuses to explain\naway the inevitable delays. \u201cPoor Gwynne,\u201d Brogan said. \u201cJust to hear her\non the phone with the customers is agonizing.\u201d\nThere can be no question that Musk has mastered the art of getting the\nmost out of his employees. Interview three dozen SpaceX engineers and\neach one of them will have picked up on a managerial nuance that Musk has\nused to get people to meet his deadlines. One example from Brogan: Where\na typical manager may set the deadline for the employee, Musk guides his\nengineers into taking ownership of their own delivery dates. \u201cHe doesn\u2019t\nsay, \u2018You have to do this by Friday at two P.M.,\u2019\u201d Brogan said. \u201cHe says, \u2018I\nneed the impossible done by Friday at two P.M. Can you do it?\u2019 Then, when\nyou say yes, you are not working hard because he told you to. You\u2019re\nworking hard for yourself. It\u2019s a distinction you can feel. You have signed\nup to do your own work.\u201d And by recruiting hundreds of bright, self- motivated people, SpaceX has maximized the power of the individual. One\nperson putting in a sixteen-hour day ends up being much more effective\nthan two people working eight-hour days together. The individual doesn\u2019t\nhave to hold meetings, reach a consensus, or bring other people up to speed\non a project. He just keeps working and working and working. The ideal\nSpaceX employee is someone like Steve Davis, the director of advanced\nprojects at SpaceX. \u201cHe\u2019s been working sixteen hours a day every day for\nyears,\u201d Brogan said. \u201cHe gets more done than eleven people working\ntogether.\u201d\nTo find Davis, Musk called a teaching assistant* in Stanford\u2019s\naeronautics department and asked him if there were any hardworking,\nbright master\u2019s and doctoral candidates who didn\u2019t have families. The TA\npointed Musk to Davis, who was pursuing a master\u2019s degree in aerospace\nengineering to add to degrees in finance, mechanical engineering, and\nparticle physics. Musk called Davis on a Wednesday and offered him a job\nthe following Friday. Davis was the twenty-second SpaceX hire and has\nended up the twelfth most senior person still at the company. He turned\nthirty-five in 2014.\nDavis did his tour of duty on Kwaj and considered it the greatest time of\nhis life. \u201cEvery night, you could either sleep by the rocket in this tent shelter\nwhere the geckos crawled all over you or take this one-hour boat ride that\nmade you seasick back to the main island,\u201d he said. \u201cEvery night, you had\nto pick the pain that you remembered least. You got so hot and exhausted. It\nwas just amazing.\u201d After working on the Falcon 1, Davis moved to the\nFalcon 9 and then Dragon.\nThe Dragon capsule took SpaceX four years to design. It\u2019s likely the\nfastest project of its ilk done in the history of the aerospace industry. The\nproject started with Musk and a handful of engineers, most of them under\nthirty years old, and peaked at one hundred people.* They cribbed from past\ncapsule work and read over every paper published by NASA and other\naeronautics bodies around projects like Gemini and Apollo. \u201cIf you go\nsearch for something like Apollo\u2019s reentry guidance algorithm, there are\nthese great databases that will just spit out the answer,\u201d Davis said. The\nengineers at SpaceX then had to figure out how to advance these past efforts\nand bring the capsule into the modern age. Some of the areas of\nimprovement were obvious and easily accomplished, while others required\nmore ingenuity. Saturn 5 and Apollo had colossal computing bays that produced only a fraction of the computer horsepower that can be achieved\ntoday on, say, an iPad. The SpaceX engineers knew they could save a lot of\nroom by cutting out some of the computers while also adding capabilities\nwith their more powerful equipment. The engineers decided that while\nDragon would look a lot like Apollo, it would have steeper wall angles, to\nclear space for gear and for the astronauts that the company hoped to fly.\nSpaceX also got the recipe for its heat shield material, called PICA, through\na deal with NASA. The SpaceX engineers found out how to make the PICA\nmaterial less expensively and improved the underlying recipe so that\nDragon\u2014from day one\u2014could withstand the heat of a reentry coming back\nfrom Mars.* The total cost for Dragon came in at $300 million, which\nwould be on the order of 10 to 30 times less than capsule projects built by\nother companies. \u201cThe metal comes in, we roll it out, weld it, and make\nthings,\u201d Davis said. \u201cWe build almost everything in-house. That is why the\ncosts have come down.\u201d\nDavis, like Brogan and plenty of other SpaceX engineers, has had Musk\nask for the seemingly impossible. His favorite request dates back to 2004.\nSpaceX needed an actuator that would trigger the gimbal action used to\nsteer the upper stage of Falcon 1. Davis had never built a piece of hardware\nbefore in his life and naturally went out to find some suppliers who could\nmake an electromechanical actuator for him. He got a quote back for\n$120,000. \u201cElon laughed,\u201d Davis said. \u201cHe said, \u2018That part is no more\ncomplicated than a garage door opener. Your budget is five thousand\ndollars. Go make it work.\u2019\u201d Davis spent nine months building the actuator.\nAt the end of the process, he toiled for three hours writing an e-mail to\nMusk covering the pros and cons of the device. The e-mail went into gory\ndetail about how Davis had designed the part, why he had made various\nchoices, and what its cost would be. As he pressed send, Davis felt anxiety\nsurge through his body knowing that he\u2019d given his all for almost a year to\ndo something an engineer at another aerospace company would not even\nattempt. Musk rewarded all of this toil and angst with one of his standard\nresponses. He wrote back, \u201cOk.\u201d The actuator Davis designed ended up\ncosting $3,900 and flew with Falcon 1 into space. \u201cI put every ounce of\nintellectual capital I had into that e-mail and one minute later got that\nsimple response,\u201d Davis said. \u201cEveryone in the company was having that\nsame experience. One of my favorite things about Elon is his ability to\nmake enormous decisions very quickly. That is still how it works today.\u201d Kevin Watson can attest to that. He arrived at SpaceX in 2008 after\nspending twenty-four years at NASA\u2019s Jet Propulsion Laboratory. Watson\nworked on a wide variety of projects at JPL, including building and testing\ncomputing systems that could withstand the harsh conditions of space. JPL\nwould typically buy expensive, specially toughened computers, and this\nfrustrated Watson. He daydreamed about ways to handcraft much cheaper,\nequally effective computers. While having his job interview with Musk,\nWatson learned that SpaceX needed just this type of thinking. Musk wanted\nthe bulk of a rocket\u2019s computing systems to cost no more than $10,000. It\nwas an insane figure by aerospace industry standards, where the avionics\nsystems for a rocket typically cost well over $10 million. \u201cIn traditional\naerospace, it would cost you more than ten thousand dollars just for the\nfood at a meeting to discuss the cost of the avionics,\u201d Watson said.\nDuring the job interview, Watson promised Musk that he could do the\nimprobable and deliver the $10,000 avionics system. He began working on\nmaking the computers for Dragon right after being hired. The first system\nwas called CUCU, pronounced \u201ccuckoo.\u201d This communications box would\ngo inside the International Space Station and communicate back with\nDragon. A number of people at NASA referred to the SpaceX engineers as\n\u201cthe guys in the garage\u201d and were cynical about the start-up\u2019s ability to do\nmuch of anything, including building this type of machine. But SpaceX\nproduced the communication computer in record time, and it ended up as\nthe first system of its kind to pass NASA\u2019s protocol tests on the first try.\nNASA officials were forced to say \u201ccuckoo\u201d over and over again during\nmeetings\u2014a small act of defiance SpaceX had planned all along to torture\nNASA. As the months went on, Watson and other engineers built out the\ncomplete computing systems for Dragon and then adapted the technology\nfor Falcon 9. The result was a fully redundant avionics platform that used a\nmix of off-the-shelf computing gear and products built in-house by SpaceX.\nIt cost a bit more than $10,000 but came close to meeting Musk\u2019s goal.\nSpaceX reinvigorated Watson, who had become disenchanted with\nJPL\u2019s acceptance of wasteful spending and bureaucracy. Musk had to sign\noff on every expenditure over $10,000. \u201cIt was his money that we were\nspending, and he was keeping an eye on it, as he damn well should,\u201d\nWatson said. \u201cHe made sure nothing stupid was happening.\u201d Decisions\nwere made quickly during weekly meetings, and the entire company bought\ninto them. \u201cIt was amazing how fast people would adapt to what came out of those meetings,\u201d Watson said. \u201cThe entire ship could turn ninety degrees\ninstantly. Lockheed Martin could never do anything like that.\u201d Watson\ncontinued:\nElon is brilliant. He\u2019s involved in just about everything. He\nunderstands everything. If he asks you a question, you learn very\nquickly not to go give him a gut reaction. He wants answers that get\ndown to the fundamental laws of physics. One thing he understands\nreally well is the physics of the rockets. He understands that like\nnobody else. The stuff I have seen him do in his head is crazy. He\ncan get in discussions about flying a satellite and whether we can\nmake the right orbit and deliver Dragon at the same time and solve\nall these equations in real time. It\u2019s amazing to watch the amount of\nknowledge he has accumulated over the years. I don\u2019t want to be the\nperson who ever has to compete with Elon. You might as well leave\nthe business and find something else fun to do. He will outmaneuver\nyou, outthink you, and out-execute you.\nOne of Watson\u2019s top discoveries at SpaceX was the test bed on the third\nfloor of the Hawthorne factory. SpaceX has test versions of all the hardware\nand electronics that go into a rocket laid out on metal tables. It has in effect\nreplicated the innards of a rocket end to end in order to run thousands of\nflight simulations. Someone \u201claunches\u201d the rocket from a computer and\nthen every piece of mechanical and computing hardware is monitored with\nsensors. An engineer can tell a valve to open, then check to see if it opened,\nhow quickly it opened, and the level of current running to it. This testing\napparatus lets SpaceX engineers practice ahead of launches and figure out\nhow they would deal with all manner of anomalies. During the actual\nflights, SpaceX has people in the test facility who can replicate errors seen\non Falcon or Dragon and make adjustments accordingly. SpaceX has made\nnumerous changes on the fly with this system. In one case someone spotted\nan error in a software file in the hours right before a launch. SpaceX\u2019s\nengineers changed the file, checked how it affected the test hardware, and,\nwhen no problems were detected, sent the file to the Falcon 9, waiting on\nthe launchpad, all in less than thirty minutes. \u201cNASA wasn\u2019t used to this,\u201d\nWatson said. \u201cIf something went wrong with the shuttle, everyone was just\nresigned to waiting three weeks before they could try and launch again.\u201d12 From time to time, Musk will send out an e-mail to the entire company\nto enforce a new policy or let them know about something that\u2019s bothering\nhim. One of the more famous e-mails arrived in May 2010 with the subject\nline: Acronyms Seriously Suck:\nThere is a creeping tendency to use made up acronyms at SpaceX.\nExcessive use of made up acronyms is a significant impediment to\ncommunication and keeping communication good as we grow is\nincredibly important. Individually, a few acronyms here and there\nmay not seem so bad, but if a thousand people are making these up,\nover time the result will be a huge glossary that we have to issue to\nnew employees. No one can actually remember all these acronyms\nand people don\u2019t want to seem dumb in a meeting, so they just sit\nthere in ignorance. This is particularly tough on new employees.\nThat needs to stop immediately or I will take drastic action\u2014I\nhave given enough warnings over the years. Unless an acronym is\napproved by me, it should not enter the SpaceX glossary. If there is\nan existing acronym that cannot reasonably be justified, it should be\neliminated, as I have requested in the past.\nFor example, there should be no \u201cHTS\u201d [horizontal test stand] or\n\u201cVTS\u201d [vertical test stand] designations for test stands. Those are\nparticularly dumb, as they contain unnecessary words. A \u201cstand\u201d at\nour test site is obviously a *test* stand. VTS-3 is four syllables\ncompared with \u201cTripod,\u201d which is two, so the bloody acronym\nversion actually takes longer to say than the name!\nThe key test for an acronym is to ask whether it helps or hurts\ncommunication. An acronym that most engineers outside of SpaceX\nalready know, such as GUI, is fine to use. It is also ok to make up a\nfew acronyms/contractions every now and again, assuming I have\napproved them, eg MVac and M9 instead of Merlin 1C-Vacuum or\nMerlin 1C-Sea Level, but those need to be kept to a minimum.\nThis was classic Musk. The e-mail is rough in its tone and yet not really\nunwarranted for a guy who just wants things done as efficiently as possible.\nIt obsesses over something that other people might find trivial and yet he\nhas a definite point. It\u2019s comical in that Musk wants all acronym approvals\nto run directly through him, but that\u2019s entirely in keeping with the hands-on management style that has, mainly, worked well at both SpaceX and Tesla.\nEmployees have since dubbed the acronym policy the ASS Rule.\nThe guiding principle at SpaceX is to embrace your work and get stuff\ndone. People who await guidance or detailed instructions languish. The\nsame goes for workers who crave feedback. And the absolute worst thing\nthat someone can do is inform Musk that what he\u2019s asking is impossible. An\nemployee could be telling Musk that there\u2019s no way to get the cost on\nsomething like that actuator down to where he wants it or that there is\nsimply not enough time to build a part by Musk\u2019s deadline. \u201cElon will say,\n\u2018Fine. You\u2019re off the project, and I am now the CEO of the project. I will do\nyour job and be CEO of two companies at the same time. I will deliver it,\u2019\u201d\nBrogan said. \u201cWhat\u2019s crazy is that Elon actually does it. Every time he\u2019s\nfired someone and taken their job, he\u2019s delivered on whatever the project\nwas.\u201d\nIt is jarring for both parties when the SpaceX culture rubs against more\nbureaucratic bodies like NASA, the U.S. Air Force, and the Federal\nAviation Administration. The first inklings of these difficulties appeared on\nKwaj, where government officials sometimes questioned what they saw as\nSpaceX\u2019s cavalier approach to the launch process. There were times when\nSpaceX would want to make a change to its launch procedures and any such\nchange would require a pile of paperwork. SpaceX, for example, would\nhave written down all the steps needed to replace a filter\u2014put on gloves,\nwear safety goggles, remove a nut\u2014and then want to alter this procedure or\nuse a different type of filter. The FAA would need a week to review the new\nprocess before SpaceX could actually go about changing the filter on the\nrocket, a lag that both the engineers and Musk found ridiculous. On one\noccasion after this type of thing happened, Musk laid into an FAA official\nwhile on a conference call with members of the SpaceX team and NASA.\n\u201cIt got hot and heated, and he berated this guy on a personal level for like\nten minutes,\u201d Brogan said.\nMusk did not recall this incident but did remember other confrontations\nwith the FAA. One time he compiled a list of things an FAA subordinate\nhad said during a meeting that Musk found silly and sent the list along to\nthe guy\u2019s boss. \u201cAnd then his dingbat manager sent me this long e-mail\nabout how he had been in the shuttle program and in charge of twenty\nlaunches or something like that and how dare I say that the other guy was\nwrong,\u201d Musk said. \u201cI told him, \u2018Not only is he wrong, and let me rearticulate the reasons, but you\u2019re wrong, and let me articulate the\nreasons.\u2019 I don\u2019t think he sent me another e-mail after that. We\u2019re trying to\nhave a really big impact on the space industry. If the rules are such that you\ncan\u2019t make progress, then you have to fight the rules.\n\u201cThere is a fundamental problem with regulators. If a regulator agrees to\nchange a rule and something bad happens, they could easily lose their\ncareer. Whereas if they change a rule and something good happens, they\ndon\u2019t even get a reward. So, it\u2019s very asymmetric. It\u2019s then very easy to\nunderstand why regulators resist changing the rules. It\u2019s because there\u2019s a\nbig punishment on one side and no reward on the other. How would any\nrational person behave in such a scenario?\u201d\nIn the middle of 2009, SpaceX hired Ken Bowersox, a former astronaut,\nas its vice president of astronaut safety and mission assurance. Bowersox fit\nthe mold of recruit prized by a classic big aerospace company. He had a\ndegree in aerospace engineering from the U.S. Naval Academy, had been a\ntest pilot in the air force, and flew on the space shuttle a handful of times.\nMany people within SpaceX saw his arrival at the company as a good thing.\nHe was considered a diligent, dignified sort who would provide a second set\nof eyes to many of SpaceX\u2019s procedures, checking to make sure the\ncompany went about things in a safe, standardized manner. Bowersox ended\nup smack in the middle of the constant pull and push at SpaceX between\ndoing things efficiently and agonizing over traditional procedures. He and\nMusk were increasingly at odds as the months passed, and Bowersox started\nto feel as if his opinions were being ignored. During one incident in\nparticular, a part made it all the way to the test stand with a major flaw\u2014\ndescribed by one engineer as the equivalent of a coffee cup not having a\nbottom\u2014instead of being caught at the factory. According to observers,\nBowersox argued that SpaceX should go back and investigate the process\nthat led to the mistake and fix its root cause. Musk had already decided that\nhe knew the basis of the problem and dismissed Bowersox after a couple of\nyears on the job. (Bowersox declined to speak on the record about his time\nat SpaceX.) A number of people inside SpaceX saw the Bowersox incident\nas an example of Musk\u2019s hard-charging manner undermining some much-\nneeded process. Musk had a totally different take on the situation, casting\nBowersox as not being up to the engineering demands at SpaceX.\nA handful of high-ranking government officials gave me their candid\ntakes on Musk, albeit without being willing to put their names to the remarks. One found Musk\u2019s treatment of air force generals and military men\nof similar rank appalling. Musk has been known to let even high-ranking\nofficials have it when he thinks they\u2019re off base and is not apologetic about\nthis. Another could not believe it when Musk would call very intelligent\npeople idiots. \u201cImagine the worst possible way that could come out, and it\nwould come out,\u201d this person said. \u201cLife with Elon is like being in a very\nintimate married couple. He can be so gentle and loyal and then really hard\non people when it isn\u2019t necessary.\u201d One former official felt that Musk would\nneed to temper himself better in the years to come if SpaceX was to keep\ncurrying favor with the military and government agencies in its bid to defeat\nthe incumbent contractors. \u201cHis biggest enemy will be himself and the way\nhe treats people,\u201d this person said.\nWhen Musk rubs outsiders the wrong way, Shotwell is often there to try\nto smooth over the situation. Like Musk, she has a salty tongue and a fiery\npersonality, but Shotwell is willing to play the role of the conciliator. These\nskills have allowed her to handle the day-to-day operations at SpaceX,\nleaving Musk to focus on the company\u2019s overall strategy, the product\ndesigns, marketing, and motivating employees. Like all of Musk\u2019s most\ntrusted lieutenants, Shotwell has been willing to stay largely in the\nbackground, do her work, and focus on the company\u2019s cause.\nShotwell grew up in the suburbs of Chicago, the daughter of an artist\n(mom) and a neurosurgeon (dad). She played the part of a bright, pretty girl,\ngetting straight A\u2019s at school and joining the cheerleading squad. Shotwell\nhad not expressed a major inclination toward the sciences and knew only\none version of an engineer\u2014the guy who drives a train. But there were\nclues that she was wired a bit different. She was the daughter who mowed\nthe lawn and helped put the family basketball hoop together. In third grade,\nShotwell developed a brief interest in car engines, and her mom bought a\nbook detailing how they work. Later, in high school, Shotwell\u2019s mom\nforced her to attend a lecture at the Illinois Institute of Technology on a\nSaturday afternoon. As Shotwell listened to one of the panels, she grew\nenamored with a fifty-year-old mechanical engineer. \u201cShe had these\nbeautiful clothes, this suit and shoes that I loved,\u201d Shotwell said. \u201cShe was\ntall and carried off the heels really well.\u201d Shotwell chatted with the engineer\nafter the talk, learning about her job. \u201cThat was the day I decided to become\na mechanical engineer,\u201d she said. Shotwell went on to receive an undergraduate degree in mechanical\nengineering and a master\u2019s degree in applied mathematics from\nNorthwestern University. Then she took a job at Chrysler. It was a type of\nmanagement training program meant for hotshot recent graduates who\nappeared to have leadership potential. Shotwell started out going to auto\nmechanics school\u2014\u201cI loved that\u201d\u2014and then from department to\ndepartment. While working on engines research, Shotwell found that there\nwere two very expensive Cray supercomputers sitting idle because none of\nthe veterans knew how to use them. A short while later, she logged onto the\ncomputers and set them up to run computational fluid dynamics, or CFD,\noperations to simulate the performance of valves and other components.\nThe work kept Shotwell interested, but the environment started to grate on\nher. There were rules for everything, including lots of union regulations\naround who could operate certain machines. \u201cI picked up a tool once, and\ngot written up,\u201d she said. \u201cThen I opened a bottle of liquid nitrogen and got\nwritten up. I started thinking that the job was not what I had anticipated it\nwould be.\u201d\nShotwell pulled out of the Chrysler training program, regrouped at\nhome, and then briefly pursued her doctorate in applied mathematics. While\nback on the Northwestern campus, one of her professors mentioned an\nopportunity at the Aerospace Corporation. Anything but a household name,\nAerospace Corporation has been headquartered in El Segundo since 1960,\nserving as a kind of neutral, nonprofit organization that advises the air\nforce, NASA, and other federal bodies on space programs. The company\nhas a bureaucratic feel but has proved very useful over the years with its\nresearch activities and ability to champion and nix costly endeavors.\nShotwell started at Aerospace in October 1988 and worked on a wide range\nof projects. One job required her to develop a thermal model that depicted\nhow temperature fluctuations in the space shuttle\u2019s cargo bay affected the\nperformance of equipment on various payloads. She spent ten years at\nAerospace and honed her skills as a systems engineer. By the end, though,\nShotwell had become irritated by the pace of the industry. \u201cI didn\u2019t\nunderstand why it had to take fifteen years to make a military satellite,\u201d she\nsaid. \u201cYou could see my interest was waning.\u201d\nFor the next four years, Shotwell worked at Microcosm, a space start-up\njust down the road from the Aerospace Corporation, and became the head\nof its space systems division and business development. Boasting a combination of smarts, confidence, direct talk, and good looks, Shotwell\ndeveloped a reputation as a strong saleswoman. In 2002, one of her\ncoworkers, Hans Koenigsmann, left for SpaceX. Shotwell took\nKoenigsmann out for a going-away lunch and dropped him off at SpaceX\u2019s\nthen rinky-dink headquarters. \u201cHans told me to go in and meet Elon,\u201d\nShotwell said. \u201cI did, and that\u2019s when I told him, \u2018You need a good business\ndevelopment person.\u2019\u201d The next day Mary Beth Brown called Shotwell and\ntold her that Musk wanted to interview her for the new vice president of\nbusiness development position. Shotwell ended up as employee No. 7. \u201cI\ngave three weeks\u2019 notice at Microcosm and remodeled my bathroom\nbecause I knew I would not have a life after taking the job,\u201d she said.\nThrough the early years of SpaceX, Shotwell pulled off the miraculous\nfeat of selling something the company did not have. It took SpaceX so\nmuch longer than it had planned to have a successful flight. The failures\nalong the way were embarrassing and bad for business. Nonetheless,\nShotwell managed to sell about a dozen flights to a mix of government and\ncommercial customers before SpaceX put its first Falcon 1 into orbit. Her\ndeal-making skills extended to negotiating the big-ticket contracts with\nNASA that kept SpaceX alive during its leanest years, including a $278\nmillion contract in August 2006 to begin work on vehicles that could ferry\nsupplies to the ISS. Shotwell\u2019s track record of success turned her into\nMusk\u2019s ultimate confidante at SpaceX, and at the end of 2008, she became\npresident and chief operating officer at the company.\nPart of Shotwell\u2019s duties include reinforcing the SpaceX culture as the\ncompany grows larger and larger and starts to resemble the traditional\naerospace giants that it likes to mock. Shotwell can switch on an easygoing,\naffable air and address the entire company during a meeting or convince a\ncollection of possible recruits why they should sign up to be worked to the\nbone. During one such meeting with a group of interns, Shotwell pulled\nabout a hundred people into the corner of the cafeteria. She wore high-heel\nblack boots, skintight jeans, a tan jacket, and a scarf and had big hoop\nearnings dangling beside her shoulder-length blond hair. Pacing back and\nforth in front of the group with a microphone in hand, she asked them to\nannounce what school they came from and what project they were working\non while at SpaceX. One student went to Cornell and worked on Dragon,\nanother went to USC and did propulsion system design, and another went to\nthe University of Illinois and worked with the aerodynamics group. It took about thirty minutes to make it all the way around the room, and the\nstudents were, at least by academic pedigree and bright-eyed enthusiasm,\namong the most impressive youngsters in the world. The students peppered\nShotwell with questions\u2014her best moment, her advice for being successful,\nSpaceX\u2019s competitive threats\u2014and she replied with a mix of earnest\nanswers and rah-rah stuff. Shotwell made sure to emphasize the lean,\ninnovative edge SpaceX has over the more traditional aerospace companies.\n\u201cOur competitors are scared shitless of us,\u201d Shotwell told the group. \u201cThe\nbehemoths are going to have to figure out how to get it together and\ncompete. And it is our job to have them die.\u201d\nOne of SpaceX\u2019s biggest goals, Shotwell said, was to fly as often as\npossible. The company has never sought to make a fortune off each flight. It\nwould rather make a little on each launch and keep the flights flowing. A\nFalcon 9 flight costs $60 million, and the company would like to see that\nfigure drop to about $20 million through economies of scale and\nimprovements in launch technology. SpaceX spent $2.5 billion to get four\nDragon capsules to the ISS, nine flights with the Falcon 9, and five flights\nwith the Falcon 1. It\u2019s a price-per-launch total that the rest of the players in\nthe industry cannot comprehend let alone aspire to. \u201cI don\u2019t know what\nthose guys do with their money,\u201d Shotwell said. \u201cThey are smoking it. I just\ndon\u2019t know.\u201d As Shotwell saw it, a number of new nations were showing\ninterest in launches, eyeing communications technology as essential to\ngrowing their economies and leveling their status with developed nations.\nCheaper flights would help SpaceX take the majority of the business from\nthat new customer set. The company also expected to participate in an\nexpanding market for human flights. SpaceX has never had any interest in\ndoing the five-minute tourist flights to low Earth orbit like Virgin Galactic\nand XCOR. It does, however, have the ability to carry researchers to\norbiting habitats being built by Bigelow Aerospace and to orbiting science\nlabs being constructed by various countries. SpaceX will also start making\nits own satellites, turning the company into a one-stop space shop. All of\nthese plans hinge on SpaceX being able to prove that it can fly on schedule\nevery month and churn through the $5 billion backlog of launches. \u201cMost of\nour customers signed up early and wanted to be supportive and got good\ndeals on their missions,\u201d she said. \u201cWe are in a phase now where we need to\nlaunch on time and make launching Dragons more efficient.\u201d For a short while, the conversation with the interns bogged down. It\nturned to some of the annoyances of SpaceX\u2019s campus. The company leases\nits facility and has not been able to build things like a massive parking\nstructure that would make life easier for its three-thousand-person\nworkforce. Shotwell promised that more parking, more bathrooms, and\nmore of the freebies that technology start-ups in Silicon Valley offer their\nemployees would be on the way. \u201cI want a day care,\u201d she said.\nBut it was while discussing SpaceX\u2019s grandest missions that Shotwell\nreally came into her own and seemed to inspire the interns. Some of them\nclearly dreamed of becoming astronauts, and Shotwell said that working at\nSpaceX was almost certainly their best chance to get to space now that\nNASA\u2019s astronaut corps had dwindled. Musk had made designing cool-\nlooking, \u201cnon\u2013Stay Puft\u201d spacesuits a personal priority. \u201cThey can\u2019t be\nclunky and nasty,\u201d Shotwell said. \u201cYou have to do better than that.\u201d As for\nwhere the astronauts would go: well, there were the space habitats, the\nmoon, and, of course, Mars as options. SpaceX has already started testing a\ngiant rocket, called the Falcon Heavy, that will take it much farther into\nspace than the Falcon 9, and it has another, even larger spaceship on the\nway. \u201cOur Falcon Heavy rocket will not take a busload of people to Mars,\u201d\nshe said. \u201cSo, there\u2019s something after Heavy. We\u2019re working on it.\u201d To\nmake something like that vehicle happen, she said, the SpaceX employees\nneeded to be effective and pushy. \u201cMake sure your output is high,\u201d Shotwell\nsaid. \u201cIf we\u2019re throwing a bunch of shit in your way, you need to be mouthy\nabout it. That\u2019s not a quality that\u2019s widely accepted elsewhere, but it is at\nSpaceX.\u201d And, if that sounded harsh, so be it. As Shotwell saw it, the\ncommercial space race was coming down to SpaceX and China and that\u2019s it.\nAnd in the bigger picture, the race was on to ensure man\u2019s survival. \u201cIf you\nhate people and think human extinction is okay, then fuck it,\u201d Shotwell said.\n\u201cDon\u2019t go to space. If you think it is worth humans doing some risk\nmanagement and finding a second place to go live, then you should be\nfocused on this issue and willing to spend some money. I am pretty sure we\nwill be selected by NASA to drop landers and rovers off on Mars. Then the\nfirst SpaceX mission will be to drop off a bunch of supplies, so that once\npeople get there, there will be places to live and food to eat and stuff for\nthem to do.\u201d\nIt\u2019s talk like this that thrills and amazes people in the aerospace industry,\nwho have long been hoping that some company would come along and truly revolutionize space travel. Aeronautics experts will point out that twenty\nyears after the Wright brothers started their experiments, air travel had\nbecome routine. The launch business, by contrast, appears to have frozen.\nWe\u2019ve been to the moon, sent research vehicles to Mars, and explored the\nsolar system, but all of these things are still immensely expensive one-off\nprojects. \u201cThe cost remains extraordinarily high because of the rocket\nequation,\u201d said Carol Stoker, the planetary scientist at NASA. Thanks to\nmilitary and government contracts from agencies like NASA, the aerospace\nindustry has historically had massive budgets to work with and tried to\nmake the biggest, most reliable machines it could. The business has been\ntuned to strive for maximum performance, so that the aerospace contractors\ncan say they met their requirements. That strategy makes sense if you\u2019re\ntrying to send up a $1 billion military satellite for the U.S. government and\nsimply cannot afford for the payload to blow up. But on the whole, this\napproach stifles the pursuit of other endeavors. It leads to bloat and excess\nand a crippling of the commercial space industry.\nOutside of SpaceX, the American launch providers are no longer\ncompetitive against their peers in other countries. They have limited launch\nabilities and questionable ambition. SpaceX\u2019s main competitor for domestic\nmilitary satellites and other large payloads is United Launch Alliance\n(ULA), a joint venture formed in 2006 when Boeing and Lockheed Martin\ncombined forces. The thinking at the time about the union was that the\ngovernment did not have enough business for two companies and that\ncombining the research and manufacturing work of Boeing and Lockheed\nwould result in cheaper, safer launches. ULA has leaned on decades of work\naround the Delta (Boeing) and Atlas (Lockheed) launch vehicles and has\nflown many dozens of rockets successfully, making it a model of reliability.\nBut neither the joint venture nor Boeing nor Lockheed, both of which can\noffer commercial services on their own, come close to competing on price\nagainst SpaceX, the Russians, or the Chinese. \u201cFor the most part, the global\ncommercial market is dominated by Arianespace [Europe], Long March\n[China] or Russian vehicles,\u201d said Dave Bearden, the general manager of\ncivil and commercial programs at the Aerospace Corporation. \u201cThere are\njust different labor rates and differences in the way they are built.\u201d\nTo put things more bluntly, ULA has turned into an embarrassment for\nthe United States. In March 2014, ULA\u2019s then CEO, Michael Gass, faced\noff against Musk during a congressional hearing that dealt, in part, with SpaceX\u2019s request to take on more of the government\u2019s annual launch load.\nA series of slides were rolled out that showed how the government\npayments for launches have skyrocketed since Boeing and Lockheed went\nfrom a duopoly to a monopoly. According to Musk\u2019s math presented at the\nhearing, ULA charged $380 million per flight, while SpaceX would charge\n$90 million per flight. (The $90 million figure was higher than SpaceX\u2019s\nstandard $60 million because the government has certain additional\nrequirements for particularly sensitive launches.) By simply picking\nSpaceX as its launch provider, Musk pointed out, the government would\nsave enough money to pay for the satellite going on the rocket. Gass had no\nreal retort. He claimed Musk\u2019s figures for the ULA launch price were\ninaccurate but failed to provide a figure of his own. The hearing also came\nas tensions between the United States and Russia were running high due to\nRussia\u2019s aggressive actions in Ukraine. Musk rightly noted that the United\nStates could soon be placing sanctions on Russia that could carry over to\naerospace equipment. ULA, as it happens, relies on Russian-made engines\nto send up sensitive U.S. military equipment in its Atlas V rockets. \u201cOur\nFalcon 9 and Falcon Heavy launch vehicles are truly American,\u201d Musk\nsaid. \u201cWe design and manufacture our rockets in California and Texas.\u201d\nGass countered that ULA had bought a two-year supply of Russian engines\nand purchased the blueprints to the machines and had them translated from\nRussian to English, and he said this with a straight face. (A few months\nafter the hearing, ULA replaced Gass as CEO and signed a deal with Blue\nOrigin to develop American-made rockets.)\nSome of the most disheartening moments of the hearing arrived when\nSenator Richard Shelby of Alabama took the microphone for questioning.\nULA has manufacturing facilities in Alabama and close ties to the senator.\nShelby felt compelled to play the role of hometown booster by repeatedly\npointing out that ULA had enjoyed sixty-eight successful launches and then\nasking Musk what he made of that accomplishment. The aerospace industry\nstands as one of Shelby\u2019s biggest donors and he\u2019s ended up surprisingly pro-\nbureaucracy and anticompetition when it comes to getting things into space.\n\u201cTypically competition results in better quality and lower-priced contracts\n\u2014but the launch market is not typical,\u201d Shelby said. \u201cIt is limited demand\nframed by government-industrial policies.\u201d The March hearing in which\nShelby made these statements would turn out to be something of a sham.\nThe government had agreed to put fourteen of its sensitive launches up for bid instead of just awarding them directly to ULA. Musk had come to\nCongress to present his case for why SpaceX made sense as a viable\ncandidate for those and other launches. The day after the hearing, the air\nforce cut the number of launches up for bid from fourteen to between seven\nand one. One month later, SpaceX filed a lawsuit against the air force\nasking for a chance to earn its launch business. \u201cSpaceX is not seeking to be\nawarded contracts for these launches,\u201d the company said on its\nfreedomtolaunch.com website. \u201cWe are simply seeking the right to\ncompete.\u201d*\nSpaceX\u2019s main competitor for ISS resupply missions and commercial\nsatellites in the United States is Orbital Sciences Corporation. Founded in\nVirginia in 1982, the company started out not unlike SpaceX, as the new kid\nthat raised outside funding and focused on putting smaller satellites into\nlow-Earth orbit. Orbital is more experienced, although it has a limited roster\nof machine types. Orbital depends on suppliers, including Russian and\nUkrainian companies, for its engines and rocket bodies, making it more of\nan assembler of spacecraft than a true builder like SpaceX. And, also unlike\nSpaceX, Orbital\u2019s capsules cannot withstand the journey back from the ISS\nto Earth, so it\u2019s unable to return experiments and other goods. In October\n2014, one of Orbital\u2019s rockets blew up on the launchpad. With its ability to\nlaunch on hold while it investigated the incident, Orbital reached out to\nSpaceX for help. It wanted to see if Musk had any extra capacity to take\ncare of some of Orbital\u2019s customers. The company also signaled that it\nwould move away from using Russian engines as well.\nAs for getting humans to space, SpaceX and Boeing were the victors in\na four-year NASA competition to fly astronauts to the ISS. SpaceX will get\n$2.6 billion, and Boeing will get $4.2 billion to develop their capsules and\nferry people to the ISS by 2017. The companies would, in effect, be\nreplacing the space shuttle and restoring the United States\u2019 ability to\nconduct manned flights. \u201cI actually don\u2019t mind that Boeing gets twice as\nmuch money for meeting the same NASA requirements as SpaceX with\nworse technology,\u201d Musk said. \u201cHaving two companies involved is better\nfor the advancement of human spaceflight.\u201d\nSpaceX had once looked like it too would be a one-trick pony. The\ncompany\u2019s original plans were to have the smallish Falcon 1 function as its\nprimary workhorse. At $6 million to $12 million per flight, the Falcon 1\nwas by far the cheapest means of getting something into orbit, thrilling people in the space industry. When Google announced its Lunar X Prize in\n2007\u2014$30 million in awards to people who could land a robot on the moon\n\u2014many of the proposals that followed selected the Falcon 1 as their\npreferred launch vehicle because it seemed like the only reasonably priced\noption for getting something to the moon. Scientists around the world were\nequally excited, thinking that for the first time they had a means of placing\nexperiments into orbit in a cost-effective way. But for all the enthusiastic\ntalk about the Falcon 1, the demand never arrived. \u201cIt became very clear\nthat there was a huge need for the Falcon 1 but no money for it,\u201d said\nShotwell. \u201cThe market has to be able to sustain a certain amount of\nvehicles, and three Falcon 1s per year does not make a business.\u201d The last\nFalcon 1 launch took place in July 2009 from Kwajalein, when SpaceX\ncarried a satellite into orbit for the Malaysian government. People in the\naerospace industry have been grumbling ever since. \u201cWe gave Falcon 1 a\nhell of a shot,\u201d Shotwell said. \u201cI was emotional about it and disappointed.\nI\u2019d anticipated a flood of orders but, after eight years, they just did not\ncome.\u201d\nSpaceX has since expanded its launch capabilities at a remarkable pace\nand looks like it might be on the verge of getting that $12 million per flight\noption back. In June 2010, the Falcon 9 flew for the first time and orbited\nEarth successfully. In December 2010, SpaceX proved that the Falcon 9\ncould carry the Dragon capsule into space and that the capsule could be\nrecovered safely after an ocean landing.* It became the first commercial\ncompany ever to pull off this feat. Then, in May 2012, SpaceX went\nthrough the most significant moment in the company\u2019s history since that\nfirst successful launch on Kwajalein.\nOn May 22, at 3:44 A.M., a Falcon 9 rocket took off from the Kennedy\nSpace Center in Cape Canaveral, Florida. The rocket did its yeoman-like\nwork boosting Dragon into space. Then the capsule\u2019s solar panels fanned\nout and Dragon became dependent on its eighteen Draco thrusters, or small\nrocket engines, to guide its path to the International Space Station. The\nSpaceX engineers worked in shifts\u2014some of them sleeping on cots at the\nfactory\u2014as it took the capsule three days for Dragon to make its journey.\nThey spent most of the time observing Dragon\u2019s flight and checking to see\nthat its sensor systems were picking up the ISS. Originally, Dragon planned\nto dock with the ISS around 4 A.M. on the twenty-fifth, but as the capsule\napproached the space station, an unexpected glint kept throwing off the calculations of a laser used to measure the distance between Dragon and the\nISS. \u201cI remember it being two and a half hours of struggle,\u201d Shotwell said.\nHer outfit of Uggs, a fishnet sweater, and leggings started to feel like\npajamas as the night wore on, and the engineers battled this unplanned\ndifficulty. Fearing all the time that the mission would be aborted, SpaceX\ndecided to upload some new software to the Dragon that would cut the size\nof the visual frame used by the sensors to eliminate the effect of the sunlight\non the machine. Then, just before 7 A.M., Dragon got close enough to the\nISS for Don Pettit, an astronaut, to use a fifty-eight-foot robotic arm to\nreach out and grab the resupply capsule. \u201cHouston, Station, it looks like\nwe\u2019ve got us a dragon by the tail,\u201d Pettit said.13\n\u201cI\u2019d been digesting my guts,\u201d Shotwell said. \u201cAnd then I am drinking\nchampagne at six in the morning.\u201d About thirty people were in the control\nroom when the docking happened. Over the next couple of hours, workers\nstreamed into the SpaceX factory to soak up the elation of the moment.\nSpaceX had set another first, as the only private company to dock with the\nISS. A couple of months later SpaceX received $440 million from NASA to\nkeep developing Dragon so that it could transport people. \u201cElon is changing\nthe way aerospace business is done,\u201d said NASA\u2019s Stoker. \u201cHe\u2019s managed\nto keep the safety factor up while cutting costs. He\u2019s just taken the best\nthings from the tech industry like the open-floor office plans and having\neveryone talking and all this human interaction. It\u2019s a very different way to\nmost of the aerospace industry, which is designed to produce requirements\ndocuments and project reviews.\u201d\nIn May 2014, Musk invited the press to SpaceX\u2019s headquarters to\ndemonstrate what some of that NASA money had bought. He unveiled the\nDragon V2, or version two, spacecraft. Unlike most executives, who like to\nshow their products off at trade shows or daytime events, Musk prefers to\nhold true Hollywood-style galas in the evenings. People arrived in\nHawthorne by the hundreds and snacked on hors d\u2019oeuvres until the 7:30\nP.M. showing. Musk appeared wearing a purplish velvet jacket and popping\nopen the capsule\u2019s door with a bump of his fist like the Fonz. What he\nrevealed was spectacular. The cramped quarters of past capsules were gone.\nThere were seven thin, sturdy, contoured seats arranged with four seats\nclose to the main console and a row of three seats in the back. Musk walked\naround in the capsule to show how roomy it was and then plopped down in\nthe central captain\u2019s chair. He reached up and unlocked a four-paneled flat- screen console that gracefully slid down right in front of the first row of\nseats.* In the middle of the console was a joystick for flying the aircraft and\nsome physical buttons for essential functions that astronauts could press in\ncase of an emergency or a malfunctioning touch-screen. The inside of the\ncapsule had a bright, metallic finish. Someone had finally built a spaceship\nworthy of scientist and moviemaker dreams.\nThere was substance to go with the style. The Dragon 2 will be able to\ndock with the ISS and other space habitats automatically without needing\nthe intervention of a robotic arm. It will run on a SuperDraco engine\u2014a\nthruster made by SpaceX and the first engine ever built completely by a 3-D\nprinter to go into space. This means that a machine guided by a computer\nformed the engine out of single piece of metal\u2014in this case the high-\nstrength alloy Inconel\u2014so that its strength and performance should exceed\nanything built by humans by welding various parts together. And most\nmind-boggling of all, Musk revealed that the Dragon 2 will be able to land\nanywhere on Earth that SpaceX wants by using the SuperDraco engines and\nthrusters to come to a gentle stop on the ground. No more landings at sea.\nNo more throwing spaceships away. \u201cThat is how a twenty-first-century\nspaceship should land,\u201d Musk said. \u201cYou can just reload propellant and fly\nagain. So long as we continue to throw away rockets and spacecraft, we will\nnever have true access to space.\u201d\nThe Dragon 2 is just one of the machines that SpaceX continues to\ndevelop in parallel. One of the company\u2019s next milestones will be the first\nflight of the Falcon Heavy, which is designed to be the world\u2019s most\npowerful rocket.* SpaceX has found a way to combine three Falcon 9s into\na single craft with 27 of the Merlin engines and the ability to carry more\nthan 53 metric tons of stuff into orbit. Part of the genius of Musk and\nMueller\u2019s designs is that SpaceX can reuse the same engine in different\nconfigurations\u2014from the Falcon 1 up to the Falcon Heavy\u2014saving on cost\nand time. \u201cWe make our main combustion chambers, turbo pump, gas\ngenerators, injectors, and main valves,\u201d Mueller said. \u201cWe have complete\ncontrol. We have our own test site, while most of the other guys use\ngovernment test sites. The labor hours are cut in half and so is the work\naround the materials. Four years ago, we could make two rockets a year and\nnow we can make twenty a year.\u201d SpaceX boasts that the Falcon Heavy can\ntake up twice the payload of the nearest competitor\u2014the Delta IV Heavy\nfrom Boeing/ULA\u2014at one-third the cost. SpaceX is also busy building a spaceport from the ground up. The goal is to be able to launch many rockets\nan hour from this facility located in Brownsville, Texas, by automating the\nprocesses needed to stand a rocket up on the pad, fuel it, and send it off.\nJust as it did in the early days, SpaceX continues to experiment with\nthese new vehicles during actual launches in ways that other companies\nwould dare not do. SpaceX will often announce that it\u2019s trying out a new\nengine or its landing legs and place the emphasis on that one upgrade in the\nmarketing material leading up to a launch. It\u2019s common, though, for SpaceX\nto test out a dozen other objectives in secret during a mission. Musk\nessentially asks employees to do the impossible on top of the impossible.\nOne former SpaceX executive described the working atmosphere as a\nperpetual-motion machine that runs on a weird mix of dissatisfaction and\neternal hope. \u201cIt\u2019s like he has everyone working on this car that is meant to\nget from Los Angeles to New York on one tank of gas,\u201d this executive said.\n\u201cThey will work on the car for a year and test all of its parts. Then, when\nthey set off for New York after that year, all of the vice presidents think\nprivately that the car will be lucky to get to Las Vegas. What ends up\nhappening is that the car gets to New Mexico\u2014twice as far as they ever\nexpected\u2014and Elon is still mad. He gets twice as much as anyone else out\nof people.\u201d\nThere\u2019s a degree to which it\u2019s just never enough for Musk, no matter\nwhat it is. Case in point: the December 2010 launch in which SpaceX got\nthe Dragon capsule to orbit Earth and return successfully. This had been one\nof the company\u2019s great achievements, and people had worked tirelessly for\nmonths, if not years. The launch had taken place on December 8, and\nSpaceX had a Christmas party on December 16. About ninety minutes\nbefore the party started, Musk had called his top executives to SpaceX for a\nmeeting. Six of them, including Mueller, were decked out in party attire and\nready to celebrate the holidays and SpaceX\u2019s historic achievement around\nDragon. Musk laid into them for about an hour because the truss structure\nfor a future rocket was running behind schedule. \u201cTheir wives were sitting\nthree cubes over waiting for the berating to end,\u201d Brogan said. Other\nexamples of similar behavior have cropped up from time to time. Musk, for\nexample, rewarded a group of thirty employees who had pulled off a tough\nproject for NASA with bonuses that consisted of additional stock option\ngrants. Many of the employees, seeking instant, more tangible gratification,\ndemanded cash. \u201cHe chided us for not valuing the stock,\u201d Drew Eldeen, a former engineer, said. \u201cHe said, \u2018In the long run, this is worth a lot more\nthan a thousand dollars in cash.\u2019 He wasn\u2019t screaming or anything like that,\nbut he seemed disappointed in us. It was hard to hear that.\u201d\nThe lingering question for many SpaceX employees is when exactly\nthey will see a big reward for all their work. SpaceX\u2019s staff is paid well but\nby no means exorbitantly. Many of them expect to make their money when\nSpaceX files for an initial public offering. The thing is that Musk does not\nwant to go public anytime soon, and understandably so. It\u2019s a bit hard to\nexplain the whole Mars thing to investors, when it\u2019s unclear what the\nbusiness model around starting a colony on another planet will be. When\nthe employees heard Musk say that an IPO was years away and would not\noccur until the Mars mission looked more secure, they started to grumble,\nand when Musk found out, he addressed all of SpaceX in an e-mail that is a\nfantastic window into his thinking and how it differs from almost every\nother CEO\u2019s. (The full e-mail appears in Appendix 3.)\nJune 7, 2013\nGoing Public\nPer my recent comments, I am increasingly concerned about\nSpaceX going public before the Mars transport system is in place.\nCreating the technology needed to establish life on Mars is and\nalways has been the fundamental goal of SpaceX. If being a public\ncompany diminishes that likelihood, then we should not do so until\nMars is secure. This is something that I am open to reconsidering,\nbut, given my experiences with Tesla and SolarCity, I am hesitant to\nfoist being public on SpaceX, especially given the long term nature\nof our mission.\nSome at SpaceX who have not been through a public company\nexperience may think that being public is desirable. This is not so.\nPublic company stocks, particularly if big step changes in\ntechnology are involved, go through extreme volatility, both for\nreasons of internal execution and for reasons that have nothing to do\nwith anything except the economy. This causes people to be\ndistracted by the manic-depressive nature of the stock instead of\ncreating great products. For those who are under the impression that they are so clever\nthat they can outsmart public market investors and would sell\nSpaceX stock at the \u201cright time,\u201d let me relieve you of any such\nnotion. If you really are better than most hedge fund managers, then\nthere is no need to worry about the value of your SpaceX stock, as\nyou can just invest in other public company stocks and make\nbillions of dollars in the market.\nElon 10 THE REVENGE OF THE ELECTRIC CAR\nT\nHERE ARE SO MANY TELEVISION COMMERCIALS FOR CARS\nAND TRUCKS that it\u2019s easy to become immune to them and ignore what\u2019s\ntaking place in the ads. That\u2019s okay. Because there\u2019s not really much of note\nhappening. Carmakers looking to put a modicum of effort into their ads\nhave been hawking the exact same things for decades: a car with a bit more\nroom, a few extra miles per gallon, better handling, or an extra cup holder.\nThose that can\u2019t find anything interesting at all to tout about their cars turn\nto scantily clad women, men with British accents, and, when necessary,\ndancing mice in tuxedos to try and convince people that their products are\nbetter than the rest. Next time a car ad appears on your television, pause for\na moment and really listen to what\u2019s being said. When you realize that the\nVolkswagen sign-and-drive \u201cevent\u201d is code for \u201cwe\u2019re making the\nexperience of buying a car slightly less miserable than usual,\u201d you\u2019ll start to\nappreciate just how low the automotive industry has sunk.\nIn the middle of 2012, Tesla Motors stunned its complacent peers in the\nautomotive industry. It began shipping the Model S sedan. This all-electric\nluxury vehicle could go more than 300 miles on a single charge. It could\nreach 60 miles per hour in 4.2 seconds. It could seat seven people, if you\nused a couple of optional rear-facing seats in the back for kids. It also had\ntwo trunks. There was the standard one and then what Tesla calls a \u201cfrunk\u201d\nup front, where the bulky engine would usually be. The Model S ran on an\nelectric battery pack that makes up the base of the car and a watermelon-\nsized electric motor located between the rear tires. Getting rid of the engine\nand its din of clanging machinery also meant that the Model S ran silently.\nThe Model S outclassed most other luxury sedans in terms of raw speed,\nmileage, handling, and storage space.\nAnd there was more\u2014like a cutesy thing with the door handles, which\nwere flush with the car\u2019s body until the driver got close to the Model S.\nThen the silver handles would pop out, the driver would open the door and\nget in, and the handles would retract flush with the car\u2019s body again. Once inside, the driver encountered a seventeen-inch touch-screen that controlled\nthe vast majority of the car\u2019s functions, be it raising the volume on the\nstereo* or opening the sunroof with a slide of the finger. Whereas most cars\nhave a large dashboard to accommodate various displays and buttons and to\nprotect people from the noise of the engine, the Model S offered up vast\namounts of space. The Model S had an ever-present Internet connection,\nallowing the driver to stream music through the touch console and to\ndisplay massive Google maps for navigation. The driver didn\u2019t need to turn\na key or even push an ignition button to start the car. His weight in the seat\ncoupled with a sensor in the key fob, which is shaped like a tiny Model S,\nwas enough to activate the vehicle. Made of lightweight aluminum, the car\nachieved the highest safety rating in history. And it could be recharged for\nfree at Tesla\u2019s stations lining highways across the United States and later\naround the world.\nFor both engineers and green-minded people, the Model S presented a\nmodel of efficiency. Traditional cars and hybrids have anywhere from\nhundreds to thousands of moving parts. The engine must perform constant,\ncontrolled explosions with pistons, crankshafts, oil filters, alternators, fans,\ndistributors, valves, coils, and cylinders among the many pieces of\nmachinery needed for the work. The oomph produced by the engine must\nthen be passed through clutches, gears, and driveshafts to make the wheels\nturn, and then exhaust systems have to deal with the waste. Cars end up\nbeing about 10\u201320 percent efficient at turning the input of gasoline into the\noutput of propulsion. Most of the energy (about 70 percent) is lost as heat in\nthe engine, while the rest is lost through wind resistance, braking, and other\nmechanical functions. The Model S, by contrast, has about a dozen moving\nparts, with the battery pack sending energy instantly to a watermelon-sized\nmotor that turns the wheels. The Model S ends up being about 60 percent\nefficient, losing most of the rest of its energy to heat. The sedan gets the\nequivalent of about 100 miles per gallon.*\nYet another distinguishing characteristic of the Model S was the\nexperience of buying and owning the car. You didn\u2019t go to a dealership and\nhaggle with a pushy salesman. Tesla sold the Model S directly through its\nown stores and website. Typically, the stores were placed in high-end malls\nor affluent suburbs, not far from the Apple stores on which they were\nmodeled. Customers would walk in and find a complete Model S in the\nmiddle of the shop and often an exposed version of the car\u2019s base near the back of the store to show off the battery pack and motor. There were\nmassive touch-screens where people could calculate how much they might\nsave on fuel costs by moving to an all-electric car, and where they could\nconfigure the look and add-ons for their future Model S. Once the\nconfiguration process was done, the customer could give the screen a big,\nforceful swipe and his Model S would theatrically appear on an even bigger\nscreen in the center of the store. If you wanted to sit in the display model, a\nsalesman would pull back a red velvet rope near the driver\u2019s-side door and\nlet you enter the car. The salespeople were not compensated on commission\nand didn\u2019t have to try to talk you into buying a suite of extras. Whether you\nultimately bought the car in the store or online, it was delivered in a\nconcierge fashion. Tesla would bring it to your home, office, or anywhere\nelse you wanted it. The company also offered customers the option of\npicking their cars up from the factory in Silicon Valley and treating their\nfriends and family to a complimentary tour of the facility. In the months that\nfollowed the delivery, there were no oil changes or tune-ups to be dealt with\nbecause the Model S didn\u2019t need them. It had done away with so much of\nthe mechanical dreck standard in an internal combustion vehicle. However,\nif something did go wrong with the car, Tesla would come pick it up and\ngive the customer a loaner while it repaired the Model S.\nThe Model S also offered a way to fix issues in a manner that people\nhad never before encountered with a mass-produced car. Some of the early\nowners complained about glitches like the door handles not popping out\nquite right or their windshield wipers operating at funky speeds. These were\ninexcusable flaws for such a costly vehicle, but Tesla typically moved with\nclever efficiency to address them. While the owner slept, Tesla\u2019s engineers\ntapped into the car via the Internet connection and downloaded software\nupdates. When the customer took the car out for a spin in the morning and\nfound it working right, he was left feeling as if magical elves had done the\nwork. Tesla soon began showing off its software skills for jobs other than\nmaking up for mistakes. It put out a smartphone app that let people turn on\ntheir air-conditioning or heating from afar and to see where the car was\nparked on a map. Tesla also began installing software updates that imbued\nthe Model S with new features. Overnight, the Model S sometimes got new\ntraction controls for hilly and highway driving or could suddenly recharge\nmuch faster than before or possess a new range of voice controls. Tesla had\ntransformed the car into a gadget\u2014a device that actually got better after you bought it. As Craig Venter, one of the earliest Model S owners and the\nfamed scientist who first decoded man\u2019s DNA, put it, \u201cIt changes\neverything about transportation. It\u2019s a computer on wheels.\u201d\nThe first people to notice what Tesla had accomplished were the\ntechnophiles in Silicon Valley. The region is filled with early adopters\nwilling to buy the latest gizmos and suffer through their bugs. Normally this\nhabit applies to computing devices ranging from $100 to $2,000 in price.\nThis time around, the early adopters proved willing not only to spend\n$100,000 on a product that might not work but also to trust their well-being\nto a start-up. Tesla needed this early boost of confidence and got it on a\nscale few expected. In the first couple of months after the Model S went on\nsale, you might see one or two per day on the streets of San Francisco and\nthe surrounding cities. Then you started to see five to ten per day. Soon\nenough, the Model S seemed to feel like the most common car in Palo Alto\nand Mountain View, the two cities at the heart of Silicon Valley. The Model\nS emerged as the ultimate status symbol for wealthy technophiles, allowing\nthem to show off, get a new gadget, and claim to be helping the\nenvironment at the same time. From Silicon Valley, the Model S\nphenomenon spread to Los Angeles, then all along the West Coast and then\nto Washington, D.C., and New York (although to a lesser degree).\nAt first the more traditional automakers viewed the Model S as a\ngimmick and its surging sales as part of a fad. These sentiments, however,\nsoon gave way to something more akin to panic. In November 2012, just a\nfew months after it started shipping, the Model S was named Motor Trend\u2019s\nCar of the Year in the first unanimous vote that anyone at the magazine\ncould remember. The Model S beat out eleven other vehicles from\ncompanies such as Porsche, BMW, Lexus, and Subaru and was heralded as\n\u201cproof positive that America can still make great things.\u201d Motor Trend\ncelebrated the Model S as the first non\u2013internal combustion engine car ever\nto win its top award and wrote that the vehicle handled like a sports car,\ndrove as smoothly as a Rolls-Royce, held as much as a Chevy Equinox, and\nwas more efficient than a Toyota Prius. Several months later, Consumer\nReports gave the Model S its highest car rating in history\u201499 out of 100\u2014\nwhile proclaiming that it was likely the best car ever built. It was at about\nthis time that sales of the Model S started to soar alongside Tesla\u2019s share\nprice and that General Motors, among other automakers, pulled together a\nteam to study the Model S, Tesla, and the methods of Elon Musk. It\u2019s worth pausing for a moment to meditate on what Tesla had\naccomplished. Musk had set out to make an electric car that did not suffer\nfrom any compromises. He did that. Then, using a form of entrepreneurial\njudo, he upended the decades of criticisms against electric cars. The Model\nS was not just the best electric car; it was best car, period, and the car\npeople desired. America had not seen a successful car company since\nChrysler emerged in 1925. Silicon Valley had done little of note in the\nautomotive industry. Musk had never run a car factory before and was\nconsidered arrogant and amateurish by Detroit. Yet, one year after the\nModel S went on sale, Tesla had posted a profit, hit $562 million in\nquarterly revenue, raised its sales forecast, and become as valuable as\nMazda Motor. Elon Musk had built the automotive equivalent of the\niPhone. And car executives in Detroit, Japan, and Germany had only their\ncrappy ads to watch as they pondered how such a thing had occurred.\nYou can forgive the automotive industry veterans for being caught\nunawares. For years Tesla had looked like an utter disaster incapable of\ndoing much of anything right. It took until early 2009 for Tesla to really hit\nits stride with the Roadster and work out the manufacturing issues behind\nthe sports car. Just as the company tried to build some momentum around\nthe Roadster, Musk sent out an e-mail to customers declaring a price hike.\nWhere the car originally started around $92,000, it would now start at\n$109,000. In the e-mail, Musk said that four hundred customers who had\nalready placed their orders for a Roadster but not yet received them would\nbear the brunt of the price change and need to cough up the extra cash. He\ntried to assuage Tesla\u2019s customer base by arguing that the company had no\nchoice but to raise prices. The manufacturing costs for the Roadster had\ncome in much higher than the company initially expected, and Tesla needed\nto prove that it could make the cars at a profit to bolster its chances of\nsecuring a large government loan that would be needed to build the Model\nS, which it vowed to deliver in 2011. \u201cI firmly believe that the plan . . .\nstrikes a reasonable compromise between being fair to early customers and\nensuring the viability of Tesla, which is obviously in the best interests of all\ncustomers,\u201d Musk wrote in the e-mail. \u201cMass market electric cars have been\nmy goal from the beginning of Tesla. I don\u2019t want and I don\u2019t think the vast\nmajority of Tesla customers want us to do anything to jeopardize that\nobjective.\u201d While some Tesla customers grumbled, Musk had largely read his customer base right. They would support just about anything he\nsuggested.\nFollowing the price increase, Tesla had a safety recall. It said that Lotus,\nthe manufacturer of the Roadster\u2019s chassis, had failed to tighten a bolt\nproperly on its assembly line. On the plus side, Tesla had only delivered\nabout 345 Roadsters, which meant that it could fix the problem in a\nmanageable fashion. On the downside, a safety recall was the last thing a\ncar start-up needs, even if it was, as Tesla claimed, more of a proactive\nmeasure than anything else. The next year, Tesla had another voluntary\nrecall. It had received a report of a power cable grinding against the body of\nthe Roadster to the point that it caused a short circuit and some smoke. That\ntime, Tesla brought 439 Roadsters in for a fix. Tesla did its best to put a\npositive spin on these issues, saying that it would make \u201chouse calls\u201d to fix\nthe Roadsters or pick up the cars and take them back to the factory. Ever\nsince, Musk has tried to turn any snafu with a Tesla into an excuse to show\noff the company\u2019s attention to service and dedication to pleasing the\ncustomer. More often than not, the strategy has worked.\nOn top of the occasional issues with the Roadster, Tesla continued to\nsuffer from public perception problems. In June 2009, Martin Eberhard\nsued Musk and went to town in the complaint detailing his ouster from the\ncompany. Eberhard accused Musk of libel, slander, and breach of contract.\nThe charges painted Musk as a bully moneyman who had pushed the\nsoulful inventor out of his own company. The lawsuit also accused Musk of\ntrumping up his role in Tesla\u2019s founding. Musk responded in kind, issuing a\nblog post that detailed his take on Eberhard\u2019s foibles and taking umbrage at\nthe suggestions that he was not a true founder of the company. A short\nwhile later, the two men settled and agreed to stop going at each other. \u201cAs\nco-founder of the company, Elon\u2019s contributions to Tesla have been\nextraordinary,\u201d Eberhard said in a statement at the time. It must have been\nexcruciating for Eberhard to agree to put that in writing and the very\nexistence of that statement points to Musk\u2019s skills and tactics as a hard-line\nnegotiator. The two men continue to despise each other today, although they\nmust do so in private, as legally required. Eberhard, though, holds no long-\nstanding grudge against Tesla. His shares in the company ended up\nbecoming very valuable. He still drives his Roadster, and his wife got a\nModel S. For so much of its early existence, Tesla appeared in the news for the\nwrong reasons. There were people in the media and the automotive industry\nwho viewed it as a gimmick. They seemed to delight in the soap opera\u2013\nworthy spats between Musk and Eberhard and other disgruntled former\nemployees. Far from being seen universally as a successful entrepreneur,\nMusk was viewed in some Silicon Valley circles as an abrasive blowhard\nwho would get what he deserved when Tesla inevitably collapsed. The\nRoadster would make its way to the electric-car graveyard. Detroit would\nprove that it had a better handle on this whole car innovation thing than\nSilicon Valley. The natural order of the world would remain intact.\nA funny thing happened, however. Tesla did just enough to survive.\nFrom 2008 to 2012, Tesla sold about 2,500 Roadsters.* The car had\naccomplished what Musk had intended from the outset. It proved that\nelectric cars could be fun to drive and that they could be objects of desire.\nWith the Roadster, Tesla kept electric cars in the public\u2019s consciousness and\ndid so under impossible circumstances, namely the collapse of the\nAmerican automotive industry and the global financial markets. Whether\nMusk was a founder of Tesla in the purest sense of the word is irrelevant at\nthis point. There would be no Tesla to talk about today were it not for\nMusk\u2019s money, marketing savvy, chicanery, engineering smarts, and\nindomitable spirit. Tesla was, in effect, willed into existence by Musk and\nreflects his personality as much as Intel, Microsoft, and Apple reflect the\npersonalities of their founders. Marc Tarpenning, the other Tesla cofounder,\nsaid as much when he reflected on what Musk has meant to the company.\n\u201cElon pushed Tesla so much farther than we ever imagined,\u201d he said.\nAs difficult as birthing the Roadster had been, the adventure had\nwhetted Musk\u2019s appetite for what he could accomplish in the automotive\nindustry with a clean slate. Tesla\u2019s next car\u2014code-named WhiteStar\u2014\nwould not be an adapted version of another company\u2019s vehicle. It would be\nmade from scratch and structured to take full advantage of what the electric-\ncar technology offered. The battery pack in the Roadster, for example, had\nto be placed near the rear of the car because of constraints imposed by the\nLotus Elise chassis. This was okay but not ideal due to the imposing weight\nof the batteries. With WhiteStar, which would become the Model S, Musk\nand Tesla\u2019s engineers knew from the start that they would place the 1,300-\npound battery pack on the base of the car. This would give the vehicle a low\ncenter of gravity and excellent handling. It would also give the Model S what\u2019s known as a low polar moment of inertia, which relates to how a car\nresists turning. Ideally, you want heavy parts like the engine as close as\npossible to the car\u2019s center of gravity, which is why the engines of race cars\ntend to be near the middle of the vehicle. Traditional cars are a mess on this\nmetric, with the bulky engine up front, passengers in the middle, and\ngasoline sloshing around the rear. In the case of the Model S, the bulk of the\ncar\u2019s mass is very close to the center of gravity and this has positive follow-\non effects to handling, performance, and safety.\nThe innards, though, were just one part of what would make the Model\nS shine. Musk wanted to make a statement with the car\u2019s look as well. It\nwould be a sedan, yes, but it would be a sexy sedan. It would also be\ncomfortable and luxurious and have none of the compromises that Tesla had\nbeen forced to embrace with the Roadster. To bring such a beautiful,\nfunctional car to life, Musk hired Henrik Fisker, a Danish automobile\ndesigner renowned for his work at Aston Martin.\nTesla first revealed its plans for the Model S to Fisker in 2007. It asked\nhim to design a sleek, four-door sedan that would cost between $50,000 and\n$70,000. Tesla could still barely make Roadsters and had no idea if its all-\nelectric powertrain would hold up over time. Musk, though, refused to wait\nand find out. He wanted the Model S to ship in late 2009 or early 2010 and\nneeded Fisker to work fast. By reputation, Fisker had a flair for the dramatic\nand had produced some of the most stunning car designs over the past\ndecade, not just for Aston Martin but also for special versions of BMW and\nMercedes-Benz vehicles.\nFisker had a studio in Orange County, California, and Musk and other\nTesla executives would meet there to go over his evolving takes on the\nModel S. Each visit was less inspiring than the last. Fisker baffled the Tesla\nteams with his stodgy designs. \u201cSome of the early styles were like a giant\negg,\u201d said Ron Lloyd, the former vice president of the WhiteStar project at\nTesla. \u201cThey were terrible.\u201d When Musk pushed back, Fisker blamed the\nphysical constraints Tesla had put in place for the Model S as too restrictive.\n\u201cHe said they would not let him make the car sexy,\u201d Lloyd said. Fisker tried\na couple of different approaches and unveiled some foam models of the car\nfor Musk and his crew to dissect. \u201cWe kept on telling him they were not\nright,\u201d Lloyd said.\nNot long after these meetings, Fisker started his own company\u2014Fisker\nAutomotive\u2014and unveiled the Fisker Karma hybrid in 2008. This luxury sedan looked like a vehicle Batman might take out for a Sunday drive. With\nits elongated lines and sharp edges, the car was stunning and truly original.\n\u201cIt rapidly became clear that he was trying to compete with us,\u201d Lloyd said.\nAs Musk dug into the situation, he discovered that Fisker had been\nshopping his idea for a car company to investors around Silicon Valley for\nsome time. Kleiner Perkins Caufield & Byers, one of the more famous\nventure capital firms in Silicon Valley, once had a chance to invest in Tesla\nand then ended up putting money into Fisker instead. All of this was too\nmuch for Musk, and he launched a lawsuit against Fisker in 2008, accusing\nhim of stealing Tesla\u2019s ideas and using the $875,000 Tesla had paid for\ndesign work to help get his rival car company off the ground. (Fisker\nultimately prevailed in the dispute with an arbitrator ordering Tesla to\nreimburse Fisker\u2019s legal fees and deeming Tesla\u2019s allegations baseless.)\nTesla had thought about doing a hybrid like Fisker where a gas engine\nwould be present to recharge the car\u2019s batteries after they had consumed an\ninitial charge. The car would be able to travel fifty to eighty miles after\nbeing plugged into an outlet and then take advantage of ubiquitous gas\nstations as needed to top up the batteries, eliminating range anxiety. Tesla\u2019s\nengineers prototyped the hybrid vehicle and ran all sorts of cost and\nperformance metrics. In the end, they found the hybrid to be too much of a\ncompromise. \u201cIt would be expensive, and the performance would not be as\ngood as the all-electric car,\u201d said J. B. Straubel. \u201cAnd we would have\nneeded to build a team to compete with the core competency of every car\ncompany in the world. We would have been betting against all the things we\nbelieve in, like the power electronics and batteries improving. We decided\nto put all the effort into going where we think the endpoint is and to never\nlook back.\u201d After coming to this conclusion, Straubel and others inside\nTesla started to let go of their anger toward Fisker. They figured he would\nend up delivering a kluge of a car and get what was coming to him.\nA large car company might spend $1 billion and need thousands of\npeople to design a new vehicle and bring it to market. Tesla had nothing\nclose to these resources as it gave birth to the Model S. According to Lloyd,\nTesla initially aimed to make about ten thousand Model S sedans per year\nand had budgeted around $130 million to achieve this goal, including\nengineering the car and acquiring the manufacturing machines needed to\nstamp out the body parts. \u201cOne of the things Elon pushed hard with\neveryone was to do as much as possible in-house,\u201d Lloyd said. Tesla would make up for its lack of R&D money by hiring smart people who could\noutwork and outthink the third parties relied on by the rest of the\nautomakers. \u201cThe mantra was that one great engineer will replace three\nmedium ones,\u201d Lloyd said.\nA small team of Tesla engineers began the process of trying to figure\nout the mechanical inner workings of the Model S. Their first step in this\njourney took place at a Mercedes dealership where they test drove a CLS 4-\nDoor Coupe and an E-Class sedan. The cars had the same chassis, and the\nTesla engineers took measurements of every inch of the vehicles, studying\nwhat they liked and didn\u2019t like. In the end, they preferred the styling on the\nCLS and settled on it as their baseline for thinking about the Model S.\nAfter purchasing a CLS, Tesla\u2019s engineers tore it apart. One team had\nreshaped the boxy, rectangular battery pack from the Roadster and made it\nflat. The engineers cut the floor out of the CLS and plopped in the pack.\nNext they put the electronics that tied the whole system together in the\ntrunk. After that, they replaced the interior of the car to restore its fit and\nfinish. Following three months of work, Tesla had in effect built an all-\nelectric Mercedes CLS. Tesla used the car to woo investors and future\npartners like Daimler that would eventually turn to Tesla for electric\npowertrains in their vehicles. Now and again, the Tesla team took the car\nout for drives on public roads. It weighed more than the Roadster but was\nstill fast and had a range of about 120 miles per charge. To perform these\njoyrides-cum-tests in relative secrecy, the engineers had to weld the tips of\nthe exhaust pipes back onto the car to make it look like any other CLS.\nIt was at this time, the summer of 2008, when an artsy car lover named\nFranz von Holzhausen joined Tesla. His job would be to breathe new life\ninto the car\u2019s early designs and, if possible, turn the Model S into an iconic\nproduct.*\nVon Holzhausen grew up in a small Connecticut town. His father\nworked on the design and marketing of consumer products, and Franz\ntreated the family basement full of markers, different kinds of paper, and\nother materials as a playground for his imagination. As he grew older, von\nHolzhausen drifted toward cars. He and a friend stripped down a dune-\nbuggy motor one winter and then built it back up, and von Holzhausen\nalways filled the margins of his school notebooks with drawings of cars and\nhad pictures of cars on his bedroom walls. Applying to college, von\nHolzhausen decided to follow his father\u2019s path and enrolled in the industrial design program at Syracuse University. Then, through a chance encounter\nwith another designer during an internship, von Holzhausen heard about the\nArt Center College of Design in Los Angeles. \u201cThis guy had been teaching\nme about car design and this school in Los Angeles, and I got super-\nintrigued,\u201d said von Holzhausen. \u201cI went to Syracuse for two years and then\ndecided to transfer out to California.\u201d\nThe move to Los Angeles kicked off a long and storied design career in\nthe automotive industry. Von Holzhausen would go on to intern in Michigan\nwith Ford and in Europe with Volkswagen, where he began to pick up on a\nmix of design sensibilities. After graduating in 1992, he started work for\nVolkswagen on just about the most exciting project imaginable\u2014a top-\nsecret new version of the Beetle. \u201cIt really was a magical time,\u201d von\nHolzhausen said. \u201cOnly fifty people in the world knew we were doing this\nproject.\u201d Von Holzhausen had a chance to work on the exterior and interior\nof the vehicle, including the signature flower vase built into the dashboard.\nIn 1997, Volkswagen launched the \u201cNew Beetle,\u201d and von Holzhausen saw\nfirsthand how the look of the car captivated the public and changed the way\npeople felt about Volkswagen, which had suffered from woeful sales in the\nUnited States. \u201cIt started a rebirth of the VW brand and brought design back\ninto their mix,\u201d he said.\nVon Holzhausen spent eight years with VW, climbing the ranks of its\ndesign team and falling in love with the car culture of Southern California.\nLos Angeles has long adored its cars, with the climate lending itself to all\nmanner of vehicles from convertibles to surfboard-toting vans. Almost all\nof the major carmakers set up design studios in the city. The presence of the\nstudios allowed von Holzhausen to hop from VW to General Motors and\nMazda, where he served as the company\u2019s director of design.\nGM taught von Holzhausen just how nasty a big car company could\nbecome. None of the cars in GM\u2019s lineup really excited him, and it seemed\nnear impossible to make a large impact on the company\u2019s culture. He was\none member of a thousand-person design team that divvyed up the makes of\ncars haphazardly without any consideration as to which person really\nwanted to work on which car. \u201cThey took all the spirit out of me,\u201d said von\nHolzhausen. \u201cI knew I didn\u2019t want to die there.\u201d Mazda, by contrast, needed\nand wanted help. It let von Holzhausen and his team in Los Angeles put\ntheir imprint on every car in the North American vehicle lineup and to\nproduce a set of concept cars that reshaped how the company approached design. As von Holzhausen put it, \u201cWe brought the zoom-zoom back into\nthe look and feel of the car.\u201d\nVon Holzhausen started a project to make Mazda\u2019s cars more green by\nrevaluating the types of materials used to fabricate the seats and the fuels\ngoing into the vehicles. He had, in fact, just made an ethanol-based concept\ncar when, in early 2008, a friend told him that Tesla needed a chief designer.\nAfter playing phone tag for a month with Musk\u2019s assistant, Mary Beth\nBrown, to inquire about the position, von Holzhausen finally got in touch\nand met Musk for an interview at the SpaceX headquarters.\nMusk instantly saw von Holzhausen, with his bouffant, trendy clothes\nand laid-back attitude, as a free-spirited, creative complement and wooed\nhim with vigor. They took a tour of the SpaceX factory in Hawthorne and\nTesla\u2019s headquarters in Silicon Valley. Both facilities were chaotic and\nreeked of start-up. Musk ramped up the charm and sold von Holzhausen on\nthe idea that he had a chance to shape the future of the automobile and that\nit made sense to leave his cushy job at a big, proven automaker for this\nonce-in-a-lifetime opportunity. \u201cElon and I went for a drive in the Roadster,\nand everyone was checking it out,\u201d von Holzhausen said. \u201cI knew I could\nstay at Mazda for ten years and get very comfortable or take a huge leap of\nfaith. At Tesla, there was no history, no baggage. There was just a vision of\nproducts that could change the world. Who wouldn\u2019t want to be involved\nwith that?\u201d\nWhile von Holzhausen knew the risks of going to a startup, he could not\nhave realized just how close Tesla was to bankruptcy when he joined the\ncompany in August 2008. Musk had coaxed von Holzhausen away from a\nsecure job and into the jaws of death. But in many ways, this is what von\nHolzhausen sought at this point in his career. Tesla did not feel as much like\na car company as a bunch of guys tinkering on a big idea. \u201cTo me, it was\nexciting,\u201d he said. \u201cIt was like a garage experiment, and it made cars cool\nagain.\u201d The suits were gone, and so were the veteran automotive hands\ndulled by years working in the industry. In their stead, von Holzhausen\nfound energetic geeks who didn\u2019t realize that what they wanted to do was\nborderline impossible. Musk\u2019s presence added to the energy and gave von\nHolzhausen confidence that Tesla actually could outflank much, much\nlarger competitors. \u201cElon\u2019s mind was always way beyond the present\nmoment,\u201d he said. \u201cYou could see that he was a step or three ahead of\neveryone else and one hundred percent committed to what we were doing.\u201d Von Holzhausen had examined the drawings of the Model S left by\nFisker and a clay model of the car and had come away unimpressed. \u201cIt was\na blob,\u201d he said. \u201cIt was clear to me that the people that had been working\non this were novices.\u201d Musk realized the same thing and tried to articulate\nwhat he wanted. Even though the words were not precise, they were good\nenough to give von Holzhausen a feel for Musk\u2019s vision and the confidence\nthat he could deliver on it. \u201cI said, \u2018We\u2019re going to start over. We\u2019re going\nto work together and make this awesome.\u2019\u201d\nTo save money, the Tesla design center came to life inside the SpaceX\nfactory. A handful of people on von Holzhausen\u2019s team took over one\ncorner and put up a tent to add some separation and secrecy to what they\nwere doing. In the tradition of many a Musk employee, von Holzhausen had\nto build his own office. He made a pilgrimage to IKEA to buy some desks\nand then went to an art store to get some paper and pens.\nAs von Holzhausen began sketching the outside of the Model S, the\nTesla engineers had started up a project to build another electric CLS. They\nripped this one down to its very core, removing all of the body structure and\nthen stretching the wheelbase by four inches to match up with some of the\nearly Model S specifications. Things began moving fast for everyone\ninvolved in the Model S project. In the span of about three months, von\nHolzhausen had designed 95 percent of what people see today with the\nModel S, and the engineers had started building a prototype exterior around\nthe skeleton.\nThroughout this process, von Holzhausen and Musk talked every day.\nTheir desks were close, and the men had a natural rapport. Musk said he\nwanted an aesthetic that borrowed from Aston Martin and Porsche and\nsome specific functions. He insisted, for example, that the car seat seven\npeople. \u201cIt was like \u2018Holy shit, how do we pull this off in a sedan?\u2019\u201d von\nHolzhausen said. \u201cBut I understood. He had five kids and wanted\nsomething that could be thought of as a family vehicle, and he knew other\npeople would have this issue.\u201d\nMusk wanted to make another statement with a huge touchscreen. This\nwas years before the iPad would be released. The touch-screens that people\nran into now and again at airports or shopping kiosks were for the most part\nterrible. But to Musk, the iPhone and all of its touch functions made it\nobvious that this type of technology would soon become commonplace. He\nwould make a giant iPhone and have it handle most of the car\u2019s functions. To find the right size for the screen, Musk and von Holzhausen would sit in\nthe skeleton car and hold up laptops of different sizes, placing them\nhorizontally and vertically to see what looked best. They settled on a\nseventeen-inch screen in a vertical position. Drivers would tap on this\nscreen for every task except for opening the glove box and turning on the\nemergency lights\u2014jobs required by law to be performed with physical\nbuttons.\nSince the battery pack at the base of the car would weigh so much,\nMusk, the designers, and the engineers were always looking for ways to\nreduce the Model S\u2019s weight in other spots. Musk opted to solve a big\nchunk of this problem by making the body of the Model S out of\nlightweight aluminum instead of steel. \u201cThe non-battery-pack portion of the\ncar has to be lighter than comparable gasoline cars, and making it all\naluminum became the obvious decision,\u201d Musk said. \u201cThe fundamental\nproblem was that if we didn\u2019t make it out of aluminum the car wasn\u2019t going\nto be any good.\u201d\nMusk\u2019s word choice there\u2014\u201cobvious decision\u201d\u2014goes a long way\ntoward explaining how he operates. Yes, the car needed to be light, and, yes,\naluminum would be an option for making that happen. But at the time, car\nmanufacturers in North America had almost no experience producing\naluminum body panels. Aluminum tends to tear when worked by large\npresses. It also develops lines that look like stretch marks on skin and make\nit difficult to lay down smooth coats of paint. \u201cIn Europe, you had some\nJaguars and one Audi that were made of aluminum, but it was less than five\npercent of the market,\u201d Musk said. \u201cIn North America, there was nothing.\nIt\u2019s only recently that the Ford F-150 has arrived as mostly aluminum.\nBefore that, we were the only one.\u201d Inside of Tesla, attempts were\nrepeatedly made to talk Musk out of the aluminum body, but he would not\nbudge, seeing it as the only rational choice. It would be up to the Tesla team\nto figure out how to make the aluminum manufacturing happen. \u201cWe knew\nit could be done,\u201d Musk said. \u201cIt was a question of how hard it would be\nand how long it would take us to sort it out.\u201d\nJust about all of the major design choices with the Model S came with\nsimilar challenges. \u201cWhen we first talked about the touch-screen, the guys\ncame back and said, \u2018There\u2019s nothing like that in the automotive supply\nchain,\u2019\u201d Musk said. \u201cI said, \u2018I know. That\u2019s because it\u2019s never been put in a\nfucking car before.\u2019\u201d Musk figured that computer manufacturers had tons of experience making seventeen-inch laptop screens and expected them to\nknock out a screen for the Model S with relative ease. \u201cThe laptops are\npretty robust,\u201d Musk said. \u201cYou can drop them and leave them out in the\nsun, and they still have to work.\u201d After contacting the laptop suppliers,\nTesla\u2019s engineers came back and said that the temperature and vibration\nloads for the computers did not appear to be up to automotive standards.\nTesla\u2019s supplier in Asia also kept pointing the carmaker to its automotive\ndivision instead of its computing division. As Musk dug into the situation\nmore, he discovered that the laptop screens simply had not been tested\nbefore under the tougher automotive conditions, which included large\ntemperature fluctuations. When Tesla performed the tests, the electronics\nended up working just fine. Tesla also started working hand in hand with\nthe Asian manufacturers to perfect their then-immature capacitive-touch\ntechnology and to find ways to hide the wiring behind the screen that made\nthe touch technology possible. \u201cI\u2019m pretty sure that we ended up with the\nonly seventeen-inch touch-screen in the world,\u201d Musk said. \u201cNone of the\ncomputer makers or Apple had made it work yet.\u201d\nThe Tesla engineers were radical by automotive industry standards but\neven they had problems fully committing to Musk\u2019s vision. \u201cThey wanted\nto put in a bloody switch or a button for the lights,\u201d Musk said. \u201cWhy\nwould we need a switch? When it\u2019s dark, turn the lights on.\u201d Next, the\nengineers put up resistance to the door handles. Musk and von Holzhausen\nhad been studying a bunch of preliminary designs in which the handles had\nyet to be drawn in and started to fall in love with how clean the car looked.\nThey decided that the handles should only present themselves when a\npassenger needed to get in the car. Right away, the engineers realized this\nwould be a technological pain, and they completely ignored the idea in one\nprototype version of the car, much to the dismay of Musk and von\nHolzhausen. \u201cThis prototype had the handles pivot instead of popping out,\u201d\nvon Holzhausen said. \u201cI was upset about it, and Elon said, \u2018Why the fuck is\nthis different? We\u2019re not doing this.\u2019\u201d\nTo crank up the pace of the Model S design, there were engineers\nworking all day and then others who would show up at 9 P.M. and work\nthrough the night. Both groups huddled inside of the 3,000-square-foot tent\nplaced on the SpaceX factory floor. Their workspace looked like a reception\narea at an outdoor wedding. \u201cThe SpaceX guys were amazingly respectful\nand didn\u2019t peek or ask questions,\u201d said Ali Javidan, one of the main engineers. As von Holzhausen delivered his specifications, the engineers\nbuilt the prototype body of the car. Every Friday afternoon, they brought\nwhat they had made into a courtyard behind the factory where Musk would\nlook it over and provide feedback. To run tests on the body, the car would\nbe loaded up with ballast to represent five people and then do loops around\nthe factory until it overheated or broke down.\nThe more von Holzhausen learned about Tesla\u2019s financial struggles, the\nmore he wanted the public to see the Model S. \u201cThings were so precarious,\nand I didn\u2019t want to miss our opportunity to get this thing finished and show\nit to the world,\u201d he said. That moment came in March 2009, when, just six\nmonths after von Holzhausen had arrived, Tesla unveiled the Model S at a\npress event held at SpaceX.\nAmid rocket engines and hunks of aluminum, Tesla showcased a gray\nModel S sedan. From a distance, the display model looked glamorous and\nrefined. The media reports from the day described the car as the love child\nof an Aston Martin and a Maserati. In reality, the sedan barely held together.\nIt still had the base structure of a Mercedes CLS, although no one in the\npress knew that, and some of the body panels and the hood were stuck to\nthe frame with magnets. \u201cThey could just slide the hood right off,\u201d said\nBruce Leak, a Tesla owner invited to attend the event. \u201cIt wasn\u2019t really\nattached. They would put it back on and try and align it to get the fit and\nfinish right, but then someone would push on it, and it would move again. It\nwas one of those Wizard of Oz, man behind the curtain moments.\u201d A couple\nof the Tesla engineers practiced test-driving the car for a couple of days\nleading up to the event to make sure that they knew just how long the car\nwould go before it overheated. While not perfect, the display accomplished\nexactly what Musk had intended. It reminded people that Tesla had a\ncredible plan to make electric cars more mainstream and that its cars were\nfar more ambitious than what big-time automakers like GM and Nissan\nseemed to have in mind both from a design and a range perspective.\nThe messy reality behind the display was that the odds of Tesla\nadvancing the Model S from a prop to a sellable car were infinitesimal. The\ncompany had the technical know-how and the will for the job. It just didn\u2019t\nhave much money or a factory that could crank out cars by the thousands.\nBuilding an entire car would require blanking machines that take sheets of\naluminum and chop them up into the appropriate size for doors, hoods, and\nbody panels. Next up would be the massive stamping machines and metal dies used to take the aluminum and bend it into precise shapes. Then there\nwould be dozens of robots that would aid in assembling the cars, computer-\ncontrolled milling machines for precise metalwork, painting equipment, and\na bevy of other machines for running tests. It was an investment that would\nrun into the hundreds of millions of dollars. Musk would also need to hire\nthousands of workers.\nAs with SpaceX, Musk preferred to build as much of Tesla\u2019s vehicles\nin-house as possible, but the high costs were limiting just how much Tesla\ncould take on. \u201cThe original plan was that we would do final assembly,\u201d\nsaid Diarmuid O\u2019Connell, the vice president of business development at\nTesla. Partners would stamp out the body parts, do the welding and handle\nthe painting, and ship everything to Tesla, where workers would turn the\nparts into a whole car. Tesla proposed to build a factory to handle this type\nof work first in Albuquerque, New Mexico, and then later in San Jose,\nCalifornia, and then pulled back on these proposals, much to the dismay of\ncity officials in both locales. The public hemming and hawing around\npicking the factory site did little to inspire confidence in Tesla\u2019s ability to\nknock out a second car and generated the same type of negative headlines\nthat had surrounded the Roadster\u2019s protracted delivery.\nO\u2019Connell had joined Tesla in 2006 to help solve some of the factory\nand financing issues. He grew up near Boston in a middle-class Irish family\nand went on to earn a bachelor\u2019s degree from Dartmouth College. After\nthat, O\u2019Connell attended the University of Virginia to get a master\u2019s degree\nin foreign policy and then Northwestern, where he got an MBA from the\nKellogg School of Management. He had fancied himself a scholar of the\nSoviet Union and its foreign and economic policy and had studied these\nareas at UVa. \u201cBut then, in 1988 and 1989, they\u2019re starting to close down\nthe Soviet Union, and, at the very least, I had a brand problem,\u201d O\u2019Connell\nsaid. \u201cIt started looking to me like I was heading to a career in academia or\nintelligence.\u201d It was then that O\u2019Connell\u2019s career took a detour into the\nbusiness world, where he became a management consultant working for\nMcCann Erickson Worldwide, Young & Rubicam, and Accenture, advising\ncompanies like Coca-Cola and AT&T.\nO\u2019Connell\u2019s career path changed more drastically in 2001 when the\nplanes hit the twin towers in New York. In the wake of the terrorist attacks,\nO\u2019Connell, like many people, decided to serve the United States in any\ncapacity that he could. In his late thirties, he had missed the window to be a soldier and instead focused his attention on trying to get into national\nsecurity work. O\u2019Connell went from office to office in Washington, D.C.,\nlooking for a job and had little luck until Lincoln Bloomfield, the assistant\nsecretary of state for political-military affairs, heard him out. Bloomfield\nneeded someone who could help prioritize missions in the Middle East and\nmake sure the right people were working on the right things, and he figured\nthat O\u2019Connell\u2019s management consulting experience made him a nice fit for\nthe job. O\u2019Connell became Bloomfield\u2019s chief of staff and dealt with a wide\nrange of charged situations, from trade negotiations to setting up an\nembassy in Baghdad. After gaining security clearance, O\u2019Connell also had\naccess to a daily report that collected information from intelligence and\nmilitary personnel on the status of operations in Iraq and Afghanistan.\n\u201cEvery morning at six A.M., the first thing to hit my desk was this overnight\nreport that included information on who got killed and what killed them,\u201d\nO\u2019Connell said. \u201cI kept thinking, This is insane. Why are we in this place?\nIt was not just Iraq but the whole picture. Why were we so invested in that\npart of the world?\u201d The unsurprising answer that O\u2019Connell came up with\nwas oil.\nThe more O\u2019Connell dug into the United States\u2019 dependence on foreign\noil, the more frustrated and despondent he became. \u201cMy clients were\nbasically the combat commanders\u2014people in charge of Latin America and\nCentral Command,\u201d he said. \u201cAs I talked with them and studied and\nresearched, I realized that even in peacetime, so many of our assets were\nemployed to support the economic pipeline around oil.\u201d O\u2019Connell decided\nthat the rational thing to do for his country and for his newborn son was to\nalter this equation. He looked at the wind industry and the solar industry\nand the traditional automakers but came away unconvinced that what they\nwere doing could have a radical enough impact on the status quo. Then,\nwhile reading Businessweek, he stumbled on an article about a start-up\ncalled Tesla Motors and went to the company\u2019s website, which described\nTesla as a place \u201cwhere we are doing things, not talking about things.\u201d \u201cI\nsent an e-mail telling them I had come from the national security area and\nwas really passionate about reducing our dependence on oil and figured it\nwas just a dead-letter type of thing,\u201d O\u2019Connell said. \u201cI got an e-mail back\nthe next day.\u201d\nMusk hired O\u2019Connell and quickly dispatched him to Washington, D.C.,\nto start poking around on what types of tax credits and rebates Tesla might be able to drum up around its electric vehicles. At the same time, O\u2019Connell\ndrafted an application for a Department of Energy stimulus package.* \u201cAll I\nknew is that we were going to need a shitload of money to build this\ncompany,\u201d O\u2019Connell said. \u201cMy view was that we needed to explore\neverything.\u201d Tesla had been looking for between $100 million and $200\nmillion, grossly underestimating what it would take to build the Model S.\n\u201cWe were na\u00efve and learning our way in the business,\u201d O\u2019Connell said.\nIt January 2009, Tesla took over Porsche\u2019s usual spot at the Detroit auto\nshow, getting the space cheap because so many other car companies had\nbailed out on the event. Fisker had a luxurious booth across the hallway\nwith wood flooring and pretty blond booth babes draped over its car. Tesla\nhad the Roadster, its electric powertrain, and no frills.\nThe technology that Tesla\u2019s engineers displayed proved good enough to\nattract the attention of the big boys. Not long after the show, Daimler voiced\nsome interest in seeing what an electric Mercedes A Class car might look\nand feel like. Daimler executives said they would visit Tesla in about a\nmonth to discuss this proposition in detail, and the Tesla engineers decided\nto blow them away by producing two prototype vehicles before the visit.\nWhen the Daimler executives saw what Tesla had done, they ordered four\nthousand of Tesla\u2019s battery packs for a fleet of test vehicles in Germany.\nThe Tesla team pulled off the same kind of feats for Toyota and won its\nbusiness, too.\nIn May 2009, things started to take off for Tesla. The Model S had been\nunveiled, and Daimler followed that by acquiring a 10 percent stake in Tesla\nfor $50 million. The companies also formed a strategic partnership to have\nTesla provide the battery packs for one thousand of Daimler\u2019s Smart cars.\n\u201cThat money was important and went a long way back then,\u201d said\nO\u2019Connell. \u201cIt was also a validation. Here is the company that invented the\ninternal combustion engine, and they are investing in us. It was a seminal\nmoment, and I am sure it gave the guys over at the DOE the feeling that we\nwere real. It\u2019s not just our scientists saying this stuff is good. It\u2019s Mercedes\nfreaking Benz.\u201d\nSure enough, in January 2010, the Department of Energy struck a $465\nmillion loan agreement with Tesla.* The money was far more than Tesla\nhad ever expected to get from the government. But it still represented just a\nfraction of the $1 billion plus that most carmakers needed to bring a new\nvehicle to market. So, while Musk and O\u2019Connell were thrilled to get the money, they still wondered if Tesla would be able to live up to the bargain.\nTesla would need one more windfall or, perhaps, to steal a car factory. And\nin May 2010, that\u2019s more or less what it did.\nGeneral Motors and Toyota had teamed up in 1984 to build New United\nMotor Manufacturing Inc., or NUMMI, on the site of a former GM\nassembly plant in Fremont, California, a city on the outskirts of Silicon\nValley. The companies hoped the joint facility would combine the best of\nAmerican and Japanese automaking skills and result in higher-quality,\ncheaper cars. The factory went on to pump out millions of vehicles like the\nChevy Nova and Toyota Corolla. Then the recession hit, and GM found\nitself trying to climb out of bankruptcy. It decided to abandon the plant in\n2009, and Toyota followed right after, saying it would close down the whole\nfacility, leaving five thousand people without jobs.\nAll of a sudden, Tesla had the chance to buy a 5.3-million-square-foot\nplant in its backyard. Just one month after the last Toyota Corolla went off\nthe manufacturing line in April 2010, Tesla and Toyota announced a\npartnership and transfer of the factory. Tesla agreed to pay $42 million for a\nlarge portion of the factory (once worth $1 billion), while Toyota invested\n$50 million in Tesla for a 2.5 percent stake in the company. Tesla had\nbasically secured a factory, including the massive metal-stamping machines\nand other equipment, for free.*\nThe string of fortunate turns for Tesla left Musk feeling good. Just after\nthe factory deal closed in the summer of 2010, Tesla started the process of\nfiling for an initial public offering. The company obviously needed as much\ncapital as it could get to bring the Model S to market and push forward with\nits other technology projects. Tesla hoped to raise about $200 million.\nFor Musk, going public represented something of a Faustian bargain.\nEver since the Zip2 and PayPal days, Musk has done everything in his\npower to maintain absolute control over his companies. Even if he remained\nthe largest shareholder in Tesla, the company would be subjected to the\ncapricious nature of the public markets. Musk, the ultimate long-term\nthinker, would face constant second-guessing from investors looking for\nshort-term returns. Tesla would also be subject to public scrutiny, as it\nwould be forced to open its books for public consumption. This was bad\nbecause Musk prefers to operate in secrecy and because Tesla\u2019s financial\nsituation looked awful. The company had one product (the Roadster), had\nhuge development costs, and had bordered on bankruptcy months earlier. The car blog Jalopnik greeted the Tesla IPO as a Hail Mary rather than a\nsound fiscal move. \u201cFor lack of a better phrase, Tesla is a money pit,\u201d the\nblog wrote. \u201cSince the company\u2019s founding in 2003, it\u2019s managed to incur\nover $290 million in losses on just $147.6 million in revenue.\u201d Told by a\nsource that Tesla hoped to sell 20,000 units of the Model S per year at\n$58,000 a pop, Jalopnik scoffed. \u201cEven considering the supposed pent-up\ndemand among environmentalists for a car like the Model S, those are\nambitious goals for a small company planning to launch a niche luxury\nproduct into a soft market. Frankly, we\u2019re skeptical. We\u2019ve seen how brutal\nand unforgiving the market can be, and other automakers aren\u2019t simply\ngoing to roll over and surrender that volume to Tesla.\u201d Other pundits\nconcurred with this assessment.\nTesla went public on June 29, 2010, nonetheless. It raised $226 million,\nwith the company\u2019s shares shooting up 41 percent that day. Investors looked\npast Tesla\u2019s $55.7 million loss in 2009 and the more than $300 million the\ncompany had spent in seven years. The IPO stood as the first for an\nAmerican carmaker since Ford went public in 1956. Competitors continued\nto treat Tesla like an annoying, ankle-biting dachshund. Nissan\u2019s CEO,\nCarlos Ghosn, used the event to remind people that Tesla was but a\npipsqueak and that his company had plans to pump out up to 500,000\nelectric cars by 2012.\nFlush with funds, Musk began expanding some of the engineering teams\nand formalizing the development work around the Model S. Tesla\u2019s main\noffices moved from San Mateo to a larger building in Palo Alto, and von\nHolzhausen expanded the design team in Los Angeles. Javidan hopped\nbetween projects, helping develop technology for the electrified Mercedes-\nBenz, an electric Toyota Rav4, and prototypes of the Model S. The Tesla\nteam worked fast inside of a tiny lab with about 45 people knocking out 35\nRav4 test vehicles at the rate of about two cars per week. The alpha version\nof the Model S, including newly stamped body parts from the Fremont\nfactory, a revamped battery pack, and revamped power electronics, came to\nlife in the basement of the Palo Alto office. \u201cThe first prototype was\nfinished at about two A.M.,\u201d Javidan said. \u201cWe were so excited that we\ndrove it around without glass, any interior, or a hood.\u201d\nA day or two later, Musk came to check out the vehicle. He jumped into\nthe car and drove it to the opposite end of the basement, where he could\nspend some time alone with it. He got out and walked around the vehicle, and then the engineers came over to hear his take on the machine. This\nprocess would be repeated many times in the months to come. \u201cHe would\ngenerally be positive but constructive,\u201d Javidan said. \u201cWe would try and get\nhim rides whenever we could, and he might ask for the steering to be tighter\nor something like that before running off to another meeting.\u201d\nAbout a dozen of the alpha cars were produced. A couple went to\nsuppliers like Bosch to begin work on the braking systems, while others\nwere used for various tests and design tweaks. Tesla\u2019s executives kept the\nvehicles rotating on a strict schedule, giving one team two weeks for cold-\nweather testing and then shipping that alpha car to another team right away\nfor powertrain tuning. \u201cThe guys from Toyota and Daimler were blown\naway,\u201d Javidan said. \u201cThey might have two hundred alpha cars and several\nhundred to a thousand beta cars. We were doing everything from crash tests\nto the interior design with about fifteen cars. That was amazing to them.\u201d\nTesla employees developed similar techniques to their counterparts at\nSpaceX for dealing with Musk\u2019s high demands. The savvy engineers knew\nbetter than to go into a meeting and deliver bad news without some sort of\nalternative plan at the ready. \u201cOne of the scariest meetings was when we\nneeded to ask Elon for an extra two weeks and more money to build out\nanother version of the Model S,\u201d Javidan said. \u201cWe put together a plan,\nstating how long things would take and what they would cost. We told him\nthat if he wanted the car in thirty days it would require hiring some new\npeople, and we presented him with a stack of resumes. You don\u2019t tell Elon\nyou can\u2019t do something. That will get you kicked out of the room. You need\neverything lined up. After we presented the plan, he said, \u2018Okay, thanks.\u2019\nEveryone was like, \u2018Holy shit, he didn\u2019t fire you.\u2019\u201d\nThere were times when Musk would overwhelm the Tesla engineers\nwith his requests. He took a Model S prototype home for a weekend and\ncame back on the Monday asking for around eighty changes. Since Musk\nnever writes anything down, he held all the alterations in his head and\nwould run down the checklist week by week to see what the engineers had\nfixed. The same engineering rules as those at SpaceX applied. You did what\nMusk asked or were prepared to burrow down into the properties of\nmaterials to explain why something could not be done. \u201cHe always said,\n\u2018Take it down to the physics,\u2019\u201d Javidan said.\nAs the development of the Model S neared completion in 2012, Musk\nrefined his requests and dissection style. He went over the Model S with von Holzhausen every Friday at Tesla\u2019s design studio in Los Angeles. Von\nHolzhausen and his small team had moved out of the corner in the SpaceX\nfactory and gotten their own hangar-shaped facility near the rear of the\nSpaceX complex.* The building had a few offices and then one large, wide-\nopen area where various mock-ups of vehicles and parts awaited inspection.\nDuring a visit I made in 2012, there was one complete Model S, a skeletal\nversion of the Model X\u2014an as yet to be released SUV\u2014and a selection of\ntires and hubcaps lined up against the wall. Musk sank into the Model S\ndriver seat and von Holzhausen climbed into the passenger seat. Musk\u2019s\neyes darted around for a few moments and then settled onto the sun visor. It\nwas beige and a visible seam ran around the edge and pushed the fabric out.\n\u201cIt\u2019s fish-lipped,\u201d Musk said. The screws attaching the visor to the car were\nvisible as well, and Musk insisted that every time he saw them it felt like\ntiny daggers were stabbing him in the eyes. The whole situation was\nunacceptable. \u201cWe have to decide what is the best sun visor in the world\nand then do better,\u201d Musk said. A couple of assistants taking notes outside\nof the car jotted this down.\nThis process played out again with the Model X. This was to be Tesla\u2019s\nmerger of an SUV and a minivan built off the Model S foundation. Von\nHolzhausen had four different versions of the vehicle\u2019s center console\nresting on the floor, so that they could be slotted in one by one and viewed\nby Musk. The pair spent most of their time, however, agonizing over the\nmiddle row of seats. Each one had an independent base so that each\npassenger could adjust his seat rather than moving the whole row\ncollectively. Musk loved the freedom this gave the passenger but grew\nconcerned after seeing all three seats in different positions. \u201cThe problem is\nthat they will never be aligned and might look a mess,\u201d Musk said. \u201cWe\nhave to make sure they are not too hodgy podgy.\u201d\nThe idea of Musk as a design expert has long struck me as bizarre. He\u2019s\na physicist at heart and an engineer by demeanor. So much of who Musk is\nsays that he should fall into that Silicon Valley stereotype of the schlubby\nnerd who would only know good design if he read about it in a textbook.\nThe truth is that there might be some of that going on with Musk, and he\u2019s\nturned it into an advantage. He\u2019s very visual and can store things that others\nhave deemed to look good away in his brain for recall at any time. This\nprocess has helped Musk develop a good eye, which he\u2019s combined with his\nown sensibilities, while also refining his ability to put what he wants into words. The result is a confident, assertive perspective that does resonate\nwith the tastes of consumers. Like Steve Jobs before him, Musk is able to\nthink up things that consumers did not even know they wanted\u2014the door\nhandles, the giant touch-screen\u2014and to envision a shared point of view for\nall of Tesla\u2019s products and services. \u201cElon holds Tesla up as a product\ncompany,\u201d von Holzhausen said. \u201cHe\u2019s passionate that you have to get the\nproduct right. I have to deliver for him and make sure it\u2019s beautiful and\nattractive.\u201d\nWith the Model X, Musk again turned to his role as a dad to shape some\nof the flashiest design elements of the vehicle. He and von Holzhausen were\nwalking around the floor of an auto show in Los Angeles, and they both\ncomplained about the awkwardness of getting to the middle and back row\nseats in an SUV. Parents who have felt their backs wrench while trying to\nangle a child and car seat into a vehicle know this reality all too well, as\ndoes any decent-sized human who has tried to wedge into a third row seat.\n\u201cEven on a minivan, which is supposed to have more room, almost one-\nthird of the entry space is covered by the sliding door,\u201d von Holzhausen\nsaid. \u201cIf you could open up the car in a way that is unique and special, that\ncould be a real game changer. We took that kernel of an idea back and\nworked up forty or fifty design concepts to solve the problem, and I think\nwe ended up with one of the most radical ones.\u201d The Model X has what\nMusk coined as \u201cfalcon-wing doors.\u201d They\u2019re hinged versions of the gull-\nwing doors found on some high-end cars like the DeLorean. The doors go\nup and then flop over in a constrained enough way that the Model X won\u2019t\nrub up against a car parked close to it or hit the ceiling in a garage. The end\nresult is that a parent can plop a child in the second-row passenger seat\nwithout needing to bend over or twist at all.\nWhen Tesla\u2019s engineers first heard about the falcon-wing doors, they\ncringed. Here was Musk with another crazy ask. \u201cEveryone tried to come\nup with an excuse as to why we couldn\u2019t do it,\u201d Javidan said. \u201cYou can\u2019t put\nit in the garage. It won\u2019t work with things like skis. Then, Elon took a demo\nmodel to his house and showed us that the doors opened. Everyone is\nmumbling, \u2018Yeah, in a fifteen-million-dollar house, the doors will open just\nfine.\u2019\u201d Like the controversial door handles on the Model S, the Model X\u2019s\ndoors have become one of its most striking features and the thing\nconsumers talk about the most. \u201cI was one of the first people to test it out\nwith a kid\u2019s car seat,\u201d Javidan said. \u201cWe have a minivan, and you have to be a contortionist to get the seat into the middle row. Compared to that, the\nModel X was so easy. If it\u2019s a gimmick, it\u2019s a gimmick that works.\u201d\nDuring my 2012 visit to the design studio, Tesla had a number of\ncompetitors\u2019 vehicles in the parking lot nearby, and Musk made sure to\ndemonstrate the limitations of their seating compared to the Model X. He\ntried with honest effort to sit in the third row of an Acura SUV, but, even\nthough the car claimed to have room for seven, Musk\u2019s knees were pressed\nup to his chin, and he never really fit into the seat. \u201cThat\u2019s like a midget\ncave,\u201d he said. \u201cAnyone can make a car big on the outside. The trick is to\nmake it big on the inside.\u201d Musk went from one rival\u2019s car to the next,\nilluminating the vehicles\u2019 flaws for me and von Holzhausen. \u201cIt\u2019s good to\nget a sense for just how bad the other cars are,\u201d he said.\nWhen these statements fly out of Musk\u2019s mouth, it\u2019s momentarily\nshocking. Here\u2019s a guy who needed nine years to produce about three\nthousand cars ridiculing automakers that build millions of vehicles every\nyear. In that context, his ribbing comes off as absurd.\nMusk, though, approaches everything from a Platonic perspective. As\nhe sees it, all of the design and technology choices should be directed\ntoward the goal of making a car as close to perfect as possible. To the extent\nthat rival automakers haven\u2019t, that\u2019s what Musk is judging. It\u2019s almost a\nbinary experience for him. Either you\u2019re trying to make something\nspectacular with no compromises or you\u2019re not. And if you\u2019re not, Musk\nconsiders you a failure. This position can look unreasonable or foolish to\noutsiders, but the philosophy works for Musk and constantly pushes him\nand those around him to their limits.\nOn June 22, 2012, Tesla invited all of its employees, some select\ncustomers, and the press to its factory in Fremont to watch as the first\nModel S sedans were taken home. Depending on which of the many\npromised delivery dates you pick, the Model S was anywhere from eighteen\nmonths to two-plus years late. Some of the delays were a result of Musk\u2019s\nrequests for exotic technologies that needed to be invented. Other delays\nwere simply a function of this still quite young automaker learning how to\nproduce an immaculate luxury vehicle and needing to go through the trial\nand error tied to becoming a more mature, more refined company.\nThe outsiders were blown away by their first glimpse of the Tesla\nfactory. Musk had T-E-S-L-A painted in enormous black letters on the side\nof the building so that people driving by on the freeway, or flying above for that matter, were made well aware of the company\u2019s presence. The inside of\nthe factory, once dressed in the dark, dingy tones of General Motors and\nToyota, had taken on the Musk aesthetic. The floors received a white epoxy,\nthe walls and beams were painted white, the thirty-foot tall stamping\nmachines were white, and then much of the other machinery, like the teams\nof the robots, had been painted red, making the place look like an industrial\nversion of Santa Claus\u2019s workshop. Just as he did at SpaceX, Musk placed\nthe desks of his engineers right on the factory floor, where they worked in\nan area cordoned off by rudimentary cubicle dividers. Musk had a desk in\nthis area as well.*\nThe Model S launch event took place in a section of the factory where\nthey finish off the cars. There\u2019s a part of the floor with various grooves and\nbumps that the cars pass over, as technicians listen for any rattles. There\u2019s\nalso a chamber where water can be sprayed at high pressure onto the car to\ncheck for leaks. For the very last inspection, the Model S cruises onto a\nraised platform made out of bamboo, which, when coupled with lots of\nLED lighting, is meant to provide an abundant amount of contrast so that\npeople can spot flaws on the body. For the few first months that the Model\nS came off the line, Musk went to this bamboo stage to inspect every\nvehicle. \u201cHe was down on all fours looking up under the wheel well,\u201d said\nSteve Jurvetson, the investor and Tesla board member.\nHundreds of people had gathered around this stage to watch as the first\ndozen or so cars were presented to their owners. Many of the employees\nwere factory workers who had once been part of the autoworkers\u2019 union,\nlost their jobs when the NUMMI plant closed, and were now back at work\nagain, making the car of the future. They waved American flags and wore\nred, white, and blue visors. A handful of the workers cried as the Model S\nsedans were lined up on the stage. Even Musk\u2019s most cynical critics would\nhave softened for a moment while watching the proceedings. Say what you\nwill about Tesla receiving government money or hyping up the promise of\nthe electric car, it was trying to do something big and different, and people\nwere getting hired by the thousands as a result. With machines humming in\nthe background, Musk gave a brief speech and then handed the owners their\nkeys. They drove off the bamboo platform and out the factory doors, while\nthe Tesla employees provided a standing ovation.\nJust four weeks earlier, SpaceX had flown cargo to the International\nSpace Station and had its capsule returned to Earth\u2014firsts all around for a private company. That feat coupled with the launch of the Model S led to a\nrapid transformation in the way the world outside of Silicon Valley\nperceived Musk. The guy who was always promising, promising, promising\nwas doing\u2014and doing spectacular things. \u201cI may have been optimistic with\nrespect to the timing on some of these things, but I didn\u2019t over-promise on\nthe outcome,\u201d Musk told me during an interview after the Model S launch.\n\u201cI have done everything I said I was going to do.\u201d\nMusk did not have Riley around to celebrate with and share in this run\nof good fortune. They had divorced, and Musk had begun to think about\ndating again, if he could find the time. Even with this turmoil in his\npersonal life, however, Musk had reached a point of calm that he had not\nfelt in many years. \u201cMy main emotion is that there is a bit of weight off my\nshoulders,\u201d he said at the time. Musk took his boys to Maui to meet up with\nKimbal and other relatives, marking his first real vacation in a number of\nyears.\nIt was right after this holiday that Musk let me have the first substantial\nglimpse into his life. Skin still peeling off his sunburnt arms, Musk met\nwith me at the Tesla and SpaceX headquarters, at the Tesla design studio,\nand at a Beverley Hills screening of a documentary he had helped sponsor.\nThe film, Baseball in the Time of Cholera, was good but grim and explored\na cholera outbreak in Haiti. It turned out that Musk had visited Haiti the\nprevious Christmas, filling his jet with toys and MacBook Airs for an\norphanage. Bryn Mooser, the codirector of the film, told me that during a\nbarbecue Musk had taught the kids how to fire off model rockets and then\nlater went to visit a village deeper in the jungle by traveling in a dugout\ncanoe. After the screening, Musk and I hung out on the street for a bit away\nfrom the crowd. I noted aloud that everyone wants to make him out as the\nTony Stark character but that he didn\u2019t really exude that \u201cplayboy drinking\nscotch while zooming through Afghanistan in an army convoy\u201d vibe. He\nfired back, pointing to the Haitian canoe ride. \u201cI got wasted, too, on some\ndrink they call the Zombie,\u201d Musk said. He smiled and then invited me to\ngrab some drinks across the street at Mr. Chow to celebrate the movie. All\nseemed to be going well for Musk, and he savored the moment.\nThis restful period did not last long and soon enough Tesla\u2019s battle for\nsurvival resumed. The company could only produce about ten sedans per\nweek at the outset and had thousands of back orders that it needed to fulfill.\nShort sellers, those investors who bet a company\u2019s share price will fall, had taken huge positions in Tesla, making it the most shorted stock out of one\nhundred of the largest companies listed on the NASDAQ exchange. The\nnaysayers expected numerous Model S flaws to crop up and undermine the\nenthusiasm for the car, to the point that people started canceling their orders\nin bulk. There were also huge doubts that Tesla could ramp up production in\na meaningful way and do so profitably. In October 2012, the presidential\nhopeful Mitt Romney dubbed Tesla \u201ca loser,\u201d while slagging off a couple of\nother government-backed green technology companies (the solar panel\nmaker Solyndra and Fisker) during a debate with Barack Obama.14\nWhile the doubters placed huge wagers on Tesla\u2019s impending failure,\nMusk\u2019s bluster mode engaged. He began talking about Tesla\u2019s goals to\nbecome the most profitable major automobile maker in the world, with\nbetter margins than BMW. Then, in September 2012, he unveiled something\nthat shocked both Tesla critics and proponents alike. Tesla had secretly been\nbuilding the first leg of a network of charging stations. The company\ndisclosed the location of six stations in California, Nevada, and Arizona and\npromised that hundreds more would be on the way. Tesla intended to build a\nglobal charging network that would let Model S owners making long drives\npull off the highway and recharge very quickly. And they would be able to\ndo so for free. In fact, Musk insisted that Tesla owners would soon be able\nto travel across the United States without spending a penny on fuel. Model\nS drivers would have no trouble finding these stations, not only because the\ncars\u2019 onboard computers would guide them to the nearest one but because\nMusk and von Holzhausen had designed giant red and white monoliths to\nherald the appearance of the stations.\nThe Supercharging stations, as Tesla called them, represented a huge\ninvestment for the strapped company. An argument could easily be made\nthat spending money on this sort of thing at such a precarious moment in\nthe Model S and Tesla\u2019s history was somewhere between daft and batshit\ncrazy. Surely Musk did not have the gall to try to revamp the very idea of\nthe automobile and build an energy network at the same time with a budget\nequivalent to what Ford and ExxonMobil spend on their annual holiday\nparties. But that was the exact plan. Musk, Straubel, and others inside Tesla\nhad mapped out this all-or-nothing play long ago and built certain features\ninto the Model S with the Superchargers in mind.*\nWhile the arrival of the Model S and the charging network garnered\nTesla a ton of headlines, it remained unclear if the positive press and good vibes would last. Serious trade-offs had been made as Tesla rushed to get\nthe Model S to market. The car had some spectacular, novel features. But\neveryone inside of the company knew that as far as luxury sedans went, the\nModel S did not match up feature to feature with cars from BMW and\nMercedes-Benz. The first few thousand Model S cars, for example, would\nship without the parking sensors and radar-assisted cruise control common\non other high-end cars. \u201cIt was either hire a team of fifty people right away\nto make one of these things happen or implement things as best and as fast\nas you could,\u201d Javidan said.\nThe subpar fit and finish also proved hard to explain. The early adopters\ncould tolerate a windshield wiper going haywire for a couple of days, but\nthey wanted to see seats and visors that met the $100,000 price tag. While\nTesla did its best to source the highest-quality materials, it struggled at\ntimes to convince the top suppliers to take the company seriously.15 \u201cPeople\nwere very suspect that we would deliver one thousand Model Ss,\u201d said von\nHolzhausen. \u201cIt was frustrating because we had the drive internally to make\nthe car perfect but could not get the same commitment externally. With\nsomething like the visor, we ended up having to go to a third-rate supplier\nand then work on fixing the situation after the car had already started\nshipping.\u201d The cosmetic issues, though, were minor compared to a\ntumultuous set of internal circumstances, revealed in detail here for the first\ntime, that threatened to bankrupt the company once again.\nMusk had hired George Blankenship, a former Apple executive, to run\nits stores and service-center operations. At Apple, Blankenship worked just\na couple of doors down from Steve Jobs and received credit for building\nmuch of the Apple Store strategy. When Tesla first hired Blankenship, the\npress and public were atwitter, anticipating that\u2019d he do something\nspectacular and at odds with the traditions of the automotive industry.\nBlankenship did some of that. He expanded Tesla\u2019s number of stores\nthroughout the world and imbued them with that Apple Store vibe. Along\nwith showcasing the Model S, the Tesla stores sold hoodies and hats and\nhad areas in the back where kids would find crayons and Tesla coloring\nbooks. Blankenship gave me a tour of the Tesla store on Santana Row, the\nglitzy shopping center in San Jose. He came off as a warm, grandfatherly\nsort who saw Tesla as his chance to make a difference. \u201cThe typical dealer\nwants to sell you a car on the spot to clear inventory off his lot,\u201d\nBlankenship said. \u201cThe goal here is to develop a relationship with Tesla and electric vehicles.\u201d Tesla, he said, wanted to turn the Model S into more than\na car. Ideally it would be an object of desire just like the iPod and iPhone.\nBlankenship noted that Tesla had more than ten thousand reservations for\nthe Model S at the time, the vast majority of which had arrived without the\ncustomers test-driving the car. A lot of this early interest resulted from the\naura surrounding Musk, who Blankenship said came off as similar to Jobs\nbut with a toned-down control-freak vibe. \u201cThis is the first place I have\nworked that is going to change the world,\u201d Blankenship said, taking a jab at\nthe sometimes trivial nature of Apple\u2019s gadgets.\nWhile Musk and Blankenship got along at first, their relationship fell\napart during the latter stages of 2012. Tesla did have a large number of\nreservations in which people put down $5,000 for the right to buy a Model\nS and get in the purchase queue. But the company had struggled to turn\nthese reservations into actual sales. The reasons behind this problem remain\nunclear. It may have been that the complaints about the interior and the\nearly kinks mentioned on the Tesla forums and message boards were\ncausing concerns. Tesla also lacked financing options to soften the blow of\nbuying a $100,000 car, while uncertainty surrounded the resale market for\nthe Model S. You might end up with the car of the future or you might\nspend six figures on a dud with a battery pack that loses its capacity, and\nwith no secondary buyer. Tesla\u2019s service centers at the time were also\nterrible. The early cars were unreliable and customers were being sent in\ndroves to centers unprepared to handle the volume. Many prospective Tesla\nowners likely wanted to hang out on the sidelines for a bit longer to make\nsure that the company would remain viable. As Musk put it, \u201cThe word of\nmouth on the car sucked.\u201d\nBy the middle of February 2013, Tesla had fallen into a crisis state. If it\ncould not convert its reservations to purchases quickly, its factory would sit\nidle, costing the company vast amounts of money. And if anyone caught\nwind of the factory slowdown, Tesla\u2019s shares would likely plummet,\nprospective owners would become even more cautious, and the short sellers\nwould win. The severity of this problem had been hidden from Musk, but\nonce he learned about it, he acted in his signature all-or-nothing fashion.\nMusk pulled people from recruiting, the design studio, engineering, finance,\nand wherever else he could find them and ordered them to get on the phone,\ncall people with reservations, and close deals. \u201cIf we don\u2019t deliver these\ncars, we are fucked,\u201d Musk told the employees. \u201cSo, I don\u2019t care what job you were doing. Your new job is delivering cars.\u201d He placed Jerome\nGuillen, a former Daimler executive, in charge of fixing the service issues.\nMusk fired senior leaders whom he deemed subpar performers and\npromoted a flood of junior people who had been doing above-average work.\nHe also made an announcement personally guaranteeing the resale price of\nthe Model S. Customers would be able to resell their cars for the average\ngoing rate of similar luxury sedans with Musk putting his billions behind\nthis pledge. And then Musk tried to orchestrate the ultimate fail-safe for\nTesla just in case his maneuvers did not work.\nDuring the first week of April, Musk reached out to his friend Larry\nPage at Google. According to people familiar with their discussion, Musk\nvoiced his concerns about Tesla\u2019s ability to survive the next few weeks. Not\nonly were customers failing to convert their reservations to orders at the rate\nMusk hoped, but existing customers had also started to defer their orders as\nthey heard about upcoming features and new color choices. The situation\ngot so bad that Tesla had to shut down its factory. Publicly, Tesla said it\nneeded to conduct maintenance on the factory, which was technically true,\nalthough the company would have soldiered on had the orders been closing\nas expected. Musk explained all of this to Page and then struck a handshake\ndeal for Google to acquire Tesla.\nWhile Musk did not want to sell, the deal seemed like the only viable\ncourse for Tesla\u2019s future. Musk\u2019s biggest fear about an acquisition was that\nthe new owner would not see Tesla\u2019s goals through to their conclusion. He\nwanted to make sure that the company would end up producing a mass-\nmarket electric vehicle. Musk proposed terms under which he would remain\nin control of Tesla for eight years or until it started pumping out a mass-\nmarket car. Musk also asked for access to $5 billion in capital for factory\nexpansions. Some of Google\u2019s lawyers were put off by these demands, but\nMusk and Page continued to talk about the deal. Given Tesla\u2019s value at the\ntime, it was thought that Google would need to pay about $6 billion for the\ncompany.\nAs Musk, Page, and Google\u2019s lawyers debated the parameters of an\nacquisition, a miracle happened. The five hundred or so people whom Musk\nhad turned into car salesmen quickly sold a huge volume of cars. Tesla,\nwhich only had a couple weeks of cash left in the bank, moved enough cars\nin the span of about fourteen days to end up with a blowout first fiscal\nquarter. Tesla stunned Wall Street on May 8, 2013, by posting its first-ever profit as a public company\u2014$11 million\u2014on $562 million in sales. It\ndelivered 4,900 Model S sedans during the period. This announcement sent\nTesla\u2019s shares soaring from about $30 a share to $130 per share in July. Just\na couple of weeks after revealing the first-quarter results, Tesla paid off its\n$465 million loan from the government early and with interest. Tesla\nsuddenly appeared to have vast cash reserves at its disposal, and the short\nsellers were forced to take massive losses. The solid performance of the\nstock increased consumers\u2019 confidence, creating a virtuous circle for Tesla.\nWith cars selling and Tesla\u2019s value rising, the deal with Google was no\nlonger necessary, and Tesla had become too expensive to buy. The talks\nwith Google ended.*\nWhat transpired next was the Summer of Musk. Musk put his public\nrelations staff on high alert, telling them that he wanted to try to have one\nTesla announcement per week. The company never quite lived up to that\npace, but it did issue statement after statement. Musk held a series of press\nconferences that addressed financing for the Model S, the construction of\nmore charging stations, and the opening of more retail stores. During one\nannouncement, Musk noted that Tesla\u2019s charging stations were solar-\npowered and had batteries on-site to store extra juice. \u201cI was joking that\neven if there\u2019s some zombie apocalypse, you\u2019ll still be able to travel\nthroughout the country using the Tesla Supercharger system,\u201d Musk said,\nsetting the bar very high for CEOs at other automakers. But the biggest\nevent by far was held in Los Angeles, where Tesla unveiled another secret\nfeature of the Model S.\nIn June 2013, Tesla cleared the prototype vehicles out of its Los\nAngeles design studio and invited Tesla owners and the media for a flashy\nevening soiree. Hundreds of people showed up, driving their pricey Model\nS sedans through the grungy streets of Hawthorne and parking in between\nthe design studio and the SpaceX factory. The studio had been converted\ninto a lounge. The lighting was dim, and the floor had been covered in\nAstroTurf and tiered to make plateaus where people could mingle or plop\ndown on couches. Women in tight black dresses cruised through the crowd,\nserving drinks. Daft Punk\u2019s \u201cGet Lucky\u201d played on the sound system. A\nstage had been built at the front of the room, but before Musk ascended it he\nmingled with the masses. It was clear that he had become a rock star for\nTesla owners\u2014every bit the equivalent of Steve Jobs for the Apple faithful. People surrounded him and asked to take pictures. Meanwhile, Straubel\nstood off to the side, often totally alone.\nAfter people had a couple of drinks, Musk fought through the crowd to\nthe front of the room, where old TV commercials projected onto a screen\nabove the stage showed families stopping by Esso and Chevron stations.\nThe kids were so happy to see the Esso tiger mascot. \u201cGas is a weird thing\nto love,\u201d Musk said. \u201cHonestly.\u201d That\u2019s when he brought a Model S up\nonstage. A hole opened up in the floor beneath the car. It had been possible\nall along, Musk said, to replace the battery pack underneath the Model S in\na matter of seconds\u2014the company just hadn\u2019t told anyone about this. Tesla\nwould now start adding battery swapping at its charging stations as a\nquicker option to recharging. Someone could drive right over a pit where a\nrobot would take off the car\u2019s battery pack and install a new one in ninety\nseconds, at a cost equivalent to filling up with a tank of gas. \u201cThe only\ndecision that you have to make when you come to one of our Tesla stations\nis do you prefer faster or free,\u201d Musk said.*\nIn the months that followed, a couple of events threatened to derail the\nSummer of Musk. The New York Times penned a withering review of the\ncar and its charging stations, and a couple of the Model S sedans caught fire\nafter being involved in collisions. Disobeying conventional public relations\nwisdom, Musk went after the reporter, using data pulled from the car to\nundermine the reviewer\u2019s claims. Musk penned the feisty rebuttal himself,\nwhile on vacation in Aspen with Kimbal, and friend and Tesla board\nmember Antonio Gracias. \u201cAt some other company, it would be a public\nrelations group putting something like this together,\u201d Gracias said. \u201cElon\nfelt like it was the most important problem facing Tesla at the time and\nthat\u2019s always what he deals with and how he prioritizes. It could kill the car\nand represented an existential threat against the business. Have there been\nmoments where his unconventional style in these types of situations has\nmade me cringe? Yes. But I trust that it will work out in the end.\u201d Musk\napplied a similar approach to dealing with the fires by declaring the Model\nS the safest car in America in a press release and adding a titanium\nunderbody shield and aluminum plates to the vehicle to deflect and destroy\ndebris and keep the battery pack safe.16\nThe fires, the occasional bad review\u2014none of this had any effect on\nTesla\u2019s sales or share price. Musk\u2019s star shone brighter and brighter as\nTesla\u2019s market value ballooned to about half that of GM and Ford. Tesla held another press event in October 2014 that cemented Musk\u2019s\nplace as the new titan of the auto industry. Musk unveiled a supercharged\nversion of the Model S with two motors\u2014one in the front and one in the\nback. It could go zero to 60 in 3.2 seconds. The company had turned a\nsedan into a supercar. \u201cIt\u2019s like taking off from a carrier deck,\u201d Musk said.\n\u201cIt\u2019s just bananas.\u201d Musk also unveiled a new suite of software for the\nModel S that gave it autopilot functions. The car had radar to detect objects\nand warn of possible collisions and could guide itself via GPS. \u201cLater, you\nwill be able to summon the car,\u201d Musk said. \u201cIt will come to wherever you\nare. There\u2019s also something else I would like to do. Many of our engineers\nwill be hearing this in real time. I would like the charge connector to plug\nitself into the car, sort of like an articulating snake. I think we will probably\ndo something like that.\u201d\nThousands of people waited in line for hours to see Musk demonstrate\nthis technology. Musk cracked jokes during the presentation and played off\nthe crowd\u2019s enthusiasm. The man who had been awkward in front of media\nduring the PayPal years had developed a unique, slick stagecraft. A woman\nstanding next to me in the crowd went weak in the knees when Musk first\ntook the stage. A man to my other side said he wanted a Model X and had\njust offered $15,000 to a friend to move up on the reservation list, so that he\ncould end up with model No. 700. The enthusiasm coupled with Musk\u2019s\nability to generate attention was emblematic of just how far the little\nautomaker and its eccentric CEO had come. Rival car companies would kill\nto receive such interest and had basically been left dumbfounded as Tesla\nsnuck up on them and delivered more than they had ever imagined possible.\nAs the Model S fever gripped Silicon Valley, I visited Ford\u2019s small\nresearch and development lab in Palo Alto. The head of the lab at the time\nwas a ponytailed, sandal-wearing engineer named T. J. Giuli, who felt very\njealous of Tesla. Inside of every Ford were dozens of computing systems\nmade by different companies that all had to speak to each other and work as\none. It was a mess of complexity that had evolved over time, and\nsimplifying the situation would prove near impossible at this point,\nespecially for a company like Ford, which needed to pump out hundreds of\nthousands of cars per year and could not afford to stop and reboot. Tesla, by\ncontrast, got to start from scratch and make its own software the focus of\nthe Model S. Giuli would have loved the same opportunity. \u201cSoftware is in\nmany ways the heart of the new vehicle experience,\u201d he said. \u201cFrom the powertrain to the warning chimes in the car, you\u2019re using software to create\nan expressive and pleasing environment. The level of integration that the\nsoftware has into the rest of the Model S is really impressive. Tesla is a\nbenchmark for what we do here.\u201d Not long after this chat, Giuli left Ford to\nbecome an engineer at a stealth start-up.\nThere was little the mainstream auto industry could do to slow Tesla\ndown. But that didn\u2019t stop executives from trying to be difficult whenever\npossible. Tesla, for example, wanted to call its third-generation car the\nModel E, so that its lineup of vehicles would be the Model S, E, and X\u2014\nanother playful Musk gag. But Ford\u2019s then CEO, Alan Mulally, blocked\nTesla from using Model E, with the threat of a lawsuit. \u201cSo I call up\nMulally and I was like, \u2018Alan, are you just fucking with us or are you really\ngoing to do a Model E?\u2019\u201d Musk said. \u201cAnd I\u2019m not sure which is worse.\nYou know? Like it would actually make more sense if they\u2019re just fucking\nwith us because if they actually come out with a Model E at this point, and\nwe\u2019ve got the Model S and the X and Ford comes out with the Model E, it\u2019s\ngoing to look ridiculous. So even though Ford did the Model T a hundred\nyears ago, nobody thinks of \u2018Model\u2019 as being a Ford thing anymore. So it\nwould just feel like they stole it. Like why did you go steal Tesla\u2019s E? Like\nyou\u2019re some sort of fascist army marching across the alphabet, some sort of\nSesame Street robber. And he was like, \u2018No, no, we\u2019re definitely going to\nuse it.\u2019 And I was like, \u2018Oh, I don\u2019t think that\u2019s such a good idea because\npeople are going to be confused because it\u2019s not going to make sense.\nPeople aren\u2019t used to Ford having Model something these days. It\u2019s usually\ncalled like the Ford Fusion.\u2019 And he was like, no, his guys really want to\nuse that. That\u2019s terrible.\u201d After that, Tesla registered the trademark for\nModel Y as another joke. \u201cIn fact, Ford called us up deadpan and said, \u2018We\nsee you\u2019ve registered Model Y. Is that what you\u2019re going to use instead of\nthe Model E?\u2019\u201d Musk said. \u201cI\u2019m like, \u2018No, it\u2019s a joke. S-E-X-Y. What does\nthat spell?\u2019 But trademark law is a dry profession it turns out.\u201d*\nWhat Musk had done that the rival automakers missed or didn\u2019t have\nthe means to combat was turn Tesla into a lifestyle. It did not just sell\nsomeone a car. It sold them an image, a feeling they were tapping into the\nfuture, a relationship. Apple did the same thing decades ago with the Mac\nand then again with the iPod and iPhone. Even those who were not religious\nabout their affiliation to Apple were sucked into its universe once they\nbought the hardware and downloaded software like iTunes. This sort of relationship is hard to pull off if you don\u2019t control as much\nof the lifestyle as possible. PC makers that farmed their software out to\nMicrosoft, their chips to Intel, and their design to Asia could never make\nmachines as beautiful and as complete as Apple\u2019s. They also could not\nrespond in time as Apple took this expertise to new areas and hooked\npeople on its applications.\nYou can see Musk\u2019s embrace of the car as lifestyle in Tesla\u2019s\nabandonment of model years. Tesla does not designate cars as being 2014s\nor 2015s, and it also doesn\u2019t have \u201call the 2014s in stock must go, go, go\nand make room for the new cars\u201d sales. It produces the best Model S it can\nat the time, and that\u2019s what the customer receives. This means that Tesla\ndoes not develop and hold on to a bunch of new features over the course of\nthe year and then unleash them in a new model all at once. It adds features\none by one to the manufacturing line when they\u2019re ready. Some customers\nmay be frustrated to miss out on a feature here and there. Tesla, however,\nmanages to deliver most of the upgrades as software updates that everyone\ngets, providing current Model S owners with pleasant surprises.\nFor the Model S owner, the all-electric lifestyle translates into a less\nhassled existence. Instead of going to the gas station, you just plug the car\nin at night, a rhythm familiar to anyone with a smartphone. The car will\nstart charging right away or the owner can tap into the Model S\u2019s software\nand schedule charging to take place late at night, when the cheapest\nelectricity rates are available. Tesla owners not only dodge gas stations;\nthey mostly get to skip out on visits to mechanics. A traditional vehicle\nneeds oil and transmission fluid changes to deal with all the friction and\nwear and tear produced by its thousands of moving parts. The simpler\nelectric car design eliminates this type of maintenance. Both the Roadster\nand the Model S also take advantage of what\u2019s known as regenerative\nbraking, which extends the life of the brakes. During stop-and-go situations,\nthe Tesla will brake by kicking the motor into reverse via software and\nslowing down the wheels instead of using brake pads and friction to clamp\nthem down. The Tesla motor generates electricity during this process and\nfunnels it back to the batteries, which is why electric cars get better mileage\nin city traffic. Tesla still recommends that owners bring in the Model S once\na year for a checkup but that\u2019s mostly to give the vehicle a once-over and\nmake sure that none of the components seems to be wearing down\nprematurely. Even Tesla\u2019s approach to maintenance is philosophically different from\nthat of the traditional automotive industry. Most car dealers make the\nmajority of their profits from servicing cars. They treat vehicles like a\nsubscription service, expecting people to visit their service centers multiple\ntimes a year for many years. This is the main reason dealerships have\nfought to block Tesla from selling its cars directly to consumers.* \u201cThe\nultimate goal is to never have to bring your car back in after you buy it,\u201d\nsaid Javidan. The dealers charge more than independent mechanics but give\npeople the peace of mind that their car is being worked on by a specialist\nfor a particular make of vehicle. Tesla makes its profits off the initial sale of\nthe car and then from some optional software services. \u201cI got the number\nten Model S,\u201d said Konstantin Othmer,17 the Silicon Valley software whiz\nand entrepreneur. \u201cIt was an awesome car, but it had just about every issue\nyou might have read about in the forums. They would fix all these things\nand decided to trailer the car back to the shop so that they didn\u2019t add any\nmiles to it. Then I went in for a one-year service, and they spruced up\neverything so that the car was better than new. It was surrounded by velvet\nropes in the service center. It was just beautiful.\u201d\nTesla\u2019s model isn\u2019t just about being an affront to the way carmakers and\ndealers do business. It\u2019s a more subtle play on how electric cars represent a\nnew way to think of automobiles. All car companies will soon follow\nTesla\u2019s lead and offer some form of over-the-air updates to their vehicles.\nThe practicality and scope of their updates will be limited, however. \u201cYou\njust can\u2019t do an over-the-air sparkplug change or replacement of the timing\nbelt,\u201d said Javidan. \u201cWith a gas car, you have to get under the hood at some\npoint and that forces you back to the dealership anyway. There\u2019s no real\nincentive for Mercedes to say, \u2018You don\u2019t need to bring the car in,\u2019 because\nit\u2019s not true.\u201d Tesla also has the edge of having designed so many of the key\ncomponents for its cars in-house, including the software running throughout\nthe vehicle. \u201cIf Daimler wants to change the way a gauge looks, it has to\ncontact a supplier half a world away and then wait for a series of\napprovals,\u201d Javidan said. \u201cIt would take them a year to change the way the\n\u2018P\u2019 on the instrument panel looks. At Tesla, if Elon decides he wants a\npicture of a bunny rabbit on every gauge for Easter, he can have that done in\na couple of hours.\u201d*\nAs Tesla turned into a star of modern American industry, its closest\nrivals were obliterated. Fisker Automotive filed for bankruptcy and was bought by a Chinese auto parts company in 2014. One of its main investors\nwas Ray Lane, a venture capitalist at Kleiner Perkins Caufield & Byers.\nLane had cost Kleiner Perkins a chance to invest in Tesla and then backed\nFisker\u2014a disastrous move that tarnished the firm\u2019s brand and Lane\u2019s\nreputation. Better Place was another start-up that enjoyed more hype than\nFisker and Tesla put together and raised close to $1 billion to build electric\ncars and battery-swapping stations.18 The company never produced much of\nanything and declared bankruptcy in 2013.\nThe guys like Straubel who had been at Tesla since the beginning are\nquick to remind people that the chance to build an awesome electric car had\nbeen there all along. \u201cIt\u2019s not really like there was a rush to this idea, and\nwe got there first,\u201d Straubel said. \u201cIt is frequently forgotten in hindsight that\npeople thought this was the shittiest business opportunity on the planet. The\nventure capitalists were all running for the hills.\u201d What separated Tesla\nfrom the competition was the willingness to charge after its vision without\ncompromise, a complete commitment to execute to Musk\u2019s standards. 11 THE UNIFIED FIELD THEORY OF ELON\nMUSK\nT\nHE RIVE BROTHERS USED TO BE LIKE A TECHNOLOGY GANG.\nIn the late 1990s, they would jump on skateboards and zip around the\nstreets of Santa Cruz, knocking on the doors of businesses and asking if\nthey needed any help managing their computing systems. The young men,\nwho had all grown up in South Africa with their cousin Elon Musk, soon\ndecided there must be an easier way to hawk their technology smarts than\ngoing door-to-door. They wrote some software that allowed them to take\ncontrol of their clients\u2019 systems from afar and to automate many of the\nstandard tasks that companies required, such as installing updates for\napplications. The software became the basis of a new company called\nEverdream, and the brothers promoted their technology in some compelling\nways. Billboards went up around Silicon Valley in which Lyndon Rive, a\nbuff underwater hockey player,* stood naked with his pants around his\nankles, while holding a computer in front of his crotch. Up above his photo,\nthe tagline for the ad read, \u201cDon\u2019t get caught with your systems down.\u201d\nBy 2004, Lyndon and his brothers, Peter and Russ, wanted a new\nchallenge\u2014something that not only made them money but, as Lyndon put\nit, \u201csomething that made us feel good every single day.\u201d Near the end of the\nsummer that year, Lyndon rented an RV and set out with Musk for the\nBlack Rock desert and the madness of Burning Man. The men used to go on\nadventures all the time when they were kids and looked forward to the long\ndrive as a way to catch up and brainstorm about their businesses. Musk\nknew that Lyndon and his brothers were angling for something big. While\ndriving, Musk turned to Lyndon and suggested that he look into the solar\nenergy market. Musk had studied it a bit and thought there were some\nopportunities that others had missed. \u201cHe said it was a good place to get\ninto,\u201d Lyndon recalled.\nAfter arriving at Burning Man, Musk, a regular at the event, and his\nfamily went through their standard routines. They set up camp and prepped their art car for a drive. This year, they had cut the roof off a small car,\nelevated the steering wheel, shifted it to the right so that it was placed near\nthe middle of the vehicle, and replaced the seats with a couch. Musk took a\nlot of pleasure in driving the funky creation.19 \u201cElon likes to see the\nrawness of people there,\u201d said Bill Lee, his longtime friend. \u201cIt\u2019s his version\nof camping. He wants to go and drive the art cars and see installations and\nthe great light shows. He dances a lot.\u201d Musk put on a display of strength\nand determination at the event as well. There was a wooden pole perhaps\nthirty feet high with a dancing platform at the top. Dozens of people tried\nand failed to climb it, and then Musk gave it a go. \u201cHis technique was very\nawkward, and he should not have succeeded,\u201d said Lyndon. \u201cBut he hugged\nit and just inched up and inched up until he reached the top.\u201d\nMusk and the Rives left Burning Man enthused. The Rives decided to\nbecome experts on the solar industry and find the opportunity in the market.\nThey spent two years studying solar technology and the dynamics of the\nbusiness, reading research reports, interviewing people, and attending\nconferences along the way. It was during the Solar Power International\nconference that the Rive brothers really hit on what their business model\nmight be. Only about two thousand* people showed up for the event, and\nthey all fit into a couple of hotel conference rooms for presentations and\npanels. During one open discussion session, representatives from a handful\nof the world\u2019s largest solar installers were sitting onstage, and the moderator\nasked what they were doing to make solar panels more affordable for\nconsumers. \u201cThey all gave the same answer,\u201d Lyndon said. \u201cThey said,\n\u2018We\u2019re waiting for the cost of the panels to drop.\u2019 None of them were taking\nownership of the problem.\u201d\nAt the time, it was not easy for consumers to get solar panels on their\nhouses. You had to be very proactive, acquiring the panels and finding\nsomeone else to install them. The consumer paid up front and had to make\nan educated guess as to whether or not his or her house even got enough\nsunshine to make the ordeal worthwhile. On top of all this, people were\nreluctant to buy panels, knowing that the next year\u2019s models would be more\nefficient.\nThe Rives decided to make buying into the solar proposition much\nsimpler and formed a company called SolarCity in 2006. Unlike other\ncompanies, they would not manufacture their own solar panels. Instead they\nwould buy them and then do just about everything else in-house. They built software for analyzing a customer\u2019s current energy bill and the position of\ntheir house and the amount of sunlight it typically received to determine if\nsolar made sense for the property. They built up their own teams to install\nthe solar panels. And they created a financing system in which the customer\ndid not need to pay anything up front for the panels. The consumer leased\nthe panels over a number of years at a fixed monthly rate. Consumers got a\nlower bill overall, they were no longer subject to the constantly rising rates\nof typical utilities, and, if they sold their house, they could pass the contract\nto the new owner. At the end of the lease, the homeowner could also\nupgrade to new, more efficient panels. Musk had helped his cousins come\nup with this structure and become the company\u2019s chairman and its largest\nshareholder, owning about a third of SolarCity.\nSix years later, SolarCity had become the largest installer of solar panels\nin the country. The company had lived up to its initial goals and made\ninstalling the panels painless. Rivals were rushing to mimic its business\nmodel. SolarCity had benefited along the way from a collapse in the price\nof solar panels, which occurred after Chinese panel manufacturers flooded\nthe market with product. It had also expanded its business from consumers\nto businesses with companies like Intel, Walgreens, and Wal-Mart signing\nup for large installations. In 2012, SolarCity went public and its shares\nsoared higher in the months that followed. By 2014, SolarCity was valued\nat close to $7 billion.\nDuring the entire period of SolarCity\u2019s growth, Silicon Valley had\ndumped huge amounts of money into green technology companies with\nmostly disastrous results. There were the automotive flubs like Fisker and\nBetter Place, and Solyndra, the solar cell maker that conservatives loved to\nhold up as a cautionary tale of government spending and cronyism run\namok. Some of the most famous venture capitalists in history, like John\nDoerr and Vinod Khosla, were ripped apart by the local and national press\nfor their failed green investments. The story was almost always the same.\nPeople had thrown money at green technology because it seemed like the\nright thing to do, not because it made business sense. From new kinds of\nenergy storage systems to electric cars and solar panels, the technology\nnever quite lived up to its billing and required too much government\nfunding and too many incentives to create a viable market. Much of this\ncriticism was fair. It\u2019s just that there was this Elon Musk guy hanging\naround who seemed to have figured something out that everyone else had missed. \u201cWe had a blanket rule against investing in clean-tech companies\nfor about a decade,\u201d said Peter Thiel, the PayPal cofounder and venture\ncapitalist at Founders Fund. \u201cOn the macro level, we were right because\nclean tech as a sector was quite bad. But on the micro level, it looks like\nElon has the two most successful clean-tech companies in the U.S. We\nwould rather explain his success as being a fluke. There\u2019s the whole Iron\nMan thing in which he\u2019s presented as a cartoonish businessman\u2014this very\nunusual animal at the zoo. But there is now a degree to which you have to\nask whether his success is an indictment on the rest of us who have been\nworking on much more incremental things. To the extent that the world still\ndoubts Elon, I think it\u2019s a reflection on the insanity of the world and not on\nthe supposed insanity of Elon.\u201d\nSolarCity, like the rest of Musk\u2019s ventures, did not represent a business\nopportunity so much as it represented a worldview. Musk had decided long\nago\u2014in his very rational manner\u2014that solar made sense. Enough solar\nenergy hits the Earth\u2019s surface in about an hour to equal a year\u2019s worth of\nworldwide energy consumption from all sources put together.20\nImprovements in the efficiency of solar panels have been happening at a\nsteady clip. If solar is destined to be mankind\u2019s preferred energy source in\nthe future, then this future ought to be brought about as quickly as possible.\nStarting in 2014, SolarCity began to make the full extent of its\nambitions more obvious. First, the company began selling energy storage\nsystems. These units were built through a partnership with Tesla Motors.\nBattery packs were manufactured at the Tesla factory and stacked inside\nrefrigerator-sized metal cases. Businesses and consumers could purchase\nthese storage systems to augment their solar panel arrays. Once they were\ncharged up, the battery units could be used to help large customers get\nthrough the night or during unexpected outages. Customers could also pull\nfrom the batteries instead of the grid during peak energy use periods, when\nutilities tend to tack on extra charges. While SolarCity rolled the storage\nunits out in a modest, experimental fashion, the company expects most of\nits customers to buy the systems in the years ahead to smooth out the solar\nexperience and help people and businesses leave the electrical grid\naltogether.\nThen, in June 2014, SolarCity acquired a solar cell maker called Silevo\nfor $200 million. This deal marked a huge shift in strategy. SolarCity would\nno longer buy its solar panels. It would make them at a factory in New York State. Silevo\u2019s cells were said to be 18.5 percent efficient at turning light\ninto energy, compared to 14.5 percent for most cells, and the expectations\nwere that the company could reach 24 percent efficiency with the right\nmanufacturing techniques. Buying, rather than manufacturing, solar panels\nhad been one of SolarCity\u2019s great advantages. It could capitalize on the glut\nin the solar cell market and avoid the large capital expenditures tied to\nbuilding and running factories. With 110,000 customers, however, SolarCity\nhad started to consume so many solar panels that it needed to ensure a\nconsistent supply and price. \u201cWe are currently installing more solar than\nmost of the companies are manufacturing,\u201d said Peter Rive, the cofounder\nand chief technology officer at SolarCity. \u201cIf we do the manufacturing\nourselves and take advantage of some different technology, our costs will be\nlower\u2014and this business has always been about lowering the costs.\u201d\nAfter adding the leases, the storage units, and the solar cell\nmanufacturing together, it became clear to close observers of SolarCity that\nthe company had morphed into something resembling a utility. It had built\nout a network of solar systems all under its control and managed by the\ncompany\u2019s software. By the end of 2015, SolarCity expects to have\ninstalled 2 gigawatts\u2019 worth of solar panels, producing 2.8 terawatt-hours of\nelectricity per year. \u201cThis would put us on a path to fulfill our goal to\nbecome one of the largest suppliers of electricity in the United States,\u201d the\ncompany said after announcing these figures in a quarterly earnings\nstatement. The reality is that SolarCity accounts for a tiny fraction of the\nUnited States\u2019 annual energy consumption and has a long way to go to\nbecome a major supplier of electricity in the country. There can, however,\nbe little doubt that Musk intends for the company to be a dominant force in\nthe solar industry and in the energy industry overall.\nWhat\u2019s more, SolarCity is a key part of what can be thought of as the\nunified field theory of Musk. Each one of his businesses is interconnected\nin the short term and the long term. Tesla makes battery packs that\nSolarCity can then sell to end customers. SolarCity supplies Tesla\u2019s\ncharging stations with solar panels, helping Tesla to provide free recharging\nto its drivers. Newly minted Model S owners regularly opt to begin living\nthe Musk Lifestyle and outfit their homes with solar panels. Tesla and\nSpaceX help each other as well. They exchange knowledge around\nmaterials, manufacturing techniques, and the intricacies of operating\nfactories that build so much stuff from the ground up. For most of their histories, SolarCity, Tesla, and SpaceX have been the\nclear underdogs in their respective markets and gone to war against deep-\npocketed, entrenched competitors. The solar, automotive, and aerospace\nindustries remain larded down by regulation and bureaucracy, which favors\nincumbents. To people in these industries Musk came off as a wide-eyed\ntechnologist who could be easily dismissed and ridiculed and who, as a\ncompetitor, fell somewhere on the spectrum between annoying and full of\nshit. The incumbents did their usual thing using their connections in\nWashington to make life as miserable as possible on all three of Musk\u2019s\ncompanies, and they were pretty good at it.\nAs of 2012, Musk Co. turned into a real threat, and it became harder to\ngo at SolarCity, Tesla, or SpaceX as individual companies. Musk\u2019s star\npower had surged and washed over all three ventures at the same time.\nWhen Tesla\u2019s shares jumped, quite often SolarCity\u2019s did, too. Similar\noptimistic feelings accompanied successful SpaceX launches. They proved\nMusk knew how to accomplish the most difficult of things, and investors\nseemed to buy in more to the risks Musk took with his other enterprises.\nThe executives and lobbyists of aerospace, energy, and automotive\ncompanies were suddenly going up against a rising star of big business\u2014an\nindustrialist celebrity. Some of Musk\u2019s opponents started to fear being on\nthe wrong side of history or at least the wrong side of his glow. Others\nbegan playing really dirty.\nMusk has spent years buttering up the Democrats. He\u2019s visited the\nWhite House several times and has the ear of President Obama. Musk,\nhowever, is not a blind loyalist. He first and foremost backs the beliefs\nbehind Musk Co. and then uses any pragmatic means at his disposal to\nadvance his cause. Musk plays the part of the ruthless industrialist with a\nfierce capitalist streak better than most Republicans and has the credentials\nto back it up and earn support. The politicians in states like Alabama\nlooking to protect some factory jobs for Lockheed or in New Jersey trying\nto help out the automobile dealership lobby now have to contend with a guy\nwho has an employment and manufacturing empire spread across the entire\nUnited States. As of this writing, SpaceX had a factory in Los Angeles, a\nrocket test facility in central Texas, and had just started construction on a\nspaceport in South Texas. (SpaceX does a lot of business at existing launch\nsites in California and Florida, as well.) Tesla had its car factory in Silicon\nValley, the design center in Los Angeles, and had started construction on a battery factory in Nevada. (Politicians from Nevada, Texas, California, New\nMexico, and Arizona threw themselves at Musk over the battery factory,\nwith Nevada ultimately winning the business by offering Tesla $1.4 billion\nin incentives. This event confirmed not only Musk\u2019s soaring celebrity but\nalso his unmatched ability to raise funds.) SolarCity has created thousands\nof white- and blue-collar clean-tech jobs, and it will create manufacturing\njobs at the solar panel factory that\u2019s being built in Buffalo, New York. All\ntogether, Musk Co. employed about fifteen thousand people at the end of\n2014. Far from stopping there, the plan for Musk Co. calls for tens of\nthousands of more jobs to be created on the back of ever more ambitious\nproducts.\nTesla\u2019s primary focus throughout 2015 will be bringing the Model X to\nmarket. Musk expects the SUV to sell at least as well as the Model S and\nwants Tesla\u2019s factories to be capable of making 100,000 cars per year by the\nend of 2015 to keep up with demand for both vehicles. The major downside\naccompanying the Model X is its price. The SUV will start at the same lofty\nprices as the Model S, which limits the potential customer base. The hope,\nthough, is that the Model X turns into the luxury vehicle of choice for\nfamilies and solidifies the Tesla brand\u2019s connection with women. Musk has\npledged that the Supercharger network, service centers, and the battery-\nswap stations will be built out even more in 2015 to greet the arrival of the\nnew vehicle. Beyond the Model X, Tesla has started work on the second\nversion of the Roadster, talked about making a truck, and, in all seriousness,\nhas begun modeling a type of submarine car that could transition from road\nto water. Musk paid $1 million for the Lotus Esprit that Roger Moore drove\nunderwater in The Spy Who Loved Me and wants to prove that such a\nvehicle can be done. \u201cMaybe we\u2019ll make two or three, but it wouldn\u2019t be\nmore than that,\u201d Musk told the Independent newspaper. \u201cI think the market\nfor submarine cars is quite small.\u201d\nAt the opposite end of the sales spectrum, or so Musk hopes, will be\nTesla\u2019s third-generation car, or the Model 3. Due out in 2017, this four-door\ncar would come in around $35,000 and be the real measure of Tesla\u2019s\nimpact on the world. The company hopes to sell hundreds of thousands of\nthe Model 3 and make electric cars truly mainstream. For comparison,\nBMW sells about 300,000 Minis and 500,000 of its BMW 3 Series vehicles\nper year. Tesla would look to match those figures. \u201cI think Tesla is going to make a lot of cars,\u201d Musk said. \u201cIf we continue on the current growth rate, I\nthink Tesla will be one of the most valuable companies in the world.\u201d\nTesla already consumes a huge portion of the world\u2019s lithium ion battery\nsupply and will need far more batteries to produce the Model 3. This is why,\nin 2014, Musk announced plans to build what he dubbed the Gigafactory, or\nthe world\u2019s largest lithium ion manufacturing facility. Each Gigafactory will\nemploy about 6,500 people and help Tesla meet a variety of goals. It should\nfirst allow Tesla to keep up with the battery demand created by its cars and\nthe storage units sold by SolarCity. Tesla also expects to be able to lower\nthe costs of its batteries while improving their energy density. It will build\nthe Gigafactory in conjunction with longtime battery partner Panasonic, but\nit will be Tesla that is running the factory and fine-tuning its operations.\nAccording to Straubel, the battery packs coming out of the Gigafactory\nshould be dramatically cheaper and better than the ones built today,\nallowing Tesla not only to hit the $35,000 price target for the Model 3 but\nalso to pave the way for electric vehicles with 500-plus miles of range.\nIf Tesla actually can deliver an affordable car with 500 miles of range, it\nwill have built what many people in the auto industry insisted for years was\nimpossible. To do that while also constructing a worldwide network of free\ncharging stations, revamping the way cars are sold, and revolutionizing\nautomotive technology would be an exceptional feat in the history of\ncapitalism.\nIn early 2014, Tesla raised $2 billion by selling bonds. Tesla\u2019s ability to\nraise money from eager investors was a newfound luxury. Tesla had\nbordered on bankruptcy for much of its existence and been one major\ntechnical gaffe from obsolescence at all times. The money coupled with\nTesla\u2019s still-rising share price and strong sales has put the company in a\nposition to open lots of new stores and service centers while advancing its\nmanufacturing capabilities. \u201cWe don\u2019t necessarily need all of the money for\nthe Gigafactory right now, but I decided to raise it in advance because you\nnever know when there will be some bloody meltdown,\u201d Musk said. \u201cThere\ncould be external factors or there could be some unexpected recall and then\nsuddenly we need to raise money on top of dealing with that. I feel a bit like\nmy grandmother. She lived through the Great Depression and some real\nhard times. Once you\u2019ve been through that, it stays with you for a long\ntime. I\u2019m not sure it ever leaves really. So, I do feel joy now, but there\u2019s still\nthat nagging feeling that it might all go away. Even later in life when my grandmother knew there was really no possibility of her going hungry, she\nalways had this thing about food. With Tesla, I decided to raise a huge\namount of money just in case something terrible happens.\u201d\nMusk felt optimistic enough about Tesla\u2019s future to talk to me about\nsome of his more whimsical plans. He hopes to redesign the Tesla\nheadquarters in Palo Alto, a change employees would welcome. The\nbuilding, with its tiny, 1980s-era lobby and a kitchen that can barely handle\na few people making cereal21 at the same time, has none of the perks of a\ntypical Silicon Valley darling. \u201cI think our Tesla headquarters looks like\ncrap,\u201d Musk said. \u201cWe\u2019re going to spruce things up. Not to sort of the\nGoogle level. You have to be like making money hand over fist in order to\nbe able to spend money the way that Google does. But we\u2019re going to make\nour headquarters much nicer and put in a restaurant.\u201d Naturally, Musk had\nideas for some mechanical enhancements as well. \u201cEverybody around here\nhas slides in their lobbies,\u201d he said. \u201cI\u2019m actually wondering about putting\nin a roller coaster\u2014like a functional roller coaster at the factory in Fremont.\nYou\u2019d get in, and it would take you around factory but also up and down.\nWho else has a roller coaster? I\u2019m thinking about doing that with SpaceX,\ntoo. That one might be even bigger since SpaceX has like ten buildings\nnow. It would probably be really expensive, but I like the idea of it.\u201d\nWhat\u2019s fascinating is that Musk remains willing to lose it all. He doesn\u2019t\nwant to build just one Gigafactory but several. And he needs these facilities\nto be built quickly and flawlessly, so that they\u2019re cranking out massive\nquantities of batteries right as the Model 3 arrives. If need be, Musk will\nbuild a second Gigafactory to compete with the Nevada site and place his\nown employees in competition with each other in a race to make the\nbatteries first. \u201cWe\u2019re not really trying to sort of yank anyone\u2019s chain here,\u201d\nMusk said. \u201cIt\u2019s just like this thing needs to be completed on time. If we\nsuddenly find that we\u2019re leveling the ground and laying the foundation and\nwe\u2019re on a bloody Indian burial ground, then fuck. We can\u2019t say, \u2018Oh shit.\nLet\u2019s go back to the other place that we were thinking about and get a six-\nmonth reset.\u2019 Six months for this factory is a huge deal. Do the basic math\nand it\u2019s more than a billion dollars a month in lost revenue,* assuming we\nuse it to capacity. From a different standpoint, if we spend all the money to\nprepare the car factory in Fremont to triple the volume from 150,000 per\nyear to 450,000 or 500,000 cars and hire and train all the people, and we\u2019re just sitting there waiting for the factory to come on line, we\u2019d be burning\nmoney like it was going out of fashion. I think that could kill the company.\n\u201cA six-month offset would be like, like Gallipoli. You have to make\nsure you charge right after the bombardment. Don\u2019t fucking sit around for\ntwo hours so that the Turks can go back in the trenches. Timing is\nimportant. We have to do everything we can to minimize the timing risk.\u201d\nWhat Musk struggles to fathom is why other automakers with deeper\npockets aren\u2019t making similar moves. At a minimum, Tesla seems to have\ninfluenced consumers and the auto industry enough for there to be an\nexpected surge in demand for electric vehicles. \u201cI think we have moved the\nneedle for almost every car company,\u201d Musk said. \u201cJust the twenty-two\nthousand cars we sold in 2013 had a highly leveraged effect in pushing the\nindustry toward sustainable technology.\u201d It\u2019s true that the supply for lithium\nion batteries is already constrained, and Tesla looks like the only company\naddressing the problem in a meaningful way.\n\u201cThe competitors are all sort of pooh-poohing the Gigafactory,\u201d Musk\nsaid. \u201cThey think it\u2019s a stupid idea, that the battery supplier should just go\nbuild something like that. But I know all the suppliers, and I can tell you\nthat they don\u2019t like the idea of spending several billion dollars on a battery\nfactory. You\u2019ve got a chicken-and-egg problem where the car companies are\nnot going to commit to a giant volume because they\u2019re not sure you can sell\nenough electric cars. So, I know we can\u2019t get enough lithium ion batteries\nunless we build this bloody factory, and I know no one else is building this\nthing.\u201d\nThere\u2019s the potential that Tesla is setting itself up to capitalize on a\nsituation like the one Apple found itself in when it first introduced the\niPhone. Apple\u2019s rivals spent the initial year after the iPhone\u2019s release\ndismissing the product. Once it became clear Apple had a hit, the\ncompetitors had to catch up. Even with the device right in their hands, it\ntook companies like HTC and Samsung years to produce anything\ncomparable. Other once-great companies like Nokia and BlackBerry didn\u2019t\nwithstand the shock. If, and it\u2019s a big if, Tesla\u2019s Model 3 turned into a\nmassive hit\u2014the thing that everyone with enough money wanted because\nbuying something else would just be paying for the past\u2014then the rival\nautomakers would be in a terrible bind. Most of the car companies dabbling\nin electric vehicles continue to buy bulky, off-the-shelf batteries rather than\ndeveloping their own technology. No matter how much they wanted to respond to the Model 3, the automakers would need years to come up with a\nreal challenger and even then they might not have a ready supply of\nbatteries for their vehicles.\n\u201cI think it is going to be a bit like that,\u201d Musk said. \u201cWhen will the first\nnon-Tesla Gigafactory get built? Probably no sooner than six years from\nnow. The big car companies are so derivative. They want to see it work\nsomewhere else before they will approve the project and move forward.\nThey\u2019re probably more like seven years away. But I hope I\u2019m wrong.\u201d\nMusk speaks about the cars, solar panels, and batteries with such\npassion that it\u2019s easy to forget they are more or less sideline projects. He\nbelieves in the technologies to the extent that he thinks they\u2019re the right\nthings to pursue for the betterment of mankind. They\u2019ve also brought him\nfame and fortune. Musk\u2019s ultimate goal, though, remains turning humans\ninto an interplanetary species. This may sound silly to some, but there can\nbe no doubt that this is Musk\u2019s raison d\u2019\u00eatre. Musk has decided that man\u2019s\nsurvival depends on setting up another colony on another planet and that he\nshould dedicate his life to making this happen.\nMusk is now quite rich on paper. He was worth about $10 billion at the\ntime of this writing. When he started SpaceX more than a decade ago,\nhowever, he had far less capital at his disposal. He didn\u2019t have the fuck-you\nmoney of a Jeff Bezos, who handed his space company Blue Origin a\nkingly pile of cash and asked it to make Bezos\u2019s dreams come true. If Musk\nwanted to get to Mars, he would have to earn it by building SpaceX into a\nreal business. This all seems to have worked in Musk\u2019s favor. SpaceX has\nlearned to make cheap and effective rockets and to push the limits of\naerospace technology.\nIn the near term, SpaceX will begin testing its ability to take people into\nspace. It wants to perform a manned test flight by 2016 and to fly astronauts\nto the International Space Station for NASA the next year. The company\nwill also likely make a major move into building and selling satellites,\nwhich would mark an expansion into one of the most lucrative parts of the\naerospace business. Along with these efforts, SpaceX has been testing the\nFalcon Heavy\u2014its giant rocket capable of flying the biggest payloads in the\nworld\u2014and its reusable-rocket technology. In early 2015, SpaceX almost\nmanaged to land the first stage of its rocket on a platform in the ocean.\nOnce it succeeds, it will begin performing tests on land. In 2014, SpaceX also began construction on its own spaceport in South\nTexas. It has acquired dozens of acres where it plans to construct a modern\nrocket launch facility unlike anything the world has seen. Musk wants to\nautomate a great deal of the launch process, so that the rockets can be\nrefueled, stood up, and fired on their own with computers handling the\nsafety procedures. SpaceX wants to fly rockets several times a month for its\nbusiness, and having its own spaceport should help speed up such\ncapabilities. Getting to Mars will require an even more impressive set of\nskills and technology.\n\u201cWe need to figure out how to launch multiple times a day,\u201d Musk said.\n\u201cThe thing that\u2019s important in the long run is establishing a self-sustaining\nbase on Mars. In order for that to work\u2014in order to have a self-sustaining\ncity on Mars\u2014there would need to be millions of tons of equipment and\nprobably millions of people. So how many launches is that? Well, if you\nsend up 100 people at a time, which is a lot to go on such a long journey,\nyou\u2019d need to do 10,000 flights to get to a million people. So 10,000 flights\nover what period of time? Given that you can only really depart for Mars\nonce every two years, that means you would need like forty or fifty years.\n\u201cAnd then I think for each flight that departs to Mars you want to sort of\nlaunch the spacecraft into orbit and then have it be in a parking orbit and\nrefuel its tanks with propellant. Essentially, the spacecraft would use a\nbunch of its propellant to get to orbit, but then you send up a tanker\nspacecraft to fill up the propellant tanks of the spacecraft so that it can\ndepart for Mars at high speed and can do so and get there in three months\ninstead of six months and with a large payload. I don\u2019t have a detailed plan\nfor Mars but I know of something at least that would work, which is sort of\nthis all-methane system with a big booster, a spacecraft, and a tanker\npotentially. I think SpaceX will have developed a booster and spaceship in\nthe 2025 time frame capable of taking large quantities of people and cargo\nto Mars.\n\u201cThe thing that\u2019s important is to reach an economic threshold around the\ncost per person for a trip to Mars. If it costs $1 billion per person, there will\nbe no Mars colony. At around $1 million or $500,000 per person, I think it\u2019s\nhighly likely that there will be a self-sustaining Martian colony. There will\nbe enough people interested who will sell their stuff on Earth and move. It\u2019s\nnot about tourism. It\u2019s like people coming to America back in the New\nWorld days. You move, get a job there, and make things work. If you solve the transport problem, it\u2019s not that hard to make a pressurized transparent\ngreenhouse to live in. But if you can\u2019t get there in the first place, it doesn\u2019t\nmatter.\n\u201cEventually, you\u2019d need to heat Mars up if you want it to be an\nEarthlike planet, and I don\u2019t have a plan for that. That would take a long\ntime in the best of circumstances. It would probably take, I don\u2019t know,\nsomewhere between a century and a millennium. There\u2019s zero chance of it\nbeing terraformed and Earthlike in my lifetime. Not zero, but 0.001 percent\nchance, and you would have to take real drastic measures with Mars.\u201d*\nMusk spent months pacing around his home in Los Angeles late at night\nthinking about these plans for Mars and bouncing them off Riley, whom he\nremarried near the end of 2012.* \u201cI mean, there aren\u2019t that many people you\ncan talk to about this sort of thing,\u201d Musk said. These chats included Musk\ndaydreaming aloud about becoming the first man to set foot on the Red\nPlanet. \u201cHe definitely wants to be the first man on Mars,\u201d Riley said. \u201cI\nhave begged him not to be.\u201d Perhaps Musk enjoys teasing his wife or\nmaybe he\u2019s playing coy, but he denied this ambition during one of our late-\nnight chats. \u201cI would only be on the first trip to Mars if I was confident that\nSpaceX would be fine if I die,\u201d he said. \u201cI\u2019d like to go, but I don\u2019t have to\ngo. The point is not about me visiting Mars but about enabling large\nnumbers of people to go to the planet.\u201d Musk may not even go into space.\nHe does not plan to participate in SpaceX\u2019s upcoming human test flights. \u201cI\ndon\u2019t think that would be wise,\u201d he said. \u201cIt would be like the head of\nBoeing being a test pilot for a new plane. It\u2019s not the right thing for SpaceX\nor the future of space exploration. I might be on there if it\u2019s been flying for\nthree or four years. Honestly, if I never go to space, that will be okay. The\npoint is to maximize the probable life span of humanity.\u201d\nIt\u2019s difficult to gauge just how seriously the average person takes Musk\nwhen he talks like this. A few years ago, most people would have lumped\nhim into the category of people who hype up jet packs and robots and\nwhatever else Silicon Valley decided to fixate on for the moment. Then\nMusk filed away one accomplishment after another, transforming himself\nfrom big talker to one of Silicon Valley\u2019s most revered doers. Thiel has\nwatched Musk go through this maturation\u2014from the driven but insecure\nCEO of PayPal to a confident CEO who commands the respect of\nthousands. \u201cI think there are ways he has dramatically improved over time,\u201d\nsaid Thiel. Most impressive to Thiel has been Musk\u2019s ability to find bright, ambitious people and lure them to his companies. \u201cHe has the most talented\npeople in the aerospace industry working for him, and the same case can be\nmade for Tesla, where, if you\u2019re a talented mechanical engineer who likes\nbuilding cars, then you\u2019re going to Tesla because it\u2019s probably the only\ncompany in the U.S. where you can do interesting new things. Both\ncompanies were designed with this vision of motivating a critical mass of\ntalented people to work on inspiring things.\u201d Thiel thinks Musk\u2019s goal of\ngetting humans to Mars should be taken seriously and believes it gives the\npublic hope. Not everyone will identify with the mission but the fact that\nthere\u2019s someone out there pushing exploration and our technical abilities to\ntheir limits is important. \u201cThe goal of sending a man to Mars is so much\nmore inspiring than what other people are trying to do in space,\u201d Thiel said.\n\u201cIt\u2019s this going-back-to-the-future idea. There\u2019s been this long wind-down\nof the space program, and people have abandoned the optimistic visions of\nthe future that we had in the early 1970s. SpaceX shows there is a way\ntoward bringing back that future. There\u2019s great value in what Elon is\ndoing.\u201d\nThe true believers came out in full force in August 2013 when Musk\nunveiled something called the Hyperloop. Billed as a new mode of\ntransportation, this machine was a large-scale pneumatic tube like the ones\nused to send mail around offices. Musk proposed linking cities like Los\nAngeles and San Francisco via an elevated version of this kind of tube that\nwould transport people and cars in pods. Similar ideas had been proposed\nbefore, but Musk\u2019s creation had some unique elements. He called for the\ntube to run under low pressure and for the pods to float on a bed of air\nproduced by skis at their base. Each pod would be thrust forward by an\nelectromagnetic pulse, and motors placed throughout the tube would give\nthe pods added boosts as needed. These mechanisms could keep the pods\ngoing at 800 mph, allowing someone to travel from Los Angeles to San\nFrancisco in about thirty minutes. The whole thing would, of course, be\nsolar-powered and aimed at linking cities less than a thousand miles apart.\n\u201cIt makes sense for things like L.A. to San Francisco, New York to D.C.,\nNew York to Boston,\u201d Musk said at the time. \u201cOver one thousand miles, the\ntube cost starts to become prohibitive, and you don\u2019t want tubes every\nwhich way. You don\u2019t want to live in Tube Land.\u201d\nMusk had been thinking about the Hyperloop for a number of months,\ndescribing it to friends in private. The first time he talked about it to anyone outside of his inner circle was during one of our interviews. Musk told me\nthat the idea originated out of his hatred for California\u2019s proposed high-\nspeed rail system. \u201cThe sixty-billion-dollar bullet train they\u2019re proposing in\nCalifornia would be the slowest bullet train in the world at the highest cost\nper mile,\u201d Musk said. \u201cThey\u2019re going for records in all the wrong ways.\u201d\nCalifornia\u2019s high-speed rail is meant to allow people to go from Los\nAngeles to San Francisco in about two and a half hours upon its completion\nin\u2014wait for it\u20142029. It takes about an hour to fly between the cities today\nand five hours to drive, placing the train right in the zone of mediocrity,\nwhich particularly gnawed at Musk. He insisted the Hyperloop would cost\nabout $6 billion to $10 billion, go faster than a plane, and let people drive\ntheir cars onto a pod and drive out into a new city.\nAt the time, it seemed that Musk had dished out the Hyperloop proposal\njust to make the public and legislators rethink the high-speed train. He\ndidn\u2019t actually intend to build the thing. It was more that he wanted to show\npeople that more creative ideas were out there for things that might actually\nsolve problems and push the state forward. With any luck, the high-speed\nrail would be canceled. Musk said as much to me during a series of e-mails\nand phone calls leading up to the announcement. \u201cDown the road, I might\nfund or advise on a Hyperloop project, but right now I can\u2019t take my eye off\nthe ball at either SpaceX or Tesla,\u201d he wrote.\nMusk\u2019s tune, however, started to change after he released the paper\ndetailing the Hyperloop. Bloomberg Businessweek had the first story on it,\nand the magazine\u2019s Web server began melting down as people stormed the\nwebsite to read about the invention. Twitter went nuts as well. About an\nhour after Musk released the information, he held a conference call to talk\nabout the Hyperloop, and somewhere in between our numerous earlier chats\nand that moment, he\u2019d decided to build the thing, telling reporters that he\nwould consider making at least a prototype to prove that the technology\ncould work. Some people had their fun with all of this. \u201cBillionaire unveils\nimaginary space train,\u201d teased Valleywag. \u201cWe love Elon Musk\u2019s nutso\ndetermination\u2014there was certainly a time when electric cars and private\nspace flight seemed silly, too. But what\u2019s sillier is treating this as anything\nother than a very rich man\u2019s wild imagination.\u201d Unlike its early Tesla-\nbashing days, Valleywag was now the minority voice. People seemed\nmainly to believe Musk could do it. The depth to which people believed it, I\nthink, surprised Musk and forced him to commit to the prototype. In a weird life-imitating-art moment, Musk really had become the closest thing the\nworld had to Tony Stark, and he could not let his adoring public down.\nShortly after the release of the Hyperloop plans, Shervin Pishevar, an\ninvestor and friend of Musk\u2019s, brought the detailed specifications for the\ntechnology with him during a ninety-minute meeting with President Obama\nat the White House. \u201cThe president fell in love with the idea,\u201d Pishevar\nsaid. The president\u2019s staff studied the documents and arranged a one-on-one\nwith Musk and Obama in April 2014. Since then, Pishevar, Kevin Brogan,\nand others, have formed a company called Hyperloop Technologies Inc.\nwith the hopes of building the first leg of the Hyperloop between Los\nAngeles and Las Vegas. In theory, people would be able to hop between the\ntwo cities in about ten minutes. Nevada senator Harry Reid has been briefed\non the idea as well, and efforts are under way to buy the land rights\nalongside Interstate 15 that would make the high-speed transport possible.\nFor employees like Gwynne Shotwell and J. B. Straubel, working with\nMusk means helping develop these sorts of wonderful technologies in\nrelative obscurity. They\u2019re the steady hands that will forever be expected to\nstay in the shadows. Shotwell has been a consistent presence at SpaceX\nalmost since day one, pushing the company forward and suppressing her\nego to ensure that Musk gets all the attention he desires. If you\u2019re Shotwell\nand truly believe in the cause of sending people to Mars, then the mission\ntakes precedence over personal desires. Straubel, likewise, has been the\nconstant at Tesla\u2014a go-between whom other employees could rely on to\ncarry messages to Musk, and the guy who knows everything about the cars.\nDespite his stature at the company, Straubel was one of several longtime\nemployees who confessed they were nervous to speak with me on the\nrecord. Musk likes to be the guy talking on his companies\u2019 behalf and\ncomes down hard on even his most loyal executives if they say something\ndeemed to be out of line with Musk\u2019s views or with what he wants the\npublic to think. Straubel has dedicated himself to making electric cars and\ndidn\u2019t want some dumb reporter wrecking his life\u2019s work. \u201cI try really hard\nto back away and put my ego aside,\u201d Straubel said. \u201cElon is incredibly\ndifficult to work for, but it\u2019s mostly because he\u2019s so passionate. He can be\nimpatient and say, \u2018God damn it! This is what we have to do!\u2019 and some\npeople will get shell-shocked and catatonic. It seems like people can get\nafraid of him and paralyzed in a weird way. I try to help everyone to\nunderstand what his goals and visions are, and then I have a bunch of my own goals, too, and make sure we\u2019re in synch. Then, I try and go back and\nmake sure the company is aligned. Ultimately, Elon is the boss. He has\ndriven this thing with his blood, sweat, and tears. He has risked more than\nanyone else. I respect the hell out of what he has done. It just could not\nwork without Elon. In my view, he has earned the right to be the front\nperson for this thing.\u201d\nThe rank-and-file employees tend to describe Musk in more mixed\nways. They revere his drive and respect how demanding he can be. They\nalso think he can be hard to the point of mean and come off as capricious.\nThe employees want to be close to Musk, but they also fear that he\u2019ll\nsuddenly change his mind about something and that every interaction with\nhim is an opportunity to be fired. \u201cElon\u2019s worst trait by far, in my opinion,\nis a complete lack of loyalty or human connection,\u201d said one former\nemployee. \u201cMany of us worked tirelessly for him for years and were tossed\nto the curb like a piece of litter without a second thought. Maybe it was\ncalculated to keep the rest of the workforce on their toes and scared; maybe\nhe was just able to detach from human connection to a remarkable degree.\nWhat was clear is that people who worked for him were like ammunition:\nused for a specific purpose until exhausted and discarded.\u201d\nThe communications departments of SpaceX and Tesla have witnessed\nthe latter forms of behavior more than any other group of employees. Musk\nhas burned through public relations staffers with comical efficiency. He\ntends to take on a lot of the communications work himself, writing news\nreleases and contacting the press as he sees fit. Quite often, Musk does not\nlet his communications staff in on his agenda. Ahead of the Hyperloop\nannouncement, for example, his representatives were sending me e-mails to\nfind out the time and date for the press conference. On other occasions,\nreporters have received an alert about a teleconference with Musk just\nminutes before it started. This was not a function of the PR people being\nincompetent in getting word of the event out. The truth was that Musk had\nonly let them know about his plans a couple of minutes in advance, and\nthey were scrambling to catch up to his whims. When Musk does delegate\nwork to the communications staff, they\u2019re expected to jump in without\nmissing a beat and to execute at the highest level. Some of this staff,\noperating under this mix of pressure and surprise, only lasted between a few\nweeks and a few months. A few others have hung on for a couple of years\nbefore burning out or being fired. The granddaddy example of Musk\u2019s seemingly callous interoffice style\noccurred in early 2014 when he fired Mary Beth Brown. To describe her as\na loyal executive assistant would be grossly inadequate. Brown often felt\nlike an extension of Musk\u2014the one being who crossed over into all of his\nworlds. For more than a decade, she gave up her life for Musk, traipsing\nback and forth between Los Angeles and Silicon Valley every week, while\nworking late into the night and on weekends. Brown went to Musk and\nasked that she be compensated on par with SpaceX\u2019s top executives, since\nshe was handling so much of Musk\u2019s scheduling across two companies,\ndoing public relations work and often making business decisions. Musk\nreplied that Brown should take a couple of weeks off, and he would take on\nher duties and gauge how hard they were. When Brown returned, Musk let\nher know that he didn\u2019t need her anymore, and he asked Shotwell\u2019s assistant\nto begin scheduling his meetings. Brown, still loyal and hurt, didn\u2019t want to\ndiscuss any of this with me. Musk said that she had become too comfortable\nspeaking on his behalf and that, frankly, she needed a life. Other people\ngrumbled that Brown and Riley clashed and that this was the root cause of\nBrown\u2019s ouster.* (Brown declined to be interviewed for this book, despite\nseveral requests.)\nWhatever the case, the optics of the situation were terrible. Tony Stark\ndoesn\u2019t fire Pepper Potts. He adores her and takes care of her for life. She\u2019s\nthe only person he can really trust\u2014the one who has been there through\neverything. That Musk was willing to let Brown go and in such an\nunceremonious fashion struck people inside SpaceX and Tesla as\nscandalous and as the ultimate confirmation of his cruel stoicism. The tale\nof Brown\u2019s departure became part of the lore around Musk\u2019s lack of\nempathy. It got bundled up into the stories of Musk dressing employees\ndown in legendary fashion with vicious barb after vicious barb. People also\nlinked this type of behavior to Musk\u2019s other quirky traits. He\u2019s been known\nto obsess over typos in e-mails to the point that he could not see past the\nerrors and read the actual content of the messages. Even in social settings,\nMusk might get up from the dinner table without a word of explanation to\nhead outside and look at the stars, simply because he\u2019s not willing to suffer\nfools or small talk. After adding up this behavior, dozens of people\nexpressed to me their conclusion that Musk sits somewhere on the autism\nspectrum and that he has trouble considering other people\u2019s emotions and\ncaring about their well-being. There\u2019s a tendency, especially in Silicon Valley, to label people who are\na bit different or quirky as autistic or afflicted with Asperger\u2019s syndrome.\nIt\u2019s armchair psychology for conditions that can be inherently funky to\ndiagnose or even codify. To slap this label on Musk feels ill-informed and\ntoo easy.\nMusk acts differently with his closest friends and family than he does\nwith employees, even those who have worked alongside him for a long\ntime. Among his inner circle, Musk is warm, funny, and deeply emotional.*\nHe might not engage in the standard chitchat, asking a friend how his kids\nare doing, but he would do everything in his considerable power to help that\nfriend if his child were sick or in trouble. He will protect those close to him\nat all costs and, when deemed necessary, seek to destroy those who have\nwronged him or his friends.\nMusk\u2019s behavior matches up much more closely with someone who is\ndescribed by neuropsychologists as profoundly gifted. These are people\nwho in childhood exhibit exceptional intellectual depth and max out IQ\ntests. It\u2019s not uncommon for these children to look out into the world and\nfind flaws\u2014glitches in the system\u2014and construct logical paths in their\nminds to fix them. For Musk, the call to ensure that mankind is a\nmultiplanetary species partly stems from a life richly influenced by science\nfiction and technology. Equally it\u2019s a moral imperative that dates back to his\nchildhood. In some form, this has forever been his mandate.\nEach facet of Musk\u2019s life might be an attempt to soothe a type of\nexistential depression that seems to gnaw at his every fiber. He sees man as\nself-limiting and in peril and wants to fix the situation. The people who\nsuggest bad ideas during meetings or make mistakes at work are getting in\nthe way of all of this and slowing Musk down. He does not dislike them as\npeople. It\u2019s more that he feels pained by their mistakes, which have\nconsigned man to peril that much longer. The perceived lack of emotion is a\nsymptom of Musk sometimes feeling like he\u2019s the only one who really\ngrasps the urgency of his mission. He\u2019s less sensitive and less tolerant than\nother people because the stakes are so high. Employees need to help solve\nthe problems to the absolute best of their ability or they need to get out of\nthe way.\nMusk has been pretty up front about these tendencies. He\u2019s implored\npeople to understand that he\u2019s not chasing momentary opportunities in the\nbusiness world. He\u2019s trying to solve problems that have been consuming him for decades. During our conversations, Musk went back to this very\npoint over and over again, making sure to emphasize just how long he\u2019d\nthought about electric cars and space. The same patterns are visible in his\nactions as well. When Musk announced in 2014 that Tesla would open-\nsource all of its patents, analysts tried to decide whether this was a publicity\nstunt or if it hid an ulterior motive or a catch. But the decision was a\nstraightforward one for Musk. He wants people to make and buy electric\ncars. Man\u2019s future, as he sees it, depends on this. If open-sourcing Tesla\u2019s\npatents means other companies can build electric cars more easily, then that\nis good for mankind, and the ideas should be free. The cynic will scoff at\nthis, and understandably so. Musk, however, has been programmed to\nbehave this way and tends to be sincere when explaining his thinking\u2014\nalmost to a fault.\nThe people who get closest to Musk are the ones who learn to relate to\nthis mode of thinking.22 They\u2019re the ones who can identify with his vision\nyet challenge him intellectually to complete it. When he asked me during\none of our dinners if I thought he was insane, it was a test of sorts. We had\ntalked enough that he knew I was interested in what he was doing. He had\nstarted to trust me and open up but wanted to make sure\u2014one final time\u2014\nthat I truly grasped the importance of his quest. Many of his closest friends\nhave passed much grander, more demanding tests. They\u2019ve invested in his\ncompanies. They\u2019ve defended him against critics. They helped him keep the\nwolves at bay during 2008. They\u2019ve proven their loyalty and their\ncommitment to his cause.\nPeople in the technology industry have tended to liken Musk\u2019s drive and\nthe scope of his ambition to that of Bill Gates and Steve Jobs. \u201cElon has that\ndeep appreciation for technology, the no-holds-barred attitude of a\nvisionary, and that determination to go after long-term things that they both\nhad,\u201d said Edward Jung, a child prodigy who worked for Jobs and Gates\nand ended up as Microsoft\u2019s chief software architect. \u201cAnd he has that\nconsumer sensibility of Steve along with the ability to hire good people\noutside of his own comfort areas that\u2019s more like Bill. You almost wish that\nBill and Steve had a genetically engineered love child and, who knows,\nmaybe we should genotype Elon to see if that\u2019s what happened.\u201d Steve\nJurvetson, the venture capitalist who has invested in SpaceX, Tesla, and\nSolarCity, worked for Jobs, and knows Gates well, also described Musk as\nan upgraded mix of the two. \u201cLike Jobs, Elon does not tolerate C or D players,\u201d said Jurvetson. \u201cBut I\u2019d say he\u2019s nicer than Jobs and a bit more\nrefined than Bill Gates.\u201d*\nBut the more you know about Musk, the harder it becomes to place him\namong his peers. Jobs is another CEO who ran two, large industry-changing\ncompanies\u2014Apple and Pixar. But that\u2019s where the practical similarities\nbetween the two men end. Jobs dedicated far more of his energy to Apple\nthan Pixar, unlike Musk, who has poured equal energy into both companies,\nwhile saving whatever was left over for SolarCity. Jobs was also legendary\nfor his attention to detail. No one, however, would suggest that his reach\nextended down as far as Musk\u2019s into overseeing so much of the companies\u2019\nday-to-day operations. Musk\u2019s approach has its limitations. He\u2019s less artful\nwith marketing and media strategy. Musk does not rehearse his\npresentations or polish speeches. He wings most of the announcements\nfrom Tesla and SpaceX. He\u2019ll also fire off some major bit of news on a\nFriday afternoon when it\u2019s likely to get lost as reporters head home for the\nweekend, simply because that\u2019s when he finished writing the press release\nor wanted to move on to something else. Jobs, by contrast, treated every\npresentation and media moment as precious. Musk simply does not have the\nluxury to work that way. \u201cI don\u2019t have days to practice,\u201d he said. \u201cI\u2019ve got\nto give impromptu talks, and the results may vary.\u201d\nAs for whether Musk is leading the technology industry to new heights\nlike Gates and Jobs, the professional pundits remain mixed. One camp\nholds that SolarCity, Tesla, and SpaceX offer little in the way of real hope\nfor an industry that could use some blockbuster innovations. For the other\ncamp, Musk is the real deal and the brightest shining star of what they see\nas a coming revolution in technology.\nThe economist Tyler Cowen\u2014who has earned some measure of fame in\nrecent years for his insightful writings about the state of the technology\nindustry and his ideas on where it may go\u2014falls into that first camp. In The\nGreat Stagnation, Cowen bemoaned the lack of big technological advances\nand argued that the American economy has slowed and wages have been\ndepressed as a result. \u201cIn a figurative sense, the American economy has\nenjoyed lots of low-hanging fruit since at least the seventeenth century,\nwhether it be free land, lots of immigrant labor, or powerful new\ntechnologies,\u201d he wrote. \u201cYet during the last forty years, that low-hanging\nfruit started disappearing, and we started pretending it was still there. We\nhave failed to recognize that we are at a technological plateau and the trees are more bare than we would like to think. That\u2019s it. That is what has gone\nwrong.\u201d\nIn his next book, Average Is Over, Cowen predicted an unromantic\nfuture in which a great divide had occurred between the Haves and the\nHave Nots. In Cowen\u2019s future, huge gains in artificial intelligence will lead\nto the elimination of many of today\u2019s high-employment lines of work. The\npeople who thrive in this environment will be very bright and able to\ncomplement the machines and team effectively with them. As for the\nunemployed masses? Well, many of them will eventually find jobs going to\nwork for the Haves, who will employ teams of nannies, housekeepers, and\ngardeners. If anything Musk is doing might alter the course of mankind\ntoward a rosier future, Cowen can\u2019t find it. Coming up with true\nbreakthrough ideas is much harder today than in the past, according to\nCowen, because we\u2019ve already mined the bulk of the big discoveries.\nDuring a lunch in Virginia, Cowen described Musk not as a genius inventor\nbut as an attention seeker, and not a terribly good one at that. \u201cI don\u2019t think\na lot of people care about getting to Mars,\u201d he said. \u201cAnd it seems like a\nvery expensive way to drive whatever breakthroughs you might get from it.\nThen, you hear about the Hyperloop. I don\u2019t think he has any intention of\ndoing it. You have to wonder if it\u2019s not meant just to be publicity for his\ncompanies. As for Tesla, it might work. But you\u2019re still just pushing the\nproblems back somewhere else. You still have to generate power. It could\nbe that he is challenging convention less than people think.\u201d\nThese sentiments are not far off from those of Vaclav Smil, a professor\nemeritus at the University of Manitoba. Bill Gates has hailed Smil as an\nimportant writer for his tomes on energy, the environment, and\nmanufacturing. One of Smil\u2019s latest works is Made in the USA, an\nexploration of America\u2019s past manufacturing glories and its subsequent,\ndismal loss of industry. Anyone who thinks the United States is making a\nnatural, clever shift away from manufacturing and toward higher-paying\ninformation-worker jobs will want to read this book and have a gander at\nthe long-term consequences of this change. Smil presents numerous\nexamples of the ways in which the manufacturing industry generates major\ninnovations and creates a massive ecosystem of jobs and technical smarts\naround them. \u201cFor example, when some three decades ago the United States\nstopped making virtually all \u2018commodity\u2019 consumer electronic devices and\ndisplays, it also lost its capacity to develop and mass-produce advanced flat screens and batteries, two classes of products that are quintessential for\nportable computers and cell phones and whose large-scale imports keep\nadding to the US trade deficit,\u201d Smil wrote. A bit later in the book, Smil\nemphasized that the aerospace industry, in particular, has been a huge boon\nto the U.S. economy and one of its major exporters. \u201cMaintaining the\nsector\u2019s competitiveness must be a key component of efforts to boost US\nexports, and the exports will have to be a large part of the sector\u2019s sales\nbecause the world\u2019s largest aerospace market of the next two decades will\nbe in Asia, above all in China and India, and American aircraft and\naeroengine makers should benefit from this expansion.\u201d\nSmil is consumed by the United States\u2019 waning ability to compete with\nChina and yet does not perceive Musk or his companies as any sort of\ncounter to this slide. \u201cAs, among other things, a historian of technical\nadvances I simply must see Tesla as nothing but an utterly derivative\noverhyped toy for showoffs,\u201d Smil wrote to me. \u201cThe last thing a country\nwith 50 million people on food stamps and 85 billion dollars deeper into\ndebt every month needs is anything to do with space, especially space with\nmore joyrides for the super rich. And the loop proposal was nothing but\nbamboozling people who do not know anything about kindergarten physics\nwith a very old, long publicized Gedankenexperiment in kinetics. . . . There\nare many inventive Americans, but in that lineup Musk would be trailing far\nbehind.\u201d\nThe comments were blunt and surprising given some of the things Smil\ncelebrated in his recent book. He spent a good deal of time showing the\npositive impact that Henry Ford\u2019s vertical integration had on advancing the\ncar industry and the American economy. He also wrote at length about the\nrise of \u201cmechatronic machines,\u201d or machines that rely on a lot of electronics\nand software. \u201cBy 2010 the electronic controls for a typical sedan required\nmore lines of software code than the instructions needed to operate the\nlatest Boeing jetliner,\u201d Smil wrote. \u201cAmerican manufacturing has turned\nmodern cars into remarkable mechatronic machines. The first decade of the\ntwenty-first century also brought innovations ranging from the deployment\nof new materials (carbon composites in aviation, nanostructures) to wireless\nelectronics.\u201d\nThere\u2019s a tendency among critics to dismiss Musk as a frivolous\ndreamer that stems first and foremost from a misunderstanding of what\nMusk is actually doing. People like Smil seem to catch an article or television show that hits on Musk\u2019s quest to get to Mars and immediately\nlump him with the space tourism crowd. Musk, though, hardly ever talks\nabout tourism and has, since day one, built up SpaceX to compete at the\nindustrial end of the space business. If Smil thinks Boeing selling planes is\ncrucial to the American economy, then he should be enthused about what\nSpaceX has managed to accomplish in the commercial launch market.\nSpaceX builds its products in the United States, has made dramatic\nadvances in aerospace technology, and has made similar advances in\nmaterials and manufacturing techniques. It would not take much to argue\nthat SpaceX is America\u2019s only hope of competing against China in the next\ncouple of decades. As for mechatronic machines, SpaceX and Tesla have set\nthe example of fusing together electronics, software, and metal that their\nrivals are now struggling to match. And all of Musk\u2019s companies, including\nSolarCity, have made dramatic use of vertical integration and turned in-\nhouse control of components into a real advantage.\nTo get a sense of how powerful Musk\u2019s work may end up being for the\nAmerican economy, have a think about the dominant mechatronic machine\nof the past several years: the smartphone. Pre-iPhone, the United States was\nthe laggard in the telecommunications industry. All of the exciting cell\nphones and mobile services were in Europe and Asia, while American\nconsumers bumbled along with dated equipment. When the iPhone arrived\nin 2007, it changed everything. Apple\u2019s device mimicked many of the\nfunctions of a computer and then added new abilities with its apps, sensors,\nand location awareness. Google charged to market with its Android\nsoftware and related handsets, and the United States suddenly emerged as\nthe driving force in the mobile industry. Smartphones were revolutionary\nbecause of the ways they allowed hardware, software, and services to work\nin unison. This was a mix that favored the skills of Silicon Valley. The rise\nof the smartphone led to a massive industrial boom in which Apple became\nthe most valuable company in the country, and billions of its clever devices\nwere spread all over the world.\nTony Fadell, the former Apple executive credited with bringing the iPod\nand iPhone to market, has characterized the smartphone as representative of\na type of super-cycle in which hardware and software have reached a\ncritical point of maturity. Electronics are good and cheap, while software is\nmore reliable and sophisticated. Their interplay is now resulting in science\nfiction\u2013worthy ideas we were promised long ago becoming a reality. Google has its self-driving cars and has acquired dozens of robotics\ncompanies as it looks to merge code and machine. Fadell\u2019s company Nest\nhas its intelligent thermostats and smoke alarms. General Electric has jet\nengines packed full of sensors taught to proactively report possible\nanomalies to its human mechanics. And a host of start-ups have begun\ninfusing medical devices with powerful software to help people monitor and\nanalyze their bodies and diagnose conditions. Tiny satellites are being put\ninto orbit twenty at a time, and instead of being given a fixed task for their\nentire lifetimes, like their predecessors, they\u2019re being reprogrammed on the\nfly for a wide variety of business and scientific tasks. Zee Aero, a start-up in\nMountain View, has a couple of former SpaceX staffers on hand and is\nworking on a secretive new type of transport. A flying car at last? Perhaps.\nFor Fadell, Musk\u2019s work sits at the highest end of this trend. \u201cHe could\nhave just made an electric car,\u201d Fadell said. \u201cBut he did things like use\nmotors to actuate the door handles. He\u2019s bringing the consumer electronics\nand the software together, and the other car companies are trying to figure\nout a way to get there. Whether it\u2019s Tesla or SpaceX taking Ethernet cables\nand running them inside of rocket ships, you are talking about combining\nthe old-world science of manufacturing with low-cost, consumer-grade\ntechnology. You put these things together, and they morph into something\nwe have never seen before. All of a sudden there is a wholesale change,\u201d he\nsaid. \u201cIt\u2019s a step function.\u201d\nTo the extent that Silicon Valley has searched for an inheritor to Steve\nJobs\u2019s role as the dominant, guiding force of the technology industry, Musk\nhas emerged as the most likely candidate. He\u2019s certainly the \u201cit\u201d guy of the\nmoment. Start-up founders, proven executives, and legends hold him up as\nthe person they most admire. The more mainstream Tesla can become, the\nmore Musk\u2019s reputation will rise. A hot-selling Model 3 would certify Musk\nas that rare being able to rethink an industry, read consumers, and execute.\nFrom there, his more fanciful ideas start to seem inevitable. \u201cElon is one of\nthe few people that I feel is more accomplished than I am,\u201d said Craig\nVenter, the man who decoded the human genome and went on to create\nsynthetic lifeforms. At some point he hopes to work with Musk on a type of\nDNA printer that could be sent to Mars. It would, in theory, allow humans\nto create medicines, food, and helpful microbes for early settlers of the\nplanet. \u201cI think biological teleportation is what is going to truly enable the colonization of space,\u201d he said. \u201cElon and I have been talking about how\nthis might play out.\u201d\nOne of Musk\u2019s most ardent admirers is also one of his best friends:\nLarry Page, the cofounder and CEO of Google. Page has ended up on\nMusk\u2019s house-surfing schedule. \u201cHe\u2019s kind of homeless, which I think is\nsort of funny,\u201d Page said. \u201cHe\u2019ll e-mail and say, \u2018I don\u2019t know where to stay\ntonight. Can I come over?\u2019 I haven\u2019t given him a key or anything yet.\u201d\nGoogle has invested more than just about any other technology\ncompany into Musk\u2019s sort of moon-shot projects: self-driving cars, robots,\nand even a cash prize to get a machine onto the moon cheaply. The\ncompany, however, operates under a set of constraints and expectations that\ncome with employing tens of thousands of people and being analyzed\nconstantly by investors. It\u2019s with this in mind that Page sometimes feels a\nbit envious of Musk, who has managed to make radical ideas the basis of\nhis companies. \u201cIf you think about Silicon Valley or corporate leaders in\ngeneral, they\u2019re not usually lacking in money,\u201d Page said. \u201cIf you have all\nthis money, which presumably you\u2019re going to give away and couldn\u2019t even\nspend it all if you wanted to, why then are you devoting your time to a\ncompany that\u2019s not really doing anything good? That\u2019s why I find Elon to\nbe an inspiring example. He said, \u2018Well, what should I really do in this\nworld? Solve cars, global warming, and make humans multiplanetary.\u2019 I\nmean those are pretty compelling goals, and now he has businesses to do\nthat.\u201d\n\u201cThis becomes a competitive advantage for him, too. Why would you\nwant to work for a defense contractor when you can work for a guy who\nwants to go to Mars and he\u2019s going to move heaven and earth to make it\nhappen? You can frame a problem in a way that\u2019s really good for the\nbusiness.\u201d\nAt one point, a quotation from Page made the rounds, saying that he\nwanted to leave all of his money to Musk. Page felt he was misquoted but\nstood by the sentiment. \u201cI\u2019m not leaving my money to him at the moment,\u201d\nPage said. \u201cBut Elon makes a pretty compelling case for having a\nmultiplanetary society just because, you know, otherwise we might all die,\nwhich seems like it would be sad for all sorts of different reasons. I think\nit\u2019s a very doable project, and it\u2019s a relatively modest resource that we need\nto set up a permanent human settlement on Mars. I was just trying to make\nthe point that that\u2019s a really powerful idea.\u201d As Page puts it, \u201cGood ideas are always crazy until they\u2019re not.\u201d It\u2019s a\nprinciple he\u2019s tried to apply at Google. When Page and Sergey Brin began\nwondering aloud about developing ways to search the text inside of books,\nall of the experts they consulted said it would be impossible to digitize\nevery book. The Google cofounders decided to run the numbers and see if it\nwas actually physically possible to scan the books in a reasonable amount\nof time. They concluded it was, and Google has since scanned millions of\nbooks. \u201cI\u2019ve learned that your intuition about things you don\u2019t know that\nmuch about isn\u2019t very good,\u201d Page said. \u201cThe way Elon talks about this is\nthat you always need to start with the first principles of a problem. What are\nthe physics of it? How much time will it take? How much will it cost? How\nmuch cheaper can I make it? There\u2019s this level of engineering and physics\nthat you need to make judgments about what\u2019s possible and interesting.\nElon is unusual in that he knows that, and he also knows business and\norganization and leadership and governmental issues.\u201d\nSome of the conversations between Musk and Page take place at a\nsecret apartment Google owns in downtown Palo Alto. It\u2019s inside of one of\nthe taller buildings in the area and offers views of the mountains\nsurrounding the Stanford University campus. Page and Brin will take\nprivate meetings at the apartment and have their own chef on call to prepare\nfood for guests. When Musk is present, the chats tend toward the absurd\nand fantastic. \u201cI was there once, and Elon was talking about building an\nelectric jet plane that can take off and land vertically,\u201d said George Zachary,\nthe venture capitalist and friend of Musk\u2019s. \u201cLarry said the plane should be\nable to land on ski slopes, and Sergey said it needed to be able to dock at a\nport in Manhattan. Then they started talking about building a commuter\nplane that was always circling the Earth, and you\u2019d hop up to it and get\nplaces incredibly fast. I thought everyone was kidding, but at the end I\nasked Elon, \u2018Are you really going to do that?\u2019 And he said, \u2018Yes.\u2019\u201d\n\u201cIt\u2019s kind of our recreation, I guess,\u201d said Page.23 \u201cIt\u2019s fun for the three\nof us to talk about kind of crazy things, and we find stuff that eventually\nturns out to be real. We go through hundreds or thousands of possible things\nbefore arriving at the ones that are most promising.\u201d\nPage talked about Musk at times as if he were a one-of-a-kind, a force\nof nature able to accomplish things in the business world that others would\nnever even try. \u201cWe think of SpaceX and Tesla as being these tremendously\nrisky things, but I think Elon was going to make them work no matter what. He\u2019s willing to suffer some personal cost, and I think that makes his odds\nactually pretty good. If you knew him personally, you would look back to\nwhen he started the companies and say his odds of success would be more\nthan ninety percent. I mean we just have a single proof point now that you\ncan be really passionate about something that other people think is crazy\nand you can really succeed. And you look at it with Elon and you say,\n\u2018Well, maybe it\u2019s not luck. He\u2019s done it twice. It can\u2019t be luck totally.\u2019 I\nthink that means it should be repeatable in some sense. At least it\u2019s\nrepeatable by him. Maybe we should get him to do more things.\u201d\nPage holds Musk up as a model he wishes others would emulate\u2014a\nfigure that should be replicated during a time in which the businessmen and\npoliticians have fixated on short-term, inconsequential goals. \u201cI don\u2019t think\nwe\u2019re doing a good job as a society deciding what things are really\nimportant to do,\u201d Page said. \u201cI think like we\u2019re just not educating people in\nthis kind of general way. You should have a pretty broad engineering and\nscientific background. You should have some leadership training and a bit\nof MBA training or knowledge of how to run things, organize stuff, and\nraise money. I don\u2019t think most people are doing that, and it\u2019s a big\nproblem. Engineers are usually trained in a very fixed area. When you\u2019re\nable to think about all of these disciplines together, you kind of think\ndifferently and can dream of much crazier things and how they might work.\nI think that\u2019s really an important thing for the world. That\u2019s how we make\nprogress.\u201d\nThe pressure of feeling the need to fix the world takes its toll on Musk\u2019s\nbody. There are times when you run into Musk and he looks utterly\nexhausted. He does not have bags under his eyes but rather deep, shadowy\nvalleys. During the worst of times, following weeks of sleep deprivation,\nhis eyes seem to have sunk back into his skull. Musk\u2019s weight moves up\nand down with the stress, and he\u2019s usually heavier when really overworked.\nIt\u2019s funny in a way that Musk spends so much time talking about man\u2019s\nsurvival but isn\u2019t willing to address the consequences of what his lifestyle\ndoes to his body. \u201cElon came to the conclusion early in his career that life is\nshort,\u201d Straubel said. \u201cIf you really embrace this, it leaves you with the\nobvious conclusion that you should be working as hard as you can.\u201d\nSuffering, though, has always been Musk\u2019s thing. The kids at school\ntortured him. His father played brutal mind games. Musk then abused\nhimself by working inhumane hours and forever pushing his businesses to the edge. The idea of work-life balance seems meaningless in this context.\nFor Musk, it\u2019s just life, and his wife and kids try to fit into the show where\nthey can. \u201cI\u2019m a pretty good dad,\u201d Musk said. \u201cI have the kids for slightly\nmore than half the week and spend a fair bit of time with them. I also take\nthem with me when I go out of town. Recently, we went to the Monaco\nGrand Prix and were hanging out with the prince and princess of Monaco. It\nall seemed quite normal to the kids, and they were blas\u00e9 about it. They are\ngrowing up having a set of experiences that are extremely unusual, but you\ndon\u2019t realize experiences are unusual until you are much older. They\u2019re just\nyour experiences. They have good manners at meals.\u201d\nIt bothers Musk a bit that his kids won\u2019t suffer like he did. He feels that\nthe suffering helped to make him who he is and gave him extra reserves of\nstrength and will. \u201cThey might have a little adversity at school, but these\ndays schools are so protective,\u201d he said. \u201cIf you call someone a name, you\nget sent home. When I was going to school, if they punched you and there\nwas no blood, it was like, \u2018Whatever. Shake it off.\u2019 Even if there was a little\nblood, but not a lot, it was fine. What do I do? Create artificial adversity?\nHow do you do that? The biggest battle I have is restricting their video\ngame time because they want to play all the time. The rule is they have to\nread more than they play video games. They also can\u2019t play completely\nstupid video games. There\u2019s one game they downloaded recently called\nCookies or something. You literally tap a fucking cookie. It\u2019s like a Psych\n101 experiment. I made them delete the cookie game. They had to play\nFlappy Golf instead, which is like Flappy Bird, but at least there is some\nphysics involved.\u201d\nMusk has talked about having more kids, and it\u2019s on this subject that he\ndelivers some controversial philosophizing vis-\u00e0-vis the creator of Beavis\nand Butt-head. \u201cThere\u2019s this point that Mike Judge makes in Idiocracy,\nwhich is like smart people, you know, should at least sustain their\nnumbers,\u201d Musk said. \u201cLike, if it\u2019s a negative Darwinian vector, then\nobviously that\u2019s not a good thing. It should be at least neutral. But if each\nsuccessive generation of smart people has fewer kids, that\u2019s probably bad,\ntoo. I mean, Europe, Japan, Russia, China are all headed for demographic\nimplosion. And the fact of the matter is that basically the wealthier\u2014\nbasically wealth, education, and being secular are all indicative of low birth\nrate. They all correlate with low birth rate. I\u2019m not saying like only smart\npeople should have kids. I\u2019m just saying that smart people should have kids as well. They should at least maintain\u2014at least be a replacement rate. And\nthe fact of the matter is that I notice that a lot of really smart women have\nzero or one kid. You\u2019re like, \u2018Wow, that\u2019s probably not good.\u2019\u201d\nThe next decade of Musk Co. should be quite something. Musk has\ngiven himself a chance to become one of the greatest businessmen and\ninnovators of all time. By 2025 Tesla could very well have a lineup of five\nor six cars and be the dominant force in a booming electric car market.\nPlaying off its current growth rate, SolarCity will have had time to emerge\nas a massive utility company and the leader in a solar market that had\nfinally lived up to its promise. SpaceX? Well, it\u2019s perhaps the most\nintriguing. According to Musk\u2019s calculations, SpaceX should be conducting\nweekly flights to space, carrying humans and cargo, and have put most of\nits competitors out of business. Its rockets should be capable of doing a\ncouple of laps around the moon and then landing with pinpoint accuracy\nback at the spaceport in Texas. And the preparation for the first few dozen\ntrips to Mars should be well under way.\nIf all of this were taking place, Musk, then in his mid-fifties, likely\nwould be the richest man in the world and among its most powerful. He\nwould be the majority shareholder in three public companies, and history\nwould be preparing to smile broadly on what he had accomplished. During\na time in which countries and other businesses were paralyzed by indecision\nand inaction, Musk would have mounted the most viable charge against\nglobal warming, while also providing people with an escape plan\u2014just in\ncase. He would have brought a substantial amount of crucial manufacturing\nback to the United States while also providing an example for other\nentrepreneurs hoping to harness a new age of wonderful machines. As Thiel\nsaid, Musk may well have gone so far as to give people hope and to have\nrenewed their faith in what technology can do for mankind.\nThis future, of course, remains precarious. Huge technological issues\nconfront all three of Musk\u2019s companies. He\u2019s bet on the inventiveness of\nman and the ability of solar, battery, and aerospace technology to follow\npredicted price and performance curves. Even if these bets hit as he hopes,\nTesla could face a weird, unexpected recall. SpaceX could have a rocket\ncarrying humans blow up\u2014an incident that could very well end the\ncompany on the spot. Dramatic risks accompany just about everything\nMusk does. By the time our last dinner had come around, I had decided that this\npropensity for risk had little to do with Musk being insane, as he had\nwondered aloud several months earlier. No, Musk just seems to possess a\nlevel of conviction that is so intense and exceptional as to be off-putting to\nsome. As we shared some chips and guacamole and cocktails, I asked Musk\ndirectly just how much he was willing to put on the line. His response?\nEverything that other people hold dear. \u201cI would like to die on Mars,\u201d he\nsaid. \u201cJust not on impact. Ideally I\u2019d like to go for a visit, come back for a\nwhile, and then go there when I\u2019m like seventy or something and then just\nstay there. If things turn out well, that would be the case. If my wife and I\nhave a bunch of kids, she would probably stay with them on Earth.\u201d EPILOGUE\nE\nLON MUSK IS A BODY THAT REMAINS VERY MUCH IN\nMOTION.\nBy the time this book reaches your hands, it\u2019s quite possible that Musk\nand SpaceX will have managed to land a rocket on a barge at sea or back on\na launchpad in Florida. Tesla Motors may have unveiled some of the special\nfeatures of the Model X. Musk could have formally declared war on the\nartificial intelligence machines coming to life inside of Google\u2019s data\ncenters. Who knows?\nWhat\u2019s clear is that Musk\u2019s desire to take on more keeps growing. Just\nas I was putting the finishing touches on this book, Musk unfurled a number\nof major initiatives. The most dramatic of which is a plan to surround the\nEarth with thousands of small communications satellites. Musk wants, in\neffect, to build a space-based Internet in which the satellites would be close\nenough to the planet to beam down bandwidth at high speeds. Such a\nsystem would be useful for a couple of reasons: In areas too poor or too\nremote to have fiber-optic connections, it would provide people with high-\nspeed Internet for the first time. It could also function as an efficient\nbackhaul network for businesses and consumers.\nMusk, of course, also sees this space Internet as key to his long-term\nambitions around Mars. \u201cIt will be important for Mars to have a global\ncommunications network,\u201d he said. \u201cI think this needs to be done, and I\ndon\u2019t see anyone else doing it.\u201d SpaceX will build these satellites at a new\nfactory and will also look to sell more satellites to commercial customers as\nit perfects the technology. To fund part of this unbelievably ambitious\nproject, SpaceX secured $1 billion from Google and Fidelity. In a rare\nmoment of restraint, Musk declined to provide an exact delivery date for his\nspace Internet, which he forecasts will cost more than $10 billion to build. \u201cPeople should not expect this to be active sooner than five years,\u201d he said.\n\u201cBut we see it as a long-term revenue source for SpaceX to be able to fund\na city on Mars.\u201d\nMeanwhile, SolarCity has purchased a new research and development\nfacility near the Tesla factory in Silicon Valley that\u2019s intended to aid its\nmanufacturing work. The building it acquired was the old Solyndra\nmanufacturing plant\u2014another symbol of Musk\u2019s ability to thrive in the\ngreen technology industry that has destroyed so many other entrepreneurs.\nAnd Tesla continues to build its Gigafactory in Nevada at pace, while its\nnetwork of charging stations has saved upward of four million gallons of\ngas. During a quarterly earnings announcement, J. B. Straubel promised that\nTesla would start producing battery systems for home use in 2015 that\nwould let people hop off the grid for periods of time. Musk then one-upped\nStraubel, bragging that he thinks Tesla could eventually be more valuable\nthan Apple and could challenge it in the race to be the first $1 trillion\ncompany. A handful of groups have also set to work building prototype\nHyperloop systems in and around California. Oh, and Musk starred in an\nepisode of The Simpsons titled \u201cThe Musk Who Fell to Earth,\u201d in which\nHomer became his inventive muse.\nThe heady expansion plans and triumphant rhetoric from Musk were\nstill not quite enough to hide all of Musk Co.\u2019s flaws. Early 2015 marked\nthe vociferous return of Musk\u2019s detractors on Wall Street. Tesla\u2019s sales in\nChina were lackluster by any measure, and some analysts renewed their\ndoubts about how much long-term demand there would be for the Model S.\nTesla\u2019s shares slumped and, for the first time in a while, Musk sounded\nflustered trying to defend the company\u2019s position.\nThe personal costs of Musk\u2019s lifestyle were more severe. Musk\nannounced that, once again, he would be divorcing Talulah Riley.\nAccording to Musk, Riley wanted a simpler, smaller life in England and had\ncome to despise Los Angeles. \u201cTried to talk her out of it, but she insisted,\u201d\nMusk told me. \u201cIt is possible that she will change her mind at some point,\nbut not anytime soon.\u201d\nAfter finishing my reporting and writing for this book, I had a chance to\nspeak with some of Musk\u2019s confidantes and employees in a more relaxed\nmanner and bounce various ideas off of them. I\u2019m more convinced than\never that Musk is, and has always been, a man on a quest, and that his brand\nof quest is far more fantastic and consuming than anything most of us will ever experience. It seems that he\u2019s become almost addicted to expanding his\nambitions and can\u2019t quite stop himself from announcing things like the\nHyperloop and the space Internet. I\u2019m also more convinced than ever that\nMusk is a deeply emotional person who suffers and rejoices in an epic\nfashion. This side of him is likely obscured by the fact that he feels most\ndeeply about his own humanity-altering quest and so has trouble\nrecognizing the strong emotions of those around him. This tends to make\nMusk come off as aloof and hard. I would argue, however, that his brand of\nempathy is unique. He seems to feel for the human species as a whole\nwithout always wanting to consider the wants and needs of individuals. And\nit may well be the case that this is exactly the type of person it takes to\nmake a freaking space Internet real. APPENDIX 1\nT\nHE TECHNOLOGY INDUSTRY LOVES MESSY FOUNDING\nTALES. A bit of backstabbing? A hearty helping of deceit? Perfect. And\nyet, the press has never really dug into the alleged intrigue surrounding\nMusk\u2019s formation of Zip2, nor have reporters examined the very serious\nallegations of inconsistencies in Musk\u2019s academic record.\nIn April 2007, a physicist named John O\u2019Reilly filed a lawsuit alleging\nthat Musk had stolen the idea for Zip2. According to the lawsuit, filed with\nthe Superior Court of California in Santa Clara, O\u2019Reilly first met Musk in\nOctober 1995. O\u2019Reilly had started a company called Internet Merchant\nChannel, or IMC, which planned to let businesses create primitive,\ninformation-packed online ads. A restaurant, for example, could build an ad\nthat would display its menu and perhaps even turn-by-turn directions to its\nlocation. O\u2019Reilly\u2019s ideas were mostly theoretical, but Zip2 did end up\nproviding a very similar service. O\u2019Reilly alleged that Musk had first heard\nabout this type of technology while trying to get a job working as a\nsalesman for IMC. He and Musk met on at least three occasions, according\nto the lawsuit, to talk about the job. O\u2019Reilly then went on an overseas trip\nand struggled to get back in touch with Musk upon his return.\nO\u2019Reilly declined to discuss his case against Musk with me. But in the\nlawsuit, he claimed to have learned about Zip2 through happenstance many\nyears after meeting Musk. While reading a book in 2005 about the Internet\neconomy, O\u2019Reilly stumbled upon a passage that mentioned Musk\u2019s\nfounding of Zip2 and its 1999 sale to Compaq Computer for $307 million in\ncash. The physicist was blown away as he realized that Zip2 sounded a lot\nlike IMC, which had never amounted to much of a business. O\u2019Reilly\u2019s\nmind raced back to his encounters with Musk. He began to suspect that\nMusk had avoided him on purpose and that instead of becoming an IMC salesman, Musk had run off to pursue the same concept on his own.\nO\u2019Reilly wanted to be compensated for coming up with the original\nbusiness idea. He spent about two years making his case against Musk. The\ncase file at the court runs hundreds of pages. O\u2019Reilly has affidavits from\npeople that back up parts of his version of events. A judge, however, found\nthat O\u2019Reilly lacked the necessary legal standing to bring this case against\nMusk due to issues around how his businesses had been dissolved. The\njudge ordered O\u2019Reilly to shell out $125,000 for Musk\u2019s legal fees in 2010.\nAll these years later, Musk still hasn\u2019t made O\u2019Reilly pay.\nWhile playing detective, O\u2019Reilly unearthed some information about\nMusk\u2019s past that\u2019s arguably more interesting than the allegations in the\nlawsuit. He found that the University of Pennsylvania granted Musk\u2019s\ndegrees in 1997\u2014two years later than what Musk has cited. I called Penn\u2019s\nregistrar and verified these findings. Copies of Musk\u2019s records show that he\nreceived a dual degree in economics and physics in May 1997. O\u2019Reilly\nalso subpoenaed the registrar\u2019s office at Stanford to verify Musk\u2019s\nadmittance in 1995 for his doctorate work in physics. \u201cBased on the\ninformation you provided, we are unable to locate a record in our office for\nElon Musk,\u201d wrote the director of graduate admissions. When asked during\nthe case to produce a document verifying Musk\u2019s enrollment at Stanford,\nMusk\u2019s attorney declined and called the request \u201cunduly burdensome.\u201d I\ncontacted a number of Stanford physics professors who taught in 1995, and\nthey either failed to respond or didn\u2019t remember Musk. Doug Osheroff, a\nNobel Prize winner and department chair at the time, said, \u201cI don\u2019t think I\nknew Elon, and am pretty sure that he was not in the Physics Department.\u201d\nIn the years that have followed, Musk\u2019s enemies have been quick to\nbring up the ambiguities around his admission to Stanford. When Martin\nEberhard sued Musk, his attorney introduced O\u2019Reilly\u2019s research into the\ncase. And during the course of my interviews, a number of Musk\u2019s\ndetractors from the Zip2, PayPal, and early Tesla days said flat out that they\nthink Musk fibbed about getting into Stanford in a bid to boost his\ncredentials as a fledgling entrepreneur and then had to stick with the story\nafter Zip2 took off.\nAt first, I, too, felt like there were a lot of oddities surrounding Musk\u2019s\nacademic record, particularly the Stanford days. But, as I dug in, there were\nsolid explanations for all of the inconsistencies and plenty of evidence to\nundermine the cases of Musk\u2019s detractors. During the course of my reporting, for example, I found evidence that\ncontradicted O\u2019Reilly\u2019s timeline of events. Peter Nicholson, the banker\nwhom Musk had worked for in Canada, took a stroll with Musk along the\nboardwalk in Toronto before Musk left for Stanford and chatted about the\nincarnations of something like Zip2. Musk had already started writing some\nof the early software to support the idea he\u2019d outlined to Kimbal. \u201cHe was\nagonizing whether to do a PhD at Stanford or take this piece of software\nhe\u2019d made in his spare time and make a business out of it,\u201d Nicholson said.\n\u201cHe called the thing the Virtual City Navigator. I told him there was this\ncrazy Internet thing going on, and that people will pay big money for damn\nnear anything. This software was a golden opportunity. He could do a PhD\nanytime.\u201d Kimbal and other members of Musk\u2019s family have similar\nmemories.\nMusk, speaking at length for the first time on the subject, denied\neverything alleged by O\u2019Reilly and does not even recall meeting the man.\n\u201cHe\u2019s a total scumbag,\u201d Musk said. \u201cO\u2019Reilly is like a failed physicist who\nbecame a serial litigate. And I told the guy, \u2018Look, I\u2019m not going to settle an\nunjust case. So it\u2019s just like don\u2019t even try.\u2019 But he still kept at it. His case\nwas tossed out twice on demur, which means that basically even if all the\nfacts in his case were true, he would still lose.\n\u201cHe\u2019d tried his best to like torture me through my friends and personally\n[by filing the lawsuit]. And then we\u2019ve got summary judgment. He lost the\nsummary judgment. He appealed summary judgment, then several months\nlater lost the appeal and I was like, \u2018Okay, fuck it. Let\u2019s file for fees.\u2019 And\nwe were awarded fees from when he appealed. And that\u2019s when we sent the\nsheriff after him and he claimed that he had no money basically. Whether he\ndid or didn\u2019t I don\u2019t know. He certainly claimed he had no money. So we\nwere like either we\u2019ve got to like impound his car or tap his wife\u2019s income.\nThose didn\u2019t seem like great choices. So, we decided that he doesn\u2019t have to\npay back the money he owes me, so long as he doesn\u2019t sue anyone else on\nfrivolous grounds. And, in fact, late last year or early this year [2014], he\ntried to do just that thing. But, whoever he sued was aware of the nature of\nmy judgment and contacted the lawyer I used, who then told O\u2019Reilly,\n\u2018Look, you need to drop the case against these guys or everyone\u2019s going to\nask for the money. It\u2019s kind of pointless to sue them on frivolous grounds\nbecause you\u2019re going to have fork over the winnings to Elon.\u2019 It\u2019s like go\ndo something productive with your life.\u201d As for his academic records, Musk produced a document for me dated\nJune 22, 2009, that came from Judith Haccou, the director of graduate\nadmissions in the office of the registrar at Stanford University. It read, \u201cAs\nper special request from my colleagues in the School of Engineering, I have\nsearched Stanford\u2019s admission data base and acknowledge that you applied\nand were admitted to the graduate program in Material Science Engineering\nin 1995. Since you did not enroll, Stanford is not able to issue you an\nofficial certification document.\u201d\nMusk also had an explanation for the weird timing on his degrees from\nPenn. \u201cI had a History and an English credit that I agreed with Penn that I\nwould do at Stanford,\u201d he said. \u201cThen I put Stanford on deferment. Later,\nPenn\u2019s requirements changed so that you don\u2019t need the English and History\ncredit. So then they awarded me the degree in \u201997 when it was clear I was\nnot going to go to grad school, and their requirement was no longer there.\n\u201cI finished everything that was needed for a Wharton degree in \u201994.\nThey\u2019d actually mailed me a Wharton degree. I decided to spend another\nyear and finished the physics degree, but then there was that History and\nEnglish credit thing. I was only reminded about the History and English\nthing when I tried to get an H-1B visa and called the school to get a copy of\nmy graduation certificate, and they said I hadn\u2019t graduated. Then they\nlooked into the new requirements, and said it was fine.\u201d APPENDIX 2\nW\nHILE MUSK HAS REFLECTED PUBLICLY ABOUT HIS TIME AT\nPAYPAL AND THE COUP, he went into far greater detail than ever before\nduring one of our longer interviews. Years had passed since the tumultuous\ndays surrounding his ouster, and Musk had been able to meditate more on\nwhat went right, what went wrong, and what might have been. He started by\ndiscussing his decision to go out of the country, mixing business with a\ndelayed honeymoon, and ended with an explanation of how the finance\nindustry still hasn\u2019t solved the problems X.com wanted to tackle.\n\u201cThe problem with me going away was that I was not there to reassure\nthe board on a few things. Like, the brand change, I think it would have\nbeen the right move, but it didn\u2019t need to happen right then. At the time it\nwas this weird almost hybrid brand with X.com and PayPal. I think X was\nthe right long-term brand for something that wants to be the central place\nwhere all transactions happen. That\u2019s the X. It\u2019s like the X is the transaction.\nPayPal doesn\u2019t make sense in that context, when we\u2019re talking about\nsomething more than a personal payment system. I think X was the more\nsensible approach but timing-wise it didn\u2019t need to happen then. That\nshould have probably waited longer.\n\u201cAs for the technology change, that\u2019s not really well understood. On the\nface of it, it doesn\u2019t sound like it makes much sense for us to be writing our\nfront-end code in Microsoft C++ instead of Linux. But the reason is that the\nprogramming tools for Microsoft and a PC are actually extremely powerful.\nThey\u2019re developed for the gaming industry. I mean, this is going to sound\nlike heresy in a sort of Silicon Valley context, but you can program faster,\nyou can get functionality faster in the PC C++ world. All of the games for\nthe Xbox are written in Microsoft C++. The same goes for games on the\nPC. They\u2019re incredibly sophisticated, hard things to do, and these great tools have been developed thanks to the gaming industry. There were more\nsmart programmers in the gaming industry than anywhere else. I\u2019m not sure\nthe general public understands this. It was also 2000, and there were not the\nhuge software libraries for Linux that you would find today. Microsoft had\nhuge support libraries. So you could get a DLL that could do anything, but\nyou couldn\u2019t get\u2014you couldn\u2019t get Linux libraries that could do anything.\n\u201cTwo of the guys that left PayPal went off to Blizzard and helped\ncreated World of Warcraft. When you look at the complexity of something\nlike that living on PCs and Microsoft C++, it\u2019s pretty incredible. It blows\naway any website.\n\u201cIn retrospect, I should have delayed the brand transition, and I should\nhave spent a lot more time with Max getting him comfortable on the\ntechnology. I mean, it was a little difficult because like the Linux system\nMax had created was called Max Code. So Max has had quite a strong\naffinity for Max Code. This was a bunch of libraries that Max and his\nfriends had done. But it just made it quite hard to develop new features.\nAnd if you look at PayPal today, I mean, part of the reason they haven\u2019t\ndeveloped any new features is because it\u2019s quite difficult to maintain the old\nsystem.\n\u201cUltimately, I didn\u2019t disagree with the board\u2019s decision in the PayPal\ncase, in the sense that with the information that the board had I would have\nmade maybe the same decision. I probably would have, whereas in the case\nof Zip2 I would not have. I thought they just simply made a terrible\ndecision based on information they had. I don\u2019t think the X.com board\nmade a terrible decision based on the information they had. But it did make\nme want to be careful about who invested in my companies in the future.\n\u201cI\u2019ve thought about trying to get PayPal back. I\u2019ve just been too strung\nout with other things. Almost no one understands how PayPal actually\nworked or why it took off when other payment systems before and after it\ndidn\u2019t. Most of the people at PayPal don\u2019t understand this. The reason it\nworked was because the cost of transactions in PayPal was lower than any\nother system. And the reason the cost of transactions was lower is because\nwe were able to do an increasing percentage of our transactions as ACH, or\nautomated clearinghouse, electronic transactions, and most importantly,\ninternal transactions. Internal transactions were essentially fraud-free and\ncost us nothing. An ACH transaction costs, I don\u2019t know, like twenty cents\nor something. But it was slow, so that was the bad thing. It\u2019s dependent on the bank\u2019s batch processing time. And then the credit card transaction was\nfast, but expensive in terms of the credit card processing fees and very\nprone to fraud. That\u2019s the problem Square is having now.\n\u201cSquare is doing the wrong version of PayPal. The critical thing is to\nachieve internal transactions. This is vital because they are instant, fraud-\nfree, and fee-free. If you\u2019re a seller and have various options, and PayPal\nhas the lowest fees and is the most secure, it\u2019s obviously the right thing to\nuse.\n\u201cWhen you look at like any given business, like say a business is\nmaking 10 percent profitability. They\u2019re making 10 percent profit when\nthey may net out all of their costs. You know, revenue minus expenses in a\nyear, they\u2019re 10 percent. If using PayPal means you pay 2 percent for your\ntransactions and using some other systems means you pay 4 percent, that\nmeans using PayPal gives you a 20 percent increase in your profitability.\nYou\u2019d have to be brain dead not to do that. Right?\n\u201cSo because about half of PayPal\u2019s transactions in the summer of 2001\nwere internal or ACH transactions, then our fundamental costs of\ntransactions were half because we\u2019d have half credit cards, we\u2019d have that\nand then the other half would be free. The question then is how do you give\npeople a reason to keep money in the system.\n\u201cThat\u2019s why we created a PayPal debit card. It\u2019s a little counterintuitive,\nbut the easier you make it for people to get money out of PayPal, the less\nthey\u2019ll want to do it. But if the only way for them to spend money or access\nit in any way is to move it to a traditional bank, that\u2019s what they\u2019ll do\ninstantly. The other thing was the PayPal money market fund. We did that\nbecause if you consider the reasons that people might move the money out,\nwell, they\u2019ll move it to either conduct transactions in the physical world or\nbecause they\u2019re getting a higher interest rate. So I instituted the highest-\nreturn money market fund in the country. Basically, the money market fund\nwas at cost. We didn\u2019t intend to make any money on it, in order to\nencourage people to keep their money in the system. And then we also had\nlike the ability to pay regular bills like your electricity bill and that kind of\nthing on PayPal.\n\u201cThere were a bunch of things that should have been done like checks.\nBecause even though people don\u2019t use a lot of checks they still use some\nchecks. So if you force people to say, \u2018Okay, we\u2019re not going to let you use checks ever,\u2019 they\u2019re like, \u2018Okay, I guess I have to have a bank account.\u2019\nJust give them a few checks, for God\u2019s sake.\n\u201cI mean, it\u2019s so ridiculous that PayPal today is worse than PayPal circa\nend of 2001. That\u2019s insane.\n\u201cNone of these start-ups understand the objective. The objective should\nbe\u2014what delivers fundamental value. I think it\u2019s important to look at things\nfrom a standpoint of what is actually the best thing for the economy. If\npeople can conduct their transactions quickly and securely that\u2019s better for\nthem. If it\u2019s simpler to conduct their financial life it\u2019s better for them. So, if\nall your financial affairs are seamlessly integrated one place it\u2019s very easy to\ndo transactions and the fees associated with transactions are low. These are\nall good things. Why aren\u2019t they doing this? It\u2019s mad.\u201d APPENDIX 3\nFrom: Elon Musk\nDate: June 7, 2013, 12:43:06 AM PDT\nTo: All <All@spacex.com>\nSubject: Going Public\nPer my recent comments, I am increasingly concerned about\nSpaceX going public before the Mars transport system is in place.\nCreating the technology needed to establish life on Mars is and\nalways has been the fundamental goal of SpaceX. If being a public\ncompany diminishes that likelihood, then we should not do so until\nMars is secure. This is something that I am open to reconsidering,\nbut, given my experiences with Tesla and SolarCity, I am hesitant to\nfoist being public on SpaceX, especially given the long term nature\nof our mission.\nSome at SpaceX who have not been through a public company\nexperience may think that being public is desirable. This is not so.\nPublic company stocks, particularly if big step changes in\ntechnology are involved, go through extreme volatility, both for\nreasons of internal execution and for reasons that have nothing to do\nwith anything except the economy. This causes people to be\ndistracted by the manic-depressive nature of the stock instead of\ncreating great products.\nIt is important to emphasize that Tesla and SolarCity are public\nbecause they didn\u2019t have any choice. Their private capital structure\nwas becoming unwieldy and they needed to raise a lot of equity\ncapital. SolarCity also needed to raise a huge amount of debt at the\nlowest possible interest rate to fund solar leases. The banks who\nprovide that debt wanted SolarCity to have the additional and painful scrutiny that comes with being public. Those rules, referred\nto as Sarbanes-Oxley, essentially result in a tax being levied on\ncompany execution by requiring detailed reporting right down to\nhow your meal is expensed during travel and you can be penalized\neven for minor mistakes.\nYES, BUT I COULD MAKE MORE MONEY IF\nWE WERE PUBLIC\nFor those who are under the impression that they are so clever that\nthey can outsmart public market investors and would sell SpaceX\nstock at the \u201cright time,\u201d let me relieve you of any such notion. If\nyou really are better than most hedge fund managers, then there is\nno need to worry about the value of your SpaceX stock, as you can\njust invest in other public company stocks and make billions of\ndollars in the market.\nIf you think: \u201cAh, but I know what\u2019s really going on at SpaceX\nand that will give me an edge,\u201d you are also wrong. Selling public\ncompany stock with insider knowledge is illegal. As a result, selling\npublic stock is restricted to narrow time windows a few times per\nyear. Even then, you can be prosecuted for insider trading. At Tesla,\nwe had both an employee and an investor go through a grand jury\ninvestigation for selling stock over a year ago, despite them doing\neverything right in both the letter and spirit of the law. Not fun.\nAnother thing that happens to public companies is that you\nbecome a target of the trial lawyers who create a class action lawsuit\nby getting someone to buy a few hundred shares and then pretending\nto sue the company on behalf of all investors for any drop in the\nstock price. Tesla is going through that right now even though the\nstock price is relatively high, because the drop in question occurred\nlast year.\nIt is also not correct to think that because Tesla and SolarCity\nshare prices are on the lofty side right now, that SpaceX would be\ntoo. Public companies are judged on quarterly performance. Just\nbecause some companies are doing well, doesn\u2019t mean that all would. Both of those companies (Tesla in particular) had great first\nquarter results. SpaceX did not. In fact, financially speaking, we had\nan awful first quarter. If we were public, the short sellers would be\nhitting us over the head with a large stick.\nWe would also get beaten up every time there was an anomaly\non the rocket or spacecraft, as occurred on flight 4 with the engine\nfailure and flight 5 with the Dragon prevalves. Delaying launch of\nV1.1, which is now over a year behind schedule, would result in\nparticularly severe punishment, as that is our primary revenue\ndriver. Even something as minor as pushing a launch back a few\nweeks from one quarter to the next gets you a spanking. Tesla\nvehicle production in Q4 last year was literally only three weeks\nbehind and yet the market response was brutal.\nBEST OF BOTH WORLDS\nMy goal at SpaceX is to give you the best aspects of a public and\nprivate company. When we do a financing round, the stock price is\nkeyed off of approximately what we would be worth if publicly\ntraded, excluding irrational exuberance or depression, but without\nthe pressure and distraction of being under a hot public spotlight.\nRather than have the stock be up during one liquidity window and\ndown during another, the goal is a steady upward trend and never to\nlet the share price go below the last round. The end result for you (or\nan investor in SpaceX) financially will be the same as if we were\npublic and you sold a steady amount of stock every year.\nIn case you are wondering about a specific number, I can say\nthat I\u2019m confident that our long term stock price will be over $100 if\nwe execute well on Falcon 9 and Dragon. For this to be the case, we\nmust have a steady and rapid cadence of launch that is far better\nthan what we have achieved in the past. We have more work ahead\nof us than you probably realize. Let me give you a sense of where\nthings stand financially: SpaceX expenses this year will be roug[h]ly\n$800 to $900 million (which blows my mind btw). Since we get\nrevenue of $60M for every F9 flight or double that for a FH or F9-\nDragon flight, we must have about twelve flights per year where four of those flights are either Dragon or Heavy merely in order to\nachieve 10% profitability!\nFor the next few years, we have NASA commercial crew\nfunding that helps supplement those numbers, but, after that, we are\non our own. That is not much time to finish F9, FH, Dragon V2 and\nachieve an average launch rate of at least one per month. And bear\nin mind that is an average, so if we take an extra three weeks to\nlaunch a rocket for any reason (could even be due to the satellite),\nwe have only one week to do the follow-on flight.\nMY RECOMMENDATION\nBelow is my advice about regarding selling SpaceX stock or\noptions. No complicated analysis is required, as the rules of thumb\nare pretty simple.\nIf you believe that SpaceX will execute better than the average\npublic company, then our stock price will continue to appreciate at a\nrate greater than that of the stock market, which would be the next\nhighest return place to invest money over the long term. Therefore,\nyou should sell only the amount that you need to improve your\nstandard of living in the short to medium term. I do actually\nrecommend selling some amount of stock, even if you are certain it\nwill appreciate, as life is short and a bit more cash can increase fun\nand reduce stress at home (so long as you don\u2019t ratchet up your\nongoing personal expenditures proportionately).\nTo maximize your post tax return, you are probably best off\nexercising your options to convert them to stock (if you can afford\nto do this) and then holding the stock for a year before selling it at\nour roughly biannual liquidity events. This allows you to pay the\ncapital gains tax rate, instead of the income tax rate.\nOn a final note, we are planning to do a liquidity event as soon\nas Falcon 9 qualification is complete in one to two months. I don\u2019t\nknow exactly what the share price will be yet, but, based on initial\nconversations with investors, I would estimate probably between\n$30 and $35. This places the value of SpaceX at $4 to $5 billion,\nwhich is about what it would be if we were public right now and, frankly, an excellent number considering that the new F9, FH and\nDragon V2 have yet to launch.\nElon ACKNOWLEDGMENTS\nF\nROM A PROCESS PERSPECTIVE, this will always be two books\ninstead of one in my mind. There\u2019s the time Before Elon, and the time After\nElon.\nThe first eighteen months or so of reporting were filled with tension,\nsorrow, and joy. As mentioned in the main text, Musk initially opted against\nhelping me with the project. This left me going from interview subject to\ninterview subject, giving a huge windup each time to try to talk an ex-Tesla\nemployee or an old schoolmate into an interview. The highs came when\npeople agreed to talk. The lows came when key people said no and to not\nbother them again. String four or five of those no\u2019s together in a row, and it\nfelt at times like writing a proper book about Musk was impossible.\nThe thing that keeps you going is that a few people do say yes and then\na few more, and\u2014interview by interview\u2014you start to figure out how the\npast fits together. I\u2019ll be forever grateful to the hundreds of people who\nwere willing to give freely of their time and especially to those who let me\ncome back again and again with questions. There are too many of these\npeople to list, but gracious souls\u2014like Jeremy Hollman, Kevin Brogan,\nDave Lyons, Ali Javidan, Michael Colonno, and Dolly Singh\u2014each\nprovided invaluable insights and abundant technical help. Heartfelt thanks\ngo as well to Martin Eberhard and Marc Tarpenning, both of whom added\ncrucial, rich parts to the Tesla story.\nEven in this Before Elon period, Musk did permit some of his closer\nfriends to speak with me, and they were generous with their time and\nintellect. That\u2019s a special thanks then to George Zachary and Shervin\nPishevar, and especially to Bill Lee, Antonio Gracias, and Steve Jurvetson,\nwho really went out of their way for Musk and for me. And I obviously owe\na tremendous debt of gratitude to Justine Musk, Maye Musk, Kimbal Musk, Peter Rive, Lyndon Rive, Russ Rive, and Scott Haldeman for their time and\nfor letting me hear some of the family stories. Talulah Riley was kind\nenough to let me interview her and keep prying into her husband\u2019s life. She\nreally brought out some aspects of Musk\u2019s personality that I had not\nencountered elsewhere, and she helped build a much deeper understanding\nof him. This meant a lot to me, and, I think, it will to the readers as well.\nOnce Musk agreed to work with me, much of the tension that\naccompanied the reporting went away and was replaced by excitement. I\ngot access to people like JB Straubel, Franz von Holzhausen, Diarmuid\nO\u2019Connell, Tom Mueller, and Gwynne Shotwell, who are all among the\nmost intelligent and compelling figures I\u2019ve run into during years of\nreporting. I\u2019m forever grateful for their patience explaining bits of company\nhistory and technological basics to me and for their candor. Thanks as well\nto Emily Shanklin, Hannah Post, Alexis Georgeson, Liz Jarvis-Shean, and\nJohn Taylor, for dealing with my constant requests and pestering, and for\nsetting up so many interviews at Musk\u2019s companies. Mary Beth Brown,\nChristina Ra, and Shanna Hendriks were no longer part of Musk Land near\nthe end of my reporting but were all amazing in helping me learn about\nMusk, Tesla, and SpaceX.\nMy biggest debt of gratitude, of course, goes to Musk. When we first\nstarted doing the interviews, I would spend the hours leading up to our chats\nfull of nerves. I never knew how long Musk would keep participating in the\nproject. He might have given me one interview or ten. There was real\npressure to get my most crucial questions answered up front and to be to the\npoint in my initial interviewing. As Musk stuck around, though, the\nconversations went longer, were more fluid, and became more enlightening.\nThey were the things I most looked forward to every month. Whether Musk\nwill change the course of human history in a massive way remains to be\nseen, but it was certainly a thrilling privilege to get to pick the brain of\nsomeone who is reaching so high. While reticent at first, once Musk\ncommitted to the project, he committed fully, and I\u2019m thankful and honored\nthat things turned out that way.\nOn a professional front, I\u2019d like to thank my editors and coworkers over\nthe years\u2014China Martens, James Niccolai, John Lettice, Vindu Goel, and\nSuzanne Spector\u2014each of whom taught me different lessons about the craft\nof writing. Special thanks go to Andrew Orlowski, Tim O\u2019Brien, Damon\nDarlin, Jim Aley, and Drew Cullen, who have had the most impact on how I think about writing and reporting and are among the best mentors anyone\ncould hope for. I must also offer up infinite thanks to Brad Wieners and\nJosh Tyrangiel, my bosses at Bloomberg Businessweek, for giving me the\nfreedom to pursue this project. I doubt there are two people doing more to\nsupport quality journalism.\nA special brand of thanks goes to Brad Stone, my colleague at the New\nYork Times and then at Businessweek. Brad helped me shape the idea for\nthis book, coaxed me through dark times, and was an unrivaled sounding\nboard. I feel bad for pestering Brad so incessantly with my questions and\ndoubts. Brad is a model colleague, always there to help anyone with advice\nor to step up and take on work. He\u2019s an amazing writer and an incredible\nfriend.\nThanks as well to Keith Lee and Sheila Abichandani Sandfort. They are\ntwo of the brightest, kindest, most genuine people I know, and their\nfeedback on the early text was invaluable.\nMy agent David Patterson and editor Hilary Redmon were instrumental\nin helping pull this project off. David always seemed to say the right thing\nat low moments to pick up my spirits. Frankly, I doubt the book would have\nhappened without the encouragement and momentum he provided during\nthe initial part of the project. Once things got going, Hilary talked me\nthrough the trickiest moments and elevated the book to an unexpected\nplace. She tolerated my hissy fits and made dramatic improvements to the\nwriting. It\u2019s wonderful to finish something like this and come out the other\nside with a pair of such good friends. Thanks so much to you both.\nLast, I have to thank my family. This book turned into a living,\nbreathing creature that made life difficult on my family for more than two\nyears. I didn\u2019t get to see my young boys as much as I would have liked\nduring this time, but when I did they were there with energizing smiles and\nhugs. I\u2019m thankful that they both seem to have picked up an interest in\nrockets and cars as a result of this project. As for my wife, Melinda, well,\nshe was a saint. From a practical perspective, this book could not have\nhappened without her support. Melinda was my best reader and ultimate\nconfidante. She was that best friend who knew when to try to energize me\nand when to let things go. Even though this book disrupted our lives for a\nlong while, it brought us closer together in the end. I\u2019m blessed to have such\na partner, and I will forever remember what Melinda did for our family. NOTES\n1. Journal of the Canadian Chiropractic Association, 1995.\n2. http://queensu.ca/news/alumnireview/rocket-man.\n3. http://www.marieclaire.com/sex-love/relationship-issues/millionaire-starter-wife.\n4. The investor Bill Lee, one of Musk\u2019s close friends, originated this phrase.\n5. http://archive.wired.com/science/space/magazine/15-06/ff_ space_musk?currentPage=all.\n6. http://news.cnet.com/Electric-sports-car-packs-a-punch%2C-but-will-it-sell/2100-11389_3-\n6096377.xhtml.\n7. http://www.nytimes.com/2006/07/19/business/19electric.xhtml.\n8. A southern gentleman, Currie could never get used to Musk\u2019s swearing\u2014\u201che curses like a sailor\nand does it in mixed company\u201d\u2014or the way he would churn through prized talent. \u201cHe\u2019d search\nthrough the woods, turn over every rock and dig through brambles to find the one person with\nthe specific expertise and skill he wanted,\u201d Currie said. \u201cThen, that guy would be gone three\nmonths to a year later if he didn\u2019t agree with Elon.\u201d Currie, though, remembers Musk as\ninspirational. Even as Tesla\u2019s funds dwindled, Musk urged the employees to do their jobs well\nand vowed to give them what they needed to be successful. Currie, like many people, also found\nMusk\u2019s work ethic astonishing. \u201cI would be in Europe or China and send him an email at two\nthirty in the morning his time,\u201d Currie said. \u201cFive minutes later, I\u2019d get an answer back. It\u2019s just\nunbelievable to have support on that level.\u201d\n9. http://www.mercurynews.com/greenenergy/ci_7641424.\n10. http://www.telegraph.co.uk/culture/3666994/One-more-giant-leap.xhtml.\n11. http://www.sia.org/wp-content/uploads/2013/06/2013_SSIR_ Final.pdf.\n12. Another moment like this occurred in late 2010 during a launch attempt in Florida. One of the\nSpaceX technicians had left a hatch open overnight at the launchpad, which allowed rain to\nflood a lower-level computing room. The water caused major issues with SpaceX\u2019s computing\nequipment, and another technician had to fly out from California right away with Musk\u2019s\nAmerican Express card in hand to fix the emergency in the days leading up to the launch.\nThe SpaceX engineers bought new computing gear right away and set it up in the room.\nThey needed to run the equipment through standard tests to make sure it could maintain a\ncertain voltage level. It was late at night on a Sunday, and they couldn\u2019t get access on short\nnotice to a device that could simulate the high electrical load. One of the engineers improvised\nby going to a hardware store where he bought twenty-five headlamps for golf carts. The SpaceX\ncrew strung them all together back at the launchpad and hung them from a wall. They then put\non their sunglasses and lit everything up, knowing that if a power supply for the computing\nequipment could survive this test, it would be okay for the flight. The process was repeated for\nnumerous power supplies, and the team worked from 9 P.M. that night until 7 A.M. and finished\nin time to keep the launch on track.\n13. http://www.space.com/15874-private-dragon-capsule-space-station-arrival.xhtml. 14. At the conclusion of the debate, Musk and I exchanged a couple of emails. He wrote, \u201cOil and\ngas is firmly in the Romney camp and they are feeding his campaign these talking points. Until\nrecently, they didn\u2019t care about Tesla, as they thought we would fail.\n\u201cIronically, it is because they are starting to think Tesla might not fail that they are attacking\nus. The reason is that society has to function, so the less there seems to be a viable alternative to\nburning hydrocarbons, the less pressure there is to curb carbon emissions. If an electric car\nsucceeds, it spoils that argument.\n\u201cOverall though, I think it is great that he mentioned us :) \u2018Romney Tesla\u2019 is one of the top\nGoogle searches!\u201d\nI reached out to Romney\u2019s camp months later, as sales of Tesla\u2019s soared, to see if he wanted\nto change his position but was rebuffed.\n15. As Tesla has grown in size, the company has commanded more respect from suppliers and been\nable to get better parts and better deals. But outsourcing components still bothers Musk, and for\nunderstandable reasons. When it tried to ramp up production in 2013, Tesla ran into periodic\nissues because of its suppliers. One of them made what should have been an inconsequential 12-\nvolt lead acid battery that handled a few auxiliary functions in the car. Tesla bought the part\nfrom an American supplier, which in turn outsourced the part from a company in China, which\nin turn outsourced the part from a company in Vietnam. By the time the battery arrived at Tesla\u2019s\nfactories, it didn\u2019t work, adding cost and delays during a crucial period in the Model S\u2019s history.\nIt\u2019s situations like these that typically result in Tesla playing a much more active role with its\nsuppliers when compared to other automakers. For something like an ABS braking controller,\nTesla will work hand-in-hand with its supplier\u2014in this case Bosch\u2014to tune the hardware and\nsoftware for the Model S\u2019s specific characteristics. \u201cMost companies just hand their cars over to\nBosch, but Tesla goes in with a software engineer,\u201d said Ali Javidan. \u201cWe had to change their\nmind-set and let them know we wanted to work on a very deep level.\u201d\n16. Tesla does seem to promote an obsession with safety that\u2019s unmatched in the industry. J. B.\nStraubel explained the company\u2019s thinking as follows: \u201cWith the safety stuff, it seems like car\ncompanies have evolved to a place where their design objectives are set by whatever is regulated\nor has been standardized. The rule says, \u2018Do this and nothing more.\u2019 That is amazingly boring\nengineering. It leaves you maybe fiddling with the car\u2019s shape or trying to make it a bit faster.\nWe have more crumple zones, better deceleration, a lower center of gravity. We went in\nwondering, \u2018Can we make this car twice as safe as anything else on the road?\u2019\u201d\n17. Othmer has lined up to be the lucky owner of the first Roadster II.\nMusk has developed an unconventional policy to determine the order in which cars are sold.\nWhen a new car is announced and its price is set, a race begins in which the first person to hand\nMusk a check gets the first car. With the Model S, Steve Jurvetson, a Tesla board member, had a\ncheck at the ready in his wallet and slid it across the table to Musk after spying details on the\nModel S in a packet of board meeting notes.\nOthmer caught a Wired story about a planned second version of the Roadster and emailed\nMusk right away. \u201cHe said, \u2018Okay, I will sell it to you, but you have to pay two hundred\nthousand dollars right now.\u2019\u201d Othmer agreed, and Tesla had him come to the company\u2019s\nheadquarters on a Sunday to sign some paperwork, acknowledging the price of the car and the\nfact that the company didn\u2019t quite know when it would arrive or what its specifications would\nbe. \u201cMy guess is that it will be the fastest car on the road,\u201d Othmer said. \u201cIt\u2019ll be four-wheel\ndrive. It\u2019s going to be insane. And I don\u2019t really think that will be the real price. I just don\u2019t\nthink Elon wanted me to buy it.\u201d\n18. Musk suspected Better Place came up with battery swapping as a plan after its CEO, Shai\nAgassi, heard about the technology during a tour of the Tesla factory\n19. Musk had made a number of art cars over the years at Burning Man, including an electric one\nshaped like a rocket. In 2011, he also received a lot of grief from the Wall Street Journal for having a high-end camp. \u201cElon Musk, chief executive of electric-car maker Tesla Motors and\nco-founder of eBay Inc.\u2019s PayPal unit, is among those eschewing the tent life,\u201d the paper wrote.\n\u201cHe is paying for an elaborate compound consisting of eight recreational vehicles and trailers\nstocked with food, linens, groceries and other essentials for himself and his friends and family,\nsay employees of the outfitter, Classic Adventures RV. . . . Classic is one of the festival\u2019s few\napproved vendors. It charges $5,500 to $10,000 per RV for its Camp Classic Concierge\npackages like Mr. Musk\u2019s. At Mr. Musk\u2019s RV enclave, the help empties septic tanks, brings\nwater and makes sure the vehicles\u2019 electricity, refrigeration, air conditioning, televisions, DVD\nplayers and other systems are ship shape. The staff also stocked the campers with Diet Coke,\nGatorade and Cruzan rum.\u201d Once the story hit, Musk\u2019s group felt like Classic Adventures had\nleaked the information to drum up business, and they tried to move to a new, undisclosed\nlocation.\n20. http://www.sandia.gov/~jytsao/Solar%20FAQs.pdf.\n21. Tesla employees have been known to sneak across the street to the campus of the software\nmaker SAP and to take advantage of its sumptuous, subsidized cafes.\n22. Shotwell talks about going to Mars as much as Musk and has dedicated her life to space\nexploration. Straubel has demonstrated the same type of commitment with electric vehicles and\ncan sound a lot like Musk at times. \u201cWe are not trying to corner the market on EVs,\u201d Straubel\nsaid. \u201cThere are 100 million cars built per year and 2 billion already out there. Even if we got to\n5 or 10 percent of the market, that does not solve the world\u2019s problems. I am bullish we will\nkeep up with demand and drive the whole industry forward. Elon is committed to this.\u201d\n23. Page presented one of his far-out ideas to me as follows: \u201cI was thinking it would be pretty cool\nto have a prize to fund a project where someone would have to send something lightweight to\nthe moon that could sort of replicate itself. I went over to the NASA operation center here at\nAMES in Mountain View when they were doing a mission and literally flying a satellite into the\nsouth pole of the moon. And they like hurled this thing into the moon at a high velocity and then\nit exploded and it sent matter out into space. And then they looked at that with telescopes, and\nthey discovered water on the south pole of the moon, which sounds really exciting. I started\nthinking that if there\u2019s a lot of water on the south pole of the moon, you can make rocket fuel\nfrom the hydrogen and oxygen. The other cool thing about the south pole is like it almost always\ngets sun. There\u2019s like places high up that get sun and there\u2019s places that are kind of in the craters\nthat are very cold. So you have like a lot of energy then where you could run solar cells. You\ncould almost run like a steam turbine there. You have rocket fuel ingredients, and you have solar\ncells that can be powered by sun, and you could probably run a power plant turbine. Power plant\nturbines aren\u2019t that heavy. You could send that to the moon. You have like a gigawatt of power\non the moon and make a lot of rocket fuel. It would make a good prize project. You send\nsomething to the moon that weights five pounds and have it make rocket fuel so that you could\nlaunch stuff off the moon or have it make a copy of itself, so that you can make more of them.\u201d ABOUT THE AUTHOR\nAuthor photograph \u00a9 by Melinda Vance\nASHLEE VANCE is one of the most prominent writers on technology\ntoday. After spending several years reporting on Silicon Valley and\ntechnology for the New York Times, Vance went to Bloomberg\nBusinessweek, where he has written dozens of cover and feature stories for\nthe magazine on topics ranging from cyber espionage to DNA sequencing\nand space exploration.\nDiscover great authors, exclusive offers, and more at hc.com. ALSO BY ASHLEE VANCE\nGeek Silicon Valley:\nThe Inside Guide to Palo Alto, Stanford, Menlo Park, Mountain View,\nSanta Clara, Sunnyvale, San Jose, San Francisco CREDITS\nCOVER DESIGN BY ALLISON SALTZMAN\nCOVER PHOTOGRAPH \u00a9 BY ART STREIBER/AUGUST COPYRIGHT\nELON MUSK. Copyright \u00a9 2015 by Ashlee Vance. All rights reserved under International and Pan-\nAmerican Copyright Conventions. By payment of the required fees, you have been granted the\nnonexclusive, nontransferable right to access and read the text of this e-book on-screen. No part of\nthis text may be reproduced, transmitted, downloaded, decompiled, reverse-engineered, or stored in\nor introduced into any information storage and retrieval system, in any form or by any means,\nwhether electronic or mechanical, now known or hereafter invented, without the express written\npermission of HarperCollins e-books.\nFIRST EDITION\nISBN 978-0-06-230123-9\nEPub Edition MAY 2015 ISBN 9780062301260\n15 16 17 18 19 OV/RRD 10 9 8 7 6 5 4 3 2 1 ABOUT THE PUBLISHER\nAustralia\nHarperCollins Publishers Australia Pty. Ltd.\nLevel 13, 201 Elizabeth Street\nSydney, NSW 2000, Australia\nwww.harpercollins.com.au\nCanada\nHarperCollins Canada\n2 Bloor Street East - 20th Floor\nToronto, ON M4W 1A8, Canada\nwww.harpercollins.ca\nNew Zealand\nHarperCollins Publishers New Zealand\nUnit D1, 63 Apollo Drive\nRosedale 0632\nAuckland, New Zealand\nwww.harpercollins.co.nz\nUnited Kingdom\nHarperCollins Publishers Ltd.\n1 London Bridge Street\nLondon SE1 9GF, UK\nwww.harpercollins.co.uk\nUnited States\nHarperCollins Publishers Inc.\n195 Broadway\nNew York, NY 10007 www.harpercollins.com * Two years after the birth of his son, John Elon began to show signs of diabetes. The condition\namounted to a death sentence at the time and, despite being only thirty-two, John Elon learned\nthat he would likely have six months or so to live. With a bit of nursing experience behind her,\nAlmeda took it upon herself to discover an elixir or treatment that would extend John Elon\u2019s life.\nAccording to family lore, she hit on chiropractic procedures as an effective remedy, and John\nElon lived for five years following the original diabetes diagnosis. The life-giving procedures\nestablished what would become an oddly rich chiropractic tradition in the Haldeman family.\nAlmeda studied at a chiropractic school in Minneapolis and earned her doctor of chiropractic, or,\nD.C., degree in 1905. Musk\u2019s great-grandmother went on to set up her own clinic and, as far as\nanyone can tell, became the first chiropractor to practice in Canada. * Haldeman also entered politics, trying to start his own political party in Saskatchewan, publishing\na newsletter, and espousing conservative, antisocialist ideas. He would later make an unsuccessful\nrun for Parliament and chair the Social Credit Party. * The journey took them up the African coast, across the Arabian Peninsula, all the way through\nIran, India, and Malaysia and then down the Timor Sea to Australia. It required one year of\npreparation just to secure all of the necessary visas and paperwork, and they suffered from\nconstant stomach bugs and an erratic schedule along the way. \u201cDad passed out crossing the Timor\nSea, and mum had to take over until they hit Australia. He woke up right before they landed,\u201d\nsaid Scott Haldeman. \u201cIt was fatigue.\u201d * Both Joshua and Wyn were accomplished marksmen and won national shooting competitions. In\nthe mid-1950s, they also tied for first place in the eight-thousand-mile Cape Town to Algiers\nMotor Rally, beating pros in their Ford station wagon. * Musk couldn\u2019t remember this particular conversation. \u201cI think they might be having creative\nrecollection,\u201d he said. \u201cIt\u2019s possible. I had lots of esoteric conversations the last couple years of\nhigh school, but I was more concerned about general technology than banking.\u201d * When Maye went to Canada to check out places to live, a fourteen-year-old Tosca seized the\nmoment and put the family house in South Africa up for sale. \u201cShe had sold my car as well and\nwas in the midst of putting our furniture up for sale, too,\u201d Maye said. \u201cWhen I got back, I asked\nher why. She said, \u2018There is no need to delay. We are getting out of here.\u2019\u201d * The Musk brothers were not the most aggressive businessmen at this point. \u201cI remember from\ntheir business plan that they were originally asking for a ten-thousand-dollar investment for\ntwenty-five percent of their company,\u201d said Steve Jurvetson, the venture capitalist. \u201cThat is a\ncheap deal! When I heard about the three-million-dollar investment, I wondered if Mohr\nDavidow had actually read the business plan. Somehow, the brothers ended up raising a normal\nventure round.\u201d * Musk also got to show off the new office to his mother, Maye, and Justine. Maye sometimes sat\nin on meetings and came up with the idea of adding a \u201creverse directions\u201d button on the Zip2\nmaps, which let people flip around their journeys and ended up becoming a popular feature on all\nmapping services. * At one point, the founders thought the easiest way to solve their problems would just be to buy a\nbank and revamp it. While that didn\u2019t happen, they did snag a high-profile controller from Bank\nof America, who in turn explained, in painful detail, the complexities of sourcing loans,\ntransferring money, and protecting accounts. * Fricker disputed that he yearned to be CEO, saying instead that the other employees had\nencouraged him to take over because of Musk\u2019s struggles getting the business off the ground.\nFricker and Musk, once close friends, remain unimpressed with each other. \u201cElon has his own\ncode of ethics and honor and plays the game extraordinarily hard,\u201d Fricker said. \u201cWhen it comes\ndown to it, for him, business is war.\u201d According to Musk, \u201cHarris is very smart, but I don\u2019t think\nhe has a good heart. He had a really intense desire to be running the show, and he wanted to take\nthe company in ridiculous directions.\u201d Fricker went on to have a very successful career as CEO of\nGMP Capital, a Canadian financial services company. Payne founded a private equity firm in\nToronto. * Musk had been pushed out as CEO of X.com by the company\u2019s investors, who wanted a more\nseasoned executive to lead the company toward an IPO. In December 1999, X.com hired Bill\nHarris, the former CEO of the financial software maker Intuit, as its new chief. After the merger,\nmany in the company turned on Harris, he resigned, and Musk returned as the CEO. * After feeling ill for a few days, Musk went to Stanford Hospital and informed them that he\u2019d\nbeen in a malaria zone, although the doctors could not find the parasite during tests. The doctors\nperformed a spinal tap and diagnosed him with viral meningitis. \u201cI may very well have also had\nthat, and they treated me for it, and it did get better,\u201d Musk said. The doctors discharged Musk\nfrom the hospital and warned him that some symptoms would recur. \u201cI started feeling bad a few\ndays later, and it got progressively worse,\u201d Musk said. \u201cEventually, I couldn\u2019t walk. It was like,\n\u2018Okay, this is even worse than the first time.\u2019\u201d Justine took Musk to a general practitioner in a\ncab, and he lay on the floor of the doctor\u2019s office. \u201cI was so dehydrated that she couldn\u2019t take my\nvitals,\u201d Musk said. The doctor called an ambulance, which transported Musk to Sequoia Hospital\nin Redwood City with IVs in both arms. Musk faced another misdiagnosis\u2014this time of the type\nof malaria. The doctors declined to give Musk a more aggressive treatment that came with nasty\nside effects including heart palpitations and organ failure. * When Zubrin and some of the other Mars buffs heard of Musk\u2019s plant project, they were upset. \u201cIt\ndidn\u2019t make any sense,\u201d Zubrin said. \u201cIt was a purely symbolic thing to do, and the second they\nopened that door, millions of microbes would escape and plague all of NASA\u2019s contamination\nprotocols.\u201d * Most of the stories written about Musk that touch on this period say he went to Moscow three\ntimes. According to Cantrell\u2019s detailed records, this is not the case. Musk met with the Russians\ntwice in Moscow, and once in Pasadena, California. He also met with Arianespace in Paris, and in\nLondon with Surrey Satellite Technology Ltd., which Musk considered buying. * Buzza knew Hollman\u2019s work at Boeing and coaxed him to SpaceX about six months after the\ncompany started. * Including a 1,300-pound hunk of copper. * Before returning to El Segundo, Hollman used a drill press to remove the glasses\u2019 safety shield.\n\u201cI didn\u2019t want to look like a nerd on the flight home,\u201d he said. * Hollman left the company after this incident in November 2007 and then returned for a spell to\ntrain new personnel. A number of people I interviewed for the book said that Hollman was so key\nto SpaceX\u2019s early days that they feared the company might flame out without him. * In a press release announcing the funding round, Musk was not listed as a founder of the\ncompany. In the \u201cAbout Tesla Motors\u201d section, the company stated, \u201cTesla Motors was founded\nin June 2003 by Martin Eberhard and Marc Tarpenning to create efficient electric cars for people\nwho love to drive.\u201d Musk and Eberhard would later spar over Musk\u2019s founder status. * This was how the employee remembered the text. I did not see the actual e-mail. Musk later told\nthe same employee, \u201cI want you to think ahead and think so hard every day that your head hurts. I\nwant your head to hurt every night when you go to bed.\u201d * Musk fought to set the record straight, as he saw it, on the Huffington Post and wrote a 1,500-\nword essay. Musk maintained that two months of negotiations with independent parties had gone\ninto the postnuptial agreement, which kept the couple\u2019s assets separate so that Musk could get the\nspoils from his companies and Justine could get the spoils from her books. \u201cIn mid 1999, Justine\ntold me that if I proposed to her, she would say yes,\u201d Musk wrote. \u201cSince this was not long after\nthe sale of my first company, Zip2, to Compaq, and the subsequent cofounding of PayPal, friends\nand family advised me to separate whether the marriage was for love or money.\u201d After the\nsettlement, Musk asked Arianna Huffington to remove his essay about the divorce from her\nwebsite. \u201cI don\u2019t want to dwell on past negativity,\u201d Musk said. \u201cYou can always find things on the\nInternet. So it\u2019s not like it\u2019s gone. It\u2019s just not easily found.\u201d * The pair have continued to have their difficulties. For a long time, Musk ran all of the child-\nsharing scheduling through his assistant Mary Beth Brown rather than dealing directly with\nJustine. \u201cI was really pissed-off about that,\u201d Justine said. And the time Justine cried the most\nduring our conversation came as she weighed the pros and cons of the children growing up on a\ngrand stage where they\u2019re whisked away to the Super Bowl or Spain in a private jet on a\nmoment\u2019s notice or asked to play at the Tesla factory. \u201cI know the kids really look up to him,\u201d she\nsaid. \u201cHe takes them everywhere and provides a lot of experiences for them. My role as the\nmother is to create this reality where I provide a sense of normalcy. They are not growing up in a\nnormal family with a normal dad. Their life with me is a lot more low-key. We value different\nthings. I am a lot more about empathy.\u201d * Musk recalled their meeting as follows: \u201cShe did look great, but what was going through my\nmind was \u2018Oh, I guess they are a couple of models.\u2019 You know, you can\u2019t actually talk to most\nmodels. You just can\u2019t have a conversation. But, you know, Talulah was really interested in\ntalking about rockets and electric cars. That was the interesting thing.\u201d * He asked Riley to go with him, but she turned Musk down. * By this time, Musk had built up a reputation as the hardest-charging man in the space business.\nBefore settling on the Falcon 9, Musk planned to build something called the BFR, a.k.a. the Big\nFalcon Rocket or Big Fucking Rocket. Musk wanted it to have the biggest rocket engine in\nhistory. Musk\u2019s bigger, faster mentality amused, horrified and impressed some of the suppliers\nthat SpaceX occasionally turned to for help, like Barber-Nichols Inc., a Colorado-based maker of\nrocket engine turbo pumps and other aerospace machinery. A few executives at Barber-Nichols\u2014\nRobert Linden, Gary Frey, and Mike Forsha\u2014were kind enough to recount their first meeting\nwith Musk in the middle of 2002 and their subsequent dealings with him. Here\u2019s a snippet:\n\u201cElon showed up with Tom Mueller and started telling us it was his destiny to launch things\ninto space at lower costs and to help us become space faring people. We thought the world of\nTom but weren\u2019t quite sure whether to take Elon too seriously. They began asking us for the\nimpossible. They wanted a turbo pump to be built in less than a year for under one million\ndollars. Boeing might do a project like that over five years for one hundred million. Tom told us\nto give it our best shot, and we built it in thirteen months. Build quick and learn quickly was\nElon\u2019s philosophy. He was relentless in wanting the costs to come down. Regardless of what we\nshowed him on paper with regard to the cost of materials, he wanted the cost lower because that\nwas part of his business model. It could be very frustrating to work with Elon. He has a singular\nview and doesn\u2019t deviate from that. We don\u2019t know too many people that have worked for him\nthat are happy. That said, he has driven the cost of space down and been true to his original\nbusiness plan. Boeing, Lockheed, and the rest of them have become overly cautious and spend a\nlot of money. SpaceX has balls.\u201d * To provide a glimpse of how well Musk knows the rockets, here he is explaining what happened\nfrom memory six years after the fact: \u201cIt was because we had upgraded the Merlin engine to a\nregeneratively cooled engine and the thrust transient of that engine was a few seconds longer. It\nwas only like one percent thrust for about another 1.5 seconds. And the chamber pressure was\nonly ten PSI, which is one percent of the total. But that\u2019s below sea level pressure. On the test\nstand, we didn\u2019t notice anything. We thought it was fine. We thought it was just the same as\nbefore, but actually it just had this slight difference. The ambient sea level pressure was higher at\nroughly fifteen PSI, which disguised some effects during the test. The extra thrust caused the first\nstage to continue moving after stage separation and recontact the other stage. And the upper stage\nthen started the engine inside the interstage, which caused the plasma blowback which destroyed\nthat upper stage.\u201d * Musk would later discover the identity of this employee in an ingenious way. He copied the text\nof the letter into a Word document, checked the size of the file, sent it to a printer, and looked\nover the logs of printer activity to find one of the same size. He could then trace that back to the\nperson who had printed the original file. The employee wrote a letter of apology and resigned. * Griffin had pined to build a massive new spacecraft that would solidify his mark on the industry.\nBut, with the election of Barack Obama in 2008, the Bush appointee knew that his time as NASA\nchief was coming to an end and that SpaceX appeared poised to build the most interesting\nmachines moving forward. * It should be noted that there are many people in the space industry who doubt reusable rockets\nwill work, in large part because of the stress the machines and metal go through during launch.\nIt\u2019s not clear that the most prized customers will even consider the reused spacecraft for launches\ndue to their inherent risks. This is a big reason that other countries and companies have not\npursued the technology. There\u2019s a camp of space experts who think Musk is flat-out wasting his\ntime, and that engineering calculations already prove the reusable rockets to be a fool\u2019s errand. * Blue Origin also hired away a large chunk of SpaceX\u2019s propulsion team. * Musk has taken exception to Blue Origin and Bezos filing for patents around reusable rocket\ntechnology as well. \u201cHis patent is completely ridiculous,\u201d Musk said. \u201cPeople have proposed\nlanding on a floating platform in the ocean for a half century. There\u2019s no chance whatsoever of\nthe patent being upheld because there\u2019s five decades of prior art of people who proposed that six\nways to Sunday in fiction and nonfiction. It\u2019s like Dr. Seuss, green eggs and fucking ham. That\u2019s\nhow many ways it\u2019s been proposed. The issue is doing it and like actually creating a rocket that\ncan make that happen.\u201d * Michael Colonno. * According to Musk, \u201cThe early Dragon Version 1 work was just me and maybe three or four\nengineers, as we were living hand to mouth and had no idea if NASA would award us a contract.\nTechnically, there was Magic Dragon before that, which was much simpler, as it had no NASA\nrequirements. Magic Dragon was just me and some high altitude balloon guys in the U.K.\u201d * NASA researchers studying the Dragon design have noticed several features of the capsule that\nappear to have been purpose built from the get-go to accommodate a landing on Mars. They\u2019ve\npublished a couple of papers explaining how it could be feasible for NASA to fund a mission to\nMars in which a Dragon capsule picks up samples and returns them to Earth. * The politicking in the space business can get quite nasty. Lori Garver, the former deputy\nadministrator of NASA, spent years fighting to open up NASA contracts so that private\ncompanies could bid on things like resupplying the ISS. Her position of fostering a strong\nrelationship between NASA and the private sector won out in the end but at a cost. \u201cI had death\nthreats and fake anthrax sent to me,\u201d she said. Garver also ran across SpaceX competitors that\ntried to spread unfounded gossip about the company and Musk. \u201cThey claimed he was in\nviolation of tax laws in South Africa and had another, secret family there. I said, \u2018You\u2019re making\nthis stuff up.\u2019 We\u2019re lucky that people with such long-term visions as Elon, Jeff Bezos, and\nRobert Bigelow [founder of the aerospace company that bears his name] got rich. It\u2019s nuts that\npeople would want to vilify Elon. He might say some things that rub people the wrong way, but,\nat some point, the being nice to everyone thing doesn\u2019t work.\u201d * On this flight, SpaceX secretly placed a wheel of cheese inside the Dragon capsule. It was the\nsame one Jeff Skoll had given Musk back in the mice-to-Mars days. * Musk explained the look to me in a way that only he can. \u201cI went for a similar style to the Model\nS (it uses the same screens as Model S upgraded for space ops), but kept the aluminum isogrid\nuncovered for a more exotic feel.\u201d * Rather insanely, NASA is building a next-generation, giant spaceship that could one day get to\nMars even though SpaceX is building the same type of craft\u2014the Falcon Heavy\u2014on its own.\nNASA\u2019s program is budgeted to cost $18 billion, although government studies say that figure is\nvery conservative. \u201cNASA has no fucking business doing this,\u201d said Andrew Beal, the billionaire\ninvestor and onetime commercial space entrepreneur. \u201cThe whole space shuttle system was a\ndisaster. They\u2019re fucking clueless. Who in their right mind would use huge solid boosters,\nespecially ones built in segments requiring dynamic seals? They are so lucky they only had one\ndisastrous failure of the boosters.\u201d Beal\u2019s firm criticisms come from years of watching the\ngovernment compete against private space companies by subsidizing the construction of\nspacecraft and launches. His company Beal Aerospace quit the business because the government\nkept funding competing rockets. \u201cGovernments around the world have spent billions trying to do\nwhat Elon is doing, and they have failed,\u201d he said. \u201cWe have to have governments, but the idea\nthat the government goes out and competes with companies is fucking nuts.\u201d * The volume level on the sound system naturally goes to 11\u2014an homage to This Is Spinal Tap and\na reflection of Musk\u2019s sense of humor. * And it\u2019s not just that the Model S and other electric cars are three to four times more efficient\nthan internal combustion vehicles. They can also tap into power that is produced in centralized,\nefficient ways by power plants and solar arrays. * When the very first Roadster arrived, it came in a large plywood crate. Tesla\u2019s engineers\nunpacked it furiously, installed the battery pack, and then let Musk take it for a spin. About\ntwenty Tesla engineers jumped in prototype vehicles and formed a convoy that followed Musk\naround Palo Alto and Stanford. * At some point from late 2007 to 2008, Musk also tried to hire Tony Fadell, an executive at Apple\nwho is credited with bringing the iPod and iPhone to life. Fadell remembered being recruited for\nthe CEO job at Tesla, while Musk remembered it more as a chief operating officer type of\nposition. \u201cElon and I had multiple discussions about me joining as Tesla\u2019s CEO, and he even\nwent to the lengths of staging a surprise party for me when I was going to visit their offices,\u201d\nFadell said. Steve Jobs caught wind of these meetings and turned on the charm to keep Fadell.\n\u201cHe was sure nice to me for a while,\u201d Fadell said. A couple of years later, Fadell left Apple to\nfound Nest, a maker of smart-home devices, which Google then acquired in 2014. * It took a couple of years, from about 2007 to 2009, for the Energy Department application to\nmorph into the actual possibility of a loan from the government. * The deal had two parts. Tesla would keep making battery packs and associated technology that\nother companies might use, and it would produce its own electric vehicles at a manufacturing\nfacility in the United States. * Musk had received a lot of pushback internally for trying to locate a car factory in or near\nCalifornia. \u201cAll the guys in Detroit said it needs to be in a place where the labor can afford to live\nand be happy,\u201d Lloyd said. \u201cThere\u2019s a lot of learned skill on an assembly line, and you can\u2019t\nafford turnover.\u201d Musk responded that SpaceX had found a way to build rockets in Los Angeles,\nand that Tesla would find a way to build cars in Northern California. His stubbornness ended up\nbeing fortuitous for the company. \u201cIf it hadn\u2019t been for that DOE loan, and the NUMMI plant,\nthere\u2019s no way Tesla would have ended up being so successful, so fast,\u201d Lloyd said. * Boeing used to make fuselages for the 747 in the SpaceX building and painted them in what\nbecame the Tesla design studio. * \u201cHe picks the most visible place on purpose,\u201d said the investor and Tesla board member Steve\nJurvetson. \u201cHe\u2019s at Tesla just about every Saturday and Sunday and wants people to see him and\nknow they can find him. Then, he can also call suppliers on the weekend, and let them know that\nhe\u2019s personally putting in the hours on the factory floor and expects the same from them.\u201d * Tesla got its start using the same lithium ion batteries that go into consumer electronics like\nlaptops. During the early days of the Roadster, this proved a risky but calculated choice. Tesla\nwanted to tap into Asia\u2019s mature battery suppliers and get access to cheap products that would\nkeep improving over time. The press played up Tesla\u2019s use of these types of batteries, and\nconsumers were fascinated by the idea that a car could be powered by the same energy source\nsitting inside of their gizmos.\nThere\u2019s a major misconception that Tesla still depends on these types of batteries. Yes, the\nbatteries inside the Model S look like those found in a laptop. Tesla, however, started developing\nits own battery chemistry in conjunction with partners like Panasonic dating back to late models\nof the Roadster. Tesla can still use the same manufacturing equipment as consumer electronics\ncompanies while ending up with a battery that\u2019s safer and better tuned to the intense charging\ndemands of its cars. Along with the secret formula for the battery cells themselves, Tesla has\nimproved the performance of its batteries by developing its own techniques for linking the cells\ntogether and cooling them. The battery cells have been designed to vent heat in a very particular\nway, and there\u2019s coolant running throughout the entire battery pack. The battery packs are\nassembled at the Tesla factory in an area hidden from visitors.\nThe chemistry, the batteries, the battery pack design\u2014these are all elements of a large,\ncontinuous system that Tesla has built from the ground up to allow its cars to charge at record\nspeed. To control the heat produced during the charging process, Tesla has designed an\ninterlinked system of radiators and chillers to cool both the batteries and the chargers. \u201cYou\u2019ve\ngot all that hardware plus the software management system and other controllers,\u201d said J. B.\nStraubel. \u201cAll of these things are running at maximum rate.\u201d A Model S can recharge 150 miles\nof range in 20 minutes at one of Tesla\u2019s charging stations with DC power pumping straight into\nthe batteries. By comparison, a Nissan Leaf that maxes out at 80 miles of range can take 8 hours\nto recharge. * Google\u2019s attorneys had asked to make a presentation to Tesla\u2019s board. Before he would permit\nthis, Musk asked for the right to call on Google for a loan in case Tesla encountered cash flow\nissues after acquisition talks became public, as there would otherwise be no way for Tesla to raise\nmoney. Google hesitated on this for a few weeks, by which time Tesla ended up in the clear. * Following the demonstration, Tesla struggled to deliver on the battery swap technology. Musk\nhad promised that the first few stations would arrive in late 2013. A year after the event, though,\nTesla had yet to open a single station. According to Musk, the company ended up needing to deal\nwith more pressing issues. \u201cWe\u2019re going to do it because we said we\u2019d do it,\u201d Musk said. \u201cIt may\nnot be on the schedule that we\u2019d like but we always come through in the end.\u201d * As for the origins of the Model S name, Musk said, \u201cWell, I like calling things what they are. We\nhad the Roadster, but there was no good word for a sedan. You can\u2019t call it the Tesla Sedan.\nThat\u2019s boring as hell. In the U.K., they say \u2018saloon,\u2019 but then it\u2019s sort of like, \u2018What are you? A\ncowboy or something?\u2019 We went through a bunch of iterations, and the Model S sounded the best.\nAnd it was like a vague nod to Ford being the Model T in that electric cars preceded the Model T,\nand in a way we\u2019re coming full circle and the thing that proceeded the Model T is now going into\nproduction in the twenty-first century, hence the Model S. But that\u2019s sort of more like reversing\nthe logic.\u201d * A handful of lawsuits have been filed against Tesla with auto dealers arguing that the company\nshould not be able to sell its cars directly. But even in those states that have banned Tesla\u2019s stores,\nprospective customers can usually request a test drive, and someone from Tesla will show up with\na vehicle. \u201cSometimes you have to put something out there for people to attack,\u201d Musk said. \u201cIn\nthe long run, the stores won\u2019t be important. The way things will really grow is by word of mouth.\nThe stores are like a viral seed to get things going.\u201d * Or as Straubel put it, \u201cWatching people drive the Model S across the country is phenomenal.\nThere is no way you can do that in anything else. It\u2019s not about putting a charging station in the\ndesert as a stunt. It\u2019s about realizing where this is going to go. We will end up launching the third-\ngeneration car into a world where this charging network is free and ubiquitous. It bugs me when\npeople compare us to a car company. The cars are absolutely our main product, but we are also an\nenergy company and a technology company. We are going down to the dirt and having\ndiscussions with mining companies about the materials for our batteries and going up to\ncommercialize all the pieces that make up an electronic vehicle and all the pieces that make an\nawesome product.\u201d * No, really. Both Lyndon and his wife play underwater hockey and used these skills to secure\ngreen cards, meeting the criteria for the \u201cexceptional abilities\u201d the United States desires. They\nultimately played for the U.S. national teams. * Thirteen thousand people showed up in 2013. * If you assume an average selling price of $40,000 per car for 300,000 cars sold in a year, that\u2019s\n$12 billion in annual revenue, or $1 billion per month. * For the space buffs, here\u2019s Musk talking more about the physics and chemistry of the spaceship:\n\u201cThe final piece of the puzzle for figuring out the Mars architecture is a methane engine. You\nneed to be able to generate the propellant on the surface. Most of the fuel used in rockets today is\na form of kerosene, and creating kerosene is quite complex. It\u2019s a series of long-chain\nhydrocarbons. It\u2019s much easier to create either methane or hydrogen. The problem with hydrogen\nis it\u2019s a deep cryogen. It\u2019s only a liquid very close to absolute zero. And because it\u2019s a small\nmolecule you have these issues where hydrogen will seep its way through a metal matrix and\nembrittle or destroy metal in weird ways. Hydrogen\u2019s density is also very porous, so the tanks are\nenormous and it\u2019s expensive to create and store hydrogen. It\u2019s not a good choice as a fuel.\n\u201cMethane, on the other hand, is much easier to handle. It\u2019s liquid at around the same\ntemperature as liquid oxygen so you can do a rocket stage with a common bulkhead and not\nworry about freezing one or the other solid. Methane is also the lowest-cost fossil fuel on Earth.\nAnd there needs to be a lot of energy to go to Mars.\n\u201cAnd then on Mars, because the atmosphere is carbon dioxide and there\u2019s a lot of water or ice\nin the soil, the carbon dioxide gets you CO , the water gives you H O. With that you create CH\n2 2 4\nand O , which gives you combustion. So it\u2019s all sort of nicely worked out.\n2\n\u201cAnd then one of the key questions is can you get to the surface of Mars and back to Earth on\na single stage. The answer is yes, if you reduce the return payload to approximately one-quarter\nof the outbound payload, which I thought made sense because you are going to want to transport\na lot more to Mars than you\u2019d want to transfer from Mars to Earth. For the spacecraft, the heat\nshield, the life support system, and the legs will have to be very, very light.\u201d * Musk and Riley were divorced for less than year. \u201cI refused to speak with him for as long it took\nfor the divorce to be finalized,\u201d Riley said. \u201cAnd then, once it was finalized, we immediately got\nback together.\u201d As for what caused the breakup, Riley said, \u201cI just wasn\u2019t happy. I thought maybe\nI had made the wrong decision for my life.\u201d And, about what brought her back to Musk, Riley\nsaid, \u201cOne reason was the lack of viable alternatives. I looked around, and there was no one else\nnice to be with. Number two is that Elon doesn\u2019t have to listen to anyone in life. No one. He\ndoesn\u2019t have to listen to anything that doesn\u2019t fit into his worldview. But he proved he would take\nshit from me. He said, \u2018Let me listen to her and figure these things out.\u2019 He proved that he valued\nmy opinion on things in life and was willing to listen. I thought it was quite a telling thing for the\nman\u2014that he made the effort. And then, I loved him and missed him.\u201d * As Musk recalled, \u201cI told her, \u2018Look, I think you\u2019re very valuable. Maybe that compensation is\nright. You need to take two weeks\u2019 vacation, and I\u2019m going to assess whether that\u2019s true our not.\u2019\nBefore this came up, I had offered her multiple all-expenses-paid vacations. I really wanted her to\ntake a vacation. When she got back, my conclusion was just that the relationship was not going to\nwork anymore. Twelve years is a good run for any job. She\u2019ll do a great job for someone.\u201d\nAccording to Musk, he offered Brown another position at the company. She declined the offer by\nnever showing up at the office again. Musk gave her twelve months\u2019 severance and has not\nspoken to her since. * According to Riley, \u201cElon is kind of cheeky and funny. He is very loving. He is devoted to his\nchildren. He is funny\u2014really, really, really funny. He\u2019s quite mercurial. He\u2019s genuinely the oddest\nperson I have ever met. He has moments of self-awareness and lucidity, which for me always\nbring him back around. He\u2019ll say something cheeky or funny and have this grin. He\u2019s smart in all\nsorts of areas. He\u2019s very well read and has this incredible wit. He loves movies. We went to see\nthe new Lego Movie and afterwards he insisted on being referred to as Lord Business. He tries to\ncome home early for family dinners with me and the kids and maybe play some computer games\nwith the boys. They will tell us about their day, and we\u2019ll put them to bed. Then we\u2019ll chat and\nwatch something together on the laptop like The Colbert Report. On the weekends, we\u2019re\ntraveling. The kids are good travelers. There were bajillions of nannies before. There was even a\nnanny manager. Things are a bit more normal now. We try and do stuff just as a family when we\ncan. We have the kids four days a week. I like to say that I am the disciplinarian. I want them to\nhave the sense of an ordinary life, but they live a very odd life. They were just on a trip with\nJustin Bieber. They go to the rocket factory and are like, \u2018Oh no, not again.\u2019 It\u2019s not cool if your\ndad does it. They\u2019re used to it.\n\u201cPeople don\u2019t realize that Elon has this incredible naivet\u00e9. There are certain times when he is\nincapable of anything other than pure joy. And then other times pure anger. When he feels\nsomething, he feels it so completely and purely. Nothing else can impose on it. There are so few\npeople who can do that. If he sees something funny, he will laugh so loudly. He won\u2019t realize we\nare in a crowded theater and that other people are there. He is like a child. It\u2019s sweet and amazing.\nHe says this random stuff like, \u2018I am a complicated man with very simple but specific needs\u2019 or\n\u2018No man is an island unless he is large and buoyant.\u2019 We make these lists of things we want to do.\nHis latest contributions were to walk on a beach at sunset and whisper sweet nothings in each\nother\u2019s ear and to take more horseback rides. He likes reading, playing video games, and being\nwith friends.\u201d * Jurvetson elaborated by saying, \u201cElon has that engineering prowess of Gates, but he\u2019s more\ninterpersonal. You have to be out there on the spectrum with Gates. Elon has more interpersonal\ncharms. He\u2019s like Jobs in that neither of them suffer fools. But with Jobs there was more of a\nhero-shit roller coaster where employees went from in favor to out of favor. I also think Elon has\naccomplished more.\u201d",
    "data/library/STREAM_Report.txt": "S T R E A M   A I   1 \n \n 1 \n \n R e a l - T i m e ,   M u l t i t h r e a d e d   A n o m a l y   D e t e c t i o n   S y s t e m   w i t h   A u t o n o m o u s \n \n S y n c h r o n i z a t i o n   &   S e l f - C o r r e c t i o n \n \n B r i a n   P r z e z d z i e c k i \n \n U n i v e r s i t y   o f   B u f f a l o   ( S U N Y ) \n \n \f S T R E A M   A I   1 \n \n 2 \n \n A b s t r a c t \n \n R e a l - t i m e   a n o m a l y   d e t e c t i o n   i s   c r u c i a l   d u r i n g   t h e   p r i n t i n g   p r o c e s s ,   e s p e c i a l l y   w h e n \n \n l o o k i n g   f o r   i s s u e s   l i k e   o v e r   o r   u n d e r   e x t r u s i o n .   W e   e m p l o y   a   M o b i l e N e t   m o d e l   t o   c l a s s i f y   t h e s e \n \n a n o m a l i e s .   O u r   s y s t e m   h a s   t w o   m a i n   c o m p o n e n t s :   T i p   T r a c k i n g   a n d   A n o m a l y   D e t e c t i o n .   I n   t h e \n \n T i p   T r a c k i n g   p h a s e ,   w e   e s t i m a t e   t h e   l o c a t i o n s   o f   t h e   t i p s   u s i n g   g c o d e   d a t a .   T h e n ,   w e   u s e   t h e \n \n Y O L O   m o d e l   t o   a l i g n   t h e s e   p r e d i c t i o n s   w i t h   l i v e   v i d e o   f o o t a g e ,   e n s u r i n g   s y n c h r o n i z a t i o n   a n d \n \n c o r r e c t i n g   a n y   p r e d i c t i o n   i n a c c u r a c i e s .   G i v e n   t h e   n e e d   f o r   r e a l - t i m e   p e r f o r m a n c e   o n   a   R a s p b e r r y \n \n P i ,   o u r   s y s t e m   i s   o p t i m i z e d   f o r   c o m p u t a t i o n a l   e f f i c i e n c y .   I t ' s   d e s i g n e d   t o   o p e r a t e   i n   r e a l - t i m e , \n \n r u n n i n g   a l o n g s i d e   v i d e o   f e e d s ,   a n d   s t r i k e s   a   b a l a n c e   b e t w e e n   a c c u r a c y ,   a u t o n o m y ,   r o b u s t n e s s   a n d \n \n s p e e d . \n \n \f S T R E A M   A I   1 \n \n 3 \n \n P r o b l e m \n \n W e   n e e d   t o   s p o t   e r r o r s   i n   3 D   p r i n t i n g   a s   t h e y   h a p p e n .   T h e   m a i n   m i s t a k e s   a r e   t i e d   t o   h o w \n \n m u c h   m a t e r i a l   c o m e s   o u t   -   t o o   m u c h   o r   t o o   l i t t l e   ( U n d e r / O v e r   E x t r u s i o n ) . \n \n C l a s s e s : \n \n O v e r \n \n N o r m a l \n \n U n d e r \n \n A n o t h e r   p r o b l e m   a r i s e s :   G i v e n   a   f r a m e ,   t h e   o n l y   r e l e v a n t   i n f o r m a t i o n   i s   t h e   a r e a   a r o u n d   t h e   t i p   i n \n \n t h e   d i r e c t i o n   o f   r e c e n t l y   e x t r u d e d   m a t e r i a l .   T h i s   i s   w h y   w e   n e e d   t o   t r a c k   t h e   t i p . \n \n \f S T R E A M   A I   1 \n \n 4 \n \n S o l u t i o n \n \n T h e   g i v e n   d i a g r a m   d i s p l a y s   h o w   t h e   s y s t e m   i s   d i v i d e d   i n t o   d i s t i n c t   t h r e a d s   a n d   i l l u s t r a t e s \n \n t h e   p r o c e s s   f l o w .   A l l   t h r e a d s   a c c e s s   s o m e   s h a r e d   g l o b a l   v a r i a b l e s .   T h e   t r a c k e r   t h r e a d   i n i t i a t e s   a n d \n \n m a n a g e s   a l l   o t h e r   t h r e a d s .   T h i s   s y s t e m   p r i m a r i l y   r e v o l v e s   a r o u n d   t h r e e   k e y   c o m p o n e n t s : \n \n 1 .   I n i t i a l i z a t i o n :   P r e d i c t i n g   t h e   p a t h   o f   t i p   t r a c k i n g   a n d   a l i g n i n g   w i t h   t h e   v i d e o . \n \n 2 .   E r r o r   C o r r e c t i o n :   C o r r e c t i n g   e r r o r s   t h r o u g h   a   f e e d b a c k   l o o p . \n \n 3 .   A n a l y t i c s :   I d e n t i f y i n g   a n o m a l i e s . \n \n F o r   t h e   f i r s t   t w o   s t e p s ,   w e   u t i l i z e   a   f i n e - t u n e d   Y o l o v 8 .   F o r   t h e   a n o m a l y   i d e n t i f i c a t i o n ,   w e   r e l y   o n \n \n a   f i n e - t u n e d   M o b i l e N e t v 3   m o d e l . \n \n W h y   w e   n e e d   T i p   T r a c k i n g   &   E x a m p l e s   o f   V i d e o   a n d   A n o m a l i e s \n \n \f S T R E A M   A I   1 \n \n 5 \n \n C h a l l e n g e s \n \n T h i s   s y s t e m   i s   d e s i g n e d   w i t h   s i m p l i c i t y   a n d   v e r s a t i l i t y   i n   m i n d .   O u r   g o a l   i s   t o   m a k e   i t \n \n r e a d i l y   a d o p t a b l e   f o r   a n y o n e ,   e n s u r i n g   i t   c a n   s e a m l e s s l y   i n t e g r a t e   w i t h   v a r i o u s   3 D   p r i n t e r s   i n \n \n d i v e r s e   s e t t i n g s .   T h e r e ' s   n o   n e e d   f o r   a   p r e c i s e   c a m e r a   p l a c e m e n t ,   g i v i n g   u s e r s   f l e x i b i l i t y .   T h e \n \n s y s t e m   a d j u s t s   t o   d i f f e r e n t   r e s o l u t i o n s ,   s i m p l i f y i n g   s e t u p   p r o c e s s e s . \n \n I t ' s   b u i l t   t o   w o r k   w i t h   a n y   p r i n t e r   t h a t   o p e r a t e s   o n   g - c o d e ,   e v e n   i f   t h e   g - c o d e \n \n i n t e r p r e t a t i o n s   v a r y .   W i t h   e d g e   d e p l o y m e n t   c a p a b i l i t i e s ,   i t   c a n   r u n   o n   a   R a s p b e r r y   P i ,   s t r e a m i n g \n \n i n   r e a l   t i m e .   G i v e n   t h a t   t h e   Y O L O   m o d e l   m a y   o c c a s i o n a l l y   e r r   i n   o b j e c t   d e t e c t i o n ,   o u r   s y s t e m   i s \n \n m a d e   t o   a c c o u n t   f o r   t h e s e   d i s c r e p a n c i e s .   M o r e o v e r ,   u s e r s   w i l l   b e n e f i t   f r o m   a   c l e a r   v i s u a l i z a t i o n \n \n o f   t h e   t i p   t r a c k i n g   p r o c e d u r e . \n \n G i v e n   t h e s e   c o n s t r a i n t s   a n d   r e q u i r e m e n t s ,   t h e   e n s u i n g   c h a l l e n g e s   a n d   o u r   t a i l o r e d \n \n s o l u t i o n s   w i l l   b e   e l a b o r a t e d   b e l o w . \n \n \f S T R E A M   A I   1 \n \n 6 \n \n M o d e l   T r a i n i n g \n \n h t t p s : / / g i t h u b . c o m / B r i a n P 8 7 0 1 / A n o m a l y _ C l a s s i f i c a t i o n \n \n O b j e c t   D e t e c t i o n   f o r   T i p   T r a c k i n g \n \n W e   u t i l i z e d   t h e   Y O L O v 8 s   m o d e l ,   a   p r o m i n e n t   o b j e c t   d e t e c t i o n   s y s t e m ,   a n d   f i n e - t u n e d   i t \n \n t h r o u g h   R o b o f l o w ,   a   p l a t f o r m   t h a t   s t r e a m l i n e s   t h e   c o m p u t e r   v i s i o n   d a t a   p r o c e s s . \n \n I n p u t   S i z e   D a t a s e t   S i z e   A u g m e n t a t i o n s   P r e c i s i o n   R e c a l l   M A P 5 0 \n \n 6 4 0 x 6 4 0 \n \n 2 4 0 7 \n \n 1 2 0 0 \n \n 0 . 9 6 7 \n \n 0 . 9 3 1 \n \n 0 . 9 6 8 \n \n W e   a d d e d   o n t o   o u r   d a t a s e t   a n d   r e t r a i n e d   m u l t i p l e   t i m e s ,   a i m i n g   t o   f i l l   i n   g a p s   a n d \n \n w e a k n e s s e s   o f   t h e   m o d e l ,   a d d i n g   n u l l   i m a g e s   a n d   w r o n g   d e t e c t i o n s   e t c .   W e   a p p l i e d \n \n a u g m e n t a t i o n s   o f   b l u r ,   n o i s e   a n d   b r i g h t n e s s . \n \n \f S T R E A M   A I   1 \n \n 7 \n \n O b j e c t   C l a s s i f i c a t i o n   f o r   A n o m a l i e s \n \n W e   t e s t e d   v a r i o u s   m o d e l s ,   s e t t i n g s ,   a n d   p r e p r o c e s s i n g   m e t h o d s   t o   f i n d   t h e   b e s t   m o d e l . \n \n O u r   d a t a   i s   l i m i t e d   b e c a u s e   o n l y   a   f e w   f r a m e s   i n   e a c h   v i d e o   s h o w   o v e r   o r   u n d e r   e x t r u s i o n , \n \n m a k i n g   d a t a   c o l l e c t i o n   s l o w .   H e r e ' s   o u r   d a t a   p i p e l i n e : \n \n E x a m i n i n g   t h e   p i p e l i n e :   I   d e v e l o p e d   s e v e r a l   G U I s   t o   a d d r e s s   t h e   i n i t i a l   d a t a   c o l l e c t i o n \n \n b o t t l e n e c k .   I   o p t e d   f o r   a n   8 5 x 8 5   c r o p   s i z e   a s   i t   b e s t   c a p t u r e d   m a t e r i a l   n e a r   t h e   e x t r u s i o n   t i p .   I \n \n u s e d   g r a y s c a l e   t o   r e d u c e   d i m e n s i o n s .   I n   s o m e   i m a g e s ,   e d g e s   a n d   m a t e r i a l   a r e   b a r e l y   v i s i b l e . \n \n T h e r e f o r e ,   a   f i l t e r   w a s   u s e d   i n   p r e p r o c e s s i n g   t o   e n h a n c e   t h e s e   c r u c i a l   e d g e s . \n \n I   i n i t i a l l y   a t t e m p t e d   a   f i l t e r   t h a t   s e g m e n t e d   t h e   i m a g e   i n t o \n \n d i s t i n c t   c o l o r   i n t e n s i t i e s ,   u s i n g   t h e   o r i g i n a l   i m a g e ' s   c o l o r   i n t e n s i t y \n \n s t a n d a r d   d e v i a t i o n .   W h i l e   s o m e w h a t   e f f e c t i v e ,   i t   m i s s e d   t h e \n \n s u b t l e s t   e d g e s . \n \n \f S T R E A M   A I   1 \n \n 8 \n \n D r a w i n g   i n s p i r a t i o n   f r o m   t h e   c o n v o l u t i o n a l   o p e r a t i o n s   i n   C N N s ,   I \n \n e x p e r i m e n t e d   w i t h   v a r i o u s   k e r n e l s   a n d   m a t r i x   f i l t e r   c o m b i n a t i o n s   o n   t h e \n \n i m a g e s .   H o w e v e r ,   n o   s i n g l e   c o m b i n a t i o n   c o n s i s t e n t l y   w o r k e d   a c r o s s   a l l \n \n i m a g e s .   I   t e s t e d   s t a n d a r d   f i l t e r s   s u c h   a s   t h e   N o n - l o c a l   M e a n s   D e n o i s i n g \n \n f r o m   c v 2   a n d   t h e   U n s h a r p   M a s k   f r o m   P I L .   Y e t ,   n e i t h e r   s u c c e e d e d   i n \n \n e n h a n c i n g   t h e   f a i n t   e d g e s . \n \n A n   o b s e r v a t i o n   w a s   m a d e ,   e a c h   i m a g e   f e a t u r e s   a   l i m i t e d   s e t   o f   o b j e c t s   a n d   h u e s :   m a t e r i a l , \n \n t i p ,   b e d ,   s h a d o w s ,   a n d   g l e a m .   E a c h   c o l o r   i n t e n s i t y   i n   t h e   i m a g e   u n i q u e l y   f o l l o w s   a   n o r m a l \n \n d i s t r i b u t i o n   w i t h   i t s   d i s t i n c t   m e a n   a n d   v a r i a n c e .   H e n c e ,   G a u s s i a n   M i x t u r e   M o d e l s   a r e   a p t   f o r   t h i s \n \n t a s k . \n \n \f S T R E A M   A I   1 \n \n 9 \n \n U s i n g   G a u s s i a n   M i x t u r e   M o d e l s ,   w e   c a n   s h a r p l y   d e f i n e   e v e n   t h e   s u b t l e s t   e d g e s .   I   t e s t e d \n \n v a r i o u s   c o m p o n e n t   n u m b e r s ,   i n d i c a t i n g   t h e   a s s u m e d   G a u s s i a n s   i n   t h e   m i x t u r e .   N e x t ,   w e \n \n p r i o r i t i z e   s e n s i t i v i t y   a r o u n d   b r i g h t e r   c o l o r s   s i n c e   i t ' s   c h a l l e n g i n g   t o   d i s c e r n   e d g e s   w h e n   m a t e r i a l \n \n a c c u m u l a t e s .   T h i s   f o c u s   i g n o r e s   t h e   d a r k e r   b e d ,   w h i c h   i s n ' t   o f   c o n c e r n .   T h i s   f i n a l   r e f i n e m e n t \n \n r e s u l t e d   i n   o u r   p r e p r o c e s s i n g   s t e p ,   d u b b e d   G M M S . \n \n T h r o u g h   e x p e r i m e n t a t i o n ,   G M M S 6   i s   f o u n d   t o   p e r f o r m   t h e   b e s t   w h e n   t r a i n i n g   m o d e l s . \n \n \f S T R E A M   A I   1 \n \n 1 0 \n \n B a c k   t o   e x a m i n i n g   t h e   p i p e l i n e ,   t h e \n \n i m a g e s   a r e   r e s i z e d   t o   2 2 4 x 2 2 4 ,   s u i t a b l e \n \n f o r   E f f i c i e n t N e t ,   M o b i l e N e t ,   a n d \n \n R e s N e t ,   w h i c h   h a v e   t r a i n e d   o n   t h e \n \n 2 2 4 x 2 2 4 - s i z e d   I m a g e N e t   d a t a s e t , \n \n m a k i n g   t h e m   a d e p t   a t   r e c o g n i z i n g \n \n f e a t u r e s   o f   t h i s   s c a l e .   P o s t - r e s i z i n g ,   t h e \n \n i m a g e s   u n d e r g o   a u g m e n t a t i o n   a n d   a r e   s p l i t   i n t o   t r a i n i n g ,   v a l i d a t i o n ,   a n d   t e s t   d a t a s e t s . \n \n D a t a s e t   S u m m a r y : \n \n U n d e r :   4 3 6   i m a g e s   |   N o r m a l :   4 6 3   i m a g e s   |   O v e r :   4 4 6   i m a g e s \n \n E x t r u s i o n   C l a s s i f i c a t i o n   D a t a s e t \n \n A u g m e n t a t i o n s   f o r   e a c h   c l a s s   i n c l u d e   r o t a t i o n s ,   b r i g h t n e s s ,   s a t u r a t i o n ,   c o n t r a s t ,   h u e \n \n a d j u s t m e n t s ,   a n d   f l i p s   ( b o t h   h o r i z o n t a l   a n d   v e r t i c a l )   c r e a t i n g   6 0   n e w   i m a g e s   p e r   c l a s s .   T h e   d a t a \n \n d i s t r i b u t i o n   i s   7 5 %   f o r   t r a i n i n g ,   1 5 %   f o r   v a l i d a t i o n ,   a n d   1 0 %   f o r   t e s t i n g . \n \n T r a i n i n g   S u m m a r y : \n \n F i n e   t u n i n g ,   m o d e l   c h e c k p o i n t i n g ,   4   b a t c h e s ,   ~ 1 2   e p o c h s ,   l e a r n i n g   r a t e   d e c a y ,   e a r l y   s t o p p i n g . \n \n l e a r n i n g _ r a t e = 0 . 0 1 ,   m o m e n t u m = 0 . 9 ,   s t e p _ s i z e = 5 ,   g a m m a = 0 . 1 \n \n A f t e r   t r i a l s   o f   v a r i o u s   m o d e l s ,   s e t t i n g s ,   h y p e r p a r a m e t e r s   a n d   p r e p r o c e s s i n g   s t e p s ,   t h e \n \n c o m b i n a t i o n   o f   t h e   d e s c r i b e d   p r e p r o c e s s i n g   a n d   t r a i n i n g   p r o c e d u r e s   o u t l i n e d   a b o v e   y i e l d e d   t h e \n \n h i g h e s t   a c c u r a c y .   T h e   s t a n d o u t   w a s   a   f i n e   t u n e d   M o b i l e N e t v 3 L a r g e   m o d e l ,   w h i c h   n o t   o n l y   w o n \n \n i n   a c c u r a c y   b u t   a l s o   h a d   s u p e r i o r   c o m p u t a t i o n a l   e f f i c i e n c y . \n \n \f S T R E A M   A I   1 \n \n 1 1 \n \n T o   t h e   l e f t   s h o w s   o n e   o f   t h e \n \n c o m p a r i s o n   r o u n d s ,   w h e r e   w e   i d e n t i f i e d \n \n o u r   b e s t   p e r f o r m i n g   m o d e l .   H e r e   w e \n \n c o m p a r e   3   d i f f e r e n t   s e t t i n g s   a n d   t h e i r \n \n c o m b i n a t i o n s :   F i n e   t u n i n g \n \n M o b i l e N e t v 3 S m a l l   v s   M o b i l e N e t v 3 L a r g e . \n \n U s i n g   n o   G M M   p r e p r o c e s s i n g   v s   u s i n g \n \n G M M   p r e p r o c e s s i n g   v s   u s i n g   G M M S \n \n p r e p r o c e s s i n g   w i t h   a d d i t i o n a l   s e n s i t i v i t y \n \n t o   m o r e   i n t e n s e   p i x e l s . \n \n T h e   r e s u l t s   a r e   f i t t i n g ,   s h o w i n g   t h a t \n \n i n d e e d   u s i n g   G M M S   w i t h   e x t r a \n \n s e n s i t i v i t y   i s   b e n e f i c i a l ,   a n d   t r a i n i n g   t h e \n \n l a r g e r   m o d e l   r e s u l t s   i n   b e t t e r \n \n p e r f o r m a n c e .   T h e   t r a i n i n g   a n d   v a l i d a t i o n \n \n m e t r i c s   a r e   n e a r l y   i n d i s t i n g u i s h a b l e \n \n i n d i c a t i n g   t h a t   i t   i s   n o t   o v e r f i t t i n g . \n \n B e s t   p e r f o r m i n g   m o d e l   a c c u r a c y : \n \n 0 . 9 0 2 5 9 7 4 0 2 5 9 7 4 0 2 6 \n \n \f S T R E A M   A I   1 \n \n 1 2 \n \n I m p l e m e n t a t i o n \n \n I n   t h i s   s e c t i o n   I   w i l l   d e s c r i b e   t h e   d e s i g n   c h o i c e s   a n d   d e t a i l s   o f   t h e   s y s t e m . \n \n h t t p s : / / g i t h u b . c o m / B r i a n P 8 7 0 1 / S T R E A M . A I \n \n I n p u t \n \n 1 .   G - C o d e : \n \n I n s t r u c t i o n   s e t   f o r   3 D   p r i n t . \n S i m p l e   a n d   s e q u e n t i a l :   i t   r u n s   f r o m   s t a r t   t o   f i n i s h . \n S p e c i f i e s   t h e   t o p   s p e e d   a t   a n y   m o m e n t   a n d   p r o v i d e s   m o v e m e n t   c o o r d i n a t e s . \n \n - \n - \n - \n -   T h e   p r i n t e r   m o v e s   d i r e c t l y   f r o m   o n e   p o i n t   t o   t h e   n e x t   w i t h o u t   d e v i a t i o n . \n \n 2 .   V i d e o   S t r e a m : \n \n -   L i v e   f o o t a g e   o f   t h e   3 D   p r i n t e r   b e d . \n -   T h e   a n g l e   o f   t h e   c a m e r a   r e l a t i v e   t o   t h e   p r i n t e r   i s   e x p e c t e d   t o   b e   r o u g h l y   c o n s i s t e n t . \n \n 3 .   S i g n a l   S t r e a m : \n \n -   M a n a g e d   b y   t h e   S K R   b o a r d ,   w h i c h   c o n t r o l s   t h e   3 D   p r i n t e r . \n -   A   h a r d w a r e   c o n n e c t i o n   l i n k s   t h e   S K R   b o a r d   t o   o u r   R a s p b e r r y   P i . \n -   T h e   R a s p b e r r y   P i   g e t s   a   s i g n a l   e v e r y   t i m e   a   m o v e m e n t   c o m p l e t e s .   T h i s   i s   t h e   s i g n a l ,   a n d \n \n c o n s i s t s   o f   t h e   b e d   p o s i t i o n   a n d   t i m e .   [ t i m e ,   x ,   y ,   z ] \n \n -   T h i s   d a t a   s t r e a m s   i n   r e a l   t i m e ,   p a r a l l e l   t o   t h e   p r i n t i n g   p r o c e s s . \n \n I n i t i a l i z a t i o n \n \n F i r s t   a n d   f o r e m o s t ,   t h e   s y s t e m ,   s t a r t i n g   i n   t h e   m a i n   T r a c k e r   T h r e a d ,   b e g i n s   r e c e i v i n g   a n d \n \n r o u t i n g   f r a m e s   a n d   s i g n a l s   t h r o u g h   t h e i r   c o r r e s p o n d i n g   r o u t e r s .   T h e s e   r o u t e r s   w i l l   p a s s   t h e   d a t a \n \n t o   t h e   c o r r e c t   o b j e c t s   a n d   t h r e a d s   t h r o u g h   t h e   s h a r e d   g l o b a l   v a r i a b l e   s p a c e   b a s e d   o n   t h e   c u r r e n t \n \n s t a t e   o f   t h e   s y s t e m   ( I n i t i a l i z i n g   o r   T r a c k i n g ) . \n \n \f S T R E A M   A I   1 \n \n 1 3 \n \n T h e   d u t y   o f   t h e   i n i t i a l i z a t i o n   p h a s e   i s   t o   a u t o n o m o u s l y   a d a p t   t o   t h e   c a m e r a   p o s i t i o n .   I t \n \n p r i m a r i l y   p e r f o r m s   t h e s e   t a s k s : \n \n 1 .   M a k e   i n i t i a l   p r e d i c t i o n s : \n \n a .   T o   t r a n s f o r m   G - c o d e   i n t o   p r e d i c t i o n s   f o r   e v e r y \n \n m o m e n t   i n   t i m e ,   w e   f i r s t   b r e a k   d o w n   t h e   G - c o d e \n i n t o   a   s e q u e n c e   o f   m o t i o n   i n s t r u c t i o n s   b e t w e e n \n c o r n e r s   ( o r   w a y p o i n t s )   w h e r e   t h e r e   a r e   c h a n g e s   i n \n d i r e c t i o n   o r   s p e e d . \n \n b .   F o r   e a c h   o f   t h e s e   m o t i o n s ,   w e   c a l c u l a t e   t h e   o p t i m a l \n s p e e d   p r o f i l e   c o n s i d e r i n g   c o n s t r a i n t s   l i k e   m a x i m u m \n s p e e d s   a n d   a c c e l e r a t i o n .   W e   n e e d   t o   l o o k   a t   t h e   f u t u r e   2   m o v e s   a s   w e l l ,   t o \n d e t e r m i n e   w h a t   t h e   f i n a l   s p e e d   a t   t h e   e n d   o f   t h e   m o t i o n   s h o u l d   b e . \n \n c .   G i v e n   t h e   o p t i m a l   s p e e d   p r o f i l e s   f o r   e a c h   m o t i o n ,   w e \u2019 l l   c a l c u l a t e   h o w   l o n g   e a c h \n m o v e   s h o u l d   t a k e   u s i n g   k i n e m a t i c s .   O n e   p r o b l e m   c a n n o t   b e   s o l v e d   a n a l y t i c a l l y , \n a n d   n e e d s   t o   b e   s o l v e d   i t e r a t i v e l y .   T h e   i t e r a t i v e   m e t h o d   t w e a k s   t h e   d i s t a n c e   o v e r \n w h i c h   w e   a c c e l e r a t e   o r   d e c e l e r a t e ,   a t t e m p t i n g   t o   c o n v e r g e   o n   t h e   d e s i r e d   f i n a l \n s p e e d   w i t h o u t   e x c e e d i n g   t h e   m a c h i n e ' s   c o n s t r a i n t s . \n \n d .   A f t e r   c o m p u t i n g   t h e   s p e e d   p r o f i l e ,   t h e   s c r i p t   t h e n   d i v i d e s   t h e   m o t i o n   i n t o   f r a m e s \n ( b a s e d   o n   t h e   g i v e n   f r a m e s - p e r - s e c o n d   p a r a m e t e r )   a n d   p r e d i c t s   t h e   p o s i t i o n   a n d \n a n g l e   o f   t h e   m a c h i n e ' s   t i p   f o r   e a c h   f r a m e .   B y   t h e   e n d   o f   t h i s   p r o c e s s ,   w e   h a v e   a \n d e t a i l e d   a c c o u n t   o f   w h e r e   t h e   m a c h i n e   s h o u l d   b e   a n d   a t   w h a t   a n g l e   f o r   e v e r y \n s i n g l e   f r a m e   o f   t h e   m o t i o n . \n \n 2 .   D e r i v e   m i l l i m e t e r   t o   p i x e l   r a t i o : \n \n F o r   r o b u s t n e s s ,   t h e   s y s t e m   w i l l   a u t o n o m o u s l y   m a p   b e d   p r e d i c t i o n s   t o   s c r e e n \n p r e d i c t i o n s   w i t h o u t   r e q u i r i n g   u s e r s   t o   e n t e r   a n y   c o n f i g u r i n g   v a r i a b l e s .   B y   w a i t i n g \n f o r   t w o   s i g n a l s   a n d   o b s e r v i n g   t h e i r   m i l l i m e t e r   d i s p a r i t y   o n   t h e   b e d ,   w e   c a n \n c o r r e l a t e   t h i s   w i t h   t h e   p i x e l   d i f f e r e n c e   Y O L O   d e t e c t s ,   f a c i l i t a t i n g   t h e   n e c e s s a r y \n c o n v e r s i o n .   W e   s l i d e   a   6 4 0 x 6 4 0   w i n d o w   a c r o s s   t h e   i m a g e   t o   r u n   Y O L O . \n \n 3 .   S y n c h r o n i z e   p r e d i c t i o n s   w i t h   v i d e o : \n \n T o   i n i t i a t e   t h e   p r o c e s s ,   w e   a l i g n   o u r   s c r e e n   p r e d i c t i o n s   u s i n g   t h e   t i m e s t a m p   o f   t h e \n f i r s t   Y O L O - s i g n a l   i n f e r e n c e   f r o m   t h e   v i d e o . \n \n \f S T R E A M   A I   1 \n \n 1 4 \n \n P r e d i c t i o n s   a r e   i m p e r f e c t   d u e   t o   a c c u m u l a t e d   s m a l l   t r a c k i n g   e r r o r s .   R e a s o n s   i n c l u d e : \n \n 1 .   M e r l i n   f i r m w a r e   i n   o u r   3 D   p r i n t e r   e m p l o y s   B e z i e t \u2019 s \n \n 6 t h   o r d e r   f u n c t i o n   f o r   m o v e m e n t   e x t r a p o l a t i o n ,   w h i l e \n o u r   a l g o r i t h m   u s e s   a   s i m p l e r   2 n d   o r d e r   f u n c t i o n . \n H o w e v e r ,   o u r   m o d e l   c o n s i d e r s   a c c e l e r a t i o n . \n \n 2 .   S o m e   p r i n t e r s   a d j u s t   m o v e m e n t   f a c t o r s ,   l i k e   s p e e d \n \n a n d   a c c e l e r a t i o n . \n \n 3 .   M i n o r   d e v i a t i o n s   a r i s e   w h e n   t h e   p r i n t e r   h a l t s . \n 4 .   M e r l i n   F i r m w a r e   h a s   i n t r i c a t e   m e c h a n i s m s   f o r   g c o d e \n \n i n t e r p r e t a t i o n . \n \n I n s t e a d   o f   r e p l i c a t i n g   M e r l i n ' s   e x a c t   a p p r o a c h ,   w e   d e s i g n e d   a   s t r a i g h t f o r w a r d   i n t e r p r e t e r   w i t h \n r e a l - t i m e   e r r o r   f e e d b a c k   l o o p s ,   e n h a n c i n g   s y s t e m   r o b u s t n e s s   a n d   c o m p a t i b i l i t y   w i t h   v a r i o u s   3 D \n p r i n t e r s . \n \n E r r o r   C o r r e c t i o n \n \n P r e d i c t i o n s   f a c e   t w o   e r r o r   t y p e s :   T e m p o r a l   a n d   S p a t i a l ,   a d d r e s s e d   b y   t h e   E r r o r C o r r e c t i o n   t h r e a d . \n \n 1 .   S p a t i a l   E r r o r :   M a i n l y   a r i s e s   f r o m   i n c o r r e c t   r a t i o s   o r   Y O L O   i n f e r e n c e .   T h e s e   a r e   r a r e ,   t y p i c a l l y \n \n m i n o r   i n i t i a l   t r a c k i n g   d e v i a t i o n s .   F o r   d e m o n s t r a t i o n ,   a n   e x a g g e r a t e d   e r r o r   w a s   i n t e n t i o n a l l y \n \n i n t r o d u c e d   i n   t h e   S p a t i a l   E r r o r   C o r r e c t i o n   v i d e o . \n \n M e a s u r e \n \n D e t e c t \n \n C o r r e c t \n \n E v e r y t i m e   a   s i g n a l   a r r i v e s ,   w e   r u n \n Y O L O   t o   s e e   w h e r e   t h e   t i p   i s .   W e \n c o m p a r e   t h i s   t o   o u r   p r e d i c t i o n   a n d \n m e a s u r e   t h e   d i f f e r e n c e   i n   p i x e l s . \n \n W h e n   t h e r e   i s   a   c o n s i s t e n t   s p a t i a l   e r r o r \n o f   t h e   s a m e   m a g n i t u d e   a n d   d i r e c t i o n   f o r \n s u f f i c i e n t   t i m e . \n \n S h i f t   a l l \n p r e d i c t i o n s   b y \n s p a t i a l   e r r o r . \n \n 2 .   T e m p o r a l   E r r o r :   T h i s   w a s   a   s i g n i f i c a n t   c h a l l e n g e .   W i t h o u t   i n p u t   s i g n a l s ,   g a u g i n g   t e m p o r a l \n \n e r r o r   w a s   t o u g h .   T h e   u l t i m a t e   s o l u t i o n   s i m p l y   i n v o l v e d   i m m e d i a t e   m e a s u r e m e n t   a n d   c o r r e c t i o n \n \n \f S T R E A M   A I   1 \n \n 1 5 \n \n o f   t e m p o r a l   d i s c r e p a n c i e s .   T h i s   e r r o r   s t e m s   f r o m   v a r y i n g   i n t e r p r e t a t i o n s   o f   g c o d e ,   e s p e c i a l l y \n \n d i f f e r i n g   a c c e l e r a t i o n s . \n \n M e a s u r e \n \n D e t e c t \n \n E v e r y t i m e   a   s i g n a l   a r r i v e s ,   w e \n c o m p a r e   t h e   t i m e   o f   t h e   s i g n a l , \n c o m p a r e d   t o   o u r   p r e d i c t i o n s .   W e \n m e a s u r e   e r r o r   i n   s e c o n d s . \n \n W e   d o n \u2019 t   l o o k   f o r   a n y   c o n s i s t e n t \n e r r o r s .   W h e n   t h e r e   i s   a n   e r r o r ,   w e \n s i m p l y   p e r f o r m   a   c o r r e c t i o n . \n \n C o r r e c t \n \n P a u s e   o r   s k i p \n a h e a d   t o   r e a l i g n . \n \n A n a l y t i c s \n \n I n c o r p o r a t i n g   a   r e a l - t i m e   f e e d b a c k   l o o p   ( a u t o n o m o u s   c o r r e c t i o n s )   r e s u l t s   i n   n e a r - f l a w l e s s   t i p \n \n t r a c k i n g .   U s i n g   Y O L O   i s n ' t   c o n s t a n t ;   i t ' s   e m p l o y e d   d u r i n g   s i g n a l s   a n d   e r r o r   m e a s u r e m e n t s .   T h i s \n \n e n a b l e s   e f f i c i e n t   p i n p o i n t i n g   o f   t h e   r e c e n t l y   e x t r u d e d   m a t e r i a l   a r o u n d   t h e   t i p \u2014 o u r   t a r g e t .   G i v e n \n \n o u r   k n o w l e d g e   f r o m   t h e   g c o d e   a b o u t   t h e   p r i n t e r ' s   m o v e m e n t   d i r e c t i o n ,   w e   c r o p   a c c o r d i n g l y .   T h i s \n \n c r o p p e d   i m a g e   t h e n   e n t e r s   o u r   d a t a   p r e p r o c e s s i n g   p i p e l i n e   a n d   u n d e r g o e s   c l a s s i f i c a t i o n   b y   t h e \n \n t a i l o r e d   M o b i l e N e t   m o d e l ,   d e t e r m i n i n g   w h e t h e r   e x t r u s i o n   i s   u n d e r ,   n o r m a l ,   o r   o v e r . \n \n \f S T R E A M   A I   1 \n \n 1 6 \n \n M e t r i c s \n \n T o   e v a l u a t e   o u r   s y s t e m ,   w e ' l l   f o c u s   o n   i t s   c o m p u t a t i o n a l   e f f i c i e n c y ,   w h i l e   t h e   a c c u r a c y   i s \n \n d e r i v e d   f r o m   t h e   M o b i l e N e t   m o d e l ' s   t e s t   p e r f o r m a n c e .   W e ' l l   a s s e s s : \n \n -   R A M   U s a g e :   W e ' l l   m o n i t o r   R A M   c o n s u m p t i o n   o v e r   t i m e . \n - \n \n I n f e r e n c e   I n t e r v a l s :   G i v e n   t h e   a d a p t i v e   n a t u r e   o f   o u r   M o b i l e N e t   m o d e l ,   o b s e r v i n g   t h e \n i n t e r v a l s   b e t w e e n   s u c c e s s i v e   i n f e r e n c e s   p r o v i d e s   a   m e a s u r e   o f   s y s t e m   s p e e d .   T h e   m o d e l \n r e f r a i n s   f r o m   i n f e r r i n g   w i t h   a   f u l l   b u f f e r . \n \n T h e s e   m e t r i c s   w i l l   b e   g a u g e d   o n   b o t h   m y   d e v i c e   a n d   a   R a s p b e r r y   P i   4 ,   w i t h   r e s p e c t i v e \n \n s p e c i f i c a t i o n s   p r o v i d e d   b e l o w . \n \n D e p l o y m e n t   t o   R a s p b e r r y   P i \n \n R a s p b e r r y   P i   M e t r i c s \n \n F u t u r e   I m p r o v e m e n t s \n \n \f S T R E A M   A I   1 \n \n 1 7 \n \n 1 .   P r o c e s s o r   ( C h i p s e t )   D e t a i l s : \n \n -   M o d e l :   A p p l e   M 1   P r o \n -   T o t a l   N u m b e r   o f   C o r e s :   1 4 \n \n 2 .   M e m o r y : \n \n -   S i z e :   1 6   G B \n -   T y p e :   L P D D R 5 \n -   M a n u f a c t u r e r :   H y n i x \n \n 3 .   G r a p h i c s / G P U : \n \n -   C h i p s e t   M o d e l :   A p p l e   M 1   P r o \n -   M e t a l   S u p p o r t :   M e t a l   3 \n \n 4 .   D i s p l a y : \n \n -   T y p e :   B u i l t - i n   L i q u i d   R e t i n a   X D R   D i s p l a y \n -   R e s o l u t i o n :   3 0 2 4   x   1 9 6 4   R e t i n a \n \n 5 .   S t o r a g e : \n \n -   T y p e :   S S D   ( A P P L E   S S D   A P 0 5 1 2 R ) \n -   C a p a c i t y :   5 0 0 . 2 8   G B \n -   P r o t o c o l :   A p p l e   F a b r i c   ( i n d i c a t i n g   i t ' s   a n   N V M e   S S D ) \n -   F r e e   S p a c e :   1 2 0 . 4   G B \n \n 6 .   F i l e   S y s t e m : \n -   T y p e :   A P F S \n \n h t t p s : / / q e n g i n e e r i n g . e u / i n s t a l l - o p e n c v - o n - r a s p b e r r y - p i . h t m l \n h t t p s : / / q e n g i n e e r i n g . e u / i n s t a l l - p y t o r c h - o n - r a s p b e r r y - p i - 4 . h t m l \n h t t p s : / / g i s t . g i t h u b . c o m / w e n i g / 8 b a b 8 8 d e d e 5 c 8 3 8 6 6 0 d d 0 5 b 8 e 5 b 2 e 2 3 b \n h t t p s : / / o n n x r u n t i m e . a i / d o c s / t u t o r i a l s / i o t - e d g e / r a s p - p i - c v . h t m l \n \n \f",
    "data/library/STREAM_Slides.txt": "STREAM AI Team Dr. Sun\u2019s Zoom Link: https://zoom.us/j/3114058196\nMeetings Fridays @10:00 AM\nHongyue Sun: hongyuesun@uga.edu\nBrian Przezdziecki: bprzezdz@buffalo.edu I\u2019ve identified the real challenge:\nWhen I want to search for something in my textbook, I merely need to look\nat the table of contents. This \u201ctable of contents\u201d works because:\n- I understand the semantic meaning of all the keywords and how they\nrelate to my query in mind.\n- The text is rigorously made with an organization of ideas in mind.\nOn the initial forward pass in constructing the summarization tree, there is\nno problem. But at what points do branches need to be cut off and\nrelocated?\nTree Cleanup: O(num_of_nodes^2)\nWorst Case Num of Nodes: O(log(num_of_chunks))\nTree Cleanup with respect to num_of_chunks:\nUltimate Unit of Cost is num_api_calls 9/29/23-10/6/23\nLangchain docs\nOpen Interpreter group\nGPT3.5 disability 9/22/23-9/29/23\n\u25cf Implement first rendition of knowledge graph construction on GCP cloud\nfunctions\n\u25cf Create test dataset, and run KG construction on it\n\u25cf Read Langchain docs and open-interpreter code Topk & Topp links for Brian\nhttps://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-t\nemperature/\nhttps://huggingface.co/blog/how-to-generate\nhttps://docs.cohere.com/docs/controlling-generation-with-top-k-top-p Local LLAMA\nI downloaded LLAMA 70B chat and 7B chat locally. 70B chat takes up almost 140\nGB. I used Andrej Kaparthy\u2019s LLAMA pure C++ inference repo. Anything 13+\nbillion parameters doesn't work because of integer flow in pointer arithmetic, which\nis yet to be fixed, and second, even if it were fixed, this repo is doing float32\ninference right now, so it would be fairly unusably slow. The 7B LLAMA model runs\nat 30 tokens/sec on my Macbook M1 Pro. For this reason, I\u2019ll opt to work entirely\nin the cloud.\nPerformance metrics can be changed to by token usage and model throughput. Claude2 vs GPT4\nOpenAI <-> Microsoft\nDeepmind <-> Google\nAnthropic <-> Amazon\nxAi <-> Tesla, X\nMetaAI <-> Meta\nAs of now the closest competitor to ChatGPT appears to be Claude2 from\nAnthropic. It has a max context length of 100,000 tokens, but it hallucinates more.\nGPT4 might be dethroned by Gemini, from Deepmind, said to come in December. Another agent\nThis project has a much simpler codebase, and runs directly and interacts with\nyour local terminal. It can interact with the chrome browser, file systems and more.\nOpen Interpreter\nGPT-engineer is not good. The ones of interest so far are:\n- Open Interpreter\n- Langchain\n- AutoGPT Multimodality in ChatGPT in 2 weeks\nGPT-4V (GPT-4 Vision)\nThis twitter thread shows a bunch of examples of it:\nhttps://twitter.com/youraimarketer/status/1706461715078975778\nLots of problems, but cool and shows the direction we are moving in\u2026 so fast. Data\nFirestore for persistent storage of graphs as JSONs\nCloud Storage for storing objects and documents as key-value   9/15/23-9/22/23\n\u25cf Read paper: \u2018Consciousness in Artificial Intelligence: Insights from the\nScience of Consciousness\u2019\n\u25cf Explore open source GPT projects Consciousness in Artificial Intelligence\n\u201cOur analysis suggests that no current AI systems are conscious, but also\nsuggests that there are no obvious technical barriers to building AI systems which\nsatisfy these indicators.\u201d\n\u201cThe main alternative to this approach is to use behavioural tests for\nconsciousness, but this method is unreliable because AI systems can be trained to\nmimic human behaviours while working in very different ways.\u201d GPT-Engineer\nHave GPT create an entire codebase for you using chain of thought, different\nGPTs and more. AutoGPT\nAutoGPT is an open source project that takes GPT and allows it to act as an\nagent. Give it a goal, however abstract, and AutoGPT will break the goal into\nsteps, plan, think, reason, criticize and share everything with you. It can go browse\nthe internet, try different solutions and get up to date, and always seeks your\napproval. It will even argue with you if something is wrong. It can analyze\nwebsites, save the data and analyze the results.\nYoutube video talking about what AutoGPT and GPT-Engineer can do.\nIt's all open source BookGPT\nGive GPT a book and have a conversation with it\nThe open source code for Book-GPT\nUses Pinecone for vector embeddings and OpenAI for the language model. Both\nare paid and you need to use your own API key\nThere are open source versions of these. Although GPT-4 and Pinecone are more\npowerful Future Plans\nThere\u2019s been a lot built with GPT already. Before I build anything myself, I want to\ndive into these open source projects, research papers and learn more about how\nothers are building, what works and what\u2019s already been made. This is super cool\nstuff.\nA good thing for me is, given how much time I\u2019ve spent using the base\nfoundational models, I have a very good understanding of the limitations and\ncapabilities of these models. So time to dive in and see how these people made\nthese GPT agents!\nUnfortunately, I am going to need to spend my own money to use the OpenAI API.\nI hope it won\u2019t be too expensive. Using LLM projects\nFrom Andrej Kaparthy, the famous FSD lead at Tesla and currently at OpenAI:\nhttps://github.com/karpathy/nanoGPT GPT-2\nhttps://github.com/karpathy/llama2.c LLAMA2 in C no dependencies\nhttps://lambdalabs.com/ - Good priced Nvidia H100 GPUs on cloud\nhttps://github.com/abetlen/llama-cpp-python LLAMA2 python no dependencies\nhttps://github.com/shawwn/llama-dl LLAMA weight download\nhttps://github.com/facebookresearch/llama-recipes LLAMA examples\nhttps://github.com/cocktailpeanut/dalai LLAMA/ALPACA Run\nhttps://github.com/Paitesanshi/LLM-Agent-Survey\nhttps://lilianweng.github.io/posts/2023-06-23-agent/ 9/8/23-9/15/23\n\u25cf KG, Tree of Thoughts, Reasoning, Vector Embeddings\n\u25cf Migrate pytorch to ONNX\n\u25cf Write onnx run scripts for MobileNet\n\u25cf Make RaspberryPi branch with new venv and ncnn\n\u25cf Migrate onnx to ncnn\n\u25cf Download opencv from source on raspberry pi\n\u25cf Download ncnn from source\n\u25cf Run system on raspberry pi Raspberry Pi Optimizations\n- Rather than using Pytorch, I will migrate all models to an ONNX format. Once saved as an ONNX model, we can\nimport it into an ARM-friendly C++ framework such as ncnn or MNN. It will speed up our model considerably.\n- This means we don\u2019t need Pytorch, Torch or Onnxruntime. This also allows us to save SSD and RAM. Pt -> ONNX & MobileNet ONNX Run with ChatGPT\nThis could of took hours. I ran into so many errors. With chatgpt, it took 40 minutes:\n- Load mobilenet model with changed final layer, add weights, convert to onnx with\ntorch using dummy input with same transforms.\n- To run new MobileNet onnx model create an ort.InferenceSession, apply transforms\nto images. In first index of outputs get logits, apply softmax and boom you get the\npredicted class. So easy with chatgpt.\nIn this scenario, whenever I ran into a bug I just copied and pasted it to ChatGPT. It\u2019s like\nan instant personal StackOverflow user. At one point, I did have to use my own reasoning\nto deduce that one bug was from the changed final layer, but once I mentioned it\nChatGPT recognized it. Because ChatGPT didn\u2019t have full context of my project it didn\u2019t\nknow I had changed the final layer.\nMy Conversation OpenAI Stronger Control\nLLMs produce unstructured output. False.\nOpenAI Function calling allows us to direct the model to invoke certain functions in\nany manner you wish, and produce a structured output of any format.\nAdditionally, decrease temperature hyperparameter for consistent and high\nconfidence output answers.\nEnsemble methods.\nThese are techniques to get better control over the model when using it in our\nsystem, and remove unpredictable behavior. New operations\nAs coders there are things we can use to build systems. We have arithmetic,\nconditionals loops. We\u2019ve developed functions and other things we can use like\nhashing, objects, etc.\nLLMs provide an entire new suite of possible operations. Here\u2019s some examples:\nClassification, comparisons, shallow reasoning, compression, extraction, self\nprompting and a variety of other simple tasks.\nThere are now specific things we can add to our system. We can create custom\nfinetuned injection models made for certain tasks. Example of a system\nThis is a very rough example of what I\nimagine. Very rough, there are many\nproblems and issues with this that I need to\nresolve. (I made this a while ago)\nBut essentially you see the idea of using\nthe \u2018classification\u2019 operation to organize\ndata here. I just wanted to provide an\nexample.\nDon\u2019t look too deeply into this, this is\nmostly garbage.\nAlso, I organized the google drive better:\nLLMKB Project\nThis stage of the project is very fun :)\n(Planning and research) 9/1/23-9/8/23\n\u25cf Fix initial horizontal bug\n\u25cf Collect speed and RAM metrics\n\u25cf Create long-form report\n\u25cf Read LLM papers\n\u25cf Download libraries from source on raspberry pi\n\u25cf Get streams on raspberry pi\n\u25cf Run system on raspberry pi Downloading libraries from source\nOnnxruntime, opencv, torch and pytorch all have to be downloaded in a very\narduous process on the raspberry pi. Longer form report\nI\u2019m not done with the report yet, as I\u2019ll measure more metrics, and need to get the\nsystem on the raspberry pi. But here is what I have so far. If you think anything\nneeds more clarification, or should be removed, or any other suggestions, let me\nknow and I will change it.\nReport LLM Papers\nI\u2019ve gathered more papers of interest. I am in the process of reading all of these.\nThese papers consist of knowledge graphs, reasoning graphs, vector\nembeddings, factuality, experiments, LLAMA, mathematical reasoning, new\nreinforcement learning techniques, self teaching and improvements, agents and\napplications to different financial, industrial and medical industries. I will read all of\nthese, and in particular find which ones are most relevant to this project. I plan to\nspend lots of time catching up on the most recent and up to date existing work\nbefore beginning designing and engineering my own system.\nLLM Papers Notes\nI will add all my notes while reading this paper into this doc. I will talk about the\npapers I read, my own ideas and rants and more in this doc. Upon reading all\npapers, learning more and defining the system clearly implementation may begin.\nHowever, I want to spend time really preparing for this. I care about this a lot, and\nreally want to also have this for myself throughout my life.\nLLM Notes Software things I\u2019ll need to build\nDocument and web scrapers\nVector embedding spaces\nSeries of LLAMA2, GPT3.5turbo and GPT4 models\nDatabase Computer vision and Language project\nThe last step of the computer vision project is really to just deploy the system to\nthe raspberry pi, and making some final optimizations to have it run in real time,\npossibly requiring network pruning, MobileNet fitting to raspberry pi or hardware\nspecific dependency changes. Pytorch has libraries and functions optimized for\nraspberry pis.\nFor now, I\u2019m going to begin working on both projects at once - closing and\nfinishing up the computer vision project, and beginning and getting ready for the\nNLP project. Very exciting. 8/25/23-9/1/23\n\u25cf Refactor threads into modules\n\u25cf Skip impossible predictions (When tip is covering material)\n\u25cf Optimize frame buffer in initialization stage (Can cause zsh hardware errors\nwhen RAM overflows)\n\u25cf Define metrics for metric-driven research\n\u25cf Measure diameter method\n\u25cf Build test runs to measure metrics\n\u25cf Collect metrics to measure metrics on different videos\n\u25cf Another idea: LLM for diagnosis/process training, how to integrate text and\nimages? Skip impossible predictions\nThis is an example of an impossible prediction, when the tip is blocking the view\nso we cannot see the recently extruded material. In this scenario, we simply do not\nattempt to measure or classify the extrusion.\nThe implementation simply consists of looking at the angle (direction of\nmovement)\nIf in the future, we can have another camera angle, this can be bypassed. On the endoscope\nI recall in the meeting it was mentioned that using the endoscope might be easier.\nHowever, the problem is the endoscopes shaking is unpredictable. The only way\nto locate the tip would be running YOLO more frequently which is more\ncomputationally expensive.\nUnless of course, we simply mounted (physically in the real world) the endoscope\nso it doesn't shake. Shaking of around 10 pixels in any direction is acceptable for\nclassification, but not for diameter measurement, at least with my current method.\nSo unless shaking can be addressed, the Instant 360 is better, because we don\u2019t\nneed to always use YOLO, but can rather try to predict where the top is based off\nthe gcode. Optimize frame buffer\nThe frame buffer: When in the initialization phase we need two signals where the\nhorizontal distance is greater than RATIO_INITIALIZATION_MIN_RANGE. I just\nset it to 5 mm for now. So we need to save the frames in a buffer when they\nstream in so we can look back on them for the initialization phase. When\ninitialization is done, we can discard the buffer. However, if initialization takes too\nlong, the buffer can take up too much memory.\nThus, simple solution: Keep 2 buffers. One buffer stores frames. Whenever a\nsignal arrives, find the corresponding frame, save to the second buffer, and delete\nany previous frames. When YOLO is performed on that frame in the second\nbuffer, we can once again discard that frame.\nUmm, this might seem confusing, but it\u2019s not important to understand, just a\nspecific optimization. cursor.so said to refactor\nCursor is a code editor like\nvscode, but with chatgpt built\ninto it. You can share projects\nof massive sizes with it and get\nfeedback, and it can help you\ncode much better than\nChatGPT does because it has\ncontext and can see you entire\nproject. On the left is a simple\nexample where Cursor gave\nme some refactoring\nsuggestions which I will do. Code refactor into classes\nAs cursor noted, it will be better to break up the threads into each individual\nmodule.\nAdditionally, I want Sahil to work on his own separate branch and be able to\nmerge my changes as they happen. However, for this to work smoothly, I\u2019ll need to\ngive him his own file to work in. So I\u2019ll just create an additional thread module he\ncan use which makes it easier for both of us, and allows him to develop locally and\nstill get my updates.\nBy the way, the indexing into a compressed\nknowledge base performed by cursor.so is similar\nto what I referred to for the LLM project next. Note to self: Warnings to fix in the future\nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at the same time.\nBoth libraries are known to be incompatible and this can cause random crashes or\ndeadlocks on Linux when loaded in the same Python program. Using threadpoolctl may\ncause crashes or deadlocks. For more information and possible workarounds, please\nsee https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\nwarnings.warn(msg, RuntimeWarning)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\nNeed to download onnxruntime from source on raspberry pi:\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html Update\nSummary of video: (At the end of the video I show how this looks in action)\nEvery 1/30th of a second a frame enters the system. It gets added to the video_queue.\nIf the time it takes to process frames is longer than 1/30th of a second, a backlog of frames will begin\nbuilding up. Since frames take up a lot of memory, this eventually leads to a zsh: hardware killed error.\nIn slide, Note to self: Warnings to fix in the future, there is an NNPack error. This error is causing\nMobileNet to run slower than it normally should. This causes a backlog.\nThus, the solution is that whenever there is a backlog to not run inference. Only when the video_queue is\nrecent and up to date, will we run inference.\nAnyway, in order to make real time adjustments to the tracker we want\nthe extrusion class of the most recently extruded material. Performance Metrics\n- Measure Accuracy:\nLabel video manually, and compare systems labels.\n- Speed:\nSee fps of classification.\n- RAM:\nGraph RAM usage throughout time Time Measurements\nGmms: 0.01 s\nMobilenetvsmall run: 0.05713069438934326\nLast time:\nmobilenet_v3_small: 0.022805473804473876 LLM Research Papers I will read\nFlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering\nGalactica: A Large Language Model for Science\nOpportunities and challenges of ChatGPT for design knowledge management\nChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future\nDirections Towards Knowledge Graph Chatbots\nITERATIVE ZERO-SHOT LLM PROMPTING FOR KNOWLEDGE GRAPH CONSTRUCTION\nLLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT\nLarge Language Models Struggle to Learn Long-Tail Knowledge Raspberry Pi 4 Issue\nI wrote the code to receive signals and video from the Instant360. One issue\nhowever:\nWhen I try to download the opencv-python package, because it needs to be\ninstalled from source it can take anywhere from 30 minutes to multiple hours. I\nbegan installation, but the raspberry pi eventually became burning to the touch.\nUpon googling, I found this could damage the Raspberry Pi.\nApparently, for high load tasks we should use a heat sink or cooling fan for the\nRaspberry Pi. 8/4-8/11\n\u25cf Onboard Sahil Repositories\nhttps://github.com/BrianP8701/STREAM.AI\nhttps://github.com/BrianP8701/Anomaly_Classification\nHere are the repositories again in their current version. I added the short report\nand a video presentation (Same as the one from zoom) to the README.\nI made setup easy with a conda environment and environment.yml. Anomaly Classification Coding Interface\nI made using the Anomaly Classification repository Onboarding Sahil\nI assigned Sahil his first task. To improve the MobileNet performance alongside\nthe tip tracking algorithm (One of the metrics for the \u201cmetric driven research\u201d). I\nshared with him all the google drives, and recorded a video for him detailing his\ntask and how to use the code:\nhttps://youtu.be/L-UFGaSTrxw 7/28-8/4\n\u25cf Github documentation\n\u25cf Complete report\n\u25cf Finish deploying to Raspberry Pi\n\u25cf Gather metrics and demo videos 7/21-7/28\n\u25cf Why do small models perform better?\n\u25cf Finish implementation!!!\n\u25cf Deploy on Raspberry Pi You mentioned last meeting my interpretation of the G-Code by the SKR board might be different than what I had\nimplemented. I did further investigating online and it seems that my implementation is correct and the potential\nreason for the acceleration errors is the following:\nPrinter Firmware Overrides: The firmware of many 3D printers has built-in limits for acceleration, jerk, and max speed.\nEven if the G-code requests a certain speed or acceleration, the firmware might cap it to a pre-defined limit. This is\nespecially true if the set values in the firmware are deemed safer or more reliable for the printer's hardware.\nG-code Acceleration Commands: While most of the time acceleration is defined in the firmware, some G-code might\ncontain commands to set acceleration for specific sections of the print. These commands can override the default\nacceleration values. Look out for M204 (set acceleration) and M205 (set jerk) commands in the G-code.\nPhysical Limitations: Sometimes, even if both the firmware and the G-code allow for a certain acceleration, the\nphysical characteristics of the printer (like mass of moving parts, motor capabilities, belt tension, etc.) might prevent it\nfrom reaching those values. For example, a printer with a heavy tool head might not accelerate as quickly as a lighter\none, even if the firmware settings are the same.\nSegmented Moves: Some slicers break down curves and diagonal moves into many tiny straight-line segments.\nWhen the printer processes these segments, it might not achieve the desired acceleration due to the frequent\ndirection changes.\nStart and Stop Points: The printer might be decelerating as it approaches the end of a line segment or the start of a\nnew one, especially if there's a change in direction. This can make the effective acceleration appear different from the\nset value. Kinematics Problem\nWe use the 4 kinematic equations to predict the movement of the tip of the tracker solely based off the\ngcode, which provides coordinates and speeds. At each line of gcode, we predict the time it takes to finish\nthat move. Without getting to deep into details, there is one scenario that cannot be solved analytically, but\nrather needs to be solved using numerical analysis or iterative methods.\nThe problem is as follows: Given a positive distance, acceleration, initial speed and final speed: Find the\nfastest time which you can travel the distance.\nSince we know the distance, I thought of the following iterative method:\nIf curr_speed equals final_speed, we split the distance in half.\nIf curr_speed is less than final_speed, we accelerate for more than half the distance.\nIf curr_speed is greater than final_speed, we accelerate for less than half the distance.\nWhat we can do is first try accelerating for the first half of the distance, then decelerating for the second\nhalf. Then we iterate the distance by which we accelerate to get closer to the final speed.\nWe iterate by 10% of the distance 10 times, then 1% of the distance 10 times, then 0.1% of the distance\n10 times. With this method, every 10 iterations results in an order of magnitude improvement in accuracy.\nThis is very computationally cheap. Spatial Error Implemented\n1. Run YOLO to compare where we think tip is to where it actually is. Add spatial\noffset to a list.\n2. When we have enough spatial offsets, make sure the spatial offsets are close\nenough together by finding the range. Then, make sure in the y case, that the\naverage spatial offset is large enough to be relevant.\n3. If conditions are met, make a spatial correction. Clear the previous list of\nspatial errors.\nI tested this by purposely introducing some spatial errors, and it proved to work. Temporal Graphs\nWhy are there two lines in the temporal offsets graph? Why does it increase, and then decrease?\nOkay, the two lines were an implementation error on my part. However, the \u201ctwo lines\u201d, or parts of the\ngraph with different slopes is correct. The first phase is that part of the print where it is going around and\nmaking some curves, and the second is the making of the cube in the center. This makes it clear that, the\nacceleration at each of these times are different.\nTemporal Offsets (frames) Slopes of Temporal Offsets Stdevs of Slopes\nvs. Time (frames) vs. Time (frames) vs. Time (frames) Deploying to Edge 7/14-7/21\n\u25cf Why do small models perform better?\n\u25cf Achieve acceptable performance on bubble classification models\n\u25cf Finish implementation\n\u25cf Run on video locally\n\u25cf Contact Dr. Zhou for real signal implementation\n\u25cf Implement real video stream\n\u25cf Deploy to edge   Interesting Details on Implementation\nThe main thread and all other threads run. I\u2019m not achieving true parallelism,\nmerely concurrency. Any processing of video frames must not hinder the rest of\nthe process, meaning it needs to take less than 1/30th of a second. I found\ndisplaying and saving the image could be very computationally expensive, with\ndisplaying being worst. I found a simple workaround: reduce resolution or frame\nrate of display. Yolo Update\nI found that the YOLO model was not accurate enough. The previous tip detection\ndataset was trained on images from the endoscope camera. I\u2019ve trained a new\nmodel, with 5,385 images (Including old data from previous models, and new\nones). A problem was that it sometimes found a tip when there was no tip. So I\nmade sure to add some null images in this dataset. Bubble Model\nBubble: 184 images | No Bubble: 184 images\nImages were cropped at 85x85. I resized all images to 224x224.\nI also created an additional dataset, applying the Gaussian Mixture Model +\nSensitivity filter.\nFor each class I created new images for each class using rotations, adjusting\nbrightness, saturation, contrast, hue, horizontal and vertical flips. I did 40\naugmentations per class.\nI split each dataset into 75% training set, 15% validation set and 10% testing\nset. Training\nGiven that we have found what parameters worked best, I followed the same:\nBatch size = 5\nResize to 224x224\nApply Gaussian Mixture Model Preprocessing\nI fine tuned a couple MobileNetv3 models, and immediately got performances\nabove 90%. My question now is, looking at the data, it looks like \u201cbubble\u201d is the\nsame as under extrusion. Before moving forward, is this acceptable? Yolo run: 0.22236160755157472 7/3-7/14\n\u25cf Achieved acceptable performance on classification model\n\u25cf Implemented real time, synchronized, self-correcting parser & tracker.\n\u25cf Integrated classification model to check for over or under extrusion.\n\u25cf Prepare to deploy to Raspberry Pi & use TinyYOLO New Set of Models\nDifferences:\n\u25cf More data\n\u25cf Only did fine tuning\n\u25cf Only used MobileNet\n\u25cf Only used images 224x224 Dataset descriptions\nUnder: 436 images | Normal: 463 images | Over: 446 images\nImages were cropped at 85x85. I resized all images to 224x224.\nI also created an additional dataset, applying the Gaussian Mixture Model +\nSensitivity filter.\nFor each class I created new images for each class using rotations, adjusting\nbrightness, saturation, contrast, hue, horizontal and vertical flips. I tried doing 60\naugmentations and 150 augmentations per class\nI split each dataset into 75% training set, 15% validation set and 10% testing\nset. Data for new models\nBatch size = 3 Batch size = 4 Batch size = 5\nBatch size = 2\nAugmentations per class = 150 Augmentations per class = 60 Augmentations per class = 60\nAugmentations per class = 60 6/26-7/3\n\u25cf Measure inference metrics for each model\n\u25cf Train next batch of models. Aim for 85% accuracy on test set\n\u25cf Update error detection and processing to work with corner updates\n\u25cf Collect all synchronization variables\n\u25cf Automate synchronization\n\u25cf Identify exact correction equation\n\u25cf Implement error correction Model Inference Efficiency\nI ran each model a 1000 times, and measured the average time in seconds:\n\u25cf resnet18: 0.05358100652694702\n\u25cf resnet152: 0.41191160917282105\n\u25cf mobilenet_v3_small: 0.022805473804473876\n\u25cf mobilenet_v3_large: 0.05056007623672485\n\u25cf efficientnet_v2_s: 0.21759283781051636\n\u25cf efficientnet_v2_l: 0.6030457210540772\nMobileNets are the fastest for inference, whereas EfficientNets take the longest.  More Detailed View of Initialization\n& Synchronization Phase\nI\u2019ve finished developing this\nphase. I\u2019ve made it so that the\nvideo and signals get\nstreamed in real time already\nto test locally. Some changes\nstill need to be made however:\n\u25cf Link to Instant360\n\u25cf Configure to work with\nactual signal in real time\nOnce this is done (and I finish\ncoding the next phase) this\nalgorithm will be ready to\ndeploy in real time.  New Set of Models\nDifferences:\n\u25cf Added test set\n\u25cf More data\n\u25cf More data augmentations\n\u25cf More epochs\nSee more detailed information on next slide. Dataset descriptions\nUnder: 247 images | Normal: 255 images | Over: 246 images\nImages were cropped at 50x50. I made additional datasets with 224x224 images\nbe resizing. I want to compare the performance of models trained on 50x50 vs\n224x224 sized images.\nI also created an additional dataset, taking the 50x50 images and 224x224 images\nand applying the Gaussian Mixture Model + Sensitivity filter.\nFor each class I created 30 new images for each class using rotations, and\nadjusting brightness, saturation, contrast and hue. I created 30 new images using\nhorizontal and vertical flips.\nI split each dataset into 70% training set, 20% validation set and 10% testing\nset. Training Comparisons\nI measured the means for different attributes\nacross my new set of trained models.\nComparing transfer learning to fine tuning:\ntest: ['transfer', 0.5094] vs ['finetune', 0.6043]\nval: [\u2018transfer', 0.5299] vs ['finetune', 0.6091]\nFine tuning is clearly better. Comparing MobileNet to ResNet:\nComparing MobileV3Small to MobileV3Large: test: ['mob', 0.5887] vs ['res', 0.5443]]\nval: ['mob', 0.5952] vs ['res', 0.5658]]\ntest: ['mob_s', 0.5726] vs [\u2018mob_l', 0.6047]\nMobileNet might be better.\nval: ['mob_s', 0.5866] vs ['mob_l', 0.6038]\nThe larger model seems to be better.\nComparing ResNet18 to ResNet152:\ntest: ['res18', 0.4972] vs ['res152', 0.5472]\nval: ['res18', 0.5223] vs ['res152', 0.5605]\nThe larger model seems to be better. Data Comparisons\nI tried applying (gmms to images sized 50) and\n(gmms to images sized 224, then resized to 50).\nComparing 50-50 to 224-50 : Comparing with and without GMMS (50x50):\ntest: ['50-50', 0.4800] vs ['224-50', 0.5153]\ntest: ['original', 0.5248] vs ['GMMS', 0.4976]\nval: ['50-50', 0.5151] vs ['224-50', 0.4944]\nval: ['original', 0.5215] vs ['GMMS', 0.5048]\nWhen to apply gmms, does not seem to matter.\nGMMS seems to be bad.\nComparing 50x50 to 224x224:\nComparing with and without GMMS (224x224):\ntest: ['50', 0.5067] vs ['224', 0.6340]\ntest: ['resize', 0.5997] vs ['GMMS', 0.6639]\nval: ['50', 0.5104] vs ['224', 0.6616]\nval: ['resize', 0.6397] vs ['GMMS', 0.6807]\nImages sized 224x224 are clearly better.\nGMMS seems to be good. Best Methods\nMethods that are clearly better:\n1. Use 224x224 images.\n2. Use fine tuning.\n3. Train for around 15 epochs.\nI lost the metrics for this, but I trained a set of models earlier this week. I trained 42\nmodels, each for 40 epochs. It was clear across each model that improvements\nstagnated at around 8-10 epochs. Interesting Data Points\nBest Model:\nmob_s_resize_finetune: [test_accuracy: 0.7925, val_accuracy: 0.8025]\nWorst Model:\nres18_gmms_actually50_finetune: [test_accuracy: 0.3868, val_accuracy: 0.4268]\nMoving forward:\nUsing what we\u2019ve found to be the best methods, let\u2019s train more models. I think if I\njust collect some more data we will be able to achieve above 85% accuracy.\nSee what images the models struggle on. Reshuffle data. 6/19-6/26\n\u25cf Collect more data\n\u25cf Make second round of datasets\n\u25cf Analyze metrics of models\n\u25cf Use GPU servers to train all models\n\u25cf Write inference code\n\u25cf Debug tracker\n\u25cf Refactor error detection and error processing code The process of collecting data\nLast time I collected data, I saved every\nframe from a video as a .jpg file on my\nSSD, manually identified the frames\nrelevant to each class, and looped through\nthe folders to retrieve these frames. This\nmethod was time-consuming and\ninefficient.\nTo streamline the process, I developed a\nnew graphical user interface (GUI). This\nGUI, implemented in the\nextract_frames_gui.py script, allows for a\nquicker review of a video and easier\nextraction of frames associated with each\nclass. This has significantly reduced the\ntime required for data collection.\nI also added several new methods:\nI faced some organizational challenges and my code\nrecursively remove duplicate images from\noverwrote several images, leading to confusion. As a\nfolder, reorder and format folder properly\nresult, I had to collect the data from scratch, a task that\nand combining multiple folders containing\nconsumed most of the day. I\u2019ve now more than doubled\nclasses into one folder.\nthe entire dataset. extract_frames_gui.py\nHere is a screenshot of the\ndescription of the tool I\nimplemented, that I mentioned last\nslide. ResNet\nResNet models are deep convolutional neural networks known for \"skip\nconnections,\" allowing gradients to flow through the network directly, mitigating the\nproblem of vanishing gradients, especially in very deep networks. This makes\nResNet models capable of being trained deeply. The numbers associated with\nResNet models indicate the depth of the network - ResNet18 has 18 layers, while\nResNet152 has 152 layers. The deeper the network, the better it can learn\ncomplex patterns, but at the cost of increased computational cost and risk of\noverfitting.\nResNet18: 42.72 MB\nResNet152: 222.7 MB MobileNet\nThe MobileNet architecture is made to be small and efficient, with a focus on mobile and\nembedded vision applications. It introduces depthwise separable convolutions to reduce\ncomputational complexity and model size.\nWith depthwise separable convolutions, rather than applying 3d filters (depth, width, length) to\nthe image, we initially apply \u201cdepthwise\u201d 2D matrices in each channel. Thereafter, a 1x1x3\n\u201cpointwise\u201d filter combining the channels. This is significantly cheaper (This specific operation is\nnow linear with respect to input size, as opposed to exponential).\nMobileNetV3 is very cool, introducing additional enhancements: Neural Architecture Search and\nNetAdapt algorithm. Instead of the researchers tweaking hyperparameters to find the best\narchitecture, they created an algorithm that would discover the best architecture for them.\nNetAdapt allows the model to \u201cautomatically adapt\u201d to any mobile device, which seems really\ncomplicated, and I didn\u2019t look into it (yet).\n'Small' and 'Large' refer to versions of the model with fewer or more parameters respectively,\nagain balancing between computational efficiency and model capacity.\nMobileNetv3small: 5.93 MB\nMobileNetv3large: 16.24 MB EfficientNet\nEfficientNets are made to provide a good trade-off between accuracy and computational\nefficiency. They introduce a new scaling method that uniformly scales all dimensions of\ndepth(number of layers, width(number of neurons per layer), resolution(size of input) of\nthe network. Instead of scaling these individually, they\u2019ve found some way to scale them\naltogether in a better way.\nThe authors first developed a baseline model called EfficientNet-B0. Then they used their\nuniform scaling method to systematically increase the depth, width, and resolution of this\nmodel, resulting in a family of models from EfficientNet-B1 to EfficientNet-B7, each\noffering higher accuracy but also more complexity.\nThe versions like 'v2-small' and 'v2-large' have to do with the scaling factor applied to the\nbase EfficientNet model. Larger versions have more layers or larger layers.\nThe specifics behind why this work are really interesting, and I want to get deeper into it.\nEfficientNetV2small: 77.85 MB\nEfficientNetV2large: 449.72 MB Dataset descriptions\nBubble: 43 images | No Bubble: 50 images\nUnder: 60 images | Normal: 61 images | Over: 37 images\nImages were cropped at 50x50. I made additional datasets with 224x224 images\nbe resizing and padding. I want to compare the performance of models trained\non resized vs padded images.\nI also created an additional dataset, taking the 50x50 images and applying the\nGaussian Mixture Model + Sensitivity filter.\nFor each class I created 10 new images for each class using rotations, and\nadjusting brightness, saturation, contrast and hue.\nI split each dataset into 20% validation set and 80% training set. Training\nIn fine-tuning, we continue to train all weights on our dataset, as we normally do.\nIn transfer learning, we also start with a pre-trained model. But this time, we keep\nmost of the model exactly the same as it was. We only change the final part that\nmakes the decisions (the \"classification layer\"). This is the only part of the model\nthat gets to learn from our data. The rest of the model remains unchanged.\nWe split the training set into 3 batches each epoch. After going through all the\nimages (one epoch), we backpropagate the error. Then we check the model's\nperformance using the validation set - this doesn't affect the model's learning but\nlets us see how it's doing. During the training, we keep a copy of the\nbest-performing model according to its scores on the validation set. This\napproach, called model checkpointing, ensures we always have the best version\nof our model according to how it performs on the validation set. Measured metrics\nAccuracy is measured on the validation set. Errors from the validation set never\nactually get back propagated to the model. Rather, after each epoch we only\nchoose to continue to train whichever model has performed best on the validation\nset so far.\nThe other metrics are measured every epoch, for the train and validation set. This\nlets us see if the model is overfitting to the training set, while not performing on the\nvalidation set.\nEach trained model is labeled as:\nThe following slides follow this format:\nmodel_dataset_finetune\n1. Small models + Transfer Learning\nor\n2. Large models + Transfer Learning\nmodel_dataset_transfer\n3. Small models + Fine Tuning\n4. Large models + Fine Tuning Where model and dataset are abbreviated. I\nshow these abbreviations in the next slide.\nEach slide will show metrics for all datasets. Model Model abbreviation Dataset Dataset Description\nAbbreviation\nResNet18 res18\nbubble Bubble/No Bubble Dataset, in its original 50x50\nResNet152 res152\nsize.\nEfficientNetv2small eff_s bubble_pad Bubble/No Bubble Dataset, but with padding to\nmake it 224x224\nEfficientNetv2large eff_l\nbubble_resize Bubble/No Bubble Dataset, but resized to 224x224\nMobileNetv3small mob_s\nclassification Under/Normal/Over Dataset, in its original 50x50\nMobileNetv3large mob_l\nsize.\npad Under/Normal/Over Dataset, but with padding to\nI trained each model with each dataset, once make it 224x224\nwith fine tuning and once with transfer learning.\nresize Under/Normal/Over Dataset, but resized to\n224x224\nA total of 84 models.\ngmms6 Under/Normal/Over Dataset in its 50x50 size, but\nwith Gaussian Mixture Model + Sensitivity filter. Smaller sized models, trained using transfer learning\nResnet18 MobileNet Small EfficientNet Small Largest sized models, trained using transfer learning\nResnet152 MobileNet Large EfficientNet Large Smaller sized models, trained using fine tuning\nResnet18 MobileNet Small EfficientNet Small Largest sized models, trained using fine tuning\nResnet152 MobileNet Large EfficientNet Large Bubble/No Bubble\nTransfer vs finetune\nFinetune Accuracy Transfer Learning Accuracy\nUnder/Normal/Over\neff_s_bubble_finetune 0.7083 eff_s_bubble_transfer 0.625\nFinetune Accuracy Transfer Learning Accuracy eff_s_bubble_pad_finetune 0.79166 eff_s_bubble_pad_transfer 0.625\neff_s_bubble_resize_finetune 0.79166 eff_s_bubble_resize_transfer 0.8333\neff_s_classification_finetune 0.5 eff_s_classification_transfer 0.575 eff_l_bubble_finetune 0.583 eff_l_bubble_transfer 0.708\neff_s_gmms6_finetune 0.55 eff_s_gmms6_transfer 0.5 eff_l_bubble_pad_finetune 0.666 eff_l_bubble_pad_transfer 0.75\neff_s_pad_finetune 0.4 eff_s_pad_transfer 0.625 eff_l_bubble_resize_finetune 0.7083 eff_l_bubble_resize_transfer 0.7916\neff_s_resize_finetune 0.475 eff_s_resize_transfer 0.525 res18_bubble_finetune 0.79166 res18_bubble_transfer 0.7083\neff_l_classification_finetune 0.45 eff_l_classification_transfer 0.525 res18_bubble_pad_finetune 0.7083 res18_bubble_pad_transfer 0.666\neff_l_gmms6_finetune 0.525 eff_l_gmms6_transfer 0.5 res18_bubble_resize_finetune 0.833 res18_bubble_resize_transfer 0.625\neff_l_pad_finetune 0.475 eff_l_pad_transfer 0.475 res152_bubble_finetune 0.666 res152_bubble_transfer 0.7083\neff_l_resize_finetune 0.65 eff_l_resize_transfer 0.575 res152_bubble_pad_finetune 0.875 res152_bubble_pad_transfer 0.5833\nres18_classification_finetune 0.475 res18_classification_transfer 0.675 res152_bubble_resize_finetune 0.79166 res152_bubble_resize_transfer 0.75\nres18_gmms6_finetune 0.425 res18_gmms6_transfer 0.525 mob_s_bubble_finetune 0.5833 mob_s_bubble_transfer 0.625\nres18_pad_finetune 0.45 res18_pad_transfer 0.5 mob_s_bubble_pad_finetune 0.75 mob_s_bubble_pad_transfer 0.6666\nres18_resize_finetune 0.475 res18_resize_transfer 0.625 mob_s_bubble_resize_finetune 0.75 mob_s_bubble_resize_transfer 0.7083\nres152_classification_finetune 0.5 res152_classification_transfer 0.55 mob_l_bubble_finetune 0.666 mob_l_bubble_transfer 0.625\nres152_gmms6_finetune 0.625 res152_gmms6_transfer 0.525 mob_l_bubble_pad_finetune 0.79166 mob_l_bubble_pad_transfer 0.625\nres152_pad_finetune 0.45 res152_pad_transfer 0.525 mob_l_bubble_resize_finetune 0.7083 mob_l_bubble_resize_transfer 0.7916\nres152_resize_finetune 0.425 res152_resize_transfer 0.5\nmob_s_classification_finetune 0.45 mob_s_classification_transfer 0.375 Average Accuracy: 0.73148 Average Accuracy 0.68981\nmob_s_gmms6_finetune 0.525 mob_s_gmms6_transfer 0.575\nmob_s_pad_finetune 0.475 mob_s_pad_transfer 0.45\nmob_s_resize_finetune 0.425 mob_s_resize_transfer 0.55\nmob_l_classification_finetune 0.5 mob_l_classification_transfer 0.6 Here I have collected the accuracies for each model I\nmob_l_gmms6_finetune 0.45 mob_l_gmms6_transfer 0.525\nhave trained. We can take a look to see if transfer\nmob_l_pad_finetune 0.525 mob_l_pad_transfer 0.55\nmob_l_resize_finetune 0.675 mob_l_resize_transfer 0.575 learning vs fine tuning seem to be definitively better.\nAverage accuracy: 0.4947916666 Average accuracy: 0.5385416666\nIt would appear, that neither are better. Metric Analysis\nWith the current accuracy of around 50%, it seems like we're hitting a roadblock.\nNo specific preprocessing or fine-tuning method is proving to be superior. The\nstagnant performance metrics after 10 epochs might hint that our model has\nreached its capacity with the given data.\nThings to try:\n\u25cf Cross-validation\n\u25cf Collect more data\n\u25cf Refine dataset\nI now have added around 50 new images to each class, so each class now has\n104 images. Refining my dataset\nIn my earlier work, I put images into 'over' or 'under' groups even if they only showed\na tiny bit of over or under extrusion. Now, I'm going to change that. I'll only put an\nimage in the 'over' group if the material is really getting wider. I'll only put it in the\n'under' group if it's clearly getting thinner. Some pictures I had put in 'over' or 'under'\nbefore will now go into the 'normal' group.\nI used to label these two pictures showed overextrusion, but I've changed my mind.\nI'm either going to move them to the 'normal' group or take them out completely,\nwhichever keeps the groups balanced. I only had to get rid of 5 pictures in all, but I\nhope this will make our results more accurate. 6/12 - 6/19\n\u25cf Train all classification models and provide metrics\n\u25cf Improve robustness of preprocessing step for low standard deviations\n\u25cf Optimize preprocessing step\n\u25cf Test virtual marker Training modules done\nI finished coding the training for models on Pytorch.\nBesides making a model for each dataset I mentioned earlier, I'll also\ntest fine tuning and transfer learning. Fine tuning starts with a\npre-existing model and trains the whole thing. Transfer learning also\nstarts with a pre-existing model, but only trains the last layer. Transfer\nlearning should work better because it avoids overfitting, but we'll\nhave to see.\nAlso, we need more data. I'll go through the new videos to get more. Purpose of Preprocessing\nOur preprocessing step does two things:\n1. It reduces noise and sharpens edges so we can see the most recently\nextruded material's edges and measure its diameter.\n2. It helps our trained models work better. Preprocessing can make a big\ndifference if you're dealing with small, low-quality, or noisy datasets. If the\nimages in the dataset have a lot of noise or if the objects are hard to see,\nreducing noise and sharpening edges can make the objects stand out more. Refresher of the current simplify algorithm\nThis slide is just to explain how the current preprocessing step works:\n1. Figure out the standard deviation of all the pixel intensities in the image. This tells us how much the\ncolors in the image vary.\n2. Using the standard deviation, we calculate a 'threshold'. This threshold decides which pixels are\nsimilar enough to group together. If the colors in the image are very different, the threshold will be\nlarger. If the colors are similar, the threshold will be smaller.\n3. If the standard deviation is below a certain point, we set the threshold to a fixed value.\n4. Find the brightest pixel in the image.\n5. Turn all pixels that fall within the threshold of the brightest pixel to maximum intensity, making them\nwhite.\n6. We adjust the range to include pixels that are slightly less bright, excluding the ones we just turned\nwhite. This is our new 'cutoff'.\n7. We repeat this process, finding the brightest pixel below our cutoff and adjusting all pixels within its\nthreshold to match its intensity. We do this until we've looked at and adjusted all pixels in the image.\nAfter all this, the image is simpler because similar pixels have the same intensity. This makes the edges\nstand out and reduces noise. Failings of the current preprocessing step\nWhen the image is more uniform in\nintensity, this algorithm can introduce more\nnoise.\nThis error shown to the right is clearly\nunacceptable. There bed is in the image,\nwhich leads the algorithm to calculate a\nhigher standard deviation. This leads the\nalgorithm to believe it doesn't need to be\nas sensitive, and thus ignores the faint\nline. How robust?\nLet's look at the images on the right. The edges\nare really faint. In the first image, the bottom line\nisn't even there.\nFirst, this means when we're measuring diameter,\nwe need to know when we can't get a\nmeasurement.\nSecond, for the last two images, the edges are\nthere but they're hard to see. Should we give up?\nOr can we figure out how to handle these images,\nespecially since they're common when layers\nbuild up? A question of balance\nWhat's more important - sharp edges or less noise? We need a balance. We have\nsome faint edges. Using the standard deviation doesn't always work well because\nthe bed or tip can be a dark color, which can make the algorithm unstable. We\ncould crop out the tip with accurate tip tracking, but the bed is still an issue.\nWe need to make sure we don't lose important information. Too much noise\nreduction can remove important details. Too much sharpening can highlight noise\nor make fake edges. But a bit of extra sharpening shouldn't hurt our diameter\nmeasurement algorithm.\nIn conclusion, we must prioritize avoiding relevant information loss. Cv2 and PIL Preprocessing\nNon-local Means Denoising algorithm from cv2\nUnsharp Mask from PIL\nI tried out the Non-local Means Denoising algorithm\nfrom cv2 and Unsharp Mask from PIL. They work\nalright after I played with the settings for a while. But\nwe still need something better to highlight faint edges.\nMaybe we can come up with a custom solution. New preprocessing step\nRight now, we treat each pixel as if it belongs to one\nnormal distribution. But maybe we can use a\nGaussian Mixture Model to separate our pixels into\nmultiple groups:\n\u25cf The tip\n\u25cf The tip's glow\nI remembered something from my textbook, 'Deep\n\u25cf The extruded material\nLearning'. It's about a thing called a Gaussian Mixture\n\u25cf Shadows\nModel. This model thinks data is made from different\n\u25cf Bed\ngroups. Each group has its own Gaussian distribution.\nThis is really helpful for understanding complex data. We want to pay more attention to the brighter colors,\nwhich are the extruded material, and less to the\nWe provide number of clusters we think there are to the\ndarker colors of the bed or tip.\nEM (Expectation-Maximization) algorithm, which\nprovides us the means and variations of each cluster. But a Gaussian Mixture Model doesn't let us do that.\nSo we need to find a workaround.\nCheck out the image above I found online to explain\nthis. Why we cannot use k-means\nK-means clusters data faster than a Gaussian Mixture Model and the EM\nalgorithm. But since our data isn't that big, the time difference doesn't really\nmatter. I ran both 1000 times and found that the Gaussian Mixture Model is\nactually quicker with our input.\nk-means \u2248 0.062 seconds\nGMM + EM \u2248 0.014 seconds\nAnother issue is that k-means expects clusters to be spherical with equal\nvariances. That's not the case for us. We also need to know the standard\ndeviations to break up our data into sections. Visualization\nThe x axis is pixel intensity, and the y axis is the count of how many pixels have each intensity Visualization 2\nThis time we find 2 clusters. Red: Mean. Blue: +/- Standard Deviation. Visualization 3\nThis time we find 3 clusters. Red: Mean. Blue: +/- Standard Deviation. Observation\n200-215 215-225 170-190 207-209 185-190 230-240 180-190 215-230 185-200\nI manually measured the intensity value of the extruded material. If you look back\nat the last slide, you'll see it matches the third mean and its range almost exactly.\nSo now we can approximate the color of the material. GMM Preprocessing Step\nParameters: image, num_components\nHere are the steps for the new method using the EM algorithm.\n1. Get means and variances for clusters.\n2. Create ranges around each mean using standard deviation.\n3. Fix gaps and overlaps in the ranges.\n4. Replace all colors in each range with the same color.\nThis works for any image and any number of components. GMM + Sensitivity Step\nParameters: image, num_components\nFor this method, we're focusing on the highest mean (The material):\n1. Get means and variances for three clusters.\n2. Select all data from the highest mean (plus or minus two standard deviations).\n3. Calculate means and variances for these clusters.\n4. Create ranges around each mean using standard deviation.\n5. Combine this with the lowest range from the first three clusters.\n6. Fix gaps and overlaps in the ranges.\n7. Replace all colors in each range with the same color.\nThis also works for any image and any number of components. GMMx: Gaussian Mixture Model Algorithm\nwith x components\nOriginal\nGMM8\nGMM12\nGMM16 GMMSx: Gaussian Mixture Model +\nSensitivity with x components\nOriginal\nGMMS4\nGMMS6\nGMMS8 Preprocessing Conclusion\nThe new method removes noise, highlights edges, and doesn't lose important info.\nIt even highlights faint edges that are hard to see. It's more sensitive to the\nmaterial, and it darkens the tip or bed.\nWe can use fewer clusters, which cuts down on noise and makes things faster.\nOverall, this method should help with training models for classification and\nmeasuring diameter. Colors are not important. Shape matters.\nHere are a few ways we can make the model focus on shapes, not colors:\n1. Use data augmentation techniques that change colors, like changing\nbrightness, saturation, or the color palette. This is because colors aren\u2019t\nimportant for our problem. The shape of the extruded material is what matters\nto us.\n2. Design the CNN to focus more on shapes. This could mean using larger\nconvolutional kernels or more pooling layers. That way, the model will pay\nmore attention to larger shapes and less attention to small color changes. Virtual Marker Test\nIn a comparison, the YOLO v8 model worked better than the virtual marker. I put\n30x30 markers randomly on frames with some noise, rotation, and light changes -\nbut still less than in real life. After running each model 1000 times, the YOLO v8\nmodel came out on top when looking at the averages.\nYolo v8 Tip Detection: 1920x1080 image \u2248 0.46 seconds\nVirtual Marker: 1920x1080 image, 20x20 marker \u2248 0.61 seconds\nIn a real scenario, there would be even more noise and camera variation.\nAdditionally, we haven\u2019t even pruned or implemented Tiny YOLO which would\nfurther improve the efficiency of the model.\nThus, we can safely continue with our original plan. 6/5 - 6/12\n\u25cf Plan for what and how to train classification models for\nunderflow/overflow/normal/bubble. Train a variety of models.\n\u25cf Organize all trained models, and clearly show metrics and details explaining\neach.\n\u25cf Research and choose what type of marker would be best to use in our\nscenario.\n\u25cf Test virtual marker.\n\u25cf If virtual marker is computationally efficient enough to run 30 times a second\non raspberry pi, try in real world.\n\u25cf Clean up, update and add documentation to code. Push to github.\nReorganize, add dependencies, and error messages. Classification Models\nPre-Trained models for Transfer Learning I will use:\n1. MobileNetV2: Prioritizing efficiency\n2. ResNet50: Prioritizing accuracy\n3. EfficientNet: Balance of efficiency and accuracy\nTraining on the unzoomed image will not work, because the model will not be able\nto achieve good accuracy without overfitting given our limited dataset. So I will\ncrop around the tip at sizes of 50x50.\nThese models were trained on the ImageNet dataset, thus their convolutional\nfilters have learned features at that scale. At 224x224, these models should\nprovide better accuracy, but smaller inputs will provide better efficiency.\nI\u2019ll create a variety of datasets with different preprocessing steps to compare.\n\u25cf 50x50\n\u25cf 50x50 resized to 224x224\n\u25cf 50x50 with padding to 224x224 Preparing datasets for classification models\nThe code can be found here. I\u2019ve split it into folders for image tools (image_utils), datasets\nand the files which train the models.\nFirst, I need to collect my data. I\u2019ll want to crop a 50x50 area near the tip covering the\nmaterial for each image in the dataset. To do this quickly, I created a tool to collect this\ndata, demonstrated in the video below. This is the file image_crop_gui.py\nI then created the preprocessing steps to resize and add padding. I am also taking the\npreprocessing step from my other project and will use that to create another dataset.\nPerhaps that preprocessing step, removing noise and sharpening edges will lead to better\nmodel performance. But for now I still need to make some upgrades to that algorithm.\nThis is all in the file preprocessing.py\nI made sure to add comments and make everything easy to understand and read. Training hyperparameters\nGiven our smaller dataset, we want a bigger batch size. A bigger batch size\nreduces variation as more data points are sampled at once, and will lead to better\nregularization. Overfitting is not an immense concern of ours. Additionally we want\nlearning rate decay, early stopping and data augmentation in the form of rotation.\nI\u2019ll visualize metrics in TensorBoard and choose the best performing model based\non validation set.\nhttps://pytorch.org/vision/stable/models.html\nNote: Things were going swimmingly. I had trained a MobileNetv2 model. But now\nwhen I try to import torch, I get the following error: zsh: segmentation fault\nIt gives no other details, and I\u2019m really struggling to get past this. Code improvements\n\u25cf Github: https://github.com/BrianP8701/STREAM.AI\nMy branch was behind 8 commits. My Github hasn\u2019t been updated since April 19th\nbecause I made some commits containing large files which couldn\u2019t be uploaded. I\nreset those commits, and uploaded what I have now.\n\u25cf Created Conda environment\n\u25cf Added requirements.txt with only necessary dependencies\n\u25cf Replaced all complex logic in main algorithms with helper functions. You can\nnow read through the main algorithm very easily as it only consists of\ninitialization variables and readable functions and names.\n\u25cf Added more comments and restructured code Pose Detection\nAprilTags and AruCo markers are the state of the art markers for pose detection\ntoday. AprilTags are more robust, which is what we need as the camera will not\nalways be positioned the same and there will be lighting variation.\nI will do an experiment with a virtual tag first to measure the computational\nefficiency. For this, I\u2019ll simply place the tag on the image at a realistic scale, and\nmeasure the efficiency. If it is not more efficient virtually, it surely will not be more\nefficient in real life with noise.\nI will compare to the Yolo v8 Tiny model for comparison. 4/14 - 4/21\n\u274f Temporal Correction Standard Horizontal Pre Vertical\nInference Cap Each instance only takes one Each instance we have to\ninference, so we can have a backtrack and do more\nlower cap. inferences, so set a higher cap.\nReliability Unreliable, due to inherent Near perfect metric for temporal\nvariation. offset.\nAs noted above, the prevertical is a much better way of measuring the temporal error.\nYet we still want to develop the standard horizontal method, as we will find that not all\nprints will contain those specific movements. I have included two constants, that can\neasily be adjusted to choose how much of either one you want, which can be chosen\nas needed for whatever specific print is being done. Implementation Details: Conclusive vs Inconclusive State\nThis state merely represents:\n\u25cf Inconclusive: We don\u2019t know if we are ahead or behind\n\u25cf Conclusive: We know if we are behind\nBased off whether we are in a conclusive or inconclusive state determines how we\nbehave in certain situations.\nFor example, if we are in an inconclusive state we cannot find the temporal offset\nduring a pre-vertical move. Figure 1:\nFigure 1 Figure 2\nBlue (Standard Horizontal temporal offsets)\nRed (Pre-vertical temporal offsets)\nThis graph shows all of our data collected without removing\nanomalies. In the standard horizontal when the parser falls\nsignificantly behind the offsets go bonkers. The prevertical on\nthe other hand is more steady.\nFigure 2:\nBoth images above have an initial\nRed (Standard Horizontal and pre-vertical temporal offsets)\ntime_k of 0.99. Here we are simply\nrunning our parser and measuring the\nThis graph is the same as figure 1, expect that the anomalies temporal offsets without taking any\nare removed. actions.\nFigure 3 Figure 4\nFigure 3 & 4:\nBlue (Standard Horizontal and pre-vertical temporal offsets)\nHere I just display two more graphs displaying the temporal\noffsets over time for two different time_k values. As we can\nsee, the slope appears to correspond to how much we should\nchange time_k by, as it tells us the rate at which the temporal\noffset is changing.\nTime_k = 0.98 Time_k = 0.995 Initial Slope Time_k\ntime_k correction\n0.97 0.01548156275287975 +0.015\n0.975 0.004778425833443505 +0.01\n0.98 0.0028197907793473874 +0.005\n0.983 0.0017968711589403047 +0.002\nCollecting more data to see what the slope of the error vs\n0.984 0.0016009852751633009 +0.001\nframes is for different initial time constants. Next, we want\nto do this across multiple videos and find a consistent way\n0.985 0.00014960637206662498 0\nto convert that slope into a correction.\n0.986 0.00014960637206662498 -0.001\n0.987 -0.0007433997167368042 -0.002\n0.99 -0.011362640420807129 -0.005\n0.992 -0.011970263220423117 -0.007\n0.995 -0.013227478738805885 -0.01 4/7-4/14\n\u274f Temporal Correction Overview of Temporal Correction\nTo clarify, we are no longer seeking to correct spatial error. We are trying to see if\nthe parser is behind or ahead temporally.\nWithin the gparser, we now must work with the true angles. Further processing\nand adjustment may be performed in measurement.\nWe must catch the error before it gets so bad we can\u2019t fix it.\nEven in the best tracking, there is spatial error. We must only make decisions if the\ntracker is consistently ahead or behind. Terminology Clarification\nOffset, Spatial Error, Spatial Offset: The physical distance by which the tracker is\noff from the actual location.\nSampling Inference: Performing inference on the image using Yolo v8.\nTime_k, acceleration constant: This is a mistake on my part sorry. In my code the\nline which contains the unknown constant is:\ntime_for_move = how_long(abs(magnitude(position_vector)), abs(magnitude(curr_velocity_vector)),\nabs(final_speed), abs(max_speed), ACCELERATION) * time_k\nThere are two values that can be adjusted, ACCELERATION and time_k. Both\nhave the same effect, and I will be leaving ACCELERATION constant and\nadjusting time_k. So from now on I will say time_k, but whenever in previous\nslides I had said acceleration, substitute in time_k. Terminology Clarification\nTip tracker: When I say tip tracker, I am specifically referring to the tiny box that\nattempts to follow the tip around.\nTemporal Error: My code parses through the gcode at a certain pace, and can fall\nahead or behind. Temporally ahead or behind refers to the fact that my tip tracker\nis not aligned temporally with the real print.\nTime Travel: Referring to when my tip tracker is ahead or behind, not making a\nspatial correction but rather moving forward or backwards some frames to correct. Implementation Details of Temporal Correction\nInstead of sampling inferences at given intervals, we will select our inferences\nbased off the gcode now. When the printer is moving horizontally we will take an\ninference. We can make the following logic:\nOffset to left Offset to right\nMoving left Temporally ahead Temporally behind\nMoving right Temporally behind Temporally ahead\nMoving horizontally at an angle is acceptable. So in the middle of that line of\ngcode, we can take an inference and tell if we are temporally ahead or behind. We\nwill place a cap at a maximum inference rate of 60 frames, so we won\u2019t\noversample. Implementation Details of Temporal Correction\nWe will maintain and build a list of offsets and their corresponding frames as we take inferences.\n(-) behind, (+) ahead\nExample, where we are clearly ahead: [0,0,-1,0,1,3,2,5\u2026] offsets\n[5,137,256,678\u2026] frames\nWe can now plot this to see how the offset looks as the video goes by. No correction is being made\nfor now, just measurements. The correct time_k value should be 0.985, and here we can see the\noffset for two different values of time_k throughout the video. Positive offset means the tracker is\nahead, and vice versa for negative.\ntime_k = 0.98 time_k = 0.99\nHere we can see the tracker\nHere we can see the tracker is getting behind, but around\nis clearly getting steadily 4000 frames, the tracker falls\nahead throughout the entire so much behind that our\nvideo, with a couple outliers. logic falls apart. We want to\nmake corrections far before\nthat point. Removing Anomalies\nWe first calculate the best-fit line using least squares. Then, calculate residuals.\nNext, compute the Z-scores for residuals and filters out the points with Z-scores\ngreater than the specified threshold. This approach considers anomalies based on\ntheir deviation from the best-fitting line, rather than the mean of the data. I chose\nto use a threshold of 3.\nThreshold = 2 Threshold = 3 Threshold = 4 Measured offsets vs time, and the slope of Measured offsets vs time, and the slope of\nthe best fit line vs time the best fit line vs time\nwithout removing anomalies. removing anomalies.\nWe can clearly see that, that the best fit line will go Here, the slope eventually settles on a value. We\nbonkers if we do not remove anomalies. can use this to tell us how much to change the\ntime_k constant by.\nNow, you might ask, how do we know when to\nmake a correction? When the standard deviation\nof our slope is clearly ahead or behind we can\nconfidently make a decision. However, as we will\nsee later, this is not sufficient. Implementation Details of Temporal Correction\nGiven our offsets and time, we can calculate the slope and standard deviation of\nthe best fit line.\nThe slope tells us: The magnitude of the temporal error\nSlope Time_k correction\n- -\n+ +\nThe standard deviation of the slope tells us: Our confidence in this decision. It\nappears a value of 0.003 or 0.002 tells us we are very confident Issues\n1) It takes too long for the slope to converge to a value and for the standard\ndeviation of the best fit line to settle.\n2) I mentioned last week that we want to over accelerate then decelerate to fix\nthe temporal error. I changed my mind, that introduces too much complexity.\n3) In the previous slides, I was proposing that we change the slope based on the\nmagnitude of the offset. However, these current prints are very simple,\nconsisting of lots of straight horizontal and vertical lines. If we were to move\nmore diagonally, the magnitude of the spatial offset would seem less then it\nreally is. Additionally, the speed of the tip contributes to this. If the tip is\nmoving very fast, we might see a large offset, even if the temporal error is\nsmall. Thus, we must consider the angle and speed of the tip in certain\nsituations when measuring offset. Further,\nThe entire program overall is still exceedingly fast, finishing the entire 5 minute video in\naround 15 seconds not including measurement. (Measuring diameter is quite\ninefficient, but I will do optimizations on that algorithm later).\nMy approach now is to try to find more places we can do inference so we can more\nquickly, and confidently conclude whether we are ahead or behind.\nMy issue with this, is that it doesn't make this algorithm robust with respect to the\nvariety of possible gcode. What if a print doesn't have these types of moves? Are there\nother possible movements that can be valuable? In following slides, I will attempt to\nconcisely categorize and define types of movements and the logic for determining\nwhen they occur and whether they are ahead or behind. Standard horizontal moves\nI already discussed these, but I will reiterate with the new changes. We want to\nstandardize all of these regardless of speed or angle. So we can simply:\nDividing by the x component of the velocity accounts for the angle and the speed,\ngiving us a consistent view of the offsets. With this addition, we do the same thing\nas I mentioned previously. Additionally, to be conservative and given the inherent\noffsets during print, we will only consider horizontal within the range of 60 degrees\nin either direction. The only change here, is that offsets should be more accurate\nand more robust when we have gcode that has more diagonal moves. Quick horizontal move, followed by vertical move\nIn this particular print, there are some very quick movements the tip makes\noccasionally. These are so fast, they sometimes take one frame and my algorithm\ndoesn't do inference on them. However, these are very valuable, yet the logic for\ndetermining whether we are temporally ahead or behind changes.\nAt start of move At end of move\nTracker is ahead Spatially correct Spatial error\nTracker is behind Spatial error Spatially correct\nIt is important to note, that the magnitude of the offset is not relevant in this case.\nMerely seeing if there is an offset is all we can do. Now based off repeated instances of\nan error we can make a change, but we cannot determine the magnitude of this\nchange. Quick horizontal move, followed by vertical move\nWe know the moment when we start this move from the gcode. To find the\ntemporal offset, we need to backtrack and see exactly when this spatial offset\nbegan. To achieve this the first time we do it, we need to brute force and check\nprevious frames one by one until we find the moment the spatial offset begins.\nOnce we begin collecting more data we can make a more informed decision.\nTemporally behind Temporally ahead\nParser move When spatial offset ends When spatial offset begins\nReal move When spatial offset begins When spatial offset ends Time Travel\nNow, to address the issue of having to \u201cover accelerate then decelerate.\u201d That is\nunnecessarily complex. Rather than that, we want to change the time_k constant\nto what we believe is truly correct. Then, let\u2019s \u201ctime travel\u201d, meaning move\nforwards or backwards some frames, removing the need to over accelerate then\ndecelerate. The question now is how many frames to \u201ctime travel\u201d?\nAdditionally, we cannot actually go back in time as we assume this will run in real\ntime. We will use a method I read about in my Distributed Systems textbook for\nsynchronization, using \u201cleap\u201d frames or \u201cbuffer\u201d frames. If we want to go ahead in\ntime we add skip some frames in our future moves, and we add extra frames if we\nwant to slow down. The details of when this will be performed will be detailed later. Total View, Overall Consistency\nWe cannot assume that we can make the exact perfect temporal and time travel\ncorrection the first time. We want to maintain a total view (A word I made up). I\nmean we want to keep track of all the previous data ([frames, offsets], [frames,\nahead/behind]) we\u2019ve collected and what actions (temporal corrections, time\ntravel) we\u2019ve taken.\nNow, we will make a decision as early as we can, but as we gather more data later\ninto the video and save it, we can use the entirety of all our data to eventually\ncome closer to reaching a definitive and final conclusion.\nAn important decision is that we will run two different parsers now. One which is\nthe original with none of our actions performed, alongside our \u201ccorrected\u201d one. We\nwant the original parser so we can maintain the same temporal vs time graph and\ncontinue to build confidence, rather than start all over every time we make an\naction. frames: [...] int We use this data with the least squares\noffsets: [...] int method to determine the magnitude of the\nFrames and their corresponding offset from standard change we need to make to time_k.\nhorizontal moves\nframes: [...] int\nahead/behind: [...] boolean When this data shows consistency we can\ntemporal_offset: [...] int make changes to time_k and confident time\nFrames and their corresponding values from quick travel decisions.\nhorizontal -> vertical\nframes: [...] int This data can be useful when we want to\ntime_k adjustment: [...] float\n# of time travel frames: [...] int\nActions taken Acceptable Errors\nThere is inevitably a little error even in the best of runs. Our data shows that these are acceptable errors.\nStandard Horizontal Quick Horizontal -> Vertical\nSpatial 20 pixels 8 pixels\nTemporal N/A 2 frames 4/1 - 4/7\n\u274f Run correction purely for acceleration adjustment\n\u274f Refactor code Spatial Correction\nSpatial correction has been solved. Inference rate minimum is 5 seconds, and\nmaximum 10 seconds. It starts at 5 seconds and can increase or decrease based\non whether there are errors.\nI maintain a stack data structure that keeps track of the offsets in the x direction\n(difference between yolo v8 prediction and my calculated location of the tip).\nSometimes, inferences don\u2019t meet the confidence minimum, so we simply ignore\nthose instances. Whenever the stack has two values, I will remove both and see if\nthey are offset in the same direction. If so, I will make a spatial correction that is\nthe average of both offsets. Temporal Correction\nSome things to note before talking about temporal correction:\n\u25cf Spatial corrections don\u2019t fix temporal errors\n\u25cf When making temporal decisions, do not account for the corrections made\nspatially. Keep track of all spatial corrections made, and remove it from\ntemporal offsets.\n\u25cf To catch up we need to over accelerate and then decelerate once we\u2019ve\ncaught up (Vice versa to slow down). We need to avoid oscillating back and\nforth between acceleration values and settle on a value. Temporal Correction\nWhen the parser begins we assume that it is starting correctly. In other words, in\nthe start the tip tracker is correct. But slowly throughout the video the tracker will\nbegin to deviate from the real location if the acceleration value is off. So we want\nto always be on high alert and make small corrections to the acceleration value as\nwe see fit.\nLet\u2019s ignore the spatial corrections, and look at my actual tip tracker prediction vs\nthe yolo v8 inference when correcting the temporal error. Let\u2019s not look at the\nmagnitude of the error, but the rate and magnitude of error changing over time.\nWe will be conservative in changing the acceleration value, but if it is significantly\nwrong, we will make larger corrections. Temporal Correction\nImplementation Details:\nBased off bed angle and direction of offset, we can tell if the tracker is behind or\nahead. Similarly to spatial correction we maintain a stack. One stack contains\noffsets, but here ignoring the corrections. Another stack maintains a boolean\nsaying whether we were ahead or behind\nAfter 4 inferences, if the tracker is consecutively ahead or behind we will make a\nmodification to the acceleration value.\nWe simply make a very small adjustment to the value. 3/24 - 4/1\n\u274f Reduce cropping size\n\u274f Adaptively adjust inference rate and acceleration constant based on error\n\u274f Run multiple inferences at a time\n\u274f Increase minimum confidence for inference\n\u274f Try slightly adjusting angle for more robustness in measure_diameter\nalgorithm Gparser autocorrect problem\nI\u2019ve found that through increasing the minimum confidence of inference to 0.5, the inference is\nliterally perfect.\nYet, the autocorrect still jumps around. Much less, but still unacceptable.\nI\u2019ve concluded there can only be one cause of this bug:\nWhen the tip moves really fast from one spot to another,\nMy parser is not perfectly temporally aligned with the video,\nSo when running an inference at those moments, it looks like the parser needs to be\ncorrected\nHowever, a distinction must be made between spatial and temporal errors.\nBy spatial I mean my tip tracker is off by some distance. Temporal error refers to my parser running\nbehind the actual video in time. Gparser autocorrect solution\nThe solution to this problem is simple. We merely collect multiple frames for\ninference within close time proximity, rather than doing inference on one frame.\nIf for all inferences the tip tracker is off by some amount of pixels, there is a spatial\nerror, which can be corrected by simply correcting the tracker spatially.\nIf for some frames the tip tracker is perfect, while for others it is off, then we know\nthere is a temporal error. Based off the order of the inferences and their spatial\nerrors, we can decide whether to increase or decrease the acceleration constant.\nAdditionally, we can increase the inference rate.\nAnd obviously, if all frames are perfect we make no correction and can decrease\nthe inference rate. Version control\nI\u2019ve never really bothered to learn how to properly do version control, and I\nsomehow made a very big mess. Something about submodules, nested git\nrepositories etc. I ended up accidentally deleting my .onnx file, and was not able to\nrecover it. So I need to retrain a new model and add it back in. Other than that,\nhere is the new repository on github to keep track of the code:\nhttps://github.com/BrianP8701/STREAM.AI Adjusting predicted angle\nAs you recall, as I am parsing through the gcode I am calculating the angle at\nwhich the extruder is moving on the bed. However, this angle does not always\nexactly match up with the angle that the material is coming out, for various small\nodd reasons.\nTo counter this, I will simply try multiple angles close to my predicted angle.\nI am trying these multiples: 0.85, 0.9, 1.0, 1.1, 1.15 3/17 - 3/24\n\u274f Run model concurrently alongside gparser to autocorrect tip tracker\n\u274f Train new Yolo V8 model\n\u274f Update measure diameter method to account for tip location Gparser autocorrect\nIt appears that there is some unknown variable that makes the printer behave slightly differently\nthen my calculations. Thus, to account for this I will occasionally autocorrect my tracker.\nEvery 120 frames I will crop part of the image and run it through my yolov8 model, which will give\nme the exact location of the tip. I will use this to correct my tracker. I chose 120 frames.\nAs we plan to use this in real time, we cannot have the inference blocking the rest of the process.\nHere is the process to counter that: As the gparser begins, inference will begin running\nconcurrently on a separate thread. The gparser will continue for another 120 frames, then a lock\nwill be placed waiting for the inference thread to complete. Upon inference, correction will\nhappen, then another inference will run, and repeat. Some microcontrollers do not support\nthreading, but do allow for concurrency. The value of 120 can be adjusted as needed depending\non inference time. New yolov8 model\nAfter making this program, I was confused as to why my autocorrector was\ncausing my tracker to mess up. I realized, my old yolo model was trained on the\nendoscope camera view. So I\u2019ve trained a new yolov8 model on 728 labeled\nimages. 1748 images after augmentation with blurring, noise, flipping, brightness\nand exposure.\nAlso, I have found code online that does inference on the onnx model, locally, so\nthat is no longer a problem. Measure Diameter Fix\nPreviously, upon trying to assign confidences to lines my algorithm would\noccasionally choose the wrong lines. I have concluded that the only way to\ncounter this is through knowing the exact location of the tip and assigning higher\nconfidences to lines within reasonable distance to the tip, while exponentially\ndecreasing confidence as we begin to look at lines that are surpassing what I\nconsider a reasonable distance (About 25 pixels, or 1.6 mm radius). This method\nworks very well. Looking forwards\nI believe this is the final step to measure the diameter in real time robustly:\nI will need to use the autocorrect to not only correct the position of the tracker, but to also adjust the acceleration\nconstant. If the tracker is often falling behind, increase the acceleration and vice versa.\nI think I am ready to begin thinking about the next phase of this project. After fixing up the autocorrector, and adding in\nthis final feature I should be able to robustly measure the diameter extruded material during a print in real time with\nthe following:\nConstants:\n\u25cf mm to pixel ratio (changes with camera angle)\nAdjustments:\n\u25cf Integrating my algorithm with real time video and aligning the start of my algorithm with the start of the print\nAdditionally, we will need to add a second camera angle eventually, which shouldn\u2019t be difficult from my standpoint, as\nit just requires switching views when the printer is moving in a certain direction. 3/3 - 3/10\n\u274f Use algorithm on different videos\n\u274f Use time data\n\u274f Measure diameter Gcode Parser (Tip Tracking)\nObservations: While tracking the tip, when we get to building the \u201ccube\u201d, where there are many changes in\ndirections, the tip tracker would run ahead. Upon analyzing the video from last week I noted that the tip tracker\nwas lagging behind at the start and ahead by the end. Thus, I concluded that acceleration is in fact not negligible.\nThe gcode states that the acceleration should be 1000 in the x and y direction and 200 in the z direction.\nHowever, upon experimenting I found that an acceleration of 64 best fit the video. This provided a perfect tracker\nfor that video.\nSo we now have two variable constants, which can be treated as one, the acceleration and the constant by which\nwe multiply the move times. Furthermore, these constants do not work upon other videos. It is very close, but\nupon letting the tracker run for a couple minutes it runs ahead or falls behind by an unacceptable distance.\nI am not sure what is the cause of this. My acceleration and gcode parser is certainly correct. I\u2019ve spent many\nhours reviewing, checking and testing my code. Additionally, the data from UBox relating to the time is not\naccurate enough for 1/30 of a second moves.\nTLDR: I do not know where the error in calculating the time of each line comes from. Table of experimental data\nacceleration\nVideo 1 fps time_k (mm/s)\ntest_V1 30 1 1000 Slow at start, fast at end\ntest_V2 30 0.993 1000 Too fast\ntest_V3 32.65 1 1000 Way to slow\ntest_V4 32.65 0.993 1000 Way to slow\ntest_V5 30 0.99 100 Slow at start, fast at end\ntest_V6 30 0.99 1 Acceleration wayyy to slow\ntest_V7 30 0.99 50 Too slow all throughout\nBehind by the same amount, a\ntest_V8 30 0.99 75 little\nBehind by the same amount,\ntest_V9 30 0.988 70 better\ntest_V10 30 0.985 65 Way better, gets too fast at end\ntest_V11 30 0.985 60 Almost perfect\ntest_V12 30 0.985 64 Perfect\nVideo 2 fps time_k acceleration\ntest_V1 30 0.985 64 Good to start, fast at end\ntest_V2 30 0.985 58 Slow to start, fast at end\ntest_V3 30 0.983 55 Perfect Edge Detection\nBy locating the exact location of the tip, we have a better idea of where the extruded\nmaterial should be. Find tip by convolving over image looking for the highest\nconfidence of the outline of the tip. Exactly from the center of the tip, draw a line out in\nthe direction it came from. Shift that line from that point in both directions to find the\nedges. If there aren\u2019t two lines that surpass a minimum confidence, then we assume\nthere is no material. Otherwise, simply calculate distance between two lines to find\ndiameter. We can\u2019t look at brightness, because sometimes another area might be\nbrighter. This is the most effective method I\u2019ve thought of. Edge Detection\nI have created the tip detector I mentioned last slide, and it works, but not perfectly and the runtime\nis quite bad, taking around a second per inference. Here\u2019s some images. You can further see that\nthe preprocessing step works very well. Even on the white image, it has very clearly defined the\nline, even though it\u2019s hard to see with the human eye (Difference in intensity of color doesn't\nmatter, just difference in color.) Edge Detection\nIf I could get the time data to a higher degree of accuracy (I emailed you about this) then the tip\ntracker would very accurately be able to get the point of the tip. Otherwise, I\u2019d have to fix the\nalgorithm from last slide. The next step is to draw a line in the direction we came from, and then\nshift it in both directions to find the best edges. It\u2019s important that the actual tube of the extruder is\nnot included in the cropped image, as it will interfere with edge detection.\nNote from future: What ended up happening is that I used more yolo v8 models to do inference to\nself correct, which I talk about in the future. 2/24 - 3/3\n\u25cf Made good pre-processing step\n\u25cf Edge detection\n\u25cf Fixed gcode parser Preprocessing\nBefore moving further, we want to get rid of the noise and sharpen the edges. We\ncan simplify the image by reducing similarly colored pixels to the same value with\nrespect to the variance of the color in the image Edge Detection and Diameter Measurement\nGiven the image and angle of movement, I check all lines in that direction. I\nmeasure confidences for each line, and added some extra fine details I found\nwhile experimenting. It\u2019s not perfect yet - still working on it.\nStill working on\nhow to\novercome\nsome\nsituations like\nthis. Gcode Parser\nI fixed the gcode parser, but I have no math or any reasoning to back it up. It\u2019s\nquite accurate now. It turns out that the real time is 99% of the calculated time. I\nmade a few more additional tweaks, and that fixed all the problems. Acceleration\nturns out to be negligible.\nNote from future: This leads into what is the overall problem I seek to address\nlater. Across different videos, there seems to be a different constant % of the\ncalculated time I must find. 2/17 - 2/24\n\u25cf Tried convolving matrices over images of extruded material\n\u25cf Finished bulk of gcode parser, debugging\n\u25cf Planned algorithm for edge detection, taking inspiration from this paper:\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8607091 Tried convolving different filters over images\nGaussian Blur, Sharpen3, Sharpen2\nSharpen2\nI tried convolving different matrices over the\nimages, looking to find a combination that would\nwork consistently across the variety of images.\nThese 4 filters I found after hours of trying different\ncombinations, and would only work for that specific\nSharpen1\nimage. This is a very laborious process, and it\u2019s\nactually what CNNs do, learning the best kernels\nfor the image. It was pretty fun and helped build my\nintuition for CNNs more.\nLong1 Algorithm for detecting edges\nTo measure the diameter of the extruded material, we need to accurately identify\nthe edges of the material. We do not know where this is beforehand, or if it\u2019s even\nthere. However, we do know the angle at which the material should be coming out\ngiven the gcode.\nWhat we can do is treat this as a complete search problem, with all the lines of\nthat angle crossing the image being the search space. We can slightly offset the\nangle in both directions, and after searching the entire search space identify the\ntwo lines that have the greatest probability of being the edges of the extruded\nmaterial.\nWe have to account for the lines from the actual tip, which we can locate and\nfactor into our probability calculation. 2/11-2/17\n\u25cf Make gcode parser and tip tracker more accurate Improving gcode parser and tip tracker\n\u25cf New Challenges:\n\u25cb As we move in the z direction, the x axis will stretch, changing the ratio of mm to pixels.\n\u25cb To estimate time for a line of gcode to be performed, we cannot just divide distance by velocity. We\nhave to account for acceleration and jerk.\n\u25cb Even though the printer isn\u2019t changing it\u2019s speed, when it changes directions it needs to accelerate in\nthat new direction.\n\u25cb I found this quote online: \u201cI have found that the biggest error between the predicted time and the\nactual time has been the time the machine spends processing the instructions.\u201d I don\u2019t know if this is\ntrue, but will leave it here.\n\u25cf Relevant links (For myself, no need to look at this):\n\u25cb https://3dprinting.stackexchange.com/questions/10369/why-does-jerk-have-units-of-mm-s-rather-than-\nmm-s%c2%b3\n\u25cb https://3dprinting.stackexchange.com/questions/3233/calculating-the-estimated-print-time-of-an-alread\ny-sliced-file Gcode parser and interpretation\n\u25cf I wrote a working algorithm that reads gcode and tracks the movement of the\ntip accordingly frame by frame\n\u25cf However, the camera shakes hundreds of pixels in unpredictable directions. I\nwill need to record new videos where the camera does not shake so much.\n\u25cb Note from future: This turned out to be a result of the fact that the printer can move in the z\ndirection, which I didn\u2019t account for.\n\u25cf I made my code such so it\u2019s easy to adapt to different types of video angles,\nzoomed in or out and FPS of video. Plan to measure diameter\n\u25cf Make tracker that reads G-code. Tracker should output direction and speed extruder is\nmoving at each frame.\n\u25cf Based off speed and direction, crop a portion off the frame accordingly.\n\u25cf Convolve over cropped subsection to detect edges and measure width or area of\nmaterial.\n\u25cf Output diameter of new volume extruded for each frame.\n\u25cf Make code maintainable so we can easily adapt to different camera angles and add a\nsecond camera. 2/4-2/11\n\u25cf Plan design for algorithm to measure diameter of extruded substrate\nhttps://hackaday.com/2016/02/05/filament-thickness-sensors-what-are-they-and-what-are-they-good-for/ Topics Originally Planned\n\u2022Hash space anomaly detection (classification) based on Jackson DIW data/new data\n\u2022Preparation: learn from any image classification examples\n\u2022First, CNN for image based consistency classification (can be either at the extrusion point or better at the substrate), may need data augmentation\n\u2022Later, classification after deep hash (see the paper by Huining Li)\n\u2022Can be put on Arduino/Pi later\n\u2022The above steps can be applied to sensor data too\n\u2022Hash space controller based on DIW images for consistency\n\u2022Preparation: learn image feature extraction, regression examples\n\u2022First, extract the diameter info during the printing and compare it with the desired value in real time, obtain the deviation.\n\u2022Then, build a regression model to link material and printing parameters to the deviation.\n\u2022Finally, build a PID controller for the deviation minimization.\n\u2022Later, control under deep hash\n\u2022Attack to controller under hash space\n\u2022Preparation: know the concepts of controller attacks\n\u2022First, design detection mechanism to the attack\n\u2022Second, in the network setting, perform the diagnosis",
    "data/library/WealthOfNations.txt": "",
    "data/raw_library/DL_MIT.pdf": "",
    "data/raw_library/Data_Science_MIT.pdf": "",
    "data/raw_library/Distributed_Systems_Tanenbaum.pdf": "",
    "data/raw_library/Elon_Musk_Vance.pdf": "",
    "data/raw_library/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf": "",
    "data/raw_library/STREAM_Report.pdf": "",
    "data/raw_library/STREAM_Slides.pdf": "",
    "data/raw_library/Wealth_of_Nations.pdf": "",
    "main.py": "from stream_gpt.data_scrapers.web_scraper import selenium_scrape_site, bs4_scrape_site\n\nbs4_scrape_site('https://docs.llamaindex.ai/en/stable/')",
    "poetry.lock": "# This file is automatically @generated by Poetry 1.6.1 and should not be changed by hand.\n\n[[package]]\nname = \"aiohttp\"\nversion = \"3.8.6\"\ndescription = \"Async http client/server framework (asyncio)\"\noptional = false\npython-versions = \">=3.6\"\nfiles = [\n    {file = \"aiohttp-3.8.6-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:41d55fc043954cddbbd82503d9cc3f4814a40bcef30b3569bc7b5e34130718c1\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:1d84166673694841d8953f0a8d0c90e1087739d24632fe86b1a08819168b4566\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:253bf92b744b3170eb4c4ca2fa58f9c4b87aeb1df42f71d4e78815e6e8b73c9e\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3fd194939b1f764d6bb05490987bfe104287bbf51b8d862261ccf66f48fb4096\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:6c5f938d199a6fdbdc10bbb9447496561c3a9a565b43be564648d81e1102ac22\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2817b2f66ca82ee699acd90e05c95e79bbf1dc986abb62b61ec8aaf851e81c93\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0fa375b3d34e71ccccf172cab401cd94a72de7a8cc01847a7b3386204093bb47\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:9de50a199b7710fa2904be5a4a9b51af587ab24c8e540a7243ab737b45844543\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:e1d8cb0b56b3587c5c01de3bf2f600f186da7e7b5f7353d1bf26a8ddca57f965\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:8e31e9db1bee8b4f407b77fd2507337a0a80665ad7b6c749d08df595d88f1cf5\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7bc88fc494b1f0311d67f29fee6fd636606f4697e8cc793a2d912ac5b19aa38d\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:ec00c3305788e04bf6d29d42e504560e159ccaf0be30c09203b468a6c1ccd3b2\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:ad1407db8f2f49329729564f71685557157bfa42b48f4b93e53721a16eb813ed\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-win32.whl\", hash = \"sha256:ccc360e87341ad47c777f5723f68adbb52b37ab450c8bc3ca9ca1f3e849e5fe2\"},\n    {file = \"aiohttp-3.8.6-cp310-cp310-win_amd64.whl\", hash = \"sha256:93c15c8e48e5e7b89d5cb4613479d144fda8344e2d886cf694fd36db4cc86865\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:6e2f9cc8e5328f829f6e1fb74a0a3a939b14e67e80832975e01929e320386b34\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:e6a00ffcc173e765e200ceefb06399ba09c06db97f401f920513a10c803604ca\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:41bdc2ba359032e36c0e9de5a3bd00d6fb7ea558a6ce6b70acedf0da86458321\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:14cd52ccf40006c7a6cd34a0f8663734e5363fd981807173faf3a017e202fec9\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:2d5b785c792802e7b275c420d84f3397668e9d49ab1cb52bd916b3b3ffcf09ad\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1bed815f3dc3d915c5c1e556c397c8667826fbc1b935d95b0ad680787896a358\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:96603a562b546632441926cd1293cfcb5b69f0b4159e6077f7c7dbdfb686af4d\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d76e8b13161a202d14c9584590c4df4d068c9567c99506497bdd67eaedf36403\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:e3f1e3f1a1751bb62b4a1b7f4e435afcdade6c17a4fd9b9d43607cebd242924a\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:76b36b3124f0223903609944a3c8bf28a599b2cc0ce0be60b45211c8e9be97f8\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:a2ece4af1f3c967a4390c284797ab595a9f1bc1130ef8b01828915a05a6ae684\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:16d330b3b9db87c3883e565340d292638a878236418b23cc8b9b11a054aaa887\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:42c89579f82e49db436b69c938ab3e1559e5a4409eb8639eb4143989bc390f2f\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-win32.whl\", hash = \"sha256:efd2fcf7e7b9d7ab16e6b7d54205beded0a9c8566cb30f09c1abe42b4e22bdcb\"},\n    {file = \"aiohttp-3.8.6-cp311-cp311-win_amd64.whl\", hash = \"sha256:3b2ab182fc28e7a81f6c70bfbd829045d9480063f5ab06f6e601a3eddbbd49a0\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:fdee8405931b0615220e5ddf8cd7edd8592c606a8e4ca2a00704883c396e4479\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d25036d161c4fe2225d1abff2bd52c34ed0b1099f02c208cd34d8c05729882f0\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5d791245a894be071d5ab04bbb4850534261a7d4fd363b094a7b9963e8cdbd31\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:0cccd1de239afa866e4ce5c789b3032442f19c261c7d8a01183fd956b1935349\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1f13f60d78224f0dace220d8ab4ef1dbc37115eeeab8c06804fec11bec2bbd07\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8a9b5a0606faca4f6cc0d338359d6fa137104c337f489cd135bb7fbdbccb1e39\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:13da35c9ceb847732bf5c6c5781dcf4780e14392e5d3b3c689f6d22f8e15ae31\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:4d4cbe4ffa9d05f46a28252efc5941e0462792930caa370a6efaf491f412bc66\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:229852e147f44da0241954fc6cb910ba074e597f06789c867cb7fb0621e0ba7a\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-musllinux_1_1_s390x.whl\", hash = \"sha256:713103a8bdde61d13490adf47171a1039fd880113981e55401a0f7b42c37d071\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:45ad816b2c8e3b60b510f30dbd37fe74fd4a772248a52bb021f6fd65dff809b6\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-win32.whl\", hash = \"sha256:2b8d4e166e600dcfbff51919c7a3789ff6ca8b3ecce16e1d9c96d95dd569eb4c\"},\n    {file = \"aiohttp-3.8.6-cp36-cp36m-win_amd64.whl\", hash = \"sha256:0912ed87fee967940aacc5306d3aa8ba3a459fcd12add0b407081fbefc931e53\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:e2a988a0c673c2e12084f5e6ba3392d76c75ddb8ebc6c7e9ead68248101cd446\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ebf3fd9f141700b510d4b190094db0ce37ac6361a6806c153c161dc6c041ccda\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3161ce82ab85acd267c8f4b14aa226047a6bee1e4e6adb74b798bd42c6ae1f80\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d95fc1bf33a9a81469aa760617b5971331cdd74370d1214f0b3109272c0e1e3c\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6c43ecfef7deaf0617cee936836518e7424ee12cb709883f2c9a1adda63cc460\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ca80e1b90a05a4f476547f904992ae81eda5c2c85c66ee4195bb8f9c5fb47f28\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:90c72ebb7cb3a08a7f40061079817133f502a160561d0675b0a6adf231382c92\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:bb54c54510e47a8c7c8e63454a6acc817519337b2b78606c4e840871a3e15349\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:de6a1c9f6803b90e20869e6b99c2c18cef5cc691363954c93cb9adeb26d9f3ae\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:a3628b6c7b880b181a3ae0a0683698513874df63783fd89de99b7b7539e3e8a8\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:fc37e9aef10a696a5a4474802930079ccfc14d9f9c10b4662169671ff034b7df\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-win32.whl\", hash = \"sha256:f8ef51e459eb2ad8e7a66c1d6440c808485840ad55ecc3cafefadea47d1b1ba2\"},\n    {file = \"aiohttp-3.8.6-cp37-cp37m-win_amd64.whl\", hash = \"sha256:b2fe42e523be344124c6c8ef32a011444e869dc5f883c591ed87f84339de5976\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:9e2ee0ac5a1f5c7dd3197de309adfb99ac4617ff02b0603fd1e65b07dc772e4b\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:01770d8c04bd8db568abb636c1fdd4f7140b284b8b3e0b4584f070180c1e5c62\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:3c68330a59506254b556b99a91857428cab98b2f84061260a67865f7f52899f5\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:89341b2c19fb5eac30c341133ae2cc3544d40d9b1892749cdd25892bbc6ac951\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:71783b0b6455ac8f34b5ec99d83e686892c50498d5d00b8e56d47f41b38fbe04\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f628dbf3c91e12f4d6c8b3f092069567d8eb17814aebba3d7d60c149391aee3a\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b04691bc6601ef47c88f0255043df6f570ada1a9ebef99c34bd0b72866c217ae\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7ee912f7e78287516df155f69da575a0ba33b02dd7c1d6614dbc9463f43066e3\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:9c19b26acdd08dd239e0d3669a3dddafd600902e37881f13fbd8a53943079dbc\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:99c5ac4ad492b4a19fc132306cd57075c28446ec2ed970973bbf036bcda1bcc6\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:f0f03211fd14a6a0aed2997d4b1c013d49fb7b50eeb9ffdf5e51f23cfe2c77fa\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:8d399dade330c53b4106160f75f55407e9ae7505263ea86f2ccca6bfcbdb4921\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:ec4fd86658c6a8964d75426517dc01cbf840bbf32d055ce64a9e63a40fd7b771\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-win32.whl\", hash = \"sha256:33164093be11fcef3ce2571a0dccd9041c9a93fa3bde86569d7b03120d276c6f\"},\n    {file = \"aiohttp-3.8.6-cp38-cp38-win_amd64.whl\", hash = \"sha256:bdf70bfe5a1414ba9afb9d49f0c912dc524cf60141102f3a11143ba3d291870f\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:d52d5dc7c6682b720280f9d9db41d36ebe4791622c842e258c9206232251ab2b\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:4ac39027011414dbd3d87f7edb31680e1f430834c8cef029f11c66dad0670aa5\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:3f5c7ce535a1d2429a634310e308fb7d718905487257060e5d4598e29dc17f0b\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b30e963f9e0d52c28f284d554a9469af073030030cef8693106d918b2ca92f54\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:918810ef188f84152af6b938254911055a72e0f935b5fbc4c1a4ed0b0584aed1\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:002f23e6ea8d3dd8d149e569fd580c999232b5fbc601c48d55398fbc2e582e8c\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4fcf3eabd3fd1a5e6092d1242295fa37d0354b2eb2077e6eb670accad78e40e1\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:255ba9d6d5ff1a382bb9a578cd563605aa69bec845680e21c44afc2670607a95\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:d67f8baed00870aa390ea2590798766256f31dc5ed3ecc737debb6e97e2ede78\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:86f20cee0f0a317c76573b627b954c412ea766d6ada1a9fcf1b805763ae7feeb\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:39a312d0e991690ccc1a61f1e9e42daa519dcc34ad03eb6f826d94c1190190dd\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:e827d48cf802de06d9c935088c2924e3c7e7533377d66b6f31ed175c1620e05e\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:bd111d7fc5591ddf377a408ed9067045259ff2770f37e2d94e6478d0f3fc0c17\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-win32.whl\", hash = \"sha256:caf486ac1e689dda3502567eb89ffe02876546599bbf915ec94b1fa424eeffd4\"},\n    {file = \"aiohttp-3.8.6-cp39-cp39-win_amd64.whl\", hash = \"sha256:3f0e27e5b733803333bb2371249f41cf42bae8884863e8e8965ec69bebe53132\"},\n    {file = \"aiohttp-3.8.6.tar.gz\", hash = \"sha256:b0cf2a4501bff9330a8a5248b4ce951851e415bdcce9dc158e76cfd55e15085c\"},\n]\n\n[package.dependencies]\naiosignal = \">=1.1.2\"\nasync-timeout = \">=4.0.0a3,<5.0\"\nattrs = \">=17.3.0\"\ncharset-normalizer = \">=2.0,<4.0\"\nfrozenlist = \">=1.1.1\"\nmultidict = \">=4.5,<7.0\"\nyarl = \">=1.0,<2.0\"\n\n[package.extras]\nspeedups = [\"Brotli\", \"aiodns\", \"cchardet\"]\n\n[[package]]\nname = \"aiosignal\"\nversion = \"1.3.1\"\ndescription = \"aiosignal: a list of registered asynchronous callbacks\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"aiosignal-1.3.1-py3-none-any.whl\", hash = \"sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17\"},\n    {file = \"aiosignal-1.3.1.tar.gz\", hash = \"sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc\"},\n]\n\n[package.dependencies]\nfrozenlist = \">=1.1.0\"\n\n[[package]]\nname = \"anyio\"\nversion = \"4.0.0\"\ndescription = \"High level compatibility layer for multiple asynchronous event loop implementations\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"anyio-4.0.0-py3-none-any.whl\", hash = \"sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f\"},\n    {file = \"anyio-4.0.0.tar.gz\", hash = \"sha256:f7ed51751b2c2add651e5747c891b47e26d2a21be5d32d9311dfe9692f3e5d7a\"},\n]\n\n[package.dependencies]\nexceptiongroup = {version = \">=1.0.2\", markers = \"python_version < \\\"3.11\\\"\"}\nidna = \">=2.8\"\nsniffio = \">=1.1\"\n\n[package.extras]\ndoc = [\"Sphinx (>=7)\", \"packaging\", \"sphinx-autodoc-typehints (>=1.2.0)\"]\ntest = [\"anyio[trio]\", \"coverage[toml] (>=7)\", \"hypothesis (>=4.0)\", \"psutil (>=5.9)\", \"pytest (>=7.0)\", \"pytest-mock (>=3.6.1)\", \"trustme\", \"uvloop (>=0.17)\"]\ntrio = [\"trio (>=0.22)\"]\n\n[[package]]\nname = \"apify-client\"\nversion = \"1.5.0\"\ndescription = \"Apify API client for Python\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"apify_client-1.5.0-py3-none-any.whl\", hash = \"sha256:fc8f8e4f7aee19b0741c0044b483200a5bcbdc7afa593dd78b0dbb8c3923795b\"},\n    {file = \"apify_client-1.5.0.tar.gz\", hash = \"sha256:b308ac5eed28f8737d055fb9e69156611320b1865cb7b97d333ce6f530580c61\"},\n]\n\n[package.dependencies]\napify-shared = \">=1.0.1,<1.1.0\"\nhttpx = \">=0.24.1\"\n\n[package.extras]\ndev = [\"autopep8 (>=2.0.4,<2.1.0)\", \"build (>=1.0.3,<1.1.0)\", \"flake8 (>=6.1.0,<6.2.0)\", \"flake8-bugbear (>=23.9.16,<23.10.0)\", \"flake8-commas (>=2.1.0,<2.2.0)\", \"flake8-comprehensions (>=3.14.0,<3.15.0)\", \"flake8-datetimez (>=20.10.0,<20.11.0)\", \"flake8-docstrings (>=1.7.0,<1.8.0)\", \"flake8-encodings (>=0.5.0,<0.6.0)\", \"flake8-isort (>=6.1.0,<6.2.0)\", \"flake8-noqa (>=1.3.1,<1.4.0)\", \"flake8-pytest-style (>=1.7.2,<1.8.0)\", \"flake8-quotes (>=3.3.2,<3.4.0)\", \"flake8-simplify (>=0.21.0,<0.22.0)\", \"flake8-unused-arguments (>=0.0.13,<0.1.0)\", \"isort (>=5.12.0,<5.13.0)\", \"mypy (>=1.5.1,<1.6.0)\", \"pep8-naming (>=0.13.3,<0.14.0)\", \"pre-commit (>=3.4.0,<3.5.0)\", \"pydoc-markdown (>=4.8.2,<4.9.0)\", \"pytest (>=7.4.2,<7.5.0)\", \"pytest-asyncio (>=0.21.0,<0.22.0)\", \"pytest-only (>=2.0.0,<2.1.0)\", \"pytest-timeout (>=2.2.0,<2.3.0)\", \"pytest-xdist (>=3.3.1,<3.4.0)\", \"redbaron (>=0.9.2,<0.10.0)\", \"twine (>=4.0.2,<4.1.0)\"]\n\n[[package]]\nname = \"apify-shared\"\nversion = \"1.0.4\"\ndescription = \"Tools and constants shared across Apify projects.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"apify_shared-1.0.4-py3-none-any.whl\", hash = \"sha256:55c8f0cfba2d6949fe7ee0718eb7abaf66688ed69cf6f2c6259d121d9c1215e4\"},\n    {file = \"apify_shared-1.0.4.tar.gz\", hash = \"sha256:dd8c1df763b50bc78121c874cee8d8111b84ddcf0313ebee11ad6d80ef5adcea\"},\n]\n\n[package.extras]\ndev = [\"autopep8 (>=2.0.4,<2.1.0)\", \"build (>=1.0.3,<1.1.0)\", \"filelock (>=3.12.4,<3.13.0)\", \"flake8 (>=6.1.0,<6.2.0)\", \"flake8-bugbear (>=23.9.16,<23.10.0)\", \"flake8-commas (>=2.1.0,<2.2.0)\", \"flake8-comprehensions (>=3.14.0,<3.15.0)\", \"flake8-datetimez (>=20.10.0,<20.11.0)\", \"flake8-docstrings (>=1.7.0,<1.8.0)\", \"flake8-encodings (>=0.5.0,<0.6.0)\", \"flake8-isort (>=6.1.0,<6.2.0)\", \"flake8-noqa (>=1.3.1,<1.4.0)\", \"flake8-pytest-style (>=1.7.2,<1.8.0)\", \"flake8-quotes (>=3.3.2,<3.4.0)\", \"flake8-simplify (>=0.21.0,<0.22.0)\", \"flake8-unused-arguments (>=0.0.13,<0.1.0)\", \"isort (>=5.12.0,<5.13.0)\", \"mypy (>=1.5.1,<1.6.0)\", \"pep8-naming (>=0.13.3,<0.14.0)\", \"pre-commit (>=3.4.0,<3.5.0)\", \"pydoc-markdown (>=4.8.2,<4.9.0)\", \"pytest (>=7.4.2,<7.5.0)\", \"pytest-asyncio (>=0.21.0,<0.22.0)\", \"pytest-only (>=2.0.0,<2.1.0)\", \"pytest-timeout (>=2.2.0,<2.3.0)\", \"pytest-xdist (>=3.3.1,<3.4.0)\", \"respx (>=0.20.1,<0.21.0)\", \"twine (>=4.0.2,<4.1.0)\"]\n\n[[package]]\nname = \"async-timeout\"\nversion = \"4.0.3\"\ndescription = \"Timeout context manager for asyncio programs\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"async-timeout-4.0.3.tar.gz\", hash = \"sha256:4640d96be84d82d02ed59ea2b7105a0f7b33abe8703703cd0ab0bf87c427522f\"},\n    {file = \"async_timeout-4.0.3-py3-none-any.whl\", hash = \"sha256:7405140ff1230c310e51dc27b3145b9092d659ce68ff733fb0cefe3ee42be028\"},\n]\n\n[[package]]\nname = \"attrs\"\nversion = \"23.1.0\"\ndescription = \"Classes Without Boilerplate\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"attrs-23.1.0-py3-none-any.whl\", hash = \"sha256:1f28b4522cdc2fb4256ac1a020c78acf9cba2c6b461ccd2c126f3aa8e8335d04\"},\n    {file = \"attrs-23.1.0.tar.gz\", hash = \"sha256:6279836d581513a26f1bf235f9acd333bc9115683f14f7e8fae46c98fc50e015\"},\n]\n\n[package.extras]\ncov = [\"attrs[tests]\", \"coverage[toml] (>=5.3)\"]\ndev = [\"attrs[docs,tests]\", \"pre-commit\"]\ndocs = [\"furo\", \"myst-parser\", \"sphinx\", \"sphinx-notfound-page\", \"sphinxcontrib-towncrier\", \"towncrier\", \"zope-interface\"]\ntests = [\"attrs[tests-no-zope]\", \"zope-interface\"]\ntests-no-zope = [\"cloudpickle\", \"hypothesis\", \"mypy (>=1.1.1)\", \"pympler\", \"pytest (>=4.3.0)\", \"pytest-mypy-plugins\", \"pytest-xdist[psutil]\"]\n\n[[package]]\nname = \"beautifulsoup4\"\nversion = \"4.12.2\"\ndescription = \"Screen-scraping library\"\noptional = false\npython-versions = \">=3.6.0\"\nfiles = [\n    {file = \"beautifulsoup4-4.12.2-py3-none-any.whl\", hash = \"sha256:bd2520ca0d9d7d12694a53d44ac482d181b4ec1888909b035a3dbf40d0f57d4a\"},\n    {file = \"beautifulsoup4-4.12.2.tar.gz\", hash = \"sha256:492bbc69dca35d12daac71c4db1bfff0c876c00ef4a2ffacce226d4638eb72da\"},\n]\n\n[package.dependencies]\nsoupsieve = \">1.2\"\n\n[package.extras]\nhtml5lib = [\"html5lib\"]\nlxml = [\"lxml\"]\n\n[[package]]\nname = \"bs4\"\nversion = \"0.0.1\"\ndescription = \"Dummy package for Beautiful Soup\"\noptional = false\npython-versions = \"*\"\nfiles = [\n    {file = \"bs4-0.0.1.tar.gz\", hash = \"sha256:36ecea1fd7cc5c0c6e4a1ff075df26d50da647b75376626cc186e2212886dd3a\"},\n]\n\n[package.dependencies]\nbeautifulsoup4 = \"*\"\n\n[[package]]\nname = \"certifi\"\nversion = \"2023.7.22\"\ndescription = \"Python package for providing Mozilla's CA Bundle.\"\noptional = false\npython-versions = \">=3.6\"\nfiles = [\n    {file = \"certifi-2023.7.22-py3-none-any.whl\", hash = \"sha256:92d6037539857d8206b8f6ae472e8b77db8058fec5937a1ef3f54304089edbb9\"},\n    {file = \"certifi-2023.7.22.tar.gz\", hash = \"sha256:539cc1d13202e33ca466e88b2807e29f4c13049d6d87031a3c110744495cb082\"},\n]\n\n[[package]]\nname = \"cffi\"\nversion = \"1.16.0\"\ndescription = \"Foreign Function Interface for Python calling C code.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"cffi-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:6b3d6606d369fc1da4fd8c357d026317fbb9c9b75d36dc16e90e84c26854b088\"},\n    {file = \"cffi-1.16.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:ac0f5edd2360eea2f1daa9e26a41db02dd4b0451b48f7c318e217ee092a213e9\"},\n    {file = \"cffi-1.16.0-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7e61e3e4fa664a8588aa25c883eab612a188c725755afff6289454d6362b9673\"},\n    {file = \"cffi-1.16.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a72e8961a86d19bdb45851d8f1f08b041ea37d2bd8d4fd19903bc3083d80c896\"},\n    {file = \"cffi-1.16.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5b50bf3f55561dac5438f8e70bfcdfd74543fd60df5fa5f62d94e5867deca684\"},\n    {file = \"cffi-1.16.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:7651c50c8c5ef7bdb41108b7b8c5a83013bfaa8a935590c5d74627c047a583c7\"},\n    {file = \"cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e4108df7fe9b707191e55f33efbcb2d81928e10cea45527879a4749cbe472614\"},\n    {file = \"cffi-1.16.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:32c68ef735dbe5857c810328cb2481e24722a59a2003018885514d4c09af9743\"},\n    {file = \"cffi-1.16.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:673739cb539f8cdaa07d92d02efa93c9ccf87e345b9a0b556e3ecc666718468d\"},\n    {file = \"cffi-1.16.0-cp310-cp310-win32.whl\", hash = \"sha256:9f90389693731ff1f659e55c7d1640e2ec43ff725cc61b04b2f9c6d8d017df6a\"},\n    {file = \"cffi-1.16.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:e6024675e67af929088fda399b2094574609396b1decb609c55fa58b028a32a1\"},\n    {file = \"cffi-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:b84834d0cf97e7d27dd5b7f3aca7b6e9263c56308ab9dc8aae9784abb774d404\"},\n    {file = \"cffi-1.16.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:1b8ebc27c014c59692bb2664c7d13ce7a6e9a629be20e54e7271fa696ff2b417\"},\n    {file = \"cffi-1.16.0-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ee07e47c12890ef248766a6e55bd38ebfb2bb8edd4142d56db91b21ea68b7627\"},\n    {file = \"cffi-1.16.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d8a9d3ebe49f084ad71f9269834ceccbf398253c9fac910c4fd7053ff1386936\"},\n    {file = \"cffi-1.16.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e70f54f1796669ef691ca07d046cd81a29cb4deb1e5f942003f401c0c4a2695d\"},\n    {file = \"cffi-1.16.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5bf44d66cdf9e893637896c7faa22298baebcd18d1ddb6d2626a6e39793a1d56\"},\n    {file = \"cffi-1.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7b78010e7b97fef4bee1e896df8a4bbb6712b7f05b7ef630f9d1da00f6444d2e\"},\n    {file = \"cffi-1.16.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:c6a164aa47843fb1b01e941d385aab7215563bb8816d80ff3a363a9f8448a8dc\"},\n    {file = \"cffi-1.16.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:e09f3ff613345df5e8c3667da1d918f9149bd623cd9070c983c013792a9a62eb\"},\n    {file = \"cffi-1.16.0-cp311-cp311-win32.whl\", hash = \"sha256:2c56b361916f390cd758a57f2e16233eb4f64bcbeee88a4881ea90fca14dc6ab\"},\n    {file = \"cffi-1.16.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:db8e577c19c0fda0beb7e0d4e09e0ba74b1e4c092e0e40bfa12fe05b6f6d75ba\"},\n    {file = \"cffi-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:fa3a0128b152627161ce47201262d3140edb5a5c3da88d73a1b790a959126956\"},\n    {file = \"cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:68e7c44931cc171c54ccb702482e9fc723192e88d25a0e133edd7aff8fcd1f6e\"},\n    {file = \"cffi-1.16.0-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:abd808f9c129ba2beda4cfc53bde801e5bcf9d6e0f22f095e45327c038bfe68e\"},\n    {file = \"cffi-1.16.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:88e2b3c14bdb32e440be531ade29d3c50a1a59cd4e51b1dd8b0865c54ea5d2e2\"},\n    {file = \"cffi-1.16.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:fcc8eb6d5902bb1cf6dc4f187ee3ea80a1eba0a89aba40a5cb20a5087d961357\"},\n    {file = \"cffi-1.16.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b7be2d771cdba2942e13215c4e340bfd76398e9227ad10402a8767ab1865d2e6\"},\n    {file = \"cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e715596e683d2ce000574bae5d07bd522c781a822866c20495e52520564f0969\"},\n    {file = \"cffi-1.16.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:2d92b25dbf6cae33f65005baf472d2c245c050b1ce709cc4588cdcdd5495b520\"},\n    {file = \"cffi-1.16.0-cp312-cp312-win32.whl\", hash = \"sha256:b2ca4e77f9f47c55c194982e10f058db063937845bb2b7a86c84a6cfe0aefa8b\"},\n    {file = \"cffi-1.16.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:68678abf380b42ce21a5f2abde8efee05c114c2fdb2e9eef2efdb0257fba1235\"},\n    {file = \"cffi-1.16.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:0c9ef6ff37e974b73c25eecc13952c55bceed9112be2d9d938ded8e856138bcc\"},\n    {file = \"cffi-1.16.0-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a09582f178759ee8128d9270cd1344154fd473bb77d94ce0aeb2a93ebf0feaf0\"},\n    {file = \"cffi-1.16.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e760191dd42581e023a68b758769e2da259b5d52e3103c6060ddc02c9edb8d7b\"},\n    {file = \"cffi-1.16.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:80876338e19c951fdfed6198e70bc88f1c9758b94578d5a7c4c91a87af3cf31c\"},\n    {file = \"cffi-1.16.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:a6a14b17d7e17fa0d207ac08642c8820f84f25ce17a442fd15e27ea18d67c59b\"},\n    {file = \"cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6602bc8dc6f3a9e02b6c22c4fc1e47aa50f8f8e6d3f78a5e16ac33ef5fefa324\"},\n    {file = \"cffi-1.16.0-cp38-cp38-win32.whl\", hash = \"sha256:131fd094d1065b19540c3d72594260f118b231090295d8c34e19a7bbcf2e860a\"},\n    {file = \"cffi-1.16.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:31d13b0f99e0836b7ff893d37af07366ebc90b678b6664c955b54561fc36ef36\"},\n    {file = \"cffi-1.16.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:582215a0e9adbe0e379761260553ba11c58943e4bbe9c36430c4ca6ac74b15ed\"},\n    {file = \"cffi-1.16.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:b29ebffcf550f9da55bec9e02ad430c992a87e5f512cd63388abb76f1036d8d2\"},\n    {file = \"cffi-1.16.0-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:dc9b18bf40cc75f66f40a7379f6a9513244fe33c0e8aa72e2d56b0196a7ef872\"},\n    {file = \"cffi-1.16.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9cb4a35b3642fc5c005a6755a5d17c6c8b6bcb6981baf81cea8bfbc8903e8ba8\"},\n    {file = \"cffi-1.16.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b86851a328eedc692acf81fb05444bdf1891747c25af7529e39ddafaf68a4f3f\"},\n    {file = \"cffi-1.16.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c0f31130ebc2d37cdd8e44605fb5fa7ad59049298b3f745c74fa74c62fbfcfc4\"},\n    {file = \"cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8f8e709127c6c77446a8c0a8c8bf3c8ee706a06cd44b1e827c3e6a2ee6b8c098\"},\n    {file = \"cffi-1.16.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:748dcd1e3d3d7cd5443ef03ce8685043294ad6bd7c02a38d1bd367cfd968e000\"},\n    {file = \"cffi-1.16.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:8895613bcc094d4a1b2dbe179d88d7fb4a15cee43c052e8885783fac397d91fe\"},\n    {file = \"cffi-1.16.0-cp39-cp39-win32.whl\", hash = \"sha256:ed86a35631f7bfbb28e108dd96773b9d5a6ce4811cf6ea468bb6a359b256b1e4\"},\n    {file = \"cffi-1.16.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:3686dffb02459559c74dd3d81748269ffb0eb027c39a6fc99502de37d501faa8\"},\n    {file = \"cffi-1.16.0.tar.gz\", hash = \"sha256:bcb3ef43e58665bbda2fb198698fcae6776483e0c4a631aa5647806c25e02cc0\"},\n]\n\n[package.dependencies]\npycparser = \"*\"\n\n[[package]]\nname = \"charset-normalizer\"\nversion = \"3.3.0\"\ndescription = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"\noptional = false\npython-versions = \">=3.7.0\"\nfiles = [\n    {file = \"charset-normalizer-3.3.0.tar.gz\", hash = \"sha256:63563193aec44bce707e0c5ca64ff69fa72ed7cf34ce6e11d5127555756fd2f6\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:effe5406c9bd748a871dbcaf3ac69167c38d72db8c9baf3ff954c344f31c4cbe\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:4162918ef3098851fcd8a628bf9b6a98d10c380725df9e04caf5ca6dd48c847a\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:0570d21da019941634a531444364f2482e8db0b3425fcd5ac0c36565a64142c8\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5707a746c6083a3a74b46b3a631d78d129edab06195a92a8ece755aac25a3f3d\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:278c296c6f96fa686d74eb449ea1697f3c03dc28b75f873b65b5201806346a69\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:a4b71f4d1765639372a3b32d2638197f5cd5221b19531f9245fcc9ee62d38f56\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f5969baeaea61c97efa706b9b107dcba02784b1601c74ac84f2a532ea079403e\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a3f93dab657839dfa61025056606600a11d0b696d79386f974e459a3fbc568ec\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:db756e48f9c5c607b5e33dd36b1d5872d0422e960145b08ab0ec7fd420e9d649\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:232ac332403e37e4a03d209a3f92ed9071f7d3dbda70e2a5e9cff1c4ba9f0678\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e5c1502d4ace69a179305abb3f0bb6141cbe4714bc9b31d427329a95acfc8bdd\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:2502dd2a736c879c0f0d3e2161e74d9907231e25d35794584b1ca5284e43f596\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:23e8565ab7ff33218530bc817922fae827420f143479b753104ab801145b1d5b\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-win32.whl\", hash = \"sha256:1872d01ac8c618a8da634e232f24793883d6e456a66593135aeafe3784b0848d\"},\n    {file = \"charset_normalizer-3.3.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:557b21a44ceac6c6b9773bc65aa1b4cc3e248a5ad2f5b914b91579a32e22204d\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:d7eff0f27edc5afa9e405f7165f85a6d782d308f3b6b9d96016c010597958e63\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:6a685067d05e46641d5d1623d7c7fdf15a357546cbb2f71b0ebde91b175ffc3e\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:0d3d5b7db9ed8a2b11a774db2bbea7ba1884430a205dbd54a32d61d7c2a190fa\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2935ffc78db9645cb2086c2f8f4cfd23d9b73cc0dc80334bc30aac6f03f68f8c\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9fe359b2e3a7729010060fbca442ca225280c16e923b37db0e955ac2a2b72a05\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:380c4bde80bce25c6e4f77b19386f5ec9db230df9f2f2ac1e5ad7af2caa70459\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f0d1e3732768fecb052d90d62b220af62ead5748ac51ef61e7b32c266cac9293\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1b2919306936ac6efb3aed1fbf81039f7087ddadb3160882a57ee2ff74fd2382\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:f8888e31e3a85943743f8fc15e71536bda1c81d5aa36d014a3c0c44481d7db6e\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:82eb849f085624f6a607538ee7b83a6d8126df6d2f7d3b319cb837b289123078\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7b8b8bf1189b3ba9b8de5c8db4d541b406611a71a955bbbd7385bbc45fcb786c\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:5adf257bd58c1b8632046bbe43ee38c04e1038e9d37de9c57a94d6bd6ce5da34\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:c350354efb159b8767a6244c166f66e67506e06c8924ed74669b2c70bc8735b1\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-win32.whl\", hash = \"sha256:02af06682e3590ab952599fbadac535ede5d60d78848e555aa58d0c0abbde786\"},\n    {file = \"charset_normalizer-3.3.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:86d1f65ac145e2c9ed71d8ffb1905e9bba3a91ae29ba55b4c46ae6fc31d7c0d4\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:3b447982ad46348c02cb90d230b75ac34e9886273df3a93eec0539308a6296d7\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:abf0d9f45ea5fb95051c8bfe43cb40cda383772f7e5023a83cc481ca2604d74e\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:b09719a17a2301178fac4470d54b1680b18a5048b481cb8890e1ef820cb80455\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b3d9b48ee6e3967b7901c052b670c7dda6deb812c309439adaffdec55c6d7b78\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:edfe077ab09442d4ef3c52cb1f9dab89bff02f4524afc0acf2d46be17dc479f5\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3debd1150027933210c2fc321527c2299118aa929c2f5a0a80ab6953e3bd1908\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:86f63face3a527284f7bb8a9d4f78988e3c06823f7bea2bd6f0e0e9298ca0403\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:24817cb02cbef7cd499f7c9a2735286b4782bd47a5b3516a0e84c50eab44b98e\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:c71f16da1ed8949774ef79f4a0260d28b83b3a50c6576f8f4f0288d109777989\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:9cf3126b85822c4e53aa28c7ec9869b924d6fcfb76e77a45c44b83d91afd74f9\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:b3b2316b25644b23b54a6f6401074cebcecd1244c0b8e80111c9a3f1c8e83d65\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:03680bb39035fbcffe828eae9c3f8afc0428c91d38e7d61aa992ef7a59fb120e\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:4cc152c5dd831641e995764f9f0b6589519f6f5123258ccaca8c6d34572fefa8\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-win32.whl\", hash = \"sha256:b8f3307af845803fb0b060ab76cf6dd3a13adc15b6b451f54281d25911eb92df\"},\n    {file = \"charset_normalizer-3.3.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:8eaf82f0eccd1505cf39a45a6bd0a8cf1c70dcfc30dba338207a969d91b965c0\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:dc45229747b67ffc441b3de2f3ae5e62877a282ea828a5bdb67883c4ee4a8810\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2f4a0033ce9a76e391542c182f0d48d084855b5fcba5010f707c8e8c34663d77\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ada214c6fa40f8d800e575de6b91a40d0548139e5dc457d2ebb61470abf50186\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b1121de0e9d6e6ca08289583d7491e7fcb18a439305b34a30b20d8215922d43c\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1063da2c85b95f2d1a430f1c33b55c9c17ffaf5e612e10aeaad641c55a9e2b9d\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:70f1d09c0d7748b73290b29219e854b3207aea922f839437870d8cc2168e31cc\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:250c9eb0f4600361dd80d46112213dff2286231d92d3e52af1e5a6083d10cad9\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:750b446b2ffce1739e8578576092179160f6d26bd5e23eb1789c4d64d5af7dc7\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:fc52b79d83a3fe3a360902d3f5d79073a993597d48114c29485e9431092905d8\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:588245972aca710b5b68802c8cad9edaa98589b1b42ad2b53accd6910dad3545\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:e39c7eb31e3f5b1f88caff88bcff1b7f8334975b46f6ac6e9fc725d829bc35d4\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-win32.whl\", hash = \"sha256:abecce40dfebbfa6abf8e324e1860092eeca6f7375c8c4e655a8afb61af58f2c\"},\n    {file = \"charset_normalizer-3.3.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:24a91a981f185721542a0b7c92e9054b7ab4fea0508a795846bc5b0abf8118d4\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:67b8cc9574bb518ec76dc8e705d4c39ae78bb96237cb533edac149352c1f39fe\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:ac71b2977fb90c35d41c9453116e283fac47bb9096ad917b8819ca8b943abecd\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:3ae38d325b512f63f8da31f826e6cb6c367336f95e418137286ba362925c877e\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:542da1178c1c6af8873e143910e2269add130a299c9106eef2594e15dae5e482\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:30a85aed0b864ac88309b7d94be09f6046c834ef60762a8833b660139cfbad13\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:aae32c93e0f64469f74ccc730a7cb21c7610af3a775157e50bbd38f816536b38\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:15b26ddf78d57f1d143bdf32e820fd8935d36abe8a25eb9ec0b5a71c82eb3895\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7f5d10bae5d78e4551b7be7a9b29643a95aded9d0f602aa2ba584f0388e7a557\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:249c6470a2b60935bafd1d1d13cd613f8cd8388d53461c67397ee6a0f5dce741\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:c5a74c359b2d47d26cdbbc7845e9662d6b08a1e915eb015d044729e92e7050b7\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:b5bcf60a228acae568e9911f410f9d9e0d43197d030ae5799e20dca8df588287\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:187d18082694a29005ba2944c882344b6748d5be69e3a89bf3cc9d878e548d5a\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:81bf654678e575403736b85ba3a7867e31c2c30a69bc57fe88e3ace52fb17b89\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-win32.whl\", hash = \"sha256:85a32721ddde63c9df9ebb0d2045b9691d9750cb139c161c80e500d210f5e26e\"},\n    {file = \"charset_normalizer-3.3.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:468d2a840567b13a590e67dd276c570f8de00ed767ecc611994c301d0f8c014f\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:e0fc42822278451bc13a2e8626cf2218ba570f27856b536e00cfa53099724828\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:09c77f964f351a7369cc343911e0df63e762e42bac24cd7d18525961c81754f4\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:12ebea541c44fdc88ccb794a13fe861cc5e35d64ed689513a5c03d05b53b7c82\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:805dfea4ca10411a5296bcc75638017215a93ffb584c9e344731eef0dcfb026a\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:96c2b49eb6a72c0e4991d62406e365d87067ca14c1a729a870d22354e6f68115\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:aaf7b34c5bc56b38c931a54f7952f1ff0ae77a2e82496583b247f7c969eb1479\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:619d1c96099be5823db34fe89e2582b336b5b074a7f47f819d6b3a57ff7bdb86\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a0ac5e7015a5920cfce654c06618ec40c33e12801711da6b4258af59a8eff00a\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:93aa7eef6ee71c629b51ef873991d6911b906d7312c6e8e99790c0f33c576f89\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:7966951325782121e67c81299a031f4c115615e68046f79b85856b86ebffc4cd\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:02673e456dc5ab13659f85196c534dc596d4ef260e4d86e856c3b2773ce09843\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:c2af80fb58f0f24b3f3adcb9148e6203fa67dd3f61c4af146ecad033024dde43\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:153e7b6e724761741e0974fc4dcd406d35ba70b92bfe3fedcb497226c93b9da7\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-win32.whl\", hash = \"sha256:d47ecf253780c90ee181d4d871cd655a789da937454045b17b5798da9393901a\"},\n    {file = \"charset_normalizer-3.3.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:d97d85fa63f315a8bdaba2af9a6a686e0eceab77b3089af45133252618e70884\"},\n    {file = \"charset_normalizer-3.3.0-py3-none-any.whl\", hash = \"sha256:e46cd37076971c1040fc8c41273a8b3e2c624ce4f2be3f5dfcb7a430c1d3acc2\"},\n]\n\n[[package]]\nname = \"colorama\"\nversion = \"0.4.6\"\ndescription = \"Cross-platform colored terminal text.\"\noptional = false\npython-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"\nfiles = [\n    {file = \"colorama-0.4.6-py2.py3-none-any.whl\", hash = \"sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6\"},\n    {file = \"colorama-0.4.6.tar.gz\", hash = \"sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44\"},\n]\n\n[[package]]\nname = \"coverage\"\nversion = \"7.3.2\"\ndescription = \"Code coverage measurement for Python\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"coverage-7.3.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:d872145f3a3231a5f20fd48500274d7df222e291d90baa2026cc5152b7ce86bf\"},\n    {file = \"coverage-7.3.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:310b3bb9c91ea66d59c53fa4989f57d2436e08f18fb2f421a1b0b6b8cc7fffda\"},\n    {file = \"coverage-7.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f47d39359e2c3779c5331fc740cf4bce6d9d680a7b4b4ead97056a0ae07cb49a\"},\n    {file = \"coverage-7.3.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:aa72dbaf2c2068404b9870d93436e6d23addd8bbe9295f49cbca83f6e278179c\"},\n    {file = \"coverage-7.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:beaa5c1b4777f03fc63dfd2a6bd820f73f036bfb10e925fce067b00a340d0f3f\"},\n    {file = \"coverage-7.3.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:dbc1b46b92186cc8074fee9d9fbb97a9dd06c6cbbef391c2f59d80eabdf0faa6\"},\n    {file = \"coverage-7.3.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:315a989e861031334d7bee1f9113c8770472db2ac484e5b8c3173428360a9148\"},\n    {file = \"coverage-7.3.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:d1bc430677773397f64a5c88cb522ea43175ff16f8bfcc89d467d974cb2274f9\"},\n    {file = \"coverage-7.3.2-cp310-cp310-win32.whl\", hash = \"sha256:a889ae02f43aa45032afe364c8ae84ad3c54828c2faa44f3bfcafecb5c96b02f\"},\n    {file = \"coverage-7.3.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:c0ba320de3fb8c6ec16e0be17ee1d3d69adcda99406c43c0409cb5c41788a611\"},\n    {file = \"coverage-7.3.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:ac8c802fa29843a72d32ec56d0ca792ad15a302b28ca6203389afe21f8fa062c\"},\n    {file = \"coverage-7.3.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:89a937174104339e3a3ffcf9f446c00e3a806c28b1841c63edb2b369310fd074\"},\n    {file = \"coverage-7.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e267e9e2b574a176ddb983399dec325a80dbe161f1a32715c780b5d14b5f583a\"},\n    {file = \"coverage-7.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2443cbda35df0d35dcfb9bf8f3c02c57c1d6111169e3c85fc1fcc05e0c9f39a3\"},\n    {file = \"coverage-7.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4175e10cc8dda0265653e8714b3174430b07c1dca8957f4966cbd6c2b1b8065a\"},\n    {file = \"coverage-7.3.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:0cbf38419fb1a347aaf63481c00f0bdc86889d9fbf3f25109cf96c26b403fda1\"},\n    {file = \"coverage-7.3.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:5c913b556a116b8d5f6ef834038ba983834d887d82187c8f73dec21049abd65c\"},\n    {file = \"coverage-7.3.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:1981f785239e4e39e6444c63a98da3a1db8e971cb9ceb50a945ba6296b43f312\"},\n    {file = \"coverage-7.3.2-cp311-cp311-win32.whl\", hash = \"sha256:43668cabd5ca8258f5954f27a3aaf78757e6acf13c17604d89648ecc0cc66640\"},\n    {file = \"coverage-7.3.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:e10c39c0452bf6e694511c901426d6b5ac005acc0f78ff265dbe36bf81f808a2\"},\n    {file = \"coverage-7.3.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:4cbae1051ab791debecc4a5dcc4a1ff45fc27b91b9aee165c8a27514dd160836\"},\n    {file = \"coverage-7.3.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:12d15ab5833a997716d76f2ac1e4b4d536814fc213c85ca72756c19e5a6b3d63\"},\n    {file = \"coverage-7.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3c7bba973ebee5e56fe9251300c00f1579652587a9f4a5ed8404b15a0471f216\"},\n    {file = \"coverage-7.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:fe494faa90ce6381770746077243231e0b83ff3f17069d748f645617cefe19d4\"},\n    {file = \"coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f6e9589bd04d0461a417562649522575d8752904d35c12907d8c9dfeba588faf\"},\n    {file = \"coverage-7.3.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:d51ac2a26f71da1b57f2dc81d0e108b6ab177e7d30e774db90675467c847bbdf\"},\n    {file = \"coverage-7.3.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:99b89d9f76070237975b315b3d5f4d6956ae354a4c92ac2388a5695516e47c84\"},\n    {file = \"coverage-7.3.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:fa28e909776dc69efb6ed975a63691bc8172b64ff357e663a1bb06ff3c9b589a\"},\n    {file = \"coverage-7.3.2-cp312-cp312-win32.whl\", hash = \"sha256:289fe43bf45a575e3ab10b26d7b6f2ddb9ee2dba447499f5401cfb5ecb8196bb\"},\n    {file = \"coverage-7.3.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:7dbc3ed60e8659bc59b6b304b43ff9c3ed858da2839c78b804973f613d3e92ed\"},\n    {file = \"coverage-7.3.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:f94b734214ea6a36fe16e96a70d941af80ff3bfd716c141300d95ebc85339738\"},\n    {file = \"coverage-7.3.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:af3d828d2c1cbae52d34bdbb22fcd94d1ce715d95f1a012354a75e5913f1bda2\"},\n    {file = \"coverage-7.3.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:630b13e3036e13c7adc480ca42fa7afc2a5d938081d28e20903cf7fd687872e2\"},\n    {file = \"coverage-7.3.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c9eacf273e885b02a0273bb3a2170f30e2d53a6d53b72dbe02d6701b5296101c\"},\n    {file = \"coverage-7.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d8f17966e861ff97305e0801134e69db33b143bbfb36436efb9cfff6ec7b2fd9\"},\n    {file = \"coverage-7.3.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:b4275802d16882cf9c8b3d057a0839acb07ee9379fa2749eca54efbce1535b82\"},\n    {file = \"coverage-7.3.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:72c0cfa5250f483181e677ebc97133ea1ab3eb68645e494775deb6a7f6f83901\"},\n    {file = \"coverage-7.3.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:cb536f0dcd14149425996821a168f6e269d7dcd2c273a8bff8201e79f5104e76\"},\n    {file = \"coverage-7.3.2-cp38-cp38-win32.whl\", hash = \"sha256:307adb8bd3abe389a471e649038a71b4eb13bfd6b7dd9a129fa856f5c695cf92\"},\n    {file = \"coverage-7.3.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:88ed2c30a49ea81ea3b7f172e0269c182a44c236eb394718f976239892c0a27a\"},\n    {file = \"coverage-7.3.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:b631c92dfe601adf8f5ebc7fc13ced6bb6e9609b19d9a8cd59fa47c4186ad1ce\"},\n    {file = \"coverage-7.3.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:d3d9df4051c4a7d13036524b66ecf7a7537d14c18a384043f30a303b146164e9\"},\n    {file = \"coverage-7.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5f7363d3b6a1119ef05015959ca24a9afc0ea8a02c687fe7e2d557705375c01f\"},\n    {file = \"coverage-7.3.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2f11cc3c967a09d3695d2a6f03fb3e6236622b93be7a4b5dc09166a861be6d25\"},\n    {file = \"coverage-7.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:149de1d2401ae4655c436a3dced6dd153f4c3309f599c3d4bd97ab172eaf02d9\"},\n    {file = \"coverage-7.3.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:3a4006916aa6fee7cd38db3bfc95aa9c54ebb4ffbfc47c677c8bba949ceba0a6\"},\n    {file = \"coverage-7.3.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:9028a3871280110d6e1aa2df1afd5ef003bab5fb1ef421d6dc748ae1c8ef2ebc\"},\n    {file = \"coverage-7.3.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:9f805d62aec8eb92bab5b61c0f07329275b6f41c97d80e847b03eb894f38d083\"},\n    {file = \"coverage-7.3.2-cp39-cp39-win32.whl\", hash = \"sha256:d1c88ec1a7ff4ebca0219f5b1ef863451d828cccf889c173e1253aa84b1e07ce\"},\n    {file = \"coverage-7.3.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:b4767da59464bb593c07afceaddea61b154136300881844768037fd5e859353f\"},\n    {file = \"coverage-7.3.2-pp38.pp39.pp310-none-any.whl\", hash = \"sha256:ae97af89f0fbf373400970c0a21eef5aa941ffeed90aee43650b81f7d7f47637\"},\n    {file = \"coverage-7.3.2.tar.gz\", hash = \"sha256:be32ad29341b0170e795ca590e1c07e81fc061cb5b10c74ce7203491484404ef\"},\n]\n\n[package.dependencies]\ntomli = {version = \"*\", optional = true, markers = \"python_full_version <= \\\"3.11.0a6\\\" and extra == \\\"toml\\\"\"}\n\n[package.extras]\ntoml = [\"tomli\"]\n\n[[package]]\nname = \"cryptography\"\nversion = \"41.0.4\"\ndescription = \"cryptography is a package which provides cryptographic recipes and primitives to Python developers.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"cryptography-41.0.4-cp37-abi3-macosx_10_12_universal2.whl\", hash = \"sha256:80907d3faa55dc5434a16579952ac6da800935cd98d14dbd62f6f042c7f5e839\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-macosx_10_12_x86_64.whl\", hash = \"sha256:35c00f637cd0b9d5b6c6bd11b6c3359194a8eba9c46d4e875a3660e3b400005f\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:cecfefa17042941f94ab54f769c8ce0fe14beff2694e9ac684176a2535bf9714\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e40211b4923ba5a6dc9769eab704bdb3fbb58d56c5b336d30996c24fcf12aadb\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-manylinux_2_28_aarch64.whl\", hash = \"sha256:23a25c09dfd0d9f28da2352503b23e086f8e78096b9fd585d1d14eca01613e13\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-manylinux_2_28_x86_64.whl\", hash = \"sha256:2ed09183922d66c4ec5fdaa59b4d14e105c084dd0febd27452de8f6f74704143\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-musllinux_1_1_aarch64.whl\", hash = \"sha256:5a0f09cefded00e648a127048119f77bc2b2ec61e736660b5789e638f43cc397\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-musllinux_1_1_x86_64.whl\", hash = \"sha256:9eeb77214afae972a00dee47382d2591abe77bdae166bda672fb1e24702a3860\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-win32.whl\", hash = \"sha256:3b224890962a2d7b57cf5eeb16ccaafba6083f7b811829f00476309bce2fe0fd\"},\n    {file = \"cryptography-41.0.4-cp37-abi3-win_amd64.whl\", hash = \"sha256:c880eba5175f4307129784eca96f4e70b88e57aa3f680aeba3bab0e980b0f37d\"},\n    {file = \"cryptography-41.0.4-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:004b6ccc95943f6a9ad3142cfabcc769d7ee38a3f60fb0dddbfb431f818c3a67\"},\n    {file = \"cryptography-41.0.4-pp310-pypy310_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:86defa8d248c3fa029da68ce61fe735432b047e32179883bdb1e79ed9bb8195e\"},\n    {file = \"cryptography-41.0.4-pp310-pypy310_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:37480760ae08065437e6573d14be973112c9e6dcaf5f11d00147ee74f37a3829\"},\n    {file = \"cryptography-41.0.4-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:b5f4dfe950ff0479f1f00eda09c18798d4f49b98f4e2006d644b3301682ebdca\"},\n    {file = \"cryptography-41.0.4-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:7e53db173370dea832190870e975a1e09c86a879b613948f09eb49324218c14d\"},\n    {file = \"cryptography-41.0.4-pp38-pypy38_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:5b72205a360f3b6176485a333256b9bcd48700fc755fef51c8e7e67c4b63e3ac\"},\n    {file = \"cryptography-41.0.4-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:93530900d14c37a46ce3d6c9e6fd35dbe5f5601bf6b3a5c325c7bffc030344d9\"},\n    {file = \"cryptography-41.0.4-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:efc8ad4e6fc4f1752ebfb58aefece8b4e3c4cae940b0994d43649bdfce8d0d4f\"},\n    {file = \"cryptography-41.0.4-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:c3391bd8e6de35f6f1140e50aaeb3e2b3d6a9012536ca23ab0d9c35ec18c8a91\"},\n    {file = \"cryptography-41.0.4-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl\", hash = \"sha256:0d9409894f495d465fe6fda92cb70e8323e9648af912d5b9141d616df40a87b8\"},\n    {file = \"cryptography-41.0.4-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:8ac4f9ead4bbd0bc8ab2d318f97d85147167a488be0e08814a37eb2f439d5cf6\"},\n    {file = \"cryptography-41.0.4-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:047c4603aeb4bbd8db2756e38f5b8bd7e94318c047cfe4efeb5d715e08b49311\"},\n    {file = \"cryptography-41.0.4.tar.gz\", hash = \"sha256:7febc3094125fc126a7f6fb1f420d0da639f3f32cb15c8ff0dc3997c4549f51a\"},\n]\n\n[package.dependencies]\ncffi = \">=1.12\"\n\n[package.extras]\ndocs = [\"sphinx (>=5.3.0)\", \"sphinx-rtd-theme (>=1.1.1)\"]\ndocstest = [\"pyenchant (>=1.6.11)\", \"sphinxcontrib-spelling (>=4.0.1)\", \"twine (>=1.12.0)\"]\nnox = [\"nox\"]\npep8test = [\"black\", \"check-sdist\", \"mypy\", \"ruff\"]\nsdist = [\"build\"]\nssh = [\"bcrypt (>=3.1.5)\"]\ntest = [\"pretend\", \"pytest (>=6.2.0)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-xdist\"]\ntest-randomorder = [\"pytest-randomly\"]\n\n[[package]]\nname = \"exceptiongroup\"\nversion = \"1.1.3\"\ndescription = \"Backport of PEP 654 (exception groups)\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"exceptiongroup-1.1.3-py3-none-any.whl\", hash = \"sha256:343280667a4585d195ca1cf9cef84a4e178c4b6cf2274caef9859782b567d5e3\"},\n    {file = \"exceptiongroup-1.1.3.tar.gz\", hash = \"sha256:097acd85d473d75af5bb98e41b61ff7fe35efe6675e4f9370ec6ec5126d160e9\"},\n]\n\n[package.extras]\ntest = [\"pytest (>=6)\"]\n\n[[package]]\nname = \"frozenlist\"\nversion = \"1.4.0\"\ndescription = \"A list-like structure which implements collections.abc.MutableSequence\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"frozenlist-1.4.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:764226ceef3125e53ea2cb275000e309c0aa5464d43bd72abd661e27fffc26ab\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:d6484756b12f40003c6128bfcc3fa9f0d49a687e171186c2d85ec82e3758c559\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9ac08e601308e41eb533f232dbf6b7e4cea762f9f84f6357136eed926c15d12c\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d081f13b095d74b67d550de04df1c756831f3b83dc9881c38985834387487f1b\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:71932b597f9895f011f47f17d6428252fc728ba2ae6024e13c3398a087c2cdea\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:981b9ab5a0a3178ff413bca62526bb784249421c24ad7381e39d67981be2c326\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:e41f3de4df3e80de75845d3e743b3f1c4c8613c3997a912dbf0229fc61a8b963\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6918d49b1f90821e93069682c06ffde41829c346c66b721e65a5c62b4bab0300\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:0e5c8764c7829343d919cc2dfc587a8db01c4f70a4ebbc49abde5d4b158b007b\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:8d0edd6b1c7fb94922bf569c9b092ee187a83f03fb1a63076e7774b60f9481a8\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e29cda763f752553fa14c68fb2195150bfab22b352572cb36c43c47bedba70eb\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:0c7c1b47859ee2cac3846fde1c1dc0f15da6cec5a0e5c72d101e0f83dcb67ff9\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:901289d524fdd571be1c7be054f48b1f88ce8dddcbdf1ec698b27d4b8b9e5d62\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-win32.whl\", hash = \"sha256:1a0848b52815006ea6596c395f87449f693dc419061cc21e970f139d466dc0a0\"},\n    {file = \"frozenlist-1.4.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:b206646d176a007466358aa21d85cd8600a415c67c9bd15403336c331a10d956\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:de343e75f40e972bae1ef6090267f8260c1446a1695e77096db6cfa25e759a95\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:ad2a9eb6d9839ae241701d0918f54c51365a51407fd80f6b8289e2dfca977cc3\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:bd7bd3b3830247580de99c99ea2a01416dfc3c34471ca1298bccabf86d0ff4dc\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bdf1847068c362f16b353163391210269e4f0569a3c166bc6a9f74ccbfc7e839\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:38461d02d66de17455072c9ba981d35f1d2a73024bee7790ac2f9e361ef1cd0c\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d5a32087d720c608f42caed0ef36d2b3ea61a9d09ee59a5142d6070da9041b8f\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:dd65632acaf0d47608190a71bfe46b209719bf2beb59507db08ccdbe712f969b\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:261b9f5d17cac914531331ff1b1d452125bf5daa05faf73b71d935485b0c510b\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:b89ac9768b82205936771f8d2eb3ce88503b1556324c9f903e7156669f521472\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:008eb8b31b3ea6896da16c38c1b136cb9fec9e249e77f6211d479db79a4eaf01\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e74b0506fa5aa5598ac6a975a12aa8928cbb58e1f5ac8360792ef15de1aa848f\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:490132667476f6781b4c9458298b0c1cddf237488abd228b0b3650e5ecba7467\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:76d4711f6f6d08551a7e9ef28c722f4a50dd0fc204c56b4bcd95c6cc05ce6fbb\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-win32.whl\", hash = \"sha256:a02eb8ab2b8f200179b5f62b59757685ae9987996ae549ccf30f983f40602431\"},\n    {file = \"frozenlist-1.4.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:515e1abc578dd3b275d6a5114030b1330ba044ffba03f94091842852f806f1c1\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:f0ed05f5079c708fe74bf9027e95125334b6978bf07fd5ab923e9e55e5fbb9d3\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:ca265542ca427bf97aed183c1676e2a9c66942e822b14dc6e5f42e038f92a503\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:491e014f5c43656da08958808588cc6c016847b4360e327a62cb308c791bd2d9\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:17ae5cd0f333f94f2e03aaf140bb762c64783935cc764ff9c82dff626089bebf\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1e78fb68cf9c1a6aa4a9a12e960a5c9dfbdb89b3695197aa7064705662515de2\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d5655a942f5f5d2c9ed93d72148226d75369b4f6952680211972a33e59b1dfdc\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c11b0746f5d946fecf750428a95f3e9ebe792c1ee3b1e96eeba145dc631a9672\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e66d2a64d44d50d2543405fb183a21f76b3b5fd16f130f5c99187c3fb4e64919\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:88f7bc0fcca81f985f78dd0fa68d2c75abf8272b1f5c323ea4a01a4d7a614efc\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:5833593c25ac59ede40ed4de6d67eb42928cca97f26feea219f21d0ed0959b79\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:fec520865f42e5c7f050c2a79038897b1c7d1595e907a9e08e3353293ffc948e\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:b826d97e4276750beca7c8f0f1a4938892697a6bcd8ec8217b3312dad6982781\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:ceb6ec0a10c65540421e20ebd29083c50e6d1143278746a4ef6bcf6153171eb8\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-win32.whl\", hash = \"sha256:2b8bcf994563466db019fab287ff390fffbfdb4f905fc77bc1c1d604b1c689cc\"},\n    {file = \"frozenlist-1.4.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:a6c8097e01886188e5be3e6b14e94ab365f384736aa1fca6a0b9e35bd4a30bc7\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:6c38721585f285203e4b4132a352eb3daa19121a035f3182e08e437cface44bf\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:a0c6da9aee33ff0b1a451e867da0c1f47408112b3391dd43133838339e410963\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:93ea75c050c5bb3d98016b4ba2497851eadf0ac154d88a67d7a6816206f6fa7f\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f61e2dc5ad442c52b4887f1fdc112f97caeff4d9e6ebe78879364ac59f1663e1\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:aa384489fefeb62321b238e64c07ef48398fe80f9e1e6afeff22e140e0850eef\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:10ff5faaa22786315ef57097a279b833ecab1a0bfb07d604c9cbb1c4cdc2ed87\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:007df07a6e3eb3e33e9a1fe6a9db7af152bbd8a185f9aaa6ece10a3529e3e1c6\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7f4f399d28478d1f604c2ff9119907af9726aed73680e5ed1ca634d377abb087\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:c5374b80521d3d3f2ec5572e05adc94601985cc526fb276d0c8574a6d749f1b3\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:ce31ae3e19f3c902de379cf1323d90c649425b86de7bbdf82871b8a2a0615f3d\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7211ef110a9194b6042449431e08c4d80c0481e5891e58d429df5899690511c2\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:556de4430ce324c836789fa4560ca62d1591d2538b8ceb0b4f68fb7b2384a27a\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:7645a8e814a3ee34a89c4a372011dcd817964ce8cb273c8ed6119d706e9613e3\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-win32.whl\", hash = \"sha256:19488c57c12d4e8095a922f328df3f179c820c212940a498623ed39160bc3c2f\"},\n    {file = \"frozenlist-1.4.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:6221d84d463fb110bdd7619b69cb43878a11d51cbb9394ae3105d082d5199167\"},\n    {file = \"frozenlist-1.4.0.tar.gz\", hash = \"sha256:09163bdf0b2907454042edb19f887c6d33806adc71fbd54afc14908bfdc22251\"},\n]\n\n[[package]]\nname = \"graphviz\"\nversion = \"0.20.1\"\ndescription = \"Simple Python interface for Graphviz\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"graphviz-0.20.1-py3-none-any.whl\", hash = \"sha256:587c58a223b51611c0cf461132da386edd896a029524ca61a1462b880bf97977\"},\n    {file = \"graphviz-0.20.1.zip\", hash = \"sha256:8c58f14adaa3b947daf26c19bc1e98c4e0702cdc31cf99153e6f06904d492bf8\"},\n]\n\n[package.extras]\ndev = [\"flake8\", \"pep8-naming\", \"tox (>=3)\", \"twine\", \"wheel\"]\ndocs = [\"sphinx (>=5)\", \"sphinx-autodoc-typehints\", \"sphinx-rtd-theme\"]\ntest = [\"coverage\", \"mock (>=4)\", \"pytest (>=7)\", \"pytest-cov\", \"pytest-mock (>=3)\"]\n\n[[package]]\nname = \"h11\"\nversion = \"0.14.0\"\ndescription = \"A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"h11-0.14.0-py3-none-any.whl\", hash = \"sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761\"},\n    {file = \"h11-0.14.0.tar.gz\", hash = \"sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d\"},\n]\n\n[[package]]\nname = \"httpcore\"\nversion = \"0.18.0\"\ndescription = \"A minimal low-level HTTP client.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"httpcore-0.18.0-py3-none-any.whl\", hash = \"sha256:adc5398ee0a476567bf87467063ee63584a8bce86078bf748e48754f60202ced\"},\n    {file = \"httpcore-0.18.0.tar.gz\", hash = \"sha256:13b5e5cd1dca1a6636a6aaea212b19f4f85cd88c366a2b82304181b769aab3c9\"},\n]\n\n[package.dependencies]\nanyio = \">=3.0,<5.0\"\ncertifi = \"*\"\nh11 = \">=0.13,<0.15\"\nsniffio = \"==1.*\"\n\n[package.extras]\nhttp2 = [\"h2 (>=3,<5)\"]\nsocks = [\"socksio (==1.*)\"]\n\n[[package]]\nname = \"httpx\"\nversion = \"0.25.0\"\ndescription = \"The next generation HTTP client.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"httpx-0.25.0-py3-none-any.whl\", hash = \"sha256:181ea7f8ba3a82578be86ef4171554dd45fec26a02556a744db029a0a27b7100\"},\n    {file = \"httpx-0.25.0.tar.gz\", hash = \"sha256:47ecda285389cb32bb2691cc6e069e3ab0205956f681c5b2ad2325719751d875\"},\n]\n\n[package.dependencies]\ncertifi = \"*\"\nhttpcore = \">=0.18.0,<0.19.0\"\nidna = \"*\"\nsniffio = \"*\"\n\n[package.extras]\nbrotli = [\"brotli\", \"brotlicffi\"]\ncli = [\"click (==8.*)\", \"pygments (==2.*)\", \"rich (>=10,<14)\"]\nhttp2 = [\"h2 (>=3,<5)\"]\nsocks = [\"socksio (==1.*)\"]\n\n[[package]]\nname = \"idna\"\nversion = \"3.4\"\ndescription = \"Internationalized Domain Names in Applications (IDNA)\"\noptional = false\npython-versions = \">=3.5\"\nfiles = [\n    {file = \"idna-3.4-py3-none-any.whl\", hash = \"sha256:90b77e79eaa3eba6de819a0c442c0b4ceefc341a7a2ab77d7562bf49f425c5c2\"},\n    {file = \"idna-3.4.tar.gz\", hash = \"sha256:814f528e8dead7d329833b91c5faa87d60bf71824cd12a7530b5526063d02cb4\"},\n]\n\n[[package]]\nname = \"iniconfig\"\nversion = \"2.0.0\"\ndescription = \"brain-dead simple config-ini parsing\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"iniconfig-2.0.0-py3-none-any.whl\", hash = \"sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\"},\n    {file = \"iniconfig-2.0.0.tar.gz\", hash = \"sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3\"},\n]\n\n[[package]]\nname = \"multidict\"\nversion = \"6.0.4\"\ndescription = \"multidict implementation\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"multidict-6.0.4-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:0b1a97283e0c85772d613878028fec909f003993e1007eafa715b24b377cb9b8\"},\n    {file = \"multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:eeb6dcc05e911516ae3d1f207d4b0520d07f54484c49dfc294d6e7d63b734171\"},\n    {file = \"multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:d6d635d5209b82a3492508cf5b365f3446afb65ae7ebd755e70e18f287b0adf7\"},\n    {file = \"multidict-6.0.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c048099e4c9e9d615545e2001d3d8a4380bd403e1a0578734e0d31703d1b0c0b\"},\n    {file = \"multidict-6.0.4-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ea20853c6dbbb53ed34cb4d080382169b6f4554d394015f1bef35e881bf83547\"},\n    {file = \"multidict-6.0.4-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:16d232d4e5396c2efbbf4f6d4df89bfa905eb0d4dc5b3549d872ab898451f569\"},\n    {file = \"multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:36c63aaa167f6c6b04ef2c85704e93af16c11d20de1d133e39de6a0e84582a93\"},\n    {file = \"multidict-6.0.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:64bdf1086b6043bf519869678f5f2757f473dee970d7abf6da91ec00acb9cb98\"},\n    {file = \"multidict-6.0.4-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:43644e38f42e3af682690876cff722d301ac585c5b9e1eacc013b7a3f7b696a0\"},\n    {file = \"multidict-6.0.4-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:7582a1d1030e15422262de9f58711774e02fa80df0d1578995c76214f6954988\"},\n    {file = \"multidict-6.0.4-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:ddff9c4e225a63a5afab9dd15590432c22e8057e1a9a13d28ed128ecf047bbdc\"},\n    {file = \"multidict-6.0.4-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:ee2a1ece51b9b9e7752e742cfb661d2a29e7bcdba2d27e66e28a99f1890e4fa0\"},\n    {file = \"multidict-6.0.4-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:a2e4369eb3d47d2034032a26c7a80fcb21a2cb22e1173d761a162f11e562caa5\"},\n    {file = \"multidict-6.0.4-cp310-cp310-win32.whl\", hash = \"sha256:574b7eae1ab267e5f8285f0fe881f17efe4b98c39a40858247720935b893bba8\"},\n    {file = \"multidict-6.0.4-cp310-cp310-win_amd64.whl\", hash = \"sha256:4dcbb0906e38440fa3e325df2359ac6cb043df8e58c965bb45f4e406ecb162cc\"},\n    {file = \"multidict-6.0.4-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:0dfad7a5a1e39c53ed00d2dd0c2e36aed4650936dc18fd9a1826a5ae1cad6f03\"},\n    {file = \"multidict-6.0.4-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:64da238a09d6039e3bd39bb3aee9c21a5e34f28bfa5aa22518581f910ff94af3\"},\n    {file = \"multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:ff959bee35038c4624250473988b24f846cbeb2c6639de3602c073f10410ceba\"},\n    {file = \"multidict-6.0.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:01a3a55bd90018c9c080fbb0b9f4891db37d148a0a18722b42f94694f8b6d4c9\"},\n    {file = \"multidict-6.0.4-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c5cb09abb18c1ea940fb99360ea0396f34d46566f157122c92dfa069d3e0e982\"},\n    {file = \"multidict-6.0.4-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:666daae833559deb2d609afa4490b85830ab0dfca811a98b70a205621a6109fe\"},\n    {file = \"multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:11bdf3f5e1518b24530b8241529d2050014c884cf18b6fc69c0c2b30ca248710\"},\n    {file = \"multidict-6.0.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7d18748f2d30f94f498e852c67d61261c643b349b9d2a581131725595c45ec6c\"},\n    {file = \"multidict-6.0.4-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:458f37be2d9e4c95e2d8866a851663cbc76e865b78395090786f6cd9b3bbf4f4\"},\n    {file = \"multidict-6.0.4-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:b1a2eeedcead3a41694130495593a559a668f382eee0727352b9a41e1c45759a\"},\n    {file = \"multidict-6.0.4-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7d6ae9d593ef8641544d6263c7fa6408cc90370c8cb2bbb65f8d43e5b0351d9c\"},\n    {file = \"multidict-6.0.4-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:5979b5632c3e3534e42ca6ff856bb24b2e3071b37861c2c727ce220d80eee9ed\"},\n    {file = \"multidict-6.0.4-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:dcfe792765fab89c365123c81046ad4103fcabbc4f56d1c1997e6715e8015461\"},\n    {file = \"multidict-6.0.4-cp311-cp311-win32.whl\", hash = \"sha256:3601a3cece3819534b11d4efc1eb76047488fddd0c85a3948099d5da4d504636\"},\n    {file = \"multidict-6.0.4-cp311-cp311-win_amd64.whl\", hash = \"sha256:81a4f0b34bd92df3da93315c6a59034df95866014ac08535fc819f043bfd51f0\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:67040058f37a2a51ed8ea8f6b0e6ee5bd78ca67f169ce6122f3e2ec80dfe9b78\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:853888594621e6604c978ce2a0444a1e6e70c8d253ab65ba11657659dcc9100f\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:39ff62e7d0f26c248b15e364517a72932a611a9b75f35b45be078d81bdb86603\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:af048912e045a2dc732847d33821a9d84ba553f5c5f028adbd364dd4765092ac\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b1e8b901e607795ec06c9e42530788c45ac21ef3aaa11dbd0c69de543bfb79a9\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:62501642008a8b9871ddfccbf83e4222cf8ac0d5aeedf73da36153ef2ec222d2\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:99b76c052e9f1bc0721f7541e5e8c05db3941eb9ebe7b8553c625ef88d6eefde\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:509eac6cf09c794aa27bcacfd4d62c885cce62bef7b2c3e8b2e49d365b5003fe\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:21a12c4eb6ddc9952c415f24eef97e3e55ba3af61f67c7bc388dcdec1404a067\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:5cad9430ab3e2e4fa4a2ef4450f548768400a2ac635841bc2a56a2052cdbeb87\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:ab55edc2e84460694295f401215f4a58597f8f7c9466faec545093045476327d\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-win32.whl\", hash = \"sha256:5a4dcf02b908c3b8b17a45fb0f15b695bf117a67b76b7ad18b73cf8e92608775\"},\n    {file = \"multidict-6.0.4-cp37-cp37m-win_amd64.whl\", hash = \"sha256:6ed5f161328b7df384d71b07317f4d8656434e34591f20552c7bcef27b0ab88e\"},\n    {file = \"multidict-6.0.4-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:5fc1b16f586f049820c5c5b17bb4ee7583092fa0d1c4e28b5239181ff9532e0c\"},\n    {file = \"multidict-6.0.4-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1502e24330eb681bdaa3eb70d6358e818e8e8f908a22a1851dfd4e15bc2f8161\"},\n    {file = \"multidict-6.0.4-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:b692f419760c0e65d060959df05f2a531945af31fda0c8a3b3195d4efd06de11\"},\n    {file = \"multidict-6.0.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:45e1ecb0379bfaab5eef059f50115b54571acfbe422a14f668fc8c27ba410e7e\"},\n    {file = \"multidict-6.0.4-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ddd3915998d93fbcd2566ddf9cf62cdb35c9e093075f862935573d265cf8f65d\"},\n    {file = \"multidict-6.0.4-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:59d43b61c59d82f2effb39a93c48b845efe23a3852d201ed2d24ba830d0b4cf2\"},\n    {file = \"multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:cc8e1d0c705233c5dd0c5e6460fbad7827d5d36f310a0fadfd45cc3029762258\"},\n    {file = \"multidict-6.0.4-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d6aa0418fcc838522256761b3415822626f866758ee0bc6632c9486b179d0b52\"},\n    {file = \"multidict-6.0.4-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:6748717bb10339c4760c1e63da040f5f29f5ed6e59d76daee30305894069a660\"},\n    {file = \"multidict-6.0.4-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:4d1a3d7ef5e96b1c9e92f973e43aa5e5b96c659c9bc3124acbbd81b0b9c8a951\"},\n    {file = \"multidict-6.0.4-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:4372381634485bec7e46718edc71528024fcdc6f835baefe517b34a33c731d60\"},\n    {file = \"multidict-6.0.4-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:fc35cb4676846ef752816d5be2193a1e8367b4c1397b74a565a9d0389c433a1d\"},\n    {file = \"multidict-6.0.4-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:4b9d9e4e2b37daddb5c23ea33a3417901fa7c7b3dee2d855f63ee67a0b21e5b1\"},\n    {file = \"multidict-6.0.4-cp38-cp38-win32.whl\", hash = \"sha256:e41b7e2b59679edfa309e8db64fdf22399eec4b0b24694e1b2104fb789207779\"},\n    {file = \"multidict-6.0.4-cp38-cp38-win_amd64.whl\", hash = \"sha256:d6c254ba6e45d8e72739281ebc46ea5eb5f101234f3ce171f0e9f5cc86991480\"},\n    {file = \"multidict-6.0.4-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:16ab77bbeb596e14212e7bab8429f24c1579234a3a462105cda4a66904998664\"},\n    {file = \"multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:bc779e9e6f7fda81b3f9aa58e3a6091d49ad528b11ed19f6621408806204ad35\"},\n    {file = \"multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:4ceef517eca3e03c1cceb22030a3e39cb399ac86bff4e426d4fc6ae49052cc60\"},\n    {file = \"multidict-6.0.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:281af09f488903fde97923c7744bb001a9b23b039a909460d0f14edc7bf59706\"},\n    {file = \"multidict-6.0.4-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:52f2dffc8acaba9a2f27174c41c9e57f60b907bb9f096b36b1a1f3be71c6284d\"},\n    {file = \"multidict-6.0.4-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b41156839806aecb3641f3208c0dafd3ac7775b9c4c422d82ee2a45c34ba81ca\"},\n    {file = \"multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:d5e3fc56f88cc98ef8139255cf8cd63eb2c586531e43310ff859d6bb3a6b51f1\"},\n    {file = \"multidict-6.0.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8316a77808c501004802f9beebde51c9f857054a0c871bd6da8280e718444449\"},\n    {file = \"multidict-6.0.4-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:f70b98cd94886b49d91170ef23ec5c0e8ebb6f242d734ed7ed677b24d50c82cf\"},\n    {file = \"multidict-6.0.4-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:bf6774e60d67a9efe02b3616fee22441d86fab4c6d335f9d2051d19d90a40063\"},\n    {file = \"multidict-6.0.4-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e69924bfcdda39b722ef4d9aa762b2dd38e4632b3641b1d9a57ca9cd18f2f83a\"},\n    {file = \"multidict-6.0.4-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:6b181d8c23da913d4ff585afd1155a0e1194c0b50c54fcfe286f70cdaf2b7176\"},\n    {file = \"multidict-6.0.4-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:52509b5be062d9eafc8170e53026fbc54cf3b32759a23d07fd935fb04fc22d95\"},\n    {file = \"multidict-6.0.4-cp39-cp39-win32.whl\", hash = \"sha256:27c523fbfbdfd19c6867af7346332b62b586eed663887392cff78d614f9ec313\"},\n    {file = \"multidict-6.0.4-cp39-cp39-win_amd64.whl\", hash = \"sha256:33029f5734336aa0d4c0384525da0387ef89148dc7191aae00ca5fb23d7aafc2\"},\n    {file = \"multidict-6.0.4.tar.gz\", hash = \"sha256:3666906492efb76453c0e7b97f2cf459b0682e7402c0489a95484965dbc1da49\"},\n]\n\n[[package]]\nname = \"openai\"\nversion = \"0.28.1\"\ndescription = \"Python client library for the OpenAI API\"\noptional = false\npython-versions = \">=3.7.1\"\nfiles = [\n    {file = \"openai-0.28.1-py3-none-any.whl\", hash = \"sha256:d18690f9e3d31eedb66b57b88c2165d760b24ea0a01f150dd3f068155088ce68\"},\n    {file = \"openai-0.28.1.tar.gz\", hash = \"sha256:4be1dad329a65b4ce1a660fe6d5431b438f429b5855c883435f0f7fcb6d2dcc8\"},\n]\n\n[package.dependencies]\naiohttp = \"*\"\nrequests = \">=2.20\"\ntqdm = \"*\"\n\n[package.extras]\ndatalib = [\"numpy\", \"openpyxl (>=3.0.7)\", \"pandas (>=1.2.3)\", \"pandas-stubs (>=1.1.0.11)\"]\ndev = [\"black (>=21.6b0,<22.0)\", \"pytest (==6.*)\", \"pytest-asyncio\", \"pytest-mock\"]\nembeddings = [\"matplotlib\", \"numpy\", \"openpyxl (>=3.0.7)\", \"pandas (>=1.2.3)\", \"pandas-stubs (>=1.1.0.11)\", \"plotly\", \"scikit-learn (>=1.0.2)\", \"scipy\", \"tenacity (>=8.0.1)\"]\nwandb = [\"numpy\", \"openpyxl (>=3.0.7)\", \"pandas (>=1.2.3)\", \"pandas-stubs (>=1.1.0.11)\", \"wandb\"]\n\n[[package]]\nname = \"outcome\"\nversion = \"1.3.0\"\ndescription = \"Capture the outcome of Python function calls.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"outcome-1.3.0-py2.py3-none-any.whl\", hash = \"sha256:7b688fd82db72f4b0bc9e883a00359d4d4179cd97d27f09c9644d0c842ba7786\"},\n    {file = \"outcome-1.3.0.tar.gz\", hash = \"sha256:588ef4dc10b64e8df160d8d1310c44e1927129a66d6d2ef86845cef512c5f24c\"},\n]\n\n[package.dependencies]\nattrs = \">=19.2.0\"\n\n[[package]]\nname = \"packaging\"\nversion = \"23.2\"\ndescription = \"Core utilities for Python packages\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"packaging-23.2-py3-none-any.whl\", hash = \"sha256:8c491190033a9af7e1d931d0b5dacc2ef47509b34dd0de67ed209b5203fc88c7\"},\n    {file = \"packaging-23.2.tar.gz\", hash = \"sha256:048fb0e9405036518eaaf48a55953c750c11e1a1b68e0dd1a9d62ed0c092cfc5\"},\n]\n\n[[package]]\nname = \"pdf2image\"\nversion = \"1.16.3\"\ndescription = \"A wrapper around the pdftoppm and pdftocairo command line tools to convert PDF to a PIL Image list.\"\noptional = false\npython-versions = \"*\"\nfiles = [\n    {file = \"pdf2image-1.16.3-py3-none-any.whl\", hash = \"sha256:b6154164af3677211c22cbb38b2bd778b43aca02758e962fe1e231f6d3b0e380\"},\n    {file = \"pdf2image-1.16.3.tar.gz\", hash = \"sha256:74208810c2cef4d9e347769b8e62a52303982ddb4f2dfd744c7ab4b940ae287e\"},\n]\n\n[package.dependencies]\npillow = \"*\"\n\n[[package]]\nname = \"pdfminer-six\"\nversion = \"20221105\"\ndescription = \"PDF parser and analyzer\"\noptional = false\npython-versions = \">=3.6\"\nfiles = [\n    {file = \"pdfminer.six-20221105-py3-none-any.whl\", hash = \"sha256:1eaddd712d5b2732f8ac8486824533514f8ba12a0787b3d5fe1e686cd826532d\"},\n    {file = \"pdfminer.six-20221105.tar.gz\", hash = \"sha256:8448ab7b939d18b64820478ecac5394f482d7a79f5f7eaa7703c6c959c175e1d\"},\n]\n\n[package.dependencies]\ncharset-normalizer = \">=2.0.0\"\ncryptography = \">=36.0.0\"\n\n[package.extras]\ndev = [\"black\", \"mypy (==0.931)\", \"nox\", \"pytest\"]\ndocs = [\"sphinx\", \"sphinx-argparse\"]\nimage = [\"Pillow\"]\n\n[[package]]\nname = \"pdfplumber\"\nversion = \"0.10.2\"\ndescription = \"Plumb a PDF for detailed information about each char, rectangle, and line.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"pdfplumber-0.10.2-py3-none-any.whl\", hash = \"sha256:0f168b19630a3bf8fba3bf71900ec9ecdacc58bdaafc0fa6b2bd557c791713aa\"},\n    {file = \"pdfplumber-0.10.2.tar.gz\", hash = \"sha256:60db7de9258e95e7b92ddb745e2e3e294da7a4a55d18fb85750cc4f8d994734c\"},\n]\n\n[package.dependencies]\n\"pdfminer.six\" = \"20221105\"\nPillow = \">=9.1\"\npypdfium2 = \">=4.18.0\"\n\n[[package]]\nname = \"pillow\"\nversion = \"10.1.0\"\ndescription = \"Python Imaging Library (Fork)\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"Pillow-10.1.0-cp310-cp310-macosx_10_10_x86_64.whl\", hash = \"sha256:1ab05f3db77e98f93964697c8efc49c7954b08dd61cff526b7f2531a22410106\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:6932a7652464746fcb484f7fc3618e6503d2066d853f68a4bd97193a3996e273\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a5f63b5a68daedc54c7c3464508d8c12075e56dcfbd42f8c1bf40169061ae666\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c0949b55eb607898e28eaccb525ab104b2d86542a85c74baf3a6dc24002edec2\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-manylinux_2_28_aarch64.whl\", hash = \"sha256:ae88931f93214777c7a3aa0a8f92a683f83ecde27f65a45f95f22d289a69e593\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl\", hash = \"sha256:b0eb01ca85b2361b09480784a7931fc648ed8b7836f01fb9241141b968feb1db\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:d27b5997bdd2eb9fb199982bb7eb6164db0426904020dc38c10203187ae2ff2f\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:7df5608bc38bd37ef585ae9c38c9cd46d7c81498f086915b0f97255ea60c2818\"},\n    {file = \"Pillow-10.1.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:41f67248d92a5e0a2076d3517d8d4b1e41a97e2df10eb8f93106c89107f38b57\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-macosx_10_10_x86_64.whl\", hash = \"sha256:1fb29c07478e6c06a46b867e43b0bcdb241b44cc52be9bc25ce5944eed4648e7\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:2cdc65a46e74514ce742c2013cd4a2d12e8553e3a2563c64879f7c7e4d28bce7\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:50d08cd0a2ecd2a8657bd3d82c71efd5a58edb04d9308185d66c3a5a5bed9610\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:062a1610e3bc258bff2328ec43f34244fcec972ee0717200cb1425214fe5b839\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-manylinux_2_28_aarch64.whl\", hash = \"sha256:61f1a9d247317fa08a308daaa8ee7b3f760ab1809ca2da14ecc88ae4257d6172\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl\", hash = \"sha256:a646e48de237d860c36e0db37ecaecaa3619e6f3e9d5319e527ccbc8151df061\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:47e5bf85b80abc03be7455c95b6d6e4896a62f6541c1f2ce77a7d2bb832af262\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:a92386125e9ee90381c3369f57a2a50fa9e6aa8b1cf1d9c4b200d41a7dd8e992\"},\n    {file = \"Pillow-10.1.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:0f7c276c05a9767e877a0b4c5050c8bee6a6d960d7f0c11ebda6b99746068c2a\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-macosx_10_10_x86_64.whl\", hash = \"sha256:a89b8312d51715b510a4fe9fc13686283f376cfd5abca8cd1c65e4c76e21081b\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:00f438bb841382b15d7deb9a05cc946ee0f2c352653c7aa659e75e592f6fa17d\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3d929a19f5469b3f4df33a3df2983db070ebb2088a1e145e18facbc28cae5b27\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9a92109192b360634a4489c0c756364c0c3a2992906752165ecb50544c251312\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-manylinux_2_28_aarch64.whl\", hash = \"sha256:0248f86b3ea061e67817c47ecbe82c23f9dd5d5226200eb9090b3873d3ca32de\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-manylinux_2_28_x86_64.whl\", hash = \"sha256:9882a7451c680c12f232a422730f986a1fcd808da0fd428f08b671237237d651\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:1c3ac5423c8c1da5928aa12c6e258921956757d976405e9467c5f39d1d577a4b\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:806abdd8249ba3953c33742506fe414880bad78ac25cc9a9b1c6ae97bedd573f\"},\n    {file = \"Pillow-10.1.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:eaed6977fa73408b7b8a24e8b14e59e1668cfc0f4c40193ea7ced8e210adf996\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-macosx_10_10_x86_64.whl\", hash = \"sha256:fe1e26e1ffc38be097f0ba1d0d07fcade2bcfd1d023cda5b29935ae8052bd793\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:7a7e3daa202beb61821c06d2517428e8e7c1aab08943e92ec9e5755c2fc9ba5e\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:24fadc71218ad2b8ffe437b54876c9382b4a29e030a05a9879f615091f42ffc2\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fa1d323703cfdac2036af05191b969b910d8f115cf53093125e4058f62012c9a\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-manylinux_2_28_aarch64.whl\", hash = \"sha256:912e3812a1dbbc834da2b32299b124b5ddcb664ed354916fd1ed6f193f0e2d01\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl\", hash = \"sha256:7dbaa3c7de82ef37e7708521be41db5565004258ca76945ad74a8e998c30af8d\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:9d7bc666bd8c5a4225e7ac71f2f9d12466ec555e89092728ea0f5c0c2422ea80\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:baada14941c83079bf84c037e2d8b7506ce201e92e3d2fa0d1303507a8538212\"},\n    {file = \"Pillow-10.1.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:2ef6721c97894a7aa77723740a09547197533146fba8355e86d6d9a4a1056b14\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-macosx_10_10_x86_64.whl\", hash = \"sha256:0a026c188be3b443916179f5d04548092e253beb0c3e2ee0a4e2cdad72f66099\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:04f6f6149f266a100374ca3cc368b67fb27c4af9f1cc8cb6306d849dcdf12616\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bb40c011447712d2e19cc261c82655f75f32cb724788df315ed992a4d65696bb\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1a8413794b4ad9719346cd9306118450b7b00d9a15846451549314a58ac42219\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-manylinux_2_28_aarch64.whl\", hash = \"sha256:c9aeea7b63edb7884b031a35305629a7593272b54f429a9869a4f63a1bf04c34\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-manylinux_2_28_x86_64.whl\", hash = \"sha256:b4005fee46ed9be0b8fb42be0c20e79411533d1fd58edabebc0dd24626882cfd\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:4d0152565c6aa6ebbfb1e5d8624140a440f2b99bf7afaafbdbf6430426497f28\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:d921bc90b1defa55c9917ca6b6b71430e4286fc9e44c55ead78ca1a9f9eba5f2\"},\n    {file = \"Pillow-10.1.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:cfe96560c6ce2f4c07d6647af2d0f3c54cc33289894ebd88cfbb3bcd5391e256\"},\n    {file = \"Pillow-10.1.0-pp310-pypy310_pp73-macosx_10_10_x86_64.whl\", hash = \"sha256:937bdc5a7f5343d1c97dc98149a0be7eb9704e937fe3dc7140e229ae4fc572a7\"},\n    {file = \"Pillow-10.1.0-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b1c25762197144e211efb5f4e8ad656f36c8d214d390585d1d21281f46d556ba\"},\n    {file = \"Pillow-10.1.0-pp310-pypy310_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:afc8eef765d948543a4775f00b7b8c079b3321d6b675dde0d02afa2ee23000b4\"},\n    {file = \"Pillow-10.1.0-pp310-pypy310_pp73-win_amd64.whl\", hash = \"sha256:883f216eac8712b83a63f41b76ddfb7b2afab1b74abbb413c5df6680f071a6b9\"},\n    {file = \"Pillow-10.1.0-pp39-pypy39_pp73-macosx_10_10_x86_64.whl\", hash = \"sha256:b920e4d028f6442bea9a75b7491c063f0b9a3972520731ed26c83e254302eb1e\"},\n    {file = \"Pillow-10.1.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1c41d960babf951e01a49c9746f92c5a7e0d939d1652d7ba30f6b3090f27e412\"},\n    {file = \"Pillow-10.1.0-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl\", hash = \"sha256:1fafabe50a6977ac70dfe829b2d5735fd54e190ab55259ec8aea4aaea412fa0b\"},\n    {file = \"Pillow-10.1.0-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:3b834f4b16173e5b92ab6566f0473bfb09f939ba14b23b8da1f54fa63e4b623f\"},\n    {file = \"Pillow-10.1.0.tar.gz\", hash = \"sha256:e6bf8de6c36ed96c86ea3b6e1d5273c53f46ef518a062464cd7ef5dd2cf92e38\"},\n]\n\n[package.extras]\ndocs = [\"furo\", \"olefile\", \"sphinx (>=2.4)\", \"sphinx-copybutton\", \"sphinx-inline-tabs\", \"sphinx-removed-in\", \"sphinxext-opengraph\"]\ntests = [\"check-manifest\", \"coverage\", \"defusedxml\", \"markdown2\", \"olefile\", \"packaging\", \"pyroma\", \"pytest\", \"pytest-cov\", \"pytest-timeout\"]\n\n[[package]]\nname = \"pluggy\"\nversion = \"1.3.0\"\ndescription = \"plugin and hook calling mechanisms for python\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"pluggy-1.3.0-py3-none-any.whl\", hash = \"sha256:d89c696a773f8bd377d18e5ecda92b7a3793cbe66c87060a6fb58c7b6e1061f7\"},\n    {file = \"pluggy-1.3.0.tar.gz\", hash = \"sha256:cf61ae8f126ac6f7c451172cf30e3e43d3ca77615509771b3a984a0730651e12\"},\n]\n\n[package.extras]\ndev = [\"pre-commit\", \"tox\"]\ntesting = [\"pytest\", \"pytest-benchmark\"]\n\n[[package]]\nname = \"pycparser\"\nversion = \"2.21\"\ndescription = \"C parser in Python\"\noptional = false\npython-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\nfiles = [\n    {file = \"pycparser-2.21-py2.py3-none-any.whl\", hash = \"sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9\"},\n    {file = \"pycparser-2.21.tar.gz\", hash = \"sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206\"},\n]\n\n[[package]]\nname = \"pypdf\"\nversion = \"3.16.4\"\ndescription = \"A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files\"\noptional = false\npython-versions = \">=3.6\"\nfiles = [\n    {file = \"pypdf-3.16.4-py3-none-any.whl\", hash = \"sha256:a9b1eaf2db4c2edd93093470d33c3f353235c4a694f8a426a92a8ce77cea9eb7\"},\n    {file = \"pypdf-3.16.4.tar.gz\", hash = \"sha256:01927771b562d4ba84939ef95b393f0179166da786c5db710d07f807c52f480d\"},\n]\n\n[package.extras]\ncrypto = [\"PyCryptodome\", \"cryptography\"]\ndev = [\"black\", \"flit\", \"pip-tools\", \"pre-commit (<2.18.0)\", \"pytest-cov\", \"pytest-socket\", \"pytest-timeout\", \"wheel\"]\ndocs = [\"myst_parser\", \"sphinx\", \"sphinx_rtd_theme\"]\nfull = [\"Pillow (>=8.0.0)\", \"PyCryptodome\", \"cryptography\"]\nimage = [\"Pillow (>=8.0.0)\"]\n\n[[package]]\nname = \"pypdfium2\"\nversion = \"4.21.0\"\ndescription = \"Python bindings to PDFium\"\noptional = false\npython-versions = \">= 3.6\"\nfiles = [\n    {file = \"pypdfium2-4.21.0-py3-none-macosx_10_13_x86_64.whl\", hash = \"sha256:db6d471439bf2be7160860807b31d93dcac7037234443b2caf009be45951b6af\"},\n    {file = \"pypdfium2-4.21.0-py3-none-macosx_11_0_arm64.whl\", hash = \"sha256:7a08147c4863d7d7a10d7b3bb3c6ff4caf88500f9ec009fa40794ed58cb6d73f\"},\n    {file = \"pypdfium2-4.21.0-py3-none-manylinux_2_17_aarch64.whl\", hash = \"sha256:df13206a301534cc3503d615de46afeb3d6bb84c2c11023edcf021f3d34fc5e5\"},\n    {file = \"pypdfium2-4.21.0-py3-none-manylinux_2_17_armv7l.whl\", hash = \"sha256:b4a73d574b76a256555fb6fb49ba2e9e84c8775f7af61d09132e1bfb4d847967\"},\n    {file = \"pypdfium2-4.21.0-py3-none-manylinux_2_17_i686.whl\", hash = \"sha256:55698aa3b00793c30126c1a034ac171b4dbb9cc58a135792d9c149c51d78f553\"},\n    {file = \"pypdfium2-4.21.0-py3-none-manylinux_2_17_x86_64.whl\", hash = \"sha256:d681eee3f6da6239cc596e2b2bac6cb7c0abf3a0cbd3d49dcfecc0df40f3f482\"},\n    {file = \"pypdfium2-4.21.0-py3-none-musllinux_1_1_i686.whl\", hash = \"sha256:3a4e13793a636dbb44a0b355ca71b6a8c94f86e9d5ef2fc9ac44f65800803e1f\"},\n    {file = \"pypdfium2-4.21.0-py3-none-musllinux_1_1_x86_64.whl\", hash = \"sha256:de28c5f613c8a4066f39f12b34d397bcca83bf0d671e184132b6876e3ce0137b\"},\n    {file = \"pypdfium2-4.21.0-py3-none-win32.whl\", hash = \"sha256:a26f8d99fddd0efbf7f1f0f80e861538d0c28df0ad0c48aac7114a615928d4ef\"},\n    {file = \"pypdfium2-4.21.0-py3-none-win_amd64.whl\", hash = \"sha256:09506a4233a1ae5846610ec619764b679a2a63985acfa62bf02bde20a799028b\"},\n    {file = \"pypdfium2-4.21.0-py3-none-win_arm64.whl\", hash = \"sha256:66f8dd99ec64dd151c51a32f64318f660097372e6bec27914a1b915632fedfcd\"},\n    {file = \"pypdfium2-4.21.0.tar.gz\", hash = \"sha256:514dcd4552bea1886d93ae34bc5f611a7af358104af2774c1d7822778f539335\"},\n]\n\n[[package]]\nname = \"pysocks\"\nversion = \"1.7.1\"\ndescription = \"A Python SOCKS client module. See https://github.com/Anorov/PySocks for more information.\"\noptional = false\npython-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\nfiles = [\n    {file = \"PySocks-1.7.1-py27-none-any.whl\", hash = \"sha256:08e69f092cc6dbe92a0fdd16eeb9b9ffbc13cadfe5ca4c7bd92ffb078b293299\"},\n    {file = \"PySocks-1.7.1-py3-none-any.whl\", hash = \"sha256:2725bd0a9925919b9b51739eea5f9e2bae91e83288108a9ad338b2e3a4435ee5\"},\n    {file = \"PySocks-1.7.1.tar.gz\", hash = \"sha256:3f8804571ebe159c380ac6de37643bb4685970655d3bba243530d6558b799aa0\"},\n]\n\n[[package]]\nname = \"pytesseract\"\nversion = \"0.3.10\"\ndescription = \"Python-tesseract is a python wrapper for Google's Tesseract-OCR\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"pytesseract-0.3.10-py3-none-any.whl\", hash = \"sha256:8f22cc98f765bf13517ead0c70effedb46c153540d25783e04014f28b55a5fc6\"},\n    {file = \"pytesseract-0.3.10.tar.gz\", hash = \"sha256:f1c3a8b0f07fd01a1085d451f5b8315be6eec1d5577a6796d46dc7a62bd4120f\"},\n]\n\n[package.dependencies]\npackaging = \">=21.3\"\nPillow = \">=8.0.0\"\n\n[[package]]\nname = \"pytest\"\nversion = \"7.4.2\"\ndescription = \"pytest: simple powerful testing with Python\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"pytest-7.4.2-py3-none-any.whl\", hash = \"sha256:1d881c6124e08ff0a1bb75ba3ec0bfd8b5354a01c194ddd5a0a870a48d99b002\"},\n    {file = \"pytest-7.4.2.tar.gz\", hash = \"sha256:a766259cfab564a2ad52cb1aae1b881a75c3eb7e34ca3779697c23ed47c47069\"},\n]\n\n[package.dependencies]\ncolorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\nexceptiongroup = {version = \">=1.0.0rc8\", markers = \"python_version < \\\"3.11\\\"\"}\niniconfig = \"*\"\npackaging = \"*\"\npluggy = \">=0.12,<2.0\"\ntomli = {version = \">=1.0.0\", markers = \"python_version < \\\"3.11\\\"\"}\n\n[package.extras]\ntesting = [\"argcomplete\", \"attrs (>=19.2.0)\", \"hypothesis (>=3.56)\", \"mock\", \"nose\", \"pygments (>=2.7.2)\", \"requests\", \"setuptools\", \"xmlschema\"]\n\n[[package]]\nname = \"pytest-cov\"\nversion = \"4.1.0\"\ndescription = \"Pytest plugin for measuring coverage.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"pytest-cov-4.1.0.tar.gz\", hash = \"sha256:3904b13dfbfec47f003b8e77fd5b589cd11904a21ddf1ab38a64f204d6a10ef6\"},\n    {file = \"pytest_cov-4.1.0-py3-none-any.whl\", hash = \"sha256:6ba70b9e97e69fcc3fb45bfeab2d0a138fb65c4d0d6a41ef33983ad114be8c3a\"},\n]\n\n[package.dependencies]\ncoverage = {version = \">=5.2.1\", extras = [\"toml\"]}\npytest = \">=4.6\"\n\n[package.extras]\ntesting = [\"fields\", \"hunter\", \"process-tests\", \"pytest-xdist\", \"six\", \"virtualenv\"]\n\n[[package]]\nname = \"regex\"\nversion = \"2023.10.3\"\ndescription = \"Alternative regular expression module, to replace re.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"regex-2023.10.3-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:4c34d4f73ea738223a094d8e0ffd6d2c1a1b4c175da34d6b0de3d8d69bee6bcc\"},\n    {file = \"regex-2023.10.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:a8f4e49fc3ce020f65411432183e6775f24e02dff617281094ba6ab079ef0915\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:4cd1bccf99d3ef1ab6ba835308ad85be040e6a11b0977ef7ea8c8005f01a3c29\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:81dce2ddc9f6e8f543d94b05d56e70d03a0774d32f6cca53e978dc01e4fc75b8\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9c6b4d23c04831e3ab61717a707a5d763b300213db49ca680edf8bf13ab5d91b\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c15ad0aee158a15e17e0495e1e18741573d04eb6da06d8b84af726cfc1ed02ee\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6239d4e2e0b52c8bd38c51b760cd870069f0bdf99700a62cd509d7a031749a55\"},\n    {file = \"regex-2023.10.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:4a8bf76e3182797c6b1afa5b822d1d5802ff30284abe4599e1247be4fd6b03be\"},\n    {file = \"regex-2023.10.3-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:d9c727bbcf0065cbb20f39d2b4f932f8fa1631c3e01fcedc979bd4f51fe051c5\"},\n    {file = \"regex-2023.10.3-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:3ccf2716add72f80714b9a63899b67fa711b654be3fcdd34fa391d2d274ce767\"},\n    {file = \"regex-2023.10.3-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:107ac60d1bfdc3edb53be75e2a52aff7481b92817cfdddd9b4519ccf0e54a6ff\"},\n    {file = \"regex-2023.10.3-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:00ba3c9818e33f1fa974693fb55d24cdc8ebafcb2e4207680669d8f8d7cca79a\"},\n    {file = \"regex-2023.10.3-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:f0a47efb1dbef13af9c9a54a94a0b814902e547b7f21acb29434504d18f36e3a\"},\n    {file = \"regex-2023.10.3-cp310-cp310-win32.whl\", hash = \"sha256:36362386b813fa6c9146da6149a001b7bd063dabc4d49522a1f7aa65b725c7ec\"},\n    {file = \"regex-2023.10.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:c65a3b5330b54103e7d21cac3f6bf3900d46f6d50138d73343d9e5b2900b2353\"},\n    {file = \"regex-2023.10.3-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:90a79bce019c442604662d17bf69df99090e24cdc6ad95b18b6725c2988a490e\"},\n    {file = \"regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:c7964c2183c3e6cce3f497e3a9f49d182e969f2dc3aeeadfa18945ff7bdd7051\"},\n    {file = \"regex-2023.10.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:4ef80829117a8061f974b2fda8ec799717242353bff55f8a29411794d635d964\"},\n    {file = \"regex-2023.10.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5addc9d0209a9afca5fc070f93b726bf7003bd63a427f65ef797a931782e7edc\"},\n    {file = \"regex-2023.10.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c148bec483cc4b421562b4bcedb8e28a3b84fcc8f0aa4418e10898f3c2c0eb9b\"},\n    {file = \"regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8d1f21af4c1539051049796a0f50aa342f9a27cde57318f2fc41ed50b0dbc4ac\"},\n    {file = \"regex-2023.10.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0b9ac09853b2a3e0d0082104036579809679e7715671cfbf89d83c1cb2a30f58\"},\n    {file = \"regex-2023.10.3-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:ebedc192abbc7fd13c5ee800e83a6df252bec691eb2c4bedc9f8b2e2903f5e2a\"},\n    {file = \"regex-2023.10.3-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:d8a993c0a0ffd5f2d3bda23d0cd75e7086736f8f8268de8a82fbc4bd0ac6791e\"},\n    {file = \"regex-2023.10.3-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:be6b7b8d42d3090b6c80793524fa66c57ad7ee3fe9722b258aec6d0672543fd0\"},\n    {file = \"regex-2023.10.3-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:4023e2efc35a30e66e938de5aef42b520c20e7eda7bb5fb12c35e5d09a4c43f6\"},\n    {file = \"regex-2023.10.3-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:0d47840dc05e0ba04fe2e26f15126de7c755496d5a8aae4a08bda4dd8d646c54\"},\n    {file = \"regex-2023.10.3-cp311-cp311-win32.whl\", hash = \"sha256:9145f092b5d1977ec8c0ab46e7b3381b2fd069957b9862a43bd383e5c01d18c2\"},\n    {file = \"regex-2023.10.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:b6104f9a46bd8743e4f738afef69b153c4b8b592d35ae46db07fc28ae3d5fb7c\"},\n    {file = \"regex-2023.10.3-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:bff507ae210371d4b1fe316d03433ac099f184d570a1a611e541923f78f05037\"},\n    {file = \"regex-2023.10.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:be5e22bbb67924dea15039c3282fa4cc6cdfbe0cbbd1c0515f9223186fc2ec5f\"},\n    {file = \"regex-2023.10.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:4a992f702c9be9c72fa46f01ca6e18d131906a7180950958f766c2aa294d4b41\"},\n    {file = \"regex-2023.10.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:7434a61b158be563c1362d9071358f8ab91b8d928728cd2882af060481244c9e\"},\n    {file = \"regex-2023.10.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c2169b2dcabf4e608416f7f9468737583ce5f0a6e8677c4efbf795ce81109d7c\"},\n    {file = \"regex-2023.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a9e908ef5889cda4de038892b9accc36d33d72fb3e12c747e2799a0e806ec841\"},\n    {file = \"regex-2023.10.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:12bd4bc2c632742c7ce20db48e0d99afdc05e03f0b4c1af90542e05b809a03d9\"},\n    {file = \"regex-2023.10.3-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:bc72c231f5449d86d6c7d9cc7cd819b6eb30134bb770b8cfdc0765e48ef9c420\"},\n    {file = \"regex-2023.10.3-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:bce8814b076f0ce5766dc87d5a056b0e9437b8e0cd351b9a6c4e1134a7dfbda9\"},\n    {file = \"regex-2023.10.3-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:ba7cd6dc4d585ea544c1412019921570ebd8a597fabf475acc4528210d7c4a6f\"},\n    {file = \"regex-2023.10.3-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:b0c7d2f698e83f15228ba41c135501cfe7d5740181d5903e250e47f617eb4292\"},\n    {file = \"regex-2023.10.3-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:5a8f91c64f390ecee09ff793319f30a0f32492e99f5dc1c72bc361f23ccd0a9a\"},\n    {file = \"regex-2023.10.3-cp312-cp312-win32.whl\", hash = \"sha256:ad08a69728ff3c79866d729b095872afe1e0557251da4abb2c5faff15a91d19a\"},\n    {file = \"regex-2023.10.3-cp312-cp312-win_amd64.whl\", hash = \"sha256:39cdf8d141d6d44e8d5a12a8569d5a227f645c87df4f92179bd06e2e2705e76b\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:4a3ee019a9befe84fa3e917a2dd378807e423d013377a884c1970a3c2792d293\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:76066d7ff61ba6bf3cb5efe2428fc82aac91802844c022d849a1f0f53820502d\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:bfe50b61bab1b1ec260fa7cd91106fa9fece57e6beba05630afe27c71259c59b\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:9fd88f373cb71e6b59b7fa597e47e518282455c2734fd4306a05ca219a1991b0\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b3ab05a182c7937fb374f7e946f04fb23a0c0699c0450e9fb02ef567412d2fa3\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:dac37cf08fcf2094159922edc7a2784cfcc5c70f8354469f79ed085f0328ebdf\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:e54ddd0bb8fb626aa1f9ba7b36629564544954fff9669b15da3610c22b9a0991\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:3367007ad1951fde612bf65b0dffc8fd681a4ab98ac86957d16491400d661302\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:16f8740eb6dbacc7113e3097b0a36065a02e37b47c936b551805d40340fb9971\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:f4f2ca6df64cbdd27f27b34f35adb640b5d2d77264228554e68deda54456eb11\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:39807cbcbe406efca2a233884e169d056c35aa7e9f343d4e78665246a332f597\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:7eece6fbd3eae4a92d7c748ae825cbc1ee41a89bb1c3db05b5578ed3cfcfd7cb\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-win32.whl\", hash = \"sha256:ce615c92d90df8373d9e13acddd154152645c0dc060871abf6bd43809673d20a\"},\n    {file = \"regex-2023.10.3-cp37-cp37m-win_amd64.whl\", hash = \"sha256:0f649fa32fe734c4abdfd4edbb8381c74abf5f34bc0b3271ce687b23729299ed\"},\n    {file = \"regex-2023.10.3-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:9b98b7681a9437262947f41c7fac567c7e1f6eddd94b0483596d320092004533\"},\n    {file = \"regex-2023.10.3-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:91dc1d531f80c862441d7b66c4505cd6ea9d312f01fb2f4654f40c6fdf5cc37a\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:82fcc1f1cc3ff1ab8a57ba619b149b907072e750815c5ba63e7aa2e1163384a4\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:7979b834ec7a33aafae34a90aad9f914c41fd6eaa8474e66953f3f6f7cbd4368\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ef71561f82a89af6cfcbee47f0fabfdb6e63788a9258e913955d89fdd96902ab\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:dd829712de97753367153ed84f2de752b86cd1f7a88b55a3a775eb52eafe8a94\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:00e871d83a45eee2f8688d7e6849609c2ca2a04a6d48fba3dff4deef35d14f07\"},\n    {file = \"regex-2023.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:706e7b739fdd17cb89e1fbf712d9dc21311fc2333f6d435eac2d4ee81985098c\"},\n    {file = \"regex-2023.10.3-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:cc3f1c053b73f20c7ad88b0d1d23be7e7b3901229ce89f5000a8399746a6e039\"},\n    {file = \"regex-2023.10.3-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:6f85739e80d13644b981a88f529d79c5bdf646b460ba190bffcaf6d57b2a9863\"},\n    {file = \"regex-2023.10.3-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:741ba2f511cc9626b7561a440f87d658aabb3d6b744a86a3c025f866b4d19e7f\"},\n    {file = \"regex-2023.10.3-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:e77c90ab5997e85901da85131fd36acd0ed2221368199b65f0d11bca44549711\"},\n    {file = \"regex-2023.10.3-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:979c24cbefaf2420c4e377ecd1f165ea08cc3d1fbb44bdc51bccbbf7c66a2cb4\"},\n    {file = \"regex-2023.10.3-cp38-cp38-win32.whl\", hash = \"sha256:58837f9d221744d4c92d2cf7201c6acd19623b50c643b56992cbd2b745485d3d\"},\n    {file = \"regex-2023.10.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:c55853684fe08d4897c37dfc5faeff70607a5f1806c8be148f1695be4a63414b\"},\n    {file = \"regex-2023.10.3-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:2c54e23836650bdf2c18222c87f6f840d4943944146ca479858404fedeb9f9af\"},\n    {file = \"regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:69c0771ca5653c7d4b65203cbfc5e66db9375f1078689459fe196fe08b7b4930\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6ac965a998e1388e6ff2e9781f499ad1eaa41e962a40d11c7823c9952c77123e\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1c0e8fae5b27caa34177bdfa5a960c46ff2f78ee2d45c6db15ae3f64ecadde14\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:6c56c3d47da04f921b73ff9415fbaa939f684d47293f071aa9cbb13c94afc17d\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7ef1e014eed78ab650bef9a6a9cbe50b052c0aebe553fb2881e0453717573f52\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d29338556a59423d9ff7b6eb0cb89ead2b0875e08fe522f3e068b955c3e7b59b\"},\n    {file = \"regex-2023.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:9c6d0ced3c06d0f183b73d3c5920727268d2201aa0fe6d55c60d68c792ff3588\"},\n    {file = \"regex-2023.10.3-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:994645a46c6a740ee8ce8df7911d4aee458d9b1bc5639bc968226763d07f00fa\"},\n    {file = \"regex-2023.10.3-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:66e2fe786ef28da2b28e222c89502b2af984858091675044d93cb50e6f46d7af\"},\n    {file = \"regex-2023.10.3-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:11175910f62b2b8c055f2b089e0fedd694fe2be3941b3e2633653bc51064c528\"},\n    {file = \"regex-2023.10.3-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:06e9abc0e4c9ab4779c74ad99c3fc10d3967d03114449acc2c2762ad4472b8ca\"},\n    {file = \"regex-2023.10.3-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:fb02e4257376ae25c6dd95a5aec377f9b18c09be6ebdefa7ad209b9137b73d48\"},\n    {file = \"regex-2023.10.3-cp39-cp39-win32.whl\", hash = \"sha256:3b2c3502603fab52d7619b882c25a6850b766ebd1b18de3df23b2f939360e1bd\"},\n    {file = \"regex-2023.10.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:adbccd17dcaff65704c856bd29951c58a1bd4b2b0f8ad6b826dbd543fe740988\"},\n    {file = \"regex-2023.10.3.tar.gz\", hash = \"sha256:3fef4f844d2290ee0ba57addcec17eec9e3df73f10a2748485dfd6a3a188cc0f\"},\n]\n\n[[package]]\nname = \"requests\"\nversion = \"2.31.0\"\ndescription = \"Python HTTP for Humans.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"requests-2.31.0-py3-none-any.whl\", hash = \"sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f\"},\n    {file = \"requests-2.31.0.tar.gz\", hash = \"sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\"},\n]\n\n[package.dependencies]\ncertifi = \">=2017.4.17\"\ncharset-normalizer = \">=2,<4\"\nidna = \">=2.5,<4\"\nurllib3 = \">=1.21.1,<3\"\n\n[package.extras]\nsocks = [\"PySocks (>=1.5.6,!=1.5.7)\"]\nuse-chardet-on-py3 = [\"chardet (>=3.0.2,<6)\"]\n\n[[package]]\nname = \"selenium\"\nversion = \"4.14.0\"\ndescription = \"\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"selenium-4.14.0-py3-none-any.whl\", hash = \"sha256:be9824a9354a7fe288e3fad9ceb6a9c65ddc7c44545d23ad0ebf4ce202b19893\"},\n    {file = \"selenium-4.14.0.tar.gz\", hash = \"sha256:0d14b0d9842366f38fb5f8f842cf7c042bcfa062affc6a0a86e4d634bdd0fe54\"},\n]\n\n[package.dependencies]\ncertifi = \">=2021.10.8\"\ntrio = \">=0.17,<1.0\"\ntrio-websocket = \">=0.9,<1.0\"\nurllib3 = {version = \">=1.26,<3\", extras = [\"socks\"]}\n\n[[package]]\nname = \"sniffio\"\nversion = \"1.3.0\"\ndescription = \"Sniff out which async library your code is running under\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"sniffio-1.3.0-py3-none-any.whl\", hash = \"sha256:eecefdce1e5bbfb7ad2eeaabf7c1eeb404d7757c379bd1f7e5cce9d8bf425384\"},\n    {file = \"sniffio-1.3.0.tar.gz\", hash = \"sha256:e60305c5e5d314f5389259b7f22aaa33d8f7dee49763119234af3755c55b9101\"},\n]\n\n[[package]]\nname = \"sortedcontainers\"\nversion = \"2.4.0\"\ndescription = \"Sorted Containers -- Sorted List, Sorted Dict, Sorted Set\"\noptional = false\npython-versions = \"*\"\nfiles = [\n    {file = \"sortedcontainers-2.4.0-py2.py3-none-any.whl\", hash = \"sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\"},\n    {file = \"sortedcontainers-2.4.0.tar.gz\", hash = \"sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88\"},\n]\n\n[[package]]\nname = \"soupsieve\"\nversion = \"2.5\"\ndescription = \"A modern CSS selector implementation for Beautiful Soup.\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"soupsieve-2.5-py3-none-any.whl\", hash = \"sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7\"},\n    {file = \"soupsieve-2.5.tar.gz\", hash = \"sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690\"},\n]\n\n[[package]]\nname = \"tiktoken\"\nversion = \"0.5.1\"\ndescription = \"tiktoken is a fast BPE tokeniser for use with OpenAI's models\"\noptional = false\npython-versions = \">=3.8\"\nfiles = [\n    {file = \"tiktoken-0.5.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:2b0bae3fd56de1c0a5874fb6577667a3c75bf231a6cef599338820210c16e40a\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:e529578d017045e2f0ed12d2e00e7e99f780f477234da4aae799ec4afca89f37\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:edd2ffbb789712d83fee19ab009949f998a35c51ad9f9beb39109357416344ff\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e4c73d47bdc1a3f1f66ffa019af0386c48effdc6e8797e5e76875f6388ff72e9\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:46b8554b9f351561b1989157c6bb54462056f3d44e43aa4e671367c5d62535fc\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:92ed3bbf71a175a6a4e5fbfcdb2c422bdd72d9b20407e00f435cf22a68b4ea9b\"},\n    {file = \"tiktoken-0.5.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:714efb2f4a082635d9f5afe0bf7e62989b72b65ac52f004eb7ac939f506c03a4\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:a10488d1d1a5f9c9d2b2052fdb4cf807bba545818cb1ef724a7f5d44d9f7c3d4\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:8079ac065572fe0e7c696dbd63e1fdc12ce4cdca9933935d038689d4732451df\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:7ef730db4097f5b13df8d960f7fdda2744fe21d203ea2bb80c120bb58661b155\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:426e7def5f3f23645dada816be119fa61e587dfb4755de250e136b47a045c365\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:323cec0031358bc09aa965c2c5c1f9f59baf76e5b17e62dcc06d1bb9bc3a3c7c\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:5abd9436f02e2c8eda5cce2ff8015ce91f33e782a7423de2a1859f772928f714\"},\n    {file = \"tiktoken-0.5.1-cp311-cp311-win_amd64.whl\", hash = \"sha256:1fe99953b63aabc0c9536fbc91c3c9000d78e4755edc28cc2e10825372046a2d\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:dcdc630461927718b317e6f8be7707bd0fc768cee1fdc78ddaa1e93f4dc6b2b1\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:1f2b3b253e22322b7f53a111e1f6d7ecfa199b4f08f3efdeb0480f4033b5cdc6\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:43ce0199f315776dec3ea7bf86f35df86d24b6fcde1babd3e53c38f17352442f\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a84657c083d458593c0235926b5c993eec0b586a2508d6a2020556e5347c2f0d\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:c008375c0f3d97c36e81725308699116cd5804fdac0f9b7afc732056329d2790\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:779c4dea5edd1d3178734d144d32231e0b814976bec1ec09636d1003ffe4725f\"},\n    {file = \"tiktoken-0.5.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:b5dcfcf9bfb798e86fbce76d40a1d5d9e3f92131aecfa3d1e5c9ea1a20f1ef1a\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:9b180a22db0bbcc447f691ffc3cf7a580e9e0587d87379e35e58b826ebf5bc7b\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:2b756a65d98b7cf760617a6b68762a23ab8b6ef79922be5afdb00f5e8a9f4e76\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ba9873c253ca1f670e662192a0afcb72b41e0ba3e730f16c665099e12f4dac2d\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:74c90d2be0b4c1a2b3f7dde95cd976757817d4df080d6af0ee8d461568c2e2ad\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:709a5220891f2b56caad8327fab86281787704931ed484d9548f65598dea9ce4\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:5d5a187ff9c786fae6aadd49f47f019ff19e99071dc5b0fe91bfecc94d37c686\"},\n    {file = \"tiktoken-0.5.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:e21840043dbe2e280e99ad41951c00eff8ee3b63daf57cd4c1508a3fd8583ea2\"},\n    {file = \"tiktoken-0.5.1.tar.gz\", hash = \"sha256:27e773564232004f4f810fd1f85236673ec3a56ed7f1206fc9ed8670ebedb97a\"},\n]\n\n[package.dependencies]\nregex = \">=2022.1.18\"\nrequests = \">=2.26.0\"\n\n[package.extras]\nblobfile = [\"blobfile (>=2)\"]\n\n[[package]]\nname = \"tomli\"\nversion = \"2.0.1\"\ndescription = \"A lil' TOML parser\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"tomli-2.0.1-py3-none-any.whl\", hash = \"sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc\"},\n    {file = \"tomli-2.0.1.tar.gz\", hash = \"sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f\"},\n]\n\n[[package]]\nname = \"tqdm\"\nversion = \"4.66.1\"\ndescription = \"Fast, Extensible Progress Meter\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"tqdm-4.66.1-py3-none-any.whl\", hash = \"sha256:d302b3c5b53d47bce91fea46679d9c3c6508cf6332229aa1e7d8653723793386\"},\n    {file = \"tqdm-4.66.1.tar.gz\", hash = \"sha256:d88e651f9db8d8551a62556d3cff9e3034274ca5d66e93197cf2490e2dcb69c7\"},\n]\n\n[package.dependencies]\ncolorama = {version = \"*\", markers = \"platform_system == \\\"Windows\\\"\"}\n\n[package.extras]\ndev = [\"pytest (>=6)\", \"pytest-cov\", \"pytest-timeout\", \"pytest-xdist\"]\nnotebook = [\"ipywidgets (>=6)\"]\nslack = [\"slack-sdk\"]\ntelegram = [\"requests\"]\n\n[[package]]\nname = \"trio\"\nversion = \"0.22.2\"\ndescription = \"A friendly Python library for async concurrency and I/O\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"trio-0.22.2-py3-none-any.whl\", hash = \"sha256:f43da357620e5872b3d940a2e3589aa251fd3f881b65a608d742e00809b1ec38\"},\n    {file = \"trio-0.22.2.tar.gz\", hash = \"sha256:3887cf18c8bcc894433420305468388dac76932e9668afa1c49aa3806b6accb3\"},\n]\n\n[package.dependencies]\nattrs = \">=20.1.0\"\ncffi = {version = \">=1.14\", markers = \"os_name == \\\"nt\\\" and implementation_name != \\\"pypy\\\"\"}\nexceptiongroup = {version = \">=1.0.0rc9\", markers = \"python_version < \\\"3.11\\\"\"}\nidna = \"*\"\noutcome = \"*\"\nsniffio = \"*\"\nsortedcontainers = \"*\"\n\n[[package]]\nname = \"trio-websocket\"\nversion = \"0.11.1\"\ndescription = \"WebSocket library for Trio\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"trio-websocket-0.11.1.tar.gz\", hash = \"sha256:18c11793647703c158b1f6e62de638acada927344d534e3c7628eedcb746839f\"},\n    {file = \"trio_websocket-0.11.1-py3-none-any.whl\", hash = \"sha256:520d046b0d030cf970b8b2b2e00c4c2245b3807853ecd44214acd33d74581638\"},\n]\n\n[package.dependencies]\nexceptiongroup = {version = \"*\", markers = \"python_version < \\\"3.11\\\"\"}\ntrio = \">=0.11\"\nwsproto = \">=0.14\"\n\n[[package]]\nname = \"urllib3\"\nversion = \"2.0.6\"\ndescription = \"HTTP library with thread-safe connection pooling, file post, and more.\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"urllib3-2.0.6-py3-none-any.whl\", hash = \"sha256:7a7c7003b000adf9e7ca2a377c9688bbc54ed41b985789ed576570342a375cd2\"},\n    {file = \"urllib3-2.0.6.tar.gz\", hash = \"sha256:b19e1a85d206b56d7df1d5e683df4a7725252a964e3993648dd0fb5a1c157564\"},\n]\n\n[package.dependencies]\npysocks = {version = \">=1.5.6,<1.5.7 || >1.5.7,<2.0\", optional = true, markers = \"extra == \\\"socks\\\"\"}\n\n[package.extras]\nbrotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\nsecure = [\"certifi\", \"cryptography (>=1.9)\", \"idna (>=2.0.0)\", \"pyopenssl (>=17.1.0)\", \"urllib3-secure-extra\"]\nsocks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\nzstd = [\"zstandard (>=0.18.0)\"]\n\n[[package]]\nname = \"wsproto\"\nversion = \"1.2.0\"\ndescription = \"WebSockets state-machine based protocol implementation\"\noptional = false\npython-versions = \">=3.7.0\"\nfiles = [\n    {file = \"wsproto-1.2.0-py3-none-any.whl\", hash = \"sha256:b9acddd652b585d75b20477888c56642fdade28bdfd3579aa24a4d2c037dd736\"},\n    {file = \"wsproto-1.2.0.tar.gz\", hash = \"sha256:ad565f26ecb92588a3e43bc3d96164de84cd9902482b130d0ddbaa9664a85065\"},\n]\n\n[package.dependencies]\nh11 = \">=0.9.0,<1\"\n\n[[package]]\nname = \"yarl\"\nversion = \"1.9.2\"\ndescription = \"Yet another URL library\"\noptional = false\npython-versions = \">=3.7\"\nfiles = [\n    {file = \"yarl-1.9.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:8c2ad583743d16ddbdf6bb14b5cd76bf43b0d0006e918809d5d4ddf7bde8dd82\"},\n    {file = \"yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:82aa6264b36c50acfb2424ad5ca537a2060ab6de158a5bd2a72a032cc75b9eb8\"},\n    {file = \"yarl-1.9.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:c0c77533b5ed4bcc38e943178ccae29b9bcf48ffd1063f5821192f23a1bd27b9\"},\n    {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ee4afac41415d52d53a9833ebae7e32b344be72835bbb589018c9e938045a560\"},\n    {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9bf345c3a4f5ba7f766430f97f9cc1320786f19584acc7086491f45524a551ac\"},\n    {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2a96c19c52ff442a808c105901d0bdfd2e28575b3d5f82e2f5fd67e20dc5f4ea\"},\n    {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:891c0e3ec5ec881541f6c5113d8df0315ce5440e244a716b95f2525b7b9f3608\"},\n    {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c3a53ba34a636a256d767c086ceb111358876e1fb6b50dfc4d3f4951d40133d5\"},\n    {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:566185e8ebc0898b11f8026447eacd02e46226716229cea8db37496c8cdd26e0\"},\n    {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:2b0738fb871812722a0ac2154be1f049c6223b9f6f22eec352996b69775b36d4\"},\n    {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:32f1d071b3f362c80f1a7d322bfd7b2d11e33d2adf395cc1dd4df36c9c243095\"},\n    {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:e9fdc7ac0d42bc3ea78818557fab03af6181e076a2944f43c38684b4b6bed8e3\"},\n    {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:56ff08ab5df8429901ebdc5d15941b59f6253393cb5da07b4170beefcf1b2528\"},\n    {file = \"yarl-1.9.2-cp310-cp310-win32.whl\", hash = \"sha256:8ea48e0a2f931064469bdabca50c2f578b565fc446f302a79ba6cc0ee7f384d3\"},\n    {file = \"yarl-1.9.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:50f33040f3836e912ed16d212f6cc1efb3231a8a60526a407aeb66c1c1956dde\"},\n    {file = \"yarl-1.9.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:646d663eb2232d7909e6601f1a9107e66f9791f290a1b3dc7057818fe44fc2b6\"},\n    {file = \"yarl-1.9.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:aff634b15beff8902d1f918012fc2a42e0dbae6f469fce134c8a0dc51ca423bb\"},\n    {file = \"yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:a83503934c6273806aed765035716216cc9ab4e0364f7f066227e1aaea90b8d0\"},\n    {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b25322201585c69abc7b0e89e72790469f7dad90d26754717f3310bfe30331c2\"},\n    {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:22a94666751778629f1ec4280b08eb11815783c63f52092a5953faf73be24191\"},\n    {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8ec53a0ea2a80c5cd1ab397925f94bff59222aa3cf9c6da938ce05c9ec20428d\"},\n    {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:159d81f22d7a43e6eabc36d7194cb53f2f15f498dbbfa8edc8a3239350f59fe7\"},\n    {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:832b7e711027c114d79dffb92576acd1bd2decc467dec60e1cac96912602d0e6\"},\n    {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:95d2ecefbcf4e744ea952d073c6922e72ee650ffc79028eb1e320e732898d7e8\"},\n    {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:d4e2c6d555e77b37288eaf45b8f60f0737c9efa3452c6c44626a5455aeb250b9\"},\n    {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:783185c75c12a017cc345015ea359cc801c3b29a2966c2655cd12b233bf5a2be\"},\n    {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:b8cc1863402472f16c600e3e93d542b7e7542a540f95c30afd472e8e549fc3f7\"},\n    {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:822b30a0f22e588b32d3120f6d41e4ed021806418b4c9f0bc3048b8c8cb3f92a\"},\n    {file = \"yarl-1.9.2-cp311-cp311-win32.whl\", hash = \"sha256:a60347f234c2212a9f0361955007fcf4033a75bf600a33c88a0a8e91af77c0e8\"},\n    {file = \"yarl-1.9.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:be6b3fdec5c62f2a67cb3f8c6dbf56bbf3f61c0f046f84645cd1ca73532ea051\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:38a3928ae37558bc1b559f67410df446d1fbfa87318b124bf5032c31e3447b74\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ac9bb4c5ce3975aeac288cfcb5061ce60e0d14d92209e780c93954076c7c4367\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3da8a678ca8b96c8606bbb8bfacd99a12ad5dd288bc6f7979baddd62f71c63ef\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:13414591ff516e04fcdee8dc051c13fd3db13b673c7a4cb1350e6b2ad9639ad3\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bf74d08542c3a9ea97bb8f343d4fcbd4d8f91bba5ec9d5d7f792dbe727f88938\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6e7221580dc1db478464cfeef9b03b95c5852cc22894e418562997df0d074ccc\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:494053246b119b041960ddcd20fd76224149cfea8ed8777b687358727911dd33\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:52a25809fcbecfc63ac9ba0c0fb586f90837f5425edfd1ec9f3372b119585e45\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e65610c5792870d45d7b68c677681376fcf9cc1c289f23e8e8b39c1485384185\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:1b1bba902cba32cdec51fca038fd53f8beee88b77efc373968d1ed021024cc04\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:662e6016409828ee910f5d9602a2729a8a57d74b163c89a837de3fea050c7582\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-win32.whl\", hash = \"sha256:f364d3480bffd3aa566e886587eaca7c8c04d74f6e8933f3f2c996b7f09bee1b\"},\n    {file = \"yarl-1.9.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:6a5883464143ab3ae9ba68daae8e7c5c95b969462bbe42e2464d60e7e2698368\"},\n    {file = \"yarl-1.9.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:5610f80cf43b6202e2c33ba3ec2ee0a2884f8f423c8f4f62906731d876ef4fac\"},\n    {file = \"yarl-1.9.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:b9a4e67ad7b646cd6f0938c7ebfd60e481b7410f574c560e455e938d2da8e0f4\"},\n    {file = \"yarl-1.9.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:83fcc480d7549ccebe9415d96d9263e2d4226798c37ebd18c930fce43dfb9574\"},\n    {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5fcd436ea16fee7d4207c045b1e340020e58a2597301cfbcfdbe5abd2356c2fb\"},\n    {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:84e0b1599334b1e1478db01b756e55937d4614f8654311eb26012091be109d59\"},\n    {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3458a24e4ea3fd8930e934c129b676c27452e4ebda80fbe47b56d8c6c7a63a9e\"},\n    {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:838162460b3a08987546e881a2bfa573960bb559dfa739e7800ceeec92e64417\"},\n    {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f4e2d08f07a3d7d3e12549052eb5ad3eab1c349c53ac51c209a0e5991bbada78\"},\n    {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:de119f56f3c5f0e2fb4dee508531a32b069a5f2c6e827b272d1e0ff5ac040333\"},\n    {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:149ddea5abf329752ea5051b61bd6c1d979e13fbf122d3a1f9f0c8be6cb6f63c\"},\n    {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:674ca19cbee4a82c9f54e0d1eee28116e63bc6fd1e96c43031d11cbab8b2afd5\"},\n    {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:9b3152f2f5677b997ae6c804b73da05a39daa6a9e85a512e0e6823d81cdad7cc\"},\n    {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:5415d5a4b080dc9612b1b63cba008db84e908b95848369aa1da3686ae27b6d2b\"},\n    {file = \"yarl-1.9.2-cp38-cp38-win32.whl\", hash = \"sha256:f7a3d8146575e08c29ed1cd287068e6d02f1c7bdff8970db96683b9591b86ee7\"},\n    {file = \"yarl-1.9.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:63c48f6cef34e6319a74c727376e95626f84ea091f92c0250a98e53e62c77c72\"},\n    {file = \"yarl-1.9.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:75df5ef94c3fdc393c6b19d80e6ef1ecc9ae2f4263c09cacb178d871c02a5ba9\"},\n    {file = \"yarl-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:c027a6e96ef77d401d8d5a5c8d6bc478e8042f1e448272e8d9752cb0aff8b5c8\"},\n    {file = \"yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:f3b078dbe227f79be488ffcfc7a9edb3409d018e0952cf13f15fd6512847f3f7\"},\n    {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:59723a029760079b7d991a401386390c4be5bfec1e7dd83e25a6a0881859e716\"},\n    {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b03917871bf859a81ccb180c9a2e6c1e04d2f6a51d953e6a5cdd70c93d4e5a2a\"},\n    {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c1012fa63eb6c032f3ce5d2171c267992ae0c00b9e164efe4d73db818465fac3\"},\n    {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a74dcbfe780e62f4b5a062714576f16c2f3493a0394e555ab141bf0d746bb955\"},\n    {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8c56986609b057b4839968ba901944af91b8e92f1725d1a2d77cbac6972b9ed1\"},\n    {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:2c315df3293cd521033533d242d15eab26583360b58f7ee5d9565f15fee1bef4\"},\n    {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:b7232f8dfbd225d57340e441d8caf8652a6acd06b389ea2d3222b8bc89cbfca6\"},\n    {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:53338749febd28935d55b41bf0bcc79d634881195a39f6b2f767870b72514caf\"},\n    {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:066c163aec9d3d073dc9ffe5dd3ad05069bcb03fcaab8d221290ba99f9f69ee3\"},\n    {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:8288d7cd28f8119b07dd49b7230d6b4562f9b61ee9a4ab02221060d21136be80\"},\n    {file = \"yarl-1.9.2-cp39-cp39-win32.whl\", hash = \"sha256:b124e2a6d223b65ba8768d5706d103280914d61f5cae3afbc50fc3dfcc016623\"},\n    {file = \"yarl-1.9.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:61016e7d582bc46a5378ffdd02cd0314fb8ba52f40f9cf4d9a5e7dbef88dee18\"},\n    {file = \"yarl-1.9.2.tar.gz\", hash = \"sha256:04ab9d4b9f587c06d801c2abfe9317b77cdf996c65a90d5e84ecc45010823571\"},\n]\n\n[package.dependencies]\nidna = \">=2.0\"\nmultidict = \">=4.0\"\n\n[metadata]\nlock-version = \"2.0\"\npython-versions = \"^3.10\"\ncontent-hash = \"7b6b9f06070c77de52ca393c8afb3a99f129ce12f28a748449785f271e8d8297\"\n",
    "pyproject.toml": "[tool.poetry]\nname = \"knowledge-tree\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"BrianP8701 <brianp8701@gmail.com>\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nopenai = \"^0.28.1\"\ntiktoken = \"^0.5.1\"\ngraphviz = \"^0.20.1\"\npdfplumber = \"^0.10.2\"\npdf2image = \"^1.16.3\"\npytesseract = \"^0.3.10\"\npypdf = \"^3.16.4\"\nbs4 = \"^0.0.1\"\nselenium = \"^4.14.0\"\napify-client = \"^1.5.0\"\n\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.4.2\"\npytest-cov = \"^4.1.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n",
    "stream_gpt/__init__.py": "",
    "stream_gpt/constants/__init__.py": "",
    "stream_gpt/constants/function_schemas.py": "import stream_gpt.constants.prompts as prompts\n\n                ################################\n                #                              #\n                #    Knowledge Tree Schemas    #\n                #                              #\n                ################################\n                \nRANK_CATEGORIES = [{\n        'name': 'rank_categories', \n        'description': f'{prompts.RANK_CATEGORIES}', \n        'parameters': {\n            'type': 'object', \n            'properties': {\n                'categories': {\n                    'type': 'array', \n                    'items': {\n                        'type': \"object\",\n                        'properties': {\n                            'idea': { \n                                'type': \"int\", \n                                'description': \"Provide numbers of relevant categories. Please say nothing if you don't think any of the categories are relevant.\"},\n                        }\n                    }\n                }\n            }, \n            'required': ['indexed_text']\n        }\n }]\n\n\n                ################################\n                #                              #\n                #     Data Scraper Schemas     #\n                #                              #\n                ################################\n                \nCHOOSE_BEST_SAMPLE = [{\n        'name': 'choose_best_sample',\n        'description': prompts.CHOOSE_BEST_SCRAPED_TEXT, \n        'parameters': {\n            'type': 'object', \n            'properties': {\n                'best_sample': {\n                    'type': 'integer', \n                    'description': 'Provide the number of the sample that you think is best formatted. (No spacing errors, no missing characters, etc.)'\n                }\n            }, \n            'required': ['indexed_text']\n        }\n }]",
    "stream_gpt/constants/local_paths.py": "CHROME_DRIVER = '/Users/brianprzezdziecki/Downloads/chromedriver-mac-arm64/chromedriver'",
    "stream_gpt/constants/prompts.py": "                ################################\n                #                              #\n                #    Knowledge Tree Prompts    #\n                #                              #\n                ################################\n\nSUMMARIZATION =              '''Summarize the main ideas discussed in the text. Only include things\nthat are discussed in depth, not superficially mentioned. Its okay and good to say nothing and as little as possible.'''\n                            \nKEYWORD_SUMMARIZATION =      '''Summarize the main ideas discussed in the text. Only include things\nthat are discussed in depth, not superficially mentioned. Its okay and good to say nothing and as little \nas possible. If you can describe everything with very well with a few keywords, do that.'''\n\nRANK_CATEGORIES =            '''Provided with a list of summaries of documents, which document\nwill best assist the user and the AI? If none of the documents are relevant, please say nothing. Rank \nthe documents in order of relevance.'''  \n\n\n                ################################\n                #                              #\n                #     Data Scraper Prompts     #\n                #                              #\n                ################################\n\nCHOOSE_BEST_SCRAPED_TEXT =    '''We used multiple methods to scrape text from pdfs. Some methods \nhave errors, like strange spacing characters etc. We'll provide you with the same text scraped using\ndifferent methods. Please choose the best sample out of the given numbered samples. Just say the number \nof the best one.'''",
    "stream_gpt/data_scrapers/__init__.py": "",
    "stream_gpt/data_scrapers/pdf_scraper.py": "'''\nThis file contains functions that scrape text from PDFs.\n\nI want this to be an automatic part of the pipeline, just pass in\na path to a pdf, and get back strings.\n\nAfter using each a little bit, I found, annoyingly, that each of these\nsometimes might break down on some pdfs for no seemeingly good reason.\nIssues, like mashing together words, interpreting pictures or math equations\nas a bunch of weird characters, adding spaces between each letter, etc.\n\nTo get around this, I just use all three of them on a pdf, and then\nask ChatGPT to look at samples from each of them and pick the one that\nlooks the best. Of course, in the rare case where all three of them\nbreak down, your going to have some ugly data.\n'''\nimport pdfplumber\nfrom pypdf import PdfReader\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom stream_gpt.utils import inference\nimport random\nimport json\nfrom pdfminer.layout import LTTextBoxHorizontal\nfrom pdfminer.high_level import extract_pages\n\nscrapers = [\n    'pdfplumber_scraper',\n    'pypdf2_scraper',\n    'pytesseract_scraper',\n    'pdfminer_scraper'\n]\n\ndef pdf_to_text(path):\n    '''\n    Extract text from a PDF using all available scrapers and have ChatGPT pick the best extraction.\n    '''\n    all_text = []  # Store results from each scraper\n    for scraper_name in scrapers:\n        scraper = globals()[scraper_name]\n        try:\n            extracted_text = scraper(path)\n            all_text.append(extracted_text)\n            \n            # Save the extracted text for review\n            with open(f'data/raw_library/{len(all_text)}{scraper_name}.json', 'w') as f:\n                json.dump(extracted_text, f)\n        except Exception as e:\n            raise Exception(f'{scraper} failed: {e}')\n\n    # Collect random samples from each scraper result for comparison\n    samples = []\n    rand_chunk_indices = [random.randint(0, len(all_text[0])-1) for _ in range(3)]\n    for scraper_index, scraped_chunks in enumerate(all_text, 1):\n        sample = ' '.join([scraped_chunks[i][:100] for i in rand_chunk_indices])\n        sample = f'<Sample {scraper_index}>: {sample}\\n'\n        samples.append(sample)\n\n    # Ask ChatGPT to determine the best sample\n    best_sample_index = inference.choose_best_scraped_text(samples)\n    \n    return all_text[best_sample_index]\n\n    \n\ndef pdfminer_scraper(path):\n    extracted_text = []\n    \n    for page_layout in extract_pages(path):\n        page_text = []  # Temporary list to store text from each element of the current page\n        for element in page_layout:\n            if isinstance(element, LTTextBoxHorizontal):\n                page_text.append(element.get_text())\n                \n        # Concatenate all the text elements of the current page and append to the main list\n        extracted_text.append(' '.join(page_text))\n                \n    return extracted_text\n\ndef pdfplumber_scraper(path):\n    all_text = []\n    with pdfplumber.open(path) as pdf:\n        # Loop through each page\n        for page_num in range(len(pdf.pages)):\n            # Get the specific page\n            page = pdf.pages[page_num]\n            \n            # Extract text from the page\n            text = page.extract_text()      \n            all_text.append(text)\n\n    return all_text\n\ndef pypdf2_scraper(path):\n    pdf_reader = PdfReader(open(path, 'rb'))\n    num_pages = len(pdf_reader.pages)\n    \n    text_list = []  \n    \n    for page_num in range(num_pages):\n        page = pdf_reader.pages[page_num]\n        text_content = page.extract_text()\n        text_list.append(text_content)\n    return text_list\n\ndef pytesseract_scraper(path):\n    imgs = convert_from_path(path)\n    chunks = []\n    for img in imgs:\n        chunks.append(pytesseract.image_to_string(img))\n    return chunks",
    "stream_gpt/data_scrapers/web_scraper.py": "from apify_client import ApifyClient\nfrom stream_gpt.constants.keys import APIFY_KEY\nimport json\n\ndef apify_website_content_crawler(url, includeUrlGlobs=[], excludeUrlGlobs=[]):\n    \"\"\"\n    Use Apify's Website Content Crawler to scrape content from a given URL.\n\n    This function leverages Apify's Website Content Crawler to extract content from websites. \n    It allows users to specify URLs to include or exclude from crawling and uses predefined \n    selectors to interact with and modify content during the scraping process.\n\n    Parameters:\n    - url (str): The starting URL for the web crawling process.\n    - includeUrlGlobs (list, optional): List of URL glob patterns to be exclusively included in the crawl.\n    - excludeUrlGlobs (list, optional): List of URL glob patterns to be excluded from the crawl.\n\n    Returns:\n    - list: A list of dataset items collected during the crawl.\n\n    Note:\n    - This function requires the APIFY_KEY to be set and valid.\n    - For more details on Apify's Website Content Crawler, visit: https://apify.com/apify/website-content-crawler\n\n    Example:\n    >>> result = apify_website_content_crawler(\"https://example.com\", [\"https://example.com/blog/*\"], [\"https://example.com/private/*\"])\n    >>> print(result)\n    \"\"\"\n    # Initialize the ApifyClient with your API token\n    client = ApifyClient(APIFY_KEY)\n\n    # Prepare the Actor input\n    run_input = {\n        \"startUrls\": [{ \"url\": url}],\n        \"includeUrlGlobs\": includeUrlGlobs,\n        \"excludeUrlGlobs\": excludeUrlGlobs,\n        \"initialCookies\": [],\n        \"proxyConfiguration\": { \"useApifyProxy\": True },\n        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n    [role=\\\"alert\\\"],\n    [role=\\\"banner\\\"],\n    [role=\\\"dialog\\\"],\n    [role=\\\"alertdialog\\\"],\n    [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n    [aria-modal=\\\"true\\\"]\"\"\",\n        \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n    }\n\n    # Run the Actor and wait for it to finish\n    run = client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\n    # Fetch and print Actor results from the run's dataset (if there are any)\n    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n        print(item)\n\n    dataset_items = client.dataset(run['defaultDatasetId']).list_items().items\n    return dataset_items",
    "stream_gpt/knowledge_tree/__init__.py": "",
    "stream_gpt/knowledge_tree/main.py": "'''\n    This is the primary file that an end user will stay in to interact with the knowledge tree.\n'''\nimport stream_gpt.knowledge_tree.tree.tree_class as knowledge_tree\nfrom stream_gpt.knowledge_tree.tree import construct_tree, chunk_text\n\ndef add_document_to_tree(tree: knowledge_tree.Global_Knowledge_Tree, text, text_name, prompt):\n    '''\n        This method breaks your text into chunks, and adds these chunks to \n        a hierarchal knowledge tree.\n\n        Args:\n        - tree      (string): Path to the JSON file that contains the tree\n        - text      (string): The actual text you want to be saved in the tree\n        - text_name (string): Unique name of the text you want to be saved in the tree\n        - prompt    (string): Describe how you want the text to be saved in the tree\n\n        Returns:\n        - Path in the knowledge tree to the document that was just added\n    '''\n    \n    # First we need to turn the text into its own knowledge tree, and then add it as a branch to the main tree\n    chunks = chunk_text.basic_chunk_text(text, chunk_char_size=6000) # Breaks the text into a list of chunks\n    subtree = construct_tree.Basic_Construct_Tree_From_Chunked_Document(chunks, text_name, prompt).tree # Creates a hierarchal knowledge tree representing the given text\n    return subtree\n    \ndef cleanup_and_reorganize_tree(tree):\n    '''\n        This method takes a knowledge tree and reorganizes it so that it is \n        more robust and token efficient. This method should be called periodically.\n        \n        Args:\n        - tree   (string): Path to the JSON file that contains the tree\n\n        Returns:\n        - None\n    '''\n    None\n\ndef get_context_from_tree(tree, messages):\n    '''\n        This method takes a knowledge tree and a list of messages and returns\n        relevant context and the original primary source that will best assist\n        the user and the AI.\n        \n        Args:\n        - tree      (string): Path to the JSON file that contains the tree\n        - messages  (list): List of messages from the user and the AI\n\n        Returns:\n        - context   (string): Context from the knowledge tree that best assists the user and the AI\n        - source    (string): Path in the knowledge tree to the document used in context\n    '''\n    None",
    "stream_gpt/knowledge_tree/tree/__init__.py": "",
    "stream_gpt/knowledge_tree/tree/chunk_text.py": "def basic_chunk_text(text, chunk_char_size=4000):\n    '''\n        This method breaks your text into chunks of chunk_char_size with 10% overlap.\n\n        Args:\n        - text   (string): The actual text you want to be saved in the tree\n        - prompt (string): Describe how you want the text to be saved in the tree\n\n        Returns:\n        - chunks (list): List of chunks of text\n    '''\n    overlap_size = int(chunk_char_size * 0.10)\n    chunks = []\n    start_idx = 0\n\n    while start_idx < len(text):\n        end_idx = start_idx + chunk_char_size\n        chunks.append(text[start_idx:end_idx])\n        start_idx += (chunk_char_size - overlap_size)  # Move start index forward, less the overlap\n\n    return chunks",
    "stream_gpt/knowledge_tree/tree/construct_tree.py": "import stream_gpt.knowledge_tree.tree.tree_class as knowledge_tree\nimport stream_gpt.utils.inference as call_models\nimport stream_gpt.constants.prompts as prompts\n\nclass Basic_Construct_Tree_From_Chunked_Document:\n    '''\n        This class creates a hierarchal knowledge tree representing the given text (Already broken into chunks), guided by the given prompt.\n        \n        1. Add each chunk to the tree as a leaf node\n        2. Summarize and label each chunk with keywords. Add these 'compressed chunks' to the tree as parents of the corresponding leaf nodes.\n        3. Combine nearby 'compressed chunks' into a single 'compressed chunk', and summarize and label it with keywords. Add this 'compressed chunk' \n           to the tree as a parent of the corresponding 'compressed chunks'.\n        4. Repeat step 3 until a singular root summary node is obtained.\n    '''\n    def __init__(self, chunks, text_id, prompt):\n        '''\n            Args:\n            - chunks    (list): List of text chunks\n            - text_id   (string): Unique name of the text you want to be saved in the tree\n            - prompt    (string): Describe how you want the text to be saved in the tree\n\n            Returns:\n            - tree      (Knowledge_Tree): A hierarchal knowledge tree representing the given text\n        '''\n        self.chunks = chunks\n        self.text_id = text_id\n        self.prompt = prompt\n        self.tree = knowledge_tree.Knowledge_Tree()\n        leaf_layer = [f'0_{text_id}{i}' for i in range(len(chunks) + 1)] # Creates a list of keys for the leaf nodes in the tree\n        self.tree.add_layer(leaf_layer, chunks) # Adds chunks to the tree as nodes with no connections yet\n        self.hierarchical_summarization(leaf_layer)\n        \n    def hierarchical_summarization(self, layer):\n        '''\n        Initiates a hierarchical summarization process from a layer of unconnected leaf nodes.\n        Each leaf node contains a text chunk. The function summarizes each chunk, elevating\n        these summaries to form a new parent layer. It then combines nearby nodes within this \n        layer, summarizes them, and continues this process iteratively, forming a hierarchy \n        of summarized information, until a singular root summary node is obtained.\n        \n            Args:\n            - layer         (list): Keys of the nodes in the layer\n            \n            Returns:\n            - None\n        '''\n        root_summary_done = False\n        depth = 0\n        while(not root_summary_done):\n            depth += 1\n            next_layer = self.summarize_layer(layer, depth)\n            if(len(next_layer) == 1):\n                root_summary_done = True\n                self.tree.root = next_layer[0]\n            layer = next_layer\n\n    def summarize_layer(self, layer, depth, granularity=2):\n        '''\n        Summarizes a layer of the knowledge base.\n        \n            Args:\n            - layer         (list): Keys of the nodes in the layer\n            - depth         (int): Depth of the layer being summarized\n            - granularity   (int): How many nodes to combine into a single node\n\n            Returns:\n            - new_layer    (list): Keys of the nodes in the layer just created\n        '''\n        # Loop through {granularity} nodes at a time, summarizing them, and adding the summary as a parent of those nodes\n        new_layer = []\n        index = 0\n        for i in range(0, len(layer), granularity):\n            nodes_to_process = layer[i:min(i+granularity, len(layer))]\n            text = ''\n            for node in nodes_to_process:\n                text += f\"\\n{self.tree.nodes[node]['data']}\"\n            summary = call_models.summarize(self.prompt, text)\n            self.tree.add_parent(f'{depth}_{self.text_id}{index}', children_ids=nodes_to_process, data=summary)\n            new_layer.append(f'{depth}_{self.text_id}{index}')\n        return new_layer",
    "stream_gpt/knowledge_tree/tree/tree_class.py": "import json\nfrom graphviz import Digraph\n\nclass Knowledge_Tree:\n    '''\n        This is a standard tree data structure, with the addition of a data field for each node.\n    '''\n    def __init__(self):\n        self.nodes = {}\n        self.root = None\n\n    def add_node(self, node_id, parent_id=None, data=None):\n        '''\n        Add a node to the tree.\n        \n        Parameters:\n            node_id: Unique identifier for the node.\n            parent_id: Identifier of the parent node (None for root).\n            data: Data to be stored in the node.\n        \n        Raises:\n            ValueError: If the node_id already exists or the parent_id does not exist.\n        '''\n        if node_id in self.nodes:\n            raise ValueError(f\"Node {node_id} already exists.\")\n        if parent_id and parent_id not in self.nodes:\n            raise ValueError(f\"Parent node {parent_id} does not exist.\")\n        \n        self.nodes[node_id] = {\"data\": data, \"children\": [], \"parent\": parent_id}\n        if parent_id:\n            self.nodes[parent_id][\"children\"].append(node_id)\n\n    def remove_node(self, node_id):\n        '''\n        Remove a node and all its children from the tree.\n        \n        Parameters:\n            node_id: Identifier of the node to be removed.\n        '''\n        children = self.nodes[node_id][\"children\"]\n        for child in children:\n            self.remove_node(child)\n        \n        parent_id = self.nodes[node_id][\"parent\"]\n        if parent_id:\n            self.nodes[parent_id][\"children\"].remove(node_id)\n        del self.nodes[node_id]\n            \n    def add_nodes(self, parent_ids, children_ids_list, data_list=None):\n        '''\n        Adds multiple nodes to the tree.\n        \n        Parameters:\n            parent_ids: List of identifiers for the parent nodes.\n            children_ids_list: List of lists, each containing identifiers for children of a parent.\n            data_list: List of data to be stored in the nodes. Defaults to None.\n        \n        Note:\n            The lengths of parent_ids, children_ids_list, and data_list must be the same.\n        '''\n        if data_list is None:\n            data_list = [None] * len(parent_ids)\n        for parent_id, children_ids, data in zip(parent_ids, children_ids_list, data_list):\n            for child_id in children_ids:\n                self.add_node(child_id, parent_id, data)\n\n    def add_parent(self, parent_id, children_ids, data=None):\n        '''\n        Adds a parent node and its children.\n        \n        Parameters:\n            parent_id: Identifier of the parent node.\n            children_ids: List of identifiers for the child nodes.\n            data: Data to be stored in the parent and child nodes.\n        '''\n        if parent_id not in self.nodes:\n            self.nodes[parent_id] = {\"data\": data, \"children\": [], \"parent\": None}\n        for child_id in children_ids:\n            if self.nodes[child_id][\"parent\"]:\n                raise ValueError(f\"Child node {child_id} already has a parent.\")\n            else:\n                self.nodes[child_id][\"parent\"] = parent_id\n                self.nodes[parent_id][\"children\"].append(child_id)\n            \n    def add_layer(self, node_ids, data_list=None):\n        '''\n        Adds a layer of nodes without connecting them to any parents.\n        \n        Parameters:\n            node_ids: List of identifiers for the nodes.\n            data_list: List of data to be stored in the nodes. Defaults to None.\n        '''\n        if data_list is None:\n            data_list = [None] * len(node_ids)\n        for node_id, data in zip(node_ids, data_list):\n            self.add_node(node_id, data=data)\n    \n    def save_to_file(self, file_path):\n        '''\n        Save the tree to a file in JSON format.\n        \n        Parameters:\n            file_path: The path to the file where the tree will be saved.\n        '''\n        with open(file_path, 'w') as f:\n            json.dump(self.nodes, f)\n\n    def load_from_file(self, file_path):\n        '''\n        Load the tree from a file.\n        \n        Parameters:\n            file_path: The path to the file from which to load the tree.\n        \n        Note:\n            This will overwrite any existing data in the tree.\n        '''\n        with open(file_path, 'r') as f:\n            self.nodes = json.load(f)\n\n    def to_graphviz(self, file_path='knowledge_tree'):\n        '''\n        Generate a Graphviz representation of the tree and render it.\n        \n        Parameters:\n            file_path: The path where the Graphviz file will be saved. Defaults to 'knowledge_tree'.\n        '''\n        dot = Digraph()\n        for node_id, node_data in self.nodes.items():\n            label = f\"{node_id}\\n{node_data['data']}\"\n            dot.node(node_id, label=label)\n            for child_id in node_data['children']:\n                dot.edge(node_id, child_id)\n        dot.render(file_path, view=True)\n\n            \nclass Global_Knowledge_Tree(Knowledge_Tree):\n    '''\n        This is the same as Knowledge_Tree, but with a list of root nodes of branches within the tree\n        that represent individual documents.\n    '''\n    def __init__(self):\n        super().__init__() \n        self.document_roots = []  \n        self.add_node('Global Root Node')\n        self.root = 'Global Root Node'\n        \n    def add_knowledge_tree(self, subtree):\n        current_node = self.root\n        document_root_data = subtree.nodes[subtree.root]['data']\n        for node in subtree.nodes:\n            self.add_node(node, data=subtree.nodes[node]['data'])\n        traversing_tree = True\n        while(traversing_tree):\n            if self.are_all_children_documents(current_node):\n                traversing_tree = False\n                self.add_node(subtree.root, current_node)\n            else:\n                current_node = subtree.nodes[current_node]['children'][0]\n                \n    def are_all_children_documents(self, node_id):\n        return all(child in self.document_roots for child in self.nodes[node_id]['children'])",
    "stream_gpt/utils/__init__.py": "",
    "stream_gpt/utils/helpers.py": "import tiktoken\n\ndef num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    num_tokens = 0\n    for message in messages:\n        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":  # if there's a name, the role is omitted\n                num_tokens += -1  # role is always required and always 1 token\n    num_tokens += 2  # every reply is primed with <im_start>assistant\n    return num_tokens\n\ndef concatenate_with_indices(string_list):\n    '''\n    Concatenates a list of strings, separating them with four new lines and \n    prefixing each with its index in the format '<Category n>'.\n\n    Args:\n    - string_list (list of str): The list of strings to concatenate.\n\n    Returns:\n    - str: The concatenated string with indices and newline separations.\n    '''\n    result = \"\"\n    for index, string in enumerate(string_list):\n        result += f'<Category {index}>\\n{string}\\n\\n\\n\\n'\n    return result",
    "stream_gpt/utils/inference.py": "import openai\nimport os\nimport json\nimport warnings\nfrom stream_gpt.constants import prompts, function_schemas\nfrom stream_gpt.utils import helpers\n\nopenai.api_key = os.environ.get('OPENAI_API_KEY')\n\ndef chat_with_gpt3_turbo(messages, temperature=0.0):\n    if type(messages) == str: # In case someone accidentally passes in a string instead of a list of messages\n        warnings.warn(\"chat_with_gpt3_turbo() expects a list of messages, not a string.\")\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    for message in messages:\n        message[\"content\"] = message[\"content\"].encode('latin-1', errors='ignore').decode('latin-1')\n    completion = openai.ChatCompletion.create(model='gpt-3.5-turbo-16k',messages=messages,temperature=temperature)\n    return completion\n\ndef function_call_with_gpt3_turbo(messages, functions, function_call='auto', temperature=0.0):\n    if type(messages) == str: # In case someone accidentally passes in a string instead of a list of messages\n        warnings.warn(\"chat_with_gpt3_turbo() expects a list of messages, not a string.\")\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    for message in messages:\n        message[\"content\"] = message[\"content\"].encode('latin-1', errors='ignore').decode('latin-1')\n    completion = openai.ChatCompletion.create(model='gpt-3.5-turbo-16k',messages=messages,temperature=temperature,functions=functions, function_call=function_call)\n    return completion\n\ndef chat_with_gpt3_instruct(prompt, temperature=0.0):\n    if type(prompt) == list: # In case someone accidentally passes in a list of messages instead of a prompt\n        warnings.warn(\"chat_with_gpt3_instruct() expects a prompt, not a list of messages.\")\n        prompt = '\\n'.join(f'{message[\"role\"]}: {message[\"content\"]}' for message in prompt)\n    response = openai.Completion.create(model=\"gpt-3.5-turbo-instruct\",prompt=prompt, temperature=temperature)\n    return response\n\ndef summarize(user_prompt, text, model=\"gpt-3.5-turbo-instruct\"):\n    if model == \"gpt-3.5-turbo-instruct\":\n        prompt = f'{prompts.KEYWORD_SUMMARIZATION} {user_prompt}\\n{text}'\n        response = chat_with_gpt3_instruct(prompt).choices[0].text\n    if model == \"gpt-3.5-turbo-16k\":\n        messages=[\n            {\"role\": \"system\", \"content\": f'{prompts.SUMMARIZATION} {user_prompt}'},\n            {\"role\": \"user\", \"content\": text}\n        ]\n        response = chat_with_gpt3_turbo(messages).choices[0]['content']\n    return response\n\ndef rank_categories(user_prompt, categories, model='gpt-3.5-turbo-16k'):\n    '''\n        Compare and rank a list of categories based on how well they match the user's prompt.\n        Must use a model that supports function calling.\n        \n        Args:\n        - user_prompt (string): Prompt from user\n        - categories  (list): List of categories to compare and rank\n        - model       (string): Model to use for inference\n        \n        Returns:\n        - ranked_categories (list): List of categories ranked by relevance\n    '''\n    messages = [{\"role\": \"user\", \"content\": user_prompt},\n                {\"role\": \"user\", \"content\": helpers.concatenate_with_indices(categories)}]\n    response = function_call_with_gpt3_turbo(messages, function_schemas.RANK_CATEGORIES, function_call={'name':'rank_categories'}).choices[0]['message']['function_call']['arguments']\n    return(json.loads(response))\n    \ndef choose_best_scraped_text(samples):\n    '''\n        When using pdf scrapers, sometimes noise can happen. Here we ask ChatGPT \n        to choose the best sample from a list of samples.\n        \n        Args:\n        - samples (list): List of sample from each scraper. Each sample is a string.\n    '''\n    user_prompt = ''\n    index = 1\n    for sample in samples:\n        user_prompt += f'{index}: {sample}\\n'\n    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n    response = function_call_with_gpt3_turbo(messages, function_schemas.CHOOSE_BEST_SAMPLE, function_call={'name':'choose_best_sample'}).choices[0]['message']['function_call']['arguments']\n    return(json.loads(response)['best_sample'])\n\n\n",
    "tests/__init__.py": "",
    "tests/test_construct_tree.py": "from stream_gpt.knowledge_tree.main import add_document_to_tree\nfrom stream_gpt.knowledge_tree.tree.tree_class import Global_Knowledge_Tree\nimport pytest\n\n@pytest.mark.skip(reason=\"Not ready yet\")\ndef test_add_document_to_tree():\n    my_tree = Global_Knowledge_Tree()\n    with open('/Users/brianprzezdziecki/Research/Mechatronics/STREAM_GPT/data/library/STREAM_Report.txt', 'r') as file:\n        text = file.read()\n    add_document_to_tree(my_tree, text, 'STREAM_Report', 'Take note of all ')",
    "tests/test_pdf_scraper.py": "from stream_gpt.data_scrapers.pdf_scraper import pdfminer_scraper, pdf_to_text, pdfplumber_scraper, pypdf2_scraper, pytesseract_scraper\nimport json\nimport pytest\n\n@pytest.mark.skip(reason=\"Already tested\")\ndef test_pdfminer_scraper():\n    text = pdfminer_scraper('data/raw_library/STREAM_Report.pdf')\n    assert len(text) > 0\n    with open('data/raw_library/STREAM_Report.json', 'w') as f:\n        json.dump(text, f)\n\n@pytest.mark.skip(reason=\"Already tested\")\ndef test_pdf_to_text():\n    text = pdf_to_text('data/raw_library/STREAM_Report.pdf')",
    "tests/test_web_scraper.py": "import pytest\nimport json\nfrom stream_gpt.data_scrapers.web_scraper import selenium_scrape_site, bs4_scrape_site\n\ndef test_bs4_scraper():\n    text = bs4_scrape_site('https://docs.llamaindex.ai/en/stable/')\n    assert len(text) > 0\n    with open('data/test/Glockkey.json', 'w') as f:\n        json.dump(text, f)\n        "
}