{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/cost_analysis/usage_pattern.html",
        "title": "Usage Pattern - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\n## Estimating LLM and Embedding Token Counts[\uf0c1](#estimating-llm-and-embedding-token-counts \"Permalink to this heading\")\n\nIn order to measure LLM and Embedding token counts, you\u2019ll need to\n\n1.  Setup `MockLLM` and `MockEmbedding` objects\n    \n\nfrom llama\\_index.llms import MockLLM\nfrom llama\\_index import MockEmbedding\n\nllm \\= MockLLM(max\\_tokens\\=256)\nembed\\_model \\= MockEmbedding(embed\\_dim\\=1536)\n\n2.  Setup the `TokenCountingCallback` handler\n    \n\nimport tiktoken\nfrom llama\\_index.callbacks import CallbackManager, TokenCountingHandler\n\ntoken\\_counter \\= TokenCountingHandler(\n    tokenizer\\=tiktoken.encoding\\_for\\_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback\\_manager \\= CallbackManager(\\[token\\_counter\\])\n\n3.  Add them to the global `ServiceContext`\n    \n\nfrom llama\\_index import ServiceContext, set\\_global\\_service\\_context\n\nset\\_global\\_service\\_context(\n    ServiceContext.from\\_defaults(\n        llm\\=llm,\n        embed\\_model\\=embed\\_model,\n        callback\\_manager\\=callback\\_manager\n    )\n)\n\n4.  Construct an Index\n    \n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader(\"./docs/examples/data/paul\\_graham\").load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n5.  Measure the counts!\n    \n\nprint(\n    \"Embedding Tokens: \",\n    token\\_counter.total\\_embedding\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Prompt Tokens: \",\n    token\\_counter.prompt\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Completion Tokens: \",\n    token\\_counter.completion\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"Total LLM Token Count: \",\n    token\\_counter.total\\_llm\\_token\\_count,\n    \"\\\\n\",\n)\n\n\\# reset counts\ntoken\\_counter.reset\\_counts()\n\n6.  Run a query, mesaure again\n    \n\nquery\\_engine \\= index.as\\_query\\_engine()\n\nresponse \\= query\\_engine.query(\"query\")\n\nprint(\n    \"Embedding Tokens: \",\n    token\\_counter.total\\_embedding\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Prompt Tokens: \",\n    token\\_counter.prompt\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"LLM Completion Tokens: \",\n    token\\_counter.completion\\_llm\\_token\\_count,\n    \"\\\\n\",\n    \"Total LLM Token Count: \",\n    token\\_counter.total\\_llm\\_token\\_count,\n    \"\\\\n\",\n)"
}