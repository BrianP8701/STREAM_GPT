["STREAM\nAI\n1\n1\nReal-T ime,\nMultithr eaded\nAnomaly\nDetection\nSystem\nwith\nAutonomous\nSynchr onization\n&\nSelf-Corr ection\nBrian\nPrzezdziecki\nUniversity\nof\nBuffalo\n(SUNY)", "STREAM\nAI\n1\n2\nAbstract\nReal-time\nanomaly\ndetection\nis\ncrucial\nduring\nthe\nprinting\nprocess,\nespecially\nwhen\nlooking\nfor\nissues\nlike\nover\nor\nunder\nextrusion.\nWe\nemploy\na\nMobileNet\nmodel\nto\nclassify\nthese\nanomalies.\nOur\nsystem\nhas\ntwo\nmain\ncomponents:\nTip\nTracking\nand\nAnomaly\nDetection\n.\nIn\nthe\nTip\nTracking\nphase,\nwe\nestimate\nthe\nlocations\nof\nthe\ntips\nusing\ngcode\ndata.\nThen,\nwe\nuse\nthe\nYOLO\nmodel\nto\nalign\nthese\npredictions\nwith\nlive\nvideo\nfootage,\nensuring\nsynchronization\nand\ncorrecting\nany\nprediction\ninaccuracies.\nGiven\nthe\nneed\nfor\nreal-time\nperformance\non\na\nRaspberry\nPi,\nour\nsystem\nis\noptimized\nfor\ncomputational\nefficiency.\nIt's\ndesigned\nto\noperate\nin\nreal-time,\nrunning\nalongside\nvideo\nfeeds,\nand\nstrikes\na\nbalance\nbetween\naccuracy,\nautonomy,\nrobustness\nand\nspeed.", "STREAM\nAI\n1\n3\nProblem\nWe\nneed\nto\nspot\nerrors\nin\n3D\nprinting\nas\nthey\nhappen.\nThe\nmain\nmistakes\nare\ntied\nto\nhow\nmuch\nmaterial\ncomes\nout\n-\ntoo\nmuch\nor\ntoo\nlittle\n(Under/Over\nExtrusion).\nClasses:\nOver\nNormal\nUnder\nAnother\nproblem\narises:\nGiven\na\nframe,\nthe\nonly\nrelevant\ninformation\nis\nthe\narea\naround\nthe\ntip\nin\nthe\ndirection\nof\nrecently\nextruded\nmaterial.\nThis\nis\nwhy\nwe\nneed\nto\ntrack\nthe\ntip.\n", "STREAM\nAI\n1\n4\nSolution\nThe\ngiven\ndiagram\ndisplays\nhow\nthe\nsystem\nis\ndivided\ninto\ndistinct\nthreads\nand\nillustrates\nthe\nprocess\nflow.\nAll\nthreads\naccess\nsome\nshared\nglobal\nvariables.\nThe\ntracker\nthread\ninitiates\nand\nmanages\nall\nother\nthreads.\nThis\nsystem\nprimarily\nrevolves\naround\nthree\nkey\ncomponents:\n1.\nInitialization\n:\nPredicting\nthe\npath\nof\ntip\ntracking\nand\naligning\nwith\nthe\nvideo.\n2.\nError\nCorr ection\n:\nCorrecting\nerrors\nthrough\na\nfeedback\nloop.\n3.\nAnalytics\n:\nIdentifying\nanomalies.\nFor\nthe\nfirst\ntwo\nsteps,\nwe\nutilize\na\nfine-tuned\nYolov8.\nFor\nthe\nanomaly\nidentification,\nwe\nrely\non\na\nfine-tuned\nMobileNetv3\nmodel.\nWhy\nwe\nneed\nTip\nTracking\n&\nExamples\nof\nVideo\nand\nAnomalies\n", "STREAM\nAI\n1\n5\nChallenges\nThis\nsystem\nis\ndesigned\nwith\nsimplicity\nand\nversatility\nin\nmind.\nOur\ngoal\nis\nto\nmake\nit\nreadily\nadoptable\nfor\nanyone,\nensuring\nit\ncan\nseamlessly\nintegrate\nwith\nvarious\n3D\nprinters\nin\ndiverse\nsettings.\nThere's\nno\nneed\nfor\na\nprecise\ncamera\nplacement,\ngiving\nusers\nflexibility.\nThe\nsystem\nadjusts\nto\ndifferent\nresolutions,\nsimplifying\nsetup\nprocesses.\nIt's\nbuilt\nto\nwork\nwith\nany\nprinter\nthat\noperates\non\ng-code,\neven\nif\nthe\ng-code\ninterpretations\nvary.\nWith\nedge\ndeployment\ncapabilities,\nit\ncan\nrun\non\na\nRaspberry\nPi,\nstreaming\nin\nreal\ntime.\nGiven\nthat\nthe\nYOLO\nmodel\nmay\noccasionally\nerr\nin\nobject\ndetection,\nour\nsystem\nis\nmade\nto\naccount\nfor\nthese\ndiscrepancies.\nMoreover,\nusers\nwill\nbenefit\nfrom\na\nclear\nvisualization\nof\nthe\ntip\ntracking\nprocedure.\nGiven\nthese\nconstraints\nand\nrequirements,\nthe\nensuing\nchallenges\nand\nour\ntailored\nsolutions\nwill\nbe\nelaborated\nbelow.", "STREAM\nAI\n1\n6\nModel\nTraining\nhttps://github.com/BrianP8701/Anomaly_Classification\nObject\nDetection\nfor\nTip\nTracking\nWe\nutilized\nthe\nYOLOv8s\nmodel,\na\nprominent\nobject\ndetection\nsystem,\nand\nfine-tuned\nit\nthrough\nRoboflow,\na\nplatform\nthat\nstreamlines\nthe\ncomputer\nvision\ndata\nprocess.\nWe\nadded\nonto\nour\ndataset\nand\nretrained\nmultiple\ntimes,\naiming\nto\nfill\nin\ngaps\nand\nweaknesses\nof\nthe\nmodel,\nadding\nnull\nimages\nand\nwrong\ndetections\netc.\nWe\napplied\naugmentations\nof\nblur,\nnoise\nand\nbrightness.\nInput\nSize\nDataset\nSize\nAugmentations\nPrecision\nRecall\nMAP50\n640x640\n2407\n1200\n0.967\n0.931\n0.968\n", "STREAM\nAI\n1\n7\nObject\nClassification\nfor\nAnomalies\nWe\ntested\nvarious\nmodels,\nsettings,\nand\npreprocessing\nmethods\nto\nfind\nthe\nbest\nmodel.\nOur\ndata\nis\nlimited\nbecause\nonly\na\nfew\nframes\nin\neach\nvideo\nshow\nover\nor\nunder\nextrusion,\nmaking\ndata\ncollection\nslow.\nHere's\nour\ndata\npipeline:\nExamining\nthe\npipeline:\nI\ndeveloped\nseveral\nGUIs\nto\naddress\nthe\ninitial\ndata\ncollection\nbottleneck.\nI\nopted\nfor\nan\n85x85\ncrop\nsize\nas\nit\nbest\ncaptured\nmaterial\nnear\nthe\nextrusion\ntip.\nI\nused\ngrayscale\nto\nreduce\ndimensions.\nIn\nsome\nimages,\nedges\nand\nmaterial\nare\nbarely\nvisible.\nTherefore,\na\nfilter\nwas\nused\nin\npreprocessing\nto\nenhance\nthese\ncrucial\nedges.\nI\ninitially\nattempted\na\nfilter\nthat\nsegmented\nthe\nimage\ninto\ndistinct\ncolor\nintensities,\nusing\nthe\noriginal\nimage's\ncolor\nintensity\nstandard\ndeviation.\nWhile\nsomewhat\neffective,\nit\nmissed\nthe\nsubtlest\nedges.\n", "STREAM\nAI\n1\n8\nDrawing\ninspiration\nfrom\nthe\nconvolutional\noperations\nin\nCNNs,\nI\nexperimented\nwith\nvarious\nkernels\nand\nmatrix\nfilter\ncombinations\non\nthe\nimages.\nHowever,\nno\nsingle\ncombination\nconsistently\nworked\nacross\nall\nimages.\nI\ntested\nstandard\nfilters\nsuch\nas\nthe\nNon-local\nMeans\nDenoising\nfrom\ncv2\nand\nthe\nUnsharp\nMask\nfrom\nPIL.\nYet,\nneither\nsucceeded\nin\nenhancing\nthe\nfaint\nedges.\nAn\nobservation\nwas\nmade,\neach\nimage\nfeatures\na\nlimited\nset\nof\nobjects\nand\nhues:\nmaterial,\ntip,\nbed,\nshadows,\nand\ngleam.\nEach\ncolor\nintensity\nin\nthe\nimage\nuniquely\nfollows\na\nnormal\ndistribution\nwith\nits\ndistinct\nmean\nand\nvariance.\nHence,\nGaussian\nMixture\nModels\nare\napt\nfor\nthis\ntask.\n", "STREAM\nAI\n1\n9\nUsing\nGaussian\nMixture\nModels,\nwe\ncan\nsharply\ndefine\neven\nthe\nsubtlest\nedges.\nI\ntested\nvarious\ncomponent\nnumbers,\nindicating\nthe\nassumed\nGaussians\nin\nthe\nmixture.\nNext,\nwe\nprioritize\nsensitivity\naround\nbrighter\ncolors\nsince\nit's\nchallenging\nto\ndiscern\nedges\nwhen\nmaterial\naccumulates.\nThis\nfocus\nignores\nthe\ndarker\nbed,\nwhich\nisn't\nof\nconcern.\nThis\nfinal\nrefinement\nresulted\nin\nour\npreprocessing\nstep,\ndubbed\nGMMS.\nThrough\nexperimentation,\nGMMS6\nis\nfound\nto\nperform\nthe\nbest\nwhen\ntraining\nmodels.\n", "STREAM\nAI\n1\n10\nBack\nto\nexamining\nthe\npipeline,\nthe\nimages\nare\nresized\nto\n224x224,\nsuitable\nfor\nEfficientNet,\nMobileNet,\nand\nResNet,\nwhich\nhave\ntrained\non\nthe\n224x224-sized\nImageNet\ndataset,\nmaking\nthem\nadept\nat\nrecognizing\nfeatures\nof\nthis\nscale.\nPost-resizing,\nthe\nimages\nundergo\naugmentation\nand\nare\nsplit\ninto\ntraining,\nvalidation,\nand\ntest\ndatasets.\nDataset\nSummary:\nUnder:\n436\nimages\n|\nNormal:\n463\nimages\n|\nOver:\n446\nimages\nExtrusion\nClassification\nDataset\nAugmentations\nfor\neach\nclass\ninclude\nrotations,\nbrightness,\nsaturation,\ncontrast,\nhue\nadjustments,\nand\nflips\n(both\nhorizontal\nand\nvertical)\ncreating\n60\nnew\nimages\nper\nclass.\nThe\ndata\ndistribution\nis\n75%\nfor\ntraining,\n15%\nfor\nvalidation,\nand\n10%\nfor\ntesting.\nTraining\nSummary:\nFine\ntuning,\nmodel\ncheckpointing,\n4\nbatches,\n~12\nepochs,\nlearning\nrate\ndecay,\nearly\nstopping.\nlearning_rate=0.01,\nmomentum=0.9,\nstep_size=5,\ngamma=0.1\nAfter\ntrials\nof\nvarious\nmodels,\nsettings,\nhyperparameters\nand\npreprocessing\nsteps,\nthe\ncombination\nof\nthe\ndescribed\npreprocessing\nand\ntraining\nprocedures\noutlined\nabove\nyielded\nthe\nhighest\naccuracy.\nThe\nstandout\nwas\na\nfine\ntuned\nMobileNetv3Lar ge\nmodel,\nwhich\nnot\nonly\nwon\nin\naccuracy\nbut\nalso\nhad\nsuperior\ncomputational\nefficiency.\n", "STREAM\nAI\n1\n11\nTo\nthe\nleft\nshows\none\nof\nthe\ncomparison\nrounds,\nwhere\nwe\nidentified\nour\nbest\nperforming\nmodel.\nHere\nwe\ncompare\n3\ndifferent\nsettings\nand\ntheir\ncombinations:\nFine\ntuning\nMobileNetv3Small\nvs\nMobileNetv3Lar ge.\nUsing\nno\nGMM\npreprocessing\nvs\nusing\nGMM\npreprocessing\nvs\nusing\nGMMS\npreprocessing\nwith\nadditional\nsensitivity\nto\nmore\nintense\npixels.\nThe\nresults\nare\nfitting,\nshowing\nthat\nindeed\nusing\nGMMS\nwith\nextra\nsensitivity\nis\nbeneficial,\nand\ntraining\nthe\nlarger\nmodel\nresults\nin\nbetter\nperformance.\nThe\ntraining\nand\nvalidation\nmetrics\nare\nnearly\nindistinguishable\nindicating\nthat\nit\nis\nnot\noverfitting.\nBest\nperforming\nmodel\naccuracy:\n0.9025974025974026\n", "STREAM\nAI\n1\n12\nImplementation\nIn\nthis\nsection\nI\nwill\ndescribe\nthe\ndesign\nchoices\nand\ndetails\nof\nthe\nsystem.\nhttps://github.com/BrianP8701/STREAM.AI\nInput\n1.\nG-Code:\n-\nInstruction\nset\nfor\n3D\nprint.\n-\nSimple\nand\nsequential:\nit\nruns\nfrom\nstart\nto\nfinish.\n-\nSpecifies\nthe\ntop\nspeed\nat\nany\nmoment\nand\nprovides\nmovement\ncoordinates.\n-\nThe\nprinter\nmoves\ndirectly\nfrom\none\npoint\nto\nthe\nnext\nwithout\ndeviation.\n2.\nVideo\nStream:\n-\nLive\nfootage\nof\nthe\n3D\nprinter\nbed.\n-\nThe\nangle\nof\nthe\ncamera\nrelative\nto\nthe\nprinter\nis\nexpected\nto\nbe\nroughly\nconsistent.\n3.\nSignal\nStream:\n-\nManaged\nby\nthe\nSKR\nboard,\nwhich\ncontrols\nthe\n3D\nprinter.\n-\nA\nhardware\nconnection\nlinks\nthe\nSKR\nboard\nto\nour\nRaspberry\nPi.\n-\nThe\nRaspberry\nPi\ngets\na\nsignal\nevery\ntime\na\nmovement\ncompletes.\nThis\nis\nthe\nsignal,\nand\nconsists\nof\nthe\nbed\nposition\nand\ntime.\n[time,\nx,\ny,\nz]\n-\nThis\ndata\nstreams\nin\nreal\ntime,\nparallel\nto\nthe\nprinting\nprocess.\nInitialization\nFirst\nand\nforemost,\nthe\nsystem,\nstarting\nin\nthe\nmain\nTracker\nThread,\nbegins\nreceiving\nand\nrouting\nframes\nand\nsignals\nthrough\ntheir\ncorresponding\nrouters.\nThese\nrouters\nwill\npass\nthe\ndata\nto\nthe\ncorrect\nobjects\nand\nthreads\nthrough\nthe\nshared\nglobal\nvariable\nspace\nbased\non\nthe\ncurrent\nstate\nof\nthe\nsystem\n(Initializing\nor\nTracking).", "STREAM\nAI\n1\n13\nThe\nduty\nof\nthe\ninitialization\nphase\nis\nto\nautonomously\nadapt\nto\nthe\ncamera\nposition.\nIt\nprimarily\nperforms\nthese\ntasks:\n1.\nMake\ninitial\npredictions:\na.\nTo\ntransform\nG-code\ninto\npredictions\nfor\nevery\nmoment\nin\ntime,\nwe\nfirst\nbreak\ndown\nthe\nG-code\ninto\na\nsequence\nof\nmotion\ninstructions\nbetween\ncorners\n(or\nwaypoints)\nwhere\nthere\nare\nchanges\nin\ndirection\nor\nspeed.\nb.\nFor\neach\nof\nthese\nmotions,\nwe\ncalculate\nthe\noptimal\nspeed\nprofile\nconsidering\nconstraints\nlike\nmaximum\nspeeds\nand\nacceleration.\nWe\nneed\nto\nlook\nat\nthe\nfuture\n2\nmoves\nas\nwell,\nto\ndetermine\nwhat\nthe\nfinal\nspeed\nat\nthe\nend\nof\nthe\nmotion\nshould\nbe.\nc.\nGiven\nthe\noptimal\nspeed\nprofiles\nfor\neach\nmotion,\nwe\u2019ll\ncalculate\nhow\nlong\neach\nmove\nshould\ntake\nusing\nkinematics.\nOne\nproblem\ncannot\nbe\nsolved\nanalytically ,\nand\nneeds\nto\nbe\nsolved\niteratively.\nThe\niterative\nmethod\ntweaks\nthe\ndistance\nover\nwhich\nwe\naccelerate\nor\ndecelerate,\nattempting\nto\nconverge\non\nthe\ndesired\nfinal\nspeed\nwithout\nexceeding\nthe\nmachine's\nconstraints.\nd.\nAfter\ncomputing\nthe\nspeed\nprofile,\nthe\nscript\nthen\ndivides\nthe\nmotion\ninto\nframes\n(based\non\nthe\ngiven\nframes-per -second\nparameter)\nand\npredicts\nthe\nposition\nand\nangle\nof\nthe\nmachine's\ntip\nfor\neach\nframe.\nBy\nthe\nend\nof\nthis\nprocess,\nwe\nhave\na\ndetailed\naccount\nof\nwhere\nthe\nmachine\nshould\nbe\nand\nat\nwhat\nangle\nfor\nevery\nsingle\nframe\nof\nthe\nmotion.\n2.\nDerive\nmillimeter\nto\npixel\nratio:\nFor\nrobustness,\nthe\nsystem\nwill\nautonomously\nmap\nbed\npredictions\nto\nscreen\npredictions\nwithout\nrequiring\nusers\nto\nenter\nany\nconfiguring\nvariables.\nBy\nwaiting\nfor\ntwo\nsignals\nand\nobserving\ntheir\nmillimeter\ndisparity\non\nthe\nbed,\nwe\ncan\ncorrelate\nthis\nwith\nthe\npixel\ndifference\nYOLO\ndetects,\nfacilitating\nthe\nnecessary\nconversion.\nWe\nslide\na\n640x640\nwindow\nacross\nthe\nimage\nto\nrun\nYOLO.\n3.\nSynchr onize\npredictions\nwith\nvideo:\nTo\ninitiate\nthe\nprocess,\nwe\nalign\nour\nscreen\npredictions\nusing\nthe\ntimestamp\nof\nthe\nfirst\nYOLO-signal\ninference\nfrom\nthe\nvideo.\n", "STREAM\nAI\n1\n14\nPredictions\nare\nimperfect\ndue\nto\naccumulated\nsmall\ntracking\nerrors.\nReasons\ninclude:\n1.\nMerlin\nfirmware\nin\nour\n3D\nprinter\nemploys\nBeziet\u2019s\n6th\norder\nfunction\nfor\nmovement\nextrapolation,\nwhile\nour\nalgorithm\nuses\na\nsimpler\n2nd\norder\nfunction.\nHowever,\nour\nmodel\nconsiders\nacceleration.\n2.\nSome\nprinters\nadjust\nmovement\nfactors,\nlike\nspeed\nand\nacceleration.\n3.\nMinor\ndeviations\narise\nwhen\nthe\nprinter\nhalts.\n4.\nMerlin\nFirmware\nhas\nintricate\nmechanisms\nfor\ngcode\ninterpretation.\nInstead\nof\nreplicating\nMerlin's\nexact\napproach,\nwe\ndesigned\na\nstraightforward\ninterpreter\nwith\nreal-time\nerror\nfeedback\nloops,\nenhancing\nsystem\nrobustness\nand\ncompatibility\nwith\nvarious\n3D\nprinters.\nError\nCorr ection\nPredictions\nface\ntwo\nerror\ntypes:\nTemporal\nand\nSpatial\n,\naddressed\nby\nthe\nErrorCorrection\nthread.\n1.\nSpatial\nError:\nMainly\narises\nfrom\nincorrect\nratios\nor\nYOLO\ninference.\nThese\nare\nrare,\ntypically\nminor\ninitial\ntracking\ndeviations.\nFor\ndemonstration,\nan\nexaggerated\nerror\nwas\nintentionally\nintroduced\nin\nthe\nSpatial\nError\nCorrection\nvideo.\nMeasure\nDetect\nCorrect\nEverytime\na\nsignal\narrives,\nwe\nrun \nYOLO\nto\nsee\nwhere\nthe\ntip\nis.\nWe \ncompare\nthis\nto\nour\nprediction\nand \nmeasure\nthe\ndifference\nin\npixels.\nWhen\nthere\nis\na\nconsistent\nspatial\nerror \nof\nthe\nsame\nmagnitude\nand\ndirection\nfor \nsufficient\ntime.\nShift\nall\npredictions\nby \nspatial\nerror.\n2.\nTemporal\nError:\nThis\nwas\na\nsignificant\nchallenge.\nWithout\ninput\nsignals,\ngauging\ntemporal\nerror\nwas\ntough.\nThe\nultimate\nsolution\nsimply\ninvolved\nimmediate\nmeasurement\nand\ncorrection\n", "STREAM\nAI\n1\n15\nof\ntemporal\ndiscrepancies.\nThis\nerror\nstems\nfrom\nvarying\ninterpretations\nof\ngcode,\nespecially\ndiffering\naccelerations.\nMeasure\nDetect\nCorrect\nEverytime\na\nsignal\narrives,\nwe \ncompare\nthe\ntime\nof\nthe\nsignal, \ncompared\nto\nour\npredictions.\nWe \nmeasure\nerror\nin\nseconds.\nWe\ndon\u2019t\nlook\nfor\nany\nconsistent \nerrors.\nWhen\nthere\nis\nan\nerror,\nwe \nsimply\nperform\na\ncorrection.\nPause\nor\nskip\nahead\nto\nrealign.\nAnalytics\nIncorporating\na\nreal-time\nfeedback\nloop\n(autonomous\ncorrections)\nresults\nin\nnear-flawless\ntip\ntracking.\nUsing\nYOLO\nisn't\nconstant;\nit's\nemployed\nduring\nsignals\nand\nerror\nmeasurements.\nThis\nenables\nefficient\npinpointing\nof\nthe\nrecently\nextruded\nmaterial\naround\nthe\ntip\u2014our\ntarget.\nGiven\nour\nknowledge\nfrom\nthe\ngcode\nabout\nthe\nprinter's\nmovement\ndirection,\nwe\ncrop\naccordingly .\nThis\ncropped\nimage\nthen\nenters\nour\ndata\npreprocessing\npipeline\nand\nundergoes\nclassification\nby\nthe\ntailored\nMobileNet\nmodel,\ndetermining\nwhether\nextrusion\nis\nunder,\nnormal,\nor\nover.", "STREAM\nAI\n1\n16\nMetrics\nTo\nevaluate\nour\nsystem,\nwe'll\nfocus\non\nits\ncomputational\nefficiency,\nwhile\nthe\naccuracy\nis\nderived\nfrom\nthe\nMobileNet\nmodel's\ntest\nperformance.\nWe'll\nassess:\n-\nRAM\nUsage\n:\nWe'll\nmonitor\nRAM\nconsumption\nover\ntime.\n-\nInfer ence\nIntervals\n:\nGiven\nthe\nadaptive\nnature\nof\nour\nMobileNet\nmodel,\nobserving\nthe\nintervals\nbetween\nsuccessive\ninferences\nprovides\na\nmeasure\nof\nsystem\nspeed.\nThe\nmodel\nrefrains\nfrom\ninferring\nwith\na\nfull\nbuffer.\nThese\nmetrics\nwill\nbe\ngauged\non\nboth\nmy\ndevice\nand\na\nRaspberry\nPi\n4,\nwith\nrespective\nspecifications\nprovided\nbelow.\nDeployment\nto\nRaspberry\nPi\nRaspberry\nPi\nMetrics\nFuture\nImprovements\n", "STREAM\nAI\n1\n17\n1.\nProcessor\n(Chipset)\nDetails:\n-\nModel:\nApple\nM1\nPro\n-\nTotal\nNumber\nof\nCores:\n14\n2.\nMemory:\n-\nSize:\n16\nGB\n-\nType:\nLPDDR5\n-\nManufacturer:\nHynix\n3.\nGraphics/GPU:\n-\nChipset\nModel:\nApple\nM1\nPro\n-\nMetal\nSupport:\nMetal\n3\n4.\nDisplay:\n-\nType:\nBuilt-in\nLiquid\nRetina\nXDR\nDisplay\n-\nResolution:\n3024\nx\n1964\nRetina\n5.\nStorage:\n-\nType:\nSSD\n(APPLE\nSSD\nAP0512R)\n-\nCapacity:\n500.28\nGB\n-\nProtocol:\nApple\nFabric\n(indicating\nit's\nan\nNVMe\nSSD)\n-\nFree\nSpace:\n120.4\nGB\n6.\nFile\nSystem:\n-\nType:\nAPFS\nhttps://qengineering.eu/install-opencv-on-raspberry-pi.html\nhttps://qengineering.eu/install-pytorch-on-raspberry-pi-4.html\nhttps://gist.github.com/wenig/8bab88dede5c838660dd05b8e5b2e23b\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv .html"]