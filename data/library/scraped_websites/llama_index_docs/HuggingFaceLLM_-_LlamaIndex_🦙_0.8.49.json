{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/llms/huggingface.html",
        "title": "HuggingFaceLLM - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "{\n   \"title\": \"HuggingFaceLLM\",\n   \"description\": \"HuggingFace LLM.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"callback\\_manager\": {\n         \"title\": \"Callback Manager\"\n      },\n      \"model\\_name\": {\n         \"title\": \"Model Name\",\n         \"description\": \"The model name to use from HuggingFace. Unused if \\`model\\` is passed in directly.\",\n         \"type\": \"string\"\n      },\n      \"context\\_window\": {\n         \"title\": \"Context Window\",\n         \"description\": \"The maximum number of tokens available for input.\",\n         \"type\": \"integer\"\n      },\n      \"max\\_new\\_tokens\": {\n         \"title\": \"Max New Tokens\",\n         \"description\": \"The maximum number of tokens to generate.\",\n         \"type\": \"integer\"\n      },\n      \"system\\_prompt\": {\n         \"title\": \"System Prompt\",\n         \"description\": \"The system prompt, containing any extra instructions or context. The model card on HuggingFace should specify if this is needed.\",\n         \"type\": \"string\"\n      },\n      \"query\\_wrapper\\_prompt\": {\n         \"title\": \"Query Wrapper Prompt\",\n         \"description\": \"The query wrapper prompt, containing the query placeholder. The model card on HuggingFace should specify if this is needed. Should contain a \\`{query\\_str}\\` placeholder.\",\n         \"type\": \"string\"\n      },\n      \"tokenizer\\_name\": {\n         \"title\": \"Tokenizer Name\",\n         \"description\": \"The name of the tokenizer to use from HuggingFace. Unused if \\`tokenizer\\` is passed in directly.\",\n         \"type\": \"string\"\n      },\n      \"device\\_map\": {\n         \"title\": \"Device Map\",\n         \"description\": \"The device\\_map to use. Defaults to 'auto'.\",\n         \"type\": \"string\"\n      },\n      \"stopping\\_ids\": {\n         \"title\": \"Stopping Ids\",\n         \"description\": \"The stopping ids to use. Generation stops when these token IDs are predicted.\",\n         \"type\": \"array\",\n         \"items\": {\n            \"type\": \"integer\"\n         }\n      },\n      \"tokenizer\\_outputs\\_to\\_remove\": {\n         \"title\": \"Tokenizer Outputs To Remove\",\n         \"description\": \"The outputs to remove from the tokenizer. Sometimes huggingface tokenizers return extra inputs that cause errors.\",\n         \"type\": \"array\",\n         \"items\": {}\n      },\n      \"tokenizer\\_kwargs\": {\n         \"title\": \"Tokenizer Kwargs\",\n         \"description\": \"The kwargs to pass to the tokenizer.\",\n         \"type\": \"object\"\n      },\n      \"model\\_kwargs\": {\n         \"title\": \"Model Kwargs\",\n         \"description\": \"The kwargs to pass to the model during initialization.\",\n         \"type\": \"object\"\n      },\n      \"generate\\_kwargs\": {\n         \"title\": \"Generate Kwargs\",\n         \"description\": \"The kwargs to pass to the model during generation.\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": \\[\n      \"model\\_name\",\n      \"context\\_window\",\n      \"max\\_new\\_tokens\",\n      \"system\\_prompt\",\n      \"query\\_wrapper\\_prompt\",\n      \"tokenizer\\_name\"\n   \\]\n}"
}