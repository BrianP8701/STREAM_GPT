{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html",
        "title": "Auto Merging Retriever - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Auto Merging Retriever[\uf0c1](#auto-merging-retriever \"Permalink to this heading\")\n\nIn this notebook, we showcase our `AutoMergingRetriever`, which looks at a set of leaf nodes and recursively \u201cmerges\u201d subsets of leaf nodes that reference a parent node beyond a given threshold. This allows us to consolidate potentially disparate, smaller contexts into a larger context that might help synthesis.\n\nYou can define this hierarchy yourself over a set of documents, or you can make use of our brand-new text parser: a HierarchicalNodeParser that takes in a candidate set of documents and outputs an entire hierarchy of nodes, from \u201ccoarse-to-fine\u201d.\n\n%load\\_ext autoreload\n%autoreload 2\n\n## Load Data[\uf0c1](#load-data \"Permalink to this heading\")\n\nLet\u2019s first load the Llama 2 paper: https://arxiv.org/pdf/2307.09288.pdf. This will be our test data.\n\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\n\n\\# from llama\\_hub.file.pdf.base import PDFReader\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\n\\# docs0 = loader.load\\_data(file=Path(\"./data/llama2.pdf\"))\ndocs0 \\= loader.load(file\\_path\\=Path(\"./data/llama2.pdf\"))\n\nBy default, the PDF reader creates a separate doc for each page. For the sake of this notebook, we stitch docs together into one doc. This will help us better highlight auto-merging capabilities that \u201cstitch\u201d chunks together later on.\n\nfrom llama\\_index import Document\n\ndoc\\_text \\= \"\\\\n\\\\n\".join(\\[d.get\\_content() for d in docs0\\])\ndocs \\= \\[Document(text\\=doc\\_text)\\]\n\n## Parse Chunk Hierarchy from Text, Load into Storage[\uf0c1](#parse-chunk-hierarchy-from-text-load-into-storage \"Permalink to this heading\")\n\nIn this section we make use of the `HierarchicalNodeParser`. This will output a hierarchy of nodes, from top-level nodes with bigger chunk sizes to child nodes with smaller chunk sizes, where each child node has a parent node with a bigger chunk size.\n\nBy default, the hierarchy is:\n\n*   1st level: chunk size 2048\n    \n*   2nd level: chunk size 512\n    \n*   3rd level: chunk size 128\n    \n\nWe then load these nodes into storage. The leaf nodes are indexed and retrieved via a vector store - these are the nodes that will first be directly retrieved via similarity search. The other nodes will be retrieved from a docstore.\n\nfrom llama\\_index.node\\_parser import HierarchicalNodeParser, SimpleNodeParser\n\nnode\\_parser \\= HierarchicalNodeParser.from\\_defaults()\n\nnodes \\= node\\_parser.get\\_nodes\\_from\\_documents(docs)\n\nHere we import a simple helper function for fetching \u201cleaf\u201d nodes within a node list. These are nodes that don\u2019t have children of their own.\n\nfrom llama\\_index.node\\_parser import get\\_leaf\\_nodes, get\\_root\\_nodes\n\nleaf\\_nodes \\= get\\_leaf\\_nodes(nodes)\n\nroot\\_nodes \\= get\\_root\\_nodes(nodes)\n\n### Load into Storage[\uf0c1](#load-into-storage \"Permalink to this heading\")\n\nWe define a docstore, which we load all nodes into.\n\nWe then define a `VectorStoreIndex` containing just the leaf-level nodes.\n\n\\# define storage context\nfrom llama\\_index.storage.docstore import SimpleDocumentStore\nfrom llama\\_index.storage import StorageContext\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import OpenAI\n\ndocstore \\= SimpleDocumentStore()\n\n\\# insert nodes into docstore\ndocstore.add\\_documents(nodes)\n\n\\# define storage context (will include vector store by default too)\nstorage\\_context \\= StorageContext.from\\_defaults(docstore\\=docstore)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=OpenAI(model\\=\"gpt-3.5-turbo\")\n)\n\n\\## Load index into vector index\nfrom llama\\_index import VectorStoreIndex\n\nbase\\_index \\= VectorStoreIndex(\n    leaf\\_nodes,\n    storage\\_context\\=storage\\_context,\n    service\\_context\\=service\\_context,\n)\n\n## Define Retriever[\uf0c1](#define-retriever \"Permalink to this heading\")\n\nfrom llama\\_index.retrievers.auto\\_merging\\_retriever import AutoMergingRetriever\n\nbase\\_retriever \\= base\\_index.as\\_retriever(similarity\\_top\\_k\\=6)\nretriever \\= AutoMergingRetriever(base\\_retriever, storage\\_context, verbose\\=True)\n\n\\# query\\_str = \"What were some lessons learned from red-teaming?\"\n\\# query\\_str = \"Can you tell me about the key concepts for safety finetuning\"\nquery\\_str \\= (\n    \"What could be the potential outcomes of adjusting the amount of safety\"\n    \" data used in the RLHF stage?\"\n)\n\nnodes \\= retriever.retrieve(query\\_str)\nbase\\_nodes \\= base\\_retriever.retrieve(query\\_str)\n\n\\> Merging 4 nodes into parent node.\n> Parent node id: caf5f81c-842f-46a4-b679-6be584bd6aff.\n> Parent node text: We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: an...\n\nfrom llama\\_index.response.notebook\\_utils import display\\_source\\_node\n\nfor node in nodes:\n    display\\_source\\_node(node, source\\_length\\=10000)\n\n**Node ID:** d4d67180-71c8-4328-b3f1-1e98fa42ab69  \n**Similarity:** 0.8694979150607424  \n**Text:** We also list two qualitative examples where safety and helpfulness reward models don\u2019t agree with each other in Table 35. A.4.2 Qualitative Results on Safety Data Scaling In Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner. Here we showcase a few samples to qualitatively examine the evolution of model behavior when we scale safety data in Tables 36, 37, and 38. In general, we are observing that Llama 2-Chat becomes safer responding to unsafe prompts with more safety data used.  \n\n**Node ID:** caf5f81c-842f-46a4-b679-6be584bd6aff  \n**Similarity:** 0.86168727941324  \n**Text:** We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: annotators write a prompt that they believe can elicit unsafe behavior, and then compare multiple model responses to the prompts, selecting the response that is safest according to a set of guidelines. We then use the human preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to sample from the model during the RLHF stage. Better Long-Tail Safety Robustness without Hurting Helpfulness Safety is inherently a long-tail problem, where the challenge comes from a small number of very specific cases. We investigate the impact of Safety RLHF by taking two intermediate Llama 2-Chat checkpoints\u2014one without adversarial prompts in the RLHF stage and one with them\u2014and score their responses on our test sets using our safety and helpfulness reward models. In Figure 14, we plot the score distribution shift of the safety RM on the safety test set (left) and that of the helpfulness RM on the helpfulness test set (right). In the left hand side of the figure, we observe that the distribution of safety RM scores on the safety set shifts to higher reward scores after safety tuning with RLHF, and that the long tail of the distribution near zero thins out. A clear cluster appears on the top-left corner suggesting the improvements of model safety. On the right side, we do not observe any gathering pattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score distribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training data, the addition of an additional stage of safety mitigation does not negatively impact model performance on helpfulness to any notable degradation. A qualitative example is shown in Table 12. Impact of Safety Data Scaling. A tension between helpfulness and safety of LLMs has been observed in previous studies (Bai et al., 2022a). To better understand how the addition of safety training data affects general model performance, especially helpfulness, we investigate the trends in safety data scaling by adjusting the amount of safety data used in the RLHF stage.  \n\n**Node ID:** d9893bef-a5a7-4248-a0a1-d7c28800ae59  \n**Similarity:** 0.8546977459150967  \n**Text:** 0 0.2 0.4 0.6 0.8 1.0 Helpfulness RM Score before Safety RLHF 0.0 0.2 0.4 0.6 0.8 1.0 Helpfulness RM Score after Safety RLHF 0 1000 0 1000 Figure 14: Impact of safety RLHF measured by reward model score distributions. Left: safety reward model scores of generations on the Meta Safety test set. The clustering of samples in the top left corner suggests the improvements of model safety.  \n\nfor node in base\\_nodes:\n    display\\_source\\_node(node, source\\_length\\=10000)\n\n**Node ID:** 16328561-9ff7-4307-8d31-adf6bb74b71b  \n**Similarity:** 0.8770715326726375  \n**Text:** A qualitative example is shown in Table 12. Impact of Safety Data Scaling. A tension between helpfulness and safety of LLMs has been observed in previous studies (Bai et al., 2022a). To better understand how the addition of safety training data affects general model performance, especially helpfulness, we investigate the trends in safety data scaling by adjusting the amount of safety data used in the RLHF stage.  \n\n**Node ID:** e756d327-1a28-4228-ac38-f8a831b1bf77  \n**Similarity:** 0.8728111844788112  \n**Text:** A clear cluster appears on the top-left corner suggesting the improvements of model safety. On the right side, we do not observe any gathering pattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score distribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training data, the addition of an additional stage of safety mitigation does not negatively impact model performance on helpfulness to any notable degradation. A qualitative example is shown in Table 12. Impact of Safety Data Scaling.  \n\n**Node ID:** d4d67180-71c8-4328-b3f1-1e98fa42ab69  \n**Similarity:** 0.8697379697028405  \n**Text:** We also list two qualitative examples where safety and helpfulness reward models don\u2019t agree with each other in Table 35. A.4.2 Qualitative Results on Safety Data Scaling In Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner. Here we showcase a few samples to qualitatively examine the evolution of model behavior when we scale safety data in Tables 36, 37, and 38. In general, we are observing that Llama 2-Chat becomes safer responding to unsafe prompts with more safety data used.  \n\n**Node ID:** d9893bef-a5a7-4248-a0a1-d7c28800ae59  \n**Similarity:** 0.855087365309258  \n**Text:** 0 0.2 0.4 0.6 0.8 1.0 Helpfulness RM Score before Safety RLHF 0.0 0.2 0.4 0.6 0.8 1.0 Helpfulness RM Score after Safety RLHF 0 1000 0 1000 Figure 14: Impact of safety RLHF measured by reward model score distributions. Left: safety reward model scores of generations on the Meta Safety test set. The clustering of samples in the top left corner suggests the improvements of model safety.  \n\n**Node ID:** d62ee107-9841-44b5-8b70-bc6487ad6315  \n**Similarity:** 0.8492541852986794  \n**Text:** Better Long-Tail Safety Robustness without Hurting Helpfulness Safety is inherently a long-tail problem, where the challenge comes from a small number of very specific cases. We investigate the impact of Safety RLHF by taking two intermediate Llama 2-Chat checkpoints\u2014one without adversarial prompts in the RLHF stage and one with them\u2014and score their responses on our test sets using our safety and helpfulness reward models.  \n\n**Node ID:** 312a63b3-5e28-4fbf-a3e1-4e8dc0c026ea  \n**Similarity:** 0.8488371951811564  \n**Text:** We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: annotators write a prompt that they believe can elicit unsafe behavior, and then compare multiple model responses to the prompts, selecting the response that is safest according to a set of guidelines. We then use the human preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to sample from the model during the RLHF stage.  \n\n## Plug it into Query Engine[\uf0c1](#plug-it-into-query-engine \"Permalink to this heading\")\n\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever)\nbase\\_query\\_engine \\= RetrieverQueryEngine.from\\_args(base\\_retriever)\n\nresponse \\= query\\_engine.query(query\\_str)\n\n\\> Merging 4 nodes into parent node.\n> Parent node id: 3671b20d-ea5e-4afc-983e-02be6ee8302d.\n> Parent node text: We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: an...\n\nAdjusting the amount of safety data used in the RLHF stage could potentially have the following outcomes:\n1. Improved model safety: Increasing the amount of safety data used in RLHF may lead to improvements in model safety. This means that the model becomes better at responding to unsafe prompts and avoids generating unsafe or harmful outputs.\n2. Thinning out of the long tail of safety RM scores: Increasing the amount of safety data may result in a shift in the distribution of safety reward model (RM) scores towards higher reward scores. This means that the model becomes more consistent in generating safe responses and reduces the occurrence of low safety scores.\n3. Preservation of helpfulness performance: Adjusting the amount of safety data used in RLHF is not expected to negatively impact model performance on helpfulness. This means that the model's ability to generate helpful responses is maintained even after incorporating additional safety training.\n4. Gathering pattern in helpfulness RM scores: There is no observed gathering pattern below the y = x line in the distribution of helpfulness RM scores after safety tuning with RLHF. This suggests that the helpfulness score distribution is preserved, indicating that the model's helpfulness performance is not significantly degraded by the addition of safety mitigation measures.\nOverall, adjusting the amount of safety data used in the RLHF stage aims to strike a balance between improving model safety without compromising its helpfulness performance.\n\nbase\\_response \\= base\\_query\\_engine.query(query\\_str)\n\nprint(str(base\\_response))\n\nAdjusting the amount of safety data used in the RLHF stage could potentially lead to improvements in model safety. This can be observed by a clear cluster appearing on the top-left corner, suggesting enhanced model safety. Additionally, it is indicated that the helpfulness score distribution is preserved after safety tuning with RLHF, indicating that the addition of safety data does not negatively impact model performance on helpfulness.\n\n## Evaluation[\uf0c1](#evaluation \"Permalink to this heading\")\n\nWe evaluate how well the hierarchical retriever works compared to the baseline retriever in a more quantitative manner.\n\n**WARNING**: This can be _expensive_, especially with GPT-4. Use caution and tune the sample size to fit your budget.\n\nfrom llama\\_index.evaluation import (\n    DatasetGenerator,\n    QueryResponseDataset,\n)\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import OpenAI\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\n\\# NOTE: run this if the dataset isn't already saved\n\\# Note: we only generate from the first 20 nodes, since the rest are references\neval\\_service\\_context \\= ServiceContext.from\\_defaults(llm\\=OpenAI(model\\=\"gpt-4\"))\ndataset\\_generator \\= DatasetGenerator(\n    root\\_nodes\\[:20\\],\n    service\\_context\\=eval\\_service\\_context,\n    show\\_progress\\=True,\n    num\\_questions\\_per\\_chunk\\=3,\n)\n\neval\\_dataset \\= await dataset\\_generator.agenerate\\_dataset\\_from\\_nodes(num\\=60)\n\neval\\_dataset.save\\_json(\"data/llama2\\_eval\\_qr\\_dataset.json\")\n\n\\# optional\neval\\_dataset \\= QueryResponseDataset.from\\_json(\n    \"data/llama2\\_eval\\_qr\\_dataset.json\"\n)\n\n### Compare Results[\uf0c1](#compare-results \"Permalink to this heading\")\n\nWe run evaluations on each of the retrievers: correctness, semantic similarity, relevance, and faithfulness.\n\nimport asyncio\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nfrom llama\\_index.evaluation import (\n    CorrectnessEvaluator,\n    SemanticSimilarityEvaluator,\n    RelevancyEvaluator,\n    FaithfulnessEvaluator,\n    PairwiseComparisonEvaluator,\n)\n\nfrom collections import defaultdict\nimport pandas as pd\n\n\\# NOTE: can uncomment other evaluators\nevaluator\\_c \\= CorrectnessEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_s \\= SemanticSimilarityEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_r \\= RelevancyEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_f \\= FaithfulnessEvaluator(service\\_context\\=eval\\_service\\_context)\n\\# pairwise\\_evaluator = PairwiseComparisonEvaluator(service\\_context=eval\\_service\\_context)\n\nfrom llama\\_index.evaluation.eval\\_utils import get\\_responses, get\\_results\\_df\nfrom llama\\_index.evaluation import BatchEvalRunner\n\neval\\_qs \\= eval\\_dataset.questions\nqr\\_pairs \\= eval\\_dataset.qr\\_pairs\nref\\_response\\_strs \\= \\[r for (\\_, r) in qr\\_pairs\\]\n\npred\\_responses \\= get\\_responses(eval\\_qs, query\\_engine, show\\_progress\\=True)\n\nbase\\_pred\\_responses \\= get\\_responses(\n    eval\\_qs, base\\_query\\_engine, show\\_progress\\=True\n)\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 \\[00:07<00:00,  8.17it/s\\]\n\nimport numpy as np\n\npred\\_response\\_strs \\= \\[str(p) for p in pred\\_responses\\]\nbase\\_pred\\_response\\_strs \\= \\[str(p) for p in base\\_pred\\_responses\\]\n\nevaluator\\_dict \\= {\n    \"correctness\": evaluator\\_c,\n    \"faithfulness\": evaluator\\_f,\n    \"relevancy\": evaluator\\_r,\n    \"semantic\\_similarity\": evaluator\\_s,\n}\nbatch\\_runner \\= BatchEvalRunner(evaluator\\_dict, workers\\=2, show\\_progress\\=True)\n\neval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    eval\\_qs, responses\\=pred\\_responses, reference\\=ref\\_response\\_strs\n)\n\nbase\\_eval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    eval\\_qs, responses\\=base\\_pred\\_responses, reference\\=ref\\_response\\_strs\n)\n\nresults\\_df \\= get\\_results\\_df(\n    \\[eval\\_results, base\\_eval\\_results\\],\n    \\[\"Auto Merging Retriever\", \"Base Retriever\"\\],\n    \\[\"correctness\", \"relevancy\", \"faithfulness\", \"semantic\\_similarity\"\\],\n)\ndisplay(results\\_df)\n\n|     | names | correctness | relevancy | faithfulness | semantic\\_similarity |\n| --- | --- | --- | --- | --- | --- |\n| 0   | Auto Merging Retriever | 4.266667 | 0.916667 | 0.95 | 0.962196 |\n| 1   | Base Retriever | 4.208333 | 0.916667 | 0.95 | 0.960602 |\n\n**Analysis**: The results are roughly the same.\n\nLet\u2019s also try to see which answer GPT-4 prefers with our pairwise evals.\n\nbatch\\_runner \\= BatchEvalRunner(\n    {\"pairwise\": pairwise\\_evaluator}, workers\\=10, show\\_progress\\=True\n)\n\npairwise\\_eval\\_results \\= await batch\\_runner.aevaluate\\_response\\_strs(\n    eval\\_qs,\n    response\\_strs\\=pred\\_response\\_strs,\n    reference\\=base\\_pred\\_response\\_strs,\n)\npairwise\\_score \\= np.array(\n    \\[r.score for r in pairwise\\_eval\\_results\\[\"pairwise\"\\]\\]\n).mean()\n\n**Analysis**: The pairwise comparison score is a measure of the percentage of time the candidate answer (using auto-merging retriever) is preferred vs. the base answer (using the base retriever). Here we see that it\u2019s roughly even."
}