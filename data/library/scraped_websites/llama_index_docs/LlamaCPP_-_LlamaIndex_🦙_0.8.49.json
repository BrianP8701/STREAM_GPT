{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/llms/llama_cpp.html",
        "title": "LlamaCPP - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "{\n   \"title\": \"LlamaCPP\",\n   \"description\": \"Simple abstract base class for custom LLMs.\\\\n\\\\nSubclasses must implement the \\`\\_\\_init\\_\\_\\`, \\`complete\\`,\\\\n    \\`stream\\_complete\\`, and \\`metadata\\` methods.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"callback\\_manager\": {\n         \"title\": \"Callback Manager\"\n      },\n      \"model\\_url\": {\n         \"title\": \"Model Url\",\n         \"description\": \"The URL llama-cpp model to download and use.\",\n         \"type\": \"string\"\n      },\n      \"model\\_path\": {\n         \"title\": \"Model Path\",\n         \"description\": \"The path to the llama-cpp model to use.\",\n         \"type\": \"string\"\n      },\n      \"temperature\": {\n         \"title\": \"Temperature\",\n         \"description\": \"The temperature to use for sampling.\",\n         \"type\": \"number\"\n      },\n      \"max\\_new\\_tokens\": {\n         \"title\": \"Max New Tokens\",\n         \"description\": \"The maximum number of tokens to generate.\",\n         \"type\": \"integer\"\n      },\n      \"context\\_window\": {\n         \"title\": \"Context Window\",\n         \"description\": \"The maximum number of context tokens for the model.\",\n         \"default\": 3900,\n         \"type\": \"integer\"\n      },\n      \"generate\\_kwargs\": {\n         \"title\": \"Generate Kwargs\",\n         \"description\": \"Kwargs used for generation.\",\n         \"type\": \"object\"\n      },\n      \"model\\_kwargs\": {\n         \"title\": \"Model Kwargs\",\n         \"description\": \"Kwargs used for model initialization.\",\n         \"type\": \"object\"\n      },\n      \"verbose\": {\n         \"title\": \"Verbose\",\n         \"description\": \"Whether to print verbose output.\",\n         \"default\": true,\n         \"type\": \"boolean\"\n      }\n   },\n   \"required\": \\[\n      \"temperature\",\n      \"max\\_new\\_tokens\"\n   \\]\n}"
}