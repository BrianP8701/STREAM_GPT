{
    "metadata": {
        "type": "repo",
        "path": "stream_gpt/data_scrapers/github_repo_scraper.py"
    },
    "text": "import requests\nimport base64\nfrom stream_gpt.types.document import Document\nfrom stream_gpt.interfaces.scraper_interface import Scraper\n\nclass RepoScraper(Scraper):\n    def __init__(self, token):\n        self.token = token\n        \n    def scrape(self, repo_url, ignore_folders=[\"venv\"], ignore_files=[]):\n        \"\"\"\n        Fetches the content of all files in a given GitHub repository.\n\n        This function uses the GitHub API to recursively fetch the content of all files in a given repository.\n        It returns a dictionary where keys are file paths and values are the content of those files. By default,\n        this function will ignore any 'venv' directories. However, specific folders and files can be excluded\n        from the results by passing them in `ignore_folders` and `ignore_files` respectively.\n\n        Parameters:\n        - repo_url (str): The URL of the GitHub repository. Should be in the format 'https://github.com/{owner}/{repository_name}'.\n        - token (str): The GitHub personal access token used for authentication.\n        - ignore_folders (list, optional): A list of folder names to ignore. Defaults to [\"venv\"].\n        - ignore_files (list, optional): A list of file names to ignore. Defaults to an empty list.\n\n        Returns:\n        - dict: A dictionary where keys are file paths (relative to the root of the repo) and values are the content of those files.\n\n        Raises:\n        - HTTPError: If there's an issue with the API request.\n\n        Example usage:\n        TOKEN = 'YOUR_PERSONAL_ACCESS_TOKEN'\n        REPO_URL = 'https://github.com/BrianP8701/STREAM_GPT'\n        content_dict = fetch_github_repo_content(REPO_URL, TOKEN)\n        for path, content in content_dict.items():\n            print(f\"File path: {path}\\nContent:\\n{content}\\n{'-'*40}\\n\")\n        \"\"\"\n        headers = {\n            'Authorization': f'token {self.token}',\n            'Accept': 'application/vnd.github.v3+json'\n        }\n        \n        # Extract owner and repo from URL\n        owner, repo = repo_url.rstrip('/').split('/')[-2:]\n        \n        # Recursive function to fetch content\n        def fetch_path_content(path):\n            response = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents/{path}', headers=headers)\n            response.raise_for_status()  # Raises exception for HTTP errors\n            contents = response.json()\n            \n            if isinstance(contents, dict):  # Single file\n                if contents['name'] not in ignore_files:\n                    return {contents['path']: base64.b64decode(contents['content']).decode('utf-8')}\n                return {}\n\n            all_contents = {}\n            for item in contents:\n                if item['type'] == 'dir' and item['name'] not in ignore_folders:\n                    all_contents.update(fetch_path_content(item['path']))\n                elif item['type'] == 'file' and item['name'] not in ignore_files:\n                    all_contents.update(fetch_path_content(item['path']))\n            return all_contents\n\n        return self.convert_scraped_data_to_documents(fetch_path_content(\"\"))\n    \n    @staticmethod\n    def convert_scraped_data_to_documents(scraped_data):\n        documents=[]\n        for path, markdown in scraped_data.items():\n            documents.append(Document(metadata={\n                'type': 'github_repo', \n                'path': path\n            }, text=markdown))\n        return documents"
}