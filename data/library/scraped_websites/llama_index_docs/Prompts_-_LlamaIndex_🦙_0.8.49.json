{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/model_modules/prompts.html",
        "title": "Prompts - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Prompts[\uf0c1](#prompts \"Permalink to this heading\")\n\n## Concept[\uf0c1](#concept \"Permalink to this heading\")\n\nPrompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion, perform traversal during querying, and to synthesize the final answer.\n\nLlamaIndex uses a set of [default prompt templates](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py) that work well out of the box.\n\nIn addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py).\n\nUsers may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\n### Defining a custom prompt[\uf0c1](#defining-a-custom-prompt \"Permalink to this heading\")\n\nDefining a custom prompt is as simple as creating a format string\n\nfrom llama\\_index.prompts import PromptTemplate\n\ntemplate \\= (\n    \"We have provided context information below. \\\\n\"\n    \"---------------------\\\\n\"\n    \"{context\\_str}\"\n    \"\\\\n\\---------------------\\\\n\"\n    \"Given this information, please answer the question: {query\\_str}\\\\n\"\n)\nqa\\_template \\= PromptTemplate(template)\n\n\\# you can create text prompt (for completion API)\nprompt \\= qa\\_template.format(context\\_str\\=..., query\\_str\\=...)\n\n\\# or easily convert to message prompts (for chat API)\nmessages \\= qa\\_template.format\\_messages(context\\_str\\=..., query\\_str\\=...)\n\n> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `PromptTemplate`). Now you can directly specify `PromptTemplate(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.\n\nYou can also define a template from chat messages\n\nfrom llama\\_index.prompts import ChatPromptTemplate, ChatMessage, MessageRole\n\nmessage\\_templates \\= \\[\n    ChatMessage(content\\=\"You are an expert system.\", role\\=MessageRole.SYSTEM),\n    ChatMessage(\n        content\\=\"Generate a short story about {topic}\",\n        role\\=MessageRole.USER,\n    ),\n\\]\nchat\\_template \\= ChatPromptTemplate(message\\_templates\\=message\\_templates)\n\n\\# you can create message prompts (for chat API)\nmessages \\= chat\\_template.format\\_messages(topic\\=...)\n\n\\# or easily convert to text prompt (for completion API)\nprompt \\= chat\\_template.format(topic\\=...)\n\n### Passing custom prompts into the pipeline[\uf0c1](#passing-custom-prompts-into-the-pipeline \"Permalink to this heading\")\n\nSince LlamaIndex is a multi-step pipeline, it\u2019s important to identify the operation that you want to modify and pass in the custom prompt at the right place.\n\nAt a high-level, prompts are used in 1) index construction, and 2) query engine execution\n\nThe most commonly used prompts will be the `text_qa_template` and the `refine_template`.\n\n*   `text_qa_template` - used to get an initial answer to a query using retrieved nodes\n    \n*   `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\"compact\"` (the default), or when more than one node is retrieved using `response_mode=\"refine\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\n    \n\n#### Modify prompts used in index construction[\uf0c1](#modify-prompts-used-in-index-construction \"Permalink to this heading\")\n\nDifferent indices use different types of prompts during construction (some don\u2019t use prompts at all). For instance, `TreeIndex` uses a summary prompt to hierarchically summarize the nodes, and `KeywordTableIndex` uses a keyword extract prompt to extract keywords.\n\nThere are two equivalent ways to override the prompts:\n\n1.  via the default nodes constructor\n    \n\nindex \\= TreeIndex(nodes, summary\\_template\\=<custom\\_prompt\\>)\n\n2.  via the documents constructor.\n    \n\nindex \\= TreeIndex.from\\_documents(docs, summary\\_template\\=<custom\\_prompt\\>)\n\nFor more details on which index uses which prompts, please visit [Index class references](https://docs.llamaindex.ai/en/stable/api_reference/indices.html).\n\n#### Modify prompts used in query engine[\uf0c1](#modify-prompts-used-in-query-engine \"Permalink to this heading\")\n\nMore commonly, prompts are used at query-time (i.e. for executing a query against an index and synthesizing the final response).\n\nThere are also two equivalent ways to override the prompts:\n\n1.  via the high-level API\n    \n\nquery\\_engine \\= index.as\\_query\\_engine(\n    text\\_qa\\_template\\=<custom\\_qa\\_prompt\\>,\n    refine\\_template\\=<custom\\_refine\\_prompt\\>\n)\n\n2.  via the low-level composition API\n    \n\nretriever \\= index.as\\_retriever()\nsynth \\= get\\_response\\_synthesizer(\n    text\\_qa\\_template\\=<custom\\_qa\\_prompt\\>,\n    refine\\_template\\=<custom\\_refine\\_prompt\\>\n)\nquery\\_engine \\= RetrieverQueryEngine(retriever, response\\_synthesizer)\n\nThe two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity. You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\n\nFor more details on which classes use which prompts, please visit [Query class references](https://docs.llamaindex.ai/en/stable/api_reference/query.html).\n\nCheck out the [reference documentation](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html) for a full set of all prompts."
}