{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores.html",
        "title": "Using Vector Stores - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "LlamaIndex offers multiple integration points with vector stores / vector databases:\n\n## Using a Vector Store as an Index[\uf0c1](#using-a-vector-store-as-an-index \"Permalink to this heading\")\n\nLlamaIndex also supports different vector stores as the storage backend for `VectorStoreIndex`.\n\n*   Azure Cognitive Search (`CognitiveSearchVectorStore`). [Quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector)\n    \n*   [Apache Cassandra\u00ae](https://cassandra.apache.org/) and compatible databases such as [Astra DB](https://www.datastax.com/press-release/datastax-adds-vector-search-to-astra-db-on-google-cloud-for-building-real-time-generative-ai-applications) (`CassandraVectorStore`)\n    \n*   Chroma (`ChromaVectorStore`) [Installation](https://docs.trychroma.com/getting-started)\n    \n*   Epsilla (`EpsillaVectorStore`) [Installation/Quickstart](https://epsilla-inc.gitbook.io/epsilladb/quick-start)\n    \n*   DeepLake (`DeepLakeVectorStore`) [Installation](https://docs.deeplake.ai/en/latest/Installation.html)\n    \n*   Elasticsearch (`ElasticsearchStore`) [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\n    \n*   Qdrant (`QdrantVectorStore`) [Installation](https://qdrant.tech/documentation/install/) [Python Client](https://qdrant.tech/documentation/install/#python-client)\n    \n*   Weaviate (`WeaviateVectorStore`). [Installation](https://weaviate.io/developers/weaviate/installation). [Python Client](https://weaviate.io/developers/weaviate/client-libraries/python).\n    \n*   Zep (`ZepVectorStore`). [Installation](https://docs.getzep.com/deployment/quickstart/). [Python Client](https://docs.getzep.com/sdk/).\n    \n*   Pinecone (`PineconeVectorStore`). [Installation/Quickstart](https://docs.pinecone.io/docs/quickstart).\n    \n*   Faiss (`FaissVectorStore`). [Installation](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md).\n    \n*   Milvus (`MilvusVectorStore`). [Installation](https://milvus.io/docs)\n    \n*   Zilliz (`MilvusVectorStore`). [Quickstart](https://zilliz.com/doc/quick_start)\n    \n*   MyScale (`MyScaleVectorStore`). [Quickstart](https://docs.myscale.com/en/quickstart/). [Installation/Python Client](https://docs.myscale.com/en/python-client/).\n    \n*   Supabase (`SupabaseVectorStore`). [Quickstart](https://supabase.github.io/vecs/api/).\n    \n*   DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). [Installation/Python Client](https://github.com/docarray/docarray#installation).\n    \n*   MongoDB Atlas (`MongoDBAtlasVectorSearch`). [Installation/Quickstart](https://www.mongodb.com/atlas/database).\n    \n*   Redis (`RedisVectorStore`). [Installation](https://redis.io/docs/getting-started/installation/).\n    \n*   Neo4j (`Neo4jVectorIndex`). [Installation](https://neo4j.com/docs/operations-manual/current/installation/).\n    \n*   TimeScale (`TimescaleVectorStore`). [Installation](https://github.com/timescale/python-vector).\n    \n*   DashVector(`DashVectorStore`).[Installation](https://help.aliyun.com/document_detail/2510230.html).\n    \n\nA detailed API reference is [found here](https://docs.llamaindex.ai/en/stable/api_reference/indices/vector_store.html).\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection of documents. We use the vector store within the index to store embeddings for the input text chunks.\n\nOnce constructed, the index can be used for querying.\n\n**Default Vector Store Index Construction/Querying**\n\nBy default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore` that\u2019s initialized as part of the default storage context.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\n\\# Load documents and build index\ndocuments \\= SimpleDirectoryReader('../paul\\_graham\\_essay/data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n\\# Query index\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n**Custom Vector Store Index Construction/Querying**\n\nWe can query over a custom vector store as follows:\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\nfrom llama\\_index.vector\\_stores import DeepLakeVectorStore\n\n\\# construct vector store and customize storage context\nstorage\\_context \\= StorageContext.from\\_defaults(\n    vector\\_store \\= DeepLakeVectorStore(dataset\\_path\\=\"<dataset\\_path>\")\n)\n\n\\# Load documents and build index\ndocuments \\= SimpleDirectoryReader('../paul\\_graham\\_essay/data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents, storage\\_context\\=storage\\_context)\n\n\\# Query index\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nBelow we show more examples of how to construct various vector stores we support.\n\n**Elasticsearch**\n\nFirst, you can start Elasticsearch either locally or on [Elastic cloud](https://cloud.elastic.co/registration?utm_source=llama-index&utm_content=documentation).\n\nTo start Elasticsearch locally with docker, run the following command:\n\ndocker run \\-p 9200:9200 \\\\\n  \\-e \"discovery.type=single-node\" \\\\\n  \\-e \"xpack.security.enabled=false\" \\\\\n  \\-e \"xpack.security.http.ssl.enabled=false\" \\\\\n  \\-e \"xpack.license.self\\_generated.type=trial\" \\\\\n  docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n\nThen connect and use Elasticsearch as a vector database with LlamaIndex\n\nfrom llama\\_index.vector\\_stores import ElasticsearchStore\nvector\\_store \\= ElasticsearchStore(\n    index\\_name\\=\"llm-project\",\n    es\\_url\\=\"http://localhost:9200\",\n    \\# Cloud connection options:\n    \\# es\\_cloud\\_id=\"<cloud\\_id>\",\n    \\# es\\_user=\"elastic\",\n    \\# es\\_password=\"<password>\",\n)\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.\n\n**Redis**\n\nFirst, start Redis-Stack (or get url from Redis provider)\n\ndocker run \\--name redis-vecdb \\-d \\-p 6379:6379 \\-p 8001:8001 redis/redis-stack:latest\n\nThen connect and use Redis as a vector database with LlamaIndex\n\nfrom llama\\_index.vector\\_stores import RedisVectorStore\nvector\\_store \\= RedisVectorStore(\n    index\\_name\\=\"llm-project\",\n    redis\\_url\\=\"redis://localhost:6379\",\n    overwrite\\=True\n)\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.\n\n**DeepLake**\n\nimport os\nimport getpath\nfrom llama\\_index.vector\\_stores import DeepLakeVectorStore\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= getpath.getpath(\"OPENAI\\_API\\_KEY: \")\nos.environ\\[\"ACTIVELOOP\\_TOKEN\"\\] \\= getpath.getpath(\"ACTIVELOOP\\_TOKEN: \")\ndataset\\_path \\= \"hub://adilkhan/paul\\_graham\\_essay\"\n\n\\# construct vector store\nvector\\_store \\= DeepLakeVectorStore(dataset\\_path\\=dataset\\_path, overwrite\\=True)\n\n**Faiss**\n\nimport faiss\nfrom llama\\_index.vector\\_stores import FaissVectorStore\n\n\\# create faiss index\nd \\= 1536\nfaiss\\_index \\= faiss.IndexFlatL2(d)\n\n\\# construct vector store\nvector\\_store \\= FaissVectorStore(faiss\\_index)\n\n...\n\n\\# NOTE: since faiss index is in-memory, we need to explicitly call\n\\#       vector\\_store.persist() or storage\\_context.persist() to save it to disk.\n\\#       persist() takes in optional arg persist\\_path. If none give, will use default paths.\nstorage\\_context.persist()\n\n**Weaviate**\n\nimport weaviate\nfrom llama\\_index.vector\\_stores import WeaviateVectorStore\n\n\\# creating a Weaviate client\nresource\\_owner\\_config \\= weaviate.AuthClientPassword(\n    username\\=\"<username>\",\n    password\\=\"<password>\",\n)\nclient \\= weaviate.Client(\n    \"https://<cluster-id>.semi.network/\", auth\\_client\\_secret\\=resource\\_owner\\_config\n)\n\n\\# construct vector store\nvector\\_store \\= WeaviateVectorStore(weaviate\\_client\\=client)\n\n**Zep**\n\nZep stores texts, metadata, and embeddings. All are returned in search results.\n\nfrom llama\\_index.vector\\_stores.zep import ZepVectorStore\n\nvector\\_store \\= ZepVectorStore(\n    api\\_url\\=\"<api\\_url>\",\n    api\\_key\\=\"<api\\_key>\",\n    collection\\_name\\=\"<unique\\_collection\\_name>\",  \\# Can either be an existing collection or a new one\n    embedding\\_dimensions\\=1536 \\# Optional, required if creating a new collection\n)\n\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\n\nindex \\= VectorStoreIndex.from\\_documents(documents, storage\\_context\\=storage\\_context)\n\n\\# Query index using both a text query and metadata filters\nfilters \\= MetadataFilters(filters\\=\\[ExactMatchFilter(key\\=\"theme\", value\\=\"Mafia\")\\])\nretriever \\= index.as\\_retriever(filters\\=filters)\nresult \\= retriever.retrieve(\"What is inception about?\")\n\n**Pinecone**\n\nimport pinecone\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\n\n\\# Creating a Pinecone index\napi\\_key \\= \"api\\_key\"\npinecone.init(api\\_key\\=api\\_key, environment\\=\"us-west1-gcp\")\npinecone.create\\_index(\n    \"quickstart\",\n    dimension\\=1536,\n    metric\\=\"euclidean\",\n    pod\\_type\\=\"p1\"\n)\nindex \\= pinecone.Index(\"quickstart\")\n\n\\# can define filters specific to this vector index (so you can\n\\# reuse pinecone indexes)\nmetadata\\_filters \\= {\"title\": \"paul\\_graham\\_essay\"}\n\n\\# construct vector store\nvector\\_store \\= PineconeVectorStore(\n    pinecone\\_index\\=index,\n    metadata\\_filters\\=metadata\\_filters\n)\n\n**Qdrant**\n\nimport qdrant\\_client\nfrom llama\\_index.vector\\_stores import QdrantVectorStore\n\n\\# Creating a Qdrant vector store\nclient \\= qdrant\\_client.QdrantClient(\n    host\\=\"<qdrant-host>\",\n    api\\_key\\=\"<qdrant-api-key>\",\n    https\\=True\n)\ncollection\\_name \\= \"paul\\_graham\"\n\n\\# construct vector store\nvector\\_store \\= QdrantVectorStore(\n    client\\=client,\n    collection\\_name\\=collection\\_name,\n)\n\n**Cassandra** (covering DataStax Astra DB as well, which is built on Cassandra)\n\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nfrom llama\\_index.vector\\_stores import CassandraVectorStore\n\n\\# for a Cassandra cluster:\ncluster \\= Cluster(\\[\"127.0.0.1\"\\])\n\\# for an Astra DB cloud instance:\ncluster \\= Cluster(\n  cloud\\={\"secure\\_connect\\_bundle\": \"/home/USER/secure-bundle.zip\"},\n  auth\\_provider\\=PlainTextAuthProvider(\"token\", \"AstraCS:...\")\n)\n#\nsession \\= cluster.connect()\nkeyspace \\= \"my\\_cassandra\\_keyspace\"\n\nvector\\_store \\= CassandraVectorStore(\n    session\\=session,\n    keyspace\\=keyspace,\n    table\\=\"llamaindex\\_vector\\_test\\_1\",\n    embedding\\_dimension\\=1536,\n    #insertion\\_batch\\_size=50,  # optional\n)\n\n**Chroma**\n\nimport chromadb\nfrom llama\\_index.vector\\_stores import ChromaVectorStore\n\n\\# Creating a Chroma client\n\\# EphemeralClient operates purely in-memory, PersistentClient will also save to disk\nchroma\\_client \\= chromadb.EphemeralClient()\nchroma\\_collection \\= chroma\\_client.create\\_collection(\"quickstart\")\n\n\\# construct vector store\nvector\\_store \\= ChromaVectorStore(\n    chroma\\_collection\\=chroma\\_collection,\n)\n\n**Epsilla**\n\nfrom pyepsilla import vectordb\nfrom llama\\_index.vector\\_stores import EpsillaVectorStore\n\n\\# Creating an Epsilla client\nepsilla\\_client \\= vectordb.Client()\n\n\\# Construct vector store\nvector\\_store \\= EpsillaVectorStore(client\\=epsilla\\_client)\n\n**Note**: `EpsillaVectorStore` depends on the `pyepsilla` library and a running Epsilla vector database. Use `pip/pip3 install pyepsilla` if not installed yet. A running Epsilla vector database could be found through docker image. For complete instructions, see the following documentation: https://epsilla-inc.gitbook.io/epsilladb/quick-start\n\n**Milvus**\n\n*   Milvus Index offers the ability to store both Documents and their embeddings.\n    \n\nimport pymilvus\nfrom llama\\_index.vector\\_stores import MilvusVectorStore\n\n\\# construct vector store\nvector\\_store \\= MilvusVectorStore(\n    uri\\='https://localhost:19530',\n    overwrite\\='True'\n)\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\u2019s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.\n\n**Zilliz**\n\n*   Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.\n    \n\nimport pymilvus\nfrom llama\\_index.vector\\_stores import MilvusVectorStore\n\n\\# construct vector store\nvector\\_store \\= MilvusVectorStore(\n    uri\\='foo.vectordb.zillizcloud.com',\n    token\\=\"your\\_token\\_here\"\n    overwrite\\='True'\n)\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\u2019s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.\n\n**MyScale**\n\nimport clickhouse\\_connect\nfrom llama\\_index.vector\\_stores import MyScaleVectorStore\n\n\\# Creating a MyScale client\nclient \\= clickhouse\\_connect.get\\_client(\n    host\\='YOUR\\_CLUSTER\\_HOST',\n    port\\=8443,\n    username\\='YOUR\\_USERNAME',\n    password\\='YOUR\\_CLUSTER\\_PASSWORD'\n)\n\n\\# construct vector store\nvector\\_store \\= MyScaleVectorStore(\n    myscale\\_client\\=client\n)\n\n**Timescale**\n\nfrom llama\\_index.vector\\_stores import TimescaleVectorStore\n\nvector\\_store \\= TimescaleVectorStore.from\\_params(\n    service\\_url\\='YOUR TIMESCALE SERVICE URL',\n    table\\_name\\=\"paul\\_graham\\_essay\",\n)\n\n**DocArray**\n\nfrom llama\\_index.vector\\_stores import (\n    DocArrayHnswVectorStore,\n    DocArrayInMemoryVectorStore,\n)\n\n\\# construct vector store\nvector\\_store \\= DocArrayHnswVectorStore(work\\_dir\\='hnsw\\_index')\n\n\\# alternatively, construct the in-memory vector store\nvector\\_store \\= DocArrayInMemoryVectorStore()\n\n**MongoDBAtlas**\n\n\\# Provide URI to constructor, or use environment variable\nimport pymongo\nfrom llama\\_index.vector\\_stores.mongodb import MongoDBAtlasVectorSearch\nfrom llama\\_index.indices.vector\\_store.base import VectorStoreIndex\nfrom llama\\_index.storage.storage\\_context import StorageContext\nfrom llama\\_index.readers.file.base import SimpleDirectoryReader\n\n\\# mongo\\_uri = os.environ\\[\"MONGO\\_URI\"\\]\nmongo\\_uri \\= \"mongodb+srv://<username>:<password>@<host>?retryWrites=true&w=majority\"\nmongodb\\_client \\= pymongo.MongoClient(mongo\\_uri)\n\n\\# construct store\nstore \\= MongoDBAtlasVectorSearch(mongodb\\_client)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=store)\nuber\\_docs \\= SimpleDirectoryReader(input\\_files\\=\\[\"../data/10k/uber\\_2021.pdf\"\\]).load\\_data()\n\n\\# construct index\nindex \\= VectorStoreIndex.from\\_documents(uber\\_docs, storage\\_context\\=storage\\_context)\n\n**Neo4j**\n\n*   Neo4j stores texts, metadata, and embeddings and can be customized to return graph data in the form of metadata.\n    \n\nfrom llama\\_index.vector\\_stores import Neo4jVectorStore\n\n\\# construct vector store\nneo4j\\_vector \\= Neo4jVectorStore(\n    username\\=\"neo4j\",\n    password\\=\"pleaseletmein\",\n    url\\=\"bolt://localhost:7687\",\n    embed\\_dim\\=1536\n)\n\n**Azure Cognitive Search**\n\nfrom azure.search.documents import SearchClient\nfrom llama\\_index.vector\\_stores import ChromaVectorStore\nfrom azure.core.credentials import AzureKeyCredential\n\nservice\\_endpoint \\= f\"https://{search\\_service\\_name}.search.windows.net\"\nindex\\_name \\= \"quickstart\"\ncognitive\\_search\\_credential \\= AzureKeyCredential(\"<API key>\")\n\nsearch\\_client \\= SearchClient(\n    endpoint\\=service\\_endpoint,\n    index\\_name\\=index\\_name,\n    credential\\=cognitive\\_search\\_credential,\n)\n\n\\# construct vector store\nvector\\_store \\= CognitiveSearchVectorStore(\n    search\\_client,\n    id\\_field\\_key\\=\"id\",\n    chunk\\_field\\_key\\=\"content\",\n    embedding\\_field\\_key\\=\"embedding\",\n    metadata\\_field\\_key\\=\"li\\_jsonMetadata\",\n    doc\\_id\\_field\\_key\\=\"li\\_doc\\_id\",\n)\n\n**DashVector**\n\nimport dashvector\nfrom llama\\_index.vector\\_stores import DashVectorStore\n\n\\# init dashvector client\nclient \\= dashvector.Client(api\\_key\\='your-dashvector-api-key')\n\n\\# creating a DashVector collection\nclient.create(\"quickstart\", dimension\\=1536)\ncollection \\= client.get(\"quickstart\")\n\n\\# construct vector store\nvector\\_store \\= DashVectorStore(collection)\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores).\n\n## Loading Data from Vector Stores using Data Connector[\uf0c1](#loading-data-from-vector-stores-using-data-connector \"Permalink to this heading\")\n\nLlamaIndex supports loading data from the following sources. See Data Connectors for more details and API documentation.\n\nChroma stores both documents and vectors. This is an example of how to use Chroma:\n\nfrom llama\\_index.readers.chroma import ChromaReader\nfrom llama\\_index.indices import SummaryIndex\n\n\\# The chroma reader loads data from a persisted Chroma collection.\n\\# This requires a collection name and a persist directory.\nreader \\= ChromaReader(\n    collection\\_name\\=\"chroma\\_collection\",\n    persist\\_directory\\=\"examples/data\\_connectors/chroma\\_collection\"\n)\n\nquery\\_vector\\=\\[n1, n2, n3, ...\\]\n\ndocuments \\= reader.load\\_data(collection\\_name\\=\"demo\", query\\_vector\\=query\\_vector, limit\\=5)\nindex \\= SummaryIndex.from\\_documents(documents)\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"<query\\_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nQdrant also stores both documents and vectors. This is an example of how to use Qdrant:\n\nfrom llama\\_index.readers.qdrant import QdrantReader\n\nreader \\= QdrantReader(host\\=\"localhost\")\n\n\\# the query\\_vector is an embedding representation of your query\\_vector\n\\# Example query\\_vector\n\\# query\\_vector = \\[0.3, 0.3, 0.3, 0.3, ...\\]\n\nquery\\_vector \\= \\[n1, n2, n3, ...\\]\n\n\\# NOTE: Required args are collection\\_name, query\\_vector.\n\\# See the Python client: https;//github.com/qdrant/qdrant\\_client\n\\# for more details\n\ndocuments \\= reader.load\\_data(collection\\_name\\=\"demo\", query\\_vector\\=query\\_vector, limit\\=5)\n\nNOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.\n\n\\# option 1: specify class\\_name and properties\n\n\\# 1) load data using class\\_name and properties\ndocuments \\= reader.load\\_data(\n    class\\_name\\=\"<class\\_name>\",\n    properties\\=\\[\"property1\", \"property2\", \"...\"\\],\n    separate\\_documents\\=True\n)\n\n\\# 2) example GraphQL query\nquery \\= \"\"\"\n{\n    Get {\n        <class\\_name> {\n            <property1>\n            <property2>\n        }\n    }\n}\n\"\"\"\n\ndocuments \\= reader.load\\_data(graphql\\_query\\=query, separate\\_documents\\=True)\n\nNOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load\\_data call.\n\nFor instance, this is an example usage of the Pinecone data loader `PineconeReader`:\n\nfrom llama\\_index.readers.pinecone import PineconeReader\n\nreader \\= PineconeReader(api\\_key\\=api\\_key, environment\\=\"us-west1-gcp\")\n\nid\\_to\\_text\\_map \\= {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\nquery\\_vector\\=\\[n1, n2, n3, ..\\]\n\ndocuments \\= reader.load\\_data(\n    index\\_name\\=\"quickstart\", id\\_to\\_text\\_map\\=id\\_to\\_text\\_map, top\\_k\\=3, vector\\=query\\_vector, separate\\_documents\\=True\n)\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/data_connectors).\n\nExamples\n\n*   [Elasticsearch](https://docs.llamaindex.ai/en/stable/examples/vector_stores/Elasticsearch_demo.html)\n*   [Simple Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemo.html)\n*   [Simple Vector Stores - Maximum Marginal Relevance Retrieval](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoMMR.html)\n*   [Redis Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/RedisIndexDemo.html)\n*   [Query the data](https://docs.llamaindex.ai/en/stable/examples/vector_stores/RedisIndexDemo.html#query-the-data)\n*   [Working with Metadata](https://docs.llamaindex.ai/en/stable/examples/vector_stores/RedisIndexDemo.html#working-with-metadata)\n*   [Qdrant Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo.html)\n*   [Faiss Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/FaissIndexDemo.html)\n*   [DeepLake Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DeepLakeIndexDemo.html)\n*   [MyScale Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MyScaleIndexDemo.html)\n*   [Metal Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MetalIndexDemo.html)\n*   [Weaviate Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html)\n*   [Zep Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ZepIndexDemo.html)\n*   [Create a Zep Vector Store and Index](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ZepIndexDemo.html#create-a-zep-vector-store-and-index)\n*   [Querying with Metadata filters](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ZepIndexDemo.html#querying-with-metadata-filters)\n*   [Opensearch Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/OpensearchDemo.html)\n*   [Pinecone Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo.html)\n*   [Cassandra Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CassandraIndexDemo.html)\n*   [Chroma](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html)\n*   [Epsilla Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/EpsillaIndexDemo.html)\n*   [LanceDB Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/LanceDBIndexDemo.html)\n*   [Milvus Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo.html)\n*   [Weaviate Vector Store - Hybrid Search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo-Hybrid.html)\n*   [Pinecone Vector Store - Hybrid Search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html)\n*   [Simple Vector Store - Async Index Creation](https://docs.llamaindex.ai/en/stable/examples/vector_stores/AsyncIndexCreationDemo.html)\n*   [Supabase Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SupabaseVectorIndexDemo.html)\n*   [DocArray Hnsw Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DocArrayHnswIndexDemo.html)\n*   [DocArray InMemory Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DocArrayInMemoryIndexDemo.html)\n*   [MongoDB Atlas](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearch.html)\n*   [Postgres Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/postgres.html)\n*   [Awadb Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/AwadbDemo.html)\n*   [Neo4j vector store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/Neo4jVectorDemo.html)\n*   [Azure Cognitive Search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html)\n*   [Basic Example](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html#basic-example)\n*   [Create Index (if it does not exist)](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html#create-index-if-it-does-not-exist)\n*   [Use Existing Index](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html#use-existing-index)\n*   [Adding a document to existing index](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html#adding-a-document-to-existing-index)\n*   [Filtering](https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html#filtering)\n*   [Timescale Vector Store (PostgreSQL)](https://docs.llamaindex.ai/en/stable/examples/vector_stores/Timescalevector.html)\n*   [DashVector Vector Store](https://docs.llamaindex.ai/en/stable/examples/vector_stores/DashvectorIndexDemo.html)"
}