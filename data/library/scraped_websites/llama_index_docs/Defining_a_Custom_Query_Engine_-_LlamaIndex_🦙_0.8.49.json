{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html",
        "title": "Defining a Custom Query Engine - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nYou can (and should) define your custom query engines in order to plug into your downstream LlamaIndex workflows, whether you\u2019re building RAG, agents, or other applications.\n\nWe provide a `CustomQueryEngine` that makes it easy to define your own queries.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe first load some sample data and index it.\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n)\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\n    \"../../../examples/paul\\_graham\\_essay/data\"\n).load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\nretriever \\= index.as\\_retriever()\n\n## Building a Custom Query Engine[\uf0c1](#building-a-custom-query-engine \"Permalink to this heading\")\n\nWe build a custom query engine that simulates a RAG pipeline. First perform retrieval, and then synthesis.\n\nTo define a `CustomQueryEngine`, you just have to define some initialization parameters as attributes and implement the `custom_query` function.\n\nBy default, the `custom_query` can return a `Response` object (which the response synthesizer returns), but it can also just return a string. These are options 1 and 2 respectively.\n\nfrom llama\\_index.query\\_engine import CustomQueryEngine\nfrom llama\\_index.retrievers import BaseRetriever\nfrom llama\\_index.response\\_synthesizers import (\n    get\\_response\\_synthesizer,\n    BaseSynthesizer,\n)\n\n### Option 1 (`RAGQueryEngine`)[\uf0c1](#option-1-ragqueryengine \"Permalink to this heading\")\n\nclass RAGQueryEngine(CustomQueryEngine):\n    \"\"\"RAG Query Engine.\"\"\"\n\n    retriever: BaseRetriever\n    response\\_synthesizer: BaseSynthesizer\n\n    def custom\\_query(self, query\\_str: str):\n        nodes \\= self.retriever.retrieve(query\\_str)\n        response\\_obj \\= self.response\\_synthesizer.synthesize(query\\_str, nodes)\n        return response\\_obj\n\n### Option 2 (`RAGStringQueryEngine`)[\uf0c1](#option-2-ragstringqueryengine \"Permalink to this heading\")\n\n\\# Option 2: return a string (we use a raw LLM call for illustration)\n\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.prompts import PromptTemplate\n\nqa\\_prompt \\= PromptTemplate(\n    \"Context information is below.\\\\n\"\n    \"---------------------\\\\n\"\n    \"{context\\_str}\\\\n\"\n    \"---------------------\\\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\\\n\"\n    \"Query: {query\\_str}\\\\n\"\n    \"Answer: \"\n)\n\nclass RAGStringQueryEngine(CustomQueryEngine):\n    \"\"\"RAG String Query Engine.\"\"\"\n\n    retriever: BaseRetriever\n    response\\_synthesizer: BaseSynthesizer\n    llm: OpenAI\n    qa\\_prompt: PromptTemplate\n\n    def custom\\_query(self, query\\_str: str):\n        nodes \\= self.retriever.retrieve(query\\_str)\n\n        context\\_str \\= \"\\\\n\\\\n\".join(\\[n.node.get\\_content() for n in nodes\\])\n        response \\= self.llm.complete(\n            qa\\_prompt.format(context\\_str\\=context\\_str, query\\_str\\=query\\_str)\n        )\n\n        return str(response)\n\n## Trying it out[\uf0c1](#trying-it-out \"Permalink to this heading\")\n\nWe now try it out on our sample data.\n\n### Trying Option 1 (`RAGQueryEngine`)[\uf0c1](#trying-option-1-ragqueryengine \"Permalink to this heading\")\n\nsynthesizer \\= get\\_response\\_synthesizer(response\\_mode\\=\"compact\")\nquery\\_engine \\= RAGQueryEngine(\n    retriever\\=retriever, response\\_synthesizer\\=synthesizer\n)\n\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nThe author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They also mentioned getting a microcomputer, building it themselves, and writing simple games and programs on it.\n\nprint(response.source\\_nodes\\[0\\].get\\_content())\n\n### Trying Option 2 (`RAGStringQueryEngine`)[\uf0c1](#trying-option-2-ragstringqueryengine \"Permalink to this heading\")\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\")\n\nquery\\_engine \\= RAGStringQueryEngine(\n    retriever\\=retriever,\n    response\\_synthesizer\\=synthesizer,\n    llm\\=llm,\n    qa\\_prompt\\=qa\\_prompt,\n)\n\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nThe author worked on writing and programming before college. They wrote short stories and started programming on the IBM 1401 computer in 9th grade. They later got a microcomputer and continued programming, writing simple games and a word processor."
}