{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/finetuning.html",
        "title": "Finetuning - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Finetuning[\uf0c1](#module-llama_index.finetuning \"Permalink to this heading\")\n\nFinetuning modules.\n\n_class_ llama\\_index.finetuning.EmbeddingAdapterFinetuneEngine(_dataset: [EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")_, _embed\\_model: BaseEmbedding_, _batch\\_size: int \\= 10_, _epochs: int \\= 1_, _adapter\\_model: Optional\\[Any\\] \\= None_, _dim: Optional\\[int\\] \\= None_, _device: Optional\\[str\\] \\= None_, _model\\_output\\_path: str \\= 'model\\_output'_, _model\\_checkpoint\\_path: Optional\\[str\\] \\= None_, _checkpoint\\_save\\_steps: int \\= 100_, _verbose: bool \\= False_, _bias: bool \\= False_, _\\*\\*train\\_kwargs: Any_)[\uf0c1](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine \"Permalink to this definition\")\n\nEmbedding adapter finetune engine.\n\nParameters\n\n*   **dataset** ([_EmbeddingQAFinetuneDataset_](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.EmbeddingQAFinetuneDataset\")) \u2013 Dataset to finetune on.\n    \n*   **embed\\_model** (_BaseEmbedding_) \u2013 Embedding model to finetune.\n    \n*   **batch\\_size** (_Optional__\\[__int__\\]_) \u2013 Batch size. Defaults to 10.\n    \n*   **epochs** (_Optional__\\[__int__\\]_) \u2013 Number of epochs. Defaults to 1.\n    \n*   **dim** (_Optional__\\[__int__\\]_) \u2013 Dimension of embedding. Defaults to None.\n    \n*   **adapter\\_model** (_Optional__\\[__BaseAdapter__\\]_) \u2013 Adapter model. Defaults to None, in which case a linear adapter is used.\n    \n*   **device** (_Optional__\\[__str__\\]_) \u2013 Device to use. Defaults to None.\n    \n*   **model\\_output\\_path** (_str_) \u2013 Path to save model output. Defaults to \u201cmodel\\_output\u201d.\n    \n*   **model\\_checkpoint\\_path** (_Optional__\\[__str__\\]_) \u2013 Path to save model checkpoints. Defaults to None (don\u2019t save checkpoints).\n    \n*   **verbose** (_bool_) \u2013 Whether to show progress bar. Defaults to False.\n    \n*   **bias** (_bool_) \u2013 Whether to use bias. Defaults to False.\n    \n\nfinetune(_\\*\\*train\\_kwargs: Any_) \u2192 None[\uf0c1](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine.finetune \"Permalink to this definition\")\n\nFinetune.\n\n_classmethod_ from\\_model\\_path(_dataset: [EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")_, _embed\\_model: BaseEmbedding_, _model\\_path: str_, _model\\_cls: Optional\\[Type\\[Any\\]\\] \\= None_, _\\*\\*kwargs: Any_) \u2192 [EmbeddingAdapterFinetuneEngine](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine \"llama_index.finetuning.embeddings.adapter.EmbeddingAdapterFinetuneEngine\")[\uf0c1](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine.from_model_path \"Permalink to this definition\")\n\nLoad from model path.\n\nParameters\n\n*   **dataset** ([_EmbeddingQAFinetuneDataset_](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.EmbeddingQAFinetuneDataset\")) \u2013 Dataset to finetune on.\n    \n*   **embed\\_model** (_BaseEmbedding_) \u2013 Embedding model to finetune.\n    \n*   **model\\_path** (_str_) \u2013 Path to model.\n    \n*   **model\\_cls** (_Optional__\\[__Type__\\[__Any__\\]__\\]_) \u2013 Adapter model class. Defaults to None.\n    \n*   **\\*\\*kwargs** (_Any_) \u2013 Additional kwargs (see \\_\\_init\\_\\_)\n    \n\nget\\_finetuned\\_model(_\\*\\*model\\_kwargs: Any_) \u2192 BaseEmbedding[\uf0c1](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine.get_finetuned_model \"Permalink to this definition\")\n\nGet finetuned model.\n\nsmart\\_batching\\_collate(_batch: List_) \u2192 Tuple\\[Any, Any\\][\uf0c1](#llama_index.finetuning.EmbeddingAdapterFinetuneEngine.smart_batching_collate \"Permalink to this definition\")\n\nSmart batching collate.\n\n_pydantic model_ llama\\_index.finetuning.EmbeddingQAFinetuneDataset[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"Permalink to this definition\")\n\nEmbedding QA Finetuning Dataset.\n\nParameters\n\n*   **queries** (_Dict__\\[__str__,_ _str__\\]_) \u2013 Dict id -> query.\n    \n*   **corpus** (_Dict__\\[__str__,_ _str__\\]_) \u2013 Dict id -> string.\n    \n*   **relevant\\_docs** (_Dict__\\[__str__,_ _List__\\[__str__\\]__\\]_) \u2013 Dict query id -> list of doc ids.\n    \n\nShow JSON schema\n\n{\n   \"title\": \"EmbeddingQAFinetuneDataset\",\n   \"description\": \"Embedding QA Finetuning Dataset.\\\\n\\\\nArgs:\\\\n    queries (Dict\\[str, str\\]): Dict id -> query.\\\\n    corpus (Dict\\[str, str\\]): Dict id -> string.\\\\n    relevant\\_docs (Dict\\[str, List\\[str\\]\\]): Dict query id -> list of doc ids.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"queries\": {\n         \"title\": \"Queries\",\n         \"type\": \"object\",\n         \"additionalProperties\": {\n            \"type\": \"string\"\n         }\n      },\n      \"corpus\": {\n         \"title\": \"Corpus\",\n         \"type\": \"object\",\n         \"additionalProperties\": {\n            \"type\": \"string\"\n         }\n      },\n      \"relevant\\_docs\": {\n         \"title\": \"Relevant Docs\",\n         \"type\": \"object\",\n         \"additionalProperties\": {\n            \"type\": \"array\",\n            \"items\": {\n               \"type\": \"string\"\n            }\n         }\n      }\n   },\n   \"required\": \\[\n      \"queries\",\n      \"corpus\",\n      \"relevant\\_docs\"\n   \\]\n}\n\nFields\n\n*   `corpus (Dict[str, str])`\n    \n*   `queries (Dict[str, str])`\n    \n*   `relevant_docs (Dict[str, List[str]])`\n    \n\n_field_ corpus_: Dict\\[str, str\\]_ _\\[Required\\]_[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.corpus \"Permalink to this definition\")\n\n_field_ queries_: Dict\\[str, str\\]_ _\\[Required\\]_[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.queries \"Permalink to this definition\")\n\n_field_ relevant\\_docs_: Dict\\[str, List\\[str\\]\\]_ _\\[Required\\]_[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.relevant_docs \"Permalink to this definition\")\n\n_classmethod_ construct(_\\_fields\\_set: Optional\\[SetStr\\] \\= None_, _\\*\\*values: Any_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.construct \"Permalink to this definition\")\n\nCreates a new model setting \\_\\_dict\\_\\_ and \\_\\_fields\\_set\\_\\_ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _update: Optional\\[DictStrAny\\] \\= None_, _deep: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.copy \"Permalink to this definition\")\n\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nParameters\n\n*   **include** \u2013 fields to include in new model\n    \n*   **exclude** \u2013 fields to exclude from new model, as with values this takes precedence over include\n    \n*   **update** \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n    \n*   **deep** \u2013 set to True to make a deep copy of the model\n    \n\nReturns\n\nnew model instance\n\ndict(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_) \u2192 DictStrAny[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.dict \"Permalink to this definition\")\n\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n_classmethod_ from\\_json(_path: str_) \u2192 [EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.from_json \"Permalink to this definition\")\n\nLoad json.\n\n_classmethod_ from\\_orm(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.from_orm \"Permalink to this definition\")\n\njson(_\\*_, _include: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _exclude: Optional\\[Union\\[AbstractSetIntStr, MappingIntStrAny\\]\\] \\= None_, _by\\_alias: bool \\= False_, _skip\\_defaults: Optional\\[bool\\] \\= None_, _exclude\\_unset: bool \\= False_, _exclude\\_defaults: bool \\= False_, _exclude\\_none: bool \\= False_, _encoder: Optional\\[Callable\\[\\[Any\\], Any\\]\\] \\= None_, _models\\_as\\_dict: bool \\= True_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.json \"Permalink to this definition\")\n\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\n_classmethod_ parse\\_file(_path: Union\\[str, Path\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.parse_file \"Permalink to this definition\")\n\n_classmethod_ parse\\_obj(_obj: Any_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.parse_obj \"Permalink to this definition\")\n\n_classmethod_ parse\\_raw(_b: Union\\[str, bytes\\]_, _\\*_, _content\\_type: unicode \\= None_, _encoding: unicode \\= 'utf8'_, _proto: Protocol \\= None_, _allow\\_pickle: bool \\= False_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.parse_raw \"Permalink to this definition\")\n\nsave\\_json(_path: str_) \u2192 None[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.save_json \"Permalink to this definition\")\n\nSave json.\n\n_classmethod_ schema(_by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_) \u2192 DictStrAny[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.schema \"Permalink to this definition\")\n\n_classmethod_ schema\\_json(_\\*_, _by\\_alias: bool \\= True_, _ref\\_template: unicode \\= '#/definitions/{model}'_, _\\*\\*dumps\\_kwargs: Any_) \u2192 unicode[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.schema_json \"Permalink to this definition\")\n\n_classmethod_ update\\_forward\\_refs(_\\*\\*localns: Any_) \u2192 None[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.update_forward_refs \"Permalink to this definition\")\n\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\n\n_classmethod_ validate(_value: Any_) \u2192 Model[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.validate \"Permalink to this definition\")\n\n_property_ query\\_docid\\_pairs_: List\\[Tuple\\[str, List\\[str\\]\\]\\]_[\uf0c1](#llama_index.finetuning.EmbeddingQAFinetuneDataset.query_docid_pairs \"Permalink to this definition\")\n\nGet query, relevant doc ids.\n\n_class_ llama\\_index.finetuning.GradientFinetuneEngine(_\\*_, _access\\_token: Optional\\[str\\] \\= None_, _base\\_model\\_slug: str_, _data\\_path: str_, _host: Optional\\[str\\] \\= None_, _learning\\_rate: Optional\\[float\\] \\= None_, _name: str_, _rank: Optional\\[int\\] \\= None_, _workspace\\_id: Optional\\[str\\] \\= None_)[\uf0c1](#llama_index.finetuning.GradientFinetuneEngine \"Permalink to this definition\")\n\n_class_ llama\\_index.finetuning.GradientFinetuneEngine(_\\*_, _access\\_token: Optional\\[str\\] \\= None_, _data\\_path: str_, _host: Optional\\[str\\] \\= None_, _model\\_adapter\\_id: str_, _workspace\\_id: Optional\\[str\\] \\= None_)\n\nfinetune() \u2192 None[\uf0c1](#llama_index.finetuning.GradientFinetuneEngine.finetune \"Permalink to this definition\")\n\nGoes off and does stuff.\n\nget\\_finetuned\\_model(_\\*\\*model\\_kwargs: Any_) \u2192 [GradientModelAdapterLLM](https://docs.llamaindex.ai/en/stable/api_reference/llms/gradient_model_adapter.html#llama_index.llms.gradient.GradientModelAdapterLLM \"llama_index.llms.gradient.GradientModelAdapterLLM\")[\uf0c1](#llama_index.finetuning.GradientFinetuneEngine.get_finetuned_model \"Permalink to this definition\")\n\nGets finetuned model.\n\n_class_ llama\\_index.finetuning.OpenAIFinetuneEngine(_base\\_model: str_, _data\\_path: str_, _verbose: bool \\= False_, _start\\_job\\_id: Optional\\[str\\] \\= None_, _validate\\_json: bool \\= True_)[\uf0c1](#llama_index.finetuning.OpenAIFinetuneEngine \"Permalink to this definition\")\n\nOpenAI Finetuning Engine.\n\nfinetune() \u2192 None[\uf0c1](#llama_index.finetuning.OpenAIFinetuneEngine.finetune \"Permalink to this definition\")\n\nFinetune model.\n\n_classmethod_ from\\_finetuning\\_handler(_finetuning\\_handler: [OpenAIFineTuningHandler](https://docs.llamaindex.ai/en/stable/api_reference/callbacks.html#llama_index.callbacks.OpenAIFineTuningHandler \"llama_index.callbacks.finetuning_handler.OpenAIFineTuningHandler\")_, _base\\_model: str_, _data\\_path: str_, _\\*\\*kwargs: Any_) \u2192 [OpenAIFinetuneEngine](#llama_index.finetuning.OpenAIFinetuneEngine \"llama_index.finetuning.openai.base.OpenAIFinetuneEngine\")[\uf0c1](#llama_index.finetuning.OpenAIFinetuneEngine.from_finetuning_handler \"Permalink to this definition\")\n\nInitialize from finetuning handler.\n\nUsed to finetune an OpenAI model into another OpenAI model (e.g. gpt-3.5-turbo on top of GPT-4).\n\nget\\_current\\_job() \u2192 Any[\uf0c1](#llama_index.finetuning.OpenAIFinetuneEngine.get_current_job \"Permalink to this definition\")\n\nGet current job.\n\nget\\_finetuned\\_model(_\\*\\*model\\_kwargs: Any_) \u2192 [LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\")[\uf0c1](#llama_index.finetuning.OpenAIFinetuneEngine.get_finetuned_model \"Permalink to this definition\")\n\nGets finetuned model.\n\n_class_ llama\\_index.finetuning.SentenceTransformersFinetuneEngine(_dataset: [EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")_, _model\\_id: str \\= 'BAAI/bge-small-en'_, _model\\_output\\_path: str \\= 'exp\\_finetune'_, _batch\\_size: int \\= 10_, _val\\_dataset: Optional\\[[EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")\\] \\= None_, _loss: Optional\\[Any\\] \\= None_, _epochs: int \\= 2_, _show\\_progress\\_bar: bool \\= True_, _evaluation\\_steps: int \\= 50_)[\uf0c1](#llama_index.finetuning.SentenceTransformersFinetuneEngine \"Permalink to this definition\")\n\nSentence Transformers Finetune Engine.\n\nfinetune(_\\*\\*train\\_kwargs: Any_) \u2192 None[\uf0c1](#llama_index.finetuning.SentenceTransformersFinetuneEngine.finetune \"Permalink to this definition\")\n\nFinetune model.\n\nget\\_finetuned\\_model(_\\*\\*model\\_kwargs: Any_) \u2192 BaseEmbedding[\uf0c1](#llama_index.finetuning.SentenceTransformersFinetuneEngine.get_finetuned_model \"Permalink to this definition\")\n\nGets finetuned model.\n\nllama\\_index.finetuning.generate\\_qa\\_embedding\\_pairs(_nodes: List\\[TextNode\\]_, _llm: Optional\\[[LLM](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLM \"llama_index.llms.base.LLM\")\\] \\= None_, _qa\\_generate\\_prompt\\_tmpl: str \\= 'Context information is below.\\\\n\\\\n---------------------\\\\n{context\\_str}\\\\n---------------------\\\\n\\\\nGiven the context information and not prior knowledge.\\\\ngenerate only questions based on the below query.\\\\n\\\\nYou are a Teacher/ Professor. Your task is to setup {num\\_questions\\_per\\_chunk} questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\\\n'_, _num\\_questions\\_per\\_chunk: int \\= 2_) \u2192 [EmbeddingQAFinetuneDataset](#llama_index.finetuning.EmbeddingQAFinetuneDataset \"llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset\")[\uf0c1](#llama_index.finetuning.generate_qa_embedding_pairs \"Permalink to this definition\")\n\nGenerate examples given a set of nodes."
}