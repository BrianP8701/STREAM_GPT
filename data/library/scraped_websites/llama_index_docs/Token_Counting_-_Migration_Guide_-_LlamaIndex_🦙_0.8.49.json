{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/callbacks/token_counting_migration.html",
        "title": "Token Counting - Migration Guide - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nThe existing token counting implementation has been **deprecated**.\n\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition.\n\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\n\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separate token counters for different indexes.\n\nHere is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\n\nimport tiktoken\nfrom llama\\_index.callbacks import CallbackManager, TokenCountingHandler\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\n\\# you can set a tokenizer directly, or optionally let it default\n\\# to the same tokenizer that was used previously for token counting\n\\# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\ntoken\\_counter \\= TokenCountingHandler(\n    tokenizer\\=tiktoken.encoding\\_for\\_model(\"text-davinci-003\").encode\n    verbose\\=False  \\# set to true to see usage printed to the console\n)\n\ncallback\\_manager \\= CallbackManager(\\[token\\_counter\\])\n\nservice\\_context \\= ServiceContext.from\\_defaults(callback\\_manager\\=callback\\_manager)\n\ndocument \\= SimpleDirectoryReader(\"./data\").load\\_data()\n\n\\# if verbose is turned on, you will see embedding token usage printed\nindex \\= VectorStoreIndex.from\\_documents(documents, service\\_context\\=service\\_context)\n\n\\# otherwise, you can access the count directly\nprint(token\\_counter.total\\_embedding\\_token\\_count)\n\n\\# reset the counts at your discretion!\ntoken\\_counter.reset\\_counts()\n\n\\# also track prompt, completion, and total LLM tokens, in addition to embeddings\nresponse \\= index.as\\_query\\_engine().query(\"What did the author do growing up?\")\nprint('Embedding Tokens: ', token\\_counter.total\\_embedding\\_token\\_count, '\\\\n',\n      'LLM Prompt Tokens: ', token\\_counter.prompt\\_llm\\_token\\_count, '\\\\n',\n      'LLM Completion Tokens: ', token\\_counter.completion\\_llm\\_token\\_count, '\\\\n',\n      'Total LLM Token Count: ', token\\_counter.total\\_llm\\_token\\_count)"
}