{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html",
        "title": "Finetune Embeddings - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Finetune Embeddings[\uf0c1](#finetune-embeddings \"Permalink to this heading\")\n\nIn this notebook, we show users how to finetune their own embedding models.\n\nWe go through three main sections:\n\n1.  Preparing the data (our `generate_qa_embedding_pairs` function makes this easy)\n    \n2.  Finetuning the model (using our `SentenceTransformersFinetuneEngine`)\n    \n3.  Evaluating the model on a validation knowledge corpus\n    \n\n## Generate Corpus[\uf0c1](#generate-corpus \"Permalink to this heading\")\n\nFirst, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks.\n\nimport json\n\nfrom llama\\_index import SimpleDirectoryReader\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom llama\\_index.schema import MetadataMode\n\nTRAIN\\_FILES \\= \\[\"../../../examples/data/10k/lyft\\_2021.pdf\"\\]\nVAL\\_FILES \\= \\[\"../../../examples/data/10k/uber\\_2021.pdf\"\\]\n\nTRAIN\\_CORPUS\\_FPATH \\= \"./data/train\\_corpus.json\"\nVAL\\_CORPUS\\_FPATH \\= \"./data/val\\_corpus.json\"\n\ndef load\\_corpus(files, verbose\\=False):\n    if verbose:\n        print(f\"Loading files {files}\")\n\n    reader \\= SimpleDirectoryReader(input\\_files\\=files)\n    docs \\= reader.load\\_data()\n    if verbose:\n        print(f\"Loaded {len(docs)} docs\")\n\n    parser \\= SimpleNodeParser.from\\_defaults()\n    nodes \\= parser.get\\_nodes\\_from\\_documents(docs, show\\_progress\\=verbose)\n\n    if verbose:\n        print(f\"Parsed {len(nodes)} nodes\")\n\n    return nodes\n\nWe do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset.\n\ntrain\\_nodes \\= load\\_corpus(TRAIN\\_FILES, verbose\\=True)\nval\\_nodes \\= load\\_corpus(VAL\\_FILES, verbose\\=True)\n\nLoading files \\['../../../examples/data/10k/lyft\\_2021.pdf'\\]\nLoaded 238 docs\n\nParsed 349 nodes\nLoading files \\['../../../examples/data/10k/uber\\_2021.pdf'\\]\nLoaded 307 docs\n\n### Generate synthetic queries[\uf0c1](#generate-synthetic-queries \"Permalink to this heading\")\n\nNow, we use an LLM (gpt-3.5-turbo) to generate questions using each text chunk in the corpus as context.\n\nEach pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation).\n\nfrom llama\\_index.finetuning import (\n    generate\\_qa\\_embedding\\_pairs,\n    EmbeddingQAFinetuneDataset,\n)\n\ntrain\\_dataset \\= generate\\_qa\\_embedding\\_pairs(train\\_nodes)\nval\\_dataset \\= generate\\_qa\\_embedding\\_pairs(val\\_nodes)\n\ntrain\\_dataset.save\\_json(\"train\\_dataset.json\")\nval\\_dataset.save\\_json(\"val\\_dataset.json\")\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 349/349 \\[11:24<00:00,  1.96s/it\\]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 418/418 \\[14:54<00:00,  2.14s/it\\]\n\n\\# \\[Optional\\] Load\ntrain\\_dataset \\= EmbeddingQAFinetuneDataset.from\\_json(\"train\\_dataset.json\")\nval\\_dataset \\= EmbeddingQAFinetuneDataset.from\\_json(\"val\\_dataset.json\")\n\n## Run Embedding Finetuning[\uf0c1](#run-embedding-finetuning \"Permalink to this heading\")\n\nfrom llama\\_index.finetuning import SentenceTransformersFinetuneEngine\n\nfinetune\\_engine \\= SentenceTransformersFinetuneEngine(\n    train\\_dataset,\n    model\\_id\\=\"BAAI/bge-small-en\",\n    model\\_output\\_path\\=\"test\\_model\",\n    val\\_dataset\\=val\\_dataset,\n)\n\nfinetune\\_engine.finetune()\n\nembed\\_model \\= finetune\\_engine.get\\_finetuned\\_model()\n\nLangchainEmbedding(model\\_name='test\\_model', embed\\_batch\\_size=10, callback\\_manager=<llama\\_index.callbacks.base.CallbackManager object at 0x17743fd60>)\n\n## Evaluate Finetuned Model[\uf0c1](#evaluate-finetuned-model \"Permalink to this heading\")\n\nIn this section, we evaluate 3 different embedding models:\n\n1.  proprietary OpenAI embedding,\n    \n2.  open source `BAAI/bge-small-en`, and\n    \n3.  our finetuned embedding model.\n    \n\nWe consider 2 evaluation approaches:\n\n1.  a simple custom **hit rate** metric\n    \n2.  using `InformationRetrievalEvaluator` from sentence\\_transformers\n    \n\nWe show that finetuning on synthetic (LLM-generated) dataset significantly improve upon an opensource embedding model.\n\nfrom llama\\_index.embeddings import OpenAIEmbedding\nfrom llama\\_index import ServiceContext, VectorStoreIndex\nfrom llama\\_index.schema import TextNode\nfrom tqdm.notebook import tqdm\nimport pandas as pd\n\n### Define eval function[\uf0c1](#define-eval-function \"Permalink to this heading\")\n\n**Option 1**: We use a simple **hit rate** metric for evaluation:\n\n*   for each (query, relevant\\_doc) pair,\n    \n*   we retrieve top-k documents with the query, and\n    \n*   it\u2019s a **hit** if the results contain the relevant\\_doc.\n    \n\nThis approach is very simple and intuitive, and we can apply it to both the proprietary OpenAI embedding as well as our open source and fine-tuned embedding models.\n\ndef evaluate(\n    dataset,\n    embed\\_model,\n    top\\_k\\=5,\n    verbose\\=False,\n):\n    corpus \\= dataset.corpus\n    queries \\= dataset.queries\n    relevant\\_docs \\= dataset.relevant\\_docs\n\n    service\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\n    nodes \\= \\[TextNode(id\\_\\=id\\_, text\\=text) for id\\_, text in corpus.items()\\]\n    index \\= VectorStoreIndex(\n        nodes, service\\_context\\=service\\_context, show\\_progress\\=True\n    )\n    retriever \\= index.as\\_retriever(similarity\\_top\\_k\\=top\\_k)\n\n    eval\\_results \\= \\[\\]\n    for query\\_id, query in tqdm(queries.items()):\n        retrieved\\_nodes \\= retriever.retrieve(query)\n        retrieved\\_ids \\= \\[node.node.node\\_id for node in retrieved\\_nodes\\]\n        expected\\_id \\= relevant\\_docs\\[query\\_id\\]\\[0\\]\n        is\\_hit \\= expected\\_id in retrieved\\_ids  \\# assume 1 relevant doc\n\n        eval\\_result \\= {\n            \"is\\_hit\": is\\_hit,\n            \"retrieved\": retrieved\\_ids,\n            \"expected\": expected\\_id,\n            \"query\": query\\_id,\n        }\n        eval\\_results.append(eval\\_result)\n    return eval\\_results\n\n**Option 2**: We use the `InformationRetrievalEvaluator` from sentence\\_transformers.\n\nThis provides a more comprehensive suite of metrics, but we can only run it against the sentencetransformers compatible models (open source and our finetuned model, _not_ the OpenAI embedding model).\n\nfrom sentence\\_transformers.evaluation import InformationRetrievalEvaluator\nfrom sentence\\_transformers import SentenceTransformer\nfrom pathlib import Path\n\ndef evaluate\\_st(\n    dataset,\n    model\\_id,\n    name,\n):\n    corpus \\= dataset.corpus\n    queries \\= dataset.queries\n    relevant\\_docs \\= dataset.relevant\\_docs\n\n    evaluator \\= InformationRetrievalEvaluator(\n        queries, corpus, relevant\\_docs, name\\=name\n    )\n    model \\= SentenceTransformer(model\\_id)\n    output\\_path \\= \"results/\"\n    Path(output\\_path).mkdir(exist\\_ok\\=True, parents\\=True)\n    return evaluator(model, output\\_path\\=output\\_path)\n\n### Run Evals[\uf0c1](#run-evals \"Permalink to this heading\")\n\n#### OpenAI[\uf0c1](#openai \"Permalink to this heading\")\n\nNote: this might take a few minutes to run since we have to embed the corpus and queries\n\nada \\= OpenAIEmbedding()\nada\\_val\\_results \\= evaluate(val\\_dataset, ada)\n\ndf\\_ada \\= pd.DataFrame(ada\\_val\\_results)\n\nhit\\_rate\\_ada \\= df\\_ada\\[\"is\\_hit\"\\].mean()\nhit\\_rate\\_ada\n\n### BAAI/bge-small-en[\uf0c1](#baai-bge-small-en \"Permalink to this heading\")\n\nbge \\= \"local:BAAI/bge-small-en\"\nbge\\_val\\_results \\= evaluate(val\\_dataset, bge)\n\ndf\\_bge \\= pd.DataFrame(bge\\_val\\_results)\n\nhit\\_rate\\_bge \\= df\\_bge\\[\"is\\_hit\"\\].mean()\nhit\\_rate\\_bge\n\nevaluate\\_st(val\\_dataset, \"BAAI/bge-small-en\", name\\=\"bge\")\n\n\\---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In\\[59\\], line 1\n\\----> 1 evaluate\\_st(val\\_dataset, \"BAAI/bge-small-en\", name\\='bge')\n\nCell In\\[49\\], line 15, in evaluate\\_st(dataset, model\\_id, name)\n     13 evaluator \\= InformationRetrievalEvaluator(queries, corpus, relevant\\_docs, name\\=name)\n     14 model \\= SentenceTransformer(model\\_id)\n\\---> 15 return evaluator(model, output\\_path\\='results/')\n\nFile ~/Programming/gpt\\_index/.venv/lib/python3.10/site-packages/sentence\\_transformers/evaluation/InformationRetrievalEvaluator.py:104, in InformationRetrievalEvaluator.\\_\\_call\\_\\_(self, model, output\\_path, epoch, steps, \\*args, \\*\\*kwargs)\n    102 csv\\_path \\= os.path.join(output\\_path, self.csv\\_file)\n    103 if not os.path.isfile(csv\\_path):\n\\--> 104     fOut \\= open(csv\\_path, mode\\=\"w\", encoding\\=\"utf-8\")\n    105     fOut.write(\",\".join(self.csv\\_headers))\n    106     fOut.write(\"\\\\n\")\n\nFileNotFoundError: \\[Errno 2\\] No such file or directory: 'results/Information-Retrieval\\_evaluation\\_bge\\_results.csv'\n\n### Finetuned[\uf0c1](#finetuned \"Permalink to this heading\")\n\nfinetuned \\= \"local:test\\_model\"\nval\\_results\\_finetuned \\= evaluate(val\\_dataset, finetuned)\n\ndf\\_finetuned \\= pd.DataFrame(val\\_results\\_finetuned)\n\nhit\\_rate\\_finetuned \\= df\\_finetuned\\[\"is\\_hit\"\\].mean()\nhit\\_rate\\_finetuned\n\nevaluate\\_st(val\\_dataset, \"test\\_model\", name\\=\"finetuned\")\n\n### Summary of Results[\uf0c1](#summary-of-results \"Permalink to this heading\")\n\n#### Hit rate[\uf0c1](#hit-rate \"Permalink to this heading\")\n\ndf\\_ada\\[\"model\"\\] \\= \"ada\"\ndf\\_bge\\[\"model\"\\] \\= \"bge\"\ndf\\_finetuned\\[\"model\"\\] \\= \"fine\\_tuned\"\n\nWe can see that fine-tuning our small open-source embedding model drastically improve its retrieval quality (even approaching the quality of the proprietary OpenAI embedding)!\n\ndf\\_all \\= pd.concat(\\[df\\_ada, df\\_bge, df\\_finetuned\\])\ndf\\_all.groupby(\"model\").mean(\"is\\_hit\")\n\n#### InformationRetrievalEvaluator[\uf0c1](#informationretrievalevaluator \"Permalink to this heading\")\n\ndf\\_st\\_bge \\= pd.read\\_csv(\n    \"results/Information-Retrieval\\_evaluation\\_bge\\_results.csv\"\n)\ndf\\_st\\_finetuned \\= pd.read\\_csv(\n    \"results/Information-Retrieval\\_evaluation\\_finetuned\\_results.csv\"\n)\n\nWe can see that embedding finetuning improves metrics consistently across the suite of eval metrics\n\ndf\\_st\\_bge\\[\"model\"\\] \\= \"bge\"\ndf\\_st\\_finetuned\\[\"model\"\\] \\= \"fine\\_tuned\"\ndf\\_st\\_all \\= pd.concat(\\[df\\_st\\_bge, df\\_st\\_finetuned\\])\ndf\\_st\\_all \\= df\\_st\\_all.set\\_index(\"model\")\ndf\\_st\\_all"
}