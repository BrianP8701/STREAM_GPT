"STREAM AI 1\n\n1\n\nReal-Time, Multithreaded Anomaly Detection System with Autonomous\n\nSynchronization & Self-Correction\n\nBrian Przezdziecki\n\nUniversity of Buffalo (SUNY)\n\n\fSTREAM AI 1\n\n2\n\nAbstract\n\nReal-time anomaly detection is crucial during the printing process, especially when\n\nlooking for issues like over or under extrusion. We employ a MobileNet model to classify these\n\nanomalies. Our system has two main components: Tip Tracking and Anomaly Detection. In the\n\nTip Tracking phase, we estimate the locations of the tips using gcode data. Then, we use the\n\nYOLO model to align these predictions with live video footage, ensuring synchronization and\n\ncorrecting any prediction inaccuracies. Given the need for real-time performance on a Raspberry\n\nPi, our system is optimized for computational efficiency. It's designed to operate in real-time,\n\nrunning alongside video feeds, and strikes a balance between accuracy, autonomy, robustness and\n\nspeed.\n\n\fSTREAM AI 1\n\n3\n\nProblem\n\nWe need to spot errors in 3D printing as they happen. The main mistakes are tied to how\n\nmuch material comes out - too much or too little (Under/Over Extrusion).\n\nClasses:\n\nOver\n\nNormal\n\nUnder\n\nAnother problem arises: Given a frame, the only relevant information is the area around the tip in\n\nthe direction of recently extruded material. This is why we need to track the tip.\n\n\fSTREAM AI 1\n\n4\n\nSolution\n\nThe given diagram displays how the system is divided into distinct threads and illustrates\n\nthe process flow. All threads access some shared global variables. The tracker thread initiates and\n\nmanages all other threads. This system primarily revolves around three key components:\n\n1. Initialization: Predicting the path of tip tracking and aligning with the video.\n\n2. Error Correction: Correcting errors through a feedback loop.\n\n3. Analytics: Identifying anomalies.\n\nFor the first two steps, we utilize a fine-tuned Yolov8. For the anomaly identification, we rely on\n\na fine-tuned MobileNetv3 model.\n\nWhy we need Tip Tracking & Examples of Video and Anomalies\n\n\fSTREAM AI 1\n\n5\n\nChallenges\n\nThis system is designed with simplicity and versatility in mind. Our goal is to make it\n\nreadily adoptable for anyone, ensuring it can seamlessly integrate with various 3D printers in\n\ndiverse settings. There's no need for a precise camera placement, giving users flexibility. The\n\nsystem adjusts to different resolutions, simplifying setup processes.\n\nIt's built to work with any printer that operates on g-code, even if the g-code\n\ninterpretations vary. With edge deployment capabilities, it can run on a Raspberry Pi, streaming\n\nin real time. Given that the YOLO model may occasionally err in object detection, our system is\n\nmade to account for these discrepancies. Moreover, users will benefit from a clear visualization\n\nof the tip tracking procedure.\n\nGiven these constraints and requirements, the ensuing challenges and our tailored\n\nsolutions will be elaborated below.\n\n\fSTREAM AI 1\n\n6\n\nModel Training\n\nhttps://github.com/BrianP8701/Anomaly_Classification\n\nObject Detection for Tip Tracking\n\nWe utilized the YOLOv8s model, a prominent object detection system, and fine-tuned it\n\nthrough Roboflow, a platform that streamlines the computer vision data process.\n\nInput Size Dataset Size Augmentations Precision Recall MAP50\n\n640x640\n\n2407\n\n1200\n\n0.967\n\n0.931\n\n0.968\n\nWe added onto our dataset and retrained multiple times, aiming to fill in gaps and\n\nweaknesses of the model, adding null images and wrong detections etc. We applied\n\naugmentations of blur, noise and brightness.\n\n\fSTREAM AI 1\n\n7\n\nObject Classification for Anomalies\n\nWe tested various models, settings, and preprocessing methods to find the best model.\n\nOur data is limited because only a few frames in each video show over or under extrusion,\n\nmaking data collection slow. Here's our data pipeline:\n\nExamining the pipeline: I developed several GUIs to address the initial data collection\n\nbottleneck. I opted for an 85x85 crop size as it best captured material near the extrusion tip. I\n\nused grayscale to reduce dimensions. In some images, edges and material are barely visible.\n\nTherefore, a filter was used in preprocessing to enhance these crucial edges.\n\nI initially attempted a filter that segmented the image into\n\ndistinct color intensities, using the original image's color intensity\n\nstandard deviation. While somewhat effective, it missed the\n\nsubtlest edges.\n\n\fSTREAM AI 1\n\n8\n\nDrawing inspiration from the convolutional operations in CNNs, I\n\nexperimented with various kernels and matrix filter combinations on the\n\nimages. However, no single combination consistently worked across all\n\nimages. I tested standard filters such as the Non-local Means Denoising\n\nfrom cv2 and the Unsharp Mask from PIL. Yet, neither succeeded in\n\nenhancing the faint edges.\n\nAn observation was made, each image features a limited set of objects and hues: material,\n\ntip, bed, shadows, and gleam. Each color intensity in the image uniquely follows a normal\n\ndistribution with its distinct mean and variance. Hence, Gaussian Mixture Models are apt for this\n\ntask.\n\n\fSTREAM AI 1\n\n9\n\nUsing Gaussian Mixture Models, we can sharply define even the subtlest edges. I tested\n\nvarious component numbers, indicating the assumed Gaussians in the mixture. Next, we\n\nprioritize sensitivity around brighter colors since it's challenging to discern edges when material\n\naccumulates. This focus ignores the darker bed, which isn't of concern. This final refinement\n\nresulted in our preprocessing step, dubbed GMMS.\n\nThrough experimentation, GMMS6 is found to perform the best when training models.\n\n\fSTREAM AI 1\n\n10\n\nBack to examining the pipeline, the\n\nimages are resized to 224x224, suitable\n\nfor EfficientNet, MobileNet, and\n\nResNet, which have trained on the\n\n224x224-sized ImageNet dataset,\n\nmaking them adept at recognizing\n\nfeatures of this scale. Post-resizing, the\n\nimages undergo augmentation and are split into training, validation, and test datasets.\n\nDataset Summary:\n\nUnder: 436 images | Normal: 463 images | Over: 446 images\n\nExtrusion Classification Dataset\n\nAugmentations for each class include rotations, brightness, saturation, contrast, hue\n\nadjustments, and flips (both horizontal and vertical) creating 60 new images per class. The data\n\ndistribution is 75% for training, 15% for validation, and 10% for testing.\n\nTraining Summary:\n\nFine tuning, model checkpointing, 4 batches, ~12 epochs, learning rate decay, early stopping.\n\nlearning_rate=0.01, momentum=0.9, step_size=5, gamma=0.1\n\nAfter trials of various models, settings, hyperparameters and preprocessing steps, the\n\ncombination of the described preprocessing and training procedures outlined above yielded the\n\nhighest accuracy. The standout was a fine tuned MobileNetv3Large model, which not only won\n\nin accuracy but also had superior computational efficiency.\n\n\fSTREAM AI 1\n\n11\n\nTo the left shows one of the\n\ncomparison rounds, where we identified\n\nour best performing model. Here we\n\ncompare 3 different settings and their\n\ncombinations: Fine tuning\n\nMobileNetv3Small vs MobileNetv3Large.\n\nUsing no GMM preprocessing vs using\n\nGMM preprocessing vs using GMMS\n\npreprocessing with additional sensitivity\n\nto more intense pixels.\n\nThe results are fitting, showing that\n\nindeed using GMMS with extra\n\nsensitivity is beneficial, and training the\n\nlarger model results in better\n\nperformance. The training and validation\n\nmetrics are nearly indistinguishable\n\nindicating that it is not overfitting.\n\nBest performing model accuracy:\n\n0.9025974025974026\n\n\fSTREAM AI 1\n\n12\n\nImplementation\n\nIn this section I will describe the design choices and details of the system.\n\nhttps://github.com/BrianP8701/STREAM.AI\n\nInput\n\n1. G-Code:\n\nInstruction set for 3D print.\nSimple and sequential: it runs from start to finish.\nSpecifies the top speed at any moment and provides movement coordinates.\n\n-\n-\n-\n- The printer moves directly from one point to the next without deviation.\n\n2. Video Stream:\n\n- Live footage of the 3D printer bed.\n- The angle of the camera relative to the printer is expected to be roughly consistent.\n\n3. Signal Stream:\n\n- Managed by the SKR board, which controls the 3D printer.\n- A hardware connection links the SKR board to our Raspberry Pi.\n- The Raspberry Pi gets a signal every time a movement completes. This is the signal, and\n\nconsists of the bed position and time. [time, x, y, z]\n\n- This data streams in real time, parallel to the printing process.\n\nInitialization\n\nFirst and foremost, the system, starting in the main Tracker Thread, begins receiving and\n\nrouting frames and signals through their corresponding routers. These routers will pass the data\n\nto the correct objects and threads through the shared global variable space based on the current\n\nstate of the system (Initializing or Tracking).\n\n\fSTREAM AI 1\n\n13\n\nThe duty of the initialization phase is to autonomously adapt to the camera position. It\n\nprimarily performs these tasks:\n\n1. Make initial predictions:\n\na. To transform G-code into predictions for every\n\nmoment in time, we first break down the G-code\ninto a sequence of motion instructions between\ncorners (or waypoints) where there are changes in\ndirection or speed.\n\nb. For each of these motions, we calculate the optimal\nspeed profile considering constraints like maximum\nspeeds and acceleration. We need to look at the future 2 moves as well, to\ndetermine what the final speed at the end of the motion should be.\n\nc. Given the optimal speed profiles for each motion, we\u2019ll calculate how long each\nmove should take using kinematics. One problem cannot be solved analytically,\nand needs to be solved iteratively. The iterative method tweaks the distance over\nwhich we accelerate or decelerate, attempting to converge on the desired final\nspeed without exceeding the machine's constraints.\n\nd. After computing the speed profile, the script then divides the motion into frames\n(based on the given frames-per-second parameter) and predicts the position and\nangle of the machine's tip for each frame. By the end of this process, we have a\ndetailed account of where the machine should be and at what angle for every\nsingle frame of the motion.\n\n2. Derive millimeter to pixel ratio:\n\nFor robustness, the system will autonomously map bed predictions to screen\npredictions without requiring users to enter any configuring variables. By waiting\nfor two signals and observing their millimeter disparity on the bed, we can\ncorrelate this with the pixel difference YOLO detects, facilitating the necessary\nconversion. We slide a 640x640 window across the image to run YOLO.\n\n3. Synchronize predictions with video:\n\nTo initiate the process, we align our screen predictions using the timestamp of the\nfirst YOLO-signal inference from the video.\n\n\fSTREAM AI 1\n\n14\n\nPredictions are imperfect due to accumulated small tracking errors. Reasons include:\n\n1. Merlin firmware in our 3D printer employs Beziet\u2019s\n\n6th order function for movement extrapolation, while\nour algorithm uses a simpler 2nd order function.\nHowever, our model considers acceleration.\n\n2. Some printers adjust movement factors, like speed\n\nand acceleration.\n\n3. Minor deviations arise when the printer halts.\n4. Merlin Firmware has intricate mechanisms for gcode\n\ninterpretation.\n\nInstead of replicating Merlin's exact approach, we designed a straightforward interpreter with\nreal-time error feedback loops, enhancing system robustness and compatibility with various 3D\nprinters.\n\nError Correction\n\nPredictions face two error types: Temporal and Spatial, addressed by the ErrorCorrection thread.\n\n1. Spatial Error: Mainly arises from incorrect ratios or YOLO inference. These are rare, typically\n\nminor initial tracking deviations. For demonstration, an exaggerated error was intentionally\n\nintroduced in the Spatial Error Correction video.\n\nMeasure\n\nDetect\n\nCorrect\n\nEverytime a signal arrives, we run\nYOLO to see where the tip is. We\ncompare this to our prediction and\nmeasure the difference in pixels.\n\nWhen there is a consistent spatial error\nof the same magnitude and direction for\nsufficient time.\n\nShift all\npredictions by\nspatial error.\n\n2. Temporal Error: This was a significant challenge. Without input signals, gauging temporal\n\nerror was tough. The ultimate solution simply involved immediate measurement and correction\n\n\fSTREAM AI 1\n\n15\n\nof temporal discrepancies. This error stems from varying interpretations of gcode, especially\n\ndiffering accelerations.\n\nMeasure\n\nDetect\n\nEverytime a signal arrives, we\ncompare the time of the signal,\ncompared to our predictions. We\nmeasure error in seconds.\n\nWe don\u2019t look for any consistent\nerrors. When there is an error, we\nsimply perform a correction.\n\nCorrect\n\nPause or skip\nahead to realign.\n\nAnalytics\n\nIncorporating a real-time feedback loop (autonomous corrections) results in near-flawless tip\n\ntracking. Using YOLO isn't constant; it's employed during signals and error measurements. This\n\nenables efficient pinpointing of the recently extruded material around the tip\u2014our target. Given\n\nour knowledge from the gcode about the printer's movement direction, we crop accordingly. This\n\ncropped image then enters our data preprocessing pipeline and undergoes classification by the\n\ntailored MobileNet model, determining whether extrusion is under, normal, or over.\n\n\fSTREAM AI 1\n\n16\n\nMetrics\n\nTo evaluate our system, we'll focus on its computational efficiency, while the accuracy is\n\nderived from the MobileNet model's test performance. We'll assess:\n\n- RAM Usage: We'll monitor RAM consumption over time.\n-\n\nInference Intervals: Given the adaptive nature of our MobileNet model, observing the\nintervals between successive inferences provides a measure of system speed. The model\nrefrains from inferring with a full buffer.\n\nThese metrics will be gauged on both my device and a Raspberry Pi 4, with respective\n\nspecifications provided below.\n\nDeployment to Raspberry Pi\n\nRaspberry Pi Metrics\n\nFuture Improvements\n\n\fSTREAM AI 1\n\n17\n\n1. Processor (Chipset) Details:\n\n- Model: Apple M1 Pro\n- Total Number of Cores: 14\n\n2. Memory:\n\n- Size: 16 GB\n- Type: LPDDR5\n- Manufacturer: Hynix\n\n3. Graphics/GPU:\n\n- Chipset Model: Apple M1 Pro\n- Metal Support: Metal 3\n\n4. Display:\n\n- Type: Built-in Liquid Retina XDR Display\n- Resolution: 3024 x 1964 Retina\n\n5. Storage:\n\n- Type: SSD (APPLE SSD AP0512R)\n- Capacity: 500.28 GB\n- Protocol: Apple Fabric (indicating it's an NVMe SSD)\n- Free Space: 120.4 GB\n\n6. File System:\n- Type: APFS\n\nhttps://qengineering.eu/install-opencv-on-raspberry-pi.html\nhttps://qengineering.eu/install-pytorch-on-raspberry-pi-4.html\nhttps://gist.github.com/wenig/8bab88dede5c838660dd05b8e5b2e23b\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html\n\n\f"