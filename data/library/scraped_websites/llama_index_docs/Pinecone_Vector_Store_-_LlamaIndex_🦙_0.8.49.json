{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo.html",
        "title": "Pinecone Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Pinecone Vector Store[\uf0c1](#pinecone-vector-store \"Permalink to this heading\")\n\nimport logging\nimport sys\nimport os\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\n## Creating a Pinecone Index[\uf0c1](#creating-a-pinecone-index \"Permalink to this heading\")\n\n/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from tqdm.autonotebook import tqdm\n\napi\\_key \\= os.environ\\[\"PINECONE\\_API\\_KEY\"\\]\npinecone.init(api\\_key\\=api\\_key, environment\\=\"eu-west1-gcp\")\n\n\\# dimensions are for text-embedding-ada-002\npinecone.create\\_index(\n    \"quickstart\", dimension\\=1536, metric\\=\"euclidean\", pod\\_type\\=\"p1\"\n)\n\npinecone\\_index \\= pinecone.Index(\"quickstart\")\n\n## Load documents, build the PineconeVectorStore and VectorStoreIndex[\uf0c1](#load-documents-build-the-pineconevectorstore-and-vectorstoreindex \"Permalink to this heading\")\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama\\_index.vector\\_stores import PineconeVectorStore\nfrom IPython.display import Markdown, display\n\nINFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNote: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNote: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\nNumExpr defaulting to 8 threads.\nNumExpr defaulting to 8 threads.\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\n\n\\# initialize without metadata filter\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\nvector\\_store \\= PineconeVectorStore(pinecone\\_index\\=pinecone\\_index)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 20729 tokens\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\n\\# set Logging to DEBUG for more detailed outputs\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\n> \\[retrieve\\] Total embedding token usage: 8 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1917 tokens\n> \\[get\\_response\\] Total LLM token usage: 1917 tokens\n> \\[get\\_response\\] Total LLM token usage: 1917 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author grew up writing short stories and programming on the IBM 1401. He also nagged his father to buy him a TRS-80 microcomputer, which he used to write simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but eventually switched to AI.**"
}