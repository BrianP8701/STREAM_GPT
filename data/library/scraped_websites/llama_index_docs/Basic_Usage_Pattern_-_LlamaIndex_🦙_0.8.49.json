{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/usage_pattern.html",
        "title": "Basic Usage Pattern - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Basic Usage Pattern[\uf0c1](#basic-usage-pattern \"Permalink to this heading\")\n\nThe general usage pattern of LlamaIndex is as follows:\n\n1.  Load in documents (either manually, or through a data loader)\n    \n2.  Parse the Documents into Nodes\n    \n3.  Construct Index (from Nodes or Documents)\n    \n4.  \\[Optional, Advanced\\] Building indices on top of other indices\n    \n5.  Query the index\n    \n6.  Parsing the response\n    \n\n## 1\\. Load in Documents[\uf0c1](#load-in-documents \"Permalink to this heading\")\n\nThe first step is to load in data. This data is represented in the form of `Document` objects. We provide a variety of [data loaders](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/connector/root.html) which will load in Documents through the `load_data` function, e.g.:\n\nfrom llama\\_index import SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('./data').load\\_data()\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.\n\nfrom llama\\_index import Document\n\ntext\\_list \\= \\[text1, text2, ...\\]\ndocuments \\= \\[Document(text\\=t) for t in text\\_list\\]\n\nA Document represents a lightweight container around the data source. You can now choose to proceed with one of the following steps:\n\n1.  Feed the Document object directly into the index (see section 3).\n    \n2.  First convert the Document into Node objects (see section 2).\n    \n\n## 2\\. Parse the Documents into Nodes[\uf0c1](#parse-the-documents-into-nodes \"Permalink to this heading\")\n\nThe next step is to parse these Document objects into Node objects. Nodes represent \u201cchunks\u201d of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \u201cparse\u201d source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\n\nparser \\= SimpleNodeParser.from\\_defaults()\n\nnodes \\= parser.get\\_nodes\\_from\\_documents(documents)\n\nYou can also choose to construct Node objects manually and skip the first section. For instance,\n\nfrom llama\\_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 \\= TextNode(text\\=\"<text\\_chunk>\", id\\_\\=\"<node\\_id>\")\nnode2 \\= TextNode(text\\=\"<text\\_chunk>\", id\\_\\=\"<node\\_id>\")\n\\# set relationships\nnode1.relationships\\[NodeRelationship.NEXT\\] \\= RelatedNodeInfo(node\\_id\\=node2.node\\_id)\nnode2.relationships\\[NodeRelationship.PREVIOUS\\] \\= RelatedNodeInfo(node\\_id\\=node1.node\\_id)\nnodes \\= \\[node1, node2\\]\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\nnode2.relationships\\[NodeRelationship.PARENT\\] \\= RelatedNodeInfo(node\\_id\\=node1.node\\_id, metadata\\={\"key\": \"val\"})\n\n## 3\\. Index Construction[\uf0c1](#index-construction \"Permalink to this heading\")\n\nWe can now build an index over these Document objects. The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).\n\n`from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\nYou can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex(nodes)\n\nDepending on which index you use, LlamaIndex may make LLM calls in order to build the index.\n\n### Reusing Nodes across Index Structures[\uf0c1](#reusing-nodes-across-index-structures \"Permalink to this heading\")\n\nIf you have multiple Node objects defined, and wish to share these Node objects across multiple index structures, you can do that. Simply instantiate a StorageContext object, add the Node objects to the underlying DocumentStore, and pass the StorageContext around.\n\nfrom llama\\_index import StorageContext\n\nstorage\\_context \\= StorageContext.from\\_defaults()\nstorage\\_context.docstore.add\\_documents(nodes)\n\nindex1 \\= VectorStoreIndex(nodes, storage\\_context\\=storage\\_context)\nindex2 \\= SummaryIndex(nodes, storage\\_context\\=storage\\_context)\n\n**NOTE**: If the `storage_context` argument isn\u2019t specified, then it is implicitly created for each index during index construction. You can access the docstore associated with a given index through `index.storage_context`.\n\n### Inserting Documents or Nodes[\uf0c1](#inserting-documents-or-nodes \"Permalink to this heading\")\n\nYou can also take advantage of the `insert` capability of indices to insert Document objects one at a time instead of during index construction.\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex(\\[\\])\nfor doc in documents:\n    index.insert(doc)\n\nIf you want to insert nodes on directly you can use `insert_nodes` function instead.\n\nfrom llama\\_index import VectorStoreIndex\n\n\\# nodes: Sequence\\[Node\\]\nindex \\= VectorStoreIndex(\\[\\])\nindex.insert\\_nodes(nodes)\n\nSee the [Document Management How-To](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/document_management.html) for more details on managing documents and an example notebook.\n\n### Customizing Documents[\uf0c1](#customizing-documents \"Permalink to this heading\")\n\nWhen creating documents, you can also attach useful metadata. Any metadata added to a document will be copied to the nodes that get created from their respective source document.\n\ndocument \\= Document(\n    text\\='text',\n    metadata\\={\n        'filename': '<doc\\_file\\_name>',\n        'category': '<category>'\n    }\n)\n\nMore information and approaches to this are discussed in the section [Customizing Documents](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/documents_and_nodes/usage_documents.html).\n\n### Customizing LLM\u2019s[\uf0c1](#customizing-llm-s \"Permalink to this heading\")\n\nBy default, we use OpenAI\u2019s `text-davinci-003` model. You may choose to use another LLM when constructing an index.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext, set\\_global\\_service\\_context\nfrom llama\\_index.llms import OpenAI\n\n...\n\n\\# define LLM\nllm \\= OpenAI(model\\=\"gpt-4\", temperature\\=0, max\\_tokens\\=256)\n\n\\# configure service context\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\nset\\_global\\_service\\_context(service\\_context)\n\n\\# build index\nindex \\= VectorStoreIndex.from\\_documents(\n    documents\n)\n\nTo save costs, you may want to use a local model.\n\nfrom llama\\_index import ServiceContext\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=\"local\")\n\nThis will use llama2-chat-13B from with LlamaCPP, and assumes you have `llama-cpp-python` installed. Full LlamaCPP usage guide is available in a [notebook here](https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html).\n\nSee the [Custom LLM\u2019s How-To](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/usage_custom.html) for more details.\n\n### Global ServiceContext[\uf0c1](#global-servicecontext \"Permalink to this heading\")\n\nIf you wanted the service context from the last section to always be the default, you can configure one like so:\n\nfrom llama\\_index import set\\_global\\_service\\_context\nset\\_global\\_service\\_context(service\\_context)\n\nThis service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.\n\nFor more details on the service context, including how to create a global service context, see the page [Customizing the ServiceContext](https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/service_context.html).\n\n### Customizing Prompts[\uf0c1](#customizing-prompts \"Permalink to this heading\")\n\nDepending on the index used, we used default prompt templates for constructing the index (and also insertion/querying). See [Custom Prompts How-To](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/prompts.html) for more details on how to customize your prompt.\n\n### Customizing embeddings[\uf0c1](#customizing-embeddings \"Permalink to this heading\")\n\nFor embedding-based indices, you can choose to pass in a custom embedding model. See [Custom Embeddings How-To](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/embeddings/usage_pattern.html) for more details.\n\n### Cost Analysis[\uf0c1](#cost-analysis \"Permalink to this heading\")\n\nCreating an index, inserting to an index, and querying an index may use tokens. We can track token usage through the outputs of these operations. When running operations, the token usage will be printed.\n\nYou can also fetch the token usage through `TokenCountingCallback` handler. See [Cost Analysis How-To](https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/cost_analysis/usage_pattern.html) for more details.\n\n### \\[Optional\\] Save the index for future use[\uf0c1](#optional-save-the-index-for-future-use \"Permalink to this heading\")\n\nBy default, data is stored in-memory. To persist to disk:\n\nindex.storage\\_context.persist(persist\\_dir\\=\"<persist\\_dir>\")\n\nYou may omit persist\\_dir to persist to `./storage` by default.\n\nTo reload from disk:\n\nfrom llama\\_index import StorageContext, load\\_index\\_from\\_storage\n\n\\# rebuild storage context\nstorage\\_context \\= StorageContext.from\\_defaults(persist\\_dir\\=\"<persist\\_dir>\")\n\n\\# load index\nindex \\= load\\_index\\_from\\_storage(storage\\_context)\n\n**NOTE**: If you had initialized the index with a custom `ServiceContext` object, you will also need to pass in the same ServiceContext during `load_index_from_storage` or ensure you have a global service context.\n\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\nset\\_global\\_service\\_context(service\\_context)\n\n\\# when first building the index\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, \\# service\\_context=service\\_context -> optional if not using global\n)\n\n...\n\n\\# when loading the index from disk\nindex \\= load\\_index\\_from\\_storage(\n    StorageContext.from\\_defaults(persist\\_dir\\=\"<persist\\_dir>\")\n    \\# service\\_context=service\\_context -> optional if not using global\n)\n\n## 4\\. \\[Optional, Advanced\\] Building indices on top of other indices[\uf0c1](#optional-advanced-building-indices-on-top-of-other-indices \"Permalink to this heading\")\n\nYou can build indices on top of other indices! Composability gives you greater power in indexing your heterogeneous sources of data. For a discussion on relevant use cases, see our [Query Use Cases](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/question_and_answer.html). For technical details and examples, see our [Composability How-To](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/composability.html).\n\n## 5\\. Query the index.[\uf0c1](#query-the-index \"Permalink to this heading\")\n\nAfter building the index, you can now query it with a `QueryEngine`. Note that a \u201cquery\u201d is simply an input to an LLM - this means that you can use the index for question-answering, but you can also do more than that!\n\n### High-level API[\uf0c1](#high-level-api \"Permalink to this heading\")\n\nTo start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nresponse \\= query\\_engine.query(\"Write an email to the user given their background information.\")\nprint(response)\n\n### Low-level API[\uf0c1](#low-level-api \"Permalink to this heading\")\n\nWe also support a low-level composition API that gives you more granular control over the query logic. Below we highlight a few of the possible customizations.\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    get\\_response\\_synthesizer,\n)\nfrom llama\\_index.retrievers import VectorIndexRetriever\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\nfrom llama\\_index.indices.postprocessor import SimilarityPostprocessor\n\n\\# build index\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n\\# configure retriever\nretriever \\= VectorIndexRetriever(\n    index\\=index,\n    similarity\\_top\\_k\\=2,\n)\n\n\\# configure response synthesizer\nresponse\\_synthesizer \\= get\\_response\\_synthesizer()\n\n\\# assemble query engine\nquery\\_engine \\= RetrieverQueryEngine(\n    retriever\\=retriever,\n    response\\_synthesizer\\=response\\_synthesizer,\n    node\\_postprocessors\\=\\[\n        SimilarityPostprocessor(similarity\\_cutoff\\=0.7)\n    \\]\n\n)\n\n\\# query\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nYou may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\n\nFor a full list of implemented components and the supported configurations, please see the detailed [reference docs](https://docs.llamaindex.ai/en/stable/api_reference/query.html).\n\nIn the following, we discuss some commonly used configurations in detail.\n\n### Configuring retriever[\uf0c1](#configuring-retriever \"Permalink to this heading\")\n\nAn index can have a variety of index-specific retrieval modes. For instance, a summary index supports the default `SummaryIndexRetriever` that retrieves all nodes, and `SummaryIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.\n\nFor convenience, you can also use the following shorthand:\n\n    \\# SummaryIndexRetriever\n    retriever \\= index.as\\_retriever(retriever\\_mode\\='default')\n    \\# SummaryIndexEmbeddingRetriever\n    retriever \\= index.as\\_retriever(retriever\\_mode\\='embedding')\n\nAfter choosing your desired retriever, you can construct your query engine:\n\nquery\\_engine \\= RetrieverQueryEngine(retriever)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\nThe full list of retrievers for each index (and their shorthand) is documented in the [Query Reference](https://docs.llamaindex.ai/en/stable/api_reference/query.html).\n\n### Configuring response synthesis[\uf0c1](#configuring-response-synthesis \"Permalink to this heading\")\n\nAfter a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.\n\nYou can configure it via\n\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever, response\\_mode\\=<response\\_mode\\>)\n\nRight now, we support the following options:\n\n*   `default`: \u201ccreate and refine\u201d an answer by sequentially going through each retrieved `Node`; This makes a separate LLM call per Node. Good for more detailed answers.\n    \n*   `compact`: \u201ccompact\u201d the prompt during each LLM call by stuffing as many `Node` text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \u201ccreate and refine\u201d an answer by going through multiple prompts.\n    \n*   `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\n    \n*   `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking `response.source_nodes`. The response object is covered in more detail in Section 5.\n    \n*   `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\n    \n\nindex \\= SummaryIndex.from\\_documents(documents)\nretriever \\= index.as\\_retriever()\n\n\\# default\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever, response\\_mode\\='default')\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n\\# compact\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever, response\\_mode\\='compact')\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n\\# tree summarize\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever, response\\_mode\\='tree\\_summarize')\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n\\# no text\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(retriever, response\\_mode\\='no\\_text')\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n### Configuring node postprocessors (i.e. filtering and augmentation)[\uf0c1](#configuring-node-postprocessors-i-e-filtering-and-augmentation \"Permalink to this heading\")\n\nWe also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects. This can help reduce the time/number of LLM calls/cost or improve response quality.\n\nFor example:\n\n*   `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`.\n    \n*   `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n    \n*   `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.\n    \n\nThe full list of node postprocessors is documented in the [Node Postprocessor Reference](https://docs.llamaindex.ai/en/stable/api_reference/node_postprocessor.html).\n\nTo configure the desired node postprocessors:\n\nnode\\_postprocessors \\= \\[\n    KeywordNodePostprocessor(\n        required\\_keywords\\=\\[\"Combinator\"\\],\n        exclude\\_keywords\\=\\[\"Italy\"\\]\n    )\n\\]\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(\n    retriever, node\\_postprocessors\\=node\\_postprocessors\n)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\n\n## 6\\. Parsing the response[\uf0c1](#parsing-the-response \"Permalink to this heading\")\n\nThe object returned is a [`Response` object](https://docs.llamaindex.ai/en/stable/api_reference/response.html). The object contains both the response text as well as the \u201csources\u201d of the response:\n\nresponse \\= query\\_engine.query(\"<query\\_str>\")\n\n\\# get response\n\\# response.response\nstr(response)\n\n\\# get sources\nresponse.source\\_nodes\n\\# formatted sources\nresponse.get\\_formatted\\_sources()\n\nAn example is shown below. ![](https://docs.llamaindex.ai/en/stable/_images/response_1.jpeg)"
}