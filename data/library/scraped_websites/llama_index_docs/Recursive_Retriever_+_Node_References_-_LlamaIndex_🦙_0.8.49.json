{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html",
        "title": "Recursive Retriever + Node References - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nThis guide shows how you can use recursive retrieval to traverse node relationships and fetch nodes based on \u201creferences\u201d.\n\nNode references are a powerful concept. When you first perform retrieval, you may want to retrieve the reference as opposed to the raw text. You can have multiple references point to the same node.\n\nIn this guide we explore some different usages of node references:\n\n*   **Chunk references**: Different chunk sizes referring to a bigger chunk\n    \n*   **Metadata references**: Summaries + Generated Questions referring to a bigger chunk\n    \n\n%load\\_ext autoreload\n%autoreload 2\n%env OPENAI\\_API\\_KEY=YOUR\\_API\\_KEY\n\n%pip install -U llama\\_hub llama\\_index braintrust autoevals pypdf pillow transformers torch torchvision\n\n## Load Data + Setup[\uf0c1](#load-data-setup \"Permalink to this heading\")\n\nIn this section we download the Llama 2 paper and create an initial set of nodes (chunk size 1024).\n\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pdf.base import PDFReader\nfrom llama\\_index.response.notebook\\_utils import display\\_source\\_node\nfrom llama\\_index.retrievers import RecursiveRetriever\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\nfrom llama\\_index import VectorStoreIndex, ServiceContext\nfrom llama\\_index.llms import OpenAI\nimport json\n\nloader \\= PDFReader()\ndocs0 \\= loader.load\\_data(file\\=Path(\"./data/llama2.pdf\"))\n\nfrom llama\\_index import Document\n\ndoc\\_text \\= \"\\\\n\\\\n\".join(\\[d.get\\_content() for d in docs0\\])\ndocs \\= \\[Document(text\\=doc\\_text)\\]\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom llama\\_index.schema import IndexNode\n\nnode\\_parser \\= SimpleNodeParser.from\\_defaults(chunk\\_size\\=1024)\n\nbase\\_nodes \\= node\\_parser.get\\_nodes\\_from\\_documents(docs)\n\\# set node ids to be a constant\nfor idx, node in enumerate(base\\_nodes):\n    node.id\\_ \\= f\"node-{idx}\"\n\nfrom llama\\_index.embeddings import resolve\\_embed\\_model\n\nembed\\_model \\= resolve\\_embed\\_model(\"local:BAAI/bge-small-en\")\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\")\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm, embed\\_model\\=embed\\_model\n)\n\n## Baseline Retriever[\uf0c1](#baseline-retriever \"Permalink to this heading\")\n\nDefine a baseline retriever that simply fetches the top-k raw text nodes by embedding similarity.\n\nbase\\_index \\= VectorStoreIndex(base\\_nodes, service\\_context\\=service\\_context)\nbase\\_retriever \\= base\\_index.as\\_retriever(similarity\\_top\\_k\\=2)\n\nretrievals \\= base\\_retriever.retrieve(\n    \"Can you tell me about the key concepts for safety finetuning\"\n)\n\nfor n in retrievals:\n    display\\_source\\_node(n, source\\_length\\=1500)\n\nquery\\_engine\\_base \\= RetrieverQueryEngine.from\\_args(\n    base\\_retriever, service\\_context\\=service\\_context\n)\n\nresponse \\= query\\_engine\\_base.query(\n    \"Can you tell me about the key concepts for safety finetuning\"\n)\nprint(str(response))\n\n## Chunk References: Smaller Child Chunks Referring to Bigger Parent Chunk[\uf0c1](#chunk-references-smaller-child-chunks-referring-to-bigger-parent-chunk \"Permalink to this heading\")\n\nIn this usage example, we show how to build a graph of smaller chunks pointing to bigger parent chunks.\n\nDuring query-time, we retrieve smaller chunks, but we follow references to bigger chunks. This allows us to have more context for synthesis.\n\nsub\\_chunk\\_sizes \\= \\[128, 256, 512\\]\nsub\\_node\\_parsers \\= \\[\n    SimpleNodeParser.from\\_defaults(chunk\\_size\\=c) for c in sub\\_chunk\\_sizes\n\\]\n\nall\\_nodes \\= \\[\\]\nfor base\\_node in base\\_nodes:\n    for n in sub\\_node\\_parsers:\n        sub\\_nodes \\= n.get\\_nodes\\_from\\_documents(\\[base\\_node\\])\n        sub\\_inodes \\= \\[\n            IndexNode.from\\_text\\_node(sn, base\\_node.node\\_id) for sn in sub\\_nodes\n        \\]\n        all\\_nodes.extend(sub\\_inodes)\n\n    \\# also add original node to node\n    original\\_node \\= IndexNode.from\\_text\\_node(base\\_node, base\\_node.node\\_id)\n    all\\_nodes.append(original\\_node)\n\nall\\_nodes\\_dict \\= {n.node\\_id: n for n in all\\_nodes}\n\nvector\\_index\\_chunk \\= VectorStoreIndex(\n    all\\_nodes, service\\_context\\=service\\_context\n)\n\nvector\\_retriever\\_chunk \\= vector\\_index\\_chunk.as\\_retriever(similarity\\_top\\_k\\=2)\n\nretriever\\_chunk \\= RecursiveRetriever(\n    \"vector\",\n    retriever\\_dict\\={\"vector\": vector\\_retriever\\_chunk},\n    node\\_dict\\=all\\_nodes\\_dict,\n    verbose\\=True,\n)\n\nnodes \\= retriever\\_chunk.retrieve(\n    \"Can you tell me about the key concepts for safety finetuning\"\n)\nfor node in nodes:\n    display\\_source\\_node(node, source\\_length\\=2000)\n\nquery\\_engine\\_chunk \\= RetrieverQueryEngine.from\\_args(\n    retriever\\_chunk, service\\_context\\=service\\_context\n)\n\nresponse \\= query\\_engine\\_chunk.query(\n    \"Can you tell me about the key concepts for safety finetuning\"\n)\nprint(str(response))\n\n## Evaluation[\uf0c1](#evaluation \"Permalink to this heading\")\n\nWe evaluate how well our recursive retrieval + node reference methods work. We evaluate both chunk references as well as metadata references. We use embedding similarity lookup to retrieve the reference nodes.\n\nWe compare both methods against a baseline retriever where we fetch the raw nodes directly.\n\nIn terms of metrics, we evaluate using both hit-rate and MRR.\n\n### Dataset Generation[\uf0c1](#dataset-generation \"Permalink to this heading\")\n\nWe first generate a dataset of questions from the set of text chunks.\n\nfrom llama\\_index.evaluation import (\n    generate\\_question\\_context\\_pairs,\n    EmbeddingQAFinetuneDataset,\n)\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\neval\\_dataset \\= generate\\_question\\_context\\_pairs(base\\_nodes)\n\neval\\_dataset.save\\_json(\"data/llama2\\_eval\\_dataset.json\")\n\n\\# optional\neval\\_dataset \\= EmbeddingQAFinetuneDataset.from\\_json(\n    \"data/llama2\\_eval\\_dataset.json\"\n)\n\n### Compare Results[\uf0c1](#compare-results \"Permalink to this heading\")\n\nWe run evaluations on each of the retrievers to measure hit rate and MRR.\n\nWe find that retrievers with node references (either chunk or metadata) tend to perform better than retrieving the raw chunks.\n\nimport pandas as pd\nfrom llama\\_index.evaluation import RetrieverEvaluator, get\\_retrieval\\_results\\_df\n\n\\# set vector retriever similarity top k to higher\ntop\\_k \\= 10\n\ndef display\\_results(names, results\\_arr):\n    \"\"\"Display results from evaluate.\"\"\"\n\n    hit\\_rates \\= \\[\\]\n    mrrs \\= \\[\\]\n    for name, eval\\_results in zip(names, results\\_arr):\n        metric\\_dicts \\= \\[\\]\n        for eval\\_result in eval\\_results:\n            metric\\_dict \\= eval\\_result.metric\\_vals\\_dict\n            metric\\_dicts.append(metric\\_dict)\n        results\\_df \\= pd.DataFrame(metric\\_dicts)\n\n        hit\\_rate \\= results\\_df\\[\"hit\\_rate\"\\].mean()\n        mrr \\= results\\_df\\[\"mrr\"\\].mean()\n        hit\\_rates.append(hit\\_rate)\n        mrrs.append(mrr)\n\n    final\\_df \\= pd.DataFrame(\n        {\"retrievers\": names, \"hit\\_rate\": hit\\_rates, \"mrr\": mrrs}\n    )\n    display(final\\_df)\n\nvector\\_retriever\\_chunk \\= vector\\_index\\_chunk.as\\_retriever(\n    similarity\\_top\\_k\\=top\\_k\n)\nretriever\\_chunk \\= RecursiveRetriever(\n    \"vector\",\n    retriever\\_dict\\={\"vector\": vector\\_retriever\\_chunk},\n    node\\_dict\\=all\\_nodes\\_dict,\n    verbose\\=True,\n)\nretriever\\_evaluator \\= RetrieverEvaluator.from\\_metric\\_names(\n    \\[\"mrr\", \"hit\\_rate\"\\], retriever\\=retriever\\_chunk\n)\n\\# try it out on an entire dataset\nresults\\_chunk \\= await retriever\\_evaluator.aevaluate\\_dataset(\n    eval\\_dataset, show\\_progress\\=True\n)\n\nvector\\_retriever\\_metadata \\= vector\\_index\\_metadata.as\\_retriever(\n    similarity\\_top\\_k\\=top\\_k\n)\nretriever\\_metadata \\= RecursiveRetriever(\n    \"vector\",\n    retriever\\_dict\\={\"vector\": vector\\_retriever\\_metadata},\n    node\\_dict\\=all\\_nodes\\_dict,\n    verbose\\=True,\n)\nretriever\\_evaluator \\= RetrieverEvaluator.from\\_metric\\_names(\n    \\[\"mrr\", \"hit\\_rate\"\\], retriever\\=retriever\\_metadata\n)\n\\# try it out on an entire dataset\nresults\\_metadata \\= await retriever\\_evaluator.aevaluate\\_dataset(\n    eval\\_dataset, show\\_progress\\=True\n)\n\nbase\\_retriever \\= base\\_index.as\\_retriever(similarity\\_top\\_k\\=10)\nretriever\\_evaluator \\= RetrieverEvaluator.from\\_metric\\_names(\n    \\[\"mrr\", \"hit\\_rate\"\\], retriever\\=base\\_retriever\n)\n\\# try it out on an entire dataset\nresults\\_base \\= await retriever\\_evaluator.aevaluate\\_dataset(\n    eval\\_dataset, show\\_progress\\=True\n)\n\nfull\\_results\\_df \\= get\\_retrieval\\_results\\_df(\n    \\[\n        \"Base Retriever\",\n        \"Retriever (Chunk References)\",\n        \"Retriever (Metadata References)\",\n    \\],\n    \\[results\\_base, results\\_chunk, results\\_metadata\\],\n)\ndisplay(full\\_results\\_df)"
}