["STREAM AI Team", "Dr. Sun\u2019s Zoom Link: https://zoom.us/j/3114058196\nMeetings Fridays @10:00 AM\nHongyue Sun: hongyuesun@uga.edu\nBrian Przezdziecki: bprzezdz@buffalo.edu", "I\u2019ve identified the real challenge:\nWhen I want to search for something in my textbook, I merely need to look\nat the table of contents. This \u201ctable of contents\u201d works because:\n- I understand the semantic meaning of all the keywords and how they\nrelate to my query in mind.\n- The text is rigorously made with an organization of ideas in mind.\nOn the initial forward pass in constructing the summarization tree, there is\nno problem. But at what points do branches need to be cut off and\nrelocated?\nTree Cleanup: O(num_of_nodes^2)\nWorst Case Num of Nodes: O(log(num_of_chunks))\nTree Cleanup with respect to num_of_chunks:\nUltimate Unit of Cost is num_api_calls", "9/29/23-10/6/23\nLangchain docs\nOpen Interpreter group\nGPT3.5 disability", "9/22/23-9/29/23\n\u25cf Implement first rendition of knowledge graph construction on GCP cloud\nfunctions\n\u25cf Create test dataset, and run KG construction on it\n\u25cf Read Langchain docs and open-interpreter code", "Topk & Topp links for Brian\nhttps://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-t\nemperature/\nhttps://huggingface.co/blog/how-to-generate\nhttps://docs.cohere.com/docs/controlling-generation-with-top-k-top-p", "Local LLAMA\nI downloaded LLAMA 70B chat and 7B chat locally. 70B chat takes up almost 140\nGB. I used Andrej Kaparthy\u2019s LLAMA pure C++ inference repo. Anything 13+\nbillion parameters doesn't work because of integer flow in pointer arithmetic, which\nis yet to be fixed, and second, even if it were fixed, this repo is doing float32\ninference right now, so it would be fairly unusably slow. The 7B LLAMA model runs\nat 30 tokens/sec on my Macbook M1 Pro. For this reason, I\u2019ll opt to work entirely\nin the cloud.\nPerformance metrics can be changed to by token usage and model throughput.", "Claude2 vs GPT4\nOpenAI <-> Microsoft\nDeepmind <-> Google\nAnthropic <-> Amazon\nxAi <-> Tesla, X\nMetaAI <-> Meta\nAs of now the closest competitor to ChatGPT appears to be Claude2 from\nAnthropic. It has a max context length of 100,000 tokens, but it hallucinates more.\nGPT4 might be dethroned by Gemini, from Deepmind, said to come in December.", "Another agent\nThis project has a much simpler codebase, and runs directly and interacts with\nyour local terminal. It can interact with the chrome browser, file systems and more.\nOpen Interpreter\nGPT-engineer is not good. The ones of interest so far are:\n- Open Interpreter\n- Langchain\n- AutoGPT", "Multimodality in ChatGPT in 2 weeks\nGPT-4V (GPT-4 Vision)\nThis twitter thread shows a bunch of examples of it:\nhttps://twitter.com/youraimarketer/status/1706461715078975778\nLots of problems, but cool and shows the direction we are moving in\u2026 so fast.", "Data\nFirestore for persistent storage of graphs as JSONs\nCloud Storage for storing objects and documents as key-value", "", "", "9/15/23-9/22/23\n\u25cf Read paper: \u2018Consciousness in Artificial Intelligence: Insights from the\nScience of Consciousness\u2019\n\u25cf Explore open source GPT projects", "Consciousness in Artificial Intelligence\n\u201cOur analysis suggests that no current AI systems are conscious, but also\nsuggests that there are no obvious technical barriers to building AI systems which\nsatisfy these indicators.\u201d\n\u201cThe main alternative to this approach is to use behavioural tests for\nconsciousness, but this method is unreliable because AI systems can be trained to\nmimic human behaviours while working in very different ways.\u201d", "GPT-Engineer\nHave GPT create an entire codebase for you using chain of thought, different\nGPTs and more.", "AutoGPT\nAutoGPT is an open source project that takes GPT and allows it to act as an\nagent. Give it a goal, however abstract, and AutoGPT will break the goal into\nsteps, plan, think, reason, criticize and share everything with you. It can go browse\nthe internet, try different solutions and get up to date, and always seeks your\napproval. It will even argue with you if something is wrong. It can analyze\nwebsites, save the data and analyze the results.\nYoutube video talking about what AutoGPT and GPT-Engineer can do.\nIt's all open source", "BookGPT\nGive GPT a book and have a conversation with it\nThe open source code for Book-GPT\nUses Pinecone for vector embeddings and OpenAI for the language model. Both\nare paid and you need to use your own API key\nThere are open source versions of these. Although GPT-4 and Pinecone are more\npowerful", "Future Plans\nThere\u2019s been a lot built with GPT already. Before I build anything myself, I want to\ndive into these open source projects, research papers and learn more about how\nothers are building, what works and what\u2019s already been made. This is super cool\nstuff.\nA good thing for me is, given how much time I\u2019ve spent using the base\nfoundational models, I have a very good understanding of the limitations and\ncapabilities of these models. So time to dive in and see how these people made\nthese GPT agents!\nUnfortunately, I am going to need to spend my own money to use the OpenAI API.\nI hope it won\u2019t be too expensive.", "Using LLM projects\nFrom Andrej Kaparthy, the famous FSD lead at Tesla and currently at OpenAI:\nhttps://github.com/karpathy/nanoGPT GPT-2\nhttps://github.com/karpathy/llama2.c LLAMA2 in C no dependencies\nhttps://lambdalabs.com/ - Good priced Nvidia H100 GPUs on cloud\nhttps://github.com/abetlen/llama-cpp-python LLAMA2 python no dependencies\nhttps://github.com/shawwn/llama-dl LLAMA weight download\nhttps://github.com/facebookresearch/llama-recipes LLAMA examples\nhttps://github.com/cocktailpeanut/dalai LLAMA/ALPACA Run\nhttps://github.com/Paitesanshi/LLM-Agent-Survey\nhttps://lilianweng.github.io/posts/2023-06-23-agent/", "9/8/23-9/15/23\n\u25cf KG, Tree of Thoughts, Reasoning, Vector Embeddings\n\u25cf Migrate pytorch to ONNX\n\u25cf Write onnx run scripts for MobileNet\n\u25cf Make RaspberryPi branch with new venv and ncnn\n\u25cf Migrate onnx to ncnn\n\u25cf Download opencv from source on raspberry pi\n\u25cf Download ncnn from source\n\u25cf Run system on raspberry pi", "Raspberry Pi Optimizations\n- Rather than using Pytorch, I will migrate all models to an ONNX format. Once saved as an ONNX model, we can\nimport it into an ARM-friendly C++ framework such as ncnn or MNN. It will speed up our model considerably.\n- This means we don\u2019t need Pytorch, Torch or Onnxruntime. This also allows us to save SSD and RAM.", "Pt -> ONNX & MobileNet ONNX Run with ChatGPT\nThis could of took hours. I ran into so many errors. With chatgpt, it took 40 minutes:\n- Load mobilenet model with changed final layer, add weights, convert to onnx with\ntorch using dummy input with same transforms.\n- To run new MobileNet onnx model create an ort.InferenceSession, apply transforms\nto images. In first index of outputs get logits, apply softmax and boom you get the\npredicted class. So easy with chatgpt.\nIn this scenario, whenever I ran into a bug I just copied and pasted it to ChatGPT. It\u2019s like\nan instant personal StackOverflow user. At one point, I did have to use my own reasoning\nto deduce that one bug was from the changed final layer, but once I mentioned it\nChatGPT recognized it. Because ChatGPT didn\u2019t have full context of my project it didn\u2019t\nknow I had changed the final layer.\nMy Conversation", "OpenAI Stronger Control\nLLMs produce unstructured output. False.\nOpenAI Function calling allows us to direct the model to invoke certain functions in\nany manner you wish, and produce a structured output of any format.\nAdditionally, decrease temperature hyperparameter for consistent and high\nconfidence output answers.\nEnsemble methods.\nThese are techniques to get better control over the model when using it in our\nsystem, and remove unpredictable behavior.", "New operations\nAs coders there are things we can use to build systems. We have arithmetic,\nconditionals loops. We\u2019ve developed functions and other things we can use like\nhashing, objects, etc.\nLLMs provide an entire new suite of possible operations. Here\u2019s some examples:\nClassification, comparisons, shallow reasoning, compression, extraction, self\nprompting and a variety of other simple tasks.\nThere are now specific things we can add to our system. We can create custom\nfinetuned injection models made for certain tasks.", "Example of a system\nThis is a very rough example of what I\nimagine. Very rough, there are many\nproblems and issues with this that I need to\nresolve. (I made this a while ago)\nBut essentially you see the idea of using\nthe \u2018classification\u2019 operation to organize\ndata here. I just wanted to provide an\nexample.\nDon\u2019t look too deeply into this, this is\nmostly garbage.\nAlso, I organized the google drive better:\nLLMKB Project\nThis stage of the project is very fun :)\n(Planning and research)", "9/1/23-9/8/23\n\u25cf Fix initial horizontal bug\n\u25cf Collect speed and RAM metrics\n\u25cf Create long-form report\n\u25cf Read LLM papers\n\u25cf Download libraries from source on raspberry pi\n\u25cf Get streams on raspberry pi\n\u25cf Run system on raspberry pi", "Downloading libraries from source\nOnnxruntime, opencv, torch and pytorch all have to be downloaded in a very\narduous process on the raspberry pi.", "Longer form report\nI\u2019m not done with the report yet, as I\u2019ll measure more metrics, and need to get the\nsystem on the raspberry pi. But here is what I have so far. If you think anything\nneeds more clarification, or should be removed, or any other suggestions, let me\nknow and I will change it.\nReport", "LLM Papers\nI\u2019ve gathered more papers of interest. I am in the process of reading all of these.\nThese papers consist of knowledge graphs, reasoning graphs, vector\nembeddings, factuality, experiments, LLAMA, mathematical reasoning, new\nreinforcement learning techniques, self teaching and improvements, agents and\napplications to different financial, industrial and medical industries. I will read all of\nthese, and in particular find which ones are most relevant to this project. I plan to\nspend lots of time catching up on the most recent and up to date existing work\nbefore beginning designing and engineering my own system.\nLLM Papers", "Notes\nI will add all my notes while reading this paper into this doc. I will talk about the\npapers I read, my own ideas and rants and more in this doc. Upon reading all\npapers, learning more and defining the system clearly implementation may begin.\nHowever, I want to spend time really preparing for this. I care about this a lot, and\nreally want to also have this for myself throughout my life.\nLLM Notes", "Software things I\u2019ll need to build\nDocument and web scrapers\nVector embedding spaces\nSeries of LLAMA2, GPT3.5turbo and GPT4 models\nDatabase", "Computer vision and Language project\nThe last step of the computer vision project is really to just deploy the system to\nthe raspberry pi, and making some final optimizations to have it run in real time,\npossibly requiring network pruning, MobileNet fitting to raspberry pi or hardware\nspecific dependency changes. Pytorch has libraries and functions optimized for\nraspberry pis.\nFor now, I\u2019m going to begin working on both projects at once - closing and\nfinishing up the computer vision project, and beginning and getting ready for the\nNLP project. Very exciting.", "8/25/23-9/1/23\n\u25cf Refactor threads into modules\n\u25cf Skip impossible predictions (When tip is covering material)\n\u25cf Optimize frame buffer in initialization stage (Can cause zsh hardware errors\nwhen RAM overflows)\n\u25cf Define metrics for metric-driven research\n\u25cf Measure diameter method\n\u25cf Build test runs to measure metrics\n\u25cf Collect metrics to measure metrics on different videos\n\u25cf Another idea: LLM for diagnosis/process training, how to integrate text and\nimages?", "Skip impossible predictions\nThis is an example of an impossible prediction, when the tip is blocking the view\nso we cannot see the recently extruded material. In this scenario, we simply do not\nattempt to measure or classify the extrusion.\nThe implementation simply consists of looking at the angle (direction of\nmovement)\nIf in the future, we can have another camera angle, this can be bypassed.", "On the endoscope\nI recall in the meeting it was mentioned that using the endoscope might be easier.\nHowever, the problem is the endoscopes shaking is unpredictable. The only way\nto locate the tip would be running YOLO more frequently which is more\ncomputationally expensive.\nUnless of course, we simply mounted (physically in the real world) the endoscope\nso it doesn't shake. Shaking of around 10 pixels in any direction is acceptable for\nclassification, but not for diameter measurement, at least with my current method.\nSo unless shaking can be addressed, the Instant 360 is better, because we don\u2019t\nneed to always use YOLO, but can rather try to predict where the top is based off\nthe gcode.", "Optimize frame buffer\nThe frame buffer: When in the initialization phase we need two signals where the\nhorizontal distance is greater than RATIO_INITIALIZATION_MIN_RANGE. I just\nset it to 5 mm for now. So we need to save the frames in a buffer when they\nstream in so we can look back on them for the initialization phase. When\ninitialization is done, we can discard the buffer. However, if initialization takes too\nlong, the buffer can take up too much memory.\nThus, simple solution: Keep 2 buffers. One buffer stores frames. Whenever a\nsignal arrives, find the corresponding frame, save to the second buffer, and delete\nany previous frames. When YOLO is performed on that frame in the second\nbuffer, we can once again discard that frame.\nUmm, this might seem confusing, but it\u2019s not important to understand, just a\nspecific optimization.", "cursor.so said to refactor\nCursor is a code editor like\nvscode, but with chatgpt built\ninto it. You can share projects\nof massive sizes with it and get\nfeedback, and it can help you\ncode much better than\nChatGPT does because it has\ncontext and can see you entire\nproject. On the left is a simple\nexample where Cursor gave\nme some refactoring\nsuggestions which I will do.", "Code refactor into classes\nAs cursor noted, it will be better to break up the threads into each individual\nmodule.\nAdditionally, I want Sahil to work on his own separate branch and be able to\nmerge my changes as they happen. However, for this to work smoothly, I\u2019ll need to\ngive him his own file to work in. So I\u2019ll just create an additional thread module he\ncan use which makes it easier for both of us, and allows him to develop locally and\nstill get my updates.\nBy the way, the indexing into a compressed\nknowledge base performed by cursor.so is similar\nto what I referred to for the LLM project next.", "Note to self: Warnings to fix in the future\nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at the same time.\nBoth libraries are known to be incompatible and this can cause random crashes or\ndeadlocks on Linux when loaded in the same Python program. Using threadpoolctl may\ncause crashes or deadlocks. For more information and possible workarounds, please\nsee https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\nwarnings.warn(msg, RuntimeWarning)\n[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\nNeed to download onnxruntime from source on raspberry pi:\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html", "Update\nSummary of video: (At the end of the video I show how this looks in action)\nEvery 1/30th of a second a frame enters the system. It gets added to the video_queue.\nIf the time it takes to process frames is longer than 1/30th of a second, a backlog of frames will begin\nbuilding up. Since frames take up a lot of memory, this eventually leads to a zsh: hardware killed error.\nIn slide, Note to self: Warnings to fix in the future, there is an NNPack error. This error is causing\nMobileNet to run slower than it normally should. This causes a backlog.\nThus, the solution is that whenever there is a backlog to not run inference. Only when the video_queue is\nrecent and up to date, will we run inference.\nAnyway, in order to make real time adjustments to the tracker we want\nthe extrusion class of the most recently extruded material.", "Performance Metrics\n- Measure Accuracy:\nLabel video manually, and compare systems labels.\n- Speed:\nSee fps of classification.\n- RAM:\nGraph RAM usage throughout time", "Time Measurements\nGmms: 0.01 s\nMobilenetvsmall run: 0.05713069438934326\nLast time:\nmobilenet_v3_small: 0.022805473804473876", "LLM Research Papers I will read\nFlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering\nGalactica: A Large Language Model for Science\nOpportunities and challenges of ChatGPT for design knowledge management\nChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future\nDirections Towards Knowledge Graph Chatbots\nITERATIVE ZERO-SHOT LLM PROMPTING FOR KNOWLEDGE GRAPH CONSTRUCTION\nLLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT\nLarge Language Models Struggle to Learn Long-Tail Knowledge", "Raspberry Pi 4 Issue\nI wrote the code to receive signals and video from the Instant360. One issue\nhowever:\nWhen I try to download the opencv-python package, because it needs to be\ninstalled from source it can take anywhere from 30 minutes to multiple hours. I\nbegan installation, but the raspberry pi eventually became burning to the touch.\nUpon googling, I found this could damage the Raspberry Pi.\nApparently, for high load tasks we should use a heat sink or cooling fan for the\nRaspberry Pi.", "8/4-8/11\n\u25cf Onboard Sahil", "Repositories\nhttps://github.com/BrianP8701/STREAM.AI\nhttps://github.com/BrianP8701/Anomaly_Classification\nHere are the repositories again in their current version. I added the short report\nand a video presentation (Same as the one from zoom) to the README.\nI made setup easy with a conda environment and environment.yml.", "Anomaly Classification Coding Interface\nI made using the Anomaly Classification repository", "Onboarding Sahil\nI assigned Sahil his first task. To improve the MobileNet performance alongside\nthe tip tracking algorithm (One of the metrics for the \u201cmetric driven research\u201d). I\nshared with him all the google drives, and recorded a video for him detailing his\ntask and how to use the code:\nhttps://youtu.be/L-UFGaSTrxw", "7/28-8/4\n\u25cf Github documentation\n\u25cf Complete report\n\u25cf Finish deploying to Raspberry Pi\n\u25cf Gather metrics and demo videos", "7/21-7/28\n\u25cf Why do small models perform better?\n\u25cf Finish implementation!!!\n\u25cf Deploy on Raspberry Pi", "You mentioned last meeting my interpretation of the G-Code by the SKR board might be different than what I had\nimplemented. I did further investigating online and it seems that my implementation is correct and the potential\nreason for the acceleration errors is the following:\nPrinter Firmware Overrides: The firmware of many 3D printers has built-in limits for acceleration, jerk, and max speed.\nEven if the G-code requests a certain speed or acceleration, the firmware might cap it to a pre-defined limit. This is\nespecially true if the set values in the firmware are deemed safer or more reliable for the printer's hardware.\nG-code Acceleration Commands: While most of the time acceleration is defined in the firmware, some G-code might\ncontain commands to set acceleration for specific sections of the print. These commands can override the default\nacceleration values. Look out for M204 (set acceleration) and M205 (set jerk) commands in the G-code.\nPhysical Limitations: Sometimes, even if both the firmware and the G-code allow for a certain acceleration, the\nphysical characteristics of the printer (like mass of moving parts, motor capabilities, belt tension, etc.) might prevent it\nfrom reaching those values. For example, a printer with a heavy tool head might not accelerate as quickly as a lighter\none, even if the firmware settings are the same.\nSegmented Moves: Some slicers break down curves and diagonal moves into many tiny straight-line segments.\nWhen the printer processes these segments, it might not achieve the desired acceleration due to the frequent\ndirection changes.\nStart and Stop Points: The printer might be decelerating as it approaches the end of a line segment or the start of a\nnew one, especially if there's a change in direction. This can make the effective acceleration appear different from the\nset value.", "Kinematics Problem\nWe use the 4 kinematic equations to predict the movement of the tip of the tracker solely based off the\ngcode, which provides coordinates and speeds. At each line of gcode, we predict the time it takes to finish\nthat move. Without getting to deep into details, there is one scenario that cannot be solved analytically, but\nrather needs to be solved using numerical analysis or iterative methods.\nThe problem is as follows: Given a positive distance, acceleration, initial speed and final speed: Find the\nfastest time which you can travel the distance.\nSince we know the distance, I thought of the following iterative method:\nIf curr_speed equals final_speed, we split the distance in half.\nIf curr_speed is less than final_speed, we accelerate for more than half the distance.\nIf curr_speed is greater than final_speed, we accelerate for less than half the distance.\nWhat we can do is first try accelerating for the first half of the distance, then decelerating for the second\nhalf. Then we iterate the distance by which we accelerate to get closer to the final speed.\nWe iterate by 10% of the distance 10 times, then 1% of the distance 10 times, then 0.1% of the distance\n10 times. With this method, every 10 iterations results in an order of magnitude improvement in accuracy.\nThis is very computationally cheap.", "Spatial Error Implemented\n1. Run YOLO to compare where we think tip is to where it actually is. Add spatial\noffset to a list.\n2. When we have enough spatial offsets, make sure the spatial offsets are close\nenough together by finding the range. Then, make sure in the y case, that the\naverage spatial offset is large enough to be relevant.\n3. If conditions are met, make a spatial correction. Clear the previous list of\nspatial errors.\nI tested this by purposely introducing some spatial errors, and it proved to work.", "Temporal Graphs\nWhy are there two lines in the temporal offsets graph? Why does it increase, and then decrease?\nOkay, the two lines were an implementation error on my part. However, the \u201ctwo lines\u201d, or parts of the\ngraph with different slopes is correct. The first phase is that part of the print where it is going around and\nmaking some curves, and the second is the making of the cube in the center. This makes it clear that, the\nacceleration at each of these times are different.\nTemporal Offsets (frames) Slopes of Temporal Offsets Stdevs of Slopes\nvs. Time (frames) vs. Time (frames) vs. Time (frames)", "Deploying to Edge", "7/14-7/21\n\u25cf Why do small models perform better?\n\u25cf Achieve acceptable performance on bubble classification models\n\u25cf Finish implementation\n\u25cf Run on video locally\n\u25cf Contact Dr. Zhou for real signal implementation\n\u25cf Implement real video stream\n\u25cf Deploy to edge", "", "", "Interesting Details on Implementation\nThe main thread and all other threads run. I\u2019m not achieving true parallelism,\nmerely concurrency. Any processing of video frames must not hinder the rest of\nthe process, meaning it needs to take less than 1/30th of a second. I found\ndisplaying and saving the image could be very computationally expensive, with\ndisplaying being worst. I found a simple workaround: reduce resolution or frame\nrate of display.", "Yolo Update\nI found that the YOLO model was not accurate enough. The previous tip detection\ndataset was trained on images from the endoscope camera. I\u2019ve trained a new\nmodel, with 5,385 images (Including old data from previous models, and new\nones). A problem was that it sometimes found a tip when there was no tip. So I\nmade sure to add some null images in this dataset.", "Bubble Model\nBubble: 184 images | No Bubble: 184 images\nImages were cropped at 85x85. I resized all images to 224x224.\nI also created an additional dataset, applying the Gaussian Mixture Model +\nSensitivity filter.\nFor each class I created new images for each class using rotations, adjusting\nbrightness, saturation, contrast, hue, horizontal and vertical flips. I did 40\naugmentations per class.\nI split each dataset into 75% training set, 15% validation set and 10% testing\nset.", "Training\nGiven that we have found what parameters worked best, I followed the same:\nBatch size = 5\nResize to 224x224\nApply Gaussian Mixture Model Preprocessing\nI fine tuned a couple MobileNetv3 models, and immediately got performances\nabove 90%. My question now is, looking at the data, it looks like \u201cbubble\u201d is the\nsame as under extrusion. Before moving forward, is this acceptable?", "Yolo run: 0.22236160755157472", "7/3-7/14\n\u25cf Achieved acceptable performance on classification model\n\u25cf Implemented real time, synchronized, self-correcting parser & tracker.\n\u25cf Integrated classification model to check for over or under extrusion.\n\u25cf Prepare to deploy to Raspberry Pi & use TinyYOLO", "New Set of Models\nDifferences:\n\u25cf More data\n\u25cf Only did fine tuning\n\u25cf Only used MobileNet\n\u25cf Only used images 224x224", "Dataset descriptions\nUnder: 436 images | Normal: 463 images | Over: 446 images\nImages were cropped at 85x85. I resized all images to 224x224.\nI also created an additional dataset, applying the Gaussian Mixture Model +\nSensitivity filter.\nFor each class I created new images for each class using rotations, adjusting\nbrightness, saturation, contrast, hue, horizontal and vertical flips. I tried doing 60\naugmentations and 150 augmentations per class\nI split each dataset into 75% training set, 15% validation set and 10% testing\nset.", "Data for new models\nBatch size = 3 Batch size = 4 Batch size = 5\nBatch size = 2\nAugmentations per class = 150 Augmentations per class = 60 Augmentations per class = 60\nAugmentations per class = 60", "6/26-7/3\n\u25cf Measure inference metrics for each model\n\u25cf Train next batch of models. Aim for 85% accuracy on test set\n\u25cf Update error detection and processing to work with corner updates\n\u25cf Collect all synchronization variables\n\u25cf Automate synchronization\n\u25cf Identify exact correction equation\n\u25cf Implement error correction", "Model Inference Efficiency\nI ran each model a 1000 times, and measured the average time in seconds:\n\u25cf resnet18: 0.05358100652694702\n\u25cf resnet152: 0.41191160917282105\n\u25cf mobilenet_v3_small: 0.022805473804473876\n\u25cf mobilenet_v3_large: 0.05056007623672485\n\u25cf efficientnet_v2_s: 0.21759283781051636\n\u25cf efficientnet_v2_l: 0.6030457210540772\nMobileNets are the fastest for inference, whereas EfficientNets take the longest.", "", "More Detailed View of Initialization\n& Synchronization Phase\nI\u2019ve finished developing this\nphase. I\u2019ve made it so that the\nvideo and signals get\nstreamed in real time already\nto test locally. Some changes\nstill need to be made however:\n\u25cf Link to Instant360\n\u25cf Configure to work with\nactual signal in real time\nOnce this is done (and I finish\ncoding the next phase) this\nalgorithm will be ready to\ndeploy in real time.", "", "New Set of Models\nDifferences:\n\u25cf Added test set\n\u25cf More data\n\u25cf More data augmentations\n\u25cf More epochs\nSee more detailed information on next slide.", "Dataset descriptions\nUnder: 247 images | Normal: 255 images | Over: 246 images\nImages were cropped at 50x50. I made additional datasets with 224x224 images\nbe resizing. I want to compare the performance of models trained on 50x50 vs\n224x224 sized images.\nI also created an additional dataset, taking the 50x50 images and 224x224 images\nand applying the Gaussian Mixture Model + Sensitivity filter.\nFor each class I created 30 new images for each class using rotations, and\nadjusting brightness, saturation, contrast and hue. I created 30 new images using\nhorizontal and vertical flips.\nI split each dataset into 70% training set, 20% validation set and 10% testing\nset.", "Training Comparisons\nI measured the means for different attributes\nacross my new set of trained models.\nComparing transfer learning to fine tuning:\ntest: ['transfer', 0.5094] vs ['finetune', 0.6043]\nval: [\u2018transfer', 0.5299] vs ['finetune', 0.6091]\nFine tuning is clearly better. Comparing MobileNet to ResNet:\nComparing MobileV3Small to MobileV3Large: test: ['mob', 0.5887] vs ['res', 0.5443]]\nval: ['mob', 0.5952] vs ['res', 0.5658]]\ntest: ['mob_s', 0.5726] vs [\u2018mob_l', 0.6047]\nMobileNet might be better.\nval: ['mob_s', 0.5866] vs ['mob_l', 0.6038]\nThe larger model seems to be better.\nComparing ResNet18 to ResNet152:\ntest: ['res18', 0.4972] vs ['res152', 0.5472]\nval: ['res18', 0.5223] vs ['res152', 0.5605]\nThe larger model seems to be better.", "Data Comparisons\nI tried applying (gmms to images sized 50) and\n(gmms to images sized 224, then resized to 50).\nComparing 50-50 to 224-50 : Comparing with and without GMMS (50x50):\ntest: ['50-50', 0.4800] vs ['224-50', 0.5153]\ntest: ['original', 0.5248] vs ['GMMS', 0.4976]\nval: ['50-50', 0.5151] vs ['224-50', 0.4944]\nval: ['original', 0.5215] vs ['GMMS', 0.5048]\nWhen to apply gmms, does not seem to matter.\nGMMS seems to be bad.\nComparing 50x50 to 224x224:\nComparing with and without GMMS (224x224):\ntest: ['50', 0.5067] vs ['224', 0.6340]\ntest: ['resize', 0.5997] vs ['GMMS', 0.6639]\nval: ['50', 0.5104] vs ['224', 0.6616]\nval: ['resize', 0.6397] vs ['GMMS', 0.6807]\nImages sized 224x224 are clearly better.\nGMMS seems to be good.", "Best Methods\nMethods that are clearly better:\n1. Use 224x224 images.\n2. Use fine tuning.\n3. Train for around 15 epochs.\nI lost the metrics for this, but I trained a set of models earlier this week. I trained 42\nmodels, each for 40 epochs. It was clear across each model that improvements\nstagnated at around 8-10 epochs.", "Interesting Data Points\nBest Model:\nmob_s_resize_finetune: [test_accuracy: 0.7925, val_accuracy: 0.8025]\nWorst Model:\nres18_gmms_actually50_finetune: [test_accuracy: 0.3868, val_accuracy: 0.4268]\nMoving forward:\nUsing what we\u2019ve found to be the best methods, let\u2019s train more models. I think if I\njust collect some more data we will be able to achieve above 85% accuracy.\nSee what images the models struggle on. Reshuffle data.", "6/19-6/26\n\u25cf Collect more data\n\u25cf Make second round of datasets\n\u25cf Analyze metrics of models\n\u25cf Use GPU servers to train all models\n\u25cf Write inference code\n\u25cf Debug tracker\n\u25cf Refactor error detection and error processing code", "The process of collecting data\nLast time I collected data, I saved every\nframe from a video as a .jpg file on my\nSSD, manually identified the frames\nrelevant to each class, and looped through\nthe folders to retrieve these frames. This\nmethod was time-consuming and\ninefficient.\nTo streamline the process, I developed a\nnew graphical user interface (GUI). This\nGUI, implemented in the\nextract_frames_gui.py script, allows for a\nquicker review of a video and easier\nextraction of frames associated with each\nclass. This has significantly reduced the\ntime required for data collection.\nI also added several new methods:\nI faced some organizational challenges and my code\nrecursively remove duplicate images from\noverwrote several images, leading to confusion. As a\nfolder, reorder and format folder properly\nresult, I had to collect the data from scratch, a task that\nand combining multiple folders containing\nconsumed most of the day. I\u2019ve now more than doubled\nclasses into one folder.\nthe entire dataset.", "extract_frames_gui.py\nHere is a screenshot of the\ndescription of the tool I\nimplemented, that I mentioned last\nslide.", "ResNet\nResNet models are deep convolutional neural networks known for \"skip\nconnections,\" allowing gradients to flow through the network directly, mitigating the\nproblem of vanishing gradients, especially in very deep networks. This makes\nResNet models capable of being trained deeply. The numbers associated with\nResNet models indicate the depth of the network - ResNet18 has 18 layers, while\nResNet152 has 152 layers. The deeper the network, the better it can learn\ncomplex patterns, but at the cost of increased computational cost and risk of\noverfitting.\nResNet18: 42.72 MB\nResNet152: 222.7 MB", "MobileNet\nThe MobileNet architecture is made to be small and efficient, with a focus on mobile and\nembedded vision applications. It introduces depthwise separable convolutions to reduce\ncomputational complexity and model size.\nWith depthwise separable convolutions, rather than applying 3d filters (depth, width, length) to\nthe image, we initially apply \u201cdepthwise\u201d 2D matrices in each channel. Thereafter, a 1x1x3\n\u201cpointwise\u201d filter combining the channels. This is significantly cheaper (This specific operation is\nnow linear with respect to input size, as opposed to exponential).\nMobileNetV3 is very cool, introducing additional enhancements: Neural Architecture Search and\nNetAdapt algorithm. Instead of the researchers tweaking hyperparameters to find the best\narchitecture, they created an algorithm that would discover the best architecture for them.\nNetAdapt allows the model to \u201cautomatically adapt\u201d to any mobile device, which seems really\ncomplicated, and I didn\u2019t look into it (yet).\n'Small' and 'Large' refer to versions of the model with fewer or more parameters respectively,\nagain balancing between computational efficiency and model capacity.\nMobileNetv3small: 5.93 MB\nMobileNetv3large: 16.24 MB", "EfficientNet\nEfficientNets are made to provide a good trade-off between accuracy and computational\nefficiency. They introduce a new scaling method that uniformly scales all dimensions of\ndepth(number of layers, width(number of neurons per layer), resolution(size of input) of\nthe network. Instead of scaling these individually, they\u2019ve found some way to scale them\naltogether in a better way.\nThe authors first developed a baseline model called EfficientNet-B0. Then they used their\nuniform scaling method to systematically increase the depth, width, and resolution of this\nmodel, resulting in a family of models from EfficientNet-B1 to EfficientNet-B7, each\noffering higher accuracy but also more complexity.\nThe versions like 'v2-small' and 'v2-large' have to do with the scaling factor applied to the\nbase EfficientNet model. Larger versions have more layers or larger layers.\nThe specifics behind why this work are really interesting, and I want to get deeper into it.\nEfficientNetV2small: 77.85 MB\nEfficientNetV2large: 449.72 MB", "Dataset descriptions\nBubble: 43 images | No Bubble: 50 images\nUnder: 60 images | Normal: 61 images | Over: 37 images\nImages were cropped at 50x50. I made additional datasets with 224x224 images\nbe resizing and padding. I want to compare the performance of models trained\non resized vs padded images.\nI also created an additional dataset, taking the 50x50 images and applying the\nGaussian Mixture Model + Sensitivity filter.\nFor each class I created 10 new images for each class using rotations, and\nadjusting brightness, saturation, contrast and hue.\nI split each dataset into 20% validation set and 80% training set.", "Training\nIn fine-tuning, we continue to train all weights on our dataset, as we normally do.\nIn transfer learning, we also start with a pre-trained model. But this time, we keep\nmost of the model exactly the same as it was. We only change the final part that\nmakes the decisions (the \"classification layer\"). This is the only part of the model\nthat gets to learn from our data. The rest of the model remains unchanged.\nWe split the training set into 3 batches each epoch. After going through all the\nimages (one epoch), we backpropagate the error. Then we check the model's\nperformance using the validation set - this doesn't affect the model's learning but\nlets us see how it's doing. During the training, we keep a copy of the\nbest-performing model according to its scores on the validation set. This\napproach, called model checkpointing, ensures we always have the best version\nof our model according to how it performs on the validation set.", "Measured metrics\nAccuracy is measured on the validation set. Errors from the validation set never\nactually get back propagated to the model. Rather, after each epoch we only\nchoose to continue to train whichever model has performed best on the validation\nset so far.\nThe other metrics are measured every epoch, for the train and validation set. This\nlets us see if the model is overfitting to the training set, while not performing on the\nvalidation set.\nEach trained model is labeled as:\nThe following slides follow this format:\nmodel_dataset_finetune\n1. Small models + Transfer Learning\nor\n2. Large models + Transfer Learning\nmodel_dataset_transfer\n3. Small models + Fine Tuning\n4. Large models + Fine Tuning Where model and dataset are abbreviated. I\nshow these abbreviations in the next slide.\nEach slide will show metrics for all datasets.", "Model Model abbreviation Dataset Dataset Description\nAbbreviation\nResNet18 res18\nbubble Bubble/No Bubble Dataset, in its original 50x50\nResNet152 res152\nsize.\nEfficientNetv2small eff_s bubble_pad Bubble/No Bubble Dataset, but with padding to\nmake it 224x224\nEfficientNetv2large eff_l\nbubble_resize Bubble/No Bubble Dataset, but resized to 224x224\nMobileNetv3small mob_s\nclassification Under/Normal/Over Dataset, in its original 50x50\nMobileNetv3large mob_l\nsize.\npad Under/Normal/Over Dataset, but with padding to\nI trained each model with each dataset, once make it 224x224\nwith fine tuning and once with transfer learning.\nresize Under/Normal/Over Dataset, but resized to\n224x224\nA total of 84 models.\ngmms6 Under/Normal/Over Dataset in its 50x50 size, but\nwith Gaussian Mixture Model + Sensitivity filter.", "Smaller sized models, trained using transfer learning\nResnet18 MobileNet Small EfficientNet Small", "Largest sized models, trained using transfer learning\nResnet152 MobileNet Large EfficientNet Large", "Smaller sized models, trained using fine tuning\nResnet18 MobileNet Small EfficientNet Small", "Largest sized models, trained using fine tuning\nResnet152 MobileNet Large EfficientNet Large", "Bubble/No Bubble\nTransfer vs finetune\nFinetune Accuracy Transfer Learning Accuracy\nUnder/Normal/Over\neff_s_bubble_finetune 0.7083 eff_s_bubble_transfer 0.625\nFinetune Accuracy Transfer Learning Accuracy eff_s_bubble_pad_finetune 0.79166 eff_s_bubble_pad_transfer 0.625\neff_s_bubble_resize_finetune 0.79166 eff_s_bubble_resize_transfer 0.8333\neff_s_classification_finetune 0.5 eff_s_classification_transfer 0.575 eff_l_bubble_finetune 0.583 eff_l_bubble_transfer 0.708\neff_s_gmms6_finetune 0.55 eff_s_gmms6_transfer 0.5 eff_l_bubble_pad_finetune 0.666 eff_l_bubble_pad_transfer 0.75\neff_s_pad_finetune 0.4 eff_s_pad_transfer 0.625 eff_l_bubble_resize_finetune 0.7083 eff_l_bubble_resize_transfer 0.7916\neff_s_resize_finetune 0.475 eff_s_resize_transfer 0.525 res18_bubble_finetune 0.79166 res18_bubble_transfer 0.7083\neff_l_classification_finetune 0.45 eff_l_classification_transfer 0.525 res18_bubble_pad_finetune 0.7083 res18_bubble_pad_transfer 0.666\neff_l_gmms6_finetune 0.525 eff_l_gmms6_transfer 0.5 res18_bubble_resize_finetune 0.833 res18_bubble_resize_transfer 0.625\neff_l_pad_finetune 0.475 eff_l_pad_transfer 0.475 res152_bubble_finetune 0.666 res152_bubble_transfer 0.7083\neff_l_resize_finetune 0.65 eff_l_resize_transfer 0.575 res152_bubble_pad_finetune 0.875 res152_bubble_pad_transfer 0.5833\nres18_classification_finetune 0.475 res18_classification_transfer 0.675 res152_bubble_resize_finetune 0.79166 res152_bubble_resize_transfer 0.75\nres18_gmms6_finetune 0.425 res18_gmms6_transfer 0.525 mob_s_bubble_finetune 0.5833 mob_s_bubble_transfer 0.625\nres18_pad_finetune 0.45 res18_pad_transfer 0.5 mob_s_bubble_pad_finetune 0.75 mob_s_bubble_pad_transfer 0.6666\nres18_resize_finetune 0.475 res18_resize_transfer 0.625 mob_s_bubble_resize_finetune 0.75 mob_s_bubble_resize_transfer 0.7083\nres152_classification_finetune 0.5 res152_classification_transfer 0.55 mob_l_bubble_finetune 0.666 mob_l_bubble_transfer 0.625\nres152_gmms6_finetune 0.625 res152_gmms6_transfer 0.525 mob_l_bubble_pad_finetune 0.79166 mob_l_bubble_pad_transfer 0.625\nres152_pad_finetune 0.45 res152_pad_transfer 0.525 mob_l_bubble_resize_finetune 0.7083 mob_l_bubble_resize_transfer 0.7916\nres152_resize_finetune 0.425 res152_resize_transfer 0.5\nmob_s_classification_finetune 0.45 mob_s_classification_transfer 0.375 Average Accuracy: 0.73148 Average Accuracy 0.68981\nmob_s_gmms6_finetune 0.525 mob_s_gmms6_transfer 0.575\nmob_s_pad_finetune 0.475 mob_s_pad_transfer 0.45\nmob_s_resize_finetune 0.425 mob_s_resize_transfer 0.55\nmob_l_classification_finetune 0.5 mob_l_classification_transfer 0.6 Here I have collected the accuracies for each model I\nmob_l_gmms6_finetune 0.45 mob_l_gmms6_transfer 0.525\nhave trained. We can take a look to see if transfer\nmob_l_pad_finetune 0.525 mob_l_pad_transfer 0.55\nmob_l_resize_finetune 0.675 mob_l_resize_transfer 0.575 learning vs fine tuning seem to be definitively better.\nAverage accuracy: 0.4947916666 Average accuracy: 0.5385416666\nIt would appear, that neither are better.", "Metric Analysis\nWith the current accuracy of around 50%, it seems like we're hitting a roadblock.\nNo specific preprocessing or fine-tuning method is proving to be superior. The\nstagnant performance metrics after 10 epochs might hint that our model has\nreached its capacity with the given data.\nThings to try:\n\u25cf Cross-validation\n\u25cf Collect more data\n\u25cf Refine dataset\nI now have added around 50 new images to each class, so each class now has\n104 images.", "Refining my dataset\nIn my earlier work, I put images into 'over' or 'under' groups even if they only showed\na tiny bit of over or under extrusion. Now, I'm going to change that. I'll only put an\nimage in the 'over' group if the material is really getting wider. I'll only put it in the\n'under' group if it's clearly getting thinner. Some pictures I had put in 'over' or 'under'\nbefore will now go into the 'normal' group.\nI used to label these two pictures showed overextrusion, but I've changed my mind.\nI'm either going to move them to the 'normal' group or take them out completely,\nwhichever keeps the groups balanced. I only had to get rid of 5 pictures in all, but I\nhope this will make our results more accurate.", "6/12 - 6/19\n\u25cf Train all classification models and provide metrics\n\u25cf Improve robustness of preprocessing step for low standard deviations\n\u25cf Optimize preprocessing step\n\u25cf Test virtual marker", "Training modules done\nI finished coding the training for models on Pytorch.\nBesides making a model for each dataset I mentioned earlier, I'll also\ntest fine tuning and transfer learning. Fine tuning starts with a\npre-existing model and trains the whole thing. Transfer learning also\nstarts with a pre-existing model, but only trains the last layer. Transfer\nlearning should work better because it avoids overfitting, but we'll\nhave to see.\nAlso, we need more data. I'll go through the new videos to get more.", "Purpose of Preprocessing\nOur preprocessing step does two things:\n1. It reduces noise and sharpens edges so we can see the most recently\nextruded material's edges and measure its diameter.\n2. It helps our trained models work better. Preprocessing can make a big\ndifference if you're dealing with small, low-quality, or noisy datasets. If the\nimages in the dataset have a lot of noise or if the objects are hard to see,\nreducing noise and sharpening edges can make the objects stand out more.", "Refresher of the current simplify algorithm\nThis slide is just to explain how the current preprocessing step works:\n1. Figure out the standard deviation of all the pixel intensities in the image. This tells us how much the\ncolors in the image vary.\n2. Using the standard deviation, we calculate a 'threshold'. This threshold decides which pixels are\nsimilar enough to group together. If the colors in the image are very different, the threshold will be\nlarger. If the colors are similar, the threshold will be smaller.\n3. If the standard deviation is below a certain point, we set the threshold to a fixed value.\n4. Find the brightest pixel in the image.\n5. Turn all pixels that fall within the threshold of the brightest pixel to maximum intensity, making them\nwhite.\n6. We adjust the range to include pixels that are slightly less bright, excluding the ones we just turned\nwhite. This is our new 'cutoff'.\n7. We repeat this process, finding the brightest pixel below our cutoff and adjusting all pixels within its\nthreshold to match its intensity. We do this until we've looked at and adjusted all pixels in the image.\nAfter all this, the image is simpler because similar pixels have the same intensity. This makes the edges\nstand out and reduces noise.", "Failings of the current preprocessing step\nWhen the image is more uniform in\nintensity, this algorithm can introduce more\nnoise.\nThis error shown to the right is clearly\nunacceptable. There bed is in the image,\nwhich leads the algorithm to calculate a\nhigher standard deviation. This leads the\nalgorithm to believe it doesn't need to be\nas sensitive, and thus ignores the faint\nline.", "How robust?\nLet's look at the images on the right. The edges\nare really faint. In the first image, the bottom line\nisn't even there.\nFirst, this means when we're measuring diameter,\nwe need to know when we can't get a\nmeasurement.\nSecond, for the last two images, the edges are\nthere but they're hard to see. Should we give up?\nOr can we figure out how to handle these images,\nespecially since they're common when layers\nbuild up?", "A question of balance\nWhat's more important - sharp edges or less noise? We need a balance. We have\nsome faint edges. Using the standard deviation doesn't always work well because\nthe bed or tip can be a dark color, which can make the algorithm unstable. We\ncould crop out the tip with accurate tip tracking, but the bed is still an issue.\nWe need to make sure we don't lose important information. Too much noise\nreduction can remove important details. Too much sharpening can highlight noise\nor make fake edges. But a bit of extra sharpening shouldn't hurt our diameter\nmeasurement algorithm.\nIn conclusion, we must prioritize avoiding relevant information loss.", "Cv2 and PIL Preprocessing\nNon-local Means Denoising algorithm from cv2\nUnsharp Mask from PIL\nI tried out the Non-local Means Denoising algorithm\nfrom cv2 and Unsharp Mask from PIL. They work\nalright after I played with the settings for a while. But\nwe still need something better to highlight faint edges.\nMaybe we can come up with a custom solution.", "New preprocessing step\nRight now, we treat each pixel as if it belongs to one\nnormal distribution. But maybe we can use a\nGaussian Mixture Model to separate our pixels into\nmultiple groups:\n\u25cf The tip\n\u25cf The tip's glow\nI remembered something from my textbook, 'Deep\n\u25cf The extruded material\nLearning'. It's about a thing called a Gaussian Mixture\n\u25cf Shadows\nModel. This model thinks data is made from different\n\u25cf Bed\ngroups. Each group has its own Gaussian distribution.\nThis is really helpful for understanding complex data. We want to pay more attention to the brighter colors,\nwhich are the extruded material, and less to the\nWe provide number of clusters we think there are to the\ndarker colors of the bed or tip.\nEM (Expectation-Maximization) algorithm, which\nprovides us the means and variations of each cluster. But a Gaussian Mixture Model doesn't let us do that.\nSo we need to find a workaround.\nCheck out the image above I found online to explain\nthis.", "Why we cannot use k-means\nK-means clusters data faster than a Gaussian Mixture Model and the EM\nalgorithm. But since our data isn't that big, the time difference doesn't really\nmatter. I ran both 1000 times and found that the Gaussian Mixture Model is\nactually quicker with our input.\nk-means \u2248 0.062 seconds\nGMM + EM \u2248 0.014 seconds\nAnother issue is that k-means expects clusters to be spherical with equal\nvariances. That's not the case for us. We also need to know the standard\ndeviations to break up our data into sections.", "Visualization\nThe x axis is pixel intensity, and the y axis is the count of how many pixels have each intensity", "Visualization 2\nThis time we find 2 clusters. Red: Mean. Blue: +/- Standard Deviation.", "Visualization 3\nThis time we find 3 clusters. Red: Mean. Blue: +/- Standard Deviation.", "Observation\n200-215 215-225 170-190 207-209 185-190 230-240 180-190 215-230 185-200\nI manually measured the intensity value of the extruded material. If you look back\nat the last slide, you'll see it matches the third mean and its range almost exactly.\nSo now we can approximate the color of the material.", "GMM Preprocessing Step\nParameters: image, num_components\nHere are the steps for the new method using the EM algorithm.\n1. Get means and variances for clusters.\n2. Create ranges around each mean using standard deviation.\n3. Fix gaps and overlaps in the ranges.\n4. Replace all colors in each range with the same color.\nThis works for any image and any number of components.", "GMM + Sensitivity Step\nParameters: image, num_components\nFor this method, we're focusing on the highest mean (The material):\n1. Get means and variances for three clusters.\n2. Select all data from the highest mean (plus or minus two standard deviations).\n3. Calculate means and variances for these clusters.\n4. Create ranges around each mean using standard deviation.\n5. Combine this with the lowest range from the first three clusters.\n6. Fix gaps and overlaps in the ranges.\n7. Replace all colors in each range with the same color.\nThis also works for any image and any number of components.", "GMMx: Gaussian Mixture Model Algorithm\nwith x components\nOriginal\nGMM8\nGMM12\nGMM16", "GMMSx: Gaussian Mixture Model +\nSensitivity with x components\nOriginal\nGMMS4\nGMMS6\nGMMS8", "Preprocessing Conclusion\nThe new method removes noise, highlights edges, and doesn't lose important info.\nIt even highlights faint edges that are hard to see. It's more sensitive to the\nmaterial, and it darkens the tip or bed.\nWe can use fewer clusters, which cuts down on noise and makes things faster.\nOverall, this method should help with training models for classification and\nmeasuring diameter.", "Colors are not important. Shape matters.\nHere are a few ways we can make the model focus on shapes, not colors:\n1. Use data augmentation techniques that change colors, like changing\nbrightness, saturation, or the color palette. This is because colors aren\u2019t\nimportant for our problem. The shape of the extruded material is what matters\nto us.\n2. Design the CNN to focus more on shapes. This could mean using larger\nconvolutional kernels or more pooling layers. That way, the model will pay\nmore attention to larger shapes and less attention to small color changes.", "Virtual Marker Test\nIn a comparison, the YOLO v8 model worked better than the virtual marker. I put\n30x30 markers randomly on frames with some noise, rotation, and light changes -\nbut still less than in real life. After running each model 1000 times, the YOLO v8\nmodel came out on top when looking at the averages.\nYolo v8 Tip Detection: 1920x1080 image \u2248 0.46 seconds\nVirtual Marker: 1920x1080 image, 20x20 marker \u2248 0.61 seconds\nIn a real scenario, there would be even more noise and camera variation.\nAdditionally, we haven\u2019t even pruned or implemented Tiny YOLO which would\nfurther improve the efficiency of the model.\nThus, we can safely continue with our original plan.", "6/5 - 6/12\n\u25cf Plan for what and how to train classification models for\nunderflow/overflow/normal/bubble. Train a variety of models.\n\u25cf Organize all trained models, and clearly show metrics and details explaining\neach.\n\u25cf Research and choose what type of marker would be best to use in our\nscenario.\n\u25cf Test virtual marker.\n\u25cf If virtual marker is computationally efficient enough to run 30 times a second\non raspberry pi, try in real world.\n\u25cf Clean up, update and add documentation to code. Push to github.\nReorganize, add dependencies, and error messages.", "Classification Models\nPre-Trained models for Transfer Learning I will use:\n1. MobileNetV2: Prioritizing efficiency\n2. ResNet50: Prioritizing accuracy\n3. EfficientNet: Balance of efficiency and accuracy\nTraining on the unzoomed image will not work, because the model will not be able\nto achieve good accuracy without overfitting given our limited dataset. So I will\ncrop around the tip at sizes of 50x50.\nThese models were trained on the ImageNet dataset, thus their convolutional\nfilters have learned features at that scale. At 224x224, these models should\nprovide better accuracy, but smaller inputs will provide better efficiency.\nI\u2019ll create a variety of datasets with different preprocessing steps to compare.\n\u25cf 50x50\n\u25cf 50x50 resized to 224x224\n\u25cf 50x50 with padding to 224x224", "Preparing datasets for classification models\nThe code can be found here. I\u2019ve split it into folders for image tools (image_utils), datasets\nand the files which train the models.\nFirst, I need to collect my data. I\u2019ll want to crop a 50x50 area near the tip covering the\nmaterial for each image in the dataset. To do this quickly, I created a tool to collect this\ndata, demonstrated in the video below. This is the file image_crop_gui.py\nI then created the preprocessing steps to resize and add padding. I am also taking the\npreprocessing step from my other project and will use that to create another dataset.\nPerhaps that preprocessing step, removing noise and sharpening edges will lead to better\nmodel performance. But for now I still need to make some upgrades to that algorithm.\nThis is all in the file preprocessing.py\nI made sure to add comments and make everything easy to understand and read.", "Training hyperparameters\nGiven our smaller dataset, we want a bigger batch size. A bigger batch size\nreduces variation as more data points are sampled at once, and will lead to better\nregularization. Overfitting is not an immense concern of ours. Additionally we want\nlearning rate decay, early stopping and data augmentation in the form of rotation.\nI\u2019ll visualize metrics in TensorBoard and choose the best performing model based\non validation set.\nhttps://pytorch.org/vision/stable/models.html\nNote: Things were going swimmingly. I had trained a MobileNetv2 model. But now\nwhen I try to import torch, I get the following error: zsh: segmentation fault\nIt gives no other details, and I\u2019m really struggling to get past this.", "Code improvements\n\u25cf Github: https://github.com/BrianP8701/STREAM.AI\nMy branch was behind 8 commits. My Github hasn\u2019t been updated since April 19th\nbecause I made some commits containing large files which couldn\u2019t be uploaded. I\nreset those commits, and uploaded what I have now.\n\u25cf Created Conda environment\n\u25cf Added requirements.txt with only necessary dependencies\n\u25cf Replaced all complex logic in main algorithms with helper functions. You can\nnow read through the main algorithm very easily as it only consists of\ninitialization variables and readable functions and names.\n\u25cf Added more comments and restructured code", "Pose Detection\nAprilTags and AruCo markers are the state of the art markers for pose detection\ntoday. AprilTags are more robust, which is what we need as the camera will not\nalways be positioned the same and there will be lighting variation.\nI will do an experiment with a virtual tag first to measure the computational\nefficiency. For this, I\u2019ll simply place the tag on the image at a realistic scale, and\nmeasure the efficiency. If it is not more efficient virtually, it surely will not be more\nefficient in real life with noise.\nI will compare to the Yolo v8 Tiny model for comparison.", "4/14 - 4/21\n\u274f Temporal Correction", "Standard Horizontal Pre Vertical\nInference Cap Each instance only takes one Each instance we have to\ninference, so we can have a backtrack and do more\nlower cap. inferences, so set a higher cap.\nReliability Unreliable, due to inherent Near perfect metric for temporal\nvariation. offset.\nAs noted above, the prevertical is a much better way of measuring the temporal error.\nYet we still want to develop the standard horizontal method, as we will find that not all\nprints will contain those specific movements. I have included two constants, that can\neasily be adjusted to choose how much of either one you want, which can be chosen\nas needed for whatever specific print is being done.", "Implementation Details: Conclusive vs Inconclusive State\nThis state merely represents:\n\u25cf Inconclusive: We don\u2019t know if we are ahead or behind\n\u25cf Conclusive: We know if we are behind\nBased off whether we are in a conclusive or inconclusive state determines how we\nbehave in certain situations.\nFor example, if we are in an inconclusive state we cannot find the temporal offset\nduring a pre-vertical move.", "Figure 1:\nFigure 1 Figure 2\nBlue (Standard Horizontal temporal offsets)\nRed (Pre-vertical temporal offsets)\nThis graph shows all of our data collected without removing\nanomalies. In the standard horizontal when the parser falls\nsignificantly behind the offsets go bonkers. The prevertical on\nthe other hand is more steady.\nFigure 2:\nBoth images above have an initial\nRed (Standard Horizontal and pre-vertical temporal offsets)\ntime_k of 0.99. Here we are simply\nrunning our parser and measuring the\nThis graph is the same as figure 1, expect that the anomalies temporal offsets without taking any\nare removed. actions.\nFigure 3 Figure 4\nFigure 3 & 4:\nBlue (Standard Horizontal and pre-vertical temporal offsets)\nHere I just display two more graphs displaying the temporal\noffsets over time for two different time_k values. As we can\nsee, the slope appears to correspond to how much we should\nchange time_k by, as it tells us the rate at which the temporal\noffset is changing.\nTime_k = 0.98 Time_k = 0.995", "Initial Slope Time_k\ntime_k correction\n0.97 0.01548156275287975 +0.015\n0.975 0.004778425833443505 +0.01\n0.98 0.0028197907793473874 +0.005\n0.983 0.0017968711589403047 +0.002\nCollecting more data to see what the slope of the error vs\n0.984 0.0016009852751633009 +0.001\nframes is for different initial time constants. Next, we want\nto do this across multiple videos and find a consistent way\n0.985 0.00014960637206662498 0\nto convert that slope into a correction.\n0.986 0.00014960637206662498 -0.001\n0.987 -0.0007433997167368042 -0.002\n0.99 -0.011362640420807129 -0.005\n0.992 -0.011970263220423117 -0.007\n0.995 -0.013227478738805885 -0.01", "4/7-4/14\n\u274f Temporal Correction", "Overview of Temporal Correction\nTo clarify, we are no longer seeking to correct spatial error. We are trying to see if\nthe parser is behind or ahead temporally.\nWithin the gparser, we now must work with the true angles. Further processing\nand adjustment may be performed in measurement.\nWe must catch the error before it gets so bad we can\u2019t fix it.\nEven in the best tracking, there is spatial error. We must only make decisions if the\ntracker is consistently ahead or behind.", "Terminology Clarification\nOffset, Spatial Error, Spatial Offset: The physical distance by which the tracker is\noff from the actual location.\nSampling Inference: Performing inference on the image using Yolo v8.\nTime_k, acceleration constant: This is a mistake on my part sorry. In my code the\nline which contains the unknown constant is:\ntime_for_move = how_long(abs(magnitude(position_vector)), abs(magnitude(curr_velocity_vector)),\nabs(final_speed), abs(max_speed), ACCELERATION) * time_k\nThere are two values that can be adjusted, ACCELERATION and time_k. Both\nhave the same effect, and I will be leaving ACCELERATION constant and\nadjusting time_k. So from now on I will say time_k, but whenever in previous\nslides I had said acceleration, substitute in time_k.", "Terminology Clarification\nTip tracker: When I say tip tracker, I am specifically referring to the tiny box that\nattempts to follow the tip around.\nTemporal Error: My code parses through the gcode at a certain pace, and can fall\nahead or behind. Temporally ahead or behind refers to the fact that my tip tracker\nis not aligned temporally with the real print.\nTime Travel: Referring to when my tip tracker is ahead or behind, not making a\nspatial correction but rather moving forward or backwards some frames to correct.", "Implementation Details of Temporal Correction\nInstead of sampling inferences at given intervals, we will select our inferences\nbased off the gcode now. When the printer is moving horizontally we will take an\ninference. We can make the following logic:\nOffset to left Offset to right\nMoving left Temporally ahead Temporally behind\nMoving right Temporally behind Temporally ahead\nMoving horizontally at an angle is acceptable. So in the middle of that line of\ngcode, we can take an inference and tell if we are temporally ahead or behind. We\nwill place a cap at a maximum inference rate of 60 frames, so we won\u2019t\noversample.", "Implementation Details of Temporal Correction\nWe will maintain and build a list of offsets and their corresponding frames as we take inferences.\n(-) behind, (+) ahead\nExample, where we are clearly ahead: [0,0,-1,0,1,3,2,5\u2026] offsets\n[5,137,256,678\u2026] frames\nWe can now plot this to see how the offset looks as the video goes by. No correction is being made\nfor now, just measurements. The correct time_k value should be 0.985, and here we can see the\noffset for two different values of time_k throughout the video. Positive offset means the tracker is\nahead, and vice versa for negative.\ntime_k = 0.98 time_k = 0.99\nHere we can see the tracker\nHere we can see the tracker is getting behind, but around\nis clearly getting steadily 4000 frames, the tracker falls\nahead throughout the entire so much behind that our\nvideo, with a couple outliers. logic falls apart. We want to\nmake corrections far before\nthat point.", "Removing Anomalies\nWe first calculate the best-fit line using least squares. Then, calculate residuals.\nNext, compute the Z-scores for residuals and filters out the points with Z-scores\ngreater than the specified threshold. This approach considers anomalies based on\ntheir deviation from the best-fitting line, rather than the mean of the data. I chose\nto use a threshold of 3.\nThreshold = 2 Threshold = 3 Threshold = 4", "Measured offsets vs time, and the slope of Measured offsets vs time, and the slope of\nthe best fit line vs time the best fit line vs time\nwithout removing anomalies. removing anomalies.\nWe can clearly see that, that the best fit line will go Here, the slope eventually settles on a value. We\nbonkers if we do not remove anomalies. can use this to tell us how much to change the\ntime_k constant by.\nNow, you might ask, how do we know when to\nmake a correction? When the standard deviation\nof our slope is clearly ahead or behind we can\nconfidently make a decision. However, as we will\nsee later, this is not sufficient.", "Implementation Details of Temporal Correction\nGiven our offsets and time, we can calculate the slope and standard deviation of\nthe best fit line.\nThe slope tells us: The magnitude of the temporal error\nSlope Time_k correction\n- -\n+ +\nThe standard deviation of the slope tells us: Our confidence in this decision. It\nappears a value of 0.003 or 0.002 tells us we are very confident", "Issues\n1) It takes too long for the slope to converge to a value and for the standard\ndeviation of the best fit line to settle.\n2) I mentioned last week that we want to over accelerate then decelerate to fix\nthe temporal error. I changed my mind, that introduces too much complexity.\n3) In the previous slides, I was proposing that we change the slope based on the\nmagnitude of the offset. However, these current prints are very simple,\nconsisting of lots of straight horizontal and vertical lines. If we were to move\nmore diagonally, the magnitude of the spatial offset would seem less then it\nreally is. Additionally, the speed of the tip contributes to this. If the tip is\nmoving very fast, we might see a large offset, even if the temporal error is\nsmall. Thus, we must consider the angle and speed of the tip in certain\nsituations when measuring offset.", "Further,\nThe entire program overall is still exceedingly fast, finishing the entire 5 minute video in\naround 15 seconds not including measurement. (Measuring diameter is quite\ninefficient, but I will do optimizations on that algorithm later).\nMy approach now is to try to find more places we can do inference so we can more\nquickly, and confidently conclude whether we are ahead or behind.\nMy issue with this, is that it doesn't make this algorithm robust with respect to the\nvariety of possible gcode. What if a print doesn't have these types of moves? Are there\nother possible movements that can be valuable? In following slides, I will attempt to\nconcisely categorize and define types of movements and the logic for determining\nwhen they occur and whether they are ahead or behind.", "Standard horizontal moves\nI already discussed these, but I will reiterate with the new changes. We want to\nstandardize all of these regardless of speed or angle. So we can simply:\nDividing by the x component of the velocity accounts for the angle and the speed,\ngiving us a consistent view of the offsets. With this addition, we do the same thing\nas I mentioned previously. Additionally, to be conservative and given the inherent\noffsets during print, we will only consider horizontal within the range of 60 degrees\nin either direction. The only change here, is that offsets should be more accurate\nand more robust when we have gcode that has more diagonal moves.", "Quick horizontal move, followed by vertical move\nIn this particular print, there are some very quick movements the tip makes\noccasionally. These are so fast, they sometimes take one frame and my algorithm\ndoesn't do inference on them. However, these are very valuable, yet the logic for\ndetermining whether we are temporally ahead or behind changes.\nAt start of move At end of move\nTracker is ahead Spatially correct Spatial error\nTracker is behind Spatial error Spatially correct\nIt is important to note, that the magnitude of the offset is not relevant in this case.\nMerely seeing if there is an offset is all we can do. Now based off repeated instances of\nan error we can make a change, but we cannot determine the magnitude of this\nchange.", "Quick horizontal move, followed by vertical move\nWe know the moment when we start this move from the gcode. To find the\ntemporal offset, we need to backtrack and see exactly when this spatial offset\nbegan. To achieve this the first time we do it, we need to brute force and check\nprevious frames one by one until we find the moment the spatial offset begins.\nOnce we begin collecting more data we can make a more informed decision.\nTemporally behind Temporally ahead\nParser move When spatial offset ends When spatial offset begins\nReal move When spatial offset begins When spatial offset ends", "Time Travel\nNow, to address the issue of having to \u201cover accelerate then decelerate.\u201d That is\nunnecessarily complex. Rather than that, we want to change the time_k constant\nto what we believe is truly correct. Then, let\u2019s \u201ctime travel\u201d, meaning move\nforwards or backwards some frames, removing the need to over accelerate then\ndecelerate. The question now is how many frames to \u201ctime travel\u201d?\nAdditionally, we cannot actually go back in time as we assume this will run in real\ntime. We will use a method I read about in my Distributed Systems textbook for\nsynchronization, using \u201cleap\u201d frames or \u201cbuffer\u201d frames. If we want to go ahead in\ntime we add skip some frames in our future moves, and we add extra frames if we\nwant to slow down. The details of when this will be performed will be detailed later.", "Total View, Overall Consistency\nWe cannot assume that we can make the exact perfect temporal and time travel\ncorrection the first time. We want to maintain a total view (A word I made up). I\nmean we want to keep track of all the previous data ([frames, offsets], [frames,\nahead/behind]) we\u2019ve collected and what actions (temporal corrections, time\ntravel) we\u2019ve taken.\nNow, we will make a decision as early as we can, but as we gather more data later\ninto the video and save it, we can use the entirety of all our data to eventually\ncome closer to reaching a definitive and final conclusion.\nAn important decision is that we will run two different parsers now. One which is\nthe original with none of our actions performed, alongside our \u201ccorrected\u201d one. We\nwant the original parser so we can maintain the same temporal vs time graph and\ncontinue to build confidence, rather than start all over every time we make an\naction.", "frames: [...] int We use this data with the least squares\noffsets: [...] int method to determine the magnitude of the\nFrames and their corresponding offset from standard change we need to make to time_k.\nhorizontal moves\nframes: [...] int\nahead/behind: [...] boolean When this data shows consistency we can\ntemporal_offset: [...] int make changes to time_k and confident time\nFrames and their corresponding values from quick travel decisions.\nhorizontal -> vertical\nframes: [...] int This data can be useful when we want to\ntime_k adjustment: [...] float\n# of time travel frames: [...] int\nActions taken", "Acceptable Errors\nThere is inevitably a little error even in the best of runs. Our data shows that these are acceptable errors.\nStandard Horizontal Quick Horizontal -> Vertical\nSpatial 20 pixels 8 pixels\nTemporal N/A 2 frames", "4/1 - 4/7\n\u274f Run correction purely for acceleration adjustment\n\u274f Refactor code", "Spatial Correction\nSpatial correction has been solved. Inference rate minimum is 5 seconds, and\nmaximum 10 seconds. It starts at 5 seconds and can increase or decrease based\non whether there are errors.\nI maintain a stack data structure that keeps track of the offsets in the x direction\n(difference between yolo v8 prediction and my calculated location of the tip).\nSometimes, inferences don\u2019t meet the confidence minimum, so we simply ignore\nthose instances. Whenever the stack has two values, I will remove both and see if\nthey are offset in the same direction. If so, I will make a spatial correction that is\nthe average of both offsets.", "Temporal Correction\nSome things to note before talking about temporal correction:\n\u25cf Spatial corrections don\u2019t fix temporal errors\n\u25cf When making temporal decisions, do not account for the corrections made\nspatially. Keep track of all spatial corrections made, and remove it from\ntemporal offsets.\n\u25cf To catch up we need to over accelerate and then decelerate once we\u2019ve\ncaught up (Vice versa to slow down). We need to avoid oscillating back and\nforth between acceleration values and settle on a value.", "Temporal Correction\nWhen the parser begins we assume that it is starting correctly. In other words, in\nthe start the tip tracker is correct. But slowly throughout the video the tracker will\nbegin to deviate from the real location if the acceleration value is off. So we want\nto always be on high alert and make small corrections to the acceleration value as\nwe see fit.\nLet\u2019s ignore the spatial corrections, and look at my actual tip tracker prediction vs\nthe yolo v8 inference when correcting the temporal error. Let\u2019s not look at the\nmagnitude of the error, but the rate and magnitude of error changing over time.\nWe will be conservative in changing the acceleration value, but if it is significantly\nwrong, we will make larger corrections.", "Temporal Correction\nImplementation Details:\nBased off bed angle and direction of offset, we can tell if the tracker is behind or\nahead. Similarly to spatial correction we maintain a stack. One stack contains\noffsets, but here ignoring the corrections. Another stack maintains a boolean\nsaying whether we were ahead or behind\nAfter 4 inferences, if the tracker is consecutively ahead or behind we will make a\nmodification to the acceleration value.\nWe simply make a very small adjustment to the value.", "3/24 - 4/1\n\u274f Reduce cropping size\n\u274f Adaptively adjust inference rate and acceleration constant based on error\n\u274f Run multiple inferences at a time\n\u274f Increase minimum confidence for inference\n\u274f Try slightly adjusting angle for more robustness in measure_diameter\nalgorithm", "Gparser autocorrect problem\nI\u2019ve found that through increasing the minimum confidence of inference to 0.5, the inference is\nliterally perfect.\nYet, the autocorrect still jumps around. Much less, but still unacceptable.\nI\u2019ve concluded there can only be one cause of this bug:\nWhen the tip moves really fast from one spot to another,\nMy parser is not perfectly temporally aligned with the video,\nSo when running an inference at those moments, it looks like the parser needs to be\ncorrected\nHowever, a distinction must be made between spatial and temporal errors.\nBy spatial I mean my tip tracker is off by some distance. Temporal error refers to my parser running\nbehind the actual video in time.", "Gparser autocorrect solution\nThe solution to this problem is simple. We merely collect multiple frames for\ninference within close time proximity, rather than doing inference on one frame.\nIf for all inferences the tip tracker is off by some amount of pixels, there is a spatial\nerror, which can be corrected by simply correcting the tracker spatially.\nIf for some frames the tip tracker is perfect, while for others it is off, then we know\nthere is a temporal error. Based off the order of the inferences and their spatial\nerrors, we can decide whether to increase or decrease the acceleration constant.\nAdditionally, we can increase the inference rate.\nAnd obviously, if all frames are perfect we make no correction and can decrease\nthe inference rate.", "Version control\nI\u2019ve never really bothered to learn how to properly do version control, and I\nsomehow made a very big mess. Something about submodules, nested git\nrepositories etc. I ended up accidentally deleting my .onnx file, and was not able to\nrecover it. So I need to retrain a new model and add it back in. Other than that,\nhere is the new repository on github to keep track of the code:\nhttps://github.com/BrianP8701/STREAM.AI", "Adjusting predicted angle\nAs you recall, as I am parsing through the gcode I am calculating the angle at\nwhich the extruder is moving on the bed. However, this angle does not always\nexactly match up with the angle that the material is coming out, for various small\nodd reasons.\nTo counter this, I will simply try multiple angles close to my predicted angle.\nI am trying these multiples: 0.85, 0.9, 1.0, 1.1, 1.15", "3/17 - 3/24\n\u274f Run model concurrently alongside gparser to autocorrect tip tracker\n\u274f Train new Yolo V8 model\n\u274f Update measure diameter method to account for tip location", "Gparser autocorrect\nIt appears that there is some unknown variable that makes the printer behave slightly differently\nthen my calculations. Thus, to account for this I will occasionally autocorrect my tracker.\nEvery 120 frames I will crop part of the image and run it through my yolov8 model, which will give\nme the exact location of the tip. I will use this to correct my tracker. I chose 120 frames.\nAs we plan to use this in real time, we cannot have the inference blocking the rest of the process.\nHere is the process to counter that: As the gparser begins, inference will begin running\nconcurrently on a separate thread. The gparser will continue for another 120 frames, then a lock\nwill be placed waiting for the inference thread to complete. Upon inference, correction will\nhappen, then another inference will run, and repeat. Some microcontrollers do not support\nthreading, but do allow for concurrency. The value of 120 can be adjusted as needed depending\non inference time.", "New yolov8 model\nAfter making this program, I was confused as to why my autocorrector was\ncausing my tracker to mess up. I realized, my old yolo model was trained on the\nendoscope camera view. So I\u2019ve trained a new yolov8 model on 728 labeled\nimages. 1748 images after augmentation with blurring, noise, flipping, brightness\nand exposure.\nAlso, I have found code online that does inference on the onnx model, locally, so\nthat is no longer a problem.", "Measure Diameter Fix\nPreviously, upon trying to assign confidences to lines my algorithm would\noccasionally choose the wrong lines. I have concluded that the only way to\ncounter this is through knowing the exact location of the tip and assigning higher\nconfidences to lines within reasonable distance to the tip, while exponentially\ndecreasing confidence as we begin to look at lines that are surpassing what I\nconsider a reasonable distance (About 25 pixels, or 1.6 mm radius). This method\nworks very well.", "Looking forwards\nI believe this is the final step to measure the diameter in real time robustly:\nI will need to use the autocorrect to not only correct the position of the tracker, but to also adjust the acceleration\nconstant. If the tracker is often falling behind, increase the acceleration and vice versa.\nI think I am ready to begin thinking about the next phase of this project. After fixing up the autocorrector, and adding in\nthis final feature I should be able to robustly measure the diameter extruded material during a print in real time with\nthe following:\nConstants:\n\u25cf mm to pixel ratio (changes with camera angle)\nAdjustments:\n\u25cf Integrating my algorithm with real time video and aligning the start of my algorithm with the start of the print\nAdditionally, we will need to add a second camera angle eventually, which shouldn\u2019t be difficult from my standpoint, as\nit just requires switching views when the printer is moving in a certain direction.", "3/3 - 3/10\n\u274f Use algorithm on different videos\n\u274f Use time data\n\u274f Measure diameter", "Gcode Parser (Tip Tracking)\nObservations: While tracking the tip, when we get to building the \u201ccube\u201d, where there are many changes in\ndirections, the tip tracker would run ahead. Upon analyzing the video from last week I noted that the tip tracker\nwas lagging behind at the start and ahead by the end. Thus, I concluded that acceleration is in fact not negligible.\nThe gcode states that the acceleration should be 1000 in the x and y direction and 200 in the z direction.\nHowever, upon experimenting I found that an acceleration of 64 best fit the video. This provided a perfect tracker\nfor that video.\nSo we now have two variable constants, which can be treated as one, the acceleration and the constant by which\nwe multiply the move times. Furthermore, these constants do not work upon other videos. It is very close, but\nupon letting the tracker run for a couple minutes it runs ahead or falls behind by an unacceptable distance.\nI am not sure what is the cause of this. My acceleration and gcode parser is certainly correct. I\u2019ve spent many\nhours reviewing, checking and testing my code. Additionally, the data from UBox relating to the time is not\naccurate enough for 1/30 of a second moves.\nTLDR: I do not know where the error in calculating the time of each line comes from.", "Table of experimental data\nacceleration\nVideo 1 fps time_k (mm/s)\ntest_V1 30 1 1000 Slow at start, fast at end\ntest_V2 30 0.993 1000 Too fast\ntest_V3 32.65 1 1000 Way to slow\ntest_V4 32.65 0.993 1000 Way to slow\ntest_V5 30 0.99 100 Slow at start, fast at end\ntest_V6 30 0.99 1 Acceleration wayyy to slow\ntest_V7 30 0.99 50 Too slow all throughout\nBehind by the same amount, a\ntest_V8 30 0.99 75 little\nBehind by the same amount,\ntest_V9 30 0.988 70 better\ntest_V10 30 0.985 65 Way better, gets too fast at end\ntest_V11 30 0.985 60 Almost perfect\ntest_V12 30 0.985 64 Perfect\nVideo 2 fps time_k acceleration\ntest_V1 30 0.985 64 Good to start, fast at end\ntest_V2 30 0.985 58 Slow to start, fast at end\ntest_V3 30 0.983 55 Perfect", "Edge Detection\nBy locating the exact location of the tip, we have a better idea of where the extruded\nmaterial should be. Find tip by convolving over image looking for the highest\nconfidence of the outline of the tip. Exactly from the center of the tip, draw a line out in\nthe direction it came from. Shift that line from that point in both directions to find the\nedges. If there aren\u2019t two lines that surpass a minimum confidence, then we assume\nthere is no material. Otherwise, simply calculate distance between two lines to find\ndiameter. We can\u2019t look at brightness, because sometimes another area might be\nbrighter. This is the most effective method I\u2019ve thought of.", "Edge Detection\nI have created the tip detector I mentioned last slide, and it works, but not perfectly and the runtime\nis quite bad, taking around a second per inference. Here\u2019s some images. You can further see that\nthe preprocessing step works very well. Even on the white image, it has very clearly defined the\nline, even though it\u2019s hard to see with the human eye (Difference in intensity of color doesn't\nmatter, just difference in color.)", "Edge Detection\nIf I could get the time data to a higher degree of accuracy (I emailed you about this) then the tip\ntracker would very accurately be able to get the point of the tip. Otherwise, I\u2019d have to fix the\nalgorithm from last slide. The next step is to draw a line in the direction we came from, and then\nshift it in both directions to find the best edges. It\u2019s important that the actual tube of the extruder is\nnot included in the cropped image, as it will interfere with edge detection.\nNote from future: What ended up happening is that I used more yolo v8 models to do inference to\nself correct, which I talk about in the future.", "2/24 - 3/3\n\u25cf Made good pre-processing step\n\u25cf Edge detection\n\u25cf Fixed gcode parser", "Preprocessing\nBefore moving further, we want to get rid of the noise and sharpen the edges. We\ncan simplify the image by reducing similarly colored pixels to the same value with\nrespect to the variance of the color in the image", "Edge Detection and Diameter Measurement\nGiven the image and angle of movement, I check all lines in that direction. I\nmeasure confidences for each line, and added some extra fine details I found\nwhile experimenting. It\u2019s not perfect yet - still working on it.\nStill working on\nhow to\novercome\nsome\nsituations like\nthis.", "Gcode Parser\nI fixed the gcode parser, but I have no math or any reasoning to back it up. It\u2019s\nquite accurate now. It turns out that the real time is 99% of the calculated time. I\nmade a few more additional tweaks, and that fixed all the problems. Acceleration\nturns out to be negligible.\nNote from future: This leads into what is the overall problem I seek to address\nlater. Across different videos, there seems to be a different constant % of the\ncalculated time I must find.", "2/17 - 2/24\n\u25cf Tried convolving matrices over images of extruded material\n\u25cf Finished bulk of gcode parser, debugging\n\u25cf Planned algorithm for edge detection, taking inspiration from this paper:\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8607091", "Tried convolving different filters over images\nGaussian Blur, Sharpen3, Sharpen2\nSharpen2\nI tried convolving different matrices over the\nimages, looking to find a combination that would\nwork consistently across the variety of images.\nThese 4 filters I found after hours of trying different\ncombinations, and would only work for that specific\nSharpen1\nimage. This is a very laborious process, and it\u2019s\nactually what CNNs do, learning the best kernels\nfor the image. It was pretty fun and helped build my\nintuition for CNNs more.\nLong1", "Algorithm for detecting edges\nTo measure the diameter of the extruded material, we need to accurately identify\nthe edges of the material. We do not know where this is beforehand, or if it\u2019s even\nthere. However, we do know the angle at which the material should be coming out\ngiven the gcode.\nWhat we can do is treat this as a complete search problem, with all the lines of\nthat angle crossing the image being the search space. We can slightly offset the\nangle in both directions, and after searching the entire search space identify the\ntwo lines that have the greatest probability of being the edges of the extruded\nmaterial.\nWe have to account for the lines from the actual tip, which we can locate and\nfactor into our probability calculation.", "2/11-2/17\n\u25cf Make gcode parser and tip tracker more accurate", "Improving gcode parser and tip tracker\n\u25cf New Challenges:\n\u25cb As we move in the z direction, the x axis will stretch, changing the ratio of mm to pixels.\n\u25cb To estimate time for a line of gcode to be performed, we cannot just divide distance by velocity. We\nhave to account for acceleration and jerk.\n\u25cb Even though the printer isn\u2019t changing it\u2019s speed, when it changes directions it needs to accelerate in\nthat new direction.\n\u25cb I found this quote online: \u201cI have found that the biggest error between the predicted time and the\nactual time has been the time the machine spends processing the instructions.\u201d I don\u2019t know if this is\ntrue, but will leave it here.\n\u25cf Relevant links (For myself, no need to look at this):\n\u25cb https://3dprinting.stackexchange.com/questions/10369/why-does-jerk-have-units-of-mm-s-rather-than-\nmm-s%c2%b3\n\u25cb https://3dprinting.stackexchange.com/questions/3233/calculating-the-estimated-print-time-of-an-alread\ny-sliced-file", "Gcode parser and interpretation\n\u25cf I wrote a working algorithm that reads gcode and tracks the movement of the\ntip accordingly frame by frame\n\u25cf However, the camera shakes hundreds of pixels in unpredictable directions. I\nwill need to record new videos where the camera does not shake so much.\n\u25cb Note from future: This turned out to be a result of the fact that the printer can move in the z\ndirection, which I didn\u2019t account for.\n\u25cf I made my code such so it\u2019s easy to adapt to different types of video angles,\nzoomed in or out and FPS of video.", "Plan to measure diameter\n\u25cf Make tracker that reads G-code. Tracker should output direction and speed extruder is\nmoving at each frame.\n\u25cf Based off speed and direction, crop a portion off the frame accordingly.\n\u25cf Convolve over cropped subsection to detect edges and measure width or area of\nmaterial.\n\u25cf Output diameter of new volume extruded for each frame.\n\u25cf Make code maintainable so we can easily adapt to different camera angles and add a\nsecond camera.", "2/4-2/11\n\u25cf Plan design for algorithm to measure diameter of extruded substrate\nhttps://hackaday.com/2016/02/05/filament-thickness-sensors-what-are-they-and-what-are-they-good-for/", "Topics Originally Planned\n\u2022Hash space anomaly detection (classification) based on Jackson DIW data/new data\n\u2022Preparation: learn from any image classification examples\n\u2022First, CNN for image based consistency classification (can be either at the extrusion point or better at the substrate), may need data augmentation\n\u2022Later, classification after deep hash (see the paper by Huining Li)\n\u2022Can be put on Arduino/Pi later\n\u2022The above steps can be applied to sensor data too\n\u2022Hash space controller based on DIW images for consistency\n\u2022Preparation: learn image feature extraction, regression examples\n\u2022First, extract the diameter info during the printing and compare it with the desired value in real time, obtain the deviation.\n\u2022Then, build a regression model to link material and printing parameters to the deviation.\n\u2022Finally, build a PID controller for the deviation minimization.\n\u2022Later, control under deep hash\n\u2022Attack to controller under hash space\n\u2022Preparation: know the concepts of controller attacks\n\u2022First, design detection mechanism to the attack\n\u2022Second, in the network setting, perform the diagnosis"]