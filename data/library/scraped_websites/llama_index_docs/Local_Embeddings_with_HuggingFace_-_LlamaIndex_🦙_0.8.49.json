{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface.html",
        "title": "Local Embeddings with HuggingFace - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Local Embeddings with HuggingFace[\uf0c1](#local-embeddings-with-huggingface \"Permalink to this heading\")\n\nLlamaIndex has support for HuggingFace embedding models, including BGE, Instructor, and more.\n\nFurthermore, we provide utilties to create and use ONNX models using the [Optimum library](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-optimumonnxruntime) from HuggingFace.\n\n## HuggingFaceEmbedding[\uf0c1](#huggingfaceembedding \"Permalink to this heading\")\n\nThe base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. You can set either `pooling=\"cls\"` or `pooling=\"mean\"` \u2013 in most cases, you\u2019ll want `cls` pooling. But the model card for your particular model may have other recommendations.\n\nYou can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations on embedding models.\n\nThis class depends on the transformers package, which you can install with `pip install transformers`.\n\nNOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivilant results.\n\nfrom llama\\_index.embeddings import HuggingFaceEmbedding\n\n\\# loads BAAI/bge-small-en\n\\# embed\\_model = HuggingFaceEmbedding()\n\n\\# loads BAAI/bge-small-en-v1.5\nembed\\_model \\= HuggingFaceEmbedding(model\\_name\\=\"BAAI/bge-small-en-v1.5\")\n\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/\\_\\_init\\_\\_.py:546: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\nembeddings \\= embed\\_model.get\\_text\\_embedding(\"Hello World!\")\nprint(len(embeddings))\nprint(embeddings\\[:5\\])\n\nHello World!\n384\n\\[-0.030880315229296684, -0.11021008342504501, 0.3917851448059082, -0.35962796211242676, 0.22797748446464539\\]\n\n## InstructorEmbedding[\uf0c1](#instructorembedding \"Permalink to this heading\")\n\nInstructor Embeddings are a class of embeddings specifically trained to augment their embeddings according to an instruction. By default, queries are given `query_instruction=\"Represent the question for retrieving supporting documents: \"` and text is given `text_instruction=\"Represent the document for retrieval: \"`.\n\nThey rely on the `Instructor` pip package, which you can install with `pip install InstructorEmbedding`.\n\nfrom llama\\_index.embeddings import InstructorEmbedding\n\nembed\\_model \\= InstructorEmbedding(model\\_name\\=\"hkunlp/instructor-base\")\n\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using \\`tqdm.autonotebook.tqdm\\` in notebook mode. Use \\`tqdm.tqdm\\` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import trange\n\nload INSTRUCTOR\\_Transformer\n\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/\\_\\_init\\_\\_.py:546: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\nembeddings \\= embed\\_model.get\\_text\\_embedding(\"Hello World!\")\nprint(len(embeddings))\nprint(embeddings\\[:5\\])\n\n768\n\\[ 0.02155361 -0.06098218  0.01796207  0.05490903  0.01526906\\]\n\n## OptimumEmbedding[\uf0c1](#optimumembedding \"Permalink to this heading\")\n\nOptimum in a HuggingFace library for exporting and running HuggingFace models in the ONNX format.\n\nYou can install the dependencies with `pip install transformers optimum[exporters]`.\n\nFirst, we need to create the ONNX model. ONNX models provide improved inference speeds, and can be used across platforms (i.e. in TransformersJS)\n\nfrom llama\\_index.embeddings import OptimumEmbedding\n\nOptimumEmbedding.create\\_and\\_save\\_optimum\\_model(\n    \"BAAI/bge-small-en-v1.5\", \"./bge\\_onnx\"\n)\n\n/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/\\_\\_init\\_\\_.py:546: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\nFramework not specified. Using pt to export to ONNX.\nUsing the export variant default. Available variants are:\n\t- default: The default ONNX variant.\nUsing framework PyTorch: 2.0.1+cu117\nOverriding 1 configuration item(s)\n\t- use\\_cache -> False\n\n\\============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nSaved optimum model to ./bge\\_onnx. Use it with \\`embed\\_model = OptimumEmbedding(folder\\_name='./bge\\_onnx')\\`.\n\nembed\\_model \\= OptimumEmbedding(folder\\_name\\=\"./bge\\_onnx\")\n\nembeddings \\= embed\\_model.get\\_text\\_embedding(\"Hello World!\")\nprint(len(embeddings))\nprint(embeddings\\[:5\\])\n\n384\n\\[-0.10364960134029388, -0.20998482406139374, -0.01883639395236969, -0.5241696834564209, 0.0335749015212059\\]\n\n## Benchmarking[\uf0c1](#benchmarking \"Permalink to this heading\")\n\nLet\u2019s try comparing using a classic large document \u2013 the IPCC climate report, chapter 3.\n\n!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC\\_AR6\\_WGII\\_Chapter03.pdf \\--output IPCC\\_AR6\\_WGII\\_Chapter03.pdf\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using \\`tokenizers\\` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS\\_PARALLELISM=(true | false)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 20.7M  100 20.7M    0     0  16.5M      0  0:00:01  0:00:01 --:--:-- 16.5M\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\ndocuments \\= SimpleDirectoryReader(\n    input\\_files\\=\\[\"IPCC\\_AR6\\_WGII\\_Chapter03.pdf\"\\]\n).load\\_data()\n\n### Base HuggingFace Embeddings[\uf0c1](#base-huggingface-embeddings \"Permalink to this heading\")\n\nimport os\nimport openai\n\n\\# needed to synthesize responses later\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-...\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\nfrom llama\\_index.embeddings import HuggingFaceEmbedding\n\n\\# loads BAAI/bge-small-en-v1.5\nembed\\_model \\= HuggingFaceEmbedding(model\\_name\\=\"BAAI/bge-small-en-v1.5\")\ntest\\_emeds \\= embed\\_model.get\\_text\\_embedding(\"Hello World!\")\n\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\n\n%%timeit -r 1 -n 1\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context, show\\_progress\\=True\n)\n\n1min 27s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n\n### Optimum Embeddings[\uf0c1](#optimum-embeddings \"Permalink to this heading\")\n\nWe can use the onnx embeddings we created earlier\n\nfrom llama\\_index.embeddings import OptimumEmbedding\n\nembed\\_model \\= OptimumEmbedding(folder\\_name\\=\"./bge\\_onnx\")\ntest\\_emeds \\= embed\\_model.get\\_text\\_embedding(\"Hello World!\")\n\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\n\n%%timeit -r 1 -n 1\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context, show\\_progress\\=True\n)\n\n1min 9s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)"
}