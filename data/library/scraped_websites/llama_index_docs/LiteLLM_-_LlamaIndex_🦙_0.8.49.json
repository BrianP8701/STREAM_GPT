{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/llms/litellm.html",
        "title": "LiteLLM - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## LiteLLM[\uf0c1](#litellm \"Permalink to this heading\")\n\n_pydantic model_ llama\\_index.llms.litellm.LiteLLM[\uf0c1](#llama_index.llms.litellm.LiteLLM \"Permalink to this definition\")\n\nShow JSON schema\n\n{\n   \"title\": \"LiteLLM\",\n   \"description\": \"LLM interface.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"callback\\_manager\": {\n         \"title\": \"Callback Manager\"\n      },\n      \"model\": {\n         \"title\": \"Model\",\n         \"description\": \"The LiteLLM model to use.\",\n         \"type\": \"string\"\n      },\n      \"temperature\": {\n         \"title\": \"Temperature\",\n         \"description\": \"The temperature to use during generation.\",\n         \"type\": \"number\"\n      },\n      \"max\\_tokens\": {\n         \"title\": \"Max Tokens\",\n         \"description\": \"The maximum number of tokens to generate.\",\n         \"type\": \"integer\"\n      },\n      \"additional\\_kwargs\": {\n         \"title\": \"Additional Kwargs\",\n         \"description\": \"Additional kwargs for the LLM API.\",\n         \"type\": \"object\"\n      },\n      \"max\\_retries\": {\n         \"title\": \"Max Retries\",\n         \"description\": \"The maximum number of API retries.\",\n         \"type\": \"integer\"\n      }\n   },\n   \"required\": \\[\n      \"model\",\n      \"temperature\",\n      \"max\\_retries\"\n   \\]\n}\n\nConfig\n\n*   **arbitrary\\_types\\_allowed**: _bool = True_\n    \n\nFields\n\n*   [`additional_kwargs (Dict[str, Any])`](#llama_index.llms.litellm.LiteLLM.additional_kwargs \"llama_index.llms.litellm.LiteLLM.additional_kwargs\")\n    \n*   [`max_retries (int)`](#llama_index.llms.litellm.LiteLLM.max_retries \"llama_index.llms.litellm.LiteLLM.max_retries\")\n    \n*   [`max_tokens (Optional[int])`](#llama_index.llms.litellm.LiteLLM.max_tokens \"llama_index.llms.litellm.LiteLLM.max_tokens\")\n    \n*   [`model (str)`](#llama_index.llms.litellm.LiteLLM.model \"llama_index.llms.litellm.LiteLLM.model\")\n    \n*   [`temperature (float)`](#llama_index.llms.litellm.LiteLLM.temperature \"llama_index.llms.litellm.LiteLLM.temperature\")\n    \n\nValidators\n\n*   `_validate_callback_manager` \u00bb `callback_manager`\n    \n\n_field_ additional\\_kwargs_: Dict\\[str, Any\\]_ _\\[Optional\\]_[\uf0c1](#llama_index.llms.litellm.LiteLLM.additional_kwargs \"Permalink to this definition\")\n\nAdditional kwargs for the LLM API.\n\n_field_ max\\_retries_: int_ _\\[Required\\]_[\uf0c1](#llama_index.llms.litellm.LiteLLM.max_retries \"Permalink to this definition\")\n\nThe maximum number of API retries.\n\n_field_ max\\_tokens_: Optional\\[int\\]_ _\\= None_[\uf0c1](#llama_index.llms.litellm.LiteLLM.max_tokens \"Permalink to this definition\")\n\nThe maximum number of tokens to generate.\n\n_field_ model_: str_ _\\[Required\\]_[\uf0c1](#llama_index.llms.litellm.LiteLLM.model \"Permalink to this definition\")\n\nThe LiteLLM model to use.\n\n_field_ temperature_: float_ _\\[Required\\]_[\uf0c1](#llama_index.llms.litellm.LiteLLM.temperature \"Permalink to this definition\")\n\nThe temperature to use during generation.\n\n_async_ achat(_messages: Sequence\\[[ChatMessage](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.achat \"Permalink to this definition\")\n\nAsync chat endpoint for LLM.\n\n_async_ acomplete(_\\*args: Any_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.acomplete \"Permalink to this definition\")\n\nAsync completion endpoint for LLM.\n\n_async_ astream\\_chat(_messages: Sequence\\[[ChatMessage](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.astream_chat \"Permalink to this definition\")\n\nAsync streaming chat endpoint for LLM.\n\n_async_ astream\\_complete(_\\*args: Any_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.astream_complete \"Permalink to this definition\")\n\nAsync streaming completion endpoint for LLM.\n\nchat(_messages: Sequence\\[[ChatMessage](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.chat \"Permalink to this definition\")\n\nChat endpoint for LLM.\n\n_classmethod_ class\\_name() \u2192 str[\uf0c1](#llama_index.llms.litellm.LiteLLM.class_name \"Permalink to this definition\")\n\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\ncomplete(_\\*args: Any_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.complete \"Permalink to this definition\")\n\nCompletion endpoint for LLM.\n\nstream\\_chat(_messages: Sequence\\[[ChatMessage](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.ChatMessage \"llama_index.llms.base.ChatMessage\")\\]_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.stream_chat \"Permalink to this definition\")\n\nStreaming chat endpoint for LLM.\n\nstream\\_complete(_\\*args: Any_, _\\*\\*kwargs: Any_) \u2192 Any[\uf0c1](#llama_index.llms.litellm.LiteLLM.stream_complete \"Permalink to this definition\")\n\nStreaming completion endpoint for LLM.\n\n_property_ metadata_: [LLMMetadata](https://docs.llamaindex.ai/en/stable/api_reference/llms.html#llama_index.llms.base.LLMMetadata \"llama_index.llms.base.LLMMetadata\")_[\uf0c1](#llama_index.llms.litellm.LiteLLM.metadata \"Permalink to this definition\")\n\nLLM metadata."
}