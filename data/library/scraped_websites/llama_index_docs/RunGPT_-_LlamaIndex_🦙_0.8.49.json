{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/llm/rungpt.html",
        "title": "RunGPT - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## RunGPT[\uf0c1](#rungpt \"Permalink to this heading\")\n\nRunGPT is an open-source cloud-native large-scale multimodal models (LMMs) serving framework. It is designed to simplify the deployment and management of large language models, on a distributed cluster of GPUs. RunGPT aim to make it a one-stop solution for a centralized and accessible place to gather techniques for optimizing large-scale multimodal models and make them easy to use for everyone. In RunGPT, we have supported a number of LLMs such as LLaMA, Pythia, StableLM, Vicuna, MOSS, and Large Multi-modal Model(LMMs) like MiniGPT-4 and OpenFlamingo additionally.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nFirstly, you need to install rungpt package in your python environment with `pip install`\n\nAfter installing successfully, models supported by RunGPT can be deployed with an one-line command. This option will download target language model from open source platform and deploy it as a service at a localhost port, which can be accessed by http or grpc requests. I suppose you not run this command in jupyter book, but in command line instead.\n\n!rungpt serve decapoda-research/llama-7b-hf \\--precision fp16 \\--device\\_map balanced\n\n## Basic Usage[\uf0c1](#basic-usage \"Permalink to this heading\")\n\n### Call `complete` with a prompt[\uf0c1](#call-complete-with-a-prompt \"Permalink to this heading\")\n\nfrom llama\\_index.llms.rungpt import RunGptLLM\n\nllm \\= RunGptLLM()\npromot \\= \"What public transportation might be available in a city?\"\nresponse \\= llm.complete(promot)\n\nI don't want to go to work, so what should I do?\nI have a job interview on Monday. What can I wear that will make me look professional but not too stuffy or boring?\n\n### Call `chat` with a list of messages[\uf0c1](#call-chat-with-a-list-of-messages \"Permalink to this heading\")\n\nfrom llama\\_index.llms.base import ChatMessage, MessageRole\nfrom llama\\_index.llms.rungpt import RunGptLLM\n\nmessages \\= \\[\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=\"Now, I want you to do some math for me.\",\n    ),\n    ChatMessage(\n        role\\=MessageRole.ASSISTANT, content\\=\"Sure, I would like to help you.\"\n    ),\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=\"How many points determine a straight line?\",\n    ),\n\\]\nllm \\= RunGptLLM()\nresponse \\= llm.chat(messages\\=messages, temperature\\=0.8, max\\_tokens\\=15)\n\n## Streaming[\uf0c1](#streaming \"Permalink to this heading\")\n\nUsing `stream_complete` endpoint\n\npromot \\= \"What public transportation might be available in a city?\"\nresponse \\= RunGptLLM().stream\\_complete(promot)\nfor item in response:\n    print(item.text)\n\nUsing `stream_chat` endpoint\n\nfrom llama\\_index.llms.rungpt import RunGptLLM\n\nmessages \\= \\[\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=\"Now, I want you to do some math for me.\",\n    ),\n    ChatMessage(\n        role\\=MessageRole.ASSISTANT, content\\=\"Sure, I would like to help you.\"\n    ),\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=\"How many points determine a straight line?\",\n    ),\n\\]\nresponse \\= RunGptLLM().stream\\_chat(messages\\=messages)\n\nfor item in response:\n    print(item.message)"
}