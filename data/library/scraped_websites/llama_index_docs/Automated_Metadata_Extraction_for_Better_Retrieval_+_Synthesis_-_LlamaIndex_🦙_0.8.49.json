{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MetadataExtraction_LLMSurvey.html",
        "title": "Automated Metadata Extraction for Better Retrieval + Synthesis - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nIn this tutorial, we show you how to perform automated metadata extraction for better retrieval results. We use two extractors: a QuestionAnsweredExtractor which generates question/answer pairs from a piece of text, and also a SummaryExtractor which extracts summaries, not only within the current text, but also within adjacent texts.\n\nWe show that this allows for \u201cchunk dreaming\u201d - each individual chunk can have more \u201cholistic\u201d details, leading to higher answer quality given retrieved results.\n\nOur data source is taken from Eugene Yan\u2019s popular article on LLM Patterns: https://eugeneyan.com/writing/llm-patterns/\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nimport os\nimport openai\n\n\\# OPTIONAL: setup W&B callback handling for tracing\nfrom llama\\_index import set\\_global\\_handler\n\nset\\_global\\_handler(\"wandb\", run\\_args\\={\"project\": \"llamaindex\"})\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"YOUR\\_API\\_KEY\\_HERE\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n## Define Metadata Extractors[\uf0c1](#define-metadata-extractors \"Permalink to this heading\")\n\nHere we define metadata extractors. We define two variants:\n\n*   metadata\\_extractor\\_1 only contains the QuestionsAnsweredExtractor\n    \n*   metadata\\_extractor\\_2 contains both the QuestionsAnsweredExtractor as well as the SummaryExtractor\n    \n\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index.schema import MetadataMode\n\nllm \\= OpenAI(temperature\\=0.1, model\\=\"gpt-3.5-turbo\", max\\_tokens\\=512)\n\nWe also show how to instantiate the `SummaryExtractor` and `QuestionsAnsweredExtractor`.\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom llama\\_index.node\\_parser.extractors import (\n    MetadataExtractor,\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n)\nfrom llama\\_index.text\\_splitter import TokenTextSplitter\n\ntext\\_splitter \\= TokenTextSplitter(\n    separator\\=\" \", chunk\\_size\\=256, chunk\\_overlap\\=128\n)\n\nmetadata\\_extractor\\_1 \\= MetadataExtractor(\n    extractors\\=\\[\n        QuestionsAnsweredExtractor(questions\\=3, llm\\=llm),\n    \\],\n    in\\_place\\=False,\n)\n\nmetadata\\_extractor \\= MetadataExtractor(\n    extractors\\=\\[\n        SummaryExtractor(summaries\\=\\[\"prev\", \"self\", \"next\"\\], llm\\=llm),\n        QuestionsAnsweredExtractor(questions\\=3, llm\\=llm),\n    \\],\n    in\\_place\\=False,\n)\n\nnode\\_parser \\= SimpleNodeParser.from\\_defaults(\n    text\\_splitter\\=text\\_splitter,\n    \\# metadata\\_extractor=metadata\\_extractor,\n)\n\n## Load in Data, Run Extractors[\uf0c1](#load-in-data-run-extractors \"Permalink to this heading\")\n\nWe load in Eugene\u2019s essay (https://eugeneyan.com/writing/llm-patterns/) using our LlamaHub SimpleWebPageReader.\n\nWe then run our extractors.\n\nfrom llama\\_index import SimpleDirectoryReader\n\n\\# load in blog\n\nfrom llama\\_hub.web.simple\\_web.base import SimpleWebPageReader\n\nreader \\= SimpleWebPageReader(html\\_to\\_text\\=True)\ndocs \\= reader.load\\_data(urls\\=\\[\"https://eugeneyan.com/writing/llm-patterns/\"\\])\n\nprint(docs\\[0\\].get\\_content())\n\norig\\_nodes \\= node\\_parser.get\\_nodes\\_from\\_documents(docs)\n\n\\# take just the first 8 nodes for testing\nnodes \\= orig\\_nodes\\[20:28\\]\n\nprint(nodes\\[3\\].get\\_content(metadata\\_mode\\=\"all\"))\n\nis to measure the distance that words would\nhave to move to convert one sequence to another.\n\nHowever, there are several pitfalls to using these conventional benchmarks and\nmetrics.\n\nFirst, there\u2019s \\*\\*poor correlation between these metrics and human judgments.\\*\\*\nBLEU, ROUGE, and others have had \\[negative correlation with how humans\nevaluate fluency\\](https://arxiv.org/abs/2008.12009). They also showed moderate\nto less correlation with human adequacy scores. In particular, BLEU and ROUGE\nhave \\[low correlation with tasks that require creativity and\ndiversity\\](https://arxiv.org/abs/2303.16634).\n\nSecond, these metrics often have \\*\\*poor adaptability to a wider variety of\ntasks\\*\\*. Adopting a metric proposed for one task to another is not always\nprudent. For example, exact match metrics such as BLEU and ROUGE are a poor\nfit for tasks like abstractive summarization or dialogue. Since they\u2019re based\non n-gram overlap between output and reference, they don\u2019t make sense for a\ndialogue task where a wide variety\n\n### Run metadata extractors[\uf0c1](#run-metadata-extractors \"Permalink to this heading\")\n\n\\# process nodes with metadata extractor\nnodes\\_1 \\= metadata\\_extractor\\_1.process\\_nodes(nodes)\n\nprint(nodes\\_1\\[3\\].get\\_content(metadata\\_mode\\=\"all\"))\n\n\\[Excerpt from document\\]\nquestions\\_this\\_excerpt\\_can\\_answer: 1. What is the correlation between conventional metrics like BLEU and ROUGE and human judgments of fluency and adequacy in natural language processing tasks?\n2. How well do metrics like BLEU and ROUGE perform in tasks that require creativity and diversity?\n3. Why are exact match metrics like BLEU and ROUGE not suitable for tasks like abstractive summarization or dialogue?\nExcerpt:\n-----\nis to measure the distance that words would\nhave to move to convert one sequence to another.\n\nHowever, there are several pitfalls to using these conventional benchmarks and\nmetrics.\n\nFirst, there\u2019s \\*\\*poor correlation between these metrics and human judgments.\\*\\*\nBLEU, ROUGE, and others have had \\[negative correlation with how humans\nevaluate fluency\\](https://arxiv.org/abs/2008.12009). They also showed moderate\nto less correlation with human adequacy scores. In particular, BLEU and ROUGE\nhave \\[low correlation with tasks that require creativity and\ndiversity\\](https://arxiv.org/abs/2303.16634).\n\nSecond, these metrics often have \\*\\*poor adaptability to a wider variety of\ntasks\\*\\*. Adopting a metric proposed for one task to another is not always\nprudent. For example, exact match metrics such as BLEU and ROUGE are a poor\nfit for tasks like abstractive summarization or dialogue. Since they\u2019re based\non n-gram overlap between output and reference, they don\u2019t make sense for a\ndialogue task where a wide variety\n-----\n\n\\# 2nd pass: run summaries, and then metadata extractor\n\n\\# process nodes with metadata extractor\nnodes\\_2 \\= metadata\\_extractor.process\\_nodes(nodes)\n\n### Visualize some sample data[\uf0c1](#visualize-some-sample-data \"Permalink to this heading\")\n\nprint(nodes\\_2\\[3\\].get\\_content(metadata\\_mode\\=\"all\"))\n\n\\[Excerpt from document\\]\nprev\\_section\\_summary: The section discusses the comparison between BERTScore and MoverScore, two metrics used to evaluate the quality of text generation models. MoverScore is described as a metric that measures the effort required to transform one text sequence into another by mapping semantically related words. The section also highlights the limitations of conventional benchmarks and metrics, such as poor correlation with human judgments and low correlation with tasks requiring creativity.\nnext\\_section\\_summary: The section discusses the limitations of current evaluation metrics in natural language processing tasks. It highlights three main issues: lack of creativity and diversity in metrics, poor adaptability to different tasks, and poor reproducibility. The section mentions specific metrics like BLEU and ROUGE, and also references studies that have reported high variance in metric scores.\nsection\\_summary: The section discusses the limitations of conventional benchmarks and metrics used to measure the distance between word sequences. It highlights two main issues: poor correlation with human judgments and poor adaptability to different tasks. The metrics like BLEU and ROUGE have been found to have low correlation with human evaluations of fluency and adequacy, as well as tasks requiring creativity and diversity. Additionally, these metrics are not suitable for tasks like abstractive summarization or dialogue due to their reliance on n-gram overlap.\nquestions\\_this\\_excerpt\\_can\\_answer: 1. What are the limitations of conventional benchmarks and metrics in measuring the distance between word sequences?\n2. How do metrics like BLEU and ROUGE correlate with human judgments of fluency and adequacy?\n3. Why are metrics like BLEU and ROUGE not suitable for tasks like abstractive summarization or dialogue?\nExcerpt:\n-----\nis to measure the distance that words would\nhave to move to convert one sequence to another.\n\nHowever, there are several pitfalls to using these conventional benchmarks and\nmetrics.\n\nFirst, there\u2019s \\*\\*poor correlation between these metrics and human judgments.\\*\\*\nBLEU, ROUGE, and others have had \\[negative correlation with how humans\nevaluate fluency\\](https://arxiv.org/abs/2008.12009). They also showed moderate\nto less correlation with human adequacy scores. In particular, BLEU and ROUGE\nhave \\[low correlation with tasks that require creativity and\ndiversity\\](https://arxiv.org/abs/2303.16634).\n\nSecond, these metrics often have \\*\\*poor adaptability to a wider variety of\ntasks\\*\\*. Adopting a metric proposed for one task to another is not always\nprudent. For example, exact match metrics such as BLEU and ROUGE are a poor\nfit for tasks like abstractive summarization or dialogue. Since they\u2019re based\non n-gram overlap between output and reference, they don\u2019t make sense for a\ndialogue task where a wide variety\n-----\n\nprint(nodes\\_2\\[1\\].get\\_content(metadata\\_mode\\=\"all\"))\n\n\\[Excerpt from document\\]\nprev\\_section\\_summary: The section discusses the F\\_{BERT} formula used in BERTScore and highlights the advantages of BERTScore over simpler metrics like BLEU and ROUGE. It also introduces MoverScore, another metric that uses contextualized embeddings but allows for many-to-one matching. The key topics are BERTScore, MoverScore, and the differences between them.\nnext\\_section\\_summary: The section discusses the comparison between BERTScore and MoverScore, two metrics used to evaluate the quality of text generation models. MoverScore is described as a metric that measures the effort required to transform one text sequence into another by mapping semantically related words. The section also highlights the limitations of conventional benchmarks and metrics, such as poor correlation with human judgments and low correlation with tasks requiring creativity.\nsection\\_summary: The key topics of this section are BERTScore and MoverScore, which are methods used to compute the similarity between generated output and reference in tasks like image captioning and machine translation. BERTScore uses one-to-one matching of tokens, while MoverScore allows for many-to-one matching. MoverScore solves an optimization problem to measure the distance that words would have to move to convert one sequence to another.\nquestions\\_this\\_excerpt\\_can\\_answer: 1. What is the main difference between BERTScore and MoverScore?\n2. How does MoverScore allow for many-to-one matching of tokens?\n3. What problem does MoverScore solve to measure the distance between two sequences?\nExcerpt:\n-----\nto have better correlation for tasks\nsuch as image captioning and machine translation.\n\n\\*\\*\\[MoverScore\\](https://arxiv.org/abs/1909.02622)\\*\\* also uses contextualized\nembeddings to compute the distance between tokens in the generated output and\nreference. But unlike BERTScore, which is based on one-to-one matching (or\n\u201chard alignment\u201d) of tokens, MoverScore allows for many-to-one matching (or\n\u201csoft alignment\u201d).\n\n!\\[BERTScore \\\\(left\\\\) vs. MoverScore \\\\(right\\\\)\\](/assets/mover-score.jpg)\n\nBERTScore (left) vs. MoverScore (right;\n\\[source\\](https://arxiv.org/abs/1909.02622))\n\nMoverScore enables the mapping of semantically related words in one sequence\nto their counterparts in another sequence. It does this by solving a\nconstrained optimization problem that finds the minimum effort to transform\none text into another. The idea is to measure the distance that words would\nhave to move to convert one sequence to another.\n\nHowever, there\n-----\n\n## Setup RAG Query Engines, Compare Results![\uf0c1](#setup-rag-query-engines-compare-results \"Permalink to this heading\")\n\nWe setup 3 indexes/query engines on top of the three node variants.\n\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.response.notebook\\_utils import (\n    display\\_source\\_node,\n    display\\_response,\n)\n\n\\# try out different query engines\n\n\\# index0 = VectorStoreIndex(orig\\_nodes)\n\\# index1 = VectorStoreIndex(nodes\\_1 + orig\\_nodes\\[8:\\])\n\\# index2 = VectorStoreIndex(nodes\\_2 + orig\\_nodes\\[8:\\])\n\nindex0 \\= VectorStoreIndex(orig\\_nodes)\nindex1 \\= VectorStoreIndex(orig\\_nodes\\[:20\\] + nodes\\_1 + orig\\_nodes\\[28:\\])\nindex2 \\= VectorStoreIndex(orig\\_nodes\\[:20\\] + nodes\\_2 + orig\\_nodes\\[28:\\])\n\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\n\nquery\\_engine0 \\= index0.as\\_query\\_engine(similarity\\_top\\_k\\=1)\nquery\\_engine1 \\= index1.as\\_query\\_engine(similarity\\_top\\_k\\=1)\nquery\\_engine2 \\= index2.as\\_query\\_engine(similarity\\_top\\_k\\=1)\n\n### Try out some questions[\uf0c1](#try-out-some-questions \"Permalink to this heading\")\n\nIn this question, we see that the naive response `response0` only mentions BLEU and ROUGE, and lacks context about other metrics.\n\n`response2` on the other hand has all metrics within its context.\n\n\\# query\\_str = \"In the original RAG paper, can you describe the two main approaches for generation and compare them?\"\nquery\\_str \\= (\n    \"Can you describe metrics for evaluating text generation quality, compare\"\n    \" them, and tell me about their downsides\"\n)\n\nresponse0 \\= query\\_engine0.query(query\\_str)\nresponse1 \\= query\\_engine1.query(query\\_str)\nresponse2 \\= query\\_engine2.query(query\\_str)\n\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\n\ndisplay\\_response(\n    response0, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\nprint(response0.source\\_nodes\\[0\\].node.get\\_content())\n\nrequire creativity and\ndiversity\\](https://arxiv.org/abs/2303.16634).\n\nSecond, these metrics often have \\*\\*poor adaptability to a wider variety of\ntasks\\*\\*. Adopting a metric proposed for one task to another is not always\nprudent. For example, exact match metrics such as BLEU and ROUGE are a poor\nfit for tasks like abstractive summarization or dialogue. Since they\u2019re based\non n-gram overlap between output and reference, they don\u2019t make sense for a\ndialogue task where a wide variety of responses are possible. An output can\nhave zero n-gram overlap with the reference but yet be a good response.\n\nThird, these metrics have \\*\\*poor reproducibility\\*\\*. Even for the same metric,\n\\[high variance is reported across different\nstudies\\](https://arxiv.org/abs/2008.12009), possibly due to variations in\nhuman judgment collection or metric parameter settings. Another study of\n\\[ROUGE scores\\](https://aclanthology.org/2023.acl-long.107/) across 2,000\nstudies found that scores were hard\n\ndisplay\\_response(\n    response1, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\ndisplay\\_response(\n    response2, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\nIn this next question, we ask about BERTScore/MoverScore.\n\nThe responses are similar. But `response2` gives slightly more detail than `response0` since it has more information about MoverScore contained in the Metadata.\n\n\\# query\\_str = \"What are some reproducibility issues with the ROUGE metric? Give some details related to benchmarks and also describe other ROUGE issues. \"\nquery\\_str \\= (\n    \"Can you give a high-level overview of BERTScore/MoverScore + formulas if\"\n    \" available?\"\n)\n\nresponse0 \\= query\\_engine0.query(query\\_str)\nresponse1 \\= query\\_engine1.query(query\\_str)\nresponse2 \\= query\\_engine2.query(query\\_str)\n\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\nwandb: Logged trace tree to W&B.\n\ndisplay\\_response(\n    response0, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\ndisplay\\_response(\n    response1, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\ndisplay\\_response(\n    response2, source\\_length\\=1000, show\\_source\\=True, show\\_source\\_metadata\\=True\n)\n\nresponse1.source\\_nodes\\[0\\].node.metadata"
}