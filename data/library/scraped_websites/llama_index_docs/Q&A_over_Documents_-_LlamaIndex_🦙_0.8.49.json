{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/question_and_answer.html",
        "title": "Q&A over Documents - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Q&A over Documents[\uf0c1](#q-a-over-documents \"Permalink to this heading\")\n\nAt a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case, whether it\u2019s question-answering, summarization, or a component in a chatbot.\n\nThis section describes the different ways you can query your data with LlamaIndex, roughly in order of simplest (top-k semantic search), to more advanced capabilities.\n\n## Semantic Search[\uf0c1](#semantic-search \"Permalink to this heading\")\n\nThe most basic example usage of LlamaIndex is through semantic search. We provide a simple in-memory vector store for you to get started, but you can also choose to use any one of our [vector store integrations](https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores.html):\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n**Tutorials**\n\n*   [Starter Tutorial](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n    \n*   [Basic Usage Pattern](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/usage_pattern.html)\n    \n\n**Guides**\n\n*   [Example](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemo.html) ([Notebook](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores/SimpleIndexDemo.ipynb))\n    \n\n## Summarization[\uf0c1](#summarization \"Permalink to this heading\")\n\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer. For instance, a summarization query could look like one of the following:\n\n*   \u201cWhat is a summary of this collection of text?\u201d\n    \n*   \u201cGive me a summary of person X\u2019s experience with the company.\u201d\n    \n\nIn general, a summary index would be suited for this use case. A summary index by default goes through all the data.\n\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\n\nindex \\= SummaryIndex.from\\_documents(documents)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    response\\_mode\\=\"tree\\_summarize\"\n)\nresponse \\= query\\_engine.query(\"<summarization\\_query>\")\n\n## Queries over Structured Data[\uf0c1](#queries-over-structured-data \"Permalink to this heading\")\n\nLlamaIndex supports queries over structured data, whether that\u2019s a Pandas DataFrame or a SQL Database.\n\nHere are some relevant resources:\n\n**Tutorials**\n\n*   Guide on Text-to-SQL\n    \n\n**Guides**\n\n*   [SQL Guide (Core)](https://docs.llamaindex.ai/en/stable/examples/index_structs/struct_indices/SQLIndexDemo.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/SQLIndexDemo.ipynb))\n    \n*   [Pandas Demo](https://docs.llamaindex.ai/en/stable/examples/query_engine/pandas_query_engine.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/pandas_query_engine.ipynb))\n    \n\n## Synthesis over Heterogeneous Data[\uf0c1](#synthesis-over-heterogeneous-data \"Permalink to this heading\")\n\nLlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data. Specifically, compose a summary index over your subindices. A summary index inherently combines information for each node; therefore it can synthesize information across your heterogeneous data sources.\n\nfrom llama\\_index import VectorStoreIndex, SummaryIndex\nfrom llama\\_index.indices.composability import ComposableGraph\n\nindex1 \\= VectorStoreIndex.from\\_documents(notion\\_docs)\nindex2 \\= VectorStoreIndex.from\\_documents(slack\\_docs)\n\ngraph \\= ComposableGraph.from\\_indices(SummaryIndex, \\[index1, index2\\], index\\_summaries\\=\\[\"summary1\", \"summary2\"\\])\nquery\\_engine \\= graph.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"<query\\_str>\")\n\n**Guides**\n\n*   [Composability](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/composability.html)\n    \n*   [City Analysis](https://docs.llamaindex.ai/en/stable/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\n    \n\n## Routing over Heterogeneous Data[\uf0c1](#routing-over-heterogeneous-data \"Permalink to this heading\")\n\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \u201croute\u201d a query to an underlying Document or a sub-index.\n\nTo do this, first build the sub-indices over different data sources. Then construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.\n\nfrom llama\\_index import TreeIndex, VectorStoreIndex\nfrom llama\\_index.tools import QueryEngineTool\n\n...\n\n\\# define sub-indices\nindex1 \\= VectorStoreIndex.from\\_documents(notion\\_docs)\nindex2 \\= VectorStoreIndex.from\\_documents(slack\\_docs)\n\n\\# define query engines and tools\ntool1 \\= QueryEngineTool.from\\_defaults(\n    query\\_engine\\=index1.as\\_query\\_engine(),\n    description\\=\"Use this query engine to do...\",\n)\ntool2 \\= QueryEngineTool.from\\_defaults(\n    query\\_engine\\=index2.as\\_query\\_engine(),\n    description\\=\"Use this query engine for something else...\",\n)\n\nThen, we define a `RouterQueryEngine` over them. By default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.\n\nfrom llama\\_index.query\\_engine import RouterQueryEngine\n\nquery\\_engine \\= RouterQueryEngine.from\\_defaults(\n    query\\_engine\\_tools\\=\\[tool1, tool2\\]\n)\n\nresponse \\= query\\_engine.query(\n    \"In Notion, give me a summary of the product roadmap.\"\n)\n\n**Guides**\n\n*   [Router Query Engine Guide](https://docs.llamaindex.ai/en/stable/examples/query_engine/RouterQueryEngine.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/RouterQueryEngine.ipynb))\n    \n*   [City Analysis Unified Query Interface](https://docs.llamaindex.ai/en/stable/examples/composable_indices/city_analysis/City_Analysis-Unified-Query.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\n    \n\n## Compare/Contrast Queries[\uf0c1](#compare-contrast-queries \"Permalink to this heading\")\n\nYou can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.\n\nfrom llama\\_index.indices.query.query\\_transform.base import DecomposeQueryTransform\ndecompose\\_transform \\= DecomposeQueryTransform(\n    service\\_context.llm\\_predictor, verbose\\=True\n)\n\nThis module will help break down a complex query into a simpler one over your existing index structure.\n\n**Guides**\n\n*   [Query Transformations](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/advanced/query_transformations.html)\n    \n*   [City Analysis Compare/Contrast Example](https://docs.llamaindex.ai/en/stable/examples/composable_indices/city_analysis/City_Analysis-Decompose.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/City_Analysis-Decompose.ipynb))\n    \n\nYou can also rely on the LLM to _infer_ whether to perform compare/contrast queries (see Multi-Document Queries below).\n\n## Multi-Document Queries[\uf0c1](#multi-document-queries \"Permalink to this heading\")\n\nBesides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well. It can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \u201cquery plan\u201d containing sub-queries against sub-documents before synthesizing the final answer.\n\nTo do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):\n\nfrom llama\\_index.tools import QueryEngineTool, ToolMetadata\n\nquery\\_engine\\_tools \\= \\[\n    QueryEngineTool(\n        query\\_engine\\=sept\\_engine,\n        metadata\\=ToolMetadata(name\\='sept\\_22', description\\='Provides information about Uber quarterly financials ending September 2022')\n    ),\n    QueryEngineTool(\n        query\\_engine\\=june\\_engine,\n        metadata\\=ToolMetadata(name\\='june\\_22', description\\='Provides information about Uber quarterly financials ending June 2022')\n    ),\n    QueryEngineTool(\n        query\\_engine\\=march\\_engine,\n        metadata\\=ToolMetadata(name\\='march\\_22', description\\='Provides information about Uber quarterly financials ending March 2022')\n    ),\n\\]\n\nThen, we define a `SubQuestionQueryEngine` over these tools:\n\nfrom llama\\_index.query\\_engine import SubQuestionQueryEngine\n\nquery\\_engine \\= SubQuestionQueryEngine.from\\_defaults(query\\_engine\\_tools\\=query\\_engine\\_tools)\n\nThis query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer. This makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.\n\n**Guides**\n\n*   [Sub Question Query Engine (Intro)](https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html)\n    \n*   [10Q Analysis (Uber)](https://docs.llamaindex.ai/en/stable/examples/usecases/10q_sub_question.html)\n    \n*   [10K Analysis (Uber and Lyft)](https://docs.llamaindex.ai/en/stable/examples/usecases/10k_sub_question.html)\n    \n\n## Multi-Step Queries[\uf0c1](#multi-step-queries \"Permalink to this heading\")\n\nLlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions, and sequentially generate subquestions based on returned answers until the final answer is returned.\n\nFor instance, given a question \u201cWho was in the first batch of the accelerator program the author started?\u201d, the module will first decompose the query into a simpler initial question \u201cWhat was the accelerator program the author started?\u201d, query the index, and then ask followup questions.\n\n**Guides**\n\n*   [Query Transformations](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/advanced/query_transformations.html)\n    \n*   [Multi-Step Query Decomposition](https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo.html) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\n    \n\n## Temporal Queries[\uf0c1](#temporal-queries \"Permalink to this heading\")\n\nLlamaIndex can support queries that require an understanding of time. It can do this in two ways:\n\n*   Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question.\n    \n*   Sort by recency and filter outdated context.\n    \n\n**Guides**\n\n*   [Second-Stage Postprocessing Guide](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/node_postprocessors/root.html)\n    \n*   [Prev/Next Postprocessing](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PrevNextPostprocessorDemo.html)\n    \n*   [Recency Postprocessing](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/RecencyPostprocessorDemo.html)"
}