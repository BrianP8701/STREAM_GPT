{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/MyScaleIndexDemo.html",
        "title": "MyScale Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## MyScale Vector Store[\uf0c1](#myscale-vector-store \"Permalink to this heading\")\n\nIn this notebook we are going to show a quick demo of using the MyScaleVectorStore.\n\n## Creating a MyScale Client[\uf0c1](#creating-a-myscale-client \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom os import environ\nimport clickhouse\\_connect\n\nenviron\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-\\*\"\n\n\\# initialize client\nclient \\= clickhouse\\_connect.get\\_client(\n    host\\=\"YOUR\\_CLUSTER\\_HOST\",\n    port\\=8443,\n    username\\=\"YOUR\\_USERNAME\",\n    password\\=\"YOUR\\_CLUSTER\\_PASSWORD\",\n)\n\n## Load documents, build and store the VectorStoreIndex with MyScaleVectorStore[\uf0c1](#load-documents-build-and-store-the-vectorstoreindex-with-myscalevectorstore \"Permalink to this heading\")\n\nHere we will use a set of Paul Graham essays to provide the text to turn into embeddings, store in a `MyScaleVectorStore` and query to find context for our LLM QnA loop.\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama\\_index.vector\\_stores import MyScaleVectorStore\nfrom IPython.display import Markdown, display\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\nprint(\"Document ID:\", documents\\[0\\].doc\\_id)\nprint(\"Number of Documents: \", len(documents))\n\nDocument ID: a5f2737c-ed18-4e5d-ab9a-75955edb816d\nNumber of Documents:  1\n\nYou can process your files individually using SimpleDirectoryReader:\n\nloader \\= SimpleDirectoryReader(\"../data/paul\\_graham\")\ndocuments \\= loader.load\\_data()\nfor file in loader.input\\_files:\n    print(file)\n    \\# Here is where you would do any preprocessing\n\n../data/paul\\_graham/paul\\_graham\\_essay.txt\n\n\\# initialize with metadata filter and store indexes\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\nfor document in documents:\n    document.metadata \\= {\"user\\_id\": \"123\", \"favorite\\_color\": \"blue\"}\nvector\\_store \\= MyScaleVectorStore(myscale\\_client\\=client)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\nNow MyScale vector store supports filter search and hybrid search\n\nYou can learn more about [query\\_engine](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html) and retriveve.\n\nimport textwrap\n\nfrom llama\\_index.vector\\_stores.types import ExactMatchFilter, MetadataFilters\n\n\\# set Logging to DEBUG for more detailed outputs\nquery\\_engine \\= index.as\\_query\\_engine(\n    filters\\=MetadataFilters(\n        filters\\=\\[\n            ExactMatchFilter(key\\=\"user\\_id\", value\\=\"123\"),\n        \\]\n    ),\n    similarity\\_top\\_k\\=2,\n    vector\\_store\\_query\\_mode\\=\"hybrid\",\n)\nresponse \\= query\\_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n\n## Clear All Indexes[\uf0c1](#clear-all-indexes \"Permalink to this heading\")\n\nfor document in documents:\n    index.delete\\_ref\\_doc(document.doc\\_id)"
}