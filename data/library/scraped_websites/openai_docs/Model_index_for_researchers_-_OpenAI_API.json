{
    "metadata": {
        "type": "web",
        "url": "https://platform.openai.com/docs/model-index-for-researchers/researcher-access-program",
        "title": "Model index for researchers - OpenAI API",
        "description": "Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform."
    },
    "text": "[](https://platform.openai.com/docs/model-index-for-researchers/model-index-for-researchers)\n\n## [Model index for researchers](https://platform.openai.com/docs/model-index-for-researchers/model-index-for-researchers)\n\nOur models are used for both research purposes and developer use cases in production. Researchers often learn about our models from papers that we have published, but there is often not a perfect match between what is available in the OpenAI API and what is published in a paper.\n\nThe purpose of this page is to help clarify:\n\n*   Some of the differences in the ways that our models are trained, which impacts the comparisons that can be made between models, and various evaluation results.\n*   The differences between various model series, such as GPT 3.5 and InstructGPT.\n*   Which if any of the models available in the API today match with a model in a paper. In some cases, there might not be a match.\n\n[](https://platform.openai.com/docs/model-index-for-researchers/models-referred-to-as-gpt-3-5)\n\n## [Models referred to as \"GPT 3.5\"](https://platform.openai.com/docs/model-index-for-researchers/models-referred-to-as-gpt-3-5)\n\nGPT-3.5 series is a series of models that was trained on a blend of text and code from before Q4 2021. The following models are in the GPT-3.5 series:\n\n1.  `code-davinci-002` is a base model, so good for pure code-completion tasks\n2.  `text-davinci-002` is an InstructGPT model based on `code-davinci-002`\n3.  `text-davinci-003` is an improvement on `text-davinci-002`\n4.  `gpt-3.5-turbo-0301` is an improvement on `text-davinci-003`, optimized for chat\n\n[](https://platform.openai.com/docs/model-index-for-researchers/instructgpt-models)\n\n## [InstructGPT models](https://platform.openai.com/docs/model-index-for-researchers/instructgpt-models)\n\nWe offer variants of InstructGPT models trained in 3 different ways:\n\n| Training Method | Models |\n| --- | --- |\n| **SFT**  <br>Supervised fine-tuning on human demonstrations | `davinci-instruct-beta`[1](#footnote-1) |\n| **FeedME**  <br>Supervised fine-tuning on human-written demonstrations and on model samples rated 7/7 by human labelers on an overall quality score | `text-davinci-001`, `text-davinci-002`, `text-curie-001`, `text-babbage-001` |\n| **PPO**  <br>Reinforcement learning with reward models trained from comparisons by humans | `text-davinci-003` |\n\nThe SFT and PPO models are trained similarly to the ones from the [InstructGPT paper](https://arxiv.org/abs/2203.02155). FeedME (short for \"feedback made easy\") models are trained by distilling the best completions from all of our models. Our models generally used the best available datasets at the time of training, and so different engines using the same training methodology might be trained on different data.\n\n[](https://platform.openai.com/docs/model-index-for-researchers/models-featured-in-openai-research)\n\n## [Models featured in OpenAI Research](https://platform.openai.com/docs/model-index-for-researchers/models-featured-in-openai-research)\n\nThese are the most proximate models featured in our research papers that are available in the API today. Please note that not all models available in the API correspond to a paper, and even for models that are listed below there may be subtle differences that do not allow for exact replication of the paper.\n\n| Paper | Published | Model Name in Paper | Model Name in API | Parameters[2](#footnote-2) |\n| --- | --- | --- | --- | --- |\n| [\\[2005.14165\\] Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) | 22 Jul 2020 | GPT-3 175B | davinci | 175B |\n| GPT-3 6.7B | curie | 6.7B |\n| GPT-3 1B | babbage | 1B  |\n| [\\[2107.03374\\] Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) | 14 Jul 2021 | Codex 12B | code-cushman-001[3](#footnote-3) | 12B |\n| [\\[2201.10005\\] Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/abs/2201.10005) | 14 Jan 2022 | GPT-3 unsupervised cpt-text 175B | text-similarity-davinci-001 | 175B |\n| GPT-3 unsupervised cpt-text 6B | text-similarity-curie-001 | 6B  |\n| GPT-3 unsupervised cpt-text 1.2B | No close matching model on API | 1.2B |\n| [\\[2009.01325\\] Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) | 15 Feb 2022 | GPT-3 6.7B pretrain | No close matching model on API | 6.7B |\n| GPT-3 2.7B pretrain | No close matching model on API | 2.7B |\n| GPT-3 1.3B pretrain | No close matching model on API | 1.3B |\n| [\\[2203.02155\\] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) | 4 Mar 2022 | InstructGPT-3 175B SFT | davinci-instruct-beta | 175B |\n| InstructGPT-3 175B | No close matching model on API | 175B |\n| InstructGPT-3 6B | No close matching model on API | 6B  |\n| InstructGPT-3 1.3B | No close matching model on API | 1.3B |\n\n[](https://platform.openai.com/docs/model-index-for-researchers/researcher-access-program)\n\n## [Researcher Access Program](https://platform.openai.com/docs/model-index-for-researchers/researcher-access-program)\n\nThere are a number of research directions we are excited to explore with the OpenAI API. If you are interested in the opportunity for subsidized access, please provide us with details about your research use case using [this form](https://share.hsforms.com/1b-BEAq_qQpKcfFGKwwuhxA4sk30).\n\nIn particular, we consider the following to be especially important directions, though you are free to craft your own direction:\n\n*   **Alignment**: How can we understand what objective, if any, a model is best understood as pursuing? How do we increase the extent to which that objective is aligned with human preferences, such as via prompt design or fine-tuning?\n    \n*   **Fairness and Representation**: How should performance criteria be established for fairness and representation in language models? How can language models be improved in order to effectively support the goals of fairness and representation in specific, deployed contexts?\n    \n*   **Interdisciplinary Research**: How can AI development draw on insights from other disciplines such as philosophy, cognitive science, and sociolinguistics?\n    \n*   **Interpretability / Transparency**: How do these models work, mechanistically? Can we identify what concepts they\u2019re using, or extract latent knowledge from the model, make inferences about the training procedure, or predict surprising future behavior?\n    \n*   **Misuse Potential**: How can systems like the API be misused? What sorts of \u2018red teaming\u2019 approaches can we develop to help us and other AI developers think about responsibly deploying technologies like this?\n    \n*   **Model Exploration**: Models like those served by the API have a variety of capabilities which we have yet to explore. We\u2019re excited by investigations in many areas including model limitations, linguistic properties, commonsense reasoning, and potential uses for many other problems.\n    \n*   **Robustness**: Generative models have uneven capability surfaces, with the potential for surprisingly strong and surprisingly weak areas of capability. How robust are large generative models to \"natural\" perturbations in the prompt, such as phrasing the same idea in different ways or with/without typos? Can we predict the kinds of domains and tasks for which large generative models are more likely to be robust (or not robust), and how does this relate to the training data? Are there techniques we can use to predict and mitigate worst-case behavior? How can robustness be measured in the context of few-shot learning (e.g. across variations in prompts)? Can we train models so that they satisfy safety properties with a very high level of reliability, even under adversarial inputs?\n    \n\nPlease note that due to a high volume of requests, it takes time for us to review these applications (up to 30 business days) and not all research will be prioritized for subsidy. We will only be in touch if your application is selected for subsidy. If you have questions about the Researcher Access Program, you can get in touch with us at [researcheraccess@openai.com](mailto:researcheraccess@openai.com)."
}