{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/router.html",
        "title": "Building a Router from Scratch - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nIn this tutorial, we show you how to build an LLM-powered router module that can route a user query to submodules.\n\nRouters are a simple but effective form of automated decision making that can allowing you to perform dynamic retrieval/querying over your data.\n\nIn LlamaIndex, this is abstracted away with our [Router Modules](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/router/root.html).\n\nTo build a router, we\u2019ll walk through the following steps:\n\n*   Crafting an initial prompt to select a set of choices\n    \n*   Enforcing structured output (for text completion endpoints)\n    \n*   Try integrating with a native function calling endpoint.\n    \n\nAnd then we\u2019ll plug this into a RAG pipeline to dynamically make decisions on QA vs. summarization.\n\n## 1\\. Setup a Basic Router Prompt[\uf0c1](#setup-a-basic-router-prompt \"Permalink to this heading\")\n\nAt its core, a router is a module that takes in a set of choices. Given a user query, it \u201cselects\u201d a relevant choice.\n\nFor simplicity, we\u2019ll start with the choices as a set of strings.\n\nfrom llama\\_index import PromptTemplate\n\nchoices \\= \\[\n    \"Useful for questions related to apples\",\n    \"Useful for questions related to oranges\",\n\\]\n\ndef get\\_choice\\_str(choices):\n    choices\\_str \\= \"\\\\n\\\\n\".join(\n        \\[f\"{idx+1}. {c}\" for idx, c in enumerate(choices)\\]\n    )\n    return choices\\_str\n\nchoices\\_str \\= get\\_choice\\_str(choices)\n\nrouter\\_prompt0 \\= PromptTemplate(\n    \"Some choices are given below. It is provided in a numbered list (1 to\"\n    \" {num\\_choices}), where each item in the list corresponds to a\"\n    \" summary.\\\\n\\---------------------\\\\n{context\\_list}\\\\n\\---------------------\\\\nUsing\"\n    \" only the choices above and not prior knowledge, return the top choices\"\n    \" (no more than {max\\_outputs}, but only select what is needed) that are\"\n    \" most relevant to the question: '{query\\_str}'\\\\n\"\n)\n\nLet\u2019s try this prompt on a set of toy questions and see what the output brings.\n\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo\")\n\ndef get\\_formatted\\_prompt(query\\_str):\n    fmt\\_prompt \\= router\\_prompt0.format(\n        num\\_choices\\=len(choices),\n        max\\_outputs\\=2,\n        context\\_list\\=choices\\_str,\n        query\\_str\\=query\\_str,\n    )\n    return fmt\\_prompt\n\nquery\\_str \\= \"Can you tell me more about the amount of Vitamin C in apples\"\nfmt\\_prompt \\= get\\_formatted\\_prompt(query\\_str)\nresponse \\= llm.complete(fmt\\_prompt)\n\n1\\. Useful for questions related to apples\n\nquery\\_str \\= \"What are the health benefits of eating orange peels?\"\nfmt\\_prompt \\= get\\_formatted\\_prompt(query\\_str)\nresponse \\= llm.complete(fmt\\_prompt)\n\n2\\. Useful for questions related to oranges\n\nquery\\_str \\= (\n    \"Can you tell me more about the amount of Vitamin C in apples and oranges.\"\n)\nfmt\\_prompt \\= get\\_formatted\\_prompt(query\\_str)\nresponse \\= llm.complete(fmt\\_prompt)\n\n1\\. Useful for questions related to apples\n2. Useful for questions related to oranges\n\n**Observation**: While the response corresoponds to the correct choice, it can be hacky to parse into a structured output (e.g. a single integer). We\u2019d need to do some string parsing on the choices to extract out a single number, and make it robust to failure modes.\n\n## 2\\. A Router Prompt that can generate structured outputs[\uf0c1](#a-router-prompt-that-can-generate-structured-outputs \"Permalink to this heading\")\n\nTherefore the next step is to try to prompt the model to output a more structured representation (JSON).\n\nWe define an output parser class (`RouterOutputParser`). This output parser will be responsible for both formatting the prompt and also parsing the result into a structured object (an `Answer`).\n\nWe then apply the `format` and `parse` methods of the output parser around the LLM call using the router prompt to generate a structured output.\n\n### 2.a Import Answer Class[\uf0c1](#a-import-answer-class \"Permalink to this heading\")\n\nWe load in the Answer class from our codebase. It\u2019s a very simple dataclass with two fields: `choice` and `reason`\n\nfrom dataclasses import fields\nfrom pydantic import BaseModel\nimport json\n\nclass Answer(BaseModel):\n    choice: int\n    reason: str\n\nprint(json.dumps(Answer.schema(), indent\\=2))\n\n{\n  \"title\": \"Answer\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"choice\": {\n      \"title\": \"Choice\",\n      \"type\": \"integer\"\n    },\n    \"reason\": {\n      \"title\": \"Reason\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": \\[\n    \"choice\",\n    \"reason\"\n  \\]\n}\n\n### 2.b Define Router Output Parser[\uf0c1](#b-define-router-output-parser \"Permalink to this heading\")\n\nfrom llama\\_index.types import BaseOutputParser\n\nFORMAT\\_STR \\= \"\"\"The output should be formatted as a JSON instance that conforms to \nthe JSON schema below. \n\nHere is the output schema:\n{\n  \"type\": \"array\",\n  \"items\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"choice\": {\n        \"type\": \"integer\"\n      },\n      \"reason\": {\n        \"type\": \"string\"\n      }\n    },\n    \"required\": \\[\n      \"choice\",\n      \"reason\"\n    \\],\n    \"additionalProperties\": false\n  }\n}\n\"\"\"\n\nIf we want to put `FORMAT_STR` as part of an f-string as part of a prompt template, then we\u2019ll need to escape the curly braces so that they don\u2019t get treated as template variables.\n\ndef \\_escape\\_curly\\_braces(input\\_string: str) \\-> str:\n    \\# Replace '{' with '{{' and '}' with '}}' to escape curly braces\n    escaped\\_string \\= input\\_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n    return escaped\\_string\n\nWe now define a simple parsing function to extract out the JSON string from the LLM response (by searching for square brackets)\n\ndef \\_marshal\\_output\\_to\\_json(output: str) \\-> str:\n    output \\= output.strip()\n    left \\= output.find(\"\\[\")\n    right \\= output.find(\"\\]\")\n    output \\= output\\[left : right + 1\\]\n    return output\n\nWe put these together in our `RouterOutputParser`\n\nfrom typing import List\n\nclass RouterOutputParser(BaseOutputParser):\n    def parse(self, output: str) \\-> List\\[Answer\\]:\n        \"\"\"Parse string.\"\"\"\n        json\\_output \\= \\_marshal\\_output\\_to\\_json(output)\n        json\\_dicts \\= json.loads(json\\_output)\n        answers \\= \\[Answer.from\\_dict(json\\_dict) for json\\_dict in json\\_dicts\\]\n        return answers\n\n    def format(self, prompt\\_template: str) \\-> str:\n        return prompt\\_template + \"\\\\n\\\\n\" + \\_escape\\_curly\\_braces(FORMAT\\_STR)\n\n### 2.c Give it a Try[\uf0c1](#c-give-it-a-try \"Permalink to this heading\")\n\nWe create a function called `route_query` that will take in the output parser, llm, and prompt template and output a structured answer.\n\noutput\\_parser \\= RouterOutputParser()\n\nfrom typing import List\n\ndef route\\_query(\n    query\\_str: str, choices: List\\[str\\], output\\_parser: RouterOutputParser\n):\n    choices\\_str\n\n    fmt\\_base\\_prompt \\= router\\_prompt0.format(\n        num\\_choices\\=len(choices),\n        max\\_outputs\\=len(choices),\n        context\\_list\\=choices\\_str,\n        query\\_str\\=query\\_str,\n    )\n    fmt\\_json\\_prompt \\= output\\_parser.format(fmt\\_base\\_prompt)\n\n    raw\\_output \\= llm.complete(fmt\\_json\\_prompt)\n    parsed \\= output\\_parser.parse(str(raw\\_output))\n\n    return parsed\n\n## 3\\. Perform Routing with a Function Calling Endpoint[\uf0c1](#perform-routing-with-a-function-calling-endpoint \"Permalink to this heading\")\n\nIn the previous section, we showed how to build a router with a text completion endpoint. This includes formatting the prompt to encourage the model output structured JSON, and a parse function to load in JSON.\n\nThis process can feel a bit messy. Function calling endpoints (e.g. OpenAI) abstract away this complexity by allowing the model to natively output structured functions. This obviates the need to manually prompt + parse the outputs.\n\nLlamaIndex offers an abstraction called a `PydanticProgram` that integrates with a function endpoint to produce a structured Pydantic object. We integrate with OpenAI and Guidance.\n\nWe redefine our `Answer` class with annotations, as well as an `Answers` class containing a list of answers.\n\nfrom pydantic import Field\n\nclass Answer(BaseModel):\n    \"Represents a single choice with a reason.\"\n    choice: int\n    reason: str\n\nclass Answers(BaseModel):\n    \"\"\"Represents a list of answers.\"\"\"\n\n    answers: List\\[Answer\\]\n\n{'title': 'Answers',\n 'description': 'Represents a list of answers.',\n 'type': 'object',\n 'properties': {'answers': {'title': 'Answers',\n   'type': 'array',\n   'items': {'$ref': '#/definitions/Answer'}}},\n 'required': \\['answers'\\],\n 'definitions': {'Answer': {'title': 'Answer',\n   'description': 'Represents a single choice with a reason.',\n   'type': 'object',\n   'properties': {'choice': {'title': 'Choice', 'type': 'integer'},\n    'reason': {'title': 'Reason', 'type': 'string'}},\n   'required': \\['choice', 'reason'\\]}}}\n\nfrom llama\\_index.program import OpenAIPydanticProgram\n\nrouter\\_prompt1 \\= router\\_prompt0.partial\\_format(\n    num\\_choices\\=len(choices),\n    max\\_outputs\\=len(choices),\n)\n\nprogram \\= OpenAIPydanticProgram.from\\_defaults(\n    output\\_cls\\=Answers,\n    prompt\\=router\\_prompt1,\n    verbose\\=True,\n)\n\nquery\\_str \\= \"What are the health benefits of eating orange peels?\"\noutput \\= program(context\\_list\\=choices\\_str, query\\_str\\=query\\_str)\n\nFunction call: Answers with args: {\n  \"answers\": \\[\n    {\n      \"choice\": 2,\n      \"reason\": \"Orange peels are related to oranges\"\n    }\n  \\]\n}\n\nAnswers(answers=\\[Answer(choice=2, reason='Orange peels are related to oranges')\\])\n\n## 4\\. Plug Router Module as part of a RAG pipeline[\uf0c1](#plug-router-module-as-part-of-a-rag-pipeline \"Permalink to this heading\")\n\nIn this section we\u2019ll put the router module to use in a RAG pipeline. We\u2019ll use it to dynamically decide whether to perform question-answering or summarization. We can easily get a question-answering query engine using top-k retrieval through our vector index, while summarization is performed through our summary index. Each query engine is described as a \u201cchoice\u201d to our router, and we compose the whole thing into a single query engine.\n\n### Setup: Load Data[\uf0c1](#setup-load-data \"Permalink to this heading\")\n\nWe load the Llama 2 paper as data.\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nmkdir: data: File exists\n--2023-09-17 23:37:11--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 128.84.21.199\nConnecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) \\[application/pdf\\]\nSaving to: \u2018data/llama2.pdf\u2019\n\ndata/llama2.pdf     100%\\[===================>\\]  13.03M  1.50MB/s    in 9.5s    \n\n2023-09-17 23:37:22 (1.37 MB/s) - \u2018data/llama2.pdf\u2019 saved \\[13661300/13661300\\]\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocuments \\= loader.load(file\\_path\\=\"./data/llama2.pdf\")\n\n### Setup: Define Indexes[\uf0c1](#setup-define-indexes \"Permalink to this heading\")\n\nDefine both a vector index and summary index over this data.\n\nfrom llama\\_index import ServiceContext, VectorStoreIndex, SummaryIndex\n\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=1024)\nvector\\_index \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\nsummary\\_index \\= SummaryIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\nvector\\_query\\_engine \\= vector\\_index.as\\_query\\_engine()\nsummary\\_query\\_engine \\= summary\\_index.as\\_query\\_engine()\n\n### Define RouterQueryEngine[\uf0c1](#define-routerqueryengine \"Permalink to this heading\")\n\nWe subclass our `CustomQueryEngine` to define a custom router.\n\nfrom llama\\_index.query\\_engine import CustomQueryEngine, BaseQueryEngine\nfrom llama\\_index.response\\_synthesizers import TreeSummarize\n\nclass RouterQueryEngine(CustomQueryEngine):\n    \"\"\"Use our Pydantic program to perform routing.\"\"\"\n\n    query\\_engines: List\\[BaseQueryEngine\\]\n    choice\\_descriptions: List\\[str\\]\n    verbose: bool \\= False\n    router\\_prompt: PromptTemplate\n    llm: OpenAI\n    summarizer: TreeSummarize \\= Field(default\\_factory\\=TreeSummarize)\n\n    def custom\\_query(self, query\\_str: str):\n        \"\"\"Define custom query.\"\"\"\n\n        program \\= OpenAIPydanticProgram.from\\_defaults(\n            output\\_cls\\=Answers,\n            prompt\\=router\\_prompt1,\n            verbose\\=self.verbose,\n            llm\\=self.llm,\n        )\n\n        choices\\_str \\= get\\_choice\\_str(self.choice\\_descriptions)\n        output \\= program(context\\_list\\=choices\\_str, query\\_str\\=query\\_str)\n        \\# print choice and reason, and query the underlying engine\n        if self.verbose:\n            print(f\"Selected choice(s):\")\n            for answer in output.answers:\n                print(f\"Choice: {answer.choice}, Reason: {answer.reason}\")\n\n        responses \\= \\[\\]\n        for answer in output.answers:\n            choice\\_idx \\= answer.choice \\- 1\n            query\\_engine \\= self.query\\_engines\\[choice\\_idx\\]\n            response \\= query\\_engine.query(query\\_str)\n            responses.append(response)\n\n        \\# if a single choice is picked, we can just return that response\n        if len(responses) \\== 1:\n            return responses\\[0\\]\n        else:\n            \\# if multiple choices are picked, we can pick a summarizer\n            response\\_strs \\= \\[str(r) for r in responses\\]\n            result\\_response \\= self.summarizer.get\\_response(\n                query\\_str, response\\_strs\n            )\n            return result\\_response\n\nchoices \\= \\[\n    (\n        \"Useful for answering questions about specific sections of the Llama 2\"\n        \" paper\"\n    ),\n    \"Useful for questions that ask for a summary of the whole paper\",\n\\]\n\nrouter\\_query\\_engine \\= RouterQueryEngine(\n    query\\_engines\\=\\[vector\\_query\\_engine, summary\\_query\\_engine\\],\n    choice\\_descriptions\\=choices,\n    verbose\\=True,\n    router\\_prompt\\=router\\_prompt1,\n    llm\\=OpenAI(model\\=\"gpt-4\"),\n)\n\n### Try our constructed Router Query Engine[\uf0c1](#try-our-constructed-router-query-engine \"Permalink to this heading\")\n\nLet\u2019s take our self-built router query engine for a spin! We ask a question that routes to the vector query engine, and also another question that routes to the summarization engine.\n\nresponse \\= router\\_query\\_engine.query(\n    \"How does the Llama 2 model compare to GPT-4 in the experimental results?\"\n)\n\nFunction call: Answers with args: {\n  \"answers\": \\[\n    {\n      \"choice\": 1,\n      \"reason\": \"This question is asking for specific information about the Llama 2 model and its comparison to GPT-4 in the experimental results. Therefore, the summary that is useful for answering questions about specific sections of the paper would be most relevant.\"\n    }\n  \\]\n}\nSelected choice(s):\nChoice: 1, Reason: This question is asking for specific information about the Llama 2 model and its comparison to GPT-4 in the experimental results. Therefore, the summary that is useful for answering questions about specific sections of the paper would be most relevant.\n\nThe Llama 2 model performs better than GPT-4 in the experimental results.\n\nresponse \\= router\\_query\\_engine.query(\"Can you give a summary of this paper?\")\n\nFunction call: Answers with args: {\n  \"answers\": \\[\n    {\n      \"choice\": 2,\n      \"reason\": \"This choice is directly related to providing a summary of the whole paper, which is what the question asks for.\"\n    }\n  \\]\n}\nSelected choice(s):\nChoice: 2, Reason: This choice is directly related to providing a summary of the whole paper, which is what the question asks for."
}