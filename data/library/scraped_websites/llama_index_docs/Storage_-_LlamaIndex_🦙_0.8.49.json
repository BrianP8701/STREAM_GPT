{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/root.html",
        "title": "Storage - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Storage[\uf0c1](#storage \"Permalink to this heading\")\n\n## Concept[\uf0c1](#concept \"Permalink to this heading\")\n\nLlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n\nUnder the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n\n*   **Document stores**: where ingested documents (i.e., `Node` objects) are stored,\n    \n*   **Index stores**: where index metadata are stored,\n    \n*   **Vector stores**: where embedding vectors are stored.\n    \n*   **Graph stores**: where knowledge graphs are stored (i.e. for `KnowledgeGraphIndex`).\n    \n\nThe Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.\n\nLlamaIndex supports persisting data to any storage backend supported by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html). We have confirmed support for the following storage backends:\n\n*   Local filesystem\n    \n*   AWS S3\n    \n*   Cloudflare R2\n    \n\n![](https://docs.llamaindex.ai/en/stable/_images/storage.png)\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\nMany vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This _also_ means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.\n\n\\## build a new index\nfrom llama\\_index import VectorStoreIndex, StorageContext\nfrom llama\\_index.vector\\_stores import DeepLakeVectorStore\n\\# construct vector store and customize storage context\nvector\\_store \\= DeepLakeVectorStore(dataset\\_path\\=\"<dataset\\_path>\")\nstorage\\_context \\= StorageContext.from\\_defaults(\n    vector\\_store \\= vector\\_store\n)\n\\# Load documents and build index\nindex \\= VectorStoreIndex.from\\_documents(documents, storage\\_context\\=storage\\_context)\n\n\\## reload an existing one\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store\\=vector\\_store)\n\nSee our [Vector Store Module Guide](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/vector_stores.html) below for more details.\n\nNote that in general to use storage abstractions, you need to define a `StorageContext` object:\n\nfrom llama\\_index.storage.docstore import SimpleDocumentStore\nfrom llama\\_index.storage.index\\_store import SimpleIndexStore\nfrom llama\\_index.vector\\_stores import SimpleVectorStore\nfrom llama\\_index.storage import StorageContext\n\n\\# create storage context using default stores\nstorage\\_context \\= StorageContext.from\\_defaults(\n    docstore\\=SimpleDocumentStore(),\n    vector\\_store\\=SimpleVectorStore(),\n    index\\_store\\=SimpleIndexStore(),\n)\n\nMore details on customization/persistence can be found in the guides below.\n\n*   [Customizing Storage](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/customization.html)\n*   [Persisting & Loading Data](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/save_load.html)"
}