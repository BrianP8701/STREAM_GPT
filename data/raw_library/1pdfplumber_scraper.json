["STREAM AI1 1\nReal-Time, Multithreaded Anomaly Detection System with Autonomous\nSynchronization & Self-Correction\nBrian Przezdziecki\nUniversityof Buffalo(SUNY)", "STREAM AI1 2\nAbstract\nReal-time anomaly detectionis crucial duringthe printingprocess, especiallywhen\nlooking for issues likeoveror under extrusion.We employa MobileNetmodel toclassifythese\nanomalies. Oursystem has twomain components:TipTracking andAnomaly Detection.In the\nTip Trackingphase, weestimate thelocations ofthe tips usinggcode data.Then,weuse the\nYOLOmodel toalign these predictionswithlive videofootage, ensuringsynchronizationand\ncorrecting anyprediction inaccuracies.Giventhe needfor real-time performance onaRaspberry\nPi, our system isoptimized for computationalefficiency.It'sdesignedtooperate inreal-time,\nrunning alongsidevideofeeds, andstrikes abalancebetween accuracy, autonomy,robustness and\nspeed.", "STREAM AI1 3\nProblem\nWeneedto spoterrors in3D printingas theyhappen. Themainmistakes aretied tohow\nmuchmaterial comesout- toomuchor toolittle (Under/OverExtrusion).\nClasses:\nOver Normal Under\nAnother problemarises:Givena frame,the onlyrelevant informationis theareaaround the tipin\nthedirection of recentlyextruded material.Thisis why weneedtotrack thetip.", "STREAM AI1 4\nSolution\nThegivendiagramdisplayshow the systemisdivided into distinctthreads andillustrates\ntheprocessflow.Allthreadsaccess someshared globalvariables.Thetrackerthread initiatesand\nmanages allotherthreads. Thissystemprimarilyrevolves around threekeycomponents:\n1. Initialization:Predicting the pathof tiptracking andaligningwiththe video.\n2. ErrorCorrection:Correcting errorsthrough afeedbackloop.\n3. Analytics: Identifyinganomalies.\nFor thefirsttwosteps,we utilizeafine-tunedYolov8. Forthe anomalyidentification, werelyon\nafine-tuned MobileNetv3 model.\nWhyweneedTip Tracking&Examplesof Videoand Anomalies", "STREAM AI1 5\nChallenges\nThis systemisdesignedwithsimplicityand versatilityinmind. Ourgoal istomake it\nreadilyadoptable for anyone,ensuring itcan seamlesslyintegratewithvarious3D printersin\ndiversesettings.There's noneed for aprecise camera placement,giving usersflexibility.The\nsystem adjuststo differentresolutions,simplifying setup processes.\nIt's builtto workwith anyprinter that operatesong-code,even if theg-code\ninterpretations vary.Withedge deploymentcapabilities,it canrun onaRaspberryPi, streaming\nin realtime.Given thattheYOLOmodel mayoccasionallyerr inobjectdetection,our systemis\nmade toaccountfor thesediscrepancies. Moreover,userswill benefitfrom aclearvisualization\nof thetip trackingprocedure.\nGiven theseconstraints andrequirements, theensuing challenges andourtailored\nsolutions will beelaboratedbelow.", "STREAM AI1 6\nModel Training\nhttps://github.com/BrianP8701/Anomaly_Classification\nObject Detectionfor TipTracking\nWeutilizedtheYOLOv8s model, aprominentobjectdetection system,andfine-tunedit\nthrough Roboflow,a platform that streamlinesthe computervision data process.\nInputSize DatasetSize Augmentations Precision Recall MAP50\n640x640 2407 1200 0.967 0.931 0.968\nWeaddedonto our datasetand retrainedmultipletimes, aimingtofillingaps and\nweaknessesof themodel, adding nullimages andwrongdetections etc. Weapplied\naugmentationsof blur,noise andbrightness.", "STREAM AI1 7\nObject Classification for Anomalies\nWetested variousmodels,settings, andpreprocessingmethodsto findthe best model.\nOur dataislimited becauseonly afewframesin eachvideo showover orunder extrusion,\nmaking datacollection slow.Here's ourdata pipeline:\nExamining thepipeline: Idevelopedseveral GUIsto addressthe initialdata collection\nbottleneck.I optedfor an 85x85 crop sizeasit bestcaptured materialnearthe extrusiontip. I\nusedgrayscaleto reducedimensions. Insome images,edges andmaterialarebarely visible.\nTherefore,a filter wasusedinpreprocessing toenhancethesecrucial edges.\nI initiallyattempted afilter that segmented theimageinto\ndistinct color intensities,usingthe originalimage's colorintensity\nstandard deviation.Whilesomewhateffective,it missedthe\nsubtlest edges.", "STREAM AI1 8\nDrawing inspiration fromtheconvolutional operations inCNNs,I\nexperimentedwith variouskernelsand matrixfilter combinations onthe\nimages.However,nosinglecombination consistentlyworkedacross all\nimages.I testedstandard filterssuchas theNon-local MeansDenoising\nfromcv2 andtheUnsharp Mask fromPIL. Yet,neither succeededin\nenhancingthe faintedges.\nAn observationwas made, eachimagefeatures alimitedset of objectsandhues: material,\ntip, bed, shadows,andgleam. Eachcolorintensityin theimageuniquely follows anormal\ndistribution with itsdistinct meanand variance.Hence, GaussianMixtureModelsareapt for this\ntask.", "STREAM AI1 9\nUsing GaussianMixture Models,wecansharply defineeventhe subtlestedges. I tested\nvarious componentnumbers, indicatingthe assumedGaussiansinthe mixture.Next, we\nprioritize sensitivityaroundbrighter colorssinceit'schallenging todiscern edgeswhenmaterial\naccumulates. Thisfocusignores thedarker bed,whichisn'tof concern.Thisfinal refinement\nresulted inour preprocessingstep, dubbedGMMS.\nThrough experimentation,GMMS6is foundtoperform the best whentrainingmodels.", "STREAM AI1 10\nBacktoexamining thepipeline,the\nimagesareresized to224x224,suitable\nfor EfficientNet,MobileNet,and\nResNet,which havetrained onthe\n224x224-sized ImageNetdataset,\nmaking them adeptat recognizing\nfeaturesof this scale.Post-resizing,the\nimages undergoaugmentation andaresplitintotraining, validation,and testdatasets.\nDataset Summary:\nUnder:436images | Normal:463images | Over: 446images\nExtrusion ClassificationDataset\nAugmentationsfor each class includerotations,brightness, saturation,contrast, hue\nadjustments,and flips(bothhorizontal andvertical) creating 60newimagesper class.Thedata\ndistribution is75% for training, 15%for validation, and10%for testing.\nTraining Summary:\nFine tuning, modelcheckpointing,4batches,~12epochs, learning ratedecay,early stopping.\nlearning_rate=0.01,momentum=0.9, step_size=5,gamma=0.1\nAfter trialsof various models,settings,hyperparameters andpreprocessing steps,the\ncombinationof thedescribedpreprocessing andtraining proceduresoutlinedaboveyieldedthe\nhighestaccuracy.Thestandout wasafinetuned MobileNetv3Largemodel, whichnot onlywon\nin accuracybut alsohad superiorcomputationalefficiency.", "STREAM AI1 11\nTothe leftshowsoneof the\ncomparison rounds, whereweidentified\nour bestperforming model.Here we\ncompare 3differentsettingsand their\ncombinations:Finetuning\nMobileNetv3SmallvsMobileNetv3Large.\nUsingnoGMM preprocessingvsusing\nGMMpreprocessing vsusing GMMS\npreprocessing withadditionalsensitivity\ntomoreintense pixels.\nTheresults arefitting,showing that\nindeed usingGMMSwith extra\nsensitivityis beneficial,andtraining the\nlarger modelresults inbetter\nperformance. Thetrainingand validation\nmetricsarenearlyindistinguishable\nindicating thatit isnot overfitting.\nBest performingmodel accuracy:\n0.9025974025974026", "STREAM AI1 12\nImplementation\nIn this sectionI will describethe design choicesanddetails ofthe system.\nhttps://github.com/BrianP8701/STREAM.AI\nInput\n1. G-Code:\n- Instruction setfor 3D print.\n- Simple andsequential:it runsfrom starttofinish.\n- Specifies thetopspeedat anymomentandprovides movementcoordinates.\n- Theprinter moves directlyfrom onepointto thenext withoutdeviation.\n2. VideoStream:\n- Live footageof the 3Dprinter bed.\n- Theangle of thecamera relativetotheprinter is expectedtoberoughly consistent.\n3. Signal Stream:\n- ManagedbytheSKR board,which controlsthe 3Dprinter.\n- A hardwareconnection linksthe SKRboard toourRaspberry Pi.\n- TheRaspberry Pi getsasignaleverytimea movementcompletes. Thisisthe signal,and\nconsistsof thebed positionandtime.[time, x,y,z]\n- This datastreamsinreal time,paralleltotheprinting process.\nInitialization\nFirst and foremost,the system,starting inthe mainTracker Thread, beginsreceiving and\nrouting framesand signals throughtheir correspondingrouters.These routers willpass thedata\nto the correctobjectsand threads through thesharedglobal variable spacebased onthecurrent\nstate of thesystem(Initializing orTracking).", "STREAM AI1 13\nTheduty of theinitializationphase isto autonomouslyadapt tothecamera position.It\nprimarily performsthesetasks:\n1. Make initialpredictions:\na. Totransform G-codeinto predictionsfor every\nmomentintime,wefirst breakdown theG-code\ninto a sequenceof motioninstructions between\ncorners (or waypoints)where therearechangesin\ndirection orspeed.\nb. For eachof thesemotions, wecalculatetheoptimal\nspeed profile consideringconstraints likemaximum\nspeedsandacceleration. We needtolook atthe future2moves aswell, to\ndeterminewhatthe finalspeed atthe endof themotion shouldbe.\nc. Given theoptimalspeed profilesfor each motion,we\u2019llcalculatehow longeach\nmoveshouldtake usingkinematics.Oneproblem cannotbe solvedanalytically,\nand needstobe solvediteratively. Theiterativemethod tweaksthe distanceover\nwhich weaccelerateor decelerate,attempting toconvergeonthe desiredfinal\nspeed withoutexceeding themachine's constraints.\nd. After computingthe speed profile,the scriptthen dividesthemotion into frames\n(based onthe givenframes-per-second parameter)andpredicts thepositionand\nangle of the machine's tipfor each frame. Bythe endofthis process, wehavea\ndetailedaccount ofwhere themachine shouldbeand atwhatangle for every\nsingleframe ofthe motion.\n2. Derive millimeterto pixelratio:\nFor robustness,the systemwillautonomously mapbedpredictionsto screen\npredictionswithout requiringusers toenteranyconfiguring variables.By waiting\nfor twosignalsand observingtheirmillimeter disparity onthebed,wecan\ncorrelatethiswith thepixel differenceYOLOdetects, facilitating thenecessary\nconversion. Weslidea640x640window across theimagetorun YOLO.\n3. Synchronizepredictions withvideo:\nToinitiate theprocess, wealign ourscreen predictionsusingthe timestampofthe\nfirst YOLO-signalinference fromthevideo.", "STREAM AI1 14\nPredictions areimperfectdue toaccumulatedsmalltracking errors.Reasonsinclude:\n1. Merlinfirmware inour 3Dprinter employsBeziet\u2019s\n6th orderfunction for movementextrapolation,while\nour algorithm usesasimpler 2ndorderfunction.\nHowever,our modelconsiders acceleration.\n2. Some printersadjustmovement factors,likespeed\nand acceleration.\n3. Minordeviationsarisewhenthe printerhalts.\n4. MerlinFirmware has intricatemechanismsfor gcode\ninterpretation.\nInsteadof replicating Merlin'sexactapproach, wedesignedastraightforwardinterpreterwith\nreal-time errorfeedbackloops, enhancingsystemrobustness andcompatibilitywithvarious 3D\nprinters.\nErrorCorrection\nPredictions facetwoerror types:Temporaland Spatial,addressedbythe ErrorCorrection thread.\n1.SpatialError: Mainlyarisesfrom incorrectratiosor YOLOinference.These arerare,typically\nminor initial trackingdeviations. Fordemonstration,anexaggeratederror wasintentionally\nintroducedin theSpatialError Correctionvideo.\nMeasure Detect Correct\nEverytime a signalarrives, werun Whenthere isaconsistentspatial error Shift all\nYOLOto seewhere thetip is.We of thesamemagnitude and directionfor predictionsby\ncomparethis toour prediction and sufficient time. spatialerror.\nmeasure thedifferenceinpixels.\n2.TemporalError: This was asignificantchallenge. Withoutinput signals,gaugingtemporal\nerror wastough. Theultimatesolutionsimplyinvolved immediatemeasurementand correction", "STREAM AI1 15\nof temporal discrepancies.Thiserror stemsfrom varyinginterpretationsofgcode, especially\ndiffering accelerations.\nMeasure Detect Correct\nEverytime a signalarrives, we Wedon\u2019tlook for anyconsistent Pauseor skip\ncomparethetimeof thesignal, errors.Whenthere isanerror, we ahead torealign.\ncomparedto our predictions.We simplyperform a correction.\nmeasure errorin seconds.\nAnalytics\nIncorporating a real-timefeedbackloop(autonomous corrections)results innear-flawlesstip\ntracking.Using YOLOisn't constant;it'semployedduring signalsand errormeasurements. This\nenables efficientpinpointing ofthe recentlyextruded materialaround thetip\u2014our target.Given\nour knowledgefrom thegcodeabout theprinter's movementdirection, wecropaccordingly. This\ncroppedimagethenentersour datapreprocessing pipelineandundergoesclassification bythe\ntailored MobileNetmodel, determiningwhetherextrusionis under,normal,or over.", "STREAM AI1 16\nMetrics\nToevaluateour system,we'll focusonits computationalefficiency,whilethe accuracy is\nderived fromtheMobileNetmodel's testperformance.We'll assess:\n- RAM Usage: We'll monitorRAMconsumption overtime.\n- InferenceIntervals:Giventhe adaptivenatureof ourMobileNet model,observing the\nintervalsbetween successive inferencesprovides ameasure ofsystemspeed. Themodel\nrefrainsfrom inferringwitha full buffer.\nThesemetricswill begauged onbothmydeviceand aRaspberryPi 4,withrespective\nspecificationsprovidedbelow.\nDeploymentto Raspberry Pi\nRaspberry Pi Metrics\nFuture Improvements", "STREAM AI1 17\n1.Processor (Chipset)Details:\n- Model:Apple M1Pro\n- Total Numberof Cores:14\n2.Memory:\n- Size: 16GB\n- Type:LPDDR5\n- Manufacturer: Hynix\n3.Graphics/GPU:\n- ChipsetModel:AppleM1 Pro\n- Metal Support: Metal3\n4.Display:\n- Type:Built-in LiquidRetina XDRDisplay\n- Resolution:3024 x1964Retina\n5.Storage:\n- Type:SSD (APPLE SSDAP0512R)\n- Capacity:500.28 GB\n- Protocol: AppleFabric(indicatingit'sanNVMeSSD)\n- Free Space:120.4GB\n6.File System:\n- Type:APFS\nhttps://qengineering.eu/install-opencv-on-raspberry-pi.html\nhttps://qengineering.eu/install-pytorch-on-raspberry-pi-4.html\nhttps://gist.github.com/wenig/8bab88dede5c838660dd05b8e5b2e23b\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html"]