{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo.html",
        "title": "Milvus Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Milvus Vector Store[\uf0c1](#milvus-vector-store \"Permalink to this heading\")\n\nIn this notebook we are going to show a quick demo of using the MilvusVectorStore.\n\nimport logging\nimport sys\n\n\\# Uncomment to see debug logs\n\\# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, Document\nfrom llama\\_index.vector\\_stores import MilvusVectorStore\nfrom IPython.display import Markdown, display\nimport textwrap\n\n## Setup OpenAI[\uf0c1](#setup-openai \"Permalink to this heading\")\n\nLets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt.\n\nimport openai\n\nopenai.api\\_key \\= \"sk-\"\n\n## Generate our data[\uf0c1](#generate-our-data \"Permalink to this heading\")\n\nWith our LLM set, lets start using the Milvus Index. As a first example, lets generate a document from the file found in the `paul_graham_essay/data` folder. In this folder there is a single essay from Paul Graham titled `What I Worked On`. To generate the documents we will use the SimpleDirectoryReader.\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\n\nprint(\"Document ID:\", documents\\[0\\].doc\\_id)\n\nDocument ID: d33f0397-b51a-4455-9b0f-88a101254d95\n\n## Create an index across the data[\uf0c1](#create-an-index-across-the-data \"Permalink to this heading\")\n\nNow that we have a document, we can can create an index and insert the document. For the index we will use a GPTMilvusIndex. GPTMilvusIndex takes in a few arguments:\n\n*   collection\\_name (str, optional): The name of the collection where data will be stored. Defaults to \u201cllamalection\u201d.\n    \n*   index\\_params (dict, optional): The index parameters for Milvus, if none are provided an HNSW index will be used. Defaults to None.\n    \n*   search\\_params (dict, optional): The search parameters for a Milvus query. If none are provided, default params will be generated. Defaults to None.\n    \n*   dim (int, optional): The dimension of the embeddings. If it is not provided, collection creation will be done on first insert. Defaults to None.\n    \n*   host (str, optional): The host address of Milvus. Defaults to \u201clocalhost\u201d.\n    \n*   port (int, optional): The port of Milvus. Defaults to 19530.\n    \n*   user (str, optional): The username for RBAC. Defaults to \u201c\u201d.\n    \n*   password (str, optional): The password for RBAC. Defaults to \u201c\u201d.\n    \n*   use\\_secure (bool, optional): Use https. Defaults to False.\n    \n*   overwrite (bool, optional): Whether to overwrite existing collection with same name. Defaults to False.\n    \n\n\\# Create an index over the documnts\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\nvector\\_store \\= MilvusVectorStore(dim\\=1536, overwrite\\=True)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\n## Query the data[\uf0c1](#query-the-data \"Permalink to this heading\")\n\nNow that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for chatgpt.\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author learn?\")\nprint(textwrap.fill(str(response), 100))\n\nThe author learned several things during their time at Interleaf. They learned that it's better for\ntechnology companies to be run by product people than sales people, that code edited by too many\npeople leads to bugs, that cheap office space is not worth it if it's depressing, that planned\nmeetings are inferior to corridor conversations, that big bureaucratic customers can be a dangerous\nsource of money, and that there's not much overlap between conventional office hours and the optimal\ntime for hacking. However, the most important thing the author learned is that the low end eats the\nhigh end, meaning that it's advantageous to be the \"entry level\" option because if you're not,\nsomeone else will be and will surpass you.\n\nresponse \\= query\\_engine.query(\"What was a hard moment for the author?\")\nprint(textwrap.fill(str(response), 100))\n\nThe author experienced a difficult moment when their mother had a stroke and was put in a nursing\nhome. The stroke destroyed her balance, and the author and their sister were determined to help her\nget out of the nursing home and back to her house.\n\nThis next test shows that overwriting removes the previous data.\n\nvector\\_store \\= MilvusVectorStore(dim\\=1536, overwrite\\=True)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    \\[Document(text\\=\"The number that is being searched for is ten.\")\\],\n    storage\\_context,\n)\nquery\\_engine \\= index.as\\_query\\_engine()\nres \\= query\\_engine.query(\"Who is the author?\")\nprint(\"Res:\", res)\n\nRes: I'm sorry, but based on the given context information, there is no information provided about the author.\n\nThe next test shows adding additional data to an already existing index.\n\ndel index, vector\\_store, storage\\_context, query\\_engine\n\nvector\\_store \\= MilvusVectorStore(overwrite\\=False)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\nquery\\_engine \\= index.as\\_query\\_engine()\nres \\= query\\_engine.query(\"What is the number?\")\nprint(\"Res:\", res)\n\nres \\= query\\_engine.query(\"Who is the author?\")\nprint(\"Res:\", res)\n\nRes: The author of the given context is Paul Graham."
}