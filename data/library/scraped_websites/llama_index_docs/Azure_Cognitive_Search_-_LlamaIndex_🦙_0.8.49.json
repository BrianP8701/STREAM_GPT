{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/CognitiveSearchIndexDemo.html",
        "title": "Azure Cognitive Search - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Azure Cognitive Search[\uf0c1](#azure-cognitive-search \"Permalink to this heading\")\n\n## Basic Example[\uf0c1](#basic-example \"Permalink to this heading\")\n\nIn this basic example, we take a Paul Graham essay, split it into chunks, embed it using an OpenAI embedding model, load it into an Azure Cognitive Search index, and then query it.\n\nimport logging\nimport sys\nfrom IPython.display import Markdown, display\n\n\\# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\\# logger = logging.getLogger(\\_\\_name\\_\\_)\n\n#!{sys.executable} -m pip install llama-index\n#!{sys.executable} -m pip install azure-search-documents==11.4.0b8\n#!{sys.executable} -m pip install azure-identity\n\n\\# set up OpenAI\nimport os\nimport getpass\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= getpass.getpass(\"OpenAI API Key:\")\nimport openai\n\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n\\# set up Azure Cognitive Search\nfrom azure.search.documents.indexes import SearchIndexClient\nfrom azure.search.documents import SearchClient\nfrom azure.core.credentials import AzureKeyCredential\n\nsearch\\_service\\_name \\= getpass.getpass(\"Azure Cognitive Search Service Name\")\n\nkey \\= getpass.getpass(\"Azure Cognitive Search Key\")\n\ncognitive\\_search\\_credential \\= AzureKeyCredential(key)\n\nservice\\_endpoint \\= f\"https://{search\\_service\\_name}.search.windows.net\"\n\n\\# Index name to use\nindex\\_name \\= \"quickstart\"\n\n\\# Use index client to demonstrate creating an index\nindex\\_client \\= SearchIndexClient(\n    endpoint\\=service\\_endpoint,\n    credential\\=cognitive\\_search\\_credential,\n)\n\n\\# Use search client to demonstration using existing index\nsearch\\_client \\= SearchClient(\n    endpoint\\=service\\_endpoint,\n    index\\_name\\=index\\_name,\n    credential\\=cognitive\\_search\\_credential,\n)\n\n## Create Index (if it does not exist)[\uf0c1](#create-index-if-it-does-not-exist \"Permalink to this heading\")\n\nDemonstrates creating a vector index named quickstart01 if one doesn\u2019t exist. The index has the following fields:\n\n*   id (Edm.String)\n    \n*   content (Edm.String)\n    \n*   embedding (Edm.SingleCollection)\n    \n*   li\\_jsonMetadata (Edm.String)\n    \n*   li\\_doc\\_id (Edm.String)\n    \n*   author (Edm.String)\n    \n*   theme (Edm.String)\n    \n*   director (Edm.String)\n    \n\nfrom azure.search.documents import SearchClient\nfrom llama\\_index.vector\\_stores import CognitiveSearchVectorStore\nfrom llama\\_index.vector\\_stores.cogsearch import (\n    IndexManagement,\n    MetadataIndexFieldType,\n    CognitiveSearchVectorStore,\n)\n\n\\# Example of a complex mapping, metadata field 'theme' is mapped to a differently name index field 'topic' with its type explicitly set\nmetadata\\_fields \\= {\n    \"author\": \"author\",\n    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n    \"director\": \"director\",\n}\n\n\\# A simplified metadata specification is available if all metadata and index fields are similarly named\n\\# metadata\\_fields = {\"author\", \"theme\", \"director\"}\n\nvector\\_store \\= CognitiveSearchVectorStore(\n    search\\_or\\_index\\_client\\=index\\_client,\n    index\\_name\\=index\\_name,\n    filterable\\_metadata\\_field\\_keys\\=metadata\\_fields,\n    index\\_management\\=IndexManagement.CREATE\\_IF\\_NOT\\_EXISTS,\n    id\\_field\\_key\\=\"id\",\n    chunk\\_field\\_key\\=\"content\",\n    embedding\\_field\\_key\\=\"embedding\",\n    metadata\\_string\\_field\\_key\\=\"li\\_jsonMetadata\",\n    doc\\_id\\_field\\_key\\=\"li\\_doc\\_id\",\n)\n\n\\# define embedding function\nfrom llama\\_index.embeddings import OpenAIEmbedding\nfrom llama\\_index import (\n    SimpleDirectoryReader,\n    StorageContext,\n    ServiceContext,\n    VectorStoreIndex,\n)\n\nembed\\_model \\= OpenAIEmbedding()\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\n    \"../../../examples/paul\\_graham\\_essay/data\"\n).load\\_data()\n\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context, service\\_context\\=service\\_context\n)\n\n\\# Query Data\nquery\\_engine \\= index.as\\_query\\_engine(similarity\\_top\\_k\\=3)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author wrote short stories and programmed on an IBM 1401 computer during their time in school. They later got their own microcomputer, a TRS-80, and started programming games and a word processor.**\n\nresponse \\= query\\_engine.query(\n    \"What did the author learn?\",\n)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author learned several things during their time at Interleaf. They learned that it\u2019s better for technology companies to be run by product people than sales people, that code edited by too many people leads to bugs, that cheap office space is not worth it if it\u2019s depressing, that planned meetings are inferior to corridor conversations, that big bureaucratic customers can be a dangerous source of money, and that there\u2019s not much overlap between conventional office hours and the optimal time for hacking. However, the most important thing the author learned is that the low end eats the high end, meaning that it\u2019s better to be the \u201centry level\u201d option because if you\u2019re not, someone else will be and will surpass you.**\n\n## Use Existing Index[\uf0c1](#use-existing-index \"Permalink to this heading\")\n\nfrom llama\\_index.vector\\_stores import CognitiveSearchVectorStore\nfrom llama\\_index.vector\\_stores.cogsearch import (\n    IndexManagement,\n    MetadataIndexFieldType,\n    CognitiveSearchVectorStore,\n)\n\nindex\\_name \\= \"quickstart\"\n\nmetadata\\_fields \\= {\n    \"author\": \"author\",\n    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n    \"director\": \"director\",\n}\nvector\\_store \\= CognitiveSearchVectorStore(\n    search\\_or\\_index\\_client\\=search\\_client,\n    filterable\\_metadata\\_field\\_keys\\=metadata\\_fields,\n    index\\_management\\=IndexManagement.NO\\_VALIDATION,\n    id\\_field\\_key\\=\"id\",\n    chunk\\_field\\_key\\=\"content\",\n    embedding\\_field\\_key\\=\"embedding\",\n    metadata\\_string\\_field\\_key\\=\"li\\_jsonMetadata\",\n    doc\\_id\\_field\\_key\\=\"li\\_doc\\_id\",\n)\n\n\\# define embedding function\nfrom llama\\_index.embeddings import OpenAIEmbedding\nfrom llama\\_index import (\n    SimpleDirectoryReader,\n    StorageContext,\n    ServiceContext,\n    VectorStoreIndex,\n)\n\nembed\\_model \\= OpenAIEmbedding()\n\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\nindex \\= VectorStoreIndex.from\\_documents(\n    \\[\\], storage\\_context\\=storage\\_context, service\\_context\\=service\\_context\n)\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What was a hard moment for the author?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author experienced a difficult moment when their mother had a stroke and was put in a nursing home. The stroke destroyed her balance, and the author and their sister were determined to help her get out of the nursing home and back to her house.**\n\nresponse \\= query\\_engine.query(\"Who is the author?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The author of the given context is Paul Graham.**\n\nimport time\n\nquery\\_engine \\= index.as\\_query\\_engine(streaming\\=True)\nresponse \\= query\\_engine.query(\"What happened at interleaf?\")\n\nstart\\_time \\= time.time()\n\ntoken\\_count \\= 0\nfor token in response.response\\_gen:\n    print(token, end\\=\"\")\n    token\\_count += 1\n\ntime\\_elapsed \\= time.time() \\- start\\_time\ntokens\\_per\\_second \\= token\\_count / time\\_elapsed\n\nprint(f\"\\\\n\\\\nStreamed output at {tokens\\_per\\_second} tokens/s\")\n\nAt Interleaf, there was a group called Release Engineering that seemed to be as big as the group that actually wrote the software. The software at Interleaf had to be updated on the server, and there was a lot of emphasis on high production values to make the online store builders look legitimate.\n\nStreamed output at 20.953424485215063 tokens/s\n\n## Adding a document to existing index[\uf0c1](#adding-a-document-to-existing-index \"Permalink to this heading\")\n\nresponse \\= query\\_engine.query(\"What colour is the sky?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The color of the sky can vary depending on various factors such as time of day, weather conditions, and location. It can range from shades of blue during the day to hues of orange, pink, and purple during sunrise or sunset.**\n\nfrom llama\\_index import Document\n\nindex.insert\\_nodes(\\[Document(text\\=\"The sky is indigo today\")\\])\n\nresponse \\= query\\_engine.query(\"What colour is the sky?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**The colour of the sky is indigo.**\n\n## Filtering[\uf0c1](#filtering \"Permalink to this heading\")\n\nfrom llama\\_index.schema import TextNode\n\nnodes \\= \\[\n    TextNode(\n        text\\=\"The Shawshank Redemption\",\n        metadata\\={\n            \"author\": \"Stephen King\",\n            \"theme\": \"Friendship\",\n        },\n    ),\n    TextNode(\n        text\\=\"The Godfather\",\n        metadata\\={\n            \"director\": \"Francis Ford Coppola\",\n            \"theme\": \"Mafia\",\n        },\n    ),\n    TextNode(\n        text\\=\"Inception\",\n        metadata\\={\n            \"director\": \"Christopher Nolan\",\n        },\n    ),\n\\]\n\nindex.insert\\_nodes(nodes)\n\nfrom llama\\_index.vector\\_stores.types import ExactMatchFilter, MetadataFilters\n\nfilters \\= MetadataFilters(\n    filters\\=\\[ExactMatchFilter(key\\=\"theme\", value\\=\"Mafia\")\\]\n)\n\nretriever \\= index.as\\_retriever(filters\\=filters)\nretriever.retrieve(\"What is inception about?\")\n\n\\[NodeWithScore(node=TextNode(id\\_='5a97da0c-8f04-4c63-b90b-8c474d8c273d', embedding=None, metadata={'director': 'Francis Ford Coppola', 'theme': 'Mafia'}, excluded\\_embed\\_metadata\\_keys=\\[\\], excluded\\_llm\\_metadata\\_keys=\\[\\], relationships={}, hash='81cf4b9e847ba42e83fc401e31af8e17d629f0d5cf9c0c320ec7ac69dd0257e1', text='The Godfather', start\\_char\\_idx=None, end\\_char\\_idx=None, text\\_template='{metadata\\_str}\\\\n\\\\n{content}', metadata\\_template='{key}: {value}', metadata\\_seperator='\\\\n'), score=0.81316805)\\]"
}