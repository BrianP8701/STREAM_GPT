{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html",
        "title": "Fine-tuning with Retrieval Augmentation - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Fine-tuning with Retrieval Augmentation[\uf0c1](#fine-tuning-with-retrieval-augmentation \"Permalink to this heading\")\n\nHere we try fine-tuning an LLM with retrieval augmentation, as referenced from the RA-DIT paper: https://arxiv.org/abs/2310.01352.\n\nFor a given (query, response) input/output example, we retrieve the k text chunks with a retriever (the quality of the retriever doesn\u2019t have to be perfect, and in fact can be primitive). We then format each query with individually retrieved context, to create k examples (query + context\\_i, response) for fine-tuning.\n\nThe idea is to allow the LLM to better use background knowledge to synthesize a correct answer, or to synthesize a correct answer even in the absence of good background knowledge. This will enable the LLM to reason from its priors a bit better.\n\nimport os\nimport openai\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import OpenAI\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-...\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n## Setup + Load Data[\uf0c1](#setup-load-data \"Permalink to this heading\")\n\n!mkdir data && wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pdf.base import PDFReader\nfrom llama\\_hub.file.unstructured.base import UnstructuredReader\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocs0 \\= loader.load(file\\_path\\=Path(\"./data/llama2.pdf\"))\n\nfrom llama\\_index import Document\n\ndoc\\_text \\= \"\\\\n\\\\n\".join(\\[d.get\\_content() for d in docs0\\])\nmetadata \\= {\n    \"paper\\_title\": \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"\n}\ndocs \\= \\[Document(text\\=doc\\_text, metadata\\=metadata)\\]\n\nprint(docs\\[0\\].get\\_content())\n\nfrom llama\\_index.callbacks import CallbackManager\n\ncallback\\_manager \\= CallbackManager(\\[\\])\n\ngpt\\_35\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=OpenAI(model\\=\"gpt-3.5-turbo-0613\", temperature\\=0.3),\n    callback\\_manager\\=callback\\_manager,\n)\ngpt\\_4\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=OpenAI(model\\=\"gpt-4-0613\", temperature\\=0.3),\n    callback\\_manager\\=callback\\_manager,\n)\n\n### Get Nodes, Setup Vector Index[\uf0c1](#get-nodes-setup-vector-index \"Permalink to this heading\")\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\nfrom llama\\_index import VectorStoreIndex\n\nnode\\_parser \\= SimpleNodeParser.from\\_defaults()\nnodes \\= node\\_parser.get\\_nodes\\_from\\_documents(docs)\n\nvector\\_index \\= VectorStoreIndex(nodes)\n\n## Generate Dataset[\uf0c1](#generate-dataset \"Permalink to this heading\")\n\nfrom llama\\_index.evaluation import (\n    DatasetGenerator,\n    QueryResponseDataset,\n)\n\neval\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=OpenAI(model\\=\"gpt-4\", temperature\\=0), callback\\_manager\\=callback\\_manager\n)\ndataset\\_generator \\= DatasetGenerator(\n    nodes\\[:39\\],\n    service\\_context\\=eval\\_context,\n    show\\_progress\\=True,\n    num\\_questions\\_per\\_chunk\\=20,\n)\n\neval\\_dataset \\= await dataset\\_generator.agenerate\\_dataset\\_from\\_nodes(num\\=60)\n\neval\\_dataset.save\\_json(\"data\\_rag/qa\\_pairs.json\")\n\n\\# optional\neval\\_dataset \\= QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs.json\")\n\n### Option 2: Load from existing data[\uf0c1](#option-2-load-from-existing-data \"Permalink to this heading\")\n\nIf you were already using the fine-tuning knowledge notebook, you can use that instead.\n\nimport json\n\n\\# load data in from .jsonl format\ndef load\\_dataset\\_from\\_other\\_nb(path):\n    fp \\= open(path, \"r\")\n    qr\\_pairs \\= \\[\\]\n    for line in fp:\n        qa\\_pair \\= json.loads(line)\n        query\\_str \\= qa\\_pair\\[\"query\"\\]\n        response\\_str \\= qa\\_pair\\[\"response\"\\]\n        qr\\_pairs.append((query\\_str, response\\_str))\n\n    return qr\\_pairs\n\nqr\\_pairs \\= load\\_dataset\\_from\\_other\\_nb(\"data/qa\\_pairs\\_2.jsonl\")\neval\\_dataset \\= QueryResponseDataset.from\\_qr\\_pairs(qr\\_pairs)\n\n### For each Datapoint, Fetch Retrieved Context with a Retriever[\uf0c1](#for-each-datapoint-fetch-retrieved-context-with-a-retriever \"Permalink to this heading\")\n\nFor each (question, response) pair, fetch the top-k context with a retriever.\n\nFor each pair, we create k (question + context\\_i, response) new pairs, where we format each input with the QA prompt.\n\nfrom llama\\_index import VectorStoreIndex\nfrom llama\\_index.prompts import PromptTemplate\n\nqa\\_prompt\\_tmpl\\_str \\= (\n    \"Context information is below.\\\\n\"\n    \"---------------------\\\\n\"\n    \"{context\\_str}\\\\n\"\n    \"---------------------\\\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\\\n\"\n    \"Query: {query\\_str}\\\\n\"\n    \"Answer: \"\n)\nqa\\_prompt\\_tmpl \\= PromptTemplate(qa\\_prompt\\_tmpl\\_str)\n\nvector\\_retriever \\= vector\\_index.as\\_retriever(similarity\\_top\\_k\\=1)\n\nfrom tqdm.notebook import tqdm\n\ndef augment\\_data\\_with\\_retrieval(dataset, retriever, separate\\_context\\=False):\n    data\\_list \\= dataset.qr\\_pairs\n    new\\_data\\_list \\= \\[\\]\n    for query\\_str, response in tqdm(data\\_list):\n        retrieved\\_nodes \\= retriever.retrieve(query\\_str)\n        retrieved\\_txts \\= \\[n.get\\_content() for n in retrieved\\_nodes\\]\n        if separate\\_context:\n            for retrieved\\_txt in retrieved\\_txts:\n                fmt\\_query\\_str \\= qa\\_prompt\\_tmpl.format(\n                    query\\_str\\=query\\_str, context\\_str\\=retrieved\\_txt\n                )\n                new\\_data\\_list.append((fmt\\_query\\_str, response))\n        else:\n            context\\_str \\= \"\\\\n\\\\n\".join(retrieved\\_txts)\n            fmt\\_query\\_str \\= qa\\_prompt\\_tmpl.format(\n                query\\_str\\=query\\_str, context\\_str\\=context\\_str\n            )\n            new\\_data\\_list.append((fmt\\_query\\_str, response))\n    return new\\_data\\_list\n\nnew\\_qr\\_pairs \\= augment\\_data\\_with\\_retrieval(\n    eval\\_dataset, vector\\_retriever, separate\\_context\\=False\n)\nnew\\_eval\\_dataset \\= QueryResponseDataset.from\\_qr\\_pairs(new\\_qr\\_pairs)\n\nnew\\_eval\\_dataset.save\\_json(\"data\\_rag/qa\\_pairs\\_ra.json\")\n\nnew\\_eval\\_dataset \\= QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs\\_ra.json\")\n\n### Split into Training and Validation Sets[\uf0c1](#split-into-training-and-validation-sets \"Permalink to this heading\")\n\nWe split into training and validation sets.\n\n**NOTE**: We shuffle the data before splitting. This helps ensure that the training data has coverage throughout the document.\n\nfrom copy import deepcopy\nimport random\n\ndef split\\_train\\_val(dataset, train\\_split\\=0.7):\n    lines \\= dataset.qr\\_pairs\n\n    \\# shuffle the lines to make sure that the \"train questions\" cover most fo the context\n    shuffled\\_lines \\= deepcopy(lines)\n    random.shuffle(shuffled\\_lines)\n\n    split\\_idx \\= int(train\\_split \\* len(shuffled\\_lines))\n    train\\_lines \\= shuffled\\_lines\\[:split\\_idx\\]\n    val\\_lines \\= shuffled\\_lines\\[split\\_idx:\\]\n\n    return train\\_lines, val\\_lines\n\ntrain\\_lines, val\\_lines \\= split\\_train\\_val(new\\_eval\\_dataset, train\\_split\\=0.7)\n\ntrain\\_dataset \\= QueryResponseDataset.from\\_qr\\_pairs(train\\_lines)\nval\\_dataset \\= QueryResponseDataset.from\\_qr\\_pairs(val\\_lines)\n\ntrain\\_dataset.save\\_json(\"data\\_rag/qa\\_pairs\\_train.json\")\nval\\_dataset.save\\_json(\"data\\_rag/qa\\_pairs\\_val.json\")\n\ntrain\\_dataset \\= QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs\\_train.json\")\nval\\_dataset \\= QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs\\_val.json\")\n\n### Format into Training Data[\uf0c1](#format-into-training-data \"Permalink to this heading\")\n\nFormat into training data for OpenAI\u2019s finetuning endpoints.\n\n**NOTE**: We don\u2019t use our `OpenAIFinetuningHandler` because that logs the full input prompt including context as the user message. Here we just want to log the query as the user message, because we want to fine-tune gpt-3.5-turbo to \u201cbake in knowledge\u201d into the fine-tuned model.\n\ndef save\\_openai\\_data(dataset, out\\_path):\n    \\# out\\_fp = open(\"data\\_rag/qa\\_pairs\\_openai.jsonl\", \"w\")\n    out\\_fp \\= open(out\\_path, \"w\")\n    \\# TODO: try with different system prompts\n    system\\_prompt \\= {\n        \"role\": \"system\",\n        \"content\": (\n            \"You are a helpful assistant helping to answer questions about the\"\n            \" Llama 2 paper.\"\n        ),\n    }\n    train\\_qr\\_pairs \\= dataset.qr\\_pairs\n    for line in train\\_qr\\_pairs:\n        query, response \\= line\n        user\\_prompt \\= {\"role\": \"user\", \"content\": query}\n        assistant\\_prompt \\= {\"role\": \"assistant\", \"content\": response}\n        out\\_dict \\= {\n            \"messages\": \\[system\\_prompt, user\\_prompt, assistant\\_prompt\\],\n        }\n        out\\_fp.write(json.dumps(out\\_dict) + \"\\\\n\")\n\nsave\\_openai\\_data(train\\_dataset, \"data\\_rag/qa\\_pairs\\_openai.jsonl\")\n\n## Fine-tune the Model[\uf0c1](#fine-tune-the-model \"Permalink to this heading\")\n\nfrom llama\\_index.finetuning import OpenAIFinetuneEngine\n\nfinetune\\_engine \\= OpenAIFinetuneEngine(\n    \"gpt-3.5-turbo\",\n    \"data\\_rag/qa\\_pairs\\_openai.jsonl\",\n    \\# start\\_job\\_id=\"<start-job-id>\"  # if you have an existing job, can specify id here\n)\n\nfinetune\\_engine.finetune()\n\nfinetune\\_engine.get\\_current\\_job()\n\n<FineTuningJob fine\\_tuning.job id=ftjob-Rue4Yti7XpddPFYB6CnZadGo at 0x2cf346750> JSON: {\n  \"object\": \"fine\\_tuning.job\",\n  \"id\": \"ftjob-Rue4Yti7XpddPFYB6CnZadGo\",\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"created\\_at\": 1696407754,\n  \"finished\\_at\": 1696411978,\n  \"fine\\_tuned\\_model\": \"ft:gpt-3.5-turbo-0613:llamaindex::85sXTAx1\",\n  \"organization\\_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n  \"result\\_files\": \\[\n    \"file-9EY2Wj1Gb2lzcZi1PMqVnIpt\"\n  \\],\n  \"status\": \"succeeded\",\n  \"validation\\_file\": null,\n  \"training\\_file\": \"file-0iLbjiXwv33i1eZQYNXjE4np\",\n  \"hyperparameters\": {\n    \"n\\_epochs\": 3\n  },\n  \"trained\\_tokens\": 1754577,\n  \"error\": null\n}\n\nft\\_model \\= finetune\\_engine.get\\_finetuned\\_model()\n\nOpenAI(callback\\_manager=<llama\\_index.callbacks.base.CallbackManager object at 0x176cfca90>, model='ft:gpt-3.5-turbo-0613:llamaindex::85sXTAx1', temperature=0.1, max\\_tokens=None, additional\\_kwargs={}, max\\_retries=10, api\\_key='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', api\\_type='open\\_ai', api\\_base='https://api.openai.com/v1', api\\_version='', class\\_type='openai')\n\n\\# Use fine-tuned model in RAG system\nfrom llama\\_index import ServiceContext\n\nft\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=ft\\_model,\n    callback\\_manager\\=callback\\_manager,\n    system\\_prompt\\=(\n        \"You are a helpful assistant helping to answer questions about the\"\n        \" Llama 2 paper.\"\n    ),\n)\n\\# fine-tuned RAG system\nft\\_query\\_engine \\= vector\\_index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=1, service\\_context\\=ft\\_context\n)\n\nresponse \\= ft\\_query\\_engine.query(\n    \"How is the margin component added in the loss of the reward model in\"\n    \" Llama 2?\"\n)\nprint(str(response))\n\nThe margin component is added in the loss of the reward model in Llama 2 by subtracting the reward score of the worse sample from the reward score of the better sample. This difference is then compared to a margin threshold. If the difference is greater than the margin threshold, it is considered a positive example and the loss is set to zero. If the difference is smaller than the margin threshold, it is considered a negative example and the loss is set to the margin threshold minus the difference. This margin component helps to separate the reward scores of the better and worse samples, making the reward model more accurate in distinguishing between them.\n\nbase\\_query\\_engine \\= vector\\_index.as\\_query\\_engine(similarity\\_top\\_k\\=1)\nbase\\_response \\= base\\_query\\_engine.query(\n    \"How is the margin component added in the loss of the reward model in\"\n    \" Llama 2?\"\n)\nprint(str(base\\_response))\n\nThe margin component is added in the loss of the reward model in Llama 2 by using a preference rating-based margin term. This margin term is used in Equation 2 and helps to separate comparison pairs more effectively. The magnitude of the margin term can be adjusted to achieve better performance on separable pairs, but it may regress performance on similar samples.\n\n## Evaluate Results[\uf0c1](#evaluate-results \"Permalink to this heading\")\n\nWe run evaluations, over both the validation set but also the training set (as a sanity check)\n\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nfrom llama\\_index.llms import ChatMessage\nfrom llama\\_index.evaluation.eval\\_utils import get\\_responses, get\\_results\\_df\nfrom llama\\_index.evaluation import BatchEvalRunner\n\n\\# train\\_dataset = QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs\\_train.json\")\n\\# val\\_dataset = QueryResponseDataset.from\\_json(\"data\\_rag/qa\\_pairs\\_val.json\")\n\n\\# Load dataset\n\\# NOTE: we need to run over the original questions, not the retrieval-augmented questions.\n\\# Since our query engines will perform retrieval augmentation under the hood!\n\n\\# TODO: have better code here\nqr\\_pairs \\= load\\_dataset\\_from\\_other\\_nb(\"data/qa\\_pairs\\_2.jsonl\")\neval\\_dataset \\= QueryResponseDataset.from\\_qr\\_pairs(qr\\_pairs)\n\n\\# evaluate over training dataset for now\nsample\\_size \\= 50\n\neval\\_qs \\= eval\\_dataset.questions\\[:sample\\_size\\]\nref\\_response\\_strs \\= \\[r for (\\_, r) in eval\\_dataset.qr\\_pairs\\[:sample\\_size\\]\\]\n\npred\\_responses \\= get\\_responses(eval\\_qs, ft\\_query\\_engine, show\\_progress\\=True)\n\nbase\\_pred\\_responses \\= get\\_responses(\n    eval\\_qs, base\\_query\\_engine, show\\_progress\\=True\n)\n\nimport numpy as np\n\npred\\_response\\_strs \\= \\[str(p) for p in pred\\_responses\\]\nbase\\_pred\\_response\\_strs \\= \\[str(p) for p in base\\_pred\\_responses\\]\n\nfrom llama\\_index.evaluation import (\n    CorrectnessEvaluator,\n    SemanticSimilarityEvaluator,\n)\n\neval\\_service\\_context \\= ServiceContext.from\\_defaults(llm\\=OpenAI(model\\=\"gpt-4\"))\n\\# NOTE: can uncomment other evaluators\nevaluator\\_c \\= CorrectnessEvaluator(service\\_context\\=eval\\_service\\_context)\nevaluator\\_s \\= SemanticSimilarityEvaluator(service\\_context\\=eval\\_service\\_context)\n\nevaluator\\_dict \\= {\n    \"correctness\": evaluator\\_c,\n    \"semantic\\_similarity\": evaluator\\_s,\n}\nbatch\\_runner \\= BatchEvalRunner(evaluator\\_dict, workers\\=2, show\\_progress\\=True)\n\neval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    eval\\_qs, responses\\=pred\\_responses, reference\\=ref\\_response\\_strs\n)\n\nbase\\_eval\\_results \\= await batch\\_runner.aevaluate\\_responses(\n    eval\\_qs, responses\\=base\\_pred\\_responses, reference\\=ref\\_response\\_strs\n)\n\nresults\\_df \\= get\\_results\\_df(\n    \\[eval\\_results, base\\_eval\\_results\\],\n    \\[\"RAG Fine-tuned LLM\", \"Base LLM\"\\],\n    \\[\"correctness\", \"semantic\\_similarity\"\\],\n)\ndisplay(results\\_df)\n\n|     | names | correctness | semantic\\_similarity |\n| --- | --- | --- | --- |\n| 0   | RAG Fine-tuned LLM | 3.65 | 0.941940 |\n| 1   | Base LLM | 3.25 | 0.917662 |"
}