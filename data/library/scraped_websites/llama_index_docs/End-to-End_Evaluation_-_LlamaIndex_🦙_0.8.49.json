{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/e2e_evaluation.html",
        "title": "End-to-End Evaluation - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## End-to-End Evaluation[\uf0c1](#end-to-end-evaluation \"Permalink to this heading\")\n\nEnd-to-End evaluation should be the guiding signal for your RAG application - will my pipeline generate the right responses given the data sources and a set of queries?\n\nWhile it helps initially to individually inspect queries and responses, as you deal with more failure and corner cases, it may stop being feasible to look at each query individually, and rather it may help instead to define a set of summary metrics or automated evaluation, and gain an intuition for what they might be telling you and where you might dive deeper.\n\n## Setting up an Evaluation Set[\uf0c1](#setting-up-an-evaluation-set \"Permalink to this heading\")\n\nIt is helpful to start off with a small but diverse set of queries, and build up more examples as one discovers problematic queries or interactions.\n\nWe\u2019ve created some tools that automatically generate a dataset for you given a set of documents to query. (See example below).\n\n*   [QuestionGeneration](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)\n\nIn the future, we will also be able to create datasets automatically against tools.\n\n## The Spectrum of Evaluation Options[\uf0c1](#the-spectrum-of-evaluation-options \"Permalink to this heading\")\n\nQuantitative eval is more useful when evaluating applications where there is a correct answer - for instance, validating that the choice of tools and their inputs are correct given the plan, or retrieving specific pieces of information, or attempting to produce intermediate output of a certain schema (e.g. JSON fields).\n\nQualitative eval is more useful when generating long-form responses that are meant to be _helpful_ but not necessarily completely accurate.\n\nThere is a spectrum of evaluation options ranging from metrics, cheaper models, more expensive models (GPT4), and human evaluation.\n\nBelow is some example usage of the [evaluation modules](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/evaluation.html#evaluation):\n\n## Discovery - Sensitivity Testing[\uf0c1](#discovery-sensitivity-testing \"Permalink to this heading\")\n\nWith a complex pipeline, it may be unclear which parts of the pipeline are affecting your results.\n\nSensitivity testing can be a good inroad into choosing which components to individually test or tweak more thoroughly, or which parts of your dataset (e.g. queries) may be producing problematic results.\n\nMore details on how to discover issues automatically with methods such as sensitivity testing will come soon.\n\nExamples of this in the more traditional ML domain include [Giskard](https://docs.giskard.ai/en/latest/getting-started/quickstart.html).\n\n## Metrics Ensembling[\uf0c1](#metrics-ensembling \"Permalink to this heading\")\n\nIt may be expensive to use GPT-4 to carry out evaluation especially as your dev set grows large.\n\nMetrics ensembling uses an ensemble of weaker signals (exact match, F1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output of a more expensive evaluation methods that are closer to the gold labels (human-labelled/GPT-4).\n\nIt is intenteded for two purposes:\n\n1.  Evaluating changes cheaply and quickly across a large dataset during the development stage.\n    \n2.  Flagging outliers for further evaluation (GPT-4 / human alerting) during the production monitoring stage.\n    \n\nWe also want the metrics ensembling to be interpretable - the correlation and weighting scores should give an indication of which metrics best capture the evaluation criteria.\n\nWe will discuss more about the methodology in future updates."
}