{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/api_reference/query/query_transform.html",
        "title": "Query Transform - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Query Transform[\uf0c1](#module-llama_index.indices.query.query_transform \"Permalink to this heading\")\n\nQuery Transforms.\n\n_class_ llama\\_index.indices.query.query\\_transform.DecomposeQueryTransform(_llm\\_predictor: Optional\\[BaseLLMPredictor\\] \\= None_, _decompose\\_query\\_prompt: Optional\\[[PromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.PromptTemplate \"llama_index.prompts.base.PromptTemplate\")\\] \\= None_, _verbose: bool \\= False_)[\uf0c1](#llama_index.indices.query.query_transform.DecomposeQueryTransform \"Permalink to this definition\")\n\nDecompose query transform.\n\nDecomposes query into a subquery given the current index struct. Performs a single step transformation.\n\nParameters\n\n**llm\\_predictor** (_Optional__\\[_[_LLMPredictor_](https://docs.llamaindex.ai/en/stable/api_reference/llm_predictor.html#llama_index.llm_predictor.LLMPredictor \"llama_index.llm_predictor.LLMPredictor\")_\\]_) \u2013 LLM for generating hypothetical documents\n\nrun(_query\\_bundle\\_or\\_str: Union\\[str, [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")\\]_, _metadata: Optional\\[Dict\\] \\= None_) \u2192 [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")[\uf0c1](#llama_index.indices.query.query_transform.DecomposeQueryTransform.run \"Permalink to this definition\")\n\nRun query transform.\n\n_class_ llama\\_index.indices.query.query\\_transform.HyDEQueryTransform(_llm\\_predictor: Optional\\[BaseLLMPredictor\\] \\= None_, _hyde\\_prompt: Optional\\[[BasePromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.BasePromptTemplate \"llama_index.prompts.base.BasePromptTemplate\")\\] \\= None_, _include\\_original: bool \\= True_)[\uf0c1](#llama_index.indices.query.query_transform.HyDEQueryTransform \"Permalink to this definition\")\n\nHypothetical Document Embeddings (HyDE) query transform.\n\nIt uses an LLM to generate hypothetical answer(s) to a given query, and use the resulting documents as embedding strings.\n\nAs described in \\[Precise Zero-Shot Dense Retrieval without Relevance Labels\\] (https://arxiv.org/abs/2212.10496)\n\nrun(_query\\_bundle\\_or\\_str: Union\\[str, [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")\\]_, _metadata: Optional\\[Dict\\] \\= None_) \u2192 [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")[\uf0c1](#llama_index.indices.query.query_transform.HyDEQueryTransform.run \"Permalink to this definition\")\n\nRun query transform.\n\n_class_ llama\\_index.indices.query.query\\_transform.StepDecomposeQueryTransform(_llm\\_predictor: Optional\\[BaseLLMPredictor\\] \\= None_, _step\\_decompose\\_query\\_prompt: Optional\\[[PromptTemplate](https://docs.llamaindex.ai/en/stable/api_reference/prompts.html#llama_index.prompts.base.PromptTemplate \"llama_index.prompts.base.PromptTemplate\")\\] \\= None_, _verbose: bool \\= False_)[\uf0c1](#llama_index.indices.query.query_transform.StepDecomposeQueryTransform \"Permalink to this definition\")\n\nStep decompose query transform.\n\nDecomposes query into a subquery given the current index struct and previous reasoning.\n\nNOTE: doesn\u2019t work yet.\n\nParameters\n\n**llm\\_predictor** (_Optional__\\[_[_LLMPredictor_](https://docs.llamaindex.ai/en/stable/api_reference/llm_predictor.html#llama_index.llm_predictor.LLMPredictor \"llama_index.llm_predictor.LLMPredictor\")_\\]_) \u2013 LLM for generating hypothetical documents\n\nrun(_query\\_bundle\\_or\\_str: Union\\[str, [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")\\]_, _metadata: Optional\\[Dict\\] \\= None_) \u2192 [QueryBundle](https://docs.llamaindex.ai/en/stable/api_reference/query/query_bundle.html#llama_index.indices.query.schema.QueryBundle \"llama_index.indices.query.schema.QueryBundle\")[\uf0c1](#llama_index.indices.query.query_transform.StepDecomposeQueryTransform.run \"Permalink to this definition\")\n\nRun query transform."
}