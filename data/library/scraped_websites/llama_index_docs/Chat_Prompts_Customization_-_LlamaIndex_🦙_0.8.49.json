{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/customization/prompts/chat_prompts.html",
        "title": "Chat Prompts Customization - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Chat Prompts Customization[\uf0c1](#chat-prompts-customization \"Permalink to this heading\")\n\n## Prompt Setup[\uf0c1](#prompt-setup \"Permalink to this heading\")\n\nBelow, we take the default prompts and customize them to always answer, even if the context is not helpful.\n\nfrom llama\\_index.llms import ChatMessage, MessageRole\nfrom llama\\_index.prompts import ChatPromptTemplate\n\n\\# Text QA Prompt\nchat\\_text\\_qa\\_msgs \\= \\[\n    ChatMessage(\n        role\\=MessageRole.SYSTEM,\n        content\\=(\n            \"Always answer the question, even if the context isn't helpful.\"\n        ),\n    ),\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=(\n            \"Context information is below.\\\\n\"\n            \"---------------------\\\\n\"\n            \"{context\\_str}\\\\n\"\n            \"---------------------\\\\n\"\n            \"Given the context information and not prior knowledge, \"\n            \"answer the question: {query\\_str}\\\\n\"\n        ),\n    ),\n\\]\ntext\\_qa\\_template \\= ChatPromptTemplate(chat\\_text\\_qa\\_msgs)\n\n\\# Refine Prompt\nchat\\_refine\\_msgs \\= \\[\n    ChatMessage(\n        role\\=MessageRole.SYSTEM,\n        content\\=(\n            \"Always answer the question, even if the context isn't helpful.\"\n        ),\n    ),\n    ChatMessage(\n        role\\=MessageRole.USER,\n        content\\=(\n            \"We have the opportunity to refine the original answer \"\n            \"(only if needed) with some more context below.\\\\n\"\n            \"------------\\\\n\"\n            \"{context\\_msg}\\\\n\"\n            \"------------\\\\n\"\n            \"Given the new context, refine the original answer to better \"\n            \"answer the question: {query\\_str}. \"\n            \"If the context isn't useful, output the original answer again.\\\\n\"\n            \"Original Answer: {existing\\_answer}\"\n        ),\n    ),\n\\]\nrefine\\_template \\= ChatPromptTemplate(chat\\_refine\\_msgs)\n\n## Using the Prompts[\uf0c1](#using-the-prompts \"Permalink to this heading\")\n\nNow, we use the prompts in an index query!\n\nimport openai\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-...\"\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama\\_index.llms import OpenAI\n\ndocuments \\= SimpleDirectoryReader(\"../../data/paul\\_graham/\").load\\_data()\n\n\\# Create an index using a chat model, so that we can use the chat prompts!\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=OpenAI(model\\=\"gpt-3.5-turbo\", temperature\\=0.1)\n)\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\n\n### Before Adding Templates[\uf0c1](#before-adding-templates \"Permalink to this heading\")\n\nprint(index.as\\_query\\_engine().query(\"Who is Joe Biden?\"))\n\nI'm sorry, but the given context does not provide any information about Joe Biden.\n\n### After Adding Templates[\uf0c1](#after-adding-templates \"Permalink to this heading\")\n\nprint(\n    index.as\\_query\\_engine(\n        text\\_qa\\_template\\=text\\_qa\\_template, refine\\_template\\=refine\\_template\n    ).query(\"Who is Joe Biden?\")\n)\n\nJoe Biden is the 46th President of the United States."
}