["", "", "DATA SCIENCE", "The MIT Press Essential Knowledge Series\nAuctions, Timothy P. Hubbard and Harry J. Paarsch\nThe Book, Amaranth Borsuk\nCloud Computing, Nayan Ruparelia\nComputing: A Concise History, Paul E. Ceruzzi\nThe Conscious Mind, Zoltan L. Torey\nCrowdsourcing, Daren C. Brabham\nData Science, John D. Kelleher and Brendan Tierney\nFree Will, Mark Balaguer\nThe Future, Nick Montfort\nInformation and Society, Michael Buckland\nInformation and the Modern Corporation, James W. Cortada\nIntellectual Property Strategy, John Palfrey\nThe Internet of Things, Samuel Greengard\nMachine Learning: The New AI, Ethem Alpaydin\nMachine Translation, Thierry Poibeau\nMemes in Digital Culture, Limor Shifman\nMetadata, Jeffrey Pomerantz\nThe Mind\u2013Body Problem, Jonathan Westphal\nMOOCs, Jonathan Haber\nNeuroplasticity, Moheb Costandi\nOpen Access, Peter Suber\nParadox, Margaret Cuonzo\nPost-Truth, Lee McIntyre\nRobots, John Jordan\nSelf-Tracking, Gina Neff and Dawn Nafus\nSustainability, Kent E. Portney\nSynesthesia, Richard E. Cytowic\nThe Technological Singularity, Murray Shanahan\nUnderstanding Beliefs, Nils J. Nilsson\nWaves, Frederic Raichlen", "DATA SCIENCE\nJOHN D. KELLEHER\nAND BRENDAN TIERNEY\nThe MIT Press | Cambridge, Massachusetts | London, England", "\u00a9 2018 Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by\nany electronic or mechanical means (including photocopying, recording, or\ninformation storage and retrieval) without permission in writing from the\npublisher.\nThis book was set in Chaparral Pro by Toppan Best-set Premedia Limited.\nPrinted and bound in the United States of America.\nLibrary of Congress Cataloging-in-Publication Data\nNames: Kelleher, John D., 1974- author. | Tierney, Brendan, 1970- author.\nTitle: Data science / John D. Kelleher and Brendan Tierney.\nDescription: Cambridge, MA : The MIT Press, [2018] | Series: The MIT Press\nessential knowledge series | Includes bibliographical references and index.\nIdentifiers: LCCN 2017043665 | ISBN 9780262535434 (pbk. : alk. paper)\nSubjects: LCSH: Big data. | Machine learning. | Data mining. | Quantitative\nresearch.\nClassification: LCC QA76.9.B45 K45 2018 | DDC 005.7--dc23 LC record\navailable at https://lccn.loc.gov/2017043665\n10 9 8 7 6 5 4 3 2 1", "CONTENTS\nSeries Foreword vii\nPreface ix\nAcknowledgments xiii\n1 What Is Data Science? 1\n2 What Are Data, and What Is a Data Set? 39\n3 A Data Science Ecosystem 69\n4 Machine Learning 101 97\n5 Standard Data Science Tasks 151\n6 Privacy and Ethics 181\n7 Future Trends and Principles of Success 219\nGlossary 239\nNotes 247\nFurther Readings 251\nReferences 253\nIndex 261", "", "SERIES FOREWORD\nThe MIT Press Essential Knowledge series offers acces-\nsible, concise, beautifully produced pocket-size books on\ntopics of current interest. Written by leading thinkers, the\nbooks in this series deliver expert overviews of subjects\nthat range from the cultural and the historical to the sci-\nentific and the technical.\nIn today\u2019s era of instant information gratification, we\nhave ready access to opinions, rationalizations, and super-\nficial descriptions. Much harder to come by is the founda-\ntional knowledge that informs a principled understanding\nof the world. Essential Knowledge books fill that need.\nSynthesizing specialized subject matter for nonspecialists\nand engaging critical topics through fundamentals, each\nof these compact volumes offers readers a point of access\nto complex ideas.\nBruce Tidor\nProfessor of Biological Engineering and Computer Science\nMassachusetts Institute of Technology", "", "PREFACE\nThe goal of data science is to improve decision making by\nbasing decisions on insights extracted from large data sets.\nAs a field of activity, data science encompasses a set of\nprinciples, problem definitions, algorithms, and processes\nfor extracting nonobvious and useful patterns from large\ndata sets. It is closely related to the fields of data mining\nand machine learning, but it is broader in scope. Today,\ndata science drives decision making in nearly all parts of\nmodern societies. Some of the ways that data science may\naffect your daily life include determining which advertise-\nments are presented to you online; which movies, books,\nand friend connections are recommended to you; which\nemails are filtered into your spam folder; what offers you\nreceive when you renew your cell phone service; the cost of\nyour health insurance premium; the sequencing and tim-\ning of traffic lights in your area; how the drugs you may\nneed were designed; and which locations in your city the\npolice are targeting.\nThe growth in use of data science across our societies\nis driven by the emergence of big data and social media,\nthe speedup in computing power, the massive reduction\nin the cost of computer memory, and the development of\nmore powerful methods for data analysis and modeling,\nsuch as deep learning. Together these factors mean that", "it has never been easier for organizations to gather, store,\nand process data. At the same time, these technical inno-\nvations and the broader application of data science means\nthat the ethical challenges related to the use of data and\nindividual privacy have never been more pressing. The aim\nof this book is to provide an introduction to data science\nthat covers the essential elements of the field at a depth\nthat provides a principled understanding of the field.\nChapter 1 introduces the field of data science and pro-\nvides a brief history of how it has developed and evolved.\nIt also examines why data science is important today and\nsome of the factors that are driving its adoption. The\nchapter finishes by reviewing and debunking some of the\nmyths associated with data science. Chapter 2 introduces\nfundamental concepts relating to data. It also describes\nthe standard stages in a data science project: business un-\nderstanding, data understanding, data preparation, mod-\neling, evaluation, and deployment. Chapter 3 focuses on\ndata infrastructure and the challenges posed by big data\nand the integration of data from multiple sources. One\naspect of a typical data infrastructure that can be chal-\nlenging is that data in databases and data warehouses of-\nten reside on servers different from the servers used for\ndata analysis. As a consequence, when large data sets are\nhandled, a surprisingly large amount of time can be spent\nmoving data between the servers a database or data ware-\nhouse are living on and the servers used for data analysis\nx Preface", "and machine learning. Chapter 3 begins by describing a\ntypical data science infrastructure for an organization and\nsome of the emerging solutions to the challenge of mov-\ning large data sets within a data infrastructure, which in-\nclude the use of in-database machine learning, the use of\nHadoop for data storage and processing, and the develop-\nment of hybrid database systems that seamlessly combine\ntraditional database software and Hadoop-like solutions.\nThe chapter concludes by highlighting some of the chal-\nlenges in integrating data from across an organization into\na unified representation that is suitable for machine learn-\ning. Chapter 4 introduces the field of machine learning\nand explains some of the most popular machine-learning\nalgorithms and models, including neural networks, deep\nlearning, and decision-tree models. Chapter 5 focuses on\nlinking machine-learning expertise with real-world prob-\nlems by reviewing a range of standard business problems\nand describing how they can be solved by machine-learning\nsolutions. Chapter 6 reviews the ethical implications of\ndata science, recent developments in data regulation,\nand some of the new computational approaches to pre-\nserving the privacy of individuals within the data science\nprocess. Finally, chapter 7 describes some of the areas\nwhere data science will have a significant impact in the\nnear future and sets out some of the principles that are\nimportant in determining whether a data science project\nwill succeed.\nPreface xi", "", "ACKNOWLEDGMENTS\nJohn and Brendan thank Paul McElroy and Brian Leahy for\nreading and commenting on early drafts. They also thank\nthe two anonymous reviewers who provided detailed and\nhelpful feedback on the manuscript and the staff at the\nMIT Press for their support and guidance.\nJohn thanks his family and friends for their sup-\nport and encouragement during the preparation of this\nbook and dedicates this book to his father, John Bernard\nKelleher, in recognition of his love and friendship.\nBrendan thanks Grace, Daniel, and Eleanor for their\nconstant support while he was writing yet another book\n(his fourth), juggling the day jobs, and traveling.", "", "1\nWHAT IS DATA SCIENCE?\nData science encompasses a set of principles, problem\ndefinitions, algorithms, and processes for extracting non-\nobvious and useful patterns from large data sets. Many\nof the elements of data science have been developed in\nrelated fields such as machine learning and data mining.\nIn fact, the terms data science, machine learning, and data\nmining are often used interchangeably. The commonality\nacross these disciplines is a focus on improving decision\nmaking through the analysis of data. However, although\ndata science borrows from these other fields, it is broader\nin scope. Machine learning (ML) focuses on the design\nand evaluation of algorithms for extracting patterns from\ndata. Data mining generally deals with the analysis of\nstructured data and often implies an emphasis on com-\nmercial applications. Data science takes all of these consid-\nerations into account but also takes up other challenges,", "such as the capturing, cleaning, and transforming of\nunstructured social media and web data; the use of big-\ndata technologies to store and process big, unstructured\ndata sets; and questions related to data ethics and\nregulation.\nUsing data science, we can extract different types of\npatterns. For example, we might want to extract patterns\nthat help us to identify groups of customers exhibiting\nsimilar behavior and tastes. In business jargon, this task\nis known as customer segmentation, and in data science\nterminology it is called clustering. Alternatively, we might\nwant to extract a pattern that identifies products that are\nfrequently bought together, a process called association-\nrule mining. Or we might want to extract patterns that\nidentify strange or abnormal events, such as fraudulent\ninsurance claims, a process known as anomaly or outlier\ndetection. Finally, we might want to identify patterns that\nhelp us to classify things. For example, the following rule\nillustrates what a classification pattern extracted from\nan email data set might look like: If an email contains the\nphrase \u201cMake money easily,\u201d it is likely to be a spam email.\nIdentifying these types of classification rules is known as\nprediction. The word prediction might seem an odd choice\nbecause the rule doesn\u2019t predict what will happen in the\nfuture: the email already is or isn\u2019t a spam email. So it\nis best to think of prediction patterns as predicting the\nmissing value of an attribute rather than as predicting\n2 Chapter 1", "If a human expert can\neasily create a pattern\nin his or her own\nmind, it is generally\nnot worth the time and\neffort of using data\nscience to \u201cdiscover\u201d it.", "the future. In this example, we are predicting whether the\nemail classification attribute should have the value \u201cspam\u201d\nor not.\nAlthough we can use data science to extract differ-\nent types of patterns, we always want the patterns to be\nboth nonobvious and useful. The example email classifica-\ntion rule given in the previous paragraph is so simple and\nobvious that if it were the only rule extracted by a data\nscience process, we would be disappointed. For example,\nthis email classification rule checks only one attribute of\nan email: Does the email contain the phrase \u201cmake money\neasily\u201d? If a human expert can easily create a pattern in\nhis or her own mind, it is generally not worth the time\nand effort of using data science to \u201cdiscover\u201d it. In general,\ndata science becomes useful when we have a large number\nof data examples and when the patterns are too complex\nfor humans to discover and extract manually. As a lower\nbound, we can take a large number of data examples to\nbe defined as more than a human expert can check easily.\nWith regard to the complexity of the patterns, again, we\ncan define it relative to human abilities. We humans are\nreasonably good at defining rules that check one, two, or\neven three attributes (also commonly referred to as fea-\ntures or variables), but when we go higher than three attri-\nbutes, we can start to struggle to handle the interactions\nbetween them. By contrast, data science is often applied in\ncontexts where we want to look for patterns among tens,\n4 Chapter 1", "hundreds, thousands, and, in extreme cases, millions of\nattributes.\nThe patterns that we extract using data science are\nuseful only if they give us insight into the problem that\nenables us to do something to help solve the problem. The\nphrase actionable insight is sometimes used in this context\nto describe what we want the extracted patterns to give us.\nThe term insight highlights that the pattern should give\nus relevant information about the problem that isn\u2019t ob-\nvious. The term actionable highlights that the insight we\nget should also be something that we have the capacity to\nuse in some way. For example, imagine we are working for\na cell phone company that is trying to solve a customer\nchurn problem\u2014that is, too many customers are switching\nto other companies. One way data science might be used to\naddress this problem is to extract patterns from the data\nabout previous customers that allow us to identify current\ncustomers who are churn risks and then contact these cus-\ntomers and try to persuade them to stay with us. A pattern\nthat enables us to identify likely churn customers is useful\nto us only if (a) the patterns identify the customers early\nenough that we have enough time to contact them before\nthey churn and (b) our company is able to put a team in\nplace to contact them. Both of these things are required in\norder for the company to be able to act on the insight the\npatterns give us.\nWhat Is Data sCIenCe? 5", "A Brief History of Data Science\nThe term data science has a specific history dating back\nto the 1990s. However, the fields that it draws upon have\na much longer history. One thread in this longer history\nis the history of data collection; another is the history of\ndata analysis. In this section, we review the main develop-\nments in these threads and describe how and why they\nconverged into the field of data science. Of necessity, this\nreview introduces new terminology as we describe and\nname the important technical innovations as they arose.\nFor each new term, we provide a brief explanation of its\nmeaning; we return to many of these terms later in the\nbook and provide a more detailed explanation of them.\nWe begin with a history of data collection, then give a his-\ntory of data analysis, and, finally, cover the development\nof data science.\nA History of Data Gathering\nThe earliest methods for recording data may have been\nnotches on sticks to mark the passing of the days or poles\nstuck in the ground to mark sunrise on the solstices. With\nthe development of writing, however, our ability to re-\ncord our experiences and the events in our world vastly\nincreased the amount of data we collected. The earliest\nform of writing developed in Mesopotamia around 3200\nBC and was used for commercial record keeping. This type\n6 Chapter 1", "of record keeping captures what is known as transactional\ndata. Transactional data include event information such\nas the sale of an item, the issuing of an invoice, the deliv-\nery of goods, credit card payment, insurance claims, and\nso on. Nontransactional data, such as demographic data,\nalso have a long history. The earliest-known censuses took\nplace in pharaonic Egypt around 3000 BC. The reason that\nearly states put so much effort and resources into large\ndata-collection operations was that these states needed to\nraise taxes and armies, thus proving Benjamin Franklin\u2019s\nclaim that there are only two things certain in life: death\nand taxes.\nIn the past 150 years, the development of the elec-\ntronic sensor, the digitization of data, and the invention\nof the computer have contributed to a massive increase\nin the amount of data that are collected and stored. A\nmilestone in data collection and storage occurred in 1970\nwhen Edgar F. Codd published a paper explaining the re-\nlational data model, which was revolutionary in terms of\nsetting out how data were (at the time) stored, indexed,\nand retrieved from databases. The relational data model\nenabled users to extract data from a database using simple\nqueries that defined what data the user wanted without\nrequiring the user to worry about the underlying structure\nof the data or where they were physically stored. Codd\u2019s\npaper provided the foundation for modern databases and\nthe development of structured query language (SQL), an\nWhat Is Data sCIenCe? 7", "international standard for defining database queries. Re-\nlational databases store data in tables with a structure of\none row per instance and one column per attribute. This\nstructure is ideal for storing data because it can be decom-\nposed into natural attributes.\nDatabases are the natural technology to use for storing\nand retrieving structured transactional or operational data\n(i.e., the type of data generated by a company\u2019s day-to-day\noperations). However, as companies have become larger\nand more automated, the amount and variety of data\ngenerated by different parts of these companies have dra-\nmatically increased. In the 1990s, companies realized that\nalthough they were accumulating tremendous amounts\nof data, they were repeatedly running into difficulties in\nanalyzing those data. Part of the problem was that the\ndata were often stored in numerous separate databases\nwithin the one organization. Another difficulty was that\ndatabases were optimized for storage and retrieval of data,\nactivities characterized by high volumes of simple opera-\ntions, such as SELECT, INSERT, UPDATE, and DELETE. In\norder to analyze their data, these companies needed tech-\nnology that was able to bring together and reconcile the\ndata from disparate databases and that facilitated more\ncomplex analytical data operations. This business chal-\nlenge led to the development of data warehouses. In a data\nwarehouse, data are taken from across the organization\n8 Chapter 1", "and integrated, thereby providing a more comprehensive\ndata set for analysis.\nOver the past couple of decades, our devices have be-\ncome mobile and networked, and many of us now spend\nmany hours online every day using social technologies,\ncomputer games, media platforms, and web search en-\ngines. These changes in technology and how we live have\nhad a dramatic impact on the amount of data collected.\nIt is estimated that the amount of data collected over the\nfive millennia since the invention of writing up to 2003\nis about 5 exabytes. Since 2013, humans generate and\nstore this same amount of data every day. However, it is\nnot only the amount of data collected that has grown dra-\nmatically but also the variety of data. Just consider the\nfollowing list of online data sources: emails, blogs, pho-\ntos, tweets, likes, shares, web searches, video uploads,\nonline purchases, podcasts. And if we consider the meta-\ndata (data describing the structure and properties of the\nraw data) of these events, we can begin to understand the\nmeaning of the term big data. Big data are often defined in\nterms of the three Vs: the extreme volume of data, the va-\nriety of the data types, and the velocity at which the data\nmust be processed.\nThe advent of big data has driven the development\nof a range of new database technologies. This new gen-\neration of databases is often referred to as \u201cNoSQL da-\ntabases.\u201d They typically have a simpler data model than\nWhat Is Data sCIenCe? 9", "traditional relational databases. A NoSQL database stores\ndata as objects with attributes, using an object notation\nlanguage such as the JavaScript Object Notation (JSON).\nThe advantage of using an object representation of data\n(in contrast to a relational table-based model) is that the\nset of attributes for each object is encapsulated within the\nobject, which results in a flexible representation. For ex-\nample, it may be that one of the objects in the database,\ncompared to other objects, has only a subset of attributes.\nBy contrast, in the standard tabular data structure used\nby a relational database, all the data points should have\nthe same set of attributes (i.e., columns). This flexibility in\nobject representation is important in contexts where the\ndata cannot (due to variety or type) naturally be decom-\nposed into a set of structured attributes. For example, it\ncan be difficult to define the set of attributes that should\nbe used to represent free text (such as tweets) or images.\nHowever, although this representational flexibility allows\nus to capture and store data in a variety of formats, these\ndata still have to be extracted into a structured format be-\nfore any analysis can be performed on them.\nThe existence of big data has also led to the develop-\nment of new data-processing frameworks. When you are\ndealing with large volumes of data at high speeds, it can\nbe useful from a computational and speed perspective to\ndistribute the data across multiple servers, process que-\nries by calculating partial results of a query on each server,\n10 Chapter 1", "and then merge these results to generate the response to\nthe query. This is the approach taken by the MapReduce\nframework on Hadoop. In the MapReduce framework, the\ndata and queries are mapped onto (or distributed across)\nmultiple servers, and the partial results calculated on each\nserver are then reduced (merged) together.\nA History of Data Analysis\nStatistics is the branch of science that deals with the col-\nlection and analysis of data. The term statistics originally\nreferred to the collection and analysis of data about the\nstate, such as demographics data or economic data. How-\never, over time the type of data that statistical analysis\nwas applied to broadened so that today statistics is used\nto analyze all types of data. The simplest form of statisti-\ncal analysis of data is the summarization of a data set in\nterms of summary (descriptive) statistics (including mea-\nsures of a central tendency, such as the arithmetic mean, or\nmeasures of variation, such as the range). However, in the\nseventeenth and eighteenth centuries the work of people\nsuch as Gerolamo Cardano, Blaise Pascal, Jakob Bernoulli,\nAbraham de Moivre, Thomas Bayes, and Richard Price\nlaid the foundations of probability theory, and through\nthe nineteenth century many statisticians began to use\nprobability distributions as part of their analytic tool kit.\nThese new developments in mathematics enabled statis-\nticians to move beyond descriptive statistics and to start\nWhat Is Data sCIenCe? 11", "doing statistical learning. Pierre Simon de Laplace and Carl\nFriedrich Gauss are two of the most important and famous\nnineteenth-century mathematicians, and both made im-\nportant contributions to statistical learning and modern\ndata science. Laplace took the intuitions of Thomas Bayes\nand Richard Price and developed them into the first ver-\nsion of what we now call Bayes\u2019 Rule. Gauss, in his search\nfor the missing dwarf planet Ceres, developed the method\nof least squares, which enables us to find the best model\nthat fits a data set such that the error in the fit minimizes\nthe sum of squared differences between the data points in\nthe data set and the model. The method of least squares\nprovided the foundation for statistical learning methods\nsuch as linear regression and logistic regression as well as the\ndevelopment of artificial neural network models in artifi-\ncial intelligence (we will return to least squares, regression\nanalysis, and neural networks in chapter 4).\nBetween 1780 and 1820, around the same time that\nLaplace and Gauss were making their contributions to\nstatistical learning, a Scottish engineer named William\nPlayfair was inventing statistical graphics and laying the\nfoundations for modern data visualization and exploratory\ndata analysis. Playfair invented the line chart and area chart\nfor time-series data, the bar chart to illustrate compari-\nsons between quantities of different categories, and the\npie chart to illustrate proportions within a set. The advan-\ntage of visualizing quantitative data is that it allows us to\n12 Chapter 1", "use our powerful visual abilities to summarize, compare,\nand interpret data. Admittedly, it is difficult to visualize\nlarge (many data points) or complex (many attributes)\ndata sets, but data visualization is still an important part\nof data science. In particular, it is useful in helping data sci-\nentists explore and understand the data they are working\nwith. Visualizations can also be useful to communicate the\nresults of a data science project. Since Playfair\u2019s time, the\nvariety of data-visualization graphics has steadily grown,\nand today there is research ongoing into the development\nof novel approaches to visualize large, multidimensional\ndata sets. A recent development is the t-distributed stochas-\ntic neighbor embedding (t-SNE) algorithm, which is a use-\nful technique for reducing high-dimensional data down to\ntwo or three dimensions, thereby facilitating the visualiza-\ntion of those data.\nThe developments in probability theory and statis-\ntics continued into the twentieth century. Karl Pearson\ndeveloped modern hypothesis testing, and R. A. Fisher\ndeveloped statistical methods for multivariate analysis\nand introduced the idea of maximum likelihood estimate\ninto statistical inference as a method to draw conclusions\nbased on the relative probability of events. The work of\nAlan Turing in the Second World War led to the inven-\ntion of the electronic computer, which had a dramatic\nimpact on statistics because it enabled much more com-\nplex statistical calculations. Throughout the 1940s and\nWhat Is Data sCIenCe? 13", "subsequent decades, a number of important computa-\ntional models were developed that are still widely used in\ndata science. In 1943, Warren McCulloch and Walter Pitts\nproposed the first mathematical model of a neural net-\nwork. In 1948, Claude Shannon published \u201cA Mathemati-\ncal Theory of Communication\u201d and by doing so founded\ninformation theory. In 1951, Evelyn Fix and Joseph Hodges\nproposed a model for discriminatory analysis (what would\nnow be called a classification or pattern-recognition prob-\nlem) that became the basis for modern nearest-neighbor\nmodels. These postwar developments culminated in 1956\nin the establishment of the field of artificial intelligence\nat a workshop in Dartmouth College. Even at this early\nstage in the development of artificial intelligence, the\nterm machine learning was beginning to be used to de-\nscribe programs that gave a computer the ability to learn\nfrom data. In the mid-1960s, three important contribu-\ntions to ML were made. In 1965, Nils Nilsson\u2019s book titled\nLearning Machines showed how neural networks could be\nused to learn linear models for classification. The follow-\ning year, 1966, Earl B. Hunt, Janet Marin, and Philip J.\nStone developed the concept-learning system frame-\nwork, which was the progenitor of an important family\nof ML algorithms that induced decision-tree models from\ndata in a top-down fashion. Around the same time, a\nnumber of independent researchers developed and pub-\nlished early versions of the k-means clustering algorithm,\n14 Chapter 1", "now the standard algorithm used for data (customer)\nsegmentation.\nThe field of ML is at the core of modern data science\nbecause it provides algorithms that are able to automati-\ncally analyze large data sets to extract potentially interest-\ning and useful patterns. Machine learning has continued\nto develop and innovate right up to the present day. Some\nof the most important developments include ensemble\nmodels, where predictions are made using a set (or com-\nmittee) of models, with each model voting on each query,\nand deep-learning neural networks, which have multiple\n(i.e., more than three) layers of neurons. These deeper lay-\ners in the network are able to discover and learn complex\nattribute representations (composed of multiple, interact-\ning input attributes that have been processed by earlier\nlayers), which in turn enable the network to learn patterns\nthat generalize across the input data. Because of their abil-\nity to learn complex attributes, deep-learning networks\nare particularly suitable to high-dimensional data and so\nhave revolutionized a number of fields, including machine\nvision and natural-language processing.\nAs we discussed in our review of database history, the\nearly 1970s marked the beginning of modern database\ntechnology with Edgar F. Codd\u2019s relational data model and\nthe subsequent explosion of data generation and storage\nthat led to the development of data warehousing in the\n1990s and more recently to the phenomenon of big data.\nWhat Is Data sCIenCe? 15", "However, well before the emergence of big data, in fact by\nthe late 1980s and early 1990s, the need for a field of re-\nsearch specifically targeting the analysis of these large data\nsets was apparent. It was around this time that the term\ndata mining started to be used in the database communi-\nties. As we have already discussed, one response to this\nneed was the development of data warehouses. However,\nother database researchers responded by reaching out\nto other research fields, and in 1989 Gregory Piatetsky-\nShapiro organized the first workshop on knowledge dis-\ncovery in databases (KDD). The announcement of the first\nKDD workshop neatly sums how the workshop focused on\na multidisciplinary approach to the problem of analyzing\nlarge databases:\nKnowledge discovery in databases poses many\ninteresting problems, especially when databases\nare large. Such databases are usually accompanied\nby substantial domain knowledge which can\nsignificantly facilitate discovery. Access to large\ndatabases is expensive\u2014hence the need for sampling\nand other statistical methods. Finally, knowledge\ndiscovery in databases can benefit from many\navailable tools and techniques from several different\nfields including expert systems, machine learning,\nintelligent databases, knowledge acquisition, and\nstatistics.1\n16 Chapter 1", "In fact, the terms knowledge discovery in databases and\ndata mining describe the same concept, the distinction\nbeing that data mining is more prevalent in the business\ncommunities and KDD more prevalent in academic com-\nmunities. Today, these terms are often used interchange-\nably,2 and many of the top academic venues use both.\nIndeed, the premier academic conference in the field is the\nInternational Conference on Knowledge Discovery and\nData Mining.\nThe Emergence and Evolution of Data Science\nThe term data science came to prominence in the late 1990s\nin discussions relating to the need for statisticians to join\nwith computer scientists to bring mathematical rigor to\nthe computational analysis of large data sets. In 1997,\nC. F. Jeff Wu\u2019s public lecture \u201cStatistics = Data Science?\u201d\nhighlighted a number of promising trends for statistics,\nincluding the availability of large/complex data sets in\nmassive databases and the growing use of computational\nalgorithms and models. He concluded the lecture by call-\ning for statistics to be renamed \u201cdata science.\u201d\nIn 2001, William S. Cleveland published an action plan\nfor creating a university department in the field of data\nscience (Cleveland 2001). The plan emphasizes the need\nfor data science to be a partnership between mathematics\nand computer science. It also emphasizes the need for data\nscience to be understood as a multidisciplinary endeavor\nWhat Is Data sCIenCe? 17", "and for data scientists to learn how to work and engage\nwith subject-matter experts. In the same year, Leo Brei-\nman published \u201cStatistical Modeling: The Two Cultures\u201d\n(2001). In this paper, Breiman characterizes the tradi-\ntional approach to statistics as a data-modeling culture\nthat views the primary goal of data analysis as identifying\nthe (hidden) stochastic data model (e.g., linear regression)\nthat explains how the data were generated. He contrasts\nthis culture with the algorithmic-modeling culture that\nfocuses on using computer algorithms to create prediction\nmodels that are accurate (rather than explanatory in terms\nof how the data was generated). Breiman\u2019s distinction be-\ntween a statistical focus on models that explain the data\nversus an algorithmic focus on models that can accurately\npredict the data highlights a core difference between stat-\nisticians and ML researchers. The debate between these\napproaches is still ongoing within statistics (see, for ex-\nample, Shmueli 2010). In general, today most data science\nprojects are more aligned with the ML approach of build-\ning accurate prediction models and less concerned with\nthe statistical focus on explaining the data. So although\ndata science became prominent in discussions relating\nto statistics and still borrows methods and models from\nstatistics, it has over time developed its own distinct ap-\nproach to data analysis.\nSince 2001, the concept of data science has broad-\nened well beyond that of a redefinition of statistics. For\n18 Chapter 1", "example, over the past 10 years there has been a tremen-\ndous growth in the amount of the data generated by online\nactivity (online retail, social media, and online entertain-\nment). Gathering and preparing these data for use in data\nscience projects has resulted in the need for data scientists\nto develop the programming and hacking skills to scrape,\nmerge, and clean data (sometimes unstructured data)\nfrom external web sources. Also, the emergence of big data\nhas meant that data scientists need to be able to work with\nbig-data technologies, such as Hadoop. In fact, today the\nrole of a data scientist has become so broad that there is an\nongoing debate regarding how to define the expertise and\nskills required to carry out this role.3 It is, however, pos-\nsible to list the expertise and skills that most people would\nagree are relevant to the role, which are shown in figure 1.\nIt is difficult for an individual to master all of these areas,\nand, indeed, most data scientists usually have in-depth\nknowledge and real expertise in just a subset of them.\nHowever, it is important to understand and be aware of\neach area\u2019s contribution to a data science project.\nData scientists should have some domain exper-\ntise. Most data science projects begin with a real-world,\ndomain-specific problem and the need to design a data-\ndriven solution to this problem. As a result, it is important\nfor a data scientist to have enough domain expertise that\nthey understand the problem, why it is important, and\nhow a data science solution to the problem might fit into\nWhat Is Data sCIenCe? 19", "Figure 1 A skills-set desideratum for a data scientist.\n20 Chapter 1", "an organization\u2019s processes. This domain expertise guides\nthe data scientist as she works toward identifying an op-\ntimized solution. It also enables her to engage with real\ndomain experts in a meaningful way so that she can illicit\nand understand relevant knowledge about the underly-\ning problem. Also, having some experience of the project\ndomain allows the data scientist to bring her experiences\nfrom working on similar projects in the same and related\ndomains to bear on defining the project focus and scope.\nData are at the center of all data science projects.\nHowever, the fact that an organization has access to data\ndoes not mean that it can legally or should ethically use\nthe data. In most jurisdictions, there is antidiscrimina-\ntion and personal-data-protection legislation that regu-\nlates and controls the use of data usage. As a result, a data\nscientist needs to understand these regulations and also,\nmore broadly, to have an ethical understanding of the im-\nplications of his work if he is to use data legally and ap-\npropriately. We return to this topic in chapter 6, where we\ndiscuss the legal regulations on data usage and the ethical\nquestions related to data science.\nIn most organizations, a significant portion of the\ndata will come from the databases in the organization.\nFurthermore, as the data architecture of an organization\ngrows, data science projects will start incorporating data\nfrom a variety of other data sources, which are commonly\nreferred to as \u201cbig-data sources.\u201d The data in these data\nWhat Is Data sCIenCe? 21", "sources can exist in a variety of different formats, gener-\nally a database of some form\u2014relational, NoSQL, or Ha-\ndoop. All of the data in these various databases and data\nsources will need to be integrated, cleansed, transformed,\nnormalized, and so on. These tasks go by many names,\nsuch as extraction, transformation, and load, \u201cdata mung-\ning,\u201d \u201cdata wrangling,\u201d \u201cdata fusion,\u201d \u201cdata crunching,\u201d and\nso on. Like source data, the data generated from data sci-\nence activities also need to be stored and managed. Again,\na database is the typical storage location for the data gen-\nerated by these activities because they can then be easily\ndistributed and shared with different parts of the organi-\nzation. As a consequence, data scientists need to have the\nskills to interface with and manipulate data in databases.\nA range of computer science skills and tools allows\ndata scientists to work with big data and to process it\ninto new, meaningful information. High-performance\ncomputing (HPC) involves aggregating computing power\nto deliver higher performance than one can get from a\nstand-alone computer. Many data science projects work\nwith a very large data set and ML algorithms that are com-\nputationally expensive. In these situations, having the\nskills required to access and use HPC resources is impor-\ntant. Beyond HPC, we have already mentioned the need\nfor data scientists to be able to scrap, clean, and integrate\nweb data as well as handle and process unstructured text\nand images. Furthermore, a data scientist may also end up\n22 Chapter 1", "writing in-house applications to perform a specific task or\naltering an existing application to tune it to the data and\ndomain being processed. Finally, computer science skills\nare also required to be able to understand and develop the\nML models and integrate them into the production or ana-\nlytic or back-end applications in an organization.\nPresenting data in a graphical format makes it much\neasier to see and understand what is happening with the\ndata. Data visualization applies to all phases of the data\nscience process. When data are inspected in tabular form,\nit is easy to miss things such as outliers or trends in dis-\ntributions or subtle changes in the data through time.\nHowever, when data are presented in the correct graphical\nform, these aspects of the data can pop out. Data visualiza-\ntion is an important and growing field, and we recommend\ntwo books, The Visual Display of Quantitative Information\nby Edward Tufte (2001) and Show Me the Numbers: Design-\ning Tables and Graphs to Enlighten by Stephen Few (2012)\nas excellent introductions to the principles and techniques\nof effective data visualization.\nMethods from statistics and probability are used\nthroughout the data science process, from the initial\ngathering and investigation of the data right through\nto the comparing of the results of different models and\nanalyses produced during the project. Machine learning\ninvolves using a variety of advanced statistical and com-\nputing techniques to process data to find patterns. The\nWhat Is Data sCIenCe? 23", "data scientist who is involved in the applied aspects of\nML does not have to write his own versions of ML algo-\nrithms. By understanding the ML algorithms, what they\ncan be used for, what the results they generate mean, and\nwhat type of data particular algorithms can be run on, the\ndata scientist can consider the ML algorithms as a gray\nbox. This allows him to concentrate on the applied aspects\nof data science and to test the various ML algorithms to\nsee which ones work best for the scenario and data he is\nconcerned with.\nFinally, a key aspect of being a successful data scien-\ntist is being able to communicate the story in the data.\nThis story might uncover the insight that the analysis of\nthe data has revealed or how the models created during a\nproject fit into an organization\u2019s processes and the likely\nimpact they will have on the organization\u2019s functioning.\nThere is no point executing a brilliant data science proj-\nect unless the outputs from it are used and the results are\ncommunicated in such a way that colleagues with a non-\ntechnical background can understand them and have con-\nfidence in them.\nWhere Is Data Science Used?\nData science drives decision making in nearly all parts of\nmodern societies. In this section, we describe three case\n24 Chapter 1", "studies that illustrate the impact of data science: consumer\ncompanies using data science for sales and marketing;\ngovernments using data science to improve health, crimi-\nnal justice, and urban planning; and professional sporting\nfranchises using data science in player recruitment.\nData Science in Sales and Marketing\nWalmart has access to large data sets about its customers\u2019\npreferences by using point-of-sale systems, by tracking\ncustomer behavior on the Walmart website, and by track-\ning social media commentary about Walmart and its prod-\nucts. For more than a decade, Walmart has been using data\nscience to optimize the stock levels in stores, a well-known\nexample being when it restocked strawberry Pop-Tarts\nin stores in the path of Hurricane Francis in 2004 based\non an analysis of sales data preceding Hurricane Char-\nley, which had struck a few weeks earlier. More recently,\nWalmart has used data science to drive its retail revenues\nin terms of introducing new products based on analyzing\nsocial media trends, analyzing credit card activity to make\nproduct recommendations to customers, and optimizing\nand personalizing customers\u2019 online experience on the\nWalmart website. Walmart attributes an increase of 10 to\n15 percent in online sales to data science optimizations\n(DeZyre 2015).\nThe equivalent of up-selling and cross-selling in the\nonline world is the \u201crecommender system.\u201d If you have\nWhat Is Data sCIenCe? 25", "watched a movie on Netflix or purchased an item on Ama-\nzon, you will know that these websites use the data they\ncollect to provide suggestions for what you should watch\nor buy next. These recommender systems can be designed\nto guide you in different ways: some guide you toward\nblockbusters and best sellers, whereas others guide you\ntoward niche items that are specific to your tastes. Chris\nAnderson\u2019s book The Long Tail (2008) argues that as pro-\nduction and distribution get less expensive, markets shift\nfrom selling large amounts of a small number of hit items\nto selling smaller amounts of a larger number of niche\nitems. This trade-off between driving sales of hit or niche\nproducts is a fundamental design decision for a recom-\nmender system and affects the data science algorithms\nused to implement these systems.\nGovernments Using Data Science\nIn recent years, governments have recognized the advan-\ntages of adopting data science. In 2015, for example, the\nUS government appointed Dr. D. J. Patil as the first chief\ndata scientist. Some of the largest data science initiatives\nspearheaded by the US government have been in health.\nData science is at the core of the Cancer Moonshot4 and\nPrecision Medicine Initiatives. The Precision Medicine\nInitiative combines human genome sequencing and\ndata science to design drugs for individual patients. One\npart of the initiative is the All of Us program,5 which is\n26 Chapter 1", "gathering environment, lifestyle, and biological data from\nmore than one million volunteers to create the world\u2019s\nbiggest data sets for precision medicine. Data science is\nalso revolutionizing how we organize our cities: it is used\nto track, analyze, and control environmental, energy, and\ntransport systems and to inform long-term urban plan-\nning (Kitchin 2014a). We return to health and smart cit-\nies in chapter 7 when we discuss how data science will\nbecome even more important in our lives over the coming\ndecades.\nThe US government\u2019s Police Data Initiative6 focuses\non using data science to help police departments under-\nstand the needs of their communities. Data science is\nalso being used to predict crime hot spots and recidivism.\nHowever, civil liberty groups have criticized some of the\nuses of data science in criminal justice. In chapter 6, we\ndiscuss the privacy and ethics questions raised by data\nscience, and one of the interesting factors in this discus-\nsion is that the opinions people have in relation to per-\nsonal privacy and data science vary from one domain to\nthe next. Many people who are happy for their personal\ndata to be used for publicly funded medical research have\nvery different opinions when it comes to the use of per-\nsonal data for policing and criminal justice. In chapter 6,\nwe also discuss the use of personal data and data science\nin determining life, health, car, home, and travel insurance\npremiums.\nWhat Is Data sCIenCe? 27", "Data Science in Professional Sports\nThe movie Moneyball (Bennett Miller, 2011), starring Brad\nPitt, showcases the growing use of data science in mod-\nern sports. The movie is based on the book of the same\ntitle (Lewis 2004), which tells the true story of how the\nOakland A\u2019s baseball team used data science to improve\nits player recruitment. The team\u2019s management identified\nthat a player\u2019s on-base percentage and slugging percent-\nage statistics were more informative indicators of offen-\nsive success than the statistics traditionally emphasized\nin baseball, such as a player\u2019s batting average. This insight\nenabled the Oakland A\u2019s to recruit a roster of undervalued\nplayers and outperform its budget. The Oakland A\u2019s suc-\ncess with data science has revolutionized baseball, with\nmost other baseball teams now integrating similar data-\ndriven strategies into their recruitment processes.\nThe moneyball story is a very clear example of how\ndata science can give an organization an advantage in a\ncompetitive market space. However, from a pure data sci-\nence perspective perhaps the most important aspect of\nthe moneyball story is that it highlights that sometimes\nthe primary value of data science is the identification of\ninformative attributes. A common belief is that the value\nof data science is in the models created through the pro-\ncess. However, once we know the important attributes\nin a domain, it is very easy to create data-driven models.\nThe key to success is getting the right data and finding\n28 Chapter 1", "The key to success is\ngetting the right data\nand finding the right\nattributes.", "the right attributes. In Freakonomics: A Rogue Economist\nExplores the Hidden Side of Everything, Steven D. Levitt and\nStephen Dubner illustrate the importance of this observa-\ntion across a wide range of problems. As they put it, the\nkey to understanding modern life is \u201cknowing what to\nmeasure and how to measure it\u201d (2009, 14). Using data\nscience, we can uncover the important patterns in a data\nset, and these patterns can reveal the important attributes\nin the domain. The reason why data science is used in\nso many domains is that it doesn\u2019t matter what the prob-\nlem domain is: if the right data are available and the\nproblem can be clearly defined, then data science can help.\nWhy Now?\nA number of factors have contributed to the recent growth\nof data science. As we have already touched upon, the emer-\ngence of big data has been driven by the relative ease with\nwhich organizations can gather data. Be it through point-\nof-sales transaction records, clicks on online platforms,\nsocial media posts, apps on smart phones, or myriad other\nchannels, companies can now build much richer profiles of\nindividual customers. Another factor is the commoditiza-\ntion of data storage with economies of scale, making it less\nexpensive than ever before to store data. There has also\nbeen tremendous growth in computer power. Graphics\n30 Chapter 1", "cards and graphical processing units (GPUs) were origi-\nnally developed to do fast graphics rendering for computer\ngames. The distinctive feature of GPUs is that they can\ncarry out fast matrix multiplications. However, matrix\nmultiplications are useful not only for graphics rendering\nbut also for ML. In recent years, GPUs have been adapted\nand optimized for ML use, which has contributed to large\nspeedups in data processing and model training. User-\nfriendly data science tools have also become available and\nlowered the barriers to entry into data science. Taken to-\ngether, these developments mean that it has never been\neasier to collect, store, and process data.\nIn the past 10 years there have also been major ad-\nvances in ML. In particular, deep learning has emerged\nand has revolutionized how computers can process lan-\nguage and image data. The term deep learning describes\na family of neural network models with multiple lay-\ners of units in the network. Neural networks have been\naround since the 1940s, but they work best with large,\ncomplex data sets and take a great deal of computing\nresources to train. So the emergence of deep learning is\nconnected with growth in big data and computing power.\nIt is not an exaggeration to describe the impact of deep\nlearning across a range of domains as nothing less than\nextraordinary.\nDeepMind\u2019s computer program AlphaGo7 is an ex-\ncellent example of how deep learning has transformed a\nWhat Is Data sCIenCe? 31", "field of research. Go is a board game that originated in\nChina 3,000 years ago. The rules of Go are much simpler\nthan chess; players take turns placing pieces on a board\nwith the goal of capturing their opponent\u2019s pieces or sur-\nrounding empty territory. However, the simplicity of the\nrules and the fact that Go uses a larger board means that\nthere are many more possible board configurations in Go\nthen there are in chess. In fact, there are more possible\nboard configurations for Go than there are atoms in the\nuniverse. This makes Go much more difficult than chess\nfor computers because of its much larger search space\nand difficulty in evaluating each of these possible board\nconfigurations. The DeepMind team used deep-learning\nmodels to enable AlphaGo to evaluate board configura-\ntions and to select the next move to make. The result was\nthat AlphaGo became the first computer program to beat\na professional Go player, and in March 2016 AlphaGo beat\nLed Sedol, the 18-time Go world champion, in a match\nwatched by more than 200 million people worldwide. To\nput the impact of deep learning on Go in context, as re-\ncently as 2009 the best Go computer program in the world\nwas rated at the low end of advanced amateur; seven\nyears later AlphaGo beat the world champion. In 2016,\nan article describing the deep-learning algorithms behind\nAlphaGo was published in the world\u2019s most prestigious ac-\nademic science journal, Nature (Silver, Huang, Maddison,\net al. 2016).\n32 Chapter 1", "Deep learning has also had a massive impact on a\nrange of high-profile consumer technologies. Facebook\nnow uses deep learning for face recognition and to ana-\nlyze text in order to advertise directly to individuals based\non their online conversations. Both Google and Baidu\nuse deep learning for image recognition, captioning and\nsearch, and machine translation. Apple\u2019s virtual assistant\nSiri, Amazon\u2019s Alexa, Microsoft\u2019s Cortana, and Samsung\u2019s\nBixby use speech recognition based on deep learning.\nHuawei is currently developing a virtual assistant for the\nChinese market, and it, too, will use deep-learning speech\nrecognition. In chapter 4, \u201cMachine Learning 101,\u201d we de-\nscribe neural networks and deep learning in more detail.\nHowever, although deep learning is an important techni-\ncal development, perhaps what is most significant about\nit in terms of the growth of data science is the increased\nawareness of the capabilities and benefits of data science\nand organization buy-in that has resulted from these high-\nprofile success stories.\nMyths about Data Science\nData science has many advantages for modern organiza-\ntions, but there is also a great deal of hype around it, so we\nshould understand what its limitations are. One of the big-\ngest myths is the belief that data science is an autonomous\nWhat Is Data sCIenCe? 33", "process that we can let loose on our data to find the answers\nto our problems. In reality, data science requires skilled\nhuman oversight throughout the different stages of the\nprocess. Humans analysts are needed to frame the prob-\nlem, to design and prepare the data, to select which ML\nalgorithms are most appropriate, to critically interpret the\nresults of the analysis, and to plan the appropriate action\nto take based on the insight(s) the analysis has revealed.\nWithout skilled human oversight, a data science project\nwill fail to meet its targets. The best data science outcomes\noccur when human expertise and computer power work\ntogether, as Gordon Linoff and Michael Berry put it: \u201cData\nmining lets computers do what they do best\u2014dig through\nlots of data. This, in turn, lets people do what people do\nbest, which is to set up the problem and understand the\nresults\u201d (2011, 3).\nThe widespread and growing use of data science\nmeans that today the biggest data science challenge for\nmany organizations is locating qualified human analysts\nand hiring them. Human talent in data science is at a\npremium, and sourcing this talent is currently the main\nbottleneck in the adoption of data science. To put this\ntalent shortfall in context, in 2011 a McKinsey Global\nInstitute report projected a shortfall in the United States\nof between 140,000 and 190,000 people with data sci-\nence and analytics skills and an even larger shortfall of\n1.5 million managers with the ability to understand data\n34 Chapter 1", "science and analytics processes at a level that will enable\nthem to interrogate and interpret the results of data sci-\nence appropriately (Manyika, Chui, Brown, et al. 2011).\nFive years on, in their 2016 report, the institute remained\nconvinced that data science has huge untapped value po-\ntential across an expanding range of applications but that\nthe talent shortfall will remain, with a predicted shortfall\nof 250,000 data scientists in the near term (Henke, Bug-\nhin, Chui, et al. 2016).\nThe second big myth of data science is that every data\nscience project needs big data and needs to use deep learn-\ning. In general, having more data helps, but having the\nright data is the more important requirement. Data sci-\nence projects are frequently carried out in organizations\nthat have significantly less resources in terms of data\nand computing power than Google, Baidu, or Microsoft.\nExamples indicative of the scale of smaller data science\nprojects include claim prediction in an insurance company\nthat processes around 100 claims a month; student drop-\nout prediction for a university with less than 10,000 stu-\ndents; membership dropout prediction for a union with\nseveral thousand members. So an organization doesn\u2019t\nneed to be handling terabytes of data or to have mas-\nsive computing resources at its disposal to benefit from\ndata science.\nA third data science myth is that modern data sci-\nence software is easy to use, and so data science is easy to\nWhat Is Data sCIenCe? 35", "do. It is true that data science software has become more\nuser-friendly. However, this ease of use can hide the fact\nthat doing data science properly requires both appropri-\nate domain knowledge and the expertise regarding the\nproperties of the data and the assumptions underpin-\nning the different ML algorithms. In fact, it has never\nbeen easier to do data science badly. Like everything else\nin life, if you don\u2019t understand what you are doing when\nyou do data science, you are going to make mistakes. The\ndanger with data science is that people can be intimidated\nby the technology and believe whatever results the soft-\nware presents to them. They may, however, have unwit-\ntingly framed the problem in the wrong way, entered the\nwrong data, or used analysis techniques with inappro-\npriate assumptions. So the results the software presents\nare likely to be the answer to the wrong question or to\nbe based on the wrong data or the outcome of the wrong\ncalculation.\nThe last myth about data science we want to mention\nhere is the belief that data science pays for itself quickly.\nThe truth of this belief depends on the context of the or-\nganization. Adopting data science can require significant\ninvestment in terms of developing data infrastructure\nand hiring staff with data science expertise. Furthermore,\ndata science will not give positive results on every project.\nSometimes there is no hidden gem of insight in the data,\n36 Chapter 1", "and sometimes the organization is not in a position to act\non the insight the analysis has revealed. However, in con-\ntexts where there is a well-understood business problem\nand the appropriate data and human expertise are avail-\nable, then data science can (often) provide the actionable\ninsight that gives an organization the competitive advan-\ntage it needs to succeed.\nWhat Is Data sCIenCe? 37", "", "2\nWHAT ARE DATA,\nAND WHAT IS A DATA SET?\nAs its name suggests, data science is fundamentally de-\npendent on data. In its most basic form, a datum or a piece\nof information is an abstraction of a real-world entity\n(person, object, or event). The terms variable, feature, and\nattribute are often used interchangeably to denote an in-\ndividual abstraction. Each entity is typically described by a\nnumber of attributes. For example, a book might have the\nfollowing attributes: author, title, topic, genre, publisher,\nprice, date published, word count, number of chapters,\nnumber of pages, edition, ISBN, and so on.\nA data set consists of the data relating to a collection\nof entities, with each entity described in terms of a set of\nattributes. In its most basic form,1 a data set is organized\nin an n * m data matrix called the analytics record, where n\nis the number of entities (rows) and m is the number of at-\ntributes (columns). In data science, the terms data set and", "analytics record are often used interchangeably, with the\nanalytics record being a particular representation of a data\nset. Table 1 illustrates an analytics record for a data set of\nclassic books. Each row in the table describes one book.\nThe terms instance, example, entity, object, case, individual,\nand record are used in data science literature to refer to\na row. So a data set contains a set of instances, and each\ninstance is described by a set of attributes.\nThe construction of the analytics record is a prerequi-\nsite of doing data science. In fact, the majority of the time\nand effort in data science projects is spent on creating,\ncleaning, and updating the analytics record. The analytics\nrecord is often constructed by merging information from\nmany different sources: data may have to be extracted\nfrom multiple databases, data warehouses, or computer\nfiles in different formats (e.g., spreadsheets or csv files) or\nscraped from the web or social media streams.\nTable 1 A Data Set of Classic Books\nID Title Author Year Cover Edition Price\n1 Emma Austen 1815 Paperback 20th $5.75\n2 Dracula Stoker 1897 Hardback 15th $12.00\n3 Ivanhoe Scott 1820 Hardback 8th $25.00\n4 Kidnapped Stevenson 1886 Paperback 11th $5.00\n40 Chapter 2", "Four books are listed in the data set in table 1. Ex-\ncluding the ID attribute\u2014which is simply a label for each\nrow and hence is not useful for analysis\u2014each book is\ndescribed using six attributes: title, author, year, cover,\nedition, and price. We could have included many more at-\ntributes for each book, but, as is typical of data science\nprojects, we needed to make a choice when we were de-\nsigning the data set. In this instance, we were constrained\nby the size of the page and the number of attributes we\ncould fit onto it. In most data science projects, however,\nthe constraints relate to what attributes we can actually\ngather and what attributes we believe, based on our do-\nmain knowledge, are relevant to the problem we are trying\nto solve. Including extra attributes in a data set does not\ncome without cost. First, there is the extra time and effort\nin collecting and quality checking the attribute informa-\ntion for each instance in the data set and integrating these\ndata into the analytics record. Second, including irrelevant\nor redundant attributes can have a negative effect on the\nperformance of many of the algorithms used to analyze\ndata. Including many attributes in a data set increases the\nprobability that an algorithm will find irrelevant or spuri-\nous patterns in the data that appear to be statistically sig-\nnificant only because of the particular sample of instances\nin the data set. The problem of how to choose the correct\nattribute(s) is a challenge faced by all data science projects,\nand sometimes it comes down to an iterative process of\nWhat are Data, anD What Is a Data set? 41", "trial-and-error experiments where each iteration checks\nthe results achieved using different subsets of attributes.\nThere are many different types of attributes, and for\neach attribute type different sorts of analysis are appro-\npriate. So understanding and recognizing different attri-\nbute types is a fundamental skill for a data scientist. The\nstandard types are numeric, nominal, and ordinal. Numeric\nattributes describe measurable quantities that are repre-\nsented using integer or real values. Numeric attributes can\nbe measured on either an interval scale or a ratio scale. In-\nterval attributes are measured on a scale with a fixed but\narbitrary interval and arbitrary origin\u2014for example, date\nand time measurements. It is appropriate to apply order-\ning and subtraction operations to interval attributes, but\nother arithmetic operations (such as multiplication and\ndivision) are not appropriate. Ratio scales are similar to\ninterval scales, but the scale of measurement possesses a\ntrue-zero origin. A value of zero indicates that none of the\nquantity is being measured. A consequence of a ratio scale\nhaving a true-zero origin is that we can describe a value\non a ratio scale as being a multiple (or ratio) of another\nvalue. Temperature is a useful example for distinguishing\nbetween interval and ratio scales.2 A temperature mea-\nsurement on the Celsius or Fahrenheit scale is an interval\nmeasurement because a 0 value on either of these scales\ndoes not indicate zero heat. So although we can compute\ndifferences between temperatures on these scales and\n42 Chapter 2", "compare these differences, we cannot say that a temper-\nature of 20\u00b0 Celsius is twice as warm as 10\u00b0 Celsius. By\ncontrast, a temperature measurement in Kelvins is on a\nratio scale because 0 K (absolute zero) is the temperature\nat which all thermal motion ceases. Other common exam-\nples of ratio-scale measurements include money quanti-\nties, weight, height, and marks on an exam paper (scale\n0\u2013100). In table 1, the \u201cyear\u201d attribute is an example of\nan interval-scale attribute, and the \u201cprice\u201d attribute is an\nexample of a ratio-scale attribute.\nNominal (also known as categorical) attributes take\nvalues from a finite set. These values are names (hence\n\u201cnominal\u201d) for categories, classes, or states of things. Ex-\namples of nominal attributes include marital status (sin-\ngle, married, divorced) and beer type (ale, pale ale, pils,\nporter, stout, etc.). A binary attribute is a special case of\na nominal attribute where the set of possible values is re-\nstricted to just two values. For example, we might have\nthe binary attribute \u201cspam,\u201d which describes whether an\nemail is spam (true) or not spam (false), or the binary at-\ntribute \u201csmoker,\u201d which describes whether an individual is\na smoker (true) or not (false). Nominal attributes cannot\nhave ordering or arithmetic operations applied to them.\nNote that a nominal attribute may be sorted alphabeti-\ncally, but alphabetizing is a distinct operation from order-\ning. In table 1, \u201cauthor\u201d and \u201ctitle\u201d are examples of nominal\nattributes.\nWhat are Data, anD What Is a Data set? 43", "Ordinal attributes are similar to nominal attributes,\nwith the difference that it is possible to apply a rank order\nover the categories of ordinal attributes. For example, an at-\ntribute describing the response to a survey question might\ntake values from the domain \u201cstrongly dislike, dislike, neu-\ntral, like, and strongly like.\u201d There is a natural ordering over\nthese values from \u201cstrongly dislike\u201d to \u201cstrongly like\u201d (or\nvice versa depending on the convention being used). How-\never, an important feature of ordinal data is that there is\nno notion of equal distance between these values. For ex-\nample, the cognitive distance between \u201cdislike\u201d and \u201cneu-\ntral\u201d may be different from the distance between \u201clike\u201d and\n\u201cstrongly like.\u201d As a result, it is not appropriate to apply\narithmetic operations (such as averaging) on ordinal at-\ntributes. In table 1, the \u201cedition\u201d attribute is an example of\nan ordinal attribute. The distinction between nominal and\nordinal data is not always clear-cut. For example, consider\nan attribute that describes the weather and that can take\nthe values \u201csunny,\u201d \u201crainy,\u201d \u201covercast.\u201d One person might\nview this attribute as being nominal, with no natural order\nover the values, whereas another person might argue that\nthe attribute is ordinal, with \u201covercast\u201d being treated as\nan intermediate value between \u201csunny\u201d and \u201crainy\u201d (Hall,\nWitten, and Frank 2011).\nThe data type of an attribute (numeric, ordinal, nomi-\nnal) affects the methods we can use to analyze and under-\nstand the data, including both the basic statistics we can\n44 Chapter 2", "The data type of an\nattribute (numeric,\nordinal, nominal) affects\nthe methods we can\nuse to analyze and\nunderstand the data.", "use to describe the distribution of values that an attribute\ntakes and the more complex algorithms we use to iden-\ntify the patterns of relationships between attributes. At\nthe most basic level of analysis, numeric attributes allow\narithmetic operations, and the typical statistical analysis\napplied to numeric attributes is to measure the central\ntendency (using the mean value of the attribute) and the\ndispersion of the attributes values (using the variance or\nstandard deviation statistics). However, it does not make\nsense to apply arithmetic operations to nominal or ordi-\nnal attributes. So the basic analysis of these types of at-\ntributes involves counting the number of times each of the\nvalues occurs in the data set or calculating the proportion\nof occurrence of each value or both.\nData are generated through a process of abstraction,\nso any data are the result of human decisions and choices.\nFor every abstraction, somebody (or some set of people)\nwill have made choices with regard to what to abstract\nfrom and what categories or measurements to use in the\nabstracted representation. The implication is that data are\nnever an objective description of the world. They are in-\nstead always partial and biased. As Alfred Korzybski has\nobserved, \u201cA map is not the territory it represents, but,\nif correct, it has a similar structure to the territory which\naccounts for its usefulness\u201d (1996, 58).\nIn other words, the data we use for data science are\nnot a perfect representation of the real-world entities and\n46 Chapter 2", "processes we are trying to understand, but if we are careful\nin how we design and gather the data that we use, then the\nresults of our analysis will provide useful insights into our\nreal-world problems. The moneyball story given in chapter\n1 is a great example of how the determinant of success\nin many data science projects is figuring out the correct\nabstractions (attributes) to use for a given domain. Recall\nthat the key to the moneyball story was that the Oakland\nA\u2019s figured out that a player\u2019s on-base percentage and slug-\nging percentage are better attributes to use to predict a\nplayer\u2019s offensive success than traditional baseball sta-\ntistics such as batting average. Using different attributes\nto describe players gave the Oakland A\u2019s a different and\nbetter model of baseball than the other teams had, which\nenabled it to identify undervalued players and to compete\nwith larger franchises using a smaller budget.\nThe moneyball story illustrates that the old computer\nscience adage \u201cgarbage in, garbage out\u201d is true for data\nscience: if the inputs to a computational process are in-\ncorrect, then the outputs from the process will be incor-\nrect. Indeed, two characteristics of data science cannot\nbe overemphasized: (a) for data science to be successful,\nwe need to pay a great deal of attention to how we create\nour data (in terms of both the choices we make in design-\ning the data abstractions and the quality of the data cap-\ntured by our abstraction processes), and (b) we also need\nto \u201csense check\u201d the results of a data science process\u2014that\nWhat are Data, anD What Is a Data set? 47", "is, we need to understand that just because the computer\nidentifies a pattern in the data this doesn\u2019t mean that it is\nidentifying a real insight in the processes we are trying to\nanalyze; the pattern may simply be based on the biases in\nour data design and capture.\nPerspectives on Data\nOther than type of data (numeric, nominal, and ordinal),\na number of other useful distinctions can be made re-\ngarding data. One such distinction is between structured\nand unstructured data. Structured data are data that can\nbe stored in a table, and every instance in the table has\nthe same structure (i.e., set of attributes). As an example,\nconsider the demographic data for a population, where\neach row in the table describes one person and consists of\nthe same set of demographic attributes (name, age, date\nof birth, address, gender, education level, job status, etc.).\nStructured data can be easily stored, organized, searched,\nreordered, and merged with other structured data. It is\nrelatively easy to apply data science to structured data be-\ncause, by definition, it is already in a format that is suit-\nable for integration into an analytics record. Unstructured\ndata are data where each instance in the data set may have\nits own internal structure, and this structure is not neces-\nsarily the same in every instance. For example, imagine a\n48 Chapter 2", "data set of webpages, with each webpage having a struc-\nture but this structure differing from one webpage to an-\nother. Unstructured data are much more common than\nstructured data. For example, collections of human text\n(emails, tweets, text messages, posts, novels, etc.) can be\nconsidered unstructured data, as can collections of sound,\nimage, music, video, and multimedia files. The variation in\nthe structure between the different elements means that\nit is difficult to analyze unstructured data in its raw form.\nWe can often extract structured data from unstructured\ndata using techniques from artificial intelligence (such as\nnatural-language processing and ML), digital signal pro-\ncessing, and computer vision. However, implementing\nand testing these data-transformation processes is expen-\nsive and time-consuming and can add significant financial\noverhead and time delays to a data science project.\nSometimes attributes are raw abstractions from an\nevent or object\u2014for example, a person\u2019s height, the num-\nber of words in an email, the temperature in a room, the\ntime or location of an event. But data can also be derived\nfrom other pieces of data. Consider the average salary in\na company or the variance in the temperature of a room\nacross a period of time. In both of these examples, the\nresulting data are derived from an original set of data by\napplying a function to the original raw data (individual sal-\naries or temperature readings). It is frequently the case that\nthe real value of a data science project is the identification\nWhat are Data, anD What Is a Data set? 49", "It is frequently the case\nthat the real value of a\ndata science project is\nthe identification of\none or more important\nderived attributes that\nprovide insight into a\nproblem.", "of one or more important derived attributes that provide\ninsight into a problem. Imagine we are trying to get a bet-\nter understanding of obesity within a population, and we\nare trying to understand the attributes of an individual\nthat identify him as being obese. We would begin by exam-\nining the raw attributes of individuals, such as their height\nand weight, but after studying the problem for some time\nwe might end up designing a more informative derived\nattribute such as the Body Mass Index (BMI). BMI is the\nratio of a person\u2019s mass and height. Recognizing that the\ninteraction between the raw attributes \u201cmass\u201d and \u201cheight\u201d\nprovides more information about obesity then either of\nthese two attributes can when examined independently\nwill help us to identify people in the population who are at\nrisk of obesity. Obviously, BMI is a simple example that we\nuse here to illustrate the importance of derived attributes.\nBut consider situations where the insight into the problem\nis given through multiple derived attributes, where each\nattribute involves two (or potentially more) additional\nattributes. It is in contexts where multiple attributes in-\nteract together that data science provides us with real\nbenefits because the algorithms we use can, in some cases,\nlearn the derived attributes from the raw data.\nThere are generally two terms for gathered raw data:\ncaptured data and exhaust data (Kitchin 2014a). Captured\ndata are collected through a direct measurement or obser-\nvation that is designed to gather the data. For example,\nWhat are Data, anD What Is a Data set? 51", "the primary purpose of surveys and experiments is to\ngather specific data on a particular topic of interest. By\ncontrast, exhaust data are a by-product of a process whose\nprimary purpose is something other than data capture.\nFor example, the primary purpose of many social media\ntechnologies is to enable users to connect with other peo-\nple. However, for every image shared, blog posted, tweet\nretweeted, or post liked, a range of exhaust data is gener-\nated: who shared, who viewed, what device was used, what\ntime of day, which device was used, how many people\nviewed/liked/retweeted, and so on. Similarly, the primary\npurpose of the Amazon website is to enable users to make\npurchases from the site. However, each purchase gener-\nates volumes of exhaust data: what items the user put into\nher basket, how long she stayed on the site, what other\nitems she viewed, and so on.\nOne of the most common types of exhaust data is\nmetadata\u2014that is, data that describe other data. When\nEdward Snowden released documents about the US Na-\ntional Security Agency\u2019s surveillance program PRISM, he\nrevealed that the agency was collecting a large amount of\nmetadata about people\u2019s phone calls. This meant that the\nagency was not actually recording the content of peoples\nphone calls (it was not doing wiretapping) but rather col-\nlecting the data about the calls, such as when the call was\nmade, who the recipient was, how long the call lasted, and\nso on (Pomerantz 2015). This type of data gathering may\nnot appear ominous, but the MetaPhone study carried\n52 Chapter 2", "out at Stanford highlighted the types of sensitive insights\nthat phone-call metadata can reveal about an individual\n(Mayer and Mutchler 2014). The fact that many organiza-\ntions have very specific purposes makes it relatively easy\nto infer sensitive information about a person based on his\nphone calls to these organizations. For example, some of\nthe people in the MetaPhone study made calls to Alcohol-\nics Anonymous, divorce lawyers, and medical clinics spe-\ncializing in sexually transmitted diseases. Patterns in calls\ncan also be revealing. The pattern analysis from the study\nshowed how patterns of calls reveal potentially very sensi-\ntive information:\nParticipant A communicated with multiple local\nneurology groups, a specialty pharmacy, a rare\ncondition management service, and a hotline for a\npharmaceutical used solely to treat relapsing multiple\nsclerosis. \u2026 In a span of three weeks, Participant D\ncontacted a home improvement store, locksmiths,\na hydroponics dealer, and a head shop. (Mayer and\nMutchler 2014)\nData science has traditionally focused on captured\ndata. However, as the MetaPhone study shows, exhaust\ndata can be used to reveal hidden insight into situations.\nIn recent years, exhaust data have become more and more\nuseful, particularly in the realm of customer engagement,\nwhere the linking of different exhaust data sets has the\nWhat are Data, anD What Is a Data set? 53", "potential to provide a business with a richer profile of indi-\nvidual customers, thereby enabling the business to target\nits services and marketing to certain customers. In fact,\none of the factors driving the growth in data science in\nbusiness today is the recognition of the value of exhaust\ndata and the potential that data science has to unlock this\nvalue for businesses.\nData Accumulates, Wisdom Doesn\u2019t!\nThe goal of data science is to use data to get insight and un-\nderstanding. The Bible urges us to attain understanding by\nseeking wisdom: \u201cwisdom is the principal thing, therefore\nget wisdom and with all thy getting get understanding\u201d\n(Proverbs 4:7 [King James]). This advice is reasonable, but\nit does beg the question of how one should go about seek-\ning wisdom. The following lines from T. S. Eliot\u2019s poem\n\u201cChoruses from The Rock\u201d describes a hierarchy of wis-\ndom, knowledge, and information:\nWhere is the wisdom we have lost in knowledge?\nWhere is the knowledge we have lost in information?\n(Eliot 1934, 96)\nEliot\u2019s hierarchy mirrors the standard model of the\nstructural relationships between wisdom, knowledge,\n54 Chapter 2", "information, and data known as the DIKW pyramid (see\nfigure 2). In the DIKW pyramid, data precedes informa-\ntion, which precedes knowledge, which precedes wisdom.\nAlthough the order of the layers in the hierarchy are gener-\nally agreed upon, the distinctions between the layers and\nthe processes required to move from one layer to the next\nare often contested. Broadly speaking, however,\nFigure 2 The DIKW pyramid (adapted from Kitchin 2014a).\nWhat are Data, anD What Is a Data set? 55", "\u2022 Data are created through abstractions or measurements\ntaken from the world.\n\u2022 Information is data that have been processed, structured,\nor contextualized so that it is meaningful to humans.\n\u2022 Knowledge is information that has been interpreted and\nunderstood by a human so that she can act on it if required.\n\u2022 Wisdom is acting on knowledge in an appropriate way.\nThe activities in the data science process can also be\nrepresented using a similar pyramid hierarchy where the\nwidth of the pyramid represents the amount of data be-\ning processed at each level and where the higher the layer\nin the pyramid, the more informative the results of the\nactivities are for decision making. Figure 3 illustrates the\nhierarchy of data science activities from data capture and\ngeneration through data preprocessing and aggregation,\ndata understanding and exploration, pattern discovery\nand model creation using ML, and decision support using\ndata-driven models deployed in the business context.\nThe CRISP-DM Process\nMany people and companies regularly put forward sug-\ngestions on the best process to follow to climb the data\nscience pyramid. The most commonly used process is the\n56 Chapter 2", "Figure 3 Data science pyramid (adapted from Han, Kamber, and Pei 2011).\nCross Industry Standard Process for Data Mining (CRISP-\nDM). In fact, the CRISP-DM has regularly been in the\nnumber-one spot in various industry surveys for a number\nof years. The primary advantage of CRISP-DM, the main\nreason why it is so widely used, is that it is designed to\nbe independent of any software, vendor, or data-analysis\ntechnique.\nWhat are Data, anD What Is a Data set? 57", "CRISP-DM was originally developed by a consortium\nof organizations consisting of leading data science ven-\ndors, end users, consultancy companies, and researchers.\nThe original CRISP-DM project was sponsored in part by\nthe European Commission under the ESPRIT Program,\nand the process was first presented at a workshop in\n1999. Since then, a number of attempts have been made\nto update the process, but the original version is still pre-\ndominantly in use. For many years, there was a dedicated\nwebsite for CRISP-DM, but in recent years this website is\nno longer available, and on occasion you might get redi-\nrected to the SPSS website by IBM, which was one of the\noriginal contributors to the project. The original consor-\ntium published a detailed (76-page) but readable step-by-\nstep guide to the process that is freely available online (see\nChapman et al. 1999), but the structure and major tasks of\nthe process can be summarized in a few pages.\nThe CRISP-DM life cycle consists of six stages: busi-\nness understanding, data understanding, data preparation,\nmodeling, evaluation, and deployment, as shown in figure 4.\nData are at the center of all data science activities, and that\nis why the CRISP-DM diagram has data at its center. The\narrows between the stages indicate the typical direction of\nthe process. The process is semistructured, which means\nthat a data scientist doesn\u2019t always move through these\nsix stages in a linear fashion. Depending on the outcome\nof a particular stage, a data scientist may go back to one of\n58 Chapter 2", "Business Data\nunderstanding understanding\nData\npreparation\nDeployment Data\nModeling\nEvaluation\nFigure 4 The CRISP-DM life cycle (based on figure 2 in Chapman, Clinton,\nKerber, et al. 1999).\nWhat are Data, anD What Is a Data set? 59", "the previous stages, redo the current stage, or move on to\nthe next stage.\nIn the first two stages, business understanding and\ndata understanding, the data scientist is trying to define\nthe goals of the project by understanding the business\nneeds and the data that the business has available to it.\nIn the early stages of a project, a data scientist will often\niterate between focusing on the business and exploring\nwhat data are available. This iteration typically involves\nidentifying a business problem and then exploring if the\nappropriate data are available to develop a data-driven so-\nlution to the problem. If the data are available, the project\ncan proceed; if not, the data scientist will have to identify\nan alternative problem to tackle. During this stage of a\nproject, a data scientist will spend a great deal of time in\nmeetings with colleagues in the business-focused depart-\nments (e.g., sales, marketing, operations) to understand\ntheir problems and with the database administrators to\nget an understanding of what data are available.\nOnce the data scientist has clearly defined a busi-\nness problem and is happy that the appropriate data are\navailable, she moves on to the next phase of the CRISP-\nDM: data preparation. The focus of the data-preparation\nstage is the creation of a data set that can be used for the\ndata analysis. In general, creating this data set involves\nintegrating data sources from a number of databases.\nWhen an organization has a data warehouse, this data\n60 Chapter 2", "integration can be relatively straightforward. Once a data\nset has been created, the quality of the data needs to be\nchecked and fixed. Typical data-quality problems include\noutliers and missing values. Checking the quality of the\ndata is very important because errors in the data can have\na serious effect on the performance of the data-analysis\nalgorithms.\nThe next stage of CRISP-DM is the modeling stage.\nThis is the stage where automatic algorithms are used to\nextract useful patterns from the data and to create models\nthat encode these patterns. Machine learning is the field\nof computer science that focuses on the design of these al-\ngorithms. In the modeling stage, a data scientist will nor-\nmally use a number of different ML algorithms to train\na number of different models on the data set. A model is\ntrained on a data set by running an ML algorithm on the\ndata set so as to identify useful patterns in the data and to\nreturn a model that encodes these patterns. In some cases\nan ML algorithm works by fitting a template model struc-\nture to a data set by setting the parameters of the template\nto good values for that data set (e.g., fitting a linear regres-\nsion or neural network model to a data set). In other cases\nan ML algorithm builds a model in a piecewise fashion (e.g.\ngrowing a decision tree one node at a time beginning at the\nroot node of the tree). In most data science projects it is\na model generated by an ML algorithm that is ultimately\nthe software that is deployed by an organization to help it\nWhat are Data, anD What Is a Data set? 61", "solve the problem the data science project is addressing.\nEach model is trained by a different type of ML algorithm,\nand each algorithm looks for different types of patterns\nin the data. At this stage in the project, the data scientist\ntypically doesn\u2019t know which patterns are the best ones to\nlook for in the data, so in this context it makes sense to\nexperiment with a number of different algorithms and see\nwhich algorithm returns the most accurate models when\nrun on the data set. In chapter 4 we will introduce ML al-\ngorithms and models in much more detail and explain how\nto create a test plan to evaluate model accuracy.\nIn the majority of data science projects, the initial\nmodel test results will uncover problems in the data. These\ndata errors sometimes come to light when the data scien-\ntist investigates why the performance of a model is lower\nthan expected or notices that maybe the model\u2019s perfor-\nmance is suspiciously good. Or by examining the structure\nof the models, the data scientist may find that the model is\nreliant on attributes that she would not expect, and as a re-\nsult she revisits the data to check that these attributes are\ncorrectly encoded. It is thus not uncommon for a project to\ngo through several rounds of these two stages of the pro-\ncess: modeling, data preparation; modeling, data prepara-\ntion; and so on. For example, Dan Steinberg and his team\nreported that during one data science project, they rebuilt\ntheir data set 10 times over a six-week period, and in week\nfive, having gone through a number of iterations of data\n62 Chapter 2", "cleaning and preparation, they uncovered a major error in\nthe data (Steinberg 2013). If this error had not been iden-\ntified and fixed, the project would not have succeeded.\nThe last two stages of the CRISP-DM process, evalu-\nation and deployment, are focused on how the models fit\nthe business and its processes. The tests run during the\nmodeling stage are focused purely on the accuracy of the\nmodels for the data set. The evaluation phase involves\nassessing the models in the broader context defined by\nthe business needs. Does a model meet the business ob-\njectives of the process? Is there any business reason why\na model is inadequate? At this point in the process, it is\nalso useful for the data scientist to do a general quality-\nassurance review on the project activities: Was anything\nmissed? Could anything have been done better? Based on\nthe general assessment of the models, the main decision\nmade during the evaluation phase is whether any of the\nmodels should be deployed in the business or another it-\neration of the CRISP-DM process is required to create ad-\nequate models. Assuming the evaluation process approves\na model or models, the project moves into the final stage\nof the process: deployment. The deployment phase in-\nvolves examining how to deploy the selected models into\nthe business environment. This involves planning how\nto integrate the models into the organization\u2019s techni-\ncal infrastructure and business processes. The best mod-\nels are the ones that fit smoothly into current practices.\nWhat are Data, anD What Is a Data set? 63", "Models that fit current practices have a natural set of users\nwho have a clearly defined problem that the model helps\nthem to solve. Another aspect of deployment is putting\na plan in place to periodically review the performance of\nthe model.\nThe outer circle of the CRISP-DM diagram (figure 4)\nhighlights how the whole process is iterative. The itera-\ntive nature of data science projects is perhaps the aspect of\nthese projects that is most often overlooked in discussions\nof data science. After a project has developed and deployed\na model, the model should be regularly reviewed to check\nthat it still fits the business\u2019s needs and that it hasn\u2019t be-\ncome obsolete. There are many reasons why a data-driven\nmodel can become obsolete: the business\u2019s needs might\nhave changed; the process the model emulates and pro-\nvides insight into might have changed (for example, cus-\ntomer behavior changes, spam email changes, etc.); or the\ndata streams the model uses might have changed (for ex-\nample, a sensor that feeds information into a model may\nhave been updated, and the new version of the sensor pro-\nvides slightly different readings, causing the model to be\nless accurate). The frequency of this review is dependent\non how quickly the business ecosystem and the data that\nthe model uses evolve. Constant monitoring is needed to\ndetermine the best time to go through the process again.\nThis is what the outer circle of the CRISP-DM process\nshown in figure 4 represents. For example, depending on\n64 Chapter 2", "the data, the business question, and the domain, you may\nhave go through this iterative process on a yearly, quar-\nterly, monthly, weekly, or even daily basis. Figure 5 gives a\nsummary of the different stages of the data science project\nprocess and the major tasks involved in each phase.\nA frequent mistake that many inexperienced data sci-\nentists make is to focus their efforts on the modeling stage\nof the CRISP-DM and to rush through the other stages.\nThey may think that the really important deliverable from\na project is the model, so the data scientist should devote\nmost of his time to building and finessing the model. How-\never, data science veterans will spend more time on ensur-\ning that the project has a clearly defined focus and that it\nhas the right data. For a data science project to succeed, a\ndata scientist needs to have a clear understanding of the\nbusiness need that the project is trying to solve. So the\nbusiness understanding stage of the process is really im-\nportant. With regard to getting the right data for a project,\na survey of data scientists in 2016 found that 79 percent\nof their time is spent on data preparation. The time spent\nacross the major tasks in the project was distributed as fol-\nlows: collecting data sets, 19 percent; cleaning and orga-\nnizing data, 60 percent; building training sets, 3 percent;\nmining data for patterns, 9 percent; refining algorithms,\n4 percent; and performing other tasks, 5 percent (Crowd-\nFlower 2016). The 79 percent figure for preparation comes\nfrom summing the time spent on collecting, cleaning, and\nWhat are Data, anD What Is a Data set? 65", "tnemyolpeD niatniam\nyolpeD nalp rotinoM nalp ecudorP lanif troper weiveR tcejorp\ndna\n.)9991\nnoitaulavE etaulavE stluser weiveR ssecorp enimreteD spets\ntxen .la\nte\n,rebreK\n,notnilC\ngniledoM tceleS gniledom euqinhcet etareneG tset ngised dliuB ledom ssessA ledom\n,nampahC\nni\n3\nerugif\nataD noitaraperp tceleS atad naelC atad tcurtsnoC atad etargetnI atad tamroF atad\nno\ndesab(\nsksat\ngnidnatsrednu\ndna\nataD tcelloC laitini atad ebircseD atad erolpxE atad yfireV atad ytilauq\nsegats\nMD-PSIRC\nssenisuB gnidnatsrednu enimreteD ssenisub sevitcejbo ssessA noitautis enimreteD ecneics slaog ecudorP tcejorp nalp ehT\natad 5\nerugiF", "organizing the data. That around 80 percent of project\ntime is spent on gathering and preparing data has been\na consistent finding in industry surveys for a number of\nyears. Sometimes this finding surprises people because\nthey imagine data scientists spend their time building\ncomplex models to extract insight from the data. But the\nsimple truth is that no matter how good your data analysis\nis, it won\u2019t identify useful patterns unless it is applied to\nthe right data.\nWhat are Data, anD What Is a Data set? 67", "", "3\nA DATA SCIENCE ECOSYSTEM\nThe set of technologies used to do data science varies\nacross organizations. The larger the organization or the\ngreater the amount of data being processed or both,\nthe greater the complexity of the technology ecosystem\nsupporting the data science activities. In most cases, this\necosystem contains tools and components from a number\nof different software suppliers, processing data in many\ndifferent formats. There is a spectrum of approaches from\nwhich an organization can select when developing its own\ndata science ecosystem. At one end of the spectrum, the\norganization may decide to invest in a commercial inte-\ngrated tool set. At the other end, it might build up a be-\nspoke ecosystem by integrating a set of open-source tools\nand languages. In between these two extremes, some\nsoftware suppliers provide solutions that consist of a mix-\nture of commercial products and open-source products.", "However, although the particular mix of tools will vary\nfrom one organization to the next, there is a commonality\nin terms of the components that are present in most data\nscience architectures.\nFigure 6 gives a high-level overview of a typical data\narchitecture. This architecture is not just for big-data\nenvironments, but for data environments of all sizes. In\nthis diagram, the three main areas consist of data sources,\nwhere all the data in an organization are generated; data\nstorage, where the data are stored and processed; and ap-\nplications, where the data are shared with consumers of\nthese data.\nAll organizations have applications that generate\nand capture data about customers, transactions, and op-\nerational data on everything to do with how the organiza-\ntion operates. Such data sources and applications include\ncustomer management, orders, manufacturing, delivery,\ninvoicing, banking, finance, customer-relationship man-\nagement (CRM), call center, enterprise resource planning\n(ERP) applications, and so on. These types of applications\nare commonly referred to as online transaction processing\n(OLTP) systems. For many data science projects, the data\nfrom these applications will be used to form the initial in-\nput data set for the ML algorithms. Over time, the volume\nof data captured by the various applications in the orga-\nnization grows ever larger and the organization will start\nto branch out to capture data that was ignored, wasn\u2019t\n70 Chapter 3", "erugif\nsisylana ataD\n).cte poodah/golb/moc.skrownotroh//:sptth\nsnoitacilppa\ndegakcaP\n,setisbew a yb\nsecruos\nderipsni(\n,sgolbew\natad-giB ecneics\n,rosnes\ndne-kcaB sessecorp\natad\n,laicoS(\nrof\nerutcetihcra\n,3102\nsnoitacilppa .)hcihw-esu-ot-nehw-esuoheraw-atad-eht-dna-\nmotsuC )sesabatad atad-gib ,32\nPPM\nsecruos lirpA\nSDO ,PALO dna ,rettelswen\natad-llams\nlanoitidarT ,PTLO\nesuoheraw\nssenisuB scitylana ataD\nsisylana ,snoitacilppA( skrownotroH\nlacipyt\nSMBDR\nataD\nA\n6 eht\nerugiF\nsnoitacilppA egarots ataD secruos ataD morf\na Data SCienCe eCoS yStem 71", "captured previously, or wasn\u2019t available previously. These\nnewer data are commonly referred to as \u201cbig-data sources\u201d\nbecause the volume of data that is captured is significantly\nhigher than the organization\u2019s main operational applica-\ntions. Some of the common big-data sources include\nnetwork traffic, logging data from various applications,\nsensor data, weblog data, social media data, website data,\nand so on. In traditional data sources, the data are typi-\ncally stored in a database. However, because the applica-\ntions associated with many of the newer big-data sources\nare not primarily designed to store data long term\u2014for\nexample, with streaming data\u2014the storage formats and\nstructures for this type of data vary from application to\napplication.\nAs the number of data sources increases, so does the\nchallenge of being able to use these data for analytics\nand for sharing them across the wider organization. The\ndata-storage layer, shown in figure 6, is typically used to\naddress the data sharing and data analytics across an or-\nganization. This layer is divided into two parts. The first\npart covers the typical data-sharing software used by most\norganizations. The most popular form of traditional data-\nintegration and storage software is a relational database\nmanagement system (RDBMS). These traditional sys-\ntems are often the backbone of the business intelligence\n(BI) solutions within an organization. A BI solution is a\nuser-friendly decision-support system that provides data\n72 Chapter 3", "aggregating, integration, and reporting as well as analy-\nsis functionality. Depending on the maturity level of a BI\narchitecture, it can consist of anything from a basic copy\nof an operational application to an operational data store\n(ODS) to massively parallel processing (MPP) BI database\nsolutions and data warehouses.\nData warehousing is best understood as a process of\ndata aggregation and analysis with the goal of supporting\ndecision making. However, the focus of this process is the\ncreation of a well-designed and centralized data reposi-\ntory, and the term data warehouse is sometimes used to\ndenote this type of data repository. In this sense, a data\nwarehouse is a powerful resource for data science. From a\ndata science perspective, one of the major advantages of\nhaving a data warehouse in place is a much shorter proj-\nect time. The key ingredient in any data science process\nis data, so it is not surprising that in many data science\nprojects the majority of time and effort goes into find-\ning, aggregating, and cleaning the data prior to their\nanalysis. If a data warehouse is available in a company,\nthen the effort and time that go into data preparation on\nindividual data science projects is often significantly re-\nduced. However, it is possible to do data science without\na centralized data repository. Constructing a centralized\nrepository of data involves more than simply dumping\nthe data from multiple operational databases into a single\ndatabase.\na Data SCienCe eCoS yStem 73", "Merging data from multiple databases often requires\nmuch complex manual work to resolve inconsistencies\nbetween the source databases. Extraction, transformation,\nand load (ETL) is the term used to describe the typical proc-\nesses and tools used to support the mapping, merging,\nand movement of data between databases. The typical op-\nerations carried out in a data warehouse are different from\nthe simple operations normally applied to a standard rela-\ntional data model database. The term online analytical pro-\ncessing (OLAP) is used to describe these operations. OLAP\noperations are generally focused on generating summaries\nof historic data and involve aggregating data from mul-\ntiple sources. For example, we might pose the following\nOLAP request (expressed here in English for readability):\n\u201cReport the sales of all stores by region and by quarter and\ncompare these figures to last year\u2019s figures.\u201d What this ex-\nample illustrates is that the result of an OLAP request of-\nten resembles what you would expect to see as a standard\nbusiness report. OLAP operations essentially enable users\nto slice, dice, and pivot the data in the warehouse and get\ndifferent views of these data. They work on a data repre-\nsentation called a data cube that is built on top of the data\nwarehouse. A data cube has a fixed, predefined set of di-\nmensions in which each dimension represents a particular\ncharacteristic of the data. The required data-cube dimen-\nsions for the example OLAP request given earlier would\nbe sales by stores, sales by region, and sales by quarter. The\n74 Chapter 3", "primary advantage of using a data cube with a fixed set of\ndimensions is that it speeds up the response time of OLAP\noperations. Also, because the set of data-cube dimensions\nis preprogrammed into the OLAP system, the system can\nprovide user-friendly graphical user interfaces for defining\nOLAP requests. However, the data-cube representation\nalso restricts the types of analysis that can be done using\nOLAP to the set of queries that can be generated using the\npredefined dimensions. By comparison, SQL provides a\nmore flexible query interface. Also, although OLAP sys-\ntems are useful for data exploration and reporting, they\ndon\u2019t enable data modeling or the automatic extraction of\npatterns from the data. Once the data from across an orga-\nnization has been aggregated and analyzed within the BI\nsystem, this analysis can then be used as input to a range\nof consumers in the applications layer of figure 6.\nThe second part of the data-storage layer deals with\nmanaging the data produced by an organization\u2019s big-data\nsources. In this architecture, the Hadoop platform is used\nfor the storage and analytics of these big data. Hadoop\nis an open-source framework developed by the Apache\nSoftware Foundation that is designed for the processing\nof big data. It uses distributed storage and processing\nacross clusters of commodity servers. Applying the Ma-\npReduce programming model, it speeds up the processing\nof queries on large data sets. MapReduce implements the\nsplit-apply-combine strategy: (a) a large data set is split up\na Data SCienCe eCoS yStem 75", "into separate chunks, and each chunk is stored on a dif-\nferent node in the cluster; (b) a query is then applied to\nall the chunks in parallel; and (c) the result of the query\nis then calculated by combining the results generated on\nthe different chunks. Over the past couple of years, how-\never, the Hadoop platform is also being used as an exten-\nsion of an enterprise\u2019s data warehouse. Data warehouses\noriginally would store three years of data, but now data\nwarehouses can store more than 10 years of data, and this\nnumber keeps increasing. As the amount of data in a data\nwarehouse increases, however, the storage and processing\nrequirements of the database and server also have to in-\ncrease. This requirement can have a significant cost im-\nplication. An alternative is to move some of the older data\nin a data warehouse for storage into a Hadoop cluster. For\nexample, the data warehouse would store the most recent\ndata, say three years\u2019 worth of data, which frequently need\nto be available for quick analysis and presentation, while\nthe older data and the less frequently used data are stored\non Hadoop. Most of the enterprise-level databases have\nfeatures that connect the data warehouse with Hadoop,\nallowing a data scientist, using SQL, to query the data in\nboth places as if they all are located in one environment.\nHer query could involve accessing some data in the data-\nwarehouse database and some of the data in Hadoop.\nThe query processing will be automatically divided into\ntwo distinct parts, each running independently, and the\n76 Chapter 3", "results will be automatically combined and integrated be-\nfore being presented back to the data scientist.\nData analysis is associated with both sections of the\ndata-storage layer in figure 6. Data analysis can occur on\nthe data in each section of the data layer, and the results\nfrom data analysis can be shared between each section\nwhile additional data analysis is being performed. The data\nfrom traditional sources frequently are relatively clean and\ninformation dense compared to the data captured from\nbig-data sources. However, the volume and real-time na-\nture of many big-data sources means that the effort in-\nvolved in preparing and analyzing these big-data sources\ncan be repaid in terms of additional insights not available\nthrough the data coming from traditional sources. A vari-\nety of data-analysis techniques developed across a number\nof different fields of research (including natural-language\nprocessing, computer vision, and ML) can be used to\ntransform unstructured, low-density, low-value big data\ninto high-density and high-value data. These high-value\ndata can then be integrated with the other high-value data\nfrom traditional sources for further data analysis. The de-\nscription given in this chapter and illustrated in figure 6\nis the typical architecture of the data science ecosystem.\nIt is suitable for most organizations, both small and large.\nHowever, as an organization scales in size, so too will the\ncomplexity of its data science ecosystem. For example,\nsmaller-scale organizations may not require the Hadoop\na Data SCienCe eCoS yStem 77", "component, but for very large organizations the Hadoop\ncomponent will become very important.\nMoving the Algorithms to the Data\nThe traditional approach to data analysis involves the ex-\ntraction of data from various databases, integrating the\ndata, cleaning the data, subsetting the data, and building\npredictive models. Once the prediction models have been\ncreated they can be applied to the new data. Recall from\nchapter 1 that a prediction model predicts the missing\nvalue of an attribute: a spam filter is a prediction model\nthat predicts whether the classification attribute of an\nemail should have the value of \u201cspam\u201d or not. Applying the\npredictive models to the instances in new data to generate\nthe missing values is known as \u201cscoring the data.\u201d Then\nthe final results, after scoring new data, may be loaded\nback into a database so that these new data can be used\nas part of some workflow, reporting dashboard, or some\nother company assessment practice. Figure 7 illustrates\nthat much of the data processing involved in data prepa-\nration and analysis is located on a server that is separate\nfrom the databases and the data warehouse. Therefore, a\nsignificant amount of time can be spent just moving the\ndata out of the database and moving the results back into\nthe database.\n78 Chapter 3", "Database or\nData data warehouse Loading\nextraction results\nAnalytics server\nExtract Integrate Prepare Build models\ndata data data and score data\nTime\nFigure 7 The traditional process for building predictive models and\nscoring data.\nAn experiment run at the Dublin Institute of Tech-\nnology on building a linear-regression model supplies an\nexample of the time involved in each part of the process.\nApproximately 70 to 80 percent of the time is taken with\nextracting and preparing the data; the remaining time is\nspent on building the models. For scoring data, approxi-\nmately 90 percent of the time is taken with extracting\nthe data and saving the scored data set back into the da-\ntabase; only 10 percent of the time is spent on actually\nscoring. These results are based on data sets consisting of\nanywhere from 50,000 records up to 1.5 million records.\na Data SCienCe eCoS yStem 79", "A significant amount of\ntime can be spent just\nmoving the data out\nof the database and\nmoving the results back\ninto the database.", "Most enterprise database vendors have recognized the\ntime savings that would be available if time did not have\nto be spent on moving data and have responded to this\nproblem by incorporating data-analysis functionality and\nML algorithms into their database engines. The following\nsections explore how ML algorithms have been integrated\ninto modern databases, how data storage works in the big-\ndata world of Hadoop, and how using a combination of\nthese two approaches allows organizations to easily work\nwith all their data using SQL as a common language for\naccessing, analyzing, and performing ML and predictive\nanalytics in real time.\nThe Traditional Database or the Modern Traditional\nDatabase\nDatabase vendors continuously invest in developing the\nscalability, performance, security, and functionality of\ntheir databases. Modern databases are far more advanced\nthan traditional relational databases. They can store and\nquery data in variety of different formats. In addition\nto the traditional relational formats, it is also possible\nto define object types, store documents, and store and\nquery JSON objects, spatial data, and so on. Most mod-\nern databases also come with a large number of statisti-\ncal functions, so that some have an equivalent number\nof statistical functions as most statistical applications.\nFor example, the Oracle Database comes with more than\na Data SCienCe eCoS yStem 81", "300 different statistical functions and the SQL language\nbuilt into it. These statistical functions cover the major-\nity of the statistical analyses needed by data science proj-\nects and include most if not all the statistical functions\navailable in other tools and languages, such as R. Using\nthe statistical functionality that is available in the data-\nbases in an organization may allow data analytics to be\nperformed in a more efficient and scalable manner using\nSQL. Furthermore, most leading database vendors (in-\ncluding Oracle, Microsoft, IBM, and EnterpriseDB) have\nintegrated many ML algorithms into their databases, and\nthese algorithms can be run using SQL. ML that is built\ninto the database engine and is accessible using SQL is\nknown as in-database machine learning. In-database ML\ncan lead to quicker development of models and quicker\ndeployment of models and results to applications and\nanalytic dashboards. The idea behind the in-database\nML algorithms is captured in the following directive:\n\u201cMove the algorithms to the data instead of the data to the\nalgorithms.\u201d\nThe main advantages of using the in-database ML\nalgorithms are:\n\u2022 No data movement. Some data science products re-\nquire the data to be exported from the database and\nconverted to a specialized format for input to the ML\nalgorithms. With in-database ML, no data movement or\n82 Chapter 3", "conversion is needed. This makes the entire process less\ncomplex, less time-consuming, and less error prone.\n\u2022 Faster performance. With analytical operations per-\nformed in the database and with no data movement, it is\npossible to utilize the computing capabilities of the data-\nbase server, delivering performance up to 100 times faster\nthan the traditional approach. Most database servers have\nhigh specifications, with many central processing units\n(CPUs) and efficient memory management to process data\nsets containing more than one billion records.\n\u2022 High security. The database provides controlled and\nauditable access to the data in the database, accelerating\nthe data scientist\u2019s productivity while maintaining data\nsecurity. Also, in-database ML avoids the physical secu-\nrity risks inherent in extracting and downloading data to\nalternative analytics servers. The traditional process, in\ncontrast, results in the creation of many copies (and po-\ntentially different versions) of data sets in separate silos\nacross the organization.\n\u2022 Scalability. A database can easily scale the analytics\nas the data volume increases if the ML algorithms are\nbrought into the database. The database software is de-\nsigned to manage large volumes of data efficiently, utiliz-\ning the multiple CPUs and memory on the server to allow\nthe ML algorithms to run in parallel. Databases are also\nvery efficient at processing large data sets that do not fit\na Data SCienCe eCoS yStem 83", "easily into memory. Databases have more than 40 years of\ndevelopment work behind them to ensure that they can\nprocess datasets quickly.\n\u2022 Real-time deployment and environments. The models\nthat are developed using the in-database ML algorithms\ncan be immediately deployed and used in real-time envi-\nronments. This allows the integration of the models into\neveryday applications, providing real-time predictions to\nend users and customers.\n\u2022 Production deployment. Models developed using stand-\nalone ML software may have to be recoded into other\nprogramming languages before they can be deployed\ninto enterprise applications. This is not the case with in-\ndatabase ML. SQL is the language of the database; it can\nbe used and called by any programming language and data\nscience tool. It is then a simple task to incorporate the in-\ndatabase models into production applications.\nMany organizations are exploiting the benefits of\nin-database ML. They range from small and medium or-\nganizations to large, big-data-type organizations. Some\nexamples of organizations that use in-database ML tech-\nnologies are:\n\u2022 Fiserv, an American provider of financial services and\nfraud detection and analysis. Fiserv migrated from using\n84 Chapter 3", "multiple vendors for data storage and ML to using just\nthe ML capabilities in its database. By using in-database\nML, the time used for creating/updating and deploying a\nfraud-detection model went from nearly a week to just a\nfew hours.\n\u2022 84.51\u00b0 (formally Dunnhumby USA), a customer science\ncompany. 84.51\u00b0 used many different analytic products to\ncreate its various customer models. It typically would spend\nmore than 318 hours each month moving data from its da-\ntabase to its ML tools and back again, plus an additional 67\nhours a month to create models. When it switched to using\nthe ML algorithms in its database, there was no more need\nfor data movement. The data stayed in the database. The\ncompany immediately saved more than 318 hours of time\nper month. Because it was using its database as a compute\nengine, it was able to scale its analytics, and the time taken\nto generate or update its ML models went from more than\n67 hours to one hour per month. This gave the company\na saving of sixteen days each month. It is now able to get\nsignificantly quicker results and can now provide its cus-\ntomers with results much sooner after they have made a\npurchase.\n\u2022 Wargaming, the creators of World of Tanks and many\nother games. Wargaming uses in-database ML to model\nand predict how to interact with their more than 120 mil-\nlion customers.\na Data SCienCe eCoS yStem 85", "Big Data Infrastructure\nAlthough the traditional (modern) database is incredibly\nefficient at processing transactional data, in the age of big\ndata new infrastructure is required to manage all the other\nforms of data and for longer-term storage of the data. The\nmodern traditional database can cope with data volumes\nup to a few petabytes, but for this scale of data, traditional\ndatabase solutions may become prohibitively expensive.\nThis cost issue is commonly referred to as vertical scaling.\nIn the traditional data paradigm, the more data an or-\nganization has to store and process within a reasonable\namount of time, the larger the database server required\nand in turn the greater the cost for server configuration\nand database licensing. Organizations may be able to in-\ngest and query one billion records on a daily/weekly bases\nusing traditional databases, but for this scale of processing\nthey may need to invest more than $100,000 just purchas-\ning the required hardware.\nHadoop is an open-source platform developed and\nreleased by the Apache Software Foundation. It is a well-\nproven platform for ingesting and storing large volumes of\ndata in an efficient manner and can be much less expensive\nthan the traditional database approach. In Hadoop, the\ndata are divided up and partitioned in a variety of ways,\nand these partitions or portions of data are spread across\nthe nodes of the Hadoop cluster. The various analytic tools\nthat work with Hadoop process the data that reside on each\n86 Chapter 3", "of the nodes (in some instances these data can be memory\nresident), thus allowing for speedy processing of the data\nbecause the analytics is performed in parallel across the\nnodes. No data extraction or ETL process is needed. The\ndata are analyzed where they are stored.\nAlthough Hadoop is the best known big-data proc-\nessing framework, it is by no means the only one. Other\nbig-data processing frameworks include Storm, Spark, and\nFlink. All of these frameworks are part of the Apache soft-\nware foundation projects. The difference between these\nframeworks lies in the fact that Hadoop is primarily de-\nsigned for batch processing of data. Batch processing is ap-\npropriate where the dataset is static during the processing\nand where the results of the processing are not required\nimmediately (or at least are not particularly time sensi-\ntive). The Storm framework is designed for processing\nstreams of data. In stream processing each element in the\nstream is processed as it enters the system, and conse-\nquently the processing operations are defined to work on\neach individual element in the stream rather than on the\nentire data set. For example, where a batch process might\nreturn an average over a data set of values, a stream process\nwill return an individual label or value for each element in\nthe stream (such as calculating a sentiment score for each\ntweet in a Twitter stream). Storm is designed for real-time\nprocessing of data and according to the Storm website,1 it\nhas been benchmarked at processing over a million tuples\na Data SCienCe eCoS yStem 87", "per second per node. Spark and Flink are both hybrid (batch\nand stream) processing frameworks. Spark is a fundamen-\ntally a batch processing framework, similar to Hadoop, but\nalso has some stream processing capabilities whereas Flink\nis a stream processing framework that can also be used\nfor batch processing. Although these big-data processing\nframeworks provide data scientists with a choice of tools\nto meet the specific big-data requirements of their project\nusing these frameworks can have the drawback that the\nmodern data scientist now has to analyze data in two dif-\nferent locations, in the traditional modern databases and\nin the big-data storage. The next section looks at how this\nparticular issue is being addressed.\nThe Hybrid Database World\nIf an organization does not have data of the size and scale\nthat require a Hadoop solution, then it will require only\ntraditional database software to manage its data. How-\never, some of the literature argues that the data-storage\nand processing tools available in the Hadoop world will\nreplace the more traditional databases. It is very difficult\nto see this happening, and more recently there has been\nmuch discussion about having a more balanced approach\nto managing data in what is called the \u201chybrid database\nworld.\u201d The hybrid database world is where traditional da-\ntabases and the Hadoop world coexist.\nIn the hybrid database world, the organization\u2019s da-\ntabases and Hadoop-stored data are connected and work\n88 Chapter 3", "The hybrid database\nautomatically balances\nthe location of the data\nbased on the frequency\nof access and the type\nof data science being\nperformed.", "together, allowing the efficient processing, sharing, and\nanalysis of the data. Figure 8 shows a traditional data\nwarehouse, but instead of all the data being stored in the\ndatabase or the data warehouse, the majority of the data\nis moved to Hadoop. A connection is created between the\ndatabase and Hadoop, which allows the data scientist to\nquery the data as if they all are in one location. The data\nscientist does not need to query the portion of data that\nis in the database warehouse and then in a separate step\nquery the portion that is stored in Hadoop. He can query\nthe data as he always has done, and the solution will iden-\ntify what parts of the query need to be run in each loca-\ntion. The results of the query arrived at in each location\nwill be merged together and presented to him. Similarly,\nas the data warehouse grows, some the older data will not\nbe queried as frequently. The hybrid database solution\nautomatically moves the less frequently used data to the\nHadoop environment and the more frequently used data\nto the warehouse. The hybrid database automatically bal-\nances the location of the data based on the frequency of\naccess and the type of data science being performed.\nOne of the advantages of this hybrid solution is that\nthe data scientist still uses SQL to query the data. He does\nnot have to learn another data-query language or have to\nuse a variety of different tools. Based on current trends,\nthe main database vendors, data-integration solution\nvendors, and all cloud data-storage vendors will have solu-\ntions similar to this hybrid one in the near future.\n90 Chapter 3", "Users and applications seamlessly access data\nfrom databases/data warehouses and Hadoop\n10%\nVirtual\n(90%) 90%\nVirtual\n(90%)\nData off-loaded\nand virtualized automatically\nRDBMS\nAnalytics off-loaded\nand results merged\nFigure 8 Databases, data warehousing, and Hadoop working together\n(inspired by a figure in the Gluent data platform white paper, 2017, https://\ngluent.com/wp-content/uploads/2017/09/Gluent-Overview.pdf).\na Data SCienCe eCoS yStem 91", "Data Preparation and Integration\nData integration involves taking the data from different\ndata sources and merging them to give a unified view of\nthe data from across the organization. A good example of\nsuch integration occurs with medical records. Ideally, every\nperson would have one health record, and every hospital,\nmedical facility, and general practice would use the same\npatient identifier or same units of measures, the same\ngrading system, and so on. Unfortunately, nearly every\nhospital has its own independent patient-management\nsystem, as does each of the medical labs within the hos-\npital. Think of the challenges in finding a patient\u2019s record\nand assigning the correct results to the correct patient.\nAnd these are the challenges faced by just one hospital. In\nscenarios where multiple hospitals share patient data, the\nproblem of integration becomes significant. It is because\nof these kind of challenges that the first three CRISP-DM\nstages take up to 70 to 80 percent of the total data science\nproject time, with the majority of this time being allocated\nto data integration.\nIntegrating data from multiple data sources is difficult\neven when the data are structured. However, when some\nof the newer big-data sources are involved, where semi- or\nunstructured data are the norm, then the cost of integrat-\ning the data and managing the architecture can become\nsignificant. An illustrative example of the challenges of\n92 Chapter 3", "data integration is customer data. Customer data can re-\nside in many different applications (and the applications\u2019\ncorresponding databases). Each application will contain a\nslightly different piece of customer data. For example, the\ninternal data sources might contain the customer credit\nrating, customer sales, payments, call-center contact in-\nformation, and so on. Additional data about the customer\nmay also be available from external data sources. In this\ncontext, creating an integrated view of a customer re-\nquires the data from each of these sources to be extracted\nand integrated.\nThe typical data-integration process will involve a\nnumber of different stages, consisting of extracting, clean-\ning, standardizing, transforming, and finally integrating\nto create a single unified version of the data. Extracting\ndata from multiple data sources can be challenging be-\ncause many data sources can be accessed only by using an\ninterface particular to that data source. As a consequence,\ndata scientists need to have a broad skill set to be able to\ninteract with each of the data sources in order to obtain\nthe data.\nOnce data have been extracted from a data source, the\nquality of the data needs to be checked. Data cleaning is a\nprocess that detects, cleans, or removes corrupt or inaccu-\nrate data from the extracted data. For example, customer\naddress information may have to be cleaned in order to\nconvert it into a standardized format. In addition, there\na Data SCienCe eCoS yStem 93", "may be duplicate data in the data sources, in which case it\nis necessary to identify the correct customer record that\nshould be used and to remove all the other records from\nthe data sets. It is important to ensure that the values used\nin a data set are consistent. For example, one source appli-\ncation might use numeric values to represent a customer\ncredit rating, but another might have a mixture of nu-\nmeric and character values. In such a scenario, a decision\nregarding what value to use is needed, and then the other\nrepresentations should be mapped into the standardized\nrepresentation. For example, imagine one of the attri-\nbutes in the data set is a customer\u2019s shoe size. Custom-\ners can buy shoes from various regions around the world,\nbut the numbering system used for shoe sizes in Europe,\nthe United States, the United Kingdom, and other coun-\ntries are slightly different. Prior to doing data analysis and\nmodeling, these data values need to be standardized.\nData transformation involves the changing or com-\nbining of the data from one value to another. A wide vari-\nety of techniques can be used during this step and include\ndata smoothing, binning, and normalization as well as\nwriting custom code to perform a particular transforma-\ntion. A common example of data transformation is with\nprocessing a customer\u2019s age. In many data science tasks,\nprecisely distinguishing between customer ages is not\nparticularly helpful. The difference between a 42-year-\nold customer and a 43-year-old customer is generally not\n94 Chapter 3", "significant, although differentiating between a 42-year-\nold customer and a 52-year-old customer may be in-\nformative. As a consequence, a customer\u2019s age is often\ntransformed from a raw age into a general age range. This\nprocess of converting ages into age ranges is an example of\na data-transformation technique called binning. Although\nbinning is relatively straightforward from a technical per-\nspective, the challenge here is to identify the most appro-\npriate range thresholds to apply during binning. Applying\nthe wrong thresholds may obscure important distinctions\nin the data. Finding appropriate thresholds, however, may\nrequire domain specific knowledge or a process of trial-\nand-error experimentation.\nThe final step in data integration involves creating the\ndata that are used as input to the ML algorithms. This data\nis known as the analytics base table.\nCreating the Analytics Base Table\nThe most important step in creating the analytics base\ntable is the selection of the attributes that will be included\nin the analysis. The selection is based on domain knowl-\nedge and on an analysis of the relationships between attri-\nbutes. Consider, for example, a scenario where the analysis\nis focused on customers of a service. In this scenario, some\nof the frequently used domain concepts that will inform\na Data SCienCe eCoS yStem 95", "the design and selection of attributes include customer\ncontract details, demographics, usage, changes in usage,\nspecial usage, life-cycle phase, network links, and so on.\nFurthermore, attributes that are found to have a high cor-\nrelation with other attributes are likely to be redundant,\nand so one of the correlated attributes should be excluded.\nRemoving redundant features can result in simpler mod-\nels which are easier to understand, and also reduces the\nlikelihood of an ML algorithm returning a model that is\nfitted to spurious patterns in the data. The set of attri-\nbutes selected for inclusion define what is known as the\nanalytics record. An analytics record typically includes both\nraw and derived attributes. Each instance in the analytics\nbase table is represented by one analytics record, so the set\nof attributes included in the analytics record defines the\nrepresentation of the instances the analysis will be carried\nout on.\nAfter the analytics record has been designed, a set of\nrecords needs to extracted and aggregated to create a data\nset for analysis. When these records have been created and\nstored\u2014for example, in a database\u2014this data set is com-\nmonly referred to as the analytics base table. The analytics\nbase table is the data set that is used as input to the ML\nalgorithms. The next chapter introduces the field of ML\nand describes some of the most popular ML algorithms\nused in data science.\n96 Chapter 3", "4\nMACHINE LEARNING 101\nData science is best understood as a partnership between\na data scientist and a computer. In chapter 2, we described\nthe process the data scientist follows: the CRISP-DM life\ncycle. CRISP-DM defines a sequence of decisions the data\nscientist has to make and the activities he should engage in\nto inform and implement these decisions. In CRISP-DM,\nthe major tasks for a data scientist are to define the prob-\nlem, design the data set, prepare the data, decide on the\ntype of data analysis to apply, and evaluate and interpret\nthe results of the data analysis. What the computer brings\nto this partnership is the ability to process data and search\nfor patterns in the data. Machine learning is the field of\nstudy that develops the algorithms that the computers fol-\nlow in order to identify and extract patterns from data. ML\nalgorithms and techniques are applied primarily during", "the modeling stage of CRISP-DM. ML involves a two-step\nprocess.\nFirst, an ML algorithm is applied to a data set to iden-\ntify useful patterns in the data. These patterns can be\nrepresented in a number of different ways. We describe\nsome popular representations later in this chapter, but\nthey include decision trees, regression models, and neural\nnetworks. These representations of patterns are known\nas \u201cmodels,\u201d which is why this stage of the CRISP-DM life\ncycle is known at the \u201cmodeling stage.\u201d Simply put, ML al-\ngorithms create models from data, and each algorithm is\ndesigned to create models using a particular representa-\ntion (neural network or decision tree or other).\nSecond, once a model has been created, it is used for\nanalysis. In some cases, the structure of the model is what\nis important. A model structure can reveal what the im-\nportant attributes are in a domain. For example, in a medi-\ncal domain we might apply an ML algorithm to a data set\nof stroke patients and use the structure of the model to\nidentify the factors that have a strong association with\nstroke. In other cases, a model is used to label or classify\nnew examples. For instance, the primary purpose of a\nspam-filter model is to label new emails as either spam or\nnot spam rather than to reveal the defining attributes of\nspam email.\n98 Chapter 4", "Supervised versus Unsupervised Learning\nThe majority of ML algorithms can be classified as either\nsupervised learning or unsupervised learning. The goal of\nsupervised learning is to learn a function that maps from\nthe values of the attributes describing an instance to the\nvalue of another attribute, known as the target attribute,\nof that instance. For example, when supervised learning\nis used to train a spam filter, the algorithm attempts to\nlearn a function that maps from the attributes describing\nan email to a value (spam/not spam) for the target attri-\nbute; the function the algorithm learns is the spam-filter\nmodel returned by the algorithm. So in this context the\npattern that the algorithm is looking for in the data is the\nfunction that maps from the values of the input attributes\nto the values of the target attribute, and the model that\nthe algorithm returns is a computer program that imple-\nments this function. Supervised learning works by search-\ning through lots of different functions to find the function\nthat best maps between the inputs and output. However,\nfor any data set of reasonable complexity there are so many\ncombinations of inputs and possible mappings to outputs\nthat an algorithm cannot try all possible functions. As a\nconsequence, each ML algorithm is designed to look at or\nprefer certain types of functions during its search. These\npreferences are known as the algorithm\u2019s learning bias. The\nreal challenge in using ML is to find the algorithm whose\nMaChine Learning 101 99", "learning bias is the best match for a particular data set.\nGenerally, this task involves experiments with a number\nof different algorithms to find out which one works best\non that data set.\nSupervised learning is \u201csupervised\u201d because each of\nthe instances in the data set lists both the input values\nand the output (target) value for each instance. So the\nlearning algorithm can guide its search for the best func-\ntion by checking how each function it tries matches with\nthe data set, and at the same time the data set acts as a\nsupervisor for the learning process by providing feedback.\nObviously, for supervised learning to take place, each in-\nstance in the data set must be labeled with the value of the\ntarget attribute. Often, however, the reason a target attri-\nbute is interesting is that it is not easy to directly measure,\nand therefore it is not possible to easily create a data set\nof labeled instances. In such scenarios, a great deal of time\nand effort is required to create a data set with the tar-\nget values before a model can be trained using supervised\nlearning.\nIn unsupervised learning, there is no target attribute.\nAs a consequence, unsupervised-learning algorithms can\nbe used without investing the time and effort in labeling\nthe instances of the data set with a target attribute. How-\never, not having a target attribute also means that learn-\ning becomes more difficult: instead of the specific problem\nof searching for a mapping from inputs to output that\n100 Chapter 4", "The real challenge in\nusing ML is to find\nthe algorithm whose\nlearning bias is the best\nmatch for a particular\ndata set.", "matches the data, the algorithm has the more general task\nof looking for regularities in the data. The most common\ntype of unsupervised learning is cluster analysis, where the\nalgorithm looks for clusters of instances that are more sim-\nilar to each other than they are to other instances in the\ndata. These clustering algorithms often begin by guessing\na set of clusters and then iteratively updating the clusters\n(dropping instances from one cluster and adding them to\nanother) so as to increase both the within-cluster similar-\nity and the diversity across clusters.\nA challenge for clustering is figuring out how to mea-\nsure similarity. If all the attributes in a data set are numeric\nand have similar ranges, then it probably makes sense just\nto calculate the Euclidean distance (better known as the\nstraight-line distance) between the instances (or rows).\nRows that are close together in the Euclidean space are\nthen treated as similar. A number of factors, however, can\nmake the calculation of similarity between rows complex.\nIn some data sets, different numeric attributes have dif-\nferent ranges, with the result that a variation in row values\nin one attribute may not be as significant as a variation of\na similar magnitude in another attribute. In these cases,\nthe attributes should be normalized so that they all have\nthe same range. Another complicating factor in calculating\nsimilarity is that things can be deemed similar in many\ndifferent ways. Some attributes are sometimes more im-\nportant than other attributes, so it might make sense to\n102 Chapter 4", "weight some attributes in the distance calculations, or it\nmay be that the data set includes nonnumeric data. These\nmore complex scenarios may require the design of bespoke\nsimilarity metrics for the clustering algorithm to use.\nUnsupervised learning can be illustrated with a con-\ncrete example. Imagine we are interested in analyzing the\ncauses of Type 2 diabetes in white American adult males.\nWe would begin by constructing a data set, with each row\nrepresenting one person and each column representing\nan attribute that we believe are relevant for the study. For\nthis example, we will include the following attributes: an\nindividual\u2019s height in meters and weight in kilos, the num-\nber of minutes he exercises per week, his shoe size, and\nthe likelihood that he will develop diabetes expressed as\na percentage based on a number of clinical tests and life-\nstyle surveys. Table 2 illustrates a snippet from this data\nTable 2 Diabetes Study Data Set\nID Height Weight Shoe Size Exercise Diabetes\n(meters) (kilograms) (minutes (% likelihood)\nper week)\n1 1.70 70 5 130 0.05\n2 1.77 88 9 80 0.11\n3 1.85 112 11 0 0.18\n\u2026\nMaChine Learning 101 103", "set. Obviously, other attributes could be included\u2014for\nexample, a person\u2019s age\u2014and some attributes could be\nremoved\u2014for example, shoe size, which wouldn\u2019t be par-\nticularly relevant in determining whether someone will\ndevelop diabetes. As we discussed in chapter 2, the choice\nof which attributes to include and exclude from a data set\nis a key task in data science, but for the purposes of this\ndiscussion we will work with the data set as is.\nAn unsupervised clustering algorithm will look for\ngroups of rows that are more similar to each other than\nthey are to the other rows in the data. Each of these groups\nof similar rows defines a cluster of similar instances. For\ninstance, an algorithm can identify causes of a disease or\ndisease comorbidities (diseases that occur together) by\nlooking for attribute values that are relatively frequent\nwithin a cluster. The simple idea of looking for clusters of\nsimilar rows is very powerful and has applications across\nmany areas of life. Another application of clustering rows\nis making product recommendations to customers. If a\ncustomer liked a book, song, or movie, then he may enjoy\nanother book, song, or movie from the same cluster.\nLearning Prediction Models\nPrediction is the task of estimating the value of a target\nattribute for a given instance based on the values of other\n104 Chapter 4", "attributes (or input attributes) for that instance. It is the\nproblem that supervised ML algorithms solve: they gener-\nate prediction models. The spam-filter example we used\nto illustrate supervised learning is also applicable here: we\nuse supervised learning to train a spam-filter model, and\nthe spam-filter model is a prediction model. The typical\nuse case of a prediction model is to estimate the target at-\ntribute for new instances that are not in the training data\nset. Continuing our spam example, we train our spam fil-\nter (prediction model) on a data set of old emails and then\nuse the model to predict whether new emails are spam or\nnot spam. Prediction problems are possibly the most pop-\nular type of problem that ML is used for, so the rest of this\nchapter focuses on prediction as the case study for intro-\nducing ML. We begin our introduction to prediction mod-\nels with a concept fundamental to prediction: correlation\nanalysis. Then we explain how supervised ML algorithms\nwork to create different types of popular prediction mod-\nels, including linear-regression models, neural network\nmodels, and decision trees.\nCorrelations Are Not Causations, but Some Are Useful\nA correlation describes the strength of association between\ntwo attributes.1 In a general sense, a correlation can de-\nscribe any type of association between two attributes.\nThe term correlation also has a specific statistical mean-\ning, in which it is often used as shorthand for \u201cPearson\nMaChine Learning 101 105", "correlation.\u201d A Pearson correlation measures the strength\nof a linear relationship between two numeric attributes. It\nranges in value from \u22121 to +1. The letter r is used to denote\nthe Pearson value or coefficient between two attributes.\nA coefficient of r = 0 indicates that the two attributes are\nnot correlated. A coefficient of r = +1 indicates that the\ntwo attributes have a perfect positive correlation, mean-\ning that every change in one attribute is accompanied by\nan equivalent change in the other attribute in the same\ndirection. A coefficient of r = \u22121 indicates that the two at-\ntributes have a perfect negative correlation, meaning that\nevery change in one attribute is accompanied by the oppo-\nsite change in the other attribute. The general guidelines\nfor interpreting Pearson coefficients are that a value of r \u2248\n\u00b10.7 indicates a strong linear relationship between the at-\ntributes, r \u2248 \u00b10.5 indicates a moderate linear relationship,\nr \u2248 \u00b10.3 indicates a weak relationship, and r \u2248 0 indicates\nno relationship between the attributes.\nIn the case of the diabetes study, from our knowl-\nedge of how humans are physically made we would expect\nthat there will be relationships between some of the at-\ntributes listed in table 2. For example, it is generally the\ncase that the taller someone is, the larger her shoe size is.\nWe would also expect that the more someone exercises,\nthe lighter she will be, with the caveat that a tall person\nis likely to be heavier than a shorter person who exercises\nthe same amount. We would also expect that there will be\n106 Chapter 4", "no obvious relationship between someone\u2019s shoe size and\nthe amount she exercises. Figure 9 presents three scat-\nterplots that illustrate how these intuitions are reflected\nin the data. The scatterplot at the top shows how the data\nspread out if the plotting is based on shoe size and height.\nThere is a clear pattern in this scatterplot: the data go\nfrom the bottom-left corner to the top-right corner, indi-\ncating the relationship that as people get taller (or as we\nmove to the right on the x axis), they also tend to wear\nlarger shoes (we move up on the y axis). A pattern of data\ngenerally going from bottom left to top right in a scat-\nterplot is indicative of a positive correlation between the\ntwo attributes. If we compute the Pearson correlation be-\ntween shoe size and height, the correlation coefficient is\nr = 0.898, indicating a strong positive correlation between\nthis pair of attributes. The middle scatterplot shows how\nthe data spread out when we plot weight and exercise.\nHere the general pattern is in the opposite direction,\nfrom top left to bottom right, indicating a negative cor-\nrelation: the more people exercise, the lighter they are.\nThe Pearson correlation coefficient for this pair of attri-\nbutes is r = \u22120.710, indicating a strong negative correla-\ntion. The final scatterplot, at the bottom, plots exercise\nand shoe size. The data are relatively randomly distrib-\nuted in this plot, and the Pearson correlation coefficient\nfor this pair of attributes is r = \u22120.272, indicating no\nreal correlation.\nMaChine Learning 101 107", "Figure 9 Scatterplots of shoe size and height, weight and exercise, and shoe\nsize and exercise.", "The fact that the definition of a statistical Pearson cor-\nrelation is between two attributes might appear to limit\nthe application of statistical correlation to data analysis\nto just pairs of attributes. Fortunately, however, we can\ncircumvent this problem by using functions over sets of at-\ntributes. In chapter 2, we introduced BMI as a function of\na person\u2019s weight and height. Specifically, it is the ratio of\nhis weight (in kilograms) divided by his height (in meters)\nsquared. BMI was invented in the nineteenth century by\na Belgian mathematician, Adolphe Quetelet, and is used\nto categorize individuals as underweight, normal weight,\noverweight, or obese. The ratio of weight and height is\nused because BMI is designed to have a similar value for\npeople who are in the same category (underweight, nor-\nmal weight, overweight, or obese) irrespective of their\nheight. We know that weight and height are positively cor-\nrelated (generally, the taller someone is, the heavier he is),\nso by dividing weight by height, we control for the effect\nof height on weight. We divide by the square of the height\nbecause people get wider as they get taller, so squaring the\nheight is an attempt to account for a person\u2019s total vol-\nume in the calculation. Two aspects of BMI are interest-\ning for our discussion about correlation between multiple\nattributes. First, BMI is a function that takes a number\nof attributes as input and maps them to a new value. In\neffect, this mapping creates a new derived (as opposed to\nraw) attribute in the data. Second, because a person\u2019s BMI\nMaChine Learning 101 109", "is a single numeric value, we can calculate the correlation\nbetween it and other attributes.\nIn our case study of the causes of Type 2 diabetes in\nwhite American adult males, we are interested in iden-\ntifying if any of the attributes have a strong correlation\nwith the target attribute describing a person\u2019s likelihood\nof developing diabetes. Figure 10 presents three more\nscatterplots, each plotting the correlation between the\ntarget attribute (diabetes) and another attribute: height,\nweight, and BMI. In the scatterplot of height and diabetes,\nthere doesn\u2019t appear to be any particular pattern in the\ndata indicating that there is no real correlation between\nthese two attributes (the Pearson coefficient is r = \u22120.277).\nThe middle scatterplot shows the distribution of the data\nplotted using weight and diabetes. The spread of the data\nindicates a positive correlation between these two attri-\nbutes: the more someone weighs, the more likely she is to\ndevelop diabetes (the Pearson coefficient is r = 0.655). The\nbottom scatterplot shows the data set plotted using BMI\nand diabetes. The pattern in this scatterplot is similar to\nthe middle scatterplot: the data spread from bottom left\nto top right, indicating a positive correlation. In this scat-\nterplot, however, the instances are more tightly packed\ntogether, indicating that the correlation between BMI and\ndiabetes is stronger than the correlation between weight\nand diabetes. In fact, the Pearson coefficient for diabetes\nand BMI for this data set is r = 0.877.\n110 Chapter 4", "Figure 10 Scatterplots of the likelihood of diabetes with respect to height,\nweight, and BMI.", "The BMI example illustrates that it is possible to cre-\nate a new derived attribute by defining a function that\ntakes multiple attributes as input. It also shows that it\nis possible to calculate a Pearson correlation between\nthis derived attribute and another attribute in the data\nset. Furthermore, a derived attribute can actually have a\nhigher correlation with a target attribute than any of the\nattributes used to generate the derived attribute have\nwith the target. One way of understanding why BMI has a\nhigher correlation with the diabetes attribute compared to\nthe correlation for either height or weight is that the likeli-\nhood of someone developing diabetes is dependent on the\ninteraction between height and weight, and the BMI at-\ntribute models this interaction appropriately for diabetes.\nClinicians are interested in people\u2019s BMI because it gives\nthem more information about the likelihood of someone\ndeveloping Type 2 diabetes than either just the person\u2019s\nheight or just his weight does independently.\nWe have already noted that attribute selection is a key\ntask in data science. So is attribute design. Designing a\nderived attribute that has a strong correlation with an at-\ntribute we are interested in is often where the real value of\ndata science is found. Once you know the correct attributes\nto use to represent the data, you are able to build accurate\nmodels relatively quickly. Uncovering and designing the\nright attributes is the difficult part. In the case of BMI, a\nhuman designed this derived attribute in the nineteenth\n112 Chapter 4", "century. However, ML algorithms can learn interactions\nbetween attributes and create useful derived attributes\nby searching through different combinations of attributes\nand checking the correlation between these combinations\nand the target attribute. This is why ML is useful in con-\ntexts where many weak interacting attributes contribute\nto the process we are trying to understand.\nIdentifying an attribute (raw or derived) that has a\nhigh correlation with a target attribute is useful because\nthe correlated attribute may give us insight into the pro-\ncess that causes the phenomenon the target attribute rep-\nresents: the fact that BMI is strongly correlated with the\nlikelihood of a person\u2019s developing diabetes indicates that\nit is not weight by itself that contributes to a person\u2019s de-\nveloping diabetes but whether that person is overweight.\nAlso, if an input attribute is highly correlated with a target\nattribute, it is likely to be a useful input into the prediction\nmodel. Similar to correlation analysis, prediction involves\nanalyzing the relationships between attributes. In order to\nbe able to map from the values of a set of input attributes\nto a target attribute, there must be a correlation between\nthe input attributes (or some derived function over them)\nand the target attribute. If this correlation does not exist\n(or cannot be found by the algorithm), then the input at-\ntributes are irrelevant for the prediction problem, and the\nbest a model can do is to ignore those inputs and always\npredict the central tendency of that target2 in the data set.\nMaChine Learning 101 113", "Conversely, if a strong correlation does exist between in-\nput attributes and the target, then it is likely that an ML\nalgorithm will be able to generate a very accurate predic-\ntion model.\nLinear Regression\nWhen a data set is composed of numeric attributes, then\nprediction models based on regression are frequently\nused. Regression analysis estimates the expected (or aver-\nage) value of a numeric target attribute when all the input\nattributes are fixed. The first step in a regression analysis\nis to hypothesize the structure of the relationship between\nthe input attributes and the target. Then a parameterized\nmathematical model of the hypothesized relationship is\ndefined. This parameterized model is called a regression\nfunction. You can think of a regression function as a ma-\nchine that converts inputs to an output value and of the\nparameters as the settings that control the behavior of a\nmachine. A regression function may have multiple param-\neters, and the focus of regression analysis is to find the\ncorrect settings for these parameters.\nIt is possible to hypothesize and model many different\ntypes of relationships between attributes using regression\nanalysis. In principle, the only constraint on the structure\nof the relationship that can be modeled is the ability to\ndefine the appropriate regression function. In some do-\nmains, there may be strong theoretical reasons to assert a\n114 Chapter 4", "particular type of relationship, but in the absence of this\ntype of domain theory it is good practice to begin by as-\nsuming the simplest form of relationship\u2014namely, a lin-\near relationship\u2014and then, if need be, progress to model\nmore complex relationships. One reason for starting with\na linear relationship is that linear-regression functions are\nrelatively easy to interpret. The other reason is the com-\nmonsense notion that keeping things as simple as possible\nis generally a good idea.\nWhen a linear relationship is assumed, the regression\nanalysis is called linear regression. The simplest application\nof linear regression is modeling the relationship between\ntwo attributes: an input attribute X and a target attribute\nY. In this simple linear-regression problem, the regression\nfunction has the following form:\nY =\u03c9 +\u03c9 X\n0 1\nThis regression function is just the equation of a\nline (often written as y = mx + c) that is familiar to most\npeople from high school geometry.3 The variables \u03c9 and\n0\n\u03c9 are the parameters of the regression function. Modi-\n1\nfying these parameters changes how the function maps\nfrom the input X to the output Y. The parameter \u03c9 is the\n0\ny-intercept (or c in high school geometry) that specifies\nwhere the line crosses the vertical y axis when X is equal to\nzero. The parameter \u03c9 defines the slope of the line (i.e., it\n1\nis equivalent to m in the high school version).\nMaChine Learning 101 115", "In regression analysis, the parameters of a regression\nfunction are initially unknown. Setting the parameters of\na regression function is equivalent to searching for the line\nthat best fits the data. The strategy for setting these pa-\nrameters begins by guessing parameters values and then\niteratively updating the parameters so as to reduce the\noverall error of the function on the data set. The overall\nerror is calculated in three steps:\n1. The function is applied to the data set, and for each\ninstance in the data set it estimates the value of the target\nattribute.\n2. The error of the function for each instance is calculated\nby subtracting the estimated value of the target attribute\nfrom the actual value of the target attribute.\n3. The error of the function for each instance is squared,\nand then these squared values are summed.\nThe error of the function for each instance is squared\nin step 3 so that the error in the instances where the func-\ntion overestimates the target doesn\u2019t cancel out with the\nerror when it underestimates. Squaring the error makes\nthe error positive in both cases. This measure of error is\nknown as the sum of squared errors (SSE), and the strategy\nof fitting a linear function by searching for the parameters\nthat minimize the SSE is known as least squares. The SSE\nis defined as\n116 Chapter 4", "n\nSSE=\u2211(target \u2212prediction)2\ni i\ni=i\nwhere the data set contains n instances, target is the value\ni\nof the target attribute for instance i in the data set, and\nprediction is the estimate of the target by function for the\ni\nsame instance.\nTo create a linear-regression prediction model that\nestimates the likelihood of an individual\u2019s developing\ndiabetes with respect to his BMI, we replace X with the\nBMI attribute, Y with the diabetes attribute, and apply the\nleast-squares algorithm to find the best-fit line for the dia-\nbetes data set. Figure 11a illustrates this best-fit line and\nwhere it lies relative to the instances in the data set. In\nfigure 11b, the dashed lines show the error (or residual)\nfor each instance for this line. Using the least-squares ap-\nproach, the best-fit line is the line that minimizes the sum\nof the squared residuals. The equation for this line is\nDiabetes=\u22127.38431+0.55593\u2217BMI.\nThe slope parameter value \u03c9 = 0.55593 indicates that\n1\nfor each increase of one unit in BMI, the model increases\nthe estimated likelihood of a person developing diabetes\nby a little more than half a percent. In order to predict\nthe likelihood of a person\u2019s developing diabetes, we sim-\nply input his BMI into the model. For example, when BMI\nequals 20, the model returns a prediction of a 3.73 percent\nMaChine Learning 101 117", "a.\nsetebaiD\nBMI\nb.\nsetebaiD\nBMI\nFigure 11 (a) The best-fit regression line for the model \u201cDiabetes =\n\u22127.38431 + 0.55593 BMI.\u201d (b) The dashed vertical lines illustrate the residual\nfor each instance.\n118 Chapter 4", "likelihood for the diabetes attribute, and when BMI equals\n21, the model predicts a 4.29 percent likelihood.4\nUnder the hood, a linear-regression model fitted\nusing the least-squares method is actually calculating a\nweighted average over the instances. In fact, the intercept\nparameter value \u03c9 = \u22127.38431 ensures that the best-fit\n0\nline goes through the point defined by the average BMI\nvalue and average diabetes value for the data set. If the av-\nerage BMI value in the data set (BMI = 24.0932) is entered,\nthe model estimates a 4.29 percent likelihood for the dia-\nbetes attribute, which is the average value for diabetes in\nthe data set.\nThe weighting of the instances is based on the dis-\ntance of the instance from the line: the farther an instance\nis away from the line, the larger the residual for that in-\nstance, and the algorithm will weight that instance by\nthe residual squared. One consequence of this weighting\nis that instances that have extreme values (outliers) can\nhave a disproportionately large impact on the line-fitting\nprocess, resulting in the line being dragged away from the\nother instances. Thus, it is important to check for outliers\nin a data set prior to fitting a line to the data set (or, in\nother words, training a linear regression function on the\ndata set) using the least squares algorithm.\nLinear-regression models can be extended to take mul-\ntiple inputs. A new parameter is added to the model for\neach new input attribute, and the equation for the model\nMaChine Learning 101 119", "is updated to include the result of multiplying the new at-\ntribute by the new parameter within the summation. For\nexample, to extend the model to include the exercise and\nweight attributes as input, the structure of the regression\nfunction becomes\nDiabetes=\u03c9 +\u03c9BMI+\u03c9Exercise+\u03c9Weight.\n0 1 2 3\nIn statistics, a regression function that maps from\nmultiple inputs to a single output in this way is known\nas a multiple linear regression function. The structure of a\nmulti-input regression function is the basis for a range of\nML algorithms, including neural networks.\nCorrelation and regression are similar concepts inso-\nfar as both are techniques that focus on the relationship\nacross columns in the data set. Correlation is focused\non exploring whether a relationship exists between two\nattributes, and regression is focused on modeling an as-\nsumed relationship between attributes with the purpose\nof being able to estimate the value of one target attribute\ngiven the values of one or more input attributes. In the\nspecific cases of Pearson correlation and linear regression,\na Pearson correlation measures the degree to which two\nattributes have a linear relationship, and linear regression\ntrained using least squares is a process to find the best-fit\nline that predicts the value of one attribute given the value\nof another.\n120 Chapter 4", "Neural Networks and Deep Learning\nA neural network consists of a set of neurons that are con-\nnected together. A neuron takes a set of numeric values as\ninput and maps them to a single output value. At its core, a\nneuron is simply a multi-input linear-regression function.\nThe only significant difference between the two is that in\na neuron the output of the multi-input linear-regression\nfunction is passed through another function that is called\nthe activation function.\nThese activation functions apply a nonlinear map-\nping to the output of the multi-input linear-regression\nfunction. Two commonly used activation functions are\nthe logistic function and tanh function (see figure 12). Both\nfunctions take a single value x as input; in a neuron, this x\nvalue is the output from the multi-input linear-regression\nfunction the neuron has applied to its inputs. Also, both\nfunctions use Euler\u2019s number e, which is approximately\nequal to 2.71828182. These functions are sometimes\ncalled squashing functions because they take any value be-\ntween plus infinity and minus infinity and map it into a\nsmall, predefined range. The output range of the logistic\nfunction is 0 to 1, and the tanh function is \u22121 to 1. As a\nconsequence, the outputs of a neuron that uses a logistic\nfunction as its activation function are always between 0\nand 1. The fact that both the logistic and tanh functions\napply nonlinear mappings is clear in the S shape of the\ncurves. The reason for introducing a nonlinear mapping\nMaChine Learning 101 121", "0.1\n5.0\n1\nlogistic(x)=\n1+e\u2212x )x(noitavitcA\n0.0\n5.0\u2212\ne2x\u22121\ntanh(x)=\ne2x+1\nlogistic(x)\ntanh(x) 0.1\u2212\n\u221210 \u22125 0 5 10\nx\nFigure 12 Mapping the logistic and tanh functions as applied to the input x.\ninto a neuron is that one of the limitations of a multi-\ninput linear-regression function is that the function is by\ndefinition linear, and if all the neurons within a network\nimplement only linear mappings, then the overall network\nis also limited to learning a linear functions. However, in-\ntroducing a nonlinear activation function in the neurons\n122 Chapter 4", "of a network allows the network to learn more complex\n(nonlinear) functions.\nIt is worth emphasizing that each neuron in a neural\nnetwork is doing a very simple set of operations:\n1. Multiplying each input by a weight.\n2. Adding together the results of the multiplications.\n3. Pushing this result through an activation function.\nOperations 1 and 2 are just the calculation of a multi-\ninput regression function over the inputs, and operation 3\nis the application of the activation function.\nAll the connections between the neurons in a neural\nnetwork are directed and have a weight associated with\nthem. The weight on a connection coming into a neuron is\nthe weight that the neuron applies to the input it receives\non that connection when it is calculating the multi-input\nregression function over its inputs. Figure 13 illustrates\nthe topological structure of a simple neural network. The\nsquares on the left side of the figure, labeled A and B, rep-\nresent locations in memory that we use to present input\ndata to the network. No processing or transformation of\ndata is carried out at these locations. You can think of\nthese nodes as input or sensing neurons, whose output\nactivation is set to the value of the input.5 The circles in\nfigure 13 (labeled C, D, E, and F) represent the neurons\nMaChine Learning 101 123", "C\nWA,C\nWB,C\nA WC,F\nWA,D\nWD,F\nWB,D D F Output\nWE,F\nWA,E\nB\nWB,E\nE\nFigure 13 A simple neural network.\nin the network. It is often useful to think of the neurons\nin a network as organized into layers. This network has\nthree layers of neurons: the input layer contains A and B;\none hidden layer contains C, D, and E; and the output layer\ncontains F. The term hidden layer describes the fact that\nthe neurons in a layer are in neither the input layer nor the\noutput layer, so in this sense they are hidden from view.\nThe arrows connecting the neurons in the network\nrepresent the flow of information through the network.\nTechnically, this particular network is a feed-forward neu-\nral network because there are no loops in the network: all\n124 Chapter 4", "the connections point forward from the input toward the\noutput. Also, this network is considered fully connected\nbecause each neuron is connected to all the neurons in the\nnext layer in the network. It is possible to create many dif-\nferent types of neural networks by changing the number\nof layers, the number of neurons in each layer, the type\nof activation functions used, the direction of the connec-\ntions between layers, and other parameters. In fact, much\nof the work involved in developing a neural network for\na particular task involves experimenting to find the best\nnetwork layout for that task.\nThe labels on each arrow represent the weight that the\nnode at the end of the arrow applies to the information\npassed along that connection. For example, the arrow con-\nnecting C with F indicates that the output from C is passed\nas an input to F, and F will apply the weight W to the\nC,F\ninput from C.\nIf we assume that the neurons in the network in figure\n13 use a tanh activation function, then we can define the\ncalculation carried out in neuron F of the network as\nOutput=tanh(\u03c9 C+\u03c9 D+\u03c9 E)\nC,F D,F E,F\nThe mathematical definition of the processing carried\nout in neuron F shows that the final output of the network\nis calculated using a composition of a set of functions. The\nphrase \u201ccomposing functions\u201d just means that the output\nMaChine Learning 101 125", "of one function is used as input to another function. In\nthis case, the outputs of neurons C, D, and E are used as in-\nputs to neuron F, so the function implemented by F com-\nposes the functions implemented by C, D, and E.\nFigure 14 makes this description of neural networks\nmore concrete, illustrating a neural network that takes a\nperson\u2019s body-fat percentage and VO max (a measure of\n2\nthe maximum amount of oxygen that a person can use in a\nminute) as input and calculates a fitness level for the that\nperson.6 Each neuron in the middle layer of the network\ncalculates a function based on the body-fat percentage and\nVO max: f (), f (), and f (). Each of these functions mod-\n2 1 2 3\nels the interaction between the inputs in a different way.\nThese functions essentially represent new attributes that\nare derived from the raw inputs to the network. They are\nsimilar to the BMI attribute described earlier, which was\ncalculated as a function of weight and height. Sometimes\nit is possible to interpret what the output of a neuron in\nthe network represents insofar as it is possible to provide\na domain-theoretic description of what the derived attri-\nbute represents and to understand why this derived attri-\nbute is useful to the network. Often, however, the derived\nattribute calculated by a neuron will not have a symbolic\nmeaning for humans. These attributes are instead captur-\ning interactions between the other attributes that the net-\nwork has found to be useful. The final node in the network,\nf , calculates another function\u2014over the outputs of f (),\n4 1\n126 Chapter 4", "f1()\nFat%\nf2() f4() Fitness\nVO2\nf3()\nFigure 14 A neural network that predicts a person\u2019s fitness level.\nf (), and f ()\u2014the output of which is the fitness prediction\n2 3\nreturned by the network. Again, this function may not be\nmeaningful to humans beyond the fact that it defines an\ninteraction the network has found to have a high correla-\ntion with the target attribute.\nTraining a neural network involves finding the correct\nweights for the connections in the network. To understand\nhow to train a network, it is useful to begin by thinking\nabout how to train the weights for a single neuron in the\noutput layer of the network. Assume that we have a train-\ning data set that has both inputs and target output for\nMaChine Learning 101 127", "each instance. Also, assume that the connections coming\ninto the neuron already have weights assigned to them.\nIf we take an instance from the data set and present the\nvalues of the input attributes for this instance to the net-\nwork, the neuron will output a prediction for the target.\nBy subtracting this prediction from the value for the target\nin the data set, we can measure the neuron\u2019s error on that\ninstance. Using some basic calculus, it is possible to derive\na rule to update the weights on the connections coming\ninto a neuron given a measure of the neuron\u2019s output error\nso as to reduce the neuron\u2019s error. The precise definition\nof this rule will vary depending on the activation function\nused by the neuron because the activation function affects\nthe derivative used in the derivation of the rule. But we\ncan give the following intuitive explanation of how the\nweight-update rule works:\n1. If the error is 0, then we should not change the weights\non the inputs.\n2. If the error is positive, we will decrease the error if we\nincrease the neuron\u2019s output, so we must increase the\nweights for all the connections where the input is positive\nand decrease the weights for the connections where the\ninput is negative.\n3. If the error is negative, we will decrease the error if we\ndecrease the neuron\u2019s output, so we must decrease the\n128 Chapter 4", "weights for all the connections where the input is posi-\ntive and increase the weights for the connections where\nthe input is negative.\nThe difficulty in training a neural network is that the\nweight-update rule requires an estimate of the error at a\nneuron, and although it is straightforward to calculate the\nerror for each neuron in the output layer of the network,\nit is difficult to calculate the error for the neurons in the\nearlier layers. The standard way to train a neural network\nis to use an algorithm called the backpropagation algorithm\nto calculate the error for each neuron in the network and\nthen use the weight-update rule to modify the weights in\nthe network.7 The backpropagation algorithm is a super-\nvised ML algorithm, so it assumes a training data set that\nhas both inputs and the target output for each instance.\nThe training starts by assigning random weights to each\nof the connections in the network. The algorithm then it-\neratively updates the weights in the network by showing\ntraining instances from the data set to the network and\nupdating the network weights until the network is work-\ning as expected. The algorithm\u2019s name comes from the fact\nthat after each training instance is presented to the net-\nwork, the algorithm passes (or backpropagates) the error\nof the network back through the network starting at the\noutput layer and at each layer in the network calculates the\nerror for the neurons in that layer before sharing this error\nMaChine Learning 101 129", "back to the neurons in the preceding layer. The main steps\nin the algorithm are as follows:\n1. Calculate the error for the neurons in the output layer\nand use the weight-update rule to update the weights\ncoming into these neurons.\n2. Share the error calculated at a neuron with each of the\nneurons in the preceding layer that is connected to that\nneuron in proportion to the weight of the connection\nbetween the two neurons.\n3. For each neuron in the preceding layer, calculate the\noverall error of the network that the neuron is responsi-\nble for by summing the errors that have been backpropa-\ngated to it and use the result of this error summation to\nupdate the weights on the connections coming into this\nneuron.\n4. Work back through the rest of the layers in the network\nby repeating steps 2 and 3 until the weights between the\ninput neurons and the first layer of hidden neurons have\nbeen updated.\nIn backpropagation, the weight updates for each neu-\nrons are scaled to reduce but not to eliminate the neuron\u2019s\nerror in the training instance. The reason for this is that\nthe goal of training the network is to enable it to generalize\nto new instances that are not in the training data rather\n130 Chapter 4", "than to memorize the training data. So each set of weight\nupdates nudges the network toward a set of weights that\nare generally better over the whole data set, and over many\niterations the network converges on a set of weights that\ncaptures the general distribution of the data rather than\nthe specifics of the training instances. In some versions of\nbackpropagation, the weights are updated after a number\nof instances (or batch of instances) have been presented\nto the network rather than after each training instance.\nThe only adjustment required in these versions is that the\nalgorithm uses the average error of the network on a batch\nas the measure of error at the output layer for the weight-\nupdate process.\nOne of the most exciting technical developments in\nthe past 10 years has been the emergence of deep learn-\ning. Deep-learning networks are simply neural networks\nthat have multiple8 layers of hidden units; in other words,\nthey are deep in terms of the number of hidden layers\nthey have. The neural network in figure 15 has five lay-\ners: one input layer on the left containing three neurons,\nthree hidden layers (the black circles), and one output\nlayer on the right containing two neurons. This network\nillustrates that it is possible to have a different number of\nneurons in each layer: the input layer has three neurons;\nthe first hidden layer has five; each of the next two hid-\nden layers has four; and the output layer has two. This\nnetwork also shows that it is possible to have multiple\nMaChine Learning 101 131", "Figure 15 A deep neural network.\nneurons in the output layer. Using multiple output neu-\nrons is useful if the target is a nominal or ordinal data type\nthat has distinct levels. In these scenarios, the network is\nset up so that there is one output neuron for each level,\nand the network is trained so that for each input only one\nof the output neurons outputs a high activation (denoting\nthe predicted target level).\nAs in the previous networks we have looked at, the\none shown in figure 15 is a fully connected, feed-forward\nnetwork. However, not all networks are fully connected,\nfeed-forward networks. In fact, myriad network topolo-\ngies have been developed. For example, recurrent neural\n132 Chapter 4", "networks (RNNs) introduce loops in the network topol-\nogy: the output of a neuron for one input is fed back into\nthe neuron during the processing of the next input. This\nloop gives the network a memory that enables it to pro-\ncess each input in the context of the previous inputs it has\nprocessed. As a consequence, RNNs are suitable for pro-\ncessing sequential data such as language.9 Another well-\nknown deep neural network architecture is a convolutional\nneural network (CNN). CNNs were originally designed for\nuse with image data (Le Cun 1989). A desirable character-\nistic of an image-recognition network is that it should be\nable to recognize if a visual feature has occurred in an im-\nage irrespective of where in the image it has occurred. For\nexample, if a network is doing face recognition, it needs\nto be able to recognize the shape of an eye whether the\neye is in the top-right corner of the image or in the center\nof the image. CNNs achieve this by having groups of neu-\nrons that share the same set of weights on their inputs.\nIn this context, think of a set of input weights as defining\na function that returns true if a particular visual feature\noccurs in the set of pixels that are passed into the func-\ntion. This means that each group of neurons that share\ntheir weights learns to identify a particular visual feature,\nand each neuron in the group acts as a detector for that\nfeature. In a CNN, the neurons within each group are ar-\nranged so that each neuron examines a different location\nin the image, and the group covers the entire image. As a\nMaChine Learning 101 133", "consequence, if the visual feature the group detects occurs\nanywhere in the image, one of the neurons in the group will\nidentify it.\nThe power of deep neural networks comes from the\nfact that they can automatically learn useful attributes,\nsuch as the feature detectors in CNNs. In fact, deep learn-\ning is sometimes known as representation learning because\nthese deep networks are essentially learning a new rep-\nresentation of the input data that is better at predicting\nthe target output attribute than the original raw input is.\nEach neuron in a network defines a function that maps\nthe values coming into the neuron into a new output at-\ntribute. So a neuron in the first layer of a network might\nlearn a function that maps the raw input values (such as\nweight and height) into an attribute that is more useful\nthan individual input values (such as BMI). However, the\noutput of this neuron, along with the outputs of its sister\nneurons in the first layer, is then fed into the neurons in\nthe second layer, and these second-layer neurons try to\nlearn functions that map the outputs of the first layer into\nnew and yet more useful representations. This process of\nmapping inputs to new attributes and feeding these new\nattributes as inputs to new functions continues through-\nout the network, and as a network gets deeper, it can learn\nmore and more complex mappings from raw inputs to new\nattribute representations. It is the ability to automatically\nlearn complex mappings of input data to useful attribute\n134 Chapter 4", "representations that has made deep-learning models so\naccurate in tasks with high-dimensional inputs (such as\nimage and text processing).\nIt has been known for a long time that making neural\nnetworks deeper allows the network to learn more com-\nplex mappings of data. The reason that deep learning has\nnot really taken off until the past few years, however, is\nthat the standard combination of using a random-weight\ninitialization followed by the backpropagation algorithm\ndoesn\u2019t work well with deep networks. One problem with\nthe backpropagation algorithm is that the error gets\nshared out as the process goes back through the layers, so\nthat in a deep network by the time the algorithm reaches\nthe early layers of the network, the error estimates are not\nthat useful anymore.10 As a result, the layers in the early\nparts of the network don\u2019t learn useful transformations\nfor the data. In the past few years, however, researchers\nhave developed new types of neurons and adaptations to\nthe backpropagation algorithm that deal with this prob-\nlem. It has also been found that being careful with how\nthe network weights are initialized is also helpful. Two\nother factors that formerly made training deep networks\ndifficult were that it takes a great deal of computing power\nto train a neural network, and neural networks work best\nwhen there is a great deal of training data. However, as\nwe have already discussed, in recent years significant in-\ncreases in the availability of computing power and large\nMaChine Learning 101 135", "data sets have made the training of deep networks more\nfeasible.\nDecision Trees\nLinear regression and neural networks work best with\nnumeric inputs. If the input attributes in a data set are\nprimarily nominal or ordinal, however, then other ML al-\ngorithms and models, such as decision trees, may be more\nappropriate.\nA decision tree encodes a set of if then, else rules in\na tree structure. Figure 16 illustrates a decision tree for\ndeciding whether an email is spam or not. Rectangles with\nrounded corners represent tests on attributes, and the\nsquare nodes indicate decision, or classification, nodes.\nThis tree encodes the following rules: if the email is from\nUnknown\nsender?\nFalse True\nSuspicious\nSpam\nwords?\nFalse True\nNot\nSpam\nspam\nFigure 16 A decision tree for determining whether an email is spam or not.\n136 Chapter 4", "an unknown sender, then it is spam; if it isn\u2019t from an un-\nknown sender but contains suspicious words, then it is spam;\nif it is neither from an unknown sender nor contains suspi-\ncious words, then it is not spam. In a decision tree, the deci-\nsion for an instance is made by starting at the top of the\ntree and navigating down through the tree by applying a\nsequence of attribute tests to the instance. Each node in\nthe tree specifies one attribute to test, and the process\ndescends the tree node by node by choosing the branch\nfrom the current node with the label matching the value\nof the test attribute of the instance. The final decision is\nthe label of the terminating (or leaf) node that the instance\ndescends to.\nEach path in a decision tree, from root to leaf, defines\na classification rule composed of a sequence of tests. The\ngoal of a decision-tree-learning algorithm is to find a set\nof classification rules that divide the training data set into\nsets of instances that have the same value for the target\nattribute. The idea is that if a classification rule can sepa-\nrate out from a data set a subset of instances that have the\nsame target value, and if this classification rule is true for a\nnew example (i.e., the example goes down that path in the\ntree), then it is likely that the correct prediction for this\nnew example is the target value shared by all the training\ninstances that fit this rule.\nThe progenitor of most modern ML algorithms for\ndecision-tree learning is the ID3 algorithm (Quinlan 1986).\nMaChine Learning 101 137", "ID3 builds a decision tree in a recursive, depth-first man-\nner, adding one node at a time, starting with the root\nnode. It begins by selecting an attribute to test at the root\nnode. A branch is grown from the root for each value in\nthe domain of this test attribute and is labeled with that\nvalue. For example, a node with a binary test attribute will\nhave two branches descending from it. The data set is then\ndivided up: each instance in the data set is pushed down\nthe branch and given a label that matches the value of the\ntest attribute for the instance. ID3 then grows each branch\nusing the same process used to create the root node: select\na test attribute, add a node with branches, split the data\nby funneling the instances down the relevant branches.\nThis process continues until all the instances on a branch\nhave the same value for the target attribute, in which case\na terminating node is added to the tree and labeled with\nthe target attribute value shared by all the instances on\nthe branch.11\nID3 chooses the attribute to test at each node in the\ntree so as to minimize the number of tests required to\ncreate pure sets (i.e., sets of instances that have the same\nvalue for the target attribute). One way to measure the pu-\nrity of a set is to use Claude Shannon\u2019s entropy metric. The\nminimum possible entropy for a set is zero, and a pure set\nhas an entropy of zero. The numeric value of the maxi-\nmum possible entropy for a set depends on the size of the\nset and the number of different types of elements that can\n138 Chapter 4", "be in the set. A set will have maximum entropy when all\nthe elements in it are of different types.12 ID3 selects the\nattribute to test at a node to be the attribute that results\nin the lowest-weighted entropy after splitting the data set\nat the node using this attribute. The weighted entropy\nfor an attribute is calculated by (1) splitting the data set\nusing the attribute; (2) calculating the entropy of the re-\nsulting sets; (3) weighting each of these entropies by the\nfraction of data that is in the set; and (4) then summing\nthe results.\nTable 3 lists a data set of emails in which each email\nis described by a number of attributes and whether it is a\nspam email or not. The \u201cattachment\u201d attribute is true for\nemails that have an attachment and false otherwise (in this\nsample of emails, none of the emails has an attachment).\nTable 3 A Data Set of Emails: Spam or Not Spam?\nAttachment Suspicious Unknown Spam\nWords Sender\nFalse False True True\nFalse False True True\nFalse True False True\nFalse False False False\nFalse False False False\nMaChine Learning 101 139", "The \u201csuspicious words\u201d attribute is true if the email con-\ntains one or more words on a predefined list of suspicious\nwords. The \u201cunknown sender\u201d attribute is true if the\nsender of the email is not in the recipient\u2019s address book.\nThis is the data set that was used to train the decision tree\nshown in figure 16. In this data set, the attributes \u201cattach-\nment,\u201d \u201csuspicious words,\u201d and \u201cunknown sender\u201d are the\ninput attributes and the \u201cspam\u201d attribute is the target. The\n\u201cunknown sender\u201d attribute splits the data set into purer\nsets more than any of the other attributes does (one set\ncontaining instances where \u201cSpam = True\u201d and another set\nin which \u201cSpam = False\u201d for the majority of instances). As\na consequence, \u201cunknown sender\u201d is put at the root node\n(see figure 17). After this initial split, all of the instances\nUnknown\nsender?\nFalse True\nSuspicious\nAttachment words Spam Attachment Suspicious Spam\nwords\nFalse True True\nFalse False True\nFalse False False\nFalse False True\nFalse False False\nFigure 17 Creating the root node in the tree.\n140 Chapter 4", "on the right branch have the same target value. However,\nthe instances on the left branch have different values\nfor the target. Splitting the instances on the left branch\nusing the \u201csuspicious words\u201d attribute results in two pure\nsets: one where \u201cSpam = False\u201d and another where \u201cSpam\n= True.\u201d So \u201csuspicious words\u201d is selected as the test attri-\nbute for a new node on the left branch (see figure 18). At\nthis point, the data subsets at the end of each branch are\npure, so the algorithm finishes and returns the decision\ntree shown in figure 16.\nUnknown\nSender?\nFalse True\nSuspicious\nAttachment Spam\nWords\nSuspicious\nWords? False False True\nFalse False True\nFalse True\nAttachmentSpam\nAttachmentSpam\nFalse False\nFalse True\nFalse False\nFigure 18 Adding the second node to the tree.\nMaChine Learning 101 141", "One of the strengths of decision trees is that they are\nsimple to understand. Also it is possible to create very\naccurate models based on decision trees. For example, a\nrandom-forest model is composed of a set of decision trees,\nwhere each tree is trained on a random subsample of the\ntraining data, and the prediction returned by the model\nfor an individual query is the majority prediction across all\nthe trees in the forest. Although decision trees work well\nwith both nominal and ordinal data, they struggle with\nnumeric data. In a decision tree, a separate branch de-\nscends from each node for each value in the domain of the\nattribute tested at the node. Numeric attributes, however,\nhave an infinite number of values in their domains, with\nthe implication that a tree would need an infinite number\nof branches. One solution to this problem is to transform\nnumeric attributes into ordinal attributes, although doing\nso requires the definition of appropriate thresholds, which\ncan also be difficult.\nFinally, because a decision-tree-learning algorithm\nrepeatedly divides a data set as a tree becomes large, it\nbecomes more sensitive to noise (such as mislabeled in-\nstances). The subset of examples on each branch becomes\nsmaller, and so the data sample each classification rule is\nbased on becomes smaller. The smaller the data sample\nused to define a classification rule, the more sensitive to\nnoise the rule becomes. As a consequence, it is a good idea\nto keep decision trees shallow. One approach is to stop the\ngrowth of a branch when the number of instances on the\n142 Chapter 4", "branch is still less than a predefined threshold (e.g., 20\ninstances). Other approaches allow the tree to grow and\nthen prune the tree back. These approaches typically use\nstatistical tests or the performance of the model on a set of\ninstances specifically chosen for this task to identify splits\nnear the bottom of the tree that should be removed.\nBias in Data Science\nThe goal of ML is to create models that encode appropriate\ngeneralizations from data sets. Two major factors contrib-\nute to the generalization (or model) that an ML algorithm\nwill generate from a data set. The first is the data set the\nalgorithm is run on. If the data set is not representative of\nthe population, then the model the algorithm generates\nwon\u2019t be accurate. For example, earlier we developed a re-\ngression model that predicted the likelihood that an indi-\nvidual will develop Type 2 diabetes based on his BMI. This\nmodel was generated from a data set of American white\nmales. As a consequence, this model is unlikely to be accu-\nrate if used to predict the likelihood of diabetes for females\nor for males of different race or ethnic backgrounds. The\nterm sample bias describes how the process used to select\na data set can introduce biases into later analysis, be it a\nstatistical analysis or the generation of predictive models\nusing ML.\nMaChine Learning 101 143", "The second factor that affects the model generated\nfrom a data set is the choice of ML algorithm. There are\nmany different ML algorithms, and each one encodes a\ndifferent way to generalize from a data set. The type of\ngeneralization an algorithm encodes is known as the learn-\ning bias (or sometimes the modeling or selection bias) of the\nalgorithm. For example, a linear-regression algorithm en-\ncodes a linear generalization from the data and as a result\nignores nonlinear relationships that may fit the data more\nclosely. Bias is normally understood as a bad thing. For\nexample, the sampling bias is a bias that a data scientist\nwill try to avoid. However, without a learning bias there\ncan be no learning, and the algorithm will only be able to\nmemorize the data.\nHowever, because ML algorithms are biased to look\nfor different types of patterns, and because there is no one\nlearning bias across all situations, there is no one best ML\nalgorithm. In fact, a theorem known as the \u201cno free lunch\ntheorem\u201d (Wolpert and Macready 1997) states that there\nis no one best ML algorithm that on average outperforms\nall other algorithms across all possible data sets. So the\nmodeling phase of the CRISP-DM process normally in-\nvolves building multiple models using different algorithms\nand comparing the models to identify which algorithm\ngenerates the best model. In effect, these experiments are\ntesting which learning bias on average produces the best\nmodels for the given data set and task.\n144 Chapter 4", "Evaluating Models: Generalization Not Memorization\nOnce a data scientist has selected a set of ML algorithms\nto experiment with on a data set, the next major task is to\ncreate a test plan for how the models generated by these\nalgorithms will be evaluated. The goal of the test plan is to\nensure that the evaluation provides realistic estimates of\nmodel performance on unseen data. A prediction model\nthat simply memorizes a data set is unlikely to do a good\njob at estimating values for new examples. One problem\nwith just memorizing data is that most data sets will con-\ntain noise. So a prediction model that merely memorizes\ndata is also memorizing the noise in the data. Another\nproblem with just memorizing the data is that it reduces\nthe prediction process to a table lookup and leaves un-\nsolved the problem of how to generalize from the training\ndata to new examples that aren\u2019t in the table.\nOne part of the test plan relates to how the data set\nis used to train and test the models. The data set has to\nbe used for two different purposes. The first is to find\nwhich algorithm generates the best models. The second\nis to estimate the generalization performance of the best\nmodel\u2014that is, how well the model is likely to do on un-\nseen data. The golden rule for evaluating models is that\nmodels should never be tested on the same data they were\ntrained on. Using the same data for training and test-\ning models is equivalent to giving a class of students the\nMaChine Learning 101 145", "The golden rule for\nevaluating models is\nthat models should\nnever be tested on the\nsame data they were\ntrained on.", "questions on an exam the night before the test is held. The\nstudents will of course do very well in the test, and their\nscores will not reflect their real proficiency with the gen-\neral course material. So, too, with ML models: if a model\nis evaluated on the same data that it is trained on, the re-\nsults of the evaluation will be optimistic compared to the\nmodel\u2019s real performance. The standard process for ensur-\ning that the models aren\u2019t able to peek at the test data dur-\ning training is to split the data into three parts: a training\nset, a validation set, and a test set. The proportions of the\nsplit will vary between projects, but splits of 50:20:30 and\n40:20:40 are common. The size of the data set is a key fac-\ntor in determining the splits: generally, the larger the data\nset, the larger the test set. The training set is used to train\nan initial set of models. The validation set is then used\nto compare the performance of these models on unseen\ndata. Comparing the performance of these initial models\non the validation set enables us to determine which algo-\nrithm generated the best model. Once the best algorithm\nhas been selected, the training and validation set can be\nmerged back together into a larger training set, and this\ndata set is fed into the best algorithm to create the final\nmodel. It is crucial that the test set is not used during the\nprocess to select the best algorithm, nor should it be used\nto train this final model. If these caveats are followed, then\nthe test set can be used to estimate the generalization per-\nformance of this final model on unseen data.\nMaChine Learning 101 147", "The other major component of the test plan is to\nchoose the appropriate evaluation metrics to use during\nthe testing. In general, models are evaluated based on\nhow often the outputs of the model match the outputs\nlisted in the test set. If the target attribute is a numeric\nvalue, then the sum of squared errors is one way to mea-\nsure the accuracy of a model on the test set. If the target\nattribute is nominal or ordinal, then the simplest way to\nestimate the model accuracy is to calculate the proportion\nof examples of the test set the model got correct. How-\never, in some contexts it is important to include an error\nanalysis within the evaluation. For example, if a model\nis used in a medical diagnosis setting, it is much more\nserious if the model diagnoses an ill patient as healthy\nthan if it diagnoses a healthy patient as ill. Diagnosing\nan ill patient as healthy may result in the patient being\nsent home without receiving appropriate medical atten-\ntion, but if a model diagnoses a healthy patient as ill,\nthis error is likely to be discovered through later test-\ning the patient will receive. So the evaluation metric\nused to evaluate these types of models should weight\none type of error more than the other when estimating\nmodel performance. Once the test plan has been cre-\nated, the data scientist can begin training and evaluating\nmodels.\n148 Chapter 4", "Summary\nThis chapter started by noting that data science is a part-\nnership between a data scientist and a computer. Machine\nlearning provides a set of algorithms that generate models\nfrom a large data set. However, whether these models are\nuseful will depend on the data scientist\u2019s expertise. For a\ndata science project to succeed, the data set should be rep-\nresentative of the domain and should include relevant at-\ntributes. The data scientist should evaluate a range of ML\nalgorithms to find which one generates the best models.\nThe model-evaluation process should follow the golden\nrule that a model should never be evaluated on the data\nit was trained on.\nCurrently in most data science projects, the primary\ncriterion for selecting which model to use is model accu-\nracy. However, in the near future, data usage and privacy\nregulations may affect the selection of ML algorithms.\nFor example, the General Data Protection Regulations\nwill come into force in the European Union on May 25,\n2018. We discuss these regulations in relation to data us-\nage in chapter 6, but for now we just want to point out that\nsome articles in the regulations may appear to mandate a\n\u201cright to explanation\u201d in relation to automated decision\nprocesses.13 A potential implication of such a right is that\nusing models, such a neural networks, that are difficult to\ninterpret for decisions relating to individuals may become\nMaChine Learning 101 149", "problematic. In such circumstances, the transparency and\nease of explanation of some models, such as decision trees,\nmay make the use of these models more appropriate.\nFinally, the world changes, and models don\u2019t. Implicit\nin the ML process of data set construction, model training,\nand model evaluation is the assumption that the future\nwill be the same as the past. This assumption is known\nas the stationarity assumption: the processes or behaviors\nthat are being modeled are stationary through time (i.e.,\nthey don\u2019t change). Data sets are intrinsically historic in\nthe sense that data are representations of observations\nthat were made in the past. So, in effect, ML algorithms\nsearch through the past for patterns that might general-\nize to the future. Obviously, this assumption doesn\u2019t al-\nways hold. Data scientists use the term concept drift to\ndescribe how a process or behavior can change, or drift, as\ntime passes. This is why models go out of date and need to\nbe retrained and why the CRISP-DM process includes the\nouter circle shown in figure 4 to emphasize that data sci-\nence is iterative. Processes need to put in place postmodel\ndeployment to ensure that a model has not gone stale, and\nwhen it has, it should be retrained. The majority of these\ndecisions cannot be automated and require human insight\nand knowledge. A computer will answer the question it is\nposed, but unless care is taken, it is very easy to pose the\nwrong question.\n150 Chapter 4", "5\nSTANDARD DATA SCIENCE TASKS\nOne of the most important skills for a data scientist is the\nability to frame a real-world problem as a standard data\nscience task. Most data science projects can be classified as\nbelonging to one of four general classes of task:\n\u2022 Clustering (or segmentation)\n\u2022 Anomaly (or outlier) detection\n\u2022 Association-rule mining\n\u2022 Prediction (including the subproblems of classification\nand regression)\nUnderstanding which task a project is targeting can\nhelp with many project decisions. For example, training\na prediction model requires that each of the instances in\nthe data set include the value of the target attribute. So", "knowing that the project is doing prediction gives guidance\n(through requirements) in terms of data set design. Un-\nderstanding the task also informs which ML algorithm(s)\nto use. Although there are a large number of ML algo-\nrithms, each algorithm is designed for a particular data-\nmining task. For example, ML algorithms that generate\ndecision-tree models are designed primarily for prediction\ntasks. There is a many-to-one relationship between ML al-\ngorithms and a task, so knowing the task doesn\u2019t tell you\nexactly which algorithm to use, but it does define a set of\nalgorithms that are designed for the task. Because the data\nscience task affects both the data set design and the selec-\ntion of ML algorithms, the decision regarding which task\nthe project will target has to be made early on in the proj-\nect life cycle, ideally during the business-understanding\nphase of the CRISP-DM life cycle. To provide a better un-\nderstanding of each of these tasks, this chapter describes\nhow some standard business problems map to tasks.\nWho Are Our Customers? (Clustering)\nOne of the most frequent application areas of data science\nin business is to support marketing and sales campaigns.\nDesigning a targeted marketing campaign requires an un-\nderstanding of the target customer. Most businesses have\na diverse range of customers with a variety of needs, so\n152 Chapter 5", "using a one-size-fits-all approach is likely to fail with a\nlarge segment of a customer base. A better approach is to\ntry to identify a number of customer personas or customer\nprofiles, each of which relates to a significant segment of\nthe customer base, and then to design targeted marketing\ncampaigns for each persona. These personas can be created\nusing domain expertise, but it is generally a good idea to\nbase the personas on the data that the business has about\nits customers. Human intuition about customers can of-\nten miss important nonobvious segments or not provide\nthe level of granularity that is required for nuanced mar-\nketing. For example, Meta S. Brown (2014) reports how\nin one data science project the well-known stereotype\nsoccer mom (a suburban homemaker who spends a great\ndeal of time driving her children to soccer or other sports\npractice) didn\u2019t resonate with a customer base. However,\nusing a data-driven clustering process identified more fo-\ncused personas, such as mothers working full-time outside\nthe home with young children in daycare and mothers who\nwork part-time with high-school-age children and women\ninterested in food and health and who do not have children.\nThese customer personas define clearer targets for mar-\nketing campaigns and may highlight previously unknown\nsegments in the customer base.\nThe standard data science approach to this type of\nanalysis is to frame the problem as a clustering task. Clus-\ntering involves sorting the instances in a data set into\nStandard data SCienCe t aSkS 153", "Human intuition\nabout customers can\noften miss important\nnonobvious segments\nor not provide the level\nof granularity that is\nrequired for nuanced\nmarketing.", "subgroups containing similar instances. Usually clustering\nrequires an analyst to first decide on the number of sub-\ngroups she would like identified in the data. This decision\nmay be based on domain knowledge or informed by proj-\nect goals. A clustering algorithm is then run on the data\nwith the desired number of subgroups input as one of the\nalgorithms parameters. The algorithm then creates that\nnumber of subgroups by grouping instances based on the\nsimilarity of their attribute values. Once the algorithm has\ncreated the clusters, a human domain expert reviews the\nclusters to interpret whether they are meaningful. In the\ncontext of designing a marketing campaign, this review\ninvolves checking whether the groups reflect sensible cus-\ntomer personas or identifies new personas not previously\nconsidered.\nThe range of attributes that can be used to describe\ncustomers for clustering is vast, but some typical examples\ninclude demographic information (age, gender, etc.), loca-\ntion (ZIP code, rural or urban address, etc.), transactional\ninformation (e.g., what products or services they have pur-\nchased), the revenue the company generates from them,\nhow long have they been customers, if they are a mem-\nber of a loyalty-card scheme, whether they ever returned\na product or made a complaint about a service, and so on.\nAs is true of all data science projects, one of the biggest\nchallenges with clustering is to decide which attributes to\ninclude and which to exclude so as to get the best results.\nStandard data SCienCe t aSkS 155", "Making this decision on attribute selection will involve it-\nerations of experiments and human analysis of the results\nof each iteration.\nThe best-known ML algorithm for clustering is the\nk-means algorithm. The k in the name signals that the\nalgorithm looks for k clusters in the data. The value of k\nis predefined and is often set through a process of trial-\nand-error experimentation with different values of k. The\nk-means algorithm assumes that all the attributes describ-\ning the customers in the data set are numeric. If the data\nset contains nonnumeric attributes, then these attributes\nneed to be mapped to numeric values in order to use k-\nmeans, or the algorithm will need to be amended to handle\nthese nonnumeric values. The algorithm treats each cus-\ntomer as a point in a point cloud (or scatterplot), where\nthe customer\u2019s position is determined by the attribute val-\nues in her profile. The goal of the algorithm is to find the\nposition of each cluster\u2019s center in the point cloud. There\nare k clusters, so there are k cluster centers (or means)\u2014\nhence the name of the algorithm.\nThe k-means algorithm begins by selecting k instances\nto act as initial cluster centers. Current best practice is to\nuse an algorithm called \u201ck-means++\u201d to select the initial\ncluster centers. The rationale behind k-means++ is that it\nis a good idea to spread out the initial cluster centers as\nmuch as possible. So in k-means++ the first cluster cen-\nter is set by randomly selecting one of the instances in\n156 Chapter 5", "As is true of all data\nscience projects, one of\nthe biggest challenges\nwith clustering is to\ndecide which attributes\nto include and which to\nexclude so as to get the\nbest results.", "the data set. The second and subsequent cluster centers\nare set by selecting an instance from the data set with\nthe probability that an instance selected is proportional\nto the squared distance from the closest existing cluster\ncenter. Once all k cluster centers have been initialized, the\nalgorithm works by iterating through a two-step process:\nfirst, assigning each instance to the nearest cluster center,\nand then, second, updating the cluster center to be in the\nmiddle of the instances assigned to it. In the first itera-\ntion the instances are assigned to the nearest cluster cen-\nter returned by the k-means++ algorithm, and then these\ncluster centers are moved so that they are positioned at\nthe center of instances assigned to them. Moving the clus-\nter centers is likely to move them closer to some instances\nand farther away from other instances (including farther\naway from some instances assigned to the cluster center).\nThe instances are then reassigned, again to the closest up-\ndated cluster center. Some instances will remain assigned\nto the same cluster center, and others may be reassigned\nto a new cluster center. This process of instance assign-\nment and center updating continues until no instances are\nassigned to a new cluster center during an iteration. The\nk-means algorithm is nondeterministic, meaning that dif-\nferent starting positions for the cluster centers will likely\nproduce different clusters. As a result, the algorithm is\ntypically run several times, and the results of these differ-\nent runs are then compared to see which clusters appear\n158 Chapter 5", "most sensible given the data scientist\u2019s domain knowledge\nand understanding.\nWhen a set of clusters for customer personas has been\ndeemed to be useful, the clusters are often given names\nto reflect the main characteristics of the cluster persona.\nEach cluster center defines a different customer persona,\nwith the persona description generated from the attri-\nbute values of the associated cluster center. The k-means\nalgorithm is not required to return equal-size clusters,\nand, in fact, it is likely to return different-size clusters.\nThe sizes of the clusters can be useful, though, because\nthey can help to guide marketing. For example, the clus-\ntering process may reveal small, focused clusters of cus-\ntomers that current marketing campaigns are missing. Or\nan alternative strategy might be to focus on clusters that\ncontain customers that generate a great deal of revenue.\nWhatever marketing strategy is adopted, understanding\nthe segments within a customer base is the prerequisite\nto marketing success.\nOne of the advantages of clustering as an analytics\napproach is that it can be applied to most types of data.\nBecause of its versatility, clustering is often used as a data-\nexploration tool during the data-understanding stage of\nmany data science projects. Also, clustering is also useful\nacross a wide range of domains. For example, it has been\nused to analyze students in a given course in order to iden-\ntify groups of students who need extra support or prefer\nStandard data SCienCe t aSkS 159", "different learning approaches. It has also been used to\nidentify groups of similar documents in a corpus, and in\nscience it has been used in bio-informatics to analyze gene\nsequences in microarray analysis.\nIs This Fraud? (Anomaly Detection)\nAnomaly detection or outlier analysis involves searching\nfor and identifying instances that do not conform to the\ntypical data in a data set. These nonconforming cases are\noften referred to as anomalies or outliers. Anomaly detec-\ntion is often used in analyzing financial transactions in\norder to identify potential fraudulent activities and to trig-\nger investigations. For example, anomaly detection might\nuncover fraudulent credit card transactions by identifying\ntransactions that have occurred in an unusual location or\nthat involve an unusually large amount compared to other\ntransactions on a particular credit card.\nThe first approach that most companies typically use\nfor anomaly detection is to manually define a number of\nrules based on domain expertise that help with identify-\ning anomalous events. This rule set is often defined in\nSQL or in another language and is run against the data\nin the business databases or data warehouse. Some pro-\ngramming languages have begun to include specific com-\nmands to facilitate the coding of these types of rules. For\n160 Chapter 5", "example, database implementations of SQL now includes\na MATCH_RECOGNIZE function to facilitate pattern\nmatching in data. A common pattern in credit card fraud\nis that when a credit card gets stolen, the thief first checks\nthat the card is working by purchasing a small item on the\ncard, and then if that transaction goes through, the thief\nas quickly as possible follows that purchase with the pur-\nchase of an expensive item before the card is canceled. The\nMATCH_RECOGNIZE function in SQL enables database\nprogrammers to write scripts that identify sequences of\ntransactions on a credit card that fit this pattern and ei-\nther block the card automatically or trigger a warning to\nthe credit-card company. Over time, as more anomalous\ntransactions are identified\u2014for example, by customers re-\nporting fraudulent transactions\u2014the set of rules identify-\ning anomalous transactions is expanded to handle these\nnew instances.\nThe main drawback with a rule-based approach to\nanomaly detection is that defining rules in this way means\nthat anomalous events can be identified only after they\nhave occurred and have come to the company\u2019s atten-\ntion. Ideally, most organizations would like to be able to\nidentify anomalies when they first happen or if they have\nhappened but have not been reported. In some ways,\nanomaly detection is the opposite of clustering: the goal\nof clustering is to identify groups of similar instances,\nwhereas the goal of anomaly detection is to find instances\nStandard data SCienCe t aSkS 161", "that are dissimilar to the rest of the data in the data set.\nBy this intuition, clustering can also be used to automati-\ncally identify anomalies. There are two approaches to us-\ning clustering for anomaly detection. The first is that the\nnormal data will be clustered together, and the anomalous\nrecords will be in separate clusters. The clusters containing\nthe anomalous records will be small and so will be clearly\ndistinct from the large clusters for the main body of the\nrecords. The second approach is to measure the distance\nbetween each instance and the center of the cluster. The\nfarther away the instance is from the center of the clus-\nter, the more likely it is to be anomalous and thus to need\ninvestigation.\nAnother approach to anomaly detection is to train a\nprediction model, such as a decision tree, to classify in-\nstances as anomalous or not. However, training such a\nmodel normally requires a training data set that contains\nboth anomalous records and normal records. Also, it is not\nenough to have just a few instances of anomalous records;\nin order to train a normal prediction model, the data set\nneeds to contain a reasonable number of instances from\neach class. Ideally, the data set should be balanced; in a\nbinary-outcome case, balance would imply a 50:50 split in\nthe data. In general, acquiring this type of training data for\nanomaly detection is not feasible: by definition, anomalies\nare rare events, occurring maybe in 1 to 2 percent or less of\nthe data. This data constraint precludes the use of normal,\n162 Chapter 5", "off-the-shelf prediction models. There are, however, ML\nalgorithms known as one-class classifiers that are designed\nto deal with the type of imbalanced data that are typical of\nanomaly-detection data sets.\nThe one-class support-vector machine (SVM) algorithm\nis a well-known one-class classifier. In general terms, the\none-class SVM algorithm examines the data as one unit\n(i.e., a single class) and identifies the core characteristics\nand expected behavior of the instances. The algorithm\nwill then indicate how similar or dissimilar each instance\nis from the core characteristics and expected behavior.\nThis information can then be used to identify instances\nthat warrant further investigation (i.e., the anomalous re-\ncords). The more dissimilar an instance is, the more likely\nthat it should be investigated.\nThe fact that anomalies are rare means that they can\nbe easy to miss and difficult to identify. As a result, data\nscientists often combine a number of different models to\ndetect anomalies. The idea is that different models will\ncapture different types of anomalies. In general, these\nmodels are used to supplement the known rules within the\nbusiness that already define various types of anomalous\nactivity. The different models are integrated together into\na decision-management solution that enables the predic-\ntions from each of the models to feed into a decision of\nthe final predicted outcome. For example, if a transaction\nis identified as fraudulent by only one out of four models,\nStandard data SCienCe t aSkS 163", "the decision system may decide that it isn\u2019t a true case\nof fraud, and the transaction can be ignored. Conversely,\nhowever, if three or four out of the four models have iden-\ntified the transaction as possible fraud, then the transac-\ntion would be flagged for a data scientist to investigate.\nAnomaly detection can be applied to many problem\ndomains beyond credit card fraud. More generally, it\nis used by clearinghouses to identify financial transac-\ntions that require further investigation as potentially\nfraudulent or as cases of money laundering. It is used in\ninsurance-claims analysis to identify claims that are not\nin keeping with a company\u2019s typical claims. In cybersecu-\nrity, it is used to identify network intrusions by detecting\npossible hacking or untypical behavior by employees. In\nthe medical domain, identifying anomalies in medical re-\ncords can be useful for diagnosing disease and in studying\ntreatments and their effects on the body. Finally, with the\nproliferation of sensors and the increasing usage of Inter-\nnet of Things technology, anomaly detection will play an\nimportant role in monitoring data and alerting us when\nabnormal sensor events occur and action is required.\nDo You Want Fries with That? (Association-Rule Mining)\nA standard strategy in sales is cross-selling, or suggest-\ning to customers who are buying products that they may\n164 Chapter 5", "also want to purchase other related or complementary\nproducts. The idea is to increase the customers\u2019 overall\nspending by getting them to purchase more products and\nat the same time to improve customer service by remind-\ning customers of products they probably wanted to buy\nbut may have forgotten to do so. The classic example of\nthe cross-sell is when a waiter in a hamburger restaurant\nasks a customer who has just ordered a hamburger, \u201cDo\nyou want fries with that?\u201d Supermarkets and retailer busi-\nnesses know that shoppers purchase products in groups,\nand they use this information to set up cross-selling op-\nportunities. For example, supermarket customers who\nbuy hot dogs are also likely to purchase ketchup and beer.\nUsing this type of information, a store can plan the lay-\nout of the products. Locating hot dogs, ketchup, and beer\nnear each other in the store helps customers to collect this\ngroup of items quickly and may also boost the store sales\nbecause customers who are purchasing hot dogs might\nsee and purchase the ketchup and beer that they forgot\nthey needed. Understanding these types of associations\nbetween products is the basis of all cross-selling.\nAssociation-rule mining is an unsupervised-data-anal-\nysis technique that looks to find groups of items that fre-\nquently co-occur together. The classic case of association\nmining is market-basket analysis, wherein retail companies\ntry to identify sets of items that are purchased together,\nsuch as hot dogs, ketchup, and beer. To do this type of\nStandard data SCienCe t aSkS 165", "data analysis, a business keeps track of the set (or basket)\nof items that each customer bought during each visit to\nthe store. Each row in the data set describes one basket of\ngoods purchased by a particular customer on a particular\nvisit to the store. So the attributes in the data set are the\nproducts the store sells. Given these data, association-rule\nmining looks for items that co-occur within each basket\nof goods. Unlike clustering and anomaly detection, which\nfocus on identifying similarities or differences between\ninstances (or rows) in a data set, association-rule mining\nfocuses on looking at relationships between attributes\n(or columns) in a data set. In a general sense, it looks\nfor correlations\u2014measured as co-occurrences\u2014between\nproducts. Using association-rule mining, a business can\nstart to answer questions about its customers\u2019 behaviors\nby looking for patterns that may exist in the data. Ques-\ntions that market-basket analysis can be used to answer\ninclude: Did a marketing campaign work? Have this custom-\ner\u2019s buying patterns changed? Has the customer had a major\nlife event? Does the product location affect buying behavior?\nWho should we target with our new product?\nThe Apriori algorithm is the main algorithm used to\nproduce the association rules. It has a two-step process:\n1. Find all combinations of items in a set of transactions\nthat occur with a specified minimum frequency. These\ncombinations are called frequent itemsets.\n166 Chapter 5", "2. Generate rules that express the probable co-occurrence\nof items within frequent itemsets. The Apriori algorithm\ncalculates the probability of an item being present in a\nfrequent itemset given that another item or items are\npresent.\nThe Apriori algorithm generates association rules that\nexpress probabilistic relationships between items in fre-\nquent itemsets. An association rule is of the form \u201cIF ante-\ncedent, THEN consequent.\u201d It states that an item or group\nof items, the antecedent, implies the presence of another\nitem in the same basket of goods, the consequent, with\nsome probability. For example, a rule derived from a fre-\nquent itemset containing A, B, and C might state that if A\nand B are included in a transaction, then C is likely to also\nbe included:\nIF {hot-dogs, ketchup}, THEN {beer}.\nThis rule indicates that customers who are buying\nhot dogs and ketchup are also likely to buy beer. A frequent\nexample of the power of association-rule mining is the\nbeer-diapers example that describes how an unknown US\nsupermarket in the 1980s used an early computer sys-\ntem to analyze its checkout data and identified an un-\nusual association between diapers and beer in customer\npurchases. The theory developed to understand this rule\nStandard data SCienCe t aSkS 167", "was that families with young children were preparing for\nthe weekend and knew that they would need diapers and\nwould have to socialize at home. The store placed the two\nitems near each other, and sales soared. The beer-and-dia-\npers story has been debunked as apocryphal, but it is still\na useful example of the potential benefits of association-\nrule mining for retail businesses.\nTwo main statistical measures are linked with associa-\ntion rules: support and confidence. The support percentage\nof an association rule\u2014or the ratio of transactions that\ninclude both the antecedent and consequent to the total\nnumber of transactions\u2014indicates how frequently the\nitems in the rule occur together. The confidence percent-\nage of an association rule\u2014or the ratio of the number of\ntransactions that include both the antecedent and con-\nsequent to the number of transactions that includes the\nantecedent\u2014is the conditional probability that the con-\nsequent will occur given the occurrence of the antecedent.\nSo, for example, a confidence of 75 percent for the asso-\nciation rule relating hot dogs and ketchup with beer would\nindicate that in 75 percent of cases where customers pur-\nchased both hot dogs and ketchup, they also purchased beer.\nThe support score of a rule simply records the percentage\nof baskets in the data set where the rule holds. For exam-\nple, a support of 5 percent indicates that 5 percent of all\nthe baskets in the data set contain all three items in the\nrule \u201chot dogs, ketchup, and beer.\u201d\n168 Chapter 5", "Even a small data set can result in the generation of\na large number of association rules. In order to control\nthe complexity of the analysis of these rules, it is usual\nto prune the generated rule set to include only rules that\nhave both a high support and a high confidence. Rules that\ndon\u2019t have high support or confidence are not interesting\neither because the rule covers only a very small percent-\nage of baskets (low support) or because the relationship\nbetween the items in the antecedent and the consequent is\nlow (low confidence). Rules that are trivial or inexplicable\nshould also be pruned. Trivial rules represent associations\nthat are obvious and well known to anyone who under-\nstands the business domain. An inexplicable rule repre-\nsents associations that are so strange that it is difficult to\nunderstand how to convert the rule into a useful action\nfor the company. It is likely that an inexplicable rule is the\nresult of an odd data sample (i.e., the rule represents a spu-\nrious correlation). Once the rule set has been pruned, the\ndata scientist can then analyze the remaining rules to un-\nderstand what products are associated with each other and\napply this new information in the organization. Organiza-\ntions will typically use this new information to determine\nstore layout or to perform some targeted marketing cam-\npaigns to their customers. These campaigns can involve\nupdates to their websites to include recommended prod-\nucts, in-store advertisements, direct mailings, the cross-\nselling of other products by check-out staff, and so on.\nStandard data SCienCe t aSkS 169", "Association mining becomes more powerful if the bas-\nkets of items are connected to demographic data about the\ncustomer. This is why so many retailers run loyalty-card\nschemes because such schemes allow them not only to\nconnect different baskets of goods to the same customer\nthrough time but also to connect baskets of goods to the\ncustomer\u2019s demographics. Including this demographic in-\nformation in the association analysis enables the analysis\nto be focused on particular demographics, which can fur-\nther help marketing and targeted advertising. For exam-\nple, demographic-based association rules can be used with\nnew customers, for whom the company has no buying-\nhabit information but does have demographic informa-\ntion. An example of an association rule augmented with\ndemographic information might be\nIF gender(male) and age(< 35) and {hot-dogs, ketchup},\nTHEN {beer}.\n[Support = 2%, Confidence = 90%.]\nThe standard application area for association-rule\nmining focuses on what products are in the shopping bas-\nket and what products are not in the shopping basket. This\nassumes that the products are purchased in one visit to\nthe store or website. This kind of scenario will probably\nwork in most retail and other related scenarios. However,\n170 Chapter 5", "association-rule mining is also useful in a range of domains\noutside of retail. For example, in the telecommunications\nindustry, applying association-rule mining to customer\nusage helps telecommunications companies to design how\nto bundle different services together into packages. In the\ninsurance industry, association-rule mining is used to see\nif there are associations between products and claims. In\nthe medical domain, it is used to check if there are inter-\nactions between existing and new treatments and medi-\ncines. And in banking and financial services, it is used to\nsee what products customers typically have and whether\nthese products can be applied to new or existing custom-\ners. Association-rule mining can also be used to analyze\npurchasing behavior over a period of time. For example,\ncustomers tend to buy product X and Y today, and in three\nmonths\u2019 time they buy product Z. This time period can\nbe considered a shopping basket, although it is one that\nspans three months. Applying association-rule mining to\nthis kind of temporally defined basket expands the appli-\ncations areas of association-rule mining to include main-\ntenance schedules, the replacement of parts, service calls,\nfinancial products, and so on.\nChurn or No Churn, That Is the Question (Classification)\nA standard business task in customer-relationship man-\nagement is to estimate the likelihood that an individual\ncustomer will take an action. The term propensity modeling\nStandard data SCienCe t aSkS 171", "is used to describe this task because the goal is to model an\nindividual\u2019s propensity to do something. This action could\nbe anything from responding to marketing to default-\ning on a loan or leaving a service. The ability to identify\ncustomers who are likely to leave a service is particularly\nimportant to cell phone service companies. It costs a cell\nphone service company a substantial amount of money to\nattract new customers. In fact, it is estimated that it gener-\nally costs five to six times more to attract a new customer\nthan it does to retain an established one (Verbeke et al.\n2011). As a result, many cell phone service companies are\nvery keen to retain their current customers. However, they\nalso want to minimize costs. So although it would be easy\nto retain customers by simply giving all customers reduced\nrates and great phone upgrades, this is not a realistic op-\ntion. Instead, they want to target the offers they give their\ncustomers to just those customers who are likely to leave\nin the near future. If they can identify a customer who\nis about to leave a service and persuade that customer to\nstay, perhaps by offering her an upgrade or a new billing\npackage, then they can save the difference between the\nprice of the enticement they gave the customer and the\ncost of attracting a new customer.\nThe term customer churn is used to describe the pro-\ncess of customers leaving one service and joining another.\nSo the problem of predicting which customers are likely\nto leave in the near future is known as churn prediction. As\n172 Chapter 5", "the name suggests, this is a prediction task. The predic-\ntion task is to classify a customer as being a churn risk\nor not. Many companies are using this kind of analysis to\npredict churn customers in the telecommunications, utili-\nties, banking, insurance, and other industries. A growing\narea that companies are focusing on is the prediction of\nstaff turnover or staff churn: which staff are likely to leave\nthe company within a certain time period.\nWhen a prediction model returns a label or category\nfor an input, it is known as a classification model. Training\na classification model requires historic data, where each\ninstance is labeled to indicate whether the target event\nhas happened for that instance. For example, customer-\nchurn classification requires a data set in which each cus-\ntomer (one row per customer) is assigned a label indicating\nwhether he or she has churned. The data set will include\nan attribute, known as the target attribute, that lists this\nlabel for each customer. In some instances, assigning a\nchurn label to a customer record is a relatively straightfor-\nward task. For example, the customer may have contacted\nthe organization and explicitly canceled his subscription\nor contract. However, in other cases the churn event may\nnot be explicitly signaled. For example, not all cell phone\ncustomers have a monthly contract. Some customers have\na pay-as-you-go (or prepay) contract in which they top up\ntheir account at irregular intervals when they need more\nphone credit. Defining whether a customer with this type\nStandard data SCienCe t aSkS 173", "of contract has churned can be difficult: Has a customer\nwho hasn\u2019t made a call in two weeks churned, or is it neces-\nsary for a customer to have a zero balance and no activity\nfor three weeks before she is considered to have churned?\nOnce the churn event has been defined from a business\nperspective, it is then necessary to implement this defini-\ntion in code in order to assign a target label to each cus-\ntomer in the data set.\nAnother complicating factor in constructing the train-\ning data set for a churn-prediction model is that time lags\nneed to be taken into account. The goal of churn prediction\nis to model the propensity (or likelihood) that a customer\nwill churn at some point in the future. As a consequence,\nthis type of model has a temporal dimension that needs to\nbe considered during the creation of the data set. The set of\nattributes in a propensity-model data set are drawn from\ntwo separate time periods: the observation period and the\noutcome period. The observation period is when the values\nof the input attributes are calculated. The outcome period\nis when the target attribute is calculated. The business\ngoal of creating a customer-churn model is to enable the\nbusiness to carry out some sort of intervention before the\ncustomer churns\u2014in other words, to entice the customer\nto stay with the service. This means that the prediction\nabout the customer churning must be made sometime in\nadvance of the customer\u2019s actually leaving the service. The\nlength of this period is the length of the outcome period,\n174 Chapter 5", "and the prediction that the churn model returns is actually\nthat a customer will churn within this outcome period. For\nexample, the model might be trained to predict that the\ncustomer will churn within one month or two months, de-\npending on the speed of the business process to carry out\nthe intervention.\nDefining the outcome period affects what data should\nbe used as input to the model. If the model is designed\nto predict that a customer will churn within two months\nfrom the day the model is run on that customer\u2019s record,\nthen when the model is being trained, the input attri-\nbutes that describe the historic customers who have al-\nready churned should be calculated using only the data\nthat were available about those customers two months\nprior to their leaving the service. The input attributes\ndescribing currently active customers should similarly be\ncalculated with the data available about these customers\u2019\nactivity two months earlier. Creating the data set in this\nway ensures that all the instances in the data set, including\nboth churned and active customers, describe the custom-\ners at the time in their individual customer journeys that\nthe model is being designed to make a prediction about\nthem: in this example, two months before they churn\nor stay.\nNearly all customer-propensity models will use attri-\nbutes describing the customer\u2019s demographic information\nas input: age, gender, occupation, and so on. In scenarios\nStandard data SCienCe t aSkS 175", "relating to an ongoing service, they are also likely to in-\nclude attributes describing the customer\u2019s position in the\ncustomer life cycle: coming on board, standing still midcycle,\napproaching end of a contract. There are also likely to be\nattributes that are specific to the industry. For example,\ntypical attributes used in telecommunication industry\ncustomer-churn models include the customer\u2019s average\nbill, changes in billing amount, average usage, staying\nwithin or generally exceeding plan minutes, the ratio of\ncalls within the network to those outside the network, and\npotentially the type of phone used.1 However, the specific\nattributes used in each model will vary from one project to\nthe next. Gordon Linoff and Michael Berry (2011) report\nthat in one churn-prediction project in South Korea, the\nresearchers found it useful to include an attribute that de-\nscribed the churn rate associated with a customer\u2019s phone\n(i.e., What percentage of customers with this particular\nphone churned during the observation period?). However,\nwhen they went to build a similar customer-churn model\nin Canada, the handset/churn-rate attribute was useless.\nThe difference was that in South Korea the cell phone ser-\nvice company offered large discounts on new phones to\nnew customers, whereas in Canada the same discounts\nwere offered to both existing and new customers. The\noverall effect was that in South Korea phones going out\nof date drove customer churn; people were incentivized to\nleave one operator for another in order to avail themselves\n176 Chapter 5", "of discounts, but in Canada this incentive to leave did not\nexist.\nOnce a labeled data set has been created, the major\nstage in creating a classification model is to use an ML algo-\nrithm to build the classification model. During modeling,\nit is good practice to experiment with a number of differ-\nent ML algorithms to find out which algorithm works best\non the data set. Once the final model has been selected,\nthe likely accuracy of the predictions of this model on new\ninstances is estimated by testing it on a subset of the data\nset that was not used during the model-training phase. If\na model is deemed accurate enough and suitable for the\nbusiness need, the model is then deployed and applied to\nnew data either in a batch process or in real time. A really\nimportant part of deploying the model is ensuring that\nthe appropriate business processes and resources are put\nin place so that the model is used effectively. There is no\npoint in creating a customer-churn model unless there is\na process whereby the model\u2019s predictions result in trig-\ngering customer interventions so that the business retains\ncustomers.\nIn addition to predicting the classification label, pre-\ndiction models can also give a measure of how confident\nthe model is in the prediction. This measure is called the\nprediction probability and will have a value between 0 and\n1. The higher the value, the more likely the prediction\nis correct. The prediction-probability value can be used\nStandard data SCienCe t aSkS 177", "to prioritize which customers to focus on. For example,\nin customer-churn prediction the organization wants to\nconcentrate on the customers who are most likely to leave.\nBy using the prediction probability and sorting the churn-\ners based on this value, a business can focus on the key\ncustomers (those most likely to leave) first before mov-\ning on to customers with a lower prediction-probability\nscore.\nHow Much Will It Cost? (Regression)\nPrice prediction is the task of estimating the price that a\nproduct will cost at a particular point in time. The product\ncould be a car, a house, a barrel of oil, a stock, or a medical\nprocedure. Having a good estimate of what something will\ncost is obviously valuable to anyone who is considering\nbuying the item. The accuracy of a price-prediction model\nis domain dependent. For example, due to the variability\nin the stock market, predicting the price of a stock tomor-\nrow is very difficult. By comparison, it may be easier to\npredict the price of a house at an auction because the vari-\nation in house prices fluctuates much more slowly than\nstocks.\nThe fact that price prediction involves estimating the\nvalue of a continuous attribute means that it is treated as\na regression problem. A regression problem is structurally\n178 Chapter 5", "very similar to a classification problem; in both cases, the\ndata science solution involves building a model that can\npredict the missing value of an attribute given a set of in-\nput attributes. The only difference is that classification in-\nvolves estimating the value of a categorical attribute and\nregression involves estimating the value of a continuous\nattribute. Regression analysis requires a data set where\nthe value of the target attribute for each of the historic in-\nstances is listed. The multi-input linear-regression model\nintroduced in chapter 4 illustrated the basic structure of a\nregression model, with most other regression models be-\ning variants of this approach. The basic structure of a re-\ngression model for price prediction is the same no matter\nwhat product it is applied to; all that varies are the name\nand number of the attributes. For example, to predict the\nprice of a house, the input would include attributes such\nas the size of the house, the number of rooms, the number\nof floors, the average house price in the area, the average\nhouse size in the area, and so on. By comparison, to predict\nthe price of a car, the attributes would include the age of\nthe car, the number of miles on the odometer, the engine\nsize, the make of the car, the number of doors, and so on.\nIn each case, given the appropriate data, the regression al-\ngorithm works out how each of the attributes contributes\nto the final price.\nAs has been the case with all the examples given\nthroughout this chapter, the application example of using\nStandard data SCienCe t aSkS 179", "a regression model for price prediction is illustrative only\nof the type of problem that it is appropriate to frame as\na regression-modeling task. Regression prediction can be\nused in a wide variety of other real-world problems. Typi-\ncal regression-prediction problems include calculating\nprofit, value and volume of sales, sizes, demand, distances,\nand dosage.\n180 Chapter 5", "6\nPRIVACY AND ETHICS\nThe biggest unknown facing data science today is how\nsocieties will choose to answer a new version of the old\nquestion regarding how best to balance the freedoms and\nprivacy of individuals and minorities against the security\nand interests of society. In the context of data science, this\nold question is framed as follows: What do we as a soci-\nety view are reasonable ways to gather and use the data\nrelating to individuals in contexts as diverse as fighting\nterrorism, improving medicine, supporting public-policy\nresearch, fighting crime, detecting fraud, assessing credit\nrisk, providing insurance underwriting, and advertising to\ntargeted groups?\nThe promise of data science is that it provides a way\nto understand the world through data. In the current\nera of big data, this promise is very tantalizing, and, in-\ndeed, a number of arguments can be used to support the", "development and adoption of data-driven infrastructure\nand technologies. One common argument relates to im-\nproving efficiency, effectiveness, and competiveness\u2014an\nargument that, at least in the business context, is backed\nby some academic research. For example, a study involv-\ning 179 large publicly traded firms in 2011 showed that\nthe more data driven a firm\u2019s decision making is, the more\nproductive the firm is: \u201cWe find that firms that adopt DDD\n[data-driven decision making] have output and productiv-\nity that is 5\u20136% higher than what would be expected given\ntheir other investments and information technology us-\nage\u201d (Brynjolfsson, Hitt, and Kim 2011, 1).\nAnother argument for increased adoption of data sci-\nence technologies and practices relates to securitization.\nFor a long time, governments have used the argument\nthat surveillance improves security. And since the terror-\nist attacks in the United States on September 11, 2001,\nas well as with each subsequent terrorist attack through-\nout the world, the argument has gained traction. Indeed,\nit was frequently used in the public debate caused by Ed-\nward Snowden\u2019s revelations about the US National Secu-\nrity Agency\u2019s PRISM surveillance program and the data it\nroutinely gathered on US citizens. A stark example of the\npower of this argument is the agency\u2019s US$1.7 billion in-\nvestment in a data center in Bluffdale, Utah, that has the\nability to store huge amounts of intercepted communica-\ntions (Carroll 2013).\n182 Chapter 6", "At the same time, however, societies, governments,\nand business are struggling to understand the long-term\nimplications of data science in a big-data world. Given the\nrapid development of technologies around data gather-\ning, data storage, and data analysis, it is not surprising\nthat the legal frameworks in place and the broader ethical\ndiscussions around data, in particular the question of in-\ndividual privacy, are running behind these advances. Not-\nwithstanding this difficulty, basic legal principles around\ndata collection and usage are important to understand\nand are nearly always applicable. Also, the ethical debate\naround data usage and privacy has highlighted some wor-\nrying trends that we as individuals and citizens should be\naware of.\nCommercial Interests versus Individual Privacy\nData science can be framed as making the world a more\nprosperous and secure place to live. But these same argu-\nments can be used by very different organizations that\nhave very distinct agendas. For example, contrast calls by\ncivil liberties groups for government to be more open and\ntransparent in the gathering, use, and availability of data\nin the hope of empowering citizens to hold these same\ngovernments to account with similar calls from busi-\nness communities who hope to use these data to increase\nprivaCy and ethiCs 183", "their profits (Kitchin 2014a). In truth, data science is a\ndouble-edged sword. It can be used to improve our lives\nthrough more efficient government, improved medicine\nand health care, less-expensive insurance, smarter cities,\nreduced crime, and many more ways. At the same time,\nhowever, it can also be used to spy on our private lives, to\ntarget us with unwanted advertising, and to control our\nbehavior both overtly and covertly (the fear of surveillance\ncan affect us as much as the surveillance itself does).\nThe contradictory aspects of data science can often\nbe apparent in the same applications. For example, the\nuse of data science in health insurance underwriting uses\nthird-party marketing data sets that contain informa-\ntion such as purchasing habits, web search history, along\nwith hundreds of other attributes relating to people\u2019s life-\nstyles (Batty, Tripathi, Kroll, et al. 2010). The use of these\nthird-party data is troublesome because it may trigger\nself-disciplining, wherein people avoid certain activities,\nsuch as visiting extreme-sports websites, for fear of in-\ncurring higher insurance premiums (Mayer-Sch\u00f6nberger\nand Cukier 2014). However, the justification for the use\nof these data is that it acts as a proxy for more invasive\nand expensive information sources, such as blood tests,\nand in the long term will reduce costs and premiums and\nthereby increase the number of people with health insur-\nance (Batty, Tripathi, Kroll, et al. 2010).\n184 Chapter 6", "The fault lines in the debate between the commercial\nbenefits and ethical considerations of using data science\nare apparent in the discussions around the use of per-\nsonal data for targeted marketing. From a business ad-\nvertising perspective, the incentive to use personal data\nis that there is a relationship between personalizing mar-\nketing, services, and products, on the one hand, and the\neffectiveness of the marketing, on the other. It has been\nshown that the use of personal social network data\u2014\nsuch as identifying consumers who are connected to prior\ncustomers\u2014increases the effectiveness of a direct-mail\nmarketing campaign for a telecommunications service\nby three to five times compared to traditional marketing\napproaches (Hill, Provost, and Volinsky 2006). Similar\nclaims have been made about the effectiveness of data-\ndriven personalization of online marketing. For example,\na study of online cost and effectiveness of online targeted\nadvertising in the United States in 2010 compared run-of-\nthe-network marketing (when an advertising campaign is\npushed out across a range of websites without specific tar-\ngeting of users or sites) with behavioral targeting1 (Beales\n2010). The study found that behavioral marketing was\nboth more expensive (2.68 times more) but also more ef-\nfective, with a conversion rate more than twice that of run-\nof-the-network marketing. Another well-known study on\nthe effectiveness of data-driven online advertising was\nconducted by researchers from the University of Toronto\nprivaCy and ethiCs 185", "and MIT (Goldfarb and Tucker 2011). They used the enact-\nment of a privacy-protection bill in the European Union\n(EU)2 that limited the ability of advertising companies to\ntrack users\u2019 online behavior in order to compare the effec-\ntiveness of online advertising under the new restrictions\n(i.e., in the EU) and the effectiveness online advertising\nnot under the new restrictions (i.e., in the United States\nand other non-EU countries). The study found that online\nadvertising was significantly less effective under the new\nrestrictions, with a reported drop of 65 percent in study\nparticipants\u2019 recorded purchasing intent. The results of\nthis study have been contested (see, for example, Mayer\nand Mitchell 2012), but the study has been used to sup-\nport the argument that the more data that are available\nabout an individual, the more effective the advertising\nthat is directed to that individual will be. Proponents of\ndata-driven targeted marketing frame this argument as a\nwin\u2013win for both the advertiser and the consumer, claim-\ning that advertisers lower marketing costs by reducing\nwasted advertising and achieve better conversions rates,\nand consumers get more relevant advertising.\nThis utopian perspective on the use of personal data\nfor targeted marketing is at best based on a selective un-\nderstanding of the problem. Probably one of the most wor-\nrying stories related to targeted advertising was reported\nin the New York Times in 2012 and involves the American\ndiscount retail store Target (Duhigg 2012). It is well known\n186 Chapter 6", "in marketing that one of the times in a person\u2019s life when\nhis or her shopping habits change radically is at the con-\nception and birth of a child. Because of this radical change,\nmarketers see pregnancy as an opportunity to shift a\nperson\u2019s shopping habits and brand loyalties, and many\nretailers use publicly available birth records to trigger per-\nsonalized marketing for new parents, sending them offers\nrelating to baby products. In order to get a competitive ad-\nvantage, Target wanted to identify pregnant customers at\nan early stage (ideally during the second trimester) with-\nout the mother-to-be voluntarily telling Target that she\nwas pregnant.3 This insight would enable Target to begin\nits personalized marketing before other retailers knew the\nbaby was on the way. To achieve this goal, Target initiated\na data science project with the aim of predicting whether a\ncustomer was pregnant based on an analysis of her shop-\nping habits. The starting point for the project was to ana-\nlyze the shopping habits of women who had signed up for\nTarget\u2019s baby-shower registry. The analysis revealed that\nexpectant mothers tended to purchase larger quantities of\nunscented lotion at the beginning of the second trimes-\nter as well as certain dietary supplements throughout the\nfirst 20 weeks of pregnancy. Based on this analysis, Target\ncreated a data-driven model that used around 25 products\nand indictors and assigned each customer a \u201cpregnancy-\nprediction\u201d score. The success, for want of a better word, of\nthis model was made very apparent when a man turned up\nprivaCy and ethiCs 187", "at a Target store to complain about the fact that his high-\nschool-age daughter had been mailed coupons for baby\nclothes and cribs. He accused Target of trying to encour-\nage his daughter to get pregnant. However, over the sub-\nsequent days it transpired that the man\u2019s daughter was in\nfact pregnant but hadn\u2019t told anyone. Target\u2019s pregnancy-\nprediction model was able to identify a pregnant high\nschool student and act on this information before she had\nchosen to tell her family.\nEthical Implications of Data Science: Profiling and\nDiscrimination\nThe story about Target identifying a pregnant high school\nstudent without her consent or knowledge highlights how\ndata science can be used for social profiling not only of\nindividuals but also of minority groups in society. In his\nbook The Daily You: How the New Advertising Industry Is De-\nfining Your Identity and Your Worth (2013), Joseph Turow\ndiscusses how marketers use digital profiling to categorize\npeople as either targets or waste and then use these cat-\negories to personalize the offers and promotions directed\nto individual consumers: \u201cthose considered waste are ig-\nnored or shunted to other products that marketers deem\nmore relevant to their tastes or income\u201d (11). This person-\nalization can result in preferential treatment for some and\n188 Chapter 6", "Personalization can\nresult in preferential\ntreatment for some\nand marginalization\nof others.", "marginalization of others. A clear example of this discrim-\nination is differential pricing on websites, wherein some\ncustomers are charged more than other customers for the\nsame product based on their customer profiles (Clifford\n2012).\nThese profiles are constructed by integrating data\nfrom a number of different noisy and partial data sources,\nso the profiles can often be misleading about an individual.\nWhat is worse is that these marketing profiles are treated\nas products and are often sold to other companies, with\nthe result that a negative marketing assessment of an indi-\nvidual can follow that individual across many domains. We\nhave already discussed the use of marketing data sets in in-\nsurance underwriting (Batty, Tripathi, Kroll, et al. 2010),\nbut these profiles can also make their way into credit-risk\nassessments and many other decision processes that af-\nfect people\u2019s lives. Two aspects of these marketing pro-\nfiles make them particularly problematic. First, they are a\nblack box, and, second, they are persistent. The black-box\nnature of these profiles is apparent when one considers\nthat it is difficult for an individual to know what data are\nrecorded about them, where and when the data were re-\ncorded, and how the decision processes that use these data\nwork. As a result, if an individual ends up on a no-fly list or\na credit blacklist, it is \u201cdifficult to determine the grounds\nfor discrimination and to challenge them\u201d (Kitchin 2014a,\n177). What is more, in the modern world data are often\nstored for a long time. So data recorded about an event in\n190 Chapter 6", "an individual\u2019s life persists long after an event. As Turow\nwarns, \u201cTurning individual profiles into individual evalu-\nations is what happens when a profile becomes a reputa-\ntion\u201d (2013, 6).\nFurthermore, unless used very carefully, data science\ncan actually perpetuate and reinforce prejudice. An argu-\nment is sometimes made that data science is objective:\nit is based on numbers, so it doesn\u2019t encode or have the\nprejudicial views that affect human decisions. The truth\nis that data science algorithms perform in an amoral\nmanner more than in an objective manner. Data science\nextracts patterns in data; however, if the data encode a\nprejudicial relationship in society, then the algorithm is\nlikely to identify this pattern and base its outputs on the\npattern. Indeed, the more consistent a prejudice is in a\nsociety, the stronger that prejudicial pattern will appear\nin the data about that society, and the more likely a data\nscience algorithm will extract and replicate that pattern of\nprejudice. For example, a study carried out by academic re-\nsearchers on the Google Online Advertising system found\nthat the system showed an ad relating to a high-paying\njob more frequently to participants whose Google profile\nidentified them as male compared to participants whose\nprofile identified them as female (Datta, Tschantz, and\nDatta 2015).\nThe fact that data science algorithms can reinforce\nprejudice is particularly troublesome when data science\nis applied to policing. Predictive Policing, or PredPol,4 is\nprivaCy and ethiCs 191", "a data science tool designed to predict when and where\na crime is most likely to occur. When deployed in a city,\nPredPol generates a daily report listing a number of hot\nspots on a map (small areas 500 feet by 500 feet) where the\nsystem believes crimes are likely to occur and tags each hot\nspot with the police shift during which the system believes\nthe crime will occur. Police departments in both the United\nStates and the United Kingdom have deployed PredPol.\nThe idea behind this type of intelligent-policing system is\nthat policing resources can be efficiently deployed. On the\nsurface, this seems like a sensible application of data sci-\nence, potentially resulting in efficient targeting of crime\nand reducing policing costs. However, questions have\nbeen raised about the accuracy of PredPol and the effec-\ntiveness of similar predictive-policing initiatives (Hunt,\nSaunders, and Hollywood 2014; Oakland Privacy Work-\ning Group 2015; Harkness 2016). The potential for these\ntypes of systems to encode racial or class-based profiling\nin policing has also been noted (Baldridge 2015). The de-\nployment of police resources based on historic data can re-\nsult in a higher police presence in certain areas\u2014typically\neconomically disadvantaged areas\u2014which in turn results\nin higher levels of reported crime in these areas. In other\nwords, the prediction of crime becomes a self-fulfilling\nprophesy. The result of this cycle is that some locations\nwill be disproportionately targeted by police surveillance,\ncausing a breakdown in trust between the people who live\n192 Chapter 6", "Unless used very\ncarefully, data science\ncan actually perpetuate\nand reinforce prejudice.", "in those communities and policing institutions (Harkness\n2016).\nAnother example of data-driven policing is the Stra-\ntegic Subjects List (SSL) used by the Chicago Police De-\npartment in an attempt to reduce gun crime. The list was\nfirst created in 2013, and at that time it listed 426 peo-\nple who were estimated to be at a very high risk of gun\nviolence. In an attempt to proactively prevent gun crime,\nthe Chicago Police Department contacted all the people\non the SSL to warn them that they were under surveil-\nlance. Some of the people on the list were very surprised\nto be included on it because although they did have crimi-\nnal records for minor offenses, they had no violence on\ntheir records (Gorner 2013). One question to ask about\nthis type of data gathering to prevent crime is, How ac-\ncurate is the technology? A recent study found that the\npeople on the SSL for 2013 were \u201cnot more or less likely to\nbecome a victim of a homicide or shooting than the com-\nparison group\u201d (Saunders, Hunt, and Hollywood 2016).\nHowever, this study also found that individuals on the list\nwere more likely to be arrested for a shooting incident,\nalthough it did point out that this greater likelihood could\nhave been created by the fact that these individuals were\non the list, which resulted in increasing police officers\u2019\nawareness of these individuals (Saunders, Hunt, and Hol-\nlywood 2016). Responding to this study, the Chicago Police\nDepartment stated that it regularly updated the algorithm\n194 Chapter 6", "used to compile the SSL and that the effectiveness of the\nSSL had improved since 2013 (Rhee 2016). Another ques-\ntion about data-driven crime-prevention lists is, How\ndoes an individual end up on the list? The 2013 version\nof the SSL appears to have been compiled using, among\nother attributes of an individual, an analysis of his or her\nsocial network, including the arrest and shooting histo-\nries of his or her acquaintances (Dokoupil 2013; Gorner\n2013). On the one hand, the idea of using social network\nanalysis makes sense, but it opens up the very real prob-\nlem of guilt by association. One problem with this type\nof approach is that it can be difficult to define precisely\nwhat an association between two individuals entails. Is\nliving on the same street enough to be an association?\nFurthermore, in the United States, where the vast major-\nity of inmates in prison are African American and Latino\nmales, allowing predictive-policing algorithms to use the\nconcept of association as an input is likely to result in pre-\ndictions targeting mainly young men of color (Baldridge\n2015).\nThe anticipatory nature of predictive policing means\nthat individuals may be treated differently not because of\nwhat they have done but because of data-driven inferences\nabout what they might do. As a result, these types of sys-\ntems may reinforce discriminatory practices by replicating\nthe patterns in historic data and may create self-fulfilling\nprophecies.\nprivaCy and ethiCs 195", "Ethical Implications of Data Science: Creating a\nPanopticon\nIf you spend time absorbing some of the commercial\nboosterism that surrounds data science, you get a sense\nthat any problem can be solved using data science technol-\nogy given enough of the right data. This marketing of the\npower of data science feeds into a view that a data-driven\napproach to governance is the best way to address complex\nsocial problems, such as crime, poverty, poor education,\nand poor public health: all we need to do to solve these\nproblems is to put sensors into our societies to track ev-\nerything, merge all the data, and run the algorithms to\ngenerate the key insights that provide the solution.\nWhen this argument is accepted, two processes are\noften intensified. The first is that society becomes more\ntechnocratic in nature, and aspects of life begin to be\nregulated by data-driven systems. Examples of this type\nof technological regulation already exist\u2014for example, in\nsome jurisdictions data science is currently used in parole\nhearings (Berk and Bleich 2013) and sentencing (Barry-\nJester, Casselman, and Goldstein 2015). For an example\noutside of the judicial system, consider how smart-city\ntechnologies regulate traffic flows through cities with\nalgorithms dynamically deciding which traffic flow gets\npriority at a junction at different times of day (Kitchin\n2014b). A by-product of this technocratic regulation is the\nproliferation of the sensors that support the automated\n196 Chapter 6", "regulating systems. The second process is \u201ccontrol creep,\u201d\nwherein data gathered for one purpose is repurposed and\nused to regulate in another way (Innes 2001). For exam-\nple, road cameras that were installed in London with the\nprimary purpose of regulating congestion and implement-\ning congestion charges (the London congestion charge is\na daily charge for driving a vehicle within London dur-\ning peak times) have been repurposed for security tasks\n(Dodge and Kitchin 2007). Other examples of control\ncreep include a technology called ShotSpotter that con-\nsists of a city-wide network of microphones designed to\nidentify gunshots and report the locations of them but\nthat also records conversations, some of which were used\nto achieve criminal convictions (Weissman 2015), and the\nuse of in-car navigation systems to monitor and fine rental\ncar drivers who drive out of state (Elliott 2004; Kitchin\n2014a).\nAn aspect of control creep is the drive to merge data\nfrom different sources so as to provide a more complete\npicture of a society and thereby potentially unlock deeper\ninsights into the problems in the system. There are often\ngood reasons for the repurposing of data. Indeed, calls\nare frequently made for data held by different branches\nof government to be merged for legitimate purposes\u2014for\nexample, to support health research and for the conve-\nnience of the state and its citizens. From a civil liberties\nperspective, however, these trends are very concerning.\nprivaCy and ethiCs 197", "Heightened surveillance, the integration of data from\nmultiple sources, control creep, and anticipatory gover-\nnance (such as the predictive-policing programs) may re-\nsult in a society where an individual may be treated with\nsuspicion simply because a sequence of unrelated inno-\ncent actions or encounters matches a pattern deemed sus-\npicious by a data-driven regulatory system. Living in this\ntype of a society would change each of us from free citizens\ninto inmates in Bentham\u2019s Panopticon,5 constantly self-\ndisciplining our behaviors for fear of what inferences may\nbe drawn from them. The distinction between individuals\nwho believe and act as though they are free of surveillance\nand individuals who self-discipline out of fear that they\ninhabit a Panopticon is the primary difference between a\nfree society and a totalitarian state.\n\u00c1 la recherche du privacy perdu\nAs individuals engage with and move through techni-\ncally modern societies, they have no choice but to leave\na data trail behind them. In the real world, the prolifera-\ntion of video surveillance means that location data can be\ngathered about an individual whenever she appears on a\nstreet or in a shop or car park, and the proliferation of\ncell phones means that many people can be tracked via\ntheir phones. Other examples of real-world data gathering\n198 Chapter 6", "include the recording of credit card purchases, the use of\nloyalty schemes in supermarkets, the tracking of with-\ndrawals from ATMs, and the tracking of cell phone calls\nmade. In the online world, data are gathered about in-\ndividuals when they visit or log in to websites; send an\nemail; engage in online shopping; rate a date, restaurant,\nor store; use an e-book reader; watch a lecture in a massive\nopen online course; or like or post something on a social\nmedia site. To put into perspective the amount of data that\nare gathered on the average individual in a technologically\nmodern society, a report from the Dutch Data Protection\nAuthority in 2009 estimated that the average Dutch citi-\nzen was included in 250 to 500 databases, with this figure\nrising to 1,000 databases for more socially active people\n(Koops 2011). Taken together, the data points relating to\nan individual define that person\u2019s digital footprint.\nThe data in a digital footprint can be gathered in two\ncontexts that are problematic from a privacy perspective.\nFirst, data can be collected about an individual without\nhis knowledge or awareness. Second, in some contexts an\nindividual may choose to share data about himself and\nhis opinions but may have little or no knowledge of or\ncontrol over how these data are used or how they will be\nshared with and repurposed by third parties. The terms\ndata shadow and data footprint6 are used to distinguish\nthese two contexts of data gathering: an individual\u2019s data\nshadow comprises the data gathered about an individual\nprivaCy and ethiCs 199", "without her knowledge, consent, or awareness, and an in-\ndividual\u2019s data footprint consists of the pieces of data that\nshe knowingly makes public (Koops 2011).\nThe collection of data about an individual without\nher knowledge or consent is of course worrying. However,\nthe power of modern data science techniques to uncover\nhidden patterns in data coupled with the integration and\nrepurposing of data from several sources means that even\ndata collected with an individual\u2019s knowledge and consent\nin one context can have negative effects on that individual\nthat are impossible for them to predict. Today, with the\nuse of modern data science techniques, very personal in-\nformation that we may not want to be made public and\nchoose not to share can still be reliably inferred from seem-\ningly unrelated data we willingly post on social media.\nFor example, many people are willing to like something\non Facebook because they want to demonstrate support\nto a friend. However, by simply using the items that an\nindividual has liked on Facebook, data-driven models can\naccurately predict that person\u2019s sexual orientation, politi-\ncal and religious views, intelligence and personality traits,\nand use of addictive substances such as alcohol, drugs, and\ncigarettes; they can even determine whether that person\u2019s\nparents stayed together until he or she was 21 years old\n(Kosinski, Stillwell, and Graepel 2013). The out-of-context\nlinkages made in these models is demonstrated by how lik-\ning a human rights campaign was found to be predictive of\n200 Chapter 6", "homosexuality (both male and female) and by how liking\nHondas was found to be predictive of not smoking (Kosin-\nski, Stillwell, and Graepel 2013).\nComputational Approaches to Preserving Privacy\nIn recent years, there has been a growing interest in com-\nputational approaches to preserving individual privacy\nthroughout a data-analysis process. Two of the best-known\napproaches are differential privacy and federated learning.\nDifferential privacy is a mathematical approach to the\nproblem of learning useful information about a popula-\ntion while at the same time learning nothing about the in-\ndividuals within the population. Differential privacy uses\na particular definition of privacy: the privacy of an indi-\nvidual has not been compromised by the inclusion of his\nor her data in the data-analysis process if the conclusions\nreached by the analysis would have been the same inde-\npendent of whether the individual\u2019s data were included or\nnot. A number of processes can be used to implement dif-\nferential privacy. At the core of these processes is the idea\nof injecting noise either into the data-collection process\nor into the responses to database queries. The noise pro-\ntects the privacy of individuals but can be removed from\nthe data at an aggregate level so that useful population-\nlevel statistics can be calculated. A useful example of a\nprivaCy and ethiCs 201", "procedure for injecting noise into data that provides an\nintuitive explanation of how differential privacy processes\ncan work is the randomized-response technique. The use\ncase for this technique is a survey that includes a sensitive\nyes/no question (e.g., relating to law breaking, health con-\nditions, etc.). Survey respondents are instructed to answer\nthe sensitive question using the following procedure:\n1. Flip a coin and keep the result of the coin flip secret.\n2. If tails, respond \u201cYes.\u201d\n3. If heads, respond truthfully.\nHalf the respondents will get tails and respond \u201cYes\u201d;\nthe other half will respond truthfully. Therefore, the true\nnumber of \u201cNo\u201d respondents in the total population is (ap-\nproximately) twice the number of \u201cNo\u201d responses (the coin\nis fair and selects randomly, so the distribution of yes/no\nresponses among the respondents who got tails should\nmirror the number of respondents who answered truth-\nfully). Given the true count for \u201cNo,\u201d we can calculate the\ntrue count for \u201cYes.\u201d However, although we now have an\naccurate count for the population regarding the sensitive\n\u201cYes\u201d condition, it is not possible to identify for which\nof the \u201cYes\u201d respondents the sensitive condition actually\nholds. There is a trade-off between the amount of noise\ninjected into data and the usefulness of the data for data\n202 Chapter 6", "analysis. Differential privacy addresses this trade-off by\nproviding estimates of the amount of noise required given\nfactors such as the distribution of data within the data-\nbase, the type of database query that is being processed,\nand the number of queries through which we wish to guar-\nantee an individual\u2019s privacy. Cynthia Dwork and Aaron\nRoth (2014) provide an introduction to differential pri-\nvacy and an overview of several approaches to implement-\ning differential privacy. Differential-privacy techniques\nare now being deployed in a number of consumer prod-\nucts. For example, Apple uses differential privacy in iOS\n10 to protect the privacy of individual users while at the\nsame time learning usage patterns to improve predictive\ntext in the messaging application and to improve search\nfunctionality.\nIn some scenarios, the data being used in a data sci-\nence project are coming from multiple disparate sources.\nFor example, multiple hospitals may be contributing to\na single research project, or a company is collecting data\nfrom a large number of users of a cell phone application.\nRather than centralizing these data into a single data re-\npository and doing the analysis on the combined data, an\nalternative approach is to train different models on the\nsubsets of the data at the different data sources (i.e., at\nthe individual hospitals or on the phones of each individ-\nual user) and then to merge the separately trained models.\nGoogle uses this federated-learning approach to improve\nprivaCy and ethiCs 203", "The truth is that data\nscience algorithms\nperform in an amoral\nmanner more than in\nan objective manner.", "the query suggestions made by the Google keyboard\non Android (McMahan and Ramage 2017). In Google\u2019s\nfederated-learning framework, the mobile device initially\nhas a copy of the current application loaded. As the user\nuses the application, the application data for that user are\ncollected on his phone and used by a learning algorithm\nthat is local to the phone to update the local version of\nthe model. This local update of the model is then uploaded\nto the cloud, where it is averaged with the model updates\nuploaded from other user phones. The core model is then\nupdated using this average. With the use of this process,\nthe core model can be improved, and individual users\u2019\nprivacy can at the same time be protected to the extent\nthat only the model updates are shared\u2014not the users\u2019\nusage data.\nLegal Frameworks for Regulating Data Use and\nProtecting Privacy\nThere is variation across jurisdictions in the laws relating\nto privacy protection and permissible data usage. How-\never, two core pillars are present across most democratic\njurisdictions: antidiscrimination legislation and personal-\ndata-protection legislation.\nIn most jurisdictions, antidiscrimination legisla-\ntion forbids discrimination based on any of the following\nprivaCy and ethiCs 205", "grounds: disability, age, sex, race, ethnicity, nationality,\nsexual orientation, and religious or political opinion. In\nthe United States, the Civil Rights Act of 19647 prohib-\nits discrimination based on color, race, sex, religion, or\nnationality. Later legislation has extended this list; for\nexample, the Americans with Disabilities Act of 19908 ex-\ntended protection to people against discrimination based\non disabilities. Similar legalization is in place in many\nother jurisdictions. For example, the Charter of Funda-\nmental Rights of the European Union prohibits discrimi-\nnation based on any grounds, including race, color, ethnic\nor social origin, genetic features, sex, age, birth, disability,\nsexual orientation, religion or belief, property, member-\nship in a national minority, and political or any other opin-\nion (Charter 2000).\nA similar situation of variation and overlap exists\nwith respect to privacy legislation across different juris-\ndictions. In the United States, the Fair Information Prac-\ntice Principles (1973)9 have provided the basis for much\nof the subsequent privacy legislation in that jurisdiction.\nIn the EU, the Data Protection Directive (Council of the\nEuropean Union and European Parliament 1995) is the\nbasis for much of that jurisdiction\u2019s privacy legislation.\nThe General Data Protection Regulations (Council of the\nEuropean Union and European Parliament 2016) expand\non the data protection principles in the Data Protection\nDirective and provide consistent and legally enforceable\ndata protection regulations across all EU member states.\n206 Chapter 6", "However, the most broadly accepted principles relating to\npersonal privacy and data are the Guidelines on the Pro-\ntection of Privacy and Transborder Flows of Personal Data\npublished by the Organisation for Economic Co-operation\nand Development (OECD 1980). Within these guidelines,\npersonal data are defined as records relating to an identifi-\nable individual, known as the data subject. The guidelines\ndefine eight (overlapping) principles that are designed to\nprotect a data subject\u2019s privacy:\n1. Collection Limitation Principle: Personal data should\nonly be obtained lawfully and with the knowledge and con-\nsent of the data subject.\n2. Data Quality Principle: Any personal data that are col-\nlected should be relevant to the purpose for which they are\nused; they should be accurate, complete, and up to date.\n3. Purpose Specification Principle: At or before the time\nthat personal data are collected, the data subject should be\ninformed of the purpose for which the data will be used.\nFurthermore, although changes of purpose are permis-\nsible, they should not be introduced arbitrarily (new pur-\nposes must be compatible with the original purpose) and\nshould be specified to the data subject.\n4. Use Limitation Principle: The use of personal data is\nlimited to the purpose that the data subject has been in-\nformed of, and the data should not be disclosed to third\nprivaCy and ethiCs 207", "parties without the data subject\u2019s consent or by authority\nof law.\n5. Safety Safeguards Principle: Personal data should be\nprotected by security safeguards against deletion, theft,\ndisclosure, modification, or unauthorized use.\n6. Openness Principle: A data subject should be able to\nacquire information with reasonable ease regarding the\ncollection, storage, and use of his or her personal data.\n7. Individual Participation Principle: A data subject has\nthe right to access and challenge personal data.\n8. Accountability Principle: A data controller is account-\nable for complying with the principles.\nMany countries, including the EU and the United\nStates, endorse the OECD guidelines. Indeed, the data\nprotection principles in the EU General Data Protection\nRegulations can be broadly traced back to the OECD guide-\nlines. The General Data Protection Regulations apply to\nthe collection, storage, transfer and processing of personal\ndata relating to EU citizens within the EU and has impli-\ncations for the flows of this data outside of the EU. Cur-\nrently, several countries are developing data protection\nlaws similar to and consistent with the General Data Pro-\ntection Regulations.\n208 Chapter 6", "Toward an Ethical Data Science\nIt is well known that despite the legal frameworks that are\nin place, nation-states frequently collect personal data on\ntheir citizens and foreign nationals without these people\u2019s\nknowledge, often in the name of security and intelligence.\nExamples include the US National Security Agency\u2019s\nPRISM program; the UK Government Communications\nHeadquarters\u2019 Tempora program (Shubber 2013); and the\nRussian government\u2019s System for Operative Investigative\nActivities (Soldatov and Borogan 2012). These programs\naffect the public\u2019s perception of governments and use of\nmodern communication technologies. The results of the\nPew survey \u201cAmericans\u2019 Privacy Strategies Post-Snowden\u201d\nin 2015 indicated that 87 percent of respondents were\naware of government surveillance of phone and Internet\ncommunications, and among those who were aware of\nthese programs 61 percent stated that they were losing\nconfidence that these programs served the public inter-\nest, and 25 percent reported that they had changed how\nthey used technologies in response to learning about these\nprograms (Rainie and Madden 2015). Similar results have\nbeen reported in European surveys, with more than half\nof Europeans aware of large-scale data collection by gov-\nernment agencies and most respondents stating that this\ntype of surveillance had a negative impact on their trust\nwith respect to how their online personal data are used\n(Eurobarometer 2015).\nprivaCy and ethiCs 209", "At the same time, many private companies avoid the\nregulations around personal data and privacy by claiming\nto use derived, aggregated, or anonymized data. By re-\npackaging data in these ways, companies claim that the\ndata are no longer personal data, which, they argue, per-\nmits them to gather data without an individual\u2019s aware-\nness or consent and without having a clear immediate\npurpose for the data; to hold the data for long periods of\ntime; and to repurpose the data or sell the data when a\ncommercial opportunity arises. Many advocates of the\ncommercial opportunities of data science and big data\nargue that the real commercial value of data comes from\ntheir reuse or \u201coptional value\u201d (Mayer-Sch\u00f6nberger and\nCukier 2014). The advocates of data reuse highlight two\ntechnical innovations that make data gathering and stor-\nage a sensible business strategy: first, today data can be\ngathered passively with little or no effort or awareness on\nthe part of the individuals being tracked; and, second, data\nstorage has become relatively inexpensive. In this context,\nit makes commercial sense to record and store data in case\nfuture (potentially unforeseeable) commercial opportuni-\nties make it valuable.\nThe modern commercial practices of hoarding, repur-\nposing, and selling data are completely at odds with the\npurpose specification and use-limitation principles of the\nOECD guidelines. Furthermore, the collection-limitation\nprinciple is undermined whenever a company presents\n210 Chapter 6", "a privacy agreement to a consumer that is designed to\nbe unreadable or reserves the right for the company to\nmodify the agreement without further consultation or\nnotification or both. Whenever this happens, the process\nof notification and granting of consent is turned into a\nmeaningless box-ticking exercise. Similar to the public\nopinion about government surveillance in the name of\nsecurity, public opinion is quite negative toward com-\nmercial websites\u2019 gathering and repurposing of personal\ndata. Again using American and European surveys as our\nlitmus test for wider public opinion, a survey of Ameri-\ncan Internet users in 2012 found that 62 percent of adults\nsurveyed stated that they did not know how to limit the\ninformation collected about them by websites, and 68 per-\ncent stated that they did not like the practice of targeted\nadvertising because they did not like their online behavior\ntracked and analyzed (Purcell, Brenner, and Rainie 2012).\nA recent survey of European citizens found similar results:\n69 percent of respondents felt that the collection of their\ndata should require their explicit approval, but only 18 per-\ncent of respondents actually fully read privacy statements.\nFurthermore, 67 percent of respondents stated that they\ndon\u2019t read privacy statements because they found them\ntoo long, and 38 percent stated that they found them un-\nclear or too difficult to understand. The survey also found\nthat 69 percent of respondents were concerned about\ntheir information being used for different purposes from\nprivaCy and ethiCs 211", "the one it was collected for, and 53 percent of respondents\nwere uncomfortable with Internet companies using their\npersonal information to tailor advertising (Eurobarom-\neter 2015).\nSo at the moment public opinion is broadly negative\ntoward both government surveillance and Internet com-\npanies\u2019 gathering, storing, and analyzing of personnel\ndata. Today, most commentators agree that data-privacy\nlegislation needs to be updated and that changes are hap-\npening. In 2012, both the EU and the United States pub-\nlished reviews and updates relating to data-protection\nand privacy policies (European Commission 2012; Federal\nTrade Commission 2012; Kitchin 2014a, 173). In 2013,\nthe OECD guidelines were extended to include, among\nother updates, more details in relation to implementing\nthe accountability principle. In particular, the new guide-\nlines define the data controller\u2019s responsibilities to have\na privacy-management program in place and to define\nclearly what such a program entails and how it should be\nframed in terms of risk management in relation to per-\nsonal data (OECD 2013). In 2014, a Spanish citizen, Mario\nCosteja Gonzalez, won a case in the EU Court of Justice\nagainst Google (C-131/12 [2014]) asserting his right to be\nforgotten. The court held that an individual could request,\nunder certain conditions, an Internet search engine to re-\nmove links to webpages that resulted from searches on the\nindividual\u2019s name. The grounds for such a request included\nthat the data are inaccurate or out of date or that the data\n212 Chapter 6", "had been kept for longer than was necessary for historical,\nstatistical, or scientific purposes. This ruling has major\nimplications for all Internet search engines but may also\nhave implications for other big-data hoarders. For ex-\nample, it is not clear at present what the implications are\nfor social media sites such as Facebook and Twitter (Marr\n2015). The concept of the right to be forgotten has been\nasserted in other jurisdictions. For example, the Califor-\nnia \u201ceraser\u201d law asserts a minor\u2019s right to have material\nhe has posted on an Internet or mobile service removed\nat his request. The law also prohibits Internet, online, or\ncell phone service companies from compiling personal\ndata relating to a minor for the purposes of targeted ad-\nvertising or allowing a third party to do so.10 As a final\nexample of the changes taking place, in 2016 the EU-US\nPrivacy Shield was signed and adopted (European Com-\nmission 2016). Its focus is on harmonizing data-privacy\nobligations across the two jurisdictions. Its purpose is to\nstrengthen the data-protection rights for EU citizens in the\ncontext where their data have been moved outside of the\nEU. This agreement imposed stronger obligations on com-\nmercial companies with regard to transparency of data us-\nage, strong oversight mechanisms and possible sanctions,\nas well as limitations and oversight mechanisms for public\nauthorities in recording or accessing personal data. How-\never, at the time of writing, the strength and effectiveness\nof the EU-US Privacy Shield is being tested in a legal case\nin the Irish courts. The reason why the Irish legal system\nprivaCy and ethiCs 213", "is at the center of this debate is that many of the large\nUS multinational Internet companies (Google, Facebook,\nTwitter, etc.) have their European, Middle East, and Africa\nheadquarters in Ireland. As a result, the data-protection\ncommissioner for Ireland is responsible for enforcing EU\nregulations on transnational data transfers made by these\ncompanies. Recent history illustrates that it is possible\nfor legal cases to result in significant and swift changes\nin the regulation of how personnel data are handled. In\nfact, the EU-US Privacy Shield is a direct consequence of a\nsuit filed by Max Schrems, an Austrian lawyer and privacy\nactivist, against Facebook. The outcome of Schrems\u2019s case\nin 2015 was to invalidate the existing EU-US Safe Harbor\nagreement with immediate effect, and the EU-US Privacy\nShield was developed as an emergency response to this\noutcome. Compared to the original Safe Harbor agree-\nment, the Privacy Shield has strengthened EU citizens\u2019\ndata-privacy rights (O\u2019Rourke and Kerr 2017), and it may\nwell be that any new framework would further strengthen\nthese rights. For example, the EU General Data Protection\nRegulations will provide legally enforceable data protec-\ntion to EU citizens from May 2018.\nFrom a data science perspective, these examples illus-\ntrate that the regulations around data privacy and protec-\ntion are in flux. Admittedly, the examples listed here are\nfrom the US and EU contexts, but they are indicative of\nbroader trends in relation to privacy and data regulation.\n214 Chapter 6", "It is very difficult to predict how these changes will play\nout in the long term. A range of vested interests exist in\nthis domain: consider the differing agendas of big Inter-\nnet, advertising and insurances companies, intelligence\nagencies, policing authorities, governments, medical and\nsocial science research, and civil liberties groups. Each of\nthese different sectors of society has differing goals and\nneeds with regard to data usage and consequently has dif-\nferent views on how data-privacy regulation should be\nshaped. Furthermore, we as individuals will probably have\nshifting views depending on the perspective we adopt.\nFor example, we might be quite happy for our personnel\ndata to be shared and reused in the context of medical re-\nsearch. However, as the public-opinion surveys in Europe\nand the United States have reported, many of us have res-\nervations about data gathering, reuse, and sharing in the\ncontext of targeted advertising. Broadly speaking, there\nare two themes in the discourse around the future of data\nprivacy. One view argues for the strengthening of regu-\nlations relating to the gathering of personal data and in\nsome cases empowering individuals to control how their\ndata are gathered, shared, and used. The other view argues\nfor deregulation in relation to the gathering of data but\nalso for stronger laws to redress the misuse of personnel\ndata. With so many different stakeholders and perspec-\ntives, there are no easy or obvious answers to the ques-\ntions posed about privacy and data. It is likely that the\nprivaCy and ethiCs 215", "eventual solutions that are developed will be defined on a\nsector-by-sector basis and consist of compromises negoti-\nated between the relevant stakeholders.\nIn such a fluid context, it is best to act conservatively\nand ethically. As we work on developing new data science\nsolutions to business problems, we should consider ethi-\ncal questions in relation to personal data. There are good\nbusiness reasons to do so. First, acting ethically and trans-\nparently with personal data will ensure that a business will\nhave good relationships with its customers. Inappropriate\npractices around personal data can cause a business severe\nreputational damage and cause its customer to move to\ncompetitors (Buytendijk and Heiser 2013). Second, there\nis a risk that as data integration, reuse, profiling, and tar-\ngeting intensify, public opinion will harden around data\nprivacy in the coming years, which will lead to more-\nstringent regulations. Consciously acting transparently\nand ethically is the best way to ensure that the data science\nsolutions we develop do not run afoul of current regula-\ntions or of the regulations that may come into existence in\nthe coming years.\nAphra Kerr (2017) reports a case from 2015 that illus-\ntrates how not taking ethical considerations into account\ncan have serious consequences for technology developers\nand vendors. The case resulted in the US Federal Trade\nCommission fining app game developers and publishers\nunder the Children\u2019s Online Privacy Protection Act. The\n216 Chapter 6", "developers had integrated third-party advertising into\ntheir free-to-play games. Integrating third-party advertis-\ning is standard practice in the free-to-play business model,\nbut the problem arose because the games were designed\nfor children younger than 13. As a result, in sharing their\nusers\u2019 data with advertising networks, the developers\nwhere in fact also sharing data relating to children and as\na result violated the Children\u2019s Online Privacy Protection\nAct. Also, in one instance the developers failed to inform\nthe advertising networks that the apps were for children.\nAs a result, it was possible that inappropriate advertising\ncould be shown to children, and in this instance the Federal\nTrade Commission ruled that the game publishers were re-\nsponsible for ensuring that age-appropriate content and\nadvertising were supplied to the game-playing children.\nThere has been an increasing number of these types of\ncases in recent years, and a number of organizations, in-\ncluding the Federal Trade Commission (2012), have called\nfor businesses to adopt the principles of privacy by design\n(Cavoukian 2013). These principles were developed in the\n1990s and have become a globally recognized framework\nfor the protection of privacy. They advocate that protect-\ning privacy should be the default mode of operation for\nthe design of technology and information systems. To fol-\nlow these principles requires a designer to consciously and\nproactively seek to embed privacy considerations into the\ndesign of technologies, organizational practices, and net-\nworked system architectures.\nprivaCy and ethiCs 217", "Although the arguments of ethical data science are\nclear, it is not always easy to act ethically. One way to make\nthe challenge of ethical data science more concrete is to\nimagine you are working for a company as a data scientist\non a business-critical project. In analyzing the data, you\nhave identified a number of interacting attributes that\ntogether are a proxy for race (or some other personal at-\ntribute, such as religion, gender, etc.). You know that le-\ngally you can\u2019t use the race attribute in your model, but\nyou believe that these proxy attributes would enable you\nto circumvent the antidiscrimination legislation. You also\nbelieve that including these attributes in the model will\nmake your model work, although you are naturally con-\ncerned that this successful outcome may be because the\nmodel will learn to reinforce discrimination that is already\npresent in the system. Ask yourself: \u201cWhat do I do?\u201d\n218 Chapter 6", "7\nFUTURE TRENDS AND PRINCIPLES\nOF SUCCESS\nAn obvious trend in modern societies is the proliferation\nof systems that can sense and react to the world: smart\nphones, smart homes, self-driving cars, and smart cities.\nThis proliferation of smart devices and sensors presents\nchallenges to our privacy, but it is also driving the growth\nof big data and the development of new technology para-\ndigms, such as the Internet of Things. In this context, data\nscience will have a growing impact across many areas of\nour lives. However, there are two areas where data science\nwill lead to significant developments in the coming decade:\npersonal medicine and the development of smart cities.\nMedical Data Science\nIn recent years, the medical industry has been looking\nat and adopting data science and predictive analytics.", "Doctors have traditionally had to rely on their experiences\nand instincts when diagnosing a condition or deciding on\nwhat the next treatment might be. The evidence-based\nmedicine and precision-medicine movement argue that\nmedical decisions should be based on data, ideally linking\nthe best available data to an individual patient\u2019s predica-\nment and preferences. For example, in the case of precision\nmedicine, fast genome-sequencing technology means that\nit is now feasible to analyze the genomes of patients with\nrare diseases in order to identify mutations that cause the\ndisease so as to design and select appropriate therapies\nspecific to that individual. Another factor driving data sci-\nence in medicine is the cost of health care. Data science,\nin particular predictive analytics, can be used to automate\nsome health care processes. For example, predictive ana-\nlytics has been used to decide when antibiotics and other\nmedicines should be administrated to babies and adults,\nand it is widely reported that many lives have been saved\nbecause of this approach.\nMedical sensors worn or ingested by the patient or\nimplanted are being developed to continuously monitor\na patient\u2019s vital signs and behaviors and how his or her\norgans are functioning throughout the day. These data\nare continuously gathered and fed back to a centralized\nmonitoring server. It is here at the monitoring server that\nhealth care professionals access the data being generated\nby all the patients, assess their conditions, understand\n220 Chapter 7", "what effects the treatment is having, and compare each\npatient\u2019s results to those of other patients with similar\nconditions to inform them regarding what should happen\nnext in each patient\u2019s treatment regime. Medical science\nis using the data generated by these sensors and integrat-\ning it with additional data from the various parts of the\nmedical profession and the pharmaceutical industry to\ndetermine the effects of current and new medicines. Per-\nsonalized treatment programs are being developed based\non the type of patient, his condition, and how his body\nresponds to various medicines. In addition, this new type\nof medical data science is now feeding into new research\non medicines and their interactions, the design of more\nefficient and detailed monitoring systems, and the uncov-\nering of greater insights from clinical trials.\nSmart Cities\nVarious cities around the world are adopting new tech-\nnology to be able to gather and use the data generated by\ntheir citizens in order to better manage the cities\u2019 orga-\nnizations, utilities, and services. There are three core en-\nablers of this trend: data science, big data, and the Internet\nof Things. The name \u201cInternet of Things\u201d describes the\ninternetworking of physical devices and sensors so that\nthese devices can share information. This may sound\nFuture trends and prinCiples oF suCCess 221", "mundane, but it has the benefit that we can now remotely\ncontrol smart devices (such as our home if it is properly\nconfigured) and opens the possibility that networked\nmachine-to-machine communication will enable smart\nenvironments to autonomously predict and react to our\nneeds (for example, there are now commercially available\nsmart refrigerators that can warn you when food is about\nto spoil and allows you to order fresh milk through your\nsmart phone).\nSmart-city projects integrate real-time data from\nmany different data sources into a single data hub, where\nthey are analyzed and used to inform management and\nplanning decisions. Some smart-city projects involve\nbuilding brand-new cities that are smart from the ground\nup. Both Masdar City in the United Arab Emirates and\nSongdo City in South Korea are brand-new cities that have\nbeen built with the smart technology at their core and a\nfocus on being eco-friendly and energy efficient. However,\nmost smart-city projects involve the retrofitting of exist-\ning cities with new sensor networks and data-processing\ncenters. For example, in the SmartSantander project in\nSpain,1 more than 12,000 networked sensors have been\ninstalled across the city to measure temperature, noise,\nambient lighting, carbon monoxide levels, and parking.\nSmart-city projects often focus on developing energy ef-\nficiency, planning and routing traffic, and planning utility\nservices to match population needs and growth.\n222 Chapter 7", "Japan has embraced the smart-city concept with a par-\nticular focus on reducing energy usage. The Tokyo Electric\nPower Company (TEPC) has installed more than 10 million\nsmart meters across homes in the TEPC service area.2 At\nthe same time, TEPC is developing and rolling out smart-\nphone applications that enable customers to track the\nelectricity used in their homes in real time and to change\ntheir electricity contract. These smart-phone applications\nalso enable the TEPC to send each customer personalized\nenergy-saving advice. Outside of the home, smart-city\ntechnology can be used to reduce energy usage through\nintelligent street lighting. The Glasgow Future Cities\nDemonstrator is piloting street lighting that switches on\nand off depending on whether people are present. Energy\nefficiency is also a top priority for all new buildings, par-\nticularly for large local government and commercial build-\nings. These buildings\u2019 energy efficiency can be optimized\nby automatically managing climate controls through a\ncombination of sensor technology, big data, and data sci-\nence. An extra benefit of these smart-building monitor-\ning systems is that they can monitor for levels of pollution\nand air quality and can activate the necessary controls and\nwarnings in real time.\nTransport is another area where cities are using data\nscience. Many cities have implemented traffic-monitoring\nand management systems. These systems use real-time\ndata to control the flow of traffic through the city. For\nFuture trends and prinCiples oF suCCess 223", "example, they can control traffic-light sequences in real\ntime, in some cases to give priority to public-transport\nvehicles. Data on city transport networks are also useful\nfor planning public transport. Cities are examining the\nroutes, schedules, and vehicle management to ensure that\nservices support the maximum number of people and to\nreduce the costs associated with delivering the transport\nservices. In addition to modeling the public network, data\nscience is also being used to monitor official city vehicles to\nensure their optimal usage. Such projects combine traffic\nconditions (collected by sensors along the road network,\nat traffic lights, etc.), the type of task being performed,\nand other conditions to optimize route planning, and dy-\nnamic route adjustments are fed to the vehicles with live\nupdates and changes to their routes.\nBeyond energy usage and transport, data science is be-\ning used to improve the provision of utility services and\nto implement longer-term planning of infrastructure proj-\nects. The efficient provision of utility services is constantly\nbeing monitored based on current usage and projected\nusages, and the monitoring takes into account previous\nusage in similar conditions. Utility companies are using\ndata science in a number of ways. One way is monitoring\nthe delivery network for the utility: the supply, the qual-\nity of the supply, any network issues, areas that require\nhigher-than-expected usage, automated rerouting of the\nsupply, and any anomalies in the network. Another way\n224 Chapter 7", "that utility companies are using data science is in monitor-\ning their customers. They are looking for unusual usage\nthat might indicate some criminality (for example, a grow\nhouse), customers who may have altered the equipment\nand meters for the building where they live, and custom-\ners who are most likely to default on their payments. Data\nscience is also being used in examining the best way to al-\nlocate housing and associated services in city planning.\nModels of population growth are built to forecast into the\nfuture, and based on various simulations the city planners\ncan estimate when and where certain support services,\nsuch as high schools, are needed.\nData Science Project Principles: Why Projects Succeed\nor Fail\nA data science project sometimes fails insofar as it doesn\u2019t\ndeliver what was hoped for because it gets bogged down\nin some technical or political issues, does not deliver use-\nful results, and, more typically, is run once (or a couple\nof times) but never run again. Just like Leo Tolstoy\u2019s\nhappy families,3 the success of a data science project is de-\npendent on a number of factors. Successful data science\nprojects need focus, good-quality data, the right people,\nthe willingness to experiment with multiple models, in-\ntegration into the business information technology (IT)\nFuture trends and prinCiples oF suCCess 225", "architecture and processes, buy-in from senior manage-\nment, and an organization\u2019s recognition that because the\nworld changes, models go out of date and need to be re-\nbuilt semiregularly. Failure in any of these areas is likely to\nresult in a failed project. This section details the common\nfactors that determine the success of data science projects\nas well as the typical reasons why data science projects fail.\nFocus\nEvery successful data science project begins by clearly\ndefining the problem that the project will help solve. In\nmany ways, this step is just common sense: it is difficult\nfor a project to be successful unless it has a clear goal.\nHaving a well-defined goal informs the decisions regard-\ning which data to use, what ML algorithms to use, how to\nevaluate the results, how the analysis and models will be\nused and deployed, and when the optimal time might be\nto go through the process again to update the analysis and\nmodels.\nData\nA well-defined question can be used to define what data\nare needed for the project. Having a clear understanding of\nwhat data are needed helps to direct the project to where\nthese required data are located. It also helps with defining\nwhat data are currently unavailable and hence identifies\nsome additional projects that can look at capturing and\n226 Chapter 7", "Every successful data\nscience project begins\nby clearly defining\nthe problem that the\nproject will help solve.", "making available these data. It is important, however, to\nensure that the data used are good-quality data. Organi-\nzations may have applications that are poorly designed, a\nvery poor data model, and staff who are not trained cor-\nrectly to ensure that good data get entered. In fact, myriad\nfactors can lead to bad-quality data in systems. Indeed, the\nneed for good-quality data is so important that some orga-\nnizations have hired people to constantly inspect the data,\nassess the quality of the data, and then feed back ideas on\nhow to improve the quality of the data captured by the\napplications and by the people inputting the data. With-\nout good-quality data, it is very difficult for a data science\nproject to succeed.\nWhen the required data are sourced, it is always im-\nportant to check what data are being captured and used\nacross an organization. Unfortunately, the approach to\nsourcing data taken by some data science projects is to\nlook at what data are available in the transactional data-\nbases (and other data sources) and then to integrate and\nclean these data before going on to data exploration and\nanalysis. This approach completely ignores the BI team\nand any data warehouse that might exist. In many or-\nganizations, the BI and data-warehouse team is already\ngathering, cleaning, transforming, and integrating the\norganization\u2019s data into one central repository. If a data\nwarehouse already exists, then it probably contains all\nor most of the data required by a project. Therefore, a\ndata warehouse can save a significant amount of time on\n228 Chapter 7", "integrating and cleaning the data. It will also have much\nmore data than the current transactional databases con-\ntain. If the data warehouse is used, it is possible to go back\na number of years, build predictive models using the his-\ntoric data, roll these models through various time periods,\nand then measure each model\u2019s level of predictive accu-\nracy. This process allows for the monitoring of changes in\nthe data and how they affect the models. In addition, it\nis possible to monitor variations in the models that are\nproduced by ML algorithms and how the models evolve\nover time. Following this kind of approach facilitates the\ndemonstration of how the models work and behave over\na number of years and helps with building up the cus-\ntomer\u2019s confidence in what is being done and what can\nbe achieved. For example, in one project where five years\nof historical data were available in the data warehouse, it\nwas possible to demonstrate that the company could have\nsaved US$40 million or more over that time period. If the\ndata warehouse had not been available or used, then it\nwould not have been possible to demonstrate this conclu-\nsion. Finally, when a project is using personal data it is\nessential to ensure that the use of this data is in line with\nthe relevant antidiscrimination and privacy regulations.\nPeople\nA successful data science project often involves a team\nof people with a blend of data science competencies and\nskills. In most organizations, a variety of people in existing\nFuture trends and prinCiples oF suCCess 229", "A successful data science\nproject often involves a\nteam of people with a\nblend of data science\ncompetencies and skills.", "roles can and should contribute to data science projects:\npeople working with databases, people who work with the\nETL process, people who perform data integration, proj-\nect managers, business analysts, domain experts, and so\non. But organizations often still need to hire data science\nspecialists\u2014that is, people with the skills to work with big\ndata, to apply ML, and to frame real-world problems in\nterms of data-driven solutions. Successful data scientists\nare willing and able to work and communicate with the\nmanagement team, end users, and all involved to show\nand explain what and how data science can support their\nwork. It is difficult to find people who have both the re-\nquired technical skill set and the ability to communicate\nand work with people across an organization. However,\nthis blend is crucial to the success of data science projects\nin most organizations.\nModels\nIt is import to experiment with a variety of ML algorithms\nto discover which works best with the data sets. All too\noften in the literature, examples are given of cases where\nonly one ML algorithm was used. Maybe the authors are\ndiscussing the algorithm that worked best for them or that\nis their favorite. Currently there is a great deal of inter-\nest in the use of neural networks and deep learning. Many\nother algorithms can be used, however, and these alterna-\ntives should be considered and tested. Furthermore, for\nFuture trends and prinCiples oF suCCess 231", "data science projects based in the EU, the General Data\nProtection Regulations, which go into effect in April\n2018, may become a factor in determining the selection\nof algorithms and model. A potential side effect of these\nregulations is that an individual\u2019s \u201cright to explanation\u201d in\nrelation to automated decision processes that affect them\nmay limit the use in some domains of complex models that\nare difficult to interpret and explain (such as deep neural\nnetwork models).\nIntegration with the Business\nWhen the goal of a data science project is being defined,\nit is vital also to define how the outputs and results of\nthe project will be deployed within the organization\u2019s IT\narchitecture and business processes. Doing so involves\nidentifying where and how the model is to be integrated\nwithin existing systems and how the generated results will\nbe used by the system end users or if the results will be fed\ninto another process. The more automated this process is,\nthe quicker the organization can respond to its customers\u2019\nchanging profile, thereby reducing costs and increasing\npotential profits. For example, if a customer-risk model is\nbuilt for the loan process in a bank, it should be built into\nthe front-end system that captures the loan application\nby the customer. That way, when the bank employee is en-\ntering the loan application, she can be given live feedback\nby the model. The employee can then use this live feedback\n232 Chapter 7", "to address any issues with the customer. Another example\nis fraud detection. It can take four to six weeks to identify\na potential fraud case that needs investigation. By using\ndata science and building it into transaction-monitoring\nsystems, organizations can now detect potential fraud\ncases in near real time. By automating and integrating\ndata-driven models, quicker response times are achieved,\nand actions can be taken at the right time. If the outputs\nand models created by a project are not integrated into the\nbusiness processes, then these outputs will not be used,\nand, ultimately, the project will fail.\nBuy-in\nFor most projects in most organizations, support by se-\nnior management is crucial to the success of many data\nscience projects. However, most senior IT managers are\nvery focused on the here and now: keeping the lights on,\nmaking sure their day-to-day applications are up and run-\nning, making sure the backups and recovery processes\nare in place (and tested), and so on. Successful data sci-\nence projects are sponsored by senior business manag-\ners (rather than by an IT manager) because the former\nare focused not on the technology but on the processes\ninvolved in the data science project and how the outputs\nof the data science project can be used to the organiza-\ntion\u2019s advantage. The more focused a project sponsor is\non these factors, the more successful the project will be.\nFuture trends and prinCiples oF suCCess 233", "For an organization to\nreap long-term benefits,\nit needs to build its\ncapacity to execute data\nscience projects often\nand to use the outputs\nof these projects.", "He or she will then act as the key to informing the rest\nof the organization about the project and selling it to\nthem. But even when data science has a senior manager\nas an internal champion, a data science strategy can still\nfail in the long term if the initial data science project is\ntreated as a box-ticking exercise. The organization should\nnot view data science as a one-off project. For an orga-\nnization to reap long-term benefits, it needs to build its\ncapacity to execute data science projects often and to use\nthe outputs of these projects. It takes long-term commit-\nment from senior management to view data science as\na strategy.\nIteration\nMost data science projects will need to be updated and\nrefreshed on a semiregular basis. For each new update\nor iteration, new data can be added, new updates can be\nadded, maybe new algorithms can be used, and so on. The\nfrequency of these iterations will vary from project to proj-\nect; it could be daily or quarterly or biannually or annually.\nChecks should be built into the productionalized data sci-\nence outputs to detect when models need updating (see\nKelleher, Mac Namee, and D\u2019Arcy 2015 for an explanation\nof how to use a stability index to identify when a model\nshould be updated).\nFuture trends and prinCiples oF suCCess 235", "Final Thoughts\nHumans have always abstracted from the world and tried\nto understand it by identifying patterns in their experi-\nences of it. Data science is the latest incarnation of this\npattern-seeking behavior. However, although data science\nhas a long history, the breadth of its impact on modern life\nis without precedent. In modern societies, the words preci-\nsion, smart, targeted, and personalized are often indicative\nof data science projects: precision medicine, precision polic-\ning, precision agriculture, smart cities, smart transport, tar-\ngeted advertising, personalized entertainment. The common\nfactor across all these areas of human life is that decisions\nhave to be made: What treatment should we use for this\npatient? Where should we allocate our policing resources?\nHow much fertilizer should we spread? How many high\nschools do we need to build in the next four years? Who\nshould we send this advertisement to? What movie or\nbook should we recommend to this person? The power\nof data science to help with decision making is driving its\nadoption. Done well, data science can provide actionable\ninsight that leads to better decisions and ultimately better\noutcomes.\nData science, in its modern guise, is driven by big data,\ncomputer power, and human ingenuity from a number of\nfields of scientific endeavor (from data mining and data-\nbase research to machine learning). This book has tried to\n236 Chapter 7", "provide an overview of the fundamental ideas and concepts\nrequired to understand data science. The CRISP-DM proj-\nect life cycle makes the data science process explicit and\nprovides a structure for the data science journey from data\nto wisdom: understand the problem, prepare the data, use\nML to extract patterns and create models, use the models\nto get actionable insight. The book also touches on some\nof the ethical concerns relating to individual privacy in a\ndata science world. People have genuine and well-founded\nconcerns that data science has the potential to be used by\ngovernments and vested interests to manipulate our be-\nhaviors and police our actions. We, as individuals, need\nto develop informed opinions about what type of a data\nworld we want to live in and to think about the laws we\nwant our societies to develop in order to steer the use of\ndata science in appropriate directions. Despite the ethical\nconcerns we may have around data science, the genie is\nalready very much out of the bottle: data science is hav-\ning and will continue to have significant effects on our\ndaily lives. When used appropriately, it has the potential\nto improve our lives. But if we want the organizations we\nwork with, the communities we live in, and the families\nwe share our lives with to benefit from data science, we\nneed to understand and explore what data science is, how\nit works, and what it can (and can\u2019t) do. We hope this book\nhas given you the essential foundations you need to go on\nthis journey.\nFuture trends and prinCiples oF suCCess 237", "", "GLOSSARY\nAnalytics Base Table\nA table in which each row contains the data relating to a specific instance\nand each column describes the values of a particular attribute for each in-\nstance. These data are the basic input to data-mining and machine-learning\nalgorithms.\nAnomaly Detection\nSearching for and identifying examples of atypical data in a data set. These\nnonconforming cases are often referred to as anomalies or outliers. This process\nis often used in analyzing financial transactions to identify potential fraudu-\nlent activities and to trigger investigations.\nAssociation-Rule Mining\nAn unsupervised data-analysis technique that looks to find groups of items\nthat frequently co-occur together. The classic use case is market-basket analy-\nsis, where retail companies try to identify sets of items that are purchased\ntogether, such as the hot dogs, ketchup, and beer.\nAttribute\nEach instance in a data set is described by a number of attributes (also known\nas features or variables). An attribute captures one piece of information relating\nto an instance. An attribute can be either raw or derived.\nBackpropagation\nThe backpropagation algorithm is an ML algorithm used to train neural net-\nworks. The algorithm calculates for each neuron in a network the contribution\nthe neuron makes to the error of the network. Using this error calculation for\neach neuron it is possible to update the weights on the inputs to each neuron\nso as to reduce the overall error of the network. The backpropagation algo-\nrithm is so named because it works in a two stage process. In the first stage an\ninstance is input to the network and the information flows forward through\nthe network until the network generates a prediction for that instance. In\nthe second stage the error of the network on that instance is calculated by\ncomparing the network's prediction to the correct output for that instance", "(as specified by the training data) and then this error is then shared back (or\nbackpropagated) through the neurons in the network on a layer by layer basis\nbeginning at the output layer.\nBig Data\nBig data are often defined in terms of the three Vs: the extreme volume of\ndata, the variety of the data types, and the velocity at which the data must\nbe processed.\nCaptured Data\nData that are captured through a direct measurement process that is designed\nto gather the data. Contrast with exhaust data.\nClassification\nThe task of predicting a value for a target attribute of an instance based on\nthe values of a set of input attributes, where the target attribute is a nominal\nor ordinal data type.\nClustering\nIdentifying groups of similar instances in a data set.\nCorrelation\nThe strength of association between two attributes.\nCross Industry Standard Process for Data Mining (CRISP-DM)\nThe CRISP-DM defines a standard life cycle for a data-mining project. The life\ncycle is often adopted for data science projects.\nData In its most basic form, a piece of data is an abstraction (or measure-\nment) from a real-world entity (person, object, or event).\nData Analysis\nAny process for extracting useful information from data. Types of data\nanalysis include data visualization, summary statistics, correlation analysis,\nand modeling using machine learning.\nDatabase\nA central repository of data. The most common database structure is a rela-\ntional database, which stores data in tables with a structure of one row per\n240 Glossary", "instance and one column per attribute. This representation is ideal for storing\ndata with a clear structure that can be decomposed into natural attributes.\nData Mining\nThe process of extracting useful patterns from a data set to solve a well-defined\nproblem. CRISP-DM defines the standard life cycle for a data-mining project.\nClosely related to data science but in general not as broad in scope.\nData Science\nAn emerging field that integrates a set of problem definitions, algorithms, and\nprocesses that can be used to analyze data so as to extract actionable insight\nfrom (large) data sets. Closely related to the field of data mining but broader\nin scope and concern. Deals with both structured and unstructured (big) data\nand encompasses principles from a range of fields, including machine learn-\ning, statistics, data ethics and regulation, and high-performance computing.\nData Set\nA collection of data relating to a set of instances, with each instance described\nin terms of a set of attributes. In its most basic form, a data set is organized in\nan n * m matrix, where n is the number of instances (rows) and m is the number\nof attributes (columns).\nData Warehouse\nA centralized repository containing data from a range of sources across an or-\nganization. The data are structured to support summary reports from the ag-\ngregated data. Online analytical processing (OLAP) is the term used to describe\nthe typical operations on a data warehouse.\nDecision Tree\nA type of prediction model that encodes if-then-else rules in a tree structure.\nEach node in the tree defines one attribute to test, and a path from the root\nnode to a terminating leaf node defines a sequence of tests that an instance\nmust pass for the label of the terminating node to be predicted for that\ninstance.\nDeep Learning\nA deep-learning model is a neural network that has multiple (more than two)\nlayers of hidden units (or neurons). Deep networks are deep in terms of the\nnumber of layers of neurons in the network. Today many deep networks have\nGlossary 241", "tens to hundreds of layers. The power of deep-learning models comes from the\nability of the neurons in the later layers to learn useful attributes derived from\nattributes that were themselves learned by the neurons in the earlier layers.\nDerived Attribute\nAn attribute whose value is generated by applying a function to other data\nrather than a direct measurement taken from the entity. An attribute that\ndescribes an average value in a population is an example of a derived attribute.\nContrast with raw attribute.\nDIKW Pyramid\nA model of the structural relationships between data, information, knowledge,\nand wisdom. In the DIKW pyramid, data precedes information, which precedes\nknowledge, which precedes wisdom.\nExhaust Data\nData that are a by-product of a process whose primary purpose is some-\nthing other than data capture. For example, for every image shared, tweeted,\nretweeted, or liked, a range of exhaust data is generated: who shared, who\nviewed, what device was used, what time of day, and so on. Contrast with\ncaptured data.\nExtraction, Transformation, and Load (ETL)\nETL is the term used to describe the typical processes and tools used to support\nthe mapping, merging, and movement of data between databases.\nHadoop\nHadoop is an open-source framework developed by the Apache Software Foun-\ndation that is designed for the processing of big data. It uses distributed stor-\nage and processing across clusters of commodity hardware.\nHigh-Performance Computing (HPC)\nThe field of HPC focuses on designing and implementing frameworks to con-\nnect large number of computers together so that the resulting computer clus-\nter can store and process large amounts of data efficiently.\nIn-Database Machine Learning\nUsing machine-learning algorithms that are built into the database solution.\nThe benefit of in-database machine learning is that it reduces the time spent\non moving data in and out of databases for analysis.\n242 Glossary", "Instance\nEach row in a data set contains the information relating to one instance (also\nknown as an example, entity, case, or record).\nInternet of Things\nThe internetworking of physical devices and sensors so that these devices can\nshare information. Includes the field of machine-to-machine communication,\nwhich develops systems that enable machines not only to share informa-\ntion but also to react to this information and trigger actions without human\ninvolvement.\nLinear Regression\nWhen a linear relationship is assumed in a regression analysis, the analysis is\ncalled linear regression. A popular type of prediction model used to estimate the\nvalue of a numeric target attribute based on a set of numeric input attributes.\nMachine Learning (ML)\nThe field of computer science research that focuses on developing and evalu-\nating algorithms that can extract useful patterns from data sets. A machine-\nlearning algorithm takes a data set as input and returns a model that encodes\nthe patterns the algorithm extracted from the data.\nMassively Parallel Processing Database (MPP)\nIn an MPP database, data is partitioned across multiple servers, and each\nserver can process the data on that server locally and independently.\nMetadata\nData describing the structures and properties of other data\u2014for example, a\ntime stamp that describes when a piece of data was collected. Metadata are one\nof the most common types of exhaust data.\nModel\nIn the context of machine learning, a model is a representation of a pattern\nextracted using machine learning from a data set. Consequently, models are\ntrained, fitted to a data set, or created by running a machine learning algo-\nrithm on a data set. Popular model representations include decision trees and\nneural networks. A prediction model defines a mapping (or function) from a\nset of input attributes to a value for a target attribute. Once a model has been\ncreated, it can then be applied to new instances from the domain. For example,\nGlossary 243", "in order to train a spam filter model, we would apply a machine learning al-\ngorithm to a data set of historic emails that have been labeled as spam or not\nspam. Once the model has been trained it can be used to label (or filter) new\nemails that were not in the original data set.\nNeural Network\nA type of machine-learning model that is implemented as a network of simple\nprocessing units called neurons. It is possible to create a variety of different\ntypes of neural networks by modifying the topology of the neurons in the\nnetwork. A feed-forward, fully connected neural network is a very common\ntype of network that can be trained using backpropagation.\nNeuron\nA neuron takes a number of input values (or activations) as input and\nmaps these values to a single output activation. This mapping is typically\nimplemented by applying a multi-input linear-regression function to the\ninputs and then pushing the result of this regression function through a\nnonlinear activation function, such as the logistic or tanh function.\nOnline Analytical Processing (OLAP)\nOLAP operations generate summaries of historic data and aggregate data from\nmultiple sources. OLAP operations are designed to generate report-type sum-\nmaries and enable users to slice, dice, and pivot data in a data warehouse using\na predefined set of dimensions on the data, such as sales by stores, sale by\nquarter, and so on. Contrast with Online Transaction Processing (OLTP).\nOnline Transaction Processing (OLTP)\nOLTPs are designed for short online data transactions (such as INSERT, DE-\nLETE, UPDATE, etc.) with an emphasis on fast query processing and main-\ntaining data integrity in a multi-access environment. Contrast with OLAP\nsystems, which are designed for more complex operations on historic data.\nOperational Data Store (ODS)\nAn ODS system integrates operational or transactional data from multiple sys-\ntems to support operational reporting.\nPrediction\nIn the context of data science and machine learning, the task of estimating\nthe value of a target attribute for a given instance based on the values of other\nattributes (or input attributes) for that instance.\n244 Glossary", "Raw Attribute\nAn abstraction from an entity that is a direct measurement taken from the\nentity\u2014for example, a person\u2019s height. Contrast with derived attribute.\nRegression Analysis\nEstimates the expected (or average) value of a numeric target attribute when\nall the input attribute values are fixed. Regression analysis assumes a param-\neterized mathematical model of the hypothesized relationship between the\ninputs and output known as a regression function. A regression function may\nhave multiple parameters, and the focus of regression analysis is to find the\ncorrect settings for these parameters.\nRelational Database Management System (RDBMS)\nDatabase management systems based on Edgar F. Codd\u2019s relational data model.\nRelational databases store data in collection of tables where each table has a\nstructure of one row per instance and one column per attribute. Links between\ntables can be created by having key attributes appear in multiple tables. This\nstructure is suited for SQL queries which define operations on the data in the\ntables.\nSmart City\nSmart-city projects generally try to integrate real-time data from many differ-\nent data sources into a single data hub, where they are analyzed and used to\ninform city-management and planning decisions.\nStructured Data\nData that can be stored in a table. Every instance in the table has the same set\nof attributes. Contrast with unstructured data.\nStructured Query Language (SQL)\nAn international standard for defining database queries.\nSupervised Learning\nA form of machine learning in which the goal is to learn a function that maps\nfrom a set of input attribute values for an instance to an estimate of the miss-\ning value for the target attribute of the same instance.\nTarget Attribute\nIn a prediction task, the attribute that the prediction model is trained to es-\ntimate the value of.\nGlossary 245", "Transactional Data\nEvent information, such as the sale of an item, the issuing of an invoice, the\ndelivery of goods, credit card payment, and so on.\nUnstructured Data\nA type of data where each instance in the data set may have its own internal\nstructure; that is, the structure is not necessarily the same in every instance.\nFor example, text data are often unstructured and require a sequence of op-\nerations to be applied to them in order to extract a structured representation\nfor each instance.\nUnsupervised Learning\nA form of machine learning in which the goal is to identify regularities in the\ndata. These regularities may include clusters of similar instances within the\ndata or regularities between attributes. In contrast to supervised learning, in\nunsupervised learning no target attribute is defined in the data set.\n246 Glossary", "NOTES\nChapter 1\n1. Quote taken from the call for participation sent out for the KDD workshop\nin 1989.\n2. Some practitioners do distinguish between data mining and KDD by\nviewing data mining as a subfield of KDD or a particular approach to KDD.\n3. For a recent review of this debate, see Battle of the Data Science Venn\nDiagrams (Taylor 2016).\n4. For more on the Cancer Moonshot Initiative, see https://www.cancer.gov/\nresearch/key-initiatives.\n5. For more on the All of Us program in the Precision Medicine Initiative, see\nhttps://allofus.nih.gov.\n6. For more on the Police Data Initiative, see https://www.policedatainitiative\n.org.\n7. For more on AlphaGo, see https://deepmind.com/research/alphago.\nChapter 2\n1. Although many data sets can be described as a flat n * m matrix, in some\nscenarios the data set is more complex: for example, if a data set describes\nthe evolution of multiple attributes through time, then each time point\nin the data set will be represented by a two-dimensional flat n * m matrix,\nlisting the state of the attributes at that point in time, but the overall data\nset will be three dimensional, where time is used to link the two-dimensional\nsnapshots. In these contexts, the term tensor is sometimes used to generalize\nthe matrix concept to higher dimensions.\n2. This example is inspired by an example in Han, Kamber, and Pei 2011.\nChapter 3\n1. See Storm website, at http://storm.apache.org.\nChapter 4\n1. This subheading, Correlations Are Not Causations, but Some Are Useful, is\ninspired by George E. P. Box\u2019s (1979) observation, \u201cEssentially, all models are\nwrong, but some are useful.\u201d\n2. For a numeric target, the average is the most common measure of central\ntendency, and for nominal or ordinal data the mode (or most frequently occur-\nring value is the most common measure of central tendency).", "3. We are using a more complex notation here involving \u03c9 and \u03c9 because a\n0 1\nfew paragraphs later we expand this function to include more than one input\nattribute, so the subscripted variables are useful notations when dealing with\nmultiple inputs.\n4. A note of caution: the numeric values reported here should be taken as\nillustrative only and not interpreted as definitive estimates of the relationship\nbetween BMI and likelihood of diabetes.\n5. In general, neural networks work best when the inputs have similar ranges.\nIf there are large differences in the ranges of input attributes, the attributes\nwith the much larger values tend to dominate the processing of the network.\nTo avoid this, it is best to normalize the input attributes so that they all have\nsimilar ranges.\n6. For the sake of simplicity, we have not included the weights on the connec-\ntions in figures 14 and 15.\n7. Technically, the backpropagation algorithm uses the chain rule from calcu-\nlus to calculate the derivative of the error of the network with respect to each\nweight for each neuron in the network, but for this discussion we will pass over\nthis distinction between the error and the derivative of the error for the sake of\nclarity in explaining the essential idea behind the backpropagation algorithm.\n8. No agreed minimum number of hidden layers is required for a network to\nbe considered \u201cdeep,\u201d but some people would argue that even two layers are\nenough to be deep. Many deep networks have tens of layers, but some net-\nworks can have hundreds or even thousands of layers.\n9. For an accessible introduction to RNNs and their natural-language process-\ning, see Kelleher 2016.\n10. Technically, the decrease in error estimates is known as the vanishing-\ngradient problem because the gradient over the error surface disappears as the\nalgorithm works back through the network.\n11. The algorithm also terminates on two corner cases: a branch ends up with\nno instances after the data set is split up, or all the input attributes have al-\nready been used at nodes between the root node and the branch. In both cases,\na terminating node is added and is labeled with the majority value of the target\nattribute at the parent node of the branch.\n12. For an introduction to entropy and its use in decision-tree algorithms, see\nKelleher, Mac Namee, and D\u2019Arcy 2015 on information-based learning.\n13. See Burt 2017 for an introduction to the debate on the \u201cright to\nexplanation.\u201d\n248 Notes to Chapter 4", "Chapter 5\n1. A customer-churn case study in Kelleher, Mac Namee, and D\u2019Arcy 2015\nprovides a longer discussion of the design of attributes in propensity\nmodels.\nChapter 6\n1. Behavioral targeting uses data from users\u2019 online activities\u2014sites visited,\nclicks made, time spent on a site, and so on\u2014and predictive modeling to select\nthe ads shown to the user.\n2. The EU Privacy and Electronic Communications Directive (2002/58/EC).\n3. For example, some expectant women explicitly tell retailers that they\nare pregnant by registering for promotional new-mother programs at the\nstores.\n4. For more on PredPol, see http://www.predpol.com.\n5. A Panopticon is an eighteenth-century design by Jeremy Bentham for in-\nstitutional buildings, such as prisons and psychiatric hospitals. The defining\ncharacteristic of a Panopticon was that the staff could observe the inmates\nwithout the inmates\u2019 knowledge. The underlying idea of this design was that\nthe inmates were forced to act as though they were being watched at all times.\n6. As distinct from digital footprint.\n7. Civil Rights Act of 1964, Pub. L. 88-352, 78 Stat. 241, at https://www.gpo\n.gov/fdsys/pkg/STATUTE-78/pdf/STATUTE-78-Pg241.pdf.\n8. Americans with Disabilities Act of 1990, Pub. L. 101-336, 104 Stat. 327,\nat https://www.gpo.gov/fdsys/pkg/STATUTE-104/pdf/STATUTE-104-Pg327\n.pdf.\n9. The Fair Information Practice Principles are available at https://www.dhs\n.gov/publication/fair-information-practice-principles-fipps.\n10. Senate of California, SB-568 Privacy: Internet: Minors, Business and\nProfessions Code, Relating to the Internet, vol. division 8, chap. 22.1 (com-\nmencing with sec. 22580) (2013), at https://leginfo.legislature.ca.gov/faces/\nbillNavClient.xhtml?bill_id=201320140SB568.\nChapter 7\n1. For more on the SmartSantander project in Spain, see http://\nsmartsantander.eu.\n2. For more on the TEPC\u2019s projects, see http://www.tepco.co.jp/en/press/\ncorp-com/release/2015/1254972_6844.html.\nNotes to Chapters 6\u20137 249", "3. Leo Tolstoy\u2019s book Anna Karenina (1877) begins: \u201cAll happy families are\nalike; each unhappy family is unhappy in its own way.\u201d Tolstoy\u2019s idea is that\nto be happy, a family must be successful in a range of areas (love, finance,\nhealth, in-laws), but failure in any of these areas will result in unhappiness.\nSo all happy families are the same because they are successful in all areas,\nbut unhappy families can be unhappy for many different combinations of\nreasons.\n250 Notes to Chapter 7", "FURTHER READINGS\nAbout Data and Big Data\nDavenport, Thomas H. Big Data at Work: Dispelling the Myths, Uncovering the\nOpportunities. Cambridge, MA: Harvard Business Review, 2014.\nHarkness, Timandra. Big Data: Does Size Matter? New York: Bloomsbury Sigma,\n2016.\nKitchin, Rob. The Data Revolution: Big Data, Open Data, Data Infrastructures, and\nTheir Consequences. Los Angeles: Sage, 2014.\nMayer-Sch\u00f6nberger, Viktor, and Kenneth Cukier. Big Data: A Revolution That\nWill Transform How We Live, Work, and Think. Boston: Eamon Dolan/Mariner\nBooks, 2014.\nPomerantz, Jeffrey. Metadata. Cambridge, MA: MIT Press, 2015.\nRudder, Christian. Dataclysm: Who We Are (When We Think No One\u2019s Looking).\nNew York: Broadway Books, 2014.\nAbout Data Science, Data Mining, and Machine Learning\nKelleher, John D., Brian Mac Namee, and Aoife D\u2019Arcy. Fundamentals\nof Machine Learning for Predictive Data Analytics. Cambridge, MA: MIT Press,\n2015.\nLinoff, Gordon S., and Michael J. A. Berry. Data Mining Techniques: For Mar-\nketing, Sales, and Customer Relationship Management. Indianapolis, IN: Wiley,\n2011.\nProvost, Foster, and Tom Fawcett. Data Science for Business: What You Need to\nKnow about Data Mining and Data-Analytic Thinking. Sebastopol, CA: O\u2019Reilly\nMedia, 2013.\nAbout Privacy, Ethics, and Advertising\nDwork, Cynthia, and Aaron Roth. 2014. \u201cThe Algorithmic Foundations of Dif-\nferential Privacy.\u201d Foundations and Trends\u00ae in Theoretical Computer Science 9\n(3\u20134): 211\u2013407.", "Nissenbaum, Helen. Privacy in Context: Technology, Policy, and the Integrity of\nSocial Life. Stanford, CA: Stanford Law Books, 2009.\nSolove, Daniel J. Nothing to Hide: The False Tradeoff between Privacy and Security.\nNew Haven, CT: Yale University Press, 2013.\nTurow, Joseph. The Daily You: How the New Advertising Industry Is Defining Your\nIdentity and Your Worth. New Haven, CT: Yale University Press, 2013.\n252 Further readings", "REFERENCES\nAnderson, Chris. 2008. The Long Tail: Why the Future of Business Is Selling Less\nof More. Rev. ed. New York: Hachette Books.\nBaldridge, Jason. 2015. \u201cMachine Learning and Human Bias: An Uneasy Pair.\u201d\nTechCrunch, August 2. http://social.techcrunch.com/2015/08/02/machine\n-learning-and-human-bias-an-uneasy-pair.\nBarry-Jester, Anna Maria, Ben Casselman, and Dana Goldstein. 2015.\n\u201cShould Prison Sentences Be Based on Crimes That Haven\u2019t Been Commit-\nted Yet?\u201d FiveThirtyEight, August 4. https://fivethirtyeight.com/features/\nprison-reform-risk-assessment.\nBatty, Mike, Arun Tripathi, Alice Kroll, Peter Wu Cheng-sheng, David Moore,\nChris Stehno, Lucas Lau, Jim Guszcza, and Mitch Katcher. 2010. \u201cPredictive\nModeling for Life Insurance: Ways Life Insurers Can Participate in the Busi-\nness Analytics Revolution.\u201d Society of Actuaries. https://www.soa.org/files/\npdf/research-pred-mod-life-batty.pdf.\nBeales, Howard. 2010. \u201cThe Value of Behavioral Targeting.\u201d Network\nAdvertising Initiative. http://www.networkadvertising.org/pdfs/Beales_NA\nI_Study.pdf.\nBerk, Richard A., and Justin Bleich. 2013. \u201cStatistical Procedures for Forecast-\ning Criminal Behavior.\u201d Criminology & Public Policy 12 (3): 513\u2013544.\nBox, George E. P. 1979. \u201cRobustness in the Strategy of Scientific Model Build-\ning,\u201d in Robustness in Statistics, ed. R. L. Launer and G. N. Wilkinson, 201\u2013236.\nNew York: Academic Press.\nBreiman, Leo. 2001. \u201cStatistical Modeling: The Two Cultures (with Com-\nments and a Rejoinder by the Author).\u201d Statistical Science 16 (3): 199\u2013231.\ndoi:10.1214/ss/1009213726.\nBrown, Meta S. 2014. Data Mining for Dummies. New York: Wiley. http://\nwww.wiley.com/WileyCDA/WileyTitle/productCd-1118893174,subjectCd\n-STB0.html.\nBrynjolfsson, Erik, Lorin M. Hitt, and Heekyung Hellen Kim. 2011. \u201cStrength in\nNumbers: How Does Data-Driven Decisionmaking Affect Firm Performance?\u201d", "SSRN Scholarly Paper ID 1819486. Social Science Research Network, Roches-\nter, NY. https://papers.ssrn.com/abstract=1819486.\nBurt, Andrew. 2017. \u201cIs There a \u2018Right to Explanation\u2019 for Machine Learning\nin the GDPR?\u201d https://iapp.org/news/a/is-there-a-right-to-explanation-for\n-machine-learning-in-the-gdpr.\nBuytendijk, Frank, and Jay Heiser. 2013. \u201cConfronting the Privacy and Ethical\nRisks of Big Data.\u201d Financial Times, September 24. https://www.ft.com/content/\n105e30a4-2549-11e3-b349-00144feab7de.\nCarroll, Rory. 2013. \u201cWelcome to Utah, the NSA\u2019s Desert Home for Eaves-\ndropping on America.\u201d Guardian, June 14. https://www.theguardian.com/\nworld/2013/jun/14/nsa-utah-data-facility.\nCavoukian, Ann. 2013. \u201cPrivacy by Design: The 7 Foundation Principles\n(Primer).\u201d Information and Privacy Commissioner, Ontario, Canada. https://\nwww.ipc.on.ca/wp-content/uploads/2013/09/pbd-primer.pdf.\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas\nReinartz, Colin Shearer, and Rudiger Wirth. 1999. \u201cCRISP-DM 1.0: Step-by-\nStep Data Mining Guide.\u201d ftp://ftp.software.ibm.com/software/analytics/\nspss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf.\nCharter of Fundamental Rights of the European Union. 2000. Official Journal\nof the European Communities C (364): 1\u201322.\nCleveland, William S. 2001. \u201cData Science: An Action Plan for Expanding the\nTechnical Areas of the Field of Statistics.\u201d International Statistical Review 69 (1):\n21\u201326. doi:10.1111/j.1751-5823.2001.tb00477.x.\nClifford, Stephanie. 2012. \u201cSupermarkets Try Customizing Prices for\nShoppers.\u201d New York Times, August 9. http://www.nytimes.com/2012/08/10/\nbusiness/supermarkets-try-customizing-prices-for-shoppers.html.\nCouncil of the European Union and European Parliament. 1995. \u201c95/46/EC\nof the European Parliament and of the Council of 24 October 1995 on the Pro-\ntection of Individuals with Regard to the Processing of Personal Data and on\nthe Free Movement of Such Data.\u201d Official Journal of the European Community\nL 281:38-1995): 31\u201350.\nCouncil of the European Union and European Parliament. 2016. \u201cGeneral\nData Protection Regulation of the European Council and Parliament.\u201d Official\n254 RefeRences", "Journal of the European Union L 119: 1\u20132016. http://ec.europa.eu/justice/\ndata-protection/reform/files/regulation_oj_en.pdf.\nCrowdFlower. 2016. 2016 Data Science Report. http://visit.crowdflower.com/\nrs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf.\nDatta, Amit, Michael Carl Tschantz, and Anupam Datta. 2015. \u201cAutomated\nExperiments on Ad Privacy Settings.\u201d Proceedings on Privacy Enhancing\nTechnologies 2015 (1): 92\u2013112.\nDeZyre. 2015. \u201cHow Big Data Analysis Helped Increase Walmart\u2019s Sales Turn-\nover.\u201d May 23. https://www.dezyre.com/article/how-big-data-analysis-helped\n-increase-walmarts-sales-turnover/109.\nDodge, Martin, and Rob Kitchin. 2007. \u201cThe Automatic Management of\nDrivers and Driving Spaces.\u201d Geoforum 38 (2): 264\u2013275.\nDokoupil, Tony. 2013. \u201c\u2018Small World of Murder\u2019: As Homicides Drop,\nChicago Police Focus on Social Networks of Gangs.\u201d NBC News, December 17.\nhttp://www.nbcnews.com/news/other/small-world-murder-homicides-drop\n-chicago-police-focus-social-networks-f2D11758025.\nDuhigg, Charles. 2012. \u201cHow Companies Learn Your Secrets.\u201d New York\nTimes, February 16. http://www.nytimes.com/2012/02/19/magazine/\nshopping-habits.html.\nDwork, Cynthia, and Aaron Roth. 2014. \u201cThe Algorithmic Foundations of\nDifferential Privacy.\u201d Foundations and Trends\u00ae in Theoretical Computer Science\n9 (3\u20134): 211\u2013407.\nEliot, T. S. 1934 [1952]. \u201cChoruses from \u2018The Rock.\u2019\u201d In T. S. Eliot: The Complete\nPoems and Plays\u20141909\u20131950. San Diego: Harcourt, Brace and Co.\nElliott, Christopher. 2004. \u201cBUSINESS TRAVEL; Some Rental Cars Are Keep-\ning Tabs on the Drivers.\u201d New York Times, January 13. http://www.nytimes\n.com/2004/01/13/business/business-travel-some-rental-cars-are-keeping\n-tabs-on-the-drivers.html.\nEurobarometer. 2015. \u201cData Protection.\u201d Special Eurobarometer 431. http://\nec.europa.eu/COMMFrontOffice/publicopinion/index.cfm/Survey/index#p\n=1&instruments=SPECIAL.\nEuropean Commission. 2012. \u201cCommission Proposes a Comprehensive Reform\nof the Data Protection Rules\u2014European Commission.\u201d January 25. http://\nec.europa.eu/justice/newsroom/data-protection/news/120125_en.htm.\nRefeRences 255", "European Commission. 2016. \u201cThe EU-U.S. Privacy Shield.\u201d December 7.\nhttp://ec.europa.eu/justice/data-protection/international-transfers/eu-us\n-privacy-shield/index_en.htm.\nFederal Trade Commission. 2012. Protecting Consumer Privacy in an Era of\nRapid Change. Washington, DC: Federal Trade Commission. https://www\n.ftc.gov/sites/default/files/documents/reports/federal-trade-commission\n-report-protecting-consumer-privacy-era-rapid-change-recommendations/12\n0326privacyreport.pdf.\nFew, Stephen. 2012. Show Me the Numbers: Designing Tables and Graphs to\nEnlighten. 2nd ed. Burlingame, CA: Analytics Press.\nGoldfarb, Avi, and Catherine E. Tucker. 2011. Online Advertising, Behavioral\nTargeting, and Privacy. Communications of the ACM 54 (5): 25\u201327.\nGorner, Jeremy. 2013. \u201cChicago Police Use Heat List as Strategy to Prevent\nViolence.\u201d Chicago Tribune, August 21. http://articles.chicagotribune.com/\n2013-08-21/news/ct-met-heat-list-20130821_1_chicago-police-commander\n-andrew-papachristos-heat-list.\nHall, Mark, Ian Witten, and Eibe Frank. 2011. Data Mining: Practical Machine\nLearning Tools and Techniques. Amsterdam: Morgan Kaufmann.\nHan, Jiawei, Micheline Kamber, and Jian Pei. 2011. Data Mining: Concepts and\nTechniques. 3rd ed. Haryana, India: Morgan Kaufmann.\nHarkness, Timandra. 2016. Big Data: Does Size Matter? New York: Bloomsbury\nSigma.\nHenke, Nicolaus, Jacques Bughin, Michael Chui, James Manyika, Tamim\nSaleh, and Bill Wiseman. 2016. The Age of Analytics: Competing in a Data-\nDriven World. Chicago: McKinsey Global Institute. http://www.mckinsey.com/\nbusiness-functions/mckinsey-analytics/our-insights/the-age-of-analytics\n-competing-in-a-data-driven-world.\nHill, Shawndra, Foster Provost, and Chris Volinsky. 2006. Network-Based\nMarketing: Identifying Likely Adopters via Consumer Networks. Statistical\nScience 21 (2): 256\u2013276. doi:10.1214/088342306000000222.\nHunt, Priscillia, Jessica Saunders, and John S. Hollywood. 2014. Evaluation\nof the Shreveport Predictive Policing Experiment. Santa Monica, CA: Rand Corpo-\nration. http://www.rand.org/pubs/research_reports/RR531.\n256 RefeRences", "Innes, Martin. 2001. Control Creep. Sociological Research Online 6 (3). https://\nideas.repec.org/a/sro/srosro/2001-45-2.html.\nKelleher, John D. 2016. \u201cFundamentals of Machine Learning for Neural\nMachine Translation.\u201d In Proceedings of the European Translation Forum, 1\u201315.\nBrussels: European Commission Directorate-General for Translation. https://\ntinyurl.com/RecurrentNeuralNetworks.\nKelleher, John D., Brian Mac Namee, and Aoife D\u2019Arcy. 2015. Fundamentals\nof Machine Learning for Predictive Data Analytics. Cambridge, MA: MIT Press.\nKerr, Aphra. 2017. Global Games: Production, Circulation, and Policy in the Net-\nworked Era. New York: Routledge.\nKitchin, Rob. 2014a. The Data Revolution: Big Data, Open Data, Data Infrastruc-\ntures, and Their Consequences. Los Angeles: Sage.\nKitchin, Rob. 2014b. \u201cThe Real-Time City? Big Data and Smart Urbanism.\u201d\nGeoJournal 79 (1): 1\u201314. doi:10.1007/s10708-013-9516-8.\nKoops, Bert-Jaap. 2011. \u201cForgetting Footprints, Shunning Shadows: A Criti-\ncal Analysis of the \u2018Right to Be Forgotten\u2019 in Big Data Practice.\u201d Tilburg Law\nSchool Legal Studies Research Paper no. 08/2012. SCRIPTed 8 (3): 229\u201356.\ndoi:10.2139/ssrn.1986719.\nKorzybski, Alfred. 1996. \u201cOn Structure.\u201d In Science and Sanity: An Introduc-\ntion to Non-Aristotelian Systems and General Semantics, CD-ROM, ed. Charlotte\nSchuchardt-Read. Englewood, NJ: Institute of General Semantics. http://\nesgs.free.fr/uk/art/sands.htm.\nKosinski, Michal, David Stillwell, and Thore Graepel. 2013. \u201cPrivate Traits and\nAttributes Are Predictable from Digital Records of Human Behavior.\u201d Proceed-\nings of the National Academy of Sciences of the United States of America 110 (15):\n5802\u20135805. doi:10.1073/pnas.1218772110.\nLe Cun, Yann. 1989. Generalization and Network Design Strategies. Technical\nReport CRG-TR-89\u20134. Toronto: University of Toronto Connectionist Research\nGroup.\nLevitt, Steven D., and Stephen J. Dubner. 2009. Freakonomics: A Rogue Economist\nExplores the Hidden Side of Everything. New York: William Morrow Paperbacks.\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair Game. New York:\nNorton.\nRefeRences 257", "Linoff, Gordon S., and Michael J.A. Berry. 2011. Data Mining Techniques: For\nMarketing, Sales, and Customer Relationship Management. Indianapolis, IN:\nWiley.\nManyika, James, Michael Chui, Brad Brown, Jacques Bughin, Richard Dobbs,\nCharles Roxburgh, and Angela Hung Byers. 2011. Big Data: The Next Fron-\ntier for Innovation, Competition, and Productivity. Chicago: McKinsey Global\nInstitute. http://www.mckinsey.com/business-functions/digital-mckinsey/\nour-insights/big-data-the-next-frontier-for-innovation.\nMarr, Bernard. 2015. Big Data: Using SMART Big Data, Analytics, and Metrics to\nMake Better Decisions and Improve Performance. Chichester, UK: Wiley.\nMayer, J. R., and J. C. Mitchell. 2012. \u201cThird-Party Web Tracking: Policy and\nTechnology.\u201d In 2012 IEEE Symposium on Security and Privacy, 413\u201327. Piscat-\naway, NJ: IEEE. doi:10.1109/SP.2012.47.\nMayer, Jonathan, and Patrick Mutchler. 2014. \u201cMetaPhone: The Sensitivity of\nTelephone Metadata.\u201d Web Policy, March 12. http://webpolicy.org/2014/03/12/\nmetaphone-the-sensitivity-of-telephone-metadata.\nMayer-Sch\u00f6nberger, Viktor, and Kenneth Cukier. 2014. Big Data: A Revolution\nThat Will Transform How We Live, Work, and Think. Reprint. Boston: Eamon\nDolan/Mariner Books.\nMcMahan, Brendan, and Daniel Ramage. 2017. \u201cFederated Learning: Collab-\norative Machine Learning without Centralized Training Data.\u201d Google Research\nBlog, April. https://research.googleblog.com/2017/04/federated-learning\n-collaborative.html.\nNilsson, Nils. 1965. Learning Machines: Foundations of Trainable Pattern-\nClassifying Systems. New York: McGraw-Hill.\nOakland Privacy Working Group. 2015. \u201cPredPol: An Open Letter to the\nOakland City Council.\u201d June 25. https://www.indybay.org/newsitems/2015/\n06/25/18773987.php.\nOrganisation for Economic Co-operation and Development (OECD). 1980.\nGuidelines on the Protection of Privacy and Transborder Flows of Personal\nData. Paris: OECD. https://www.oecd.org/sti/ieconomy/oecdguidelinesonthe\nprotectionofprivacyandtransborderflowsofpersonaldata.htm.\n258 RefeRences", "Organisation for Economic Co-operation and Development (OECD). 2013.\n2013 OECD Privacy Guidelines. Paris: OECD. https://www.oecd.org/internet/\nieconomy/privacy-guidelines.htm.\nO\u2019Rourke, Crist\u00edn, and Aphra Kerr. 2017. \u201cPrivacy Schield for Whom? Key Ac-\ntors and Privacy Discourse on Twitter and in Newspapers.\u201d In \u201cRedesigning or\nRedefining Privacy?,\u201d special issue of Westminster Papers in Communication and\nCulture 12 (3): 21\u201336. doi:http://doi.org/ 10.16997/wpcc.264.\nPomerantz, Jeffrey. 2015. Metadata. Cambridge, MA: MIT Press. https://\nmitpress.mit.edu/books/metadata-0.\nPurcell, Kristen, Joanna Brenner, and Lee Rainie. 2012. \u201cSearch Engine Use\n2012.\u201d Pew Research Center, March 9. http://www.pewinternet.org/2012/\n03/09/main-findings-11/.\nQuinlan, J. R. 1986. \u201cInduction of Decision Trees.\u201d Machine Learning 1 (1):\n81\u2013106. doi:10.1023/A:1022643204877.\nRainie, Lee, and Mary Madden. 2015. \u201cAmericans\u2019 Privacy Strategies Post-\nSnowden.\u201d Pew Research Center, March. http://www.pewinternet.org/files/\n2015/03/PI_AmericansPrivacyStrategies_0316151.pdf.\nRhee, Nissa. 2016. \u201cStudy Casts Doubt on Chicago Police\u2019s Secretive \u2018Heat\nList.\u2019\u201d Chicago Magazine, August 17. http://www.chicagomag.com/city-life/\nAugust-2016/Chicago-Police-Data/.\nSaunders, Jessica, Priscillia Hunt, and John S. Hollywood. 2016. \u201cPredictions\nPut into Practice: A Quasi-Experimental Evaluation of Chicago\u2019s Predictive\nPolicing Pilot.\u201d Journal of Experimental Criminology 12 (3): 347\u2013371.\ndoi:10.1007/s11292-016-9272-0.\nShmueli, Galit. 2010. \u201cTo Explain or to Predict?\u201d Statistical Science 25 (3):\n289\u2013310. doi:10.1214/10-STS330.\nShubber, Kadhim. 2013. \u201cA Simple Guide to GCHQ\u2019s Internet Surveillance\nProgramme Tempora.\u201d WIRED UK, July 24. http://www.wired.co.uk/article/\ngchq-tempora-101.\nSilver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,\nGeorge van den Driessche, Julian Schrittwieser, et al. 2016. \u201cMastering the\nGame of Go with Deep Neural Networks and Tree Search.\u201d Nature 529 (7587):\n484\u2013489. doi:10.1038/nature16961.\nRefeRences 259", "Soldatov, Andrei, and Irina Borogan. 2012. \u201cIn Ex-Soviet States, Russian Spy\nTech Still Watches You.\u201d WIRED, December 21. https://www.wired.com/2012/\n12/russias-hand.\nSteinberg, Dan. 2013. \u201cHow Much Time Needs to Be Spent Preparing Data\nfor Analysis?\u201d http://info.salford-systems.com/blog/bid/299181/How-Much\n-Time-Needs-to-be-Spent-Preparing-Data-for-Analysis.\nTaylor, David. 2016. \u201cBattle of the Data Science Venn Diagrams.\u201d KDnug-\ngets, October. http://www.kdnuggets.com/2016/10/battle-data-science-venn\n-diagrams.html.\nTufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed.\nCheshire, CT: Graphics Press.\nTurow, Joseph. 2013. The Daily You: How the New Advertising Industry Is Defin-\ning Your Identity and Your Worth. New Haven, CT: Yale University Press.\nVerbeke, Wouter, David Martens, Christophe Mues, and Bart Baesens. 2011.\n\u201cBuilding Comprehensible Customer Churn Prediction Models with Ad-\nvanced Rule Induction Techniques.\u201d Expert Systems with Applications 38 (3):\n2354\u20132364.\nWeissman, Cale Gutherie. 2015. \u201cThe NYPD\u2019s Newest Technology May Be Record-\ning Conversations.\u201d Business Insider, March 26. http://uk.businessinsider.com/\nthe-nypds-newest-technology-may-be-recording-conversations-2015-3.\nWolpert, D. H., and W. G. Macready. 1997. \u201cNo Free Lunch Theorems for\nOptimization.\u201d IEEE Transactions on Evolutionary Computation 1 (1): 67\u201382.\ndoi:10.1109/4235.585893.\n260 RefeRences", "INDEX\nAdvertising, 170, 181, 184, 185, nominal, 42\u201348, 132, 136, 142,\n186, 188, 191, 211, 213\u2013217, 148\n236 numeric, 42\u201348, 94, 102\u2013104,\nAlgorithm 106, 110, 115, 121, 136, 138,\nApriori, 166\u2013171 142, 148, 156\nbackpropagation, 129\u2013131 ordinal, 42\u201348, 132, 136, 142, 148\nID3, 137\u2013142 raw, 51, 109, 245\nk-means, 14, 156\u2013159 target (see Machine learning)\nleast squares, 12, 116\u2013120\nAlphaGo, 31\u201332 Backpropagation. See Algorithm\nAnalytics record, 39\u201341, 48, 96 Bias, 46\u201348, 143\u2013144\nAnomaly detection, 160\u2013164, 166, learning bias, 99\u2013101, 144\n239 sample bias, 143\nAntidiscrimination legislation, 21, Big data, 2, 9\u201310, 15\u201316, 19, 21\u201322,\n205\u2013206, 218 30\u201331, 35, 70\u201372, 75, 77, 84,\nArtificial intelligence, 12, 14, 49 86\u201392, 181, 183, 210, 213, 219,\nAssociation-rule mining, 151, 221, 223, 231, 236, 240\n164\u2013171, 239 Body Mass Index (BMI), 51,\nAttribute, 239 109\u2013113, 117\u2013120, 126, 134,\nbinary, 43 143\nderived, 49\u201351, 96, 109, 112\u2013113, Business intelligence, 72\u201373, 75,\n126, 242 228", "Children\u2019s Online Privacy Protection recurrent neural networks,\nAct, 216\u2013217 132\u2013133\nChurn, 5, 171\u2013178 representation learning, 134\nCluster analysis, 2, 14, 102\u2013104, Decision tree. See Machine learning\n151\u2013159, 161\u2013162, 166, 240 Differential privacy, 201\u2013203\nCorrelation, 96, 105\u2013114, 120, 127, DIKW pyramid, 55, 242\n166, 169, 240\nCRISP-DM, 56\u201367, 92, 97\u201398, 144, Entropy, 138\u2013139\n150, 152, 237, 240 Extraction, Transformation, and\nLoad (ETL), 21, 74, 87, 231,\nData 242\ncaptured, 51, 53, 240\nexhaust, 51\u201354, 242 Feature. See Attribute\nmetadata, 9, 52\u201354, 243 Federated learning, 201, 203, 205\noperational, 8, 70 Flink, 87\u201388\nstructured, 1, 8, 10, 48, 92, 245\ntransactional, 7\u20138, 86, 155, General Data Protection\n228\u2013229, 244 Regulations, 149, 206, 208,\nunstructured, 2, 19, 22, 48\u201349, 77, 214\u2013215, 232\n92, 244\nDatabase Hadoop, 11, 19, 22, 71, 75\u201378, 80,\nhybrid, 88\u201391 86\u201391, 242\nNoSQL, 9\u201310, 22, High-performance computing\nRelational Database Management (HPC), 22, 242\nSystem (RDBMS), 72, 91, 245\nrelational data model, 7, 15, In-database machine learning,\n74 82\u201385, 242\nData cube, 74\u201375 Internet of Things (IoT), 164, 219,\nData mining, 1, 16\u201317, 34, 236, 221\u2013222, 243\n241\nData visualization, 12\u201313, 23 Knowledge discovery in databases\nData warehouse, 8, 16, 40, 60, 71, (KDD), 16\u201317\n73\u201376, 78\u201379, 90\u201391, 160,\n228\u2013229, 241 Linear regression. See Machine\nData wrangling, 22 learning\nDeep learning, 15, 31\u201333, 35, 121,\n131\u2013135, 231, 241\u2013242 Machine learning, 1, 14\u201316, 23\u201325,\nconvolutional neural networks, 33, 61\u201362, 97\u2013150, 236,\n133\u2013134 243\n262 Index", "classification, 2, 4, 14, 78, Moneyball, 28, 47\n136\u2013137, 142, 151, 171\u2013179, Massively Parallel Processing (MPP),\n240 71, 73, 243\nconcept drift, 150\ndecision trees, 14, 61, 98, 105, Neural networks. See Deep learning;\n136\u2013143, 150, 152, 162, Machine learning\n241\nevaluation, 145\u2013148 OECD, 207, 208, 210, 212\nlearning bias (see Bias) Online Analytical Processing\nlinear regression, 12, 18, 61, 79, (OLAP), 71, 74\u201375, 244\n98, 105, 114\u2013123, 143\u2013144, Online Transaction Processing\n136, 243 (OLTP), 70\u201371, 244\nno free lunch theorem, 144 Operational Data Store (ODS), 73,\nneural networks, 12, 14\u201315, 31, 244\n33, 61, 98, 105, 120, 121\u2013136, Outliers, 23, 61, 119, 160\n149, 231\u2013232, 244 Outlier detection. See Anomaly\nprediction, 2, 15, 18, 35, 78, detection\n84, 104\u2013105, 113\u2013114, 117,\n127\u2013128, 137, 142, 145, Precision medicine, 26\u201327,\n151\u2013152, 162\u2013163, 172\u2013180, 219\u2013221, 236\n187\u2013188, 192, 195, Predictive Policing (PredPol),\n244 191\u2013195, 198\nrandom-forest model, 142 Privacy by design, 217\nstationarity assumption, 150 Privacy shield, 213\u2013214\nsum of squared errors, 116\u2013117, Propensity modeling, 171\u2013172\n148\nsupervised, 99\u2013100, 105, 129, RDBMS. See Database\n245 Recommender system, 25\u201326\ntarget attribute, 99\u2013100, 104\u2013105, Regression, 12, 98, 114\u2013116,\n110, 112\u2013117, 120, 127, 120, 143, 151, 178\u2013180,\n137\u2013138, 148, 151, 173\u2013174, 245\n179, 245 Relational data model. See\nunsupervised, 99\u2013104, 165, Database\n246 Right to be forgotten, 212\u2013213\nMapReduce, 11, 75\u201376\nMarket-basket analysis, 165\u2013171 Segmentation. See Cluster analysis\nMarketing, 25\u201326, 54, 60, 152\u2013155, Smart City, 27, 196, 219, 221\u2013223,\n159, 166, 169\u2013170, 172, 236, 245\n184\u2013187, 190, 196 Spark, 87\u201388\nIndex 263", "Storm, 87\nStructured Query Language (SQL),\n7, 75\u201376, 81\u201382, 84, 90,\n160\u2013161, 245\nTarget, pregnancy prediction,\n187\u2013188\nTarget attribute. See Machine\nlearning\nVariable. See Attribute\n264 Index", "", "JOHN D. KELLEHER is a Professor of Computer Science and the Academic\nLeader of the Information, Communication, and Entertainment Research In-\nstitute at the Dublin Institute of Technology. His research is supported by\nthe ADAPT Centre, which is funded by Science Foundation Ireland (Grant 13/\nRC/2106) and co-funded by the European Regional Development fund. He is\nthe coauthor of Fundamentals of Machine Learning for Predictive Data Analytics\n(MIT Press).\nBRENDAN TIERNEY is a lecturer in the School of Computing at the Dublin\nInstitute of Technology, an Oracle ACE director, and the author of a number\nof books on data mining using Oracle technology."]