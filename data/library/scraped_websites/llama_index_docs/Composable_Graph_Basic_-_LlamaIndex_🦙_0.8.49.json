{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/composable_indices/ComposableIndices-Prior.html",
        "title": "Composable Graph Basic - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Composable Graph Basic[\uf0c1](#composable-graph-basic \"Permalink to this heading\")\n\n\\# NOTE: This is ONLY necessary in jupyter notebook.\n\\# Details: Jupyter runs an event-loop behind the scenes.\n\\#          This results in nested event-loops when we start an event-loop to make async queries.\n\\#          This is normally not allowed, we use nest\\_asyncio to allow it for convenience.\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    EmptyIndex,\n    TreeIndex,\n    SummaryIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n    StorageContext,\n)\n\n## Load Datasets[\uf0c1](#load-datasets \"Permalink to this heading\")\n\nLoad PG\u2019s essay\n\n\\# load PG's essay\nessay\\_documents \\= SimpleDirectoryReader(\n    \"../paul\\_graham\\_essay/data/\"\n).load\\_data()\n\n## Building the document indices[\uf0c1](#building-the-document-indices \"Permalink to this heading\")\n\n*   Build a vector index for PG\u2019s essay\n    \n*   Also build an empty index (to store prior knowledge)\n    \n\n\\# configure\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=512)\nstorage\\_context \\= StorageContext.from\\_defaults()\n\n\\# build essay index\nessay\\_index \\= VectorStoreIndex.from\\_documents(\n    essay\\_documents,\n    service\\_context\\=service\\_context,\n    storage\\_context\\=storage\\_context,\n)\nempty\\_index \\= EmptyIndex(\n    service\\_context\\=service\\_context, storage\\_context\\=storage\\_context\n)\n\n## Query Indices[\uf0c1](#query-indices \"Permalink to this heading\")\n\nSee the response of querying each index\n\nquery\\_engine \\= essay\\_index.as\\_query\\_engine(\n    similarity\\_top\\_k\\=3,\n    response\\_mode\\=\"tree\\_summarize\",\n)\nresponse \\= query\\_engine.query(\n    \"Tell me about what Sam Altman did during his time in YC\",\n)\n\nquery\\_engine \\= empty\\_index.as\\_query\\_engine(response\\_mode\\=\"generation\")\nresponse \\= query\\_engine.query(\n    \"Tell me about what Sam Altman did during his time in YC\",\n)\n\nDefine summary for each index.\n\nessay\\_index\\_summary \\= (\n    \"This document describes Paul Graham's life, from early adulthood to the\"\n    \" present day.\"\n)\nempty\\_index\\_summary \\= \"This can be used for general knowledge purposes.\"\n\n## Define Graph (Summary Index as Parent Index)[\uf0c1](#define-graph-summary-index-as-parent-index \"Permalink to this heading\")\n\nThis allows us to synthesize responses both using a knowledge corpus as well as prior knowledge.\n\nfrom llama\\_index.indices.composability import ComposableGraph\n\ngraph \\= ComposableGraph.from\\_indices(\n    SummaryIndex,\n    \\[essay\\_index, empty\\_index\\],\n    index\\_summaries\\=\\[essay\\_index\\_summary, empty\\_index\\_summary\\],\n    service\\_context\\=service\\_context,\n    storage\\_context\\=storage\\_context,\n)\n\n\\# \\[optional\\] persist to disk\nstorage\\_context.persist()\nroot\\_id \\= graph.root\\_id\n\n\\# \\[optional\\] load from disk\nfrom llama\\_index.indices.loading import load\\_graph\\_from\\_storage\n\ngraph \\= load\\_graph\\_from\\_storage(storage\\_context, root\\_id\\=root\\_id)\n\n\\# configure query engines\ncustom\\_query\\_engines \\= {\n    essay\\_index.index\\_id: essay\\_index.as\\_query\\_engine(\n        similarity\\_top\\_k\\=3,\n        response\\_mode\\=\"tree\\_summarize\",\n    )\n}\n\n\\# set Logging to DEBUG for more detailed outputs\n\\# ask it a question about Sam Altman\nquery\\_engine \\= graph.as\\_query\\_engine(custom\\_query\\_engines\\=custom\\_query\\_engines)\nresponse \\= query\\_engine.query(\n    \"Tell me about what Sam Altman did during his time in YC\",\n)\n\n\\# Get source of response\nprint(response.get\\_formatted\\_sources())\n\n## Define Graph (Tree Index as Parent Index)[\uf0c1](#define-graph-tree-index-as-parent-index \"Permalink to this heading\")\n\nThis allows us to \u201croute\u201d a query to either a knowledge-augmented index, or to the LLM itself.\n\nfrom llama\\_index.indices.composability import ComposableGraph\n\n\\# configure retriever\ncustom\\_query\\_engines \\= {\n    essay\\_index.index\\_id: essay\\_index.as\\_query\\_engine(\n        similarity\\_top\\_k\\=3,\n        response\\_mode\\=\"tree\\_summarize\",\n    )\n}\n\ngraph2 \\= ComposableGraph.from\\_indices(\n    TreeIndex,\n    \\[essay\\_index, empty\\_index\\],\n    index\\_summaries\\=\\[essay\\_index\\_summary, empty\\_index\\_summary\\],\n)\n\n\\# set Logging to DEBUG for more detailed outputs\n\\# ask it a question about NYC\nquery\\_engine \\= graph2.as\\_query\\_engine(\n    custom\\_query\\_engines\\=custom\\_query\\_engines\n)\nresponse \\= query\\_engine.query(\n    \"Tell me about what Paul Graham did growing up?\",\n)\n\nprint(response.get\\_formatted\\_sources())\n\nresponse \\= query\\_engine.query(\n    \"Tell me about Barack Obama\",\n)\n\nresponse.get\\_formatted\\_sources()"
}