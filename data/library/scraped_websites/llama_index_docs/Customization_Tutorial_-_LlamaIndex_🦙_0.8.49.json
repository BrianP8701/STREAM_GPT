{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/getting_started/customization.html",
        "title": "Customization Tutorial - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Customization Tutorial[\uf0c1](#customization-tutorial \"Permalink to this heading\")\n\nIn this tutorial, we show the most common customizations with the [starter example](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html):\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n* * *\n\n**\u201cI want to parse my documents into smaller chunks\u201d**\n\nfrom llama\\_index import ServiceContext\nservice\\_context \\= ServiceContext.from\\_defaults(chunk\\_size\\=1000)\n\nTip\n\nServiceContext is a bundle of services and configurations used across a LlamaIndex pipeline, Learn more [here](https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/service_context.html).\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents, service\\_context\\=service\\_context)\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n* * *\n\n**\u201cI want to use a different vector store\u201d**\n\nimport chromadb\nfrom llama\\_index.vector\\_stores import ChromaVectorStore\nfrom llama\\_index import StorageContext\n\nchroma\\_client \\= chromadb.PersistentClient()\nchroma\\_collection \\= chroma\\_client.create\\_collection(\"quickstart\")\nvector\\_store \\= ChromaVectorStore(chroma\\_collection\\=chroma\\_collection)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\n\nTip\n\nStorageContext defines the storage backend for where the documents, embeddings, and indexes are stored. Learn more [here](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/storage/customization.html).\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents, storage\\_context\\=storage\\_context)\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n* * *\n\n**\u201cI want to retrieve more context when I query\u201d**\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine(similarity\\_top\\_k\\=5)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nTip\n\nas\\_query\\_engine builds a default retriever and query engine on top of the index. You can configure the retriever and query engine by passing in keyword arguments. Here, we configure the retriever to return the top 5 most similar documents (instead of the default of 2). Learn more about vector index [here](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/vector_store_guide.html).\n\n* * *\n\n**\u201cI want to use a different LLM\u201d**\n\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.llms import PaLM\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=PaLM())\n\nTip\n\nLearn more about customizing LLMs [here](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/usage_custom.html).\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n* * *\n\n**\u201cI want to use a different response mode\u201d**\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine(response\\_mode\\='tree\\_summarize')\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nTip\n\nLearn more about query engine usage pattern [here](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/usage_pattern.html) and available response modes [here](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/response_modes.html).\n\n* * *\n\n**\u201cI want to stream the response back\u201d**\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_query\\_engine(streaming\\=True)\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\nresponse.print\\_response\\_stream()\n\nTip\n\nLearn more about streaming [here](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/streaming.html).\n\n* * *\n\n**\u201cI want a chatbot instead of Q&A\u201d**\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader('data').load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery\\_engine \\= index.as\\_chat\\_engine()\nresponse \\= query\\_engine.chat(\"What did the author do growing up?\")\nprint(response)\n\nresponse \\= query\\_engine.chat(\"Oh interesting, tell me more.\")\nprint(response)\n\nTip\n\nLearn more about chat engine usage pattern [here](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/chat_engines/usage_pattern.html).\n\n* * *\n\nNext Steps\n\n*   want a thorough walkthrough of (almost) everything you can configure? Try the [end-to-end tutorial on basic usage pattern](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/usage_pattern.html).\n    \n*   want more in-depth understanding of specific modules? Check out the module guides \ud83d\udc48"
}