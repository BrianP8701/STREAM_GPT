["Deep L ea r ni n g\nI a n G o o d f e l l o w\nY o s h u a B e n g i o\nA a r o n C o u r v i l l e", "C on t e n t s\nWebsite vii\nAcknowledgments viii\nNotation xi\n1Introduction 1\n1.1WhoShouldReadThisBook?.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.8\n1.2HistoricalTrendsinDeepLearning.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 11\nIAppliedMathandMachineLearningBasics 29\n2LinearAlgebra 31\n2.1Scalars,Vectors,MatricesandTensors.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.31\n2.2MultiplyingMatricesandVectors.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .34\n2.3IdentityandInverseMatrices.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.36\n2.4LinearDependenceandSpan.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.37\n2.5Norms.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 39\n2.6SpecialKindsofMatricesandVectors.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 40\n2.7Eigendecomposition.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 42\n2.8SingularValueDecomposition.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.44\n2.9TheMoore-PenrosePseudoinverse.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .45\n2.10TheTraceOperator.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 46\n2.11TheDeterminant.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 47\n2.12Example:PrincipalComponentsAnalysis.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.48\n3ProbabilityandInformationTheory 53\n3.1WhyProbability?.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.54\ni", "CO NTE NT S\n3.2RandomVariables.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.56\n3.3ProbabilityDistributions.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.56\n3.4MarginalProbability.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 58\n3.5ConditionalProbability.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.59\n3.6TheChainRuleofConditionalProbabilities.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.59\n3.7IndependenceandConditionalIndependence.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.60\n3.8Expectation,VarianceandCovariance.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.60\n3.9CommonProbabilityDistributions.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.62\n3.10UsefulPropertiesofCommonFunctions.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.67\n3.11Bayes\u2019Rule.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.70\n3.12TechnicalDetailsofContinuousVariables.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 71\n3.13InformationTheory.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 73\n3.14StructuredProbabilisticModels.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 75\n4NumericalComputation 80\n4.1Over\ufb02owandUnder\ufb02ow.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.80\n4.2PoorConditioning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 82\n4.3Gradient-BasedOptimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.82\n4.4ConstrainedOptimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .93\n4.5Example:LinearLeastSquares.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.96\n5MachineLearningBasics 98\n5.1LearningAlgorithms.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.99\n5.2Capacity,Over\ufb01ttingandUnder\ufb01tting.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.110\n5.3HyperparametersandValidationSets..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.120\n5.4Estimators,BiasandVariance.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.122\n5.5MaximumLikelihoodEstimation.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.131\n5.6BayesianStatistics.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.135\n5.7SupervisedLearningAlgorithms.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 140\n5.8UnsupervisedLearningAlgorithms.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.146\n5.9StochasticGradientDescent.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 151\n5.10BuildingaMachineLearningAlgorithm.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .153\n5.11ChallengesMotivatingDeepLearning.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.155\nIIDeepNetworks:ModernPractices 166\n6DeepFeedforwardNetworks 168\n6.1Example:LearningXOR..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.171\n6.2Gradient-BasedLearning..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.177\ni i", "CO NTE NT S\n6.3HiddenUnits.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.191\n6.4ArchitectureDesign.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .197\n6.5Back-PropagationandOtherDi\ufb00erentiationAlgorithms.\u00a0.\u00a0.\u00a0.\u00a0.204\n6.6HistoricalNotes.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.224\n7RegularizationforDeepLearning 228\n7.1ParameterNormPenalties.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 230\n7.2NormPenaltiesasConstrainedOptimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.237\n7.3RegularizationandUnder-ConstrainedProblems.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.239\n7.4DatasetAugmentation.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.240\n7.5NoiseRobustness.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.242\n7.6Semi-SupervisedLearning.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.243\n7.7Multi-TaskLearning.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.244\n7.8EarlyStopping.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.246\n7.9ParameterTyingandParameterSharing\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 253\n7.10SparseRepresentations.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.254\n7.11BaggingandOtherEnsembleMethods..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.256\n7.12Dropout.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.258\n7.13AdversarialTraining.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 268\n7.14TangentDistance,TangentProp,andManifoldTangentClassi\ufb01er270\n8OptimizationforTrainingDeepModels 274\n8.1HowLearningDi\ufb00ersfromPureOptimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 275\n8.2ChallengesinNeuralNetworkOptimization .\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.282\n8.3BasicAlgorithms.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.294\n8.4ParameterInitialization Strategies..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.301\n8.5AlgorithmswithAdaptiveLearningRates.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.306\n8.6ApproximateSecond-Order Methods.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.310\n8.7Optimization StrategiesandMeta-Algorithms.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.317\n9ConvolutionalNetworks 330\n9.1TheConvolutionOperation.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.331\n9.2Motivation.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.335\n9.3Pooling.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.339\n9.4ConvolutionandPoolingasanIn\ufb01nitelyStrongPrior.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.345\n9.5VariantsoftheBasicConvolutionFunction.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 347\n9.6StructuredOutputs..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 358\n9.7DataTypes.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 360\n9.8E\ufb03cientConvolutionAlgorithms.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.362\n9.9RandomorUnsupervisedFeatures.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .363\ni i i", "CO NTE NT S\n9.10TheNeuroscienti\ufb01cBasisforConvolutionalNetworks.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning.\u00a0.\u00a0.\u00a0.371\n10\u00a0SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.375\n10.2RecurrentNeuralNetworks.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .378\n10.3BidirectionalRNNs\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.394\n10.4Encoder-DecoderSequence-to-SequenceArchitectures.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..396\n10.5DeepRecurrentNetworks.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.398\n10.6RecursiveNeuralNetworks.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 400\n10.7TheChallengeofLong-TermDependencies.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.401\n10.8EchoStateNetworks.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.404\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales.\u00a0.\u00a0..406\n10.10\u00a0TheLongShort-TermMemoryandOtherGatedRNNs.\u00a0..\u00a0.\u00a0.\u00a0.408\n10.11\u00a0Optimization forLong-TermDependencies.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.413\n10.12\u00a0Explicit Memory.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 416\n11\u00a0PracticalMethodology 421\n11.1PerformanceMetrics.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.422\n11.2DefaultBaselineModels.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.425\n11.3DeterminingWhethertoGatherMoreData.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 426\n11.4SelectingHyperparameters.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.427\n11.5DebuggingStrategies.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.436\n11.6Example:Multi-DigitNumberRecognition.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 440\n12\u00a0Applications 443\n12.1Large-ScaleDeepLearning..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.443\n12.2ComputerVision.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.452\n12.3SpeechRecognition\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.458\n12.4NaturalLanguageProcessing.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .461\n12.5OtherApplications.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .478\nIIIDeepLearningResearch 486\n13\u00a0LinearFactorModels 489\n13.1ProbabilisticPCAandFactorAnalysis.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 490\n13.2IndependentComponentAnalysis(ICA).\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.491\n13.3SlowFeatureAnalysis.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .493\n13.4SparseCoding.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.496\ni v", "CO NTE NT S\n13.5ManifoldInterpretation ofPCA.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.499\n14\u00a0Autoencoders 502\n14.1Undercomplete Autoencoders.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.503\n14.2RegularizedAutoencoders.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.504\n14.3RepresentationalPower,LayerSizeandDepth.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.508\n14.4StochasticEncodersandDecoders.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.509\n14.5DenoisingAutoencoders.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.510\n14.6LearningManifoldswithAutoencoders.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 515\n14.7ContractiveAutoencoders..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.521\n14.8PredictiveSparseDecomposition.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .523\n14.9ApplicationsofAutoencoders.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.524\n15\u00a0RepresentationLearning 526\n15.1GreedyLayer-WiseUnsupervisedPretraining.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.528\n15.2TransferLearningandDomainAdaptation.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .536\n15.3Semi-SupervisedDisentanglingofCausalFactors.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.541\n15.4DistributedRepresentation.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .546\n15.5ExponentialGainsfromDepth.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.554\n16\u00a0StructuredProbabilisticModelsforDeepLearning558\n16.1TheChallengeofUnstructuredModeling..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.559\n16.2UsingGraphstoDescribeModelStructure.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.563\n16.3SamplingfromGraphicalModels.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.580\n16.4AdvantagesofStructuredModeling\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.582\n16.5LearningaboutDependencies.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 582\n16.6InferenceandApproximateInference.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.584\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\n17\u00a0MonteCarloMethods 590\n17.1SamplingandMonteCarloMethods.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 590\n17.2ImportanceSampling.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.592\n17.3MarkovChainMonteCarloMethods.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.595\n17.4GibbsSampling\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.599\n17.5TheChallengeofMixingbetweenSeparatedModes.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..599\n18\u00a0ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.606\n18.2StochasticMaximumLikelihoodandContrastiveDivergence.\u00a0.\u00a0.607\nv", "CO NTE NT S\n18.3Pseudolikelihood.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.615\n18.4ScoreMatchingandRatioMatching.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 617\n18.5DenoisingScoreMatching.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.619\n18.6Noise-ContrastiveEstimation.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.620\n18.7EstimatingthePartitionFunction.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.623\n19\u00a0ApproximateInference 631\n19.1InferenceasOptimization ..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.633\n19.2ExpectationMaximization .\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.634\n19.3MAPInferenceandSparseCoding..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.635\n19.4VariationalInferenceandLearning.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.638\n19.5LearnedApproximateInference.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.651\n20\u00a0DeepGenerativeModels 654\n20.1BoltzmannMachines.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.654\n20.2RestrictedBoltzmannMachines.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.656\n20.3DeepBeliefNetworks..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 660\n20.4DeepBoltzmannMachines.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.663\n20.5BoltzmannMachinesforReal-ValuedData.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.676\n20.6ConvolutionalBoltzmannMachines\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .683\n20.7BoltzmannMachinesforStructuredorSequentialOutputs.\u00a0.\u00a0.\u00a0.685\n20.8OtherBoltzmannMachines.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 686\n20.9Back-PropagationthroughRandomOperations.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.687\n20.10\u00a0DirectedGenerativeNets.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.692\n20.11\u00a0DrawingSamplesfromAutoencoders.\u00a0.\u00a0.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.711\n20.12\u00a0Generativ eStochasticNetworks.\u00a0.\u00a0..\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 714\n20.13\u00a0OtherGenerationSchemes.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 716\n20.14\u00a0EvaluatingGenerativeModels\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.717\n20.15\u00a0Conclus ion.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.720\nBibliography 721\nIndex 777\nv i", "W e b s i t e\nwww.deeplearningb ook.org\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\nvii", "Acknowledgments\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\nWewouldliketothankthosewhocommentedonourproposalforthebook\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\n\u00c7a\u011flarG\u00fcl\u00e7ehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\nRoh\u00e9e.\nWewouldliketothankthepeoplewhoo\ufb00eredfeedbackonthecontentofthe\nbookitself.Someo\ufb00eredfeedbackonmanychapters:Mart\u00ednAbadi,Guillaume\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBi\u00e7ici,Matko\nBo\u0161njak,JohnBoersma,GregBrockman,AlexandredeBr\u00e9bisson,PierreLuc\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,Fr\u00e9d\u00e9ricFrancis,\nNando\u00a0deFreitas,\u00c7a\u011flar\u00a0G\u00fcl\u00e7ehre,\u00a0Jurgen\u00a0V anGael,JavierAlonso\u00a0Garc\u00eda,\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\nMathey,Mat\u00edasMattamala,AbhinavMaurya,KevinMurphy,OlegM\u00fcrk,Roman\nNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\nHalisSak,\u00a0C\u00e9sarSalgado,GrigorySapunov,YoshinoriSasaki,\u00a0MikeSchuster,\nJulianSerban,NirShabat,KenShirri\ufb00,AndreSimpelo,ScottStanley,David\nSussillo,IlyaSutskever,CarlesGeladaS\u00e1ez,GrahamTaylor,ValentinTolmer,\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin\nWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZaj\u0105candOzan\u00c7a\u011flayan.\nWewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\nindividualchapters:\n\u2022Notation:ZhangYuanhang.\n\u2022Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\nviii", "CO NTE NT S\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminP\u00e2rvulescu\nandAlfredoSolano.\n\u2022Chapter, :AmjadAlmahairi,NikolaBani\u0107,KevinBennett, 2LinearAlgebra\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\nSergeyOreshkov,Istv\u00e1nPetr\u00e1s,DennisPrangle,ThomasRoh\u00e9e,Gitanjali\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\n\u2022Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,\nAnttiRasmus,AlexeySurkovandVolkerTresp.\n\u2022Chapter\u00a0,\u00a0 :Tran\u00a0LamAnIan\u00a0Fischer\u00a0andHu 4NumericalComputation\nYuhuang.\n\u2022Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.\n\u2022Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\nElizabethBurl,IshanDurugkar,Je\ufb00Hlywa,JongWookKim,DavidKrueger\nandAdityaKumarPraharaj.\n\u2022Chapter, :MortenKolb\u00e6k,KshitijLauria, 7RegularizationforDeepLearning\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.\n\u2022Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\nKashifRasul,KlausStroblandNicholasTurner.\n\u2022Chapter,9ConvolutionalNetworks:Mart\u00ednArjovsky,EugeneBrevdo,Kon-\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\nStoutandWentaoWu.\n\u2022Chapter,10SequenceModeling:RecurrentandRecursiveNets:G\u00f6k\u00e7en\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang.\n\u2022Chapter, :DanielBeckstein. 11PracticalMethodology\n\u2022Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\nRoscher.\n\u2022Chapter,13LinearFactorModels:JayanthKoushik.\ni x", "CO NTE NT S\n\u2022Chapter, :KunalGhosh. 15RepresentationLearning\n\u2022Chapter, :\u00a0MinhL\u00ea 16StructuredProbabilisticModelsforDeepLearning\nandAntonVarfolom.\n\u2022Chapter,18ConfrontingthePartitionFunction:SamBowman.\n\u2022Chapter, :YujiaBao. 19ApproximateInference\n\u2022Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\nWenmingMa,FadyMedhat,ShakirMohamedandGr\u00e9goireMontavon.\n\u2022Bibliography:LukasMichelbacherandLeslieN.Smith.\nWealsowanttothankthosewhoallowedustoreproduceimages,\ufb01guresor\ndatafromtheirpublications.Weindicatetheircontributionsinthe\ufb01gurecaptions\nthroughoutthetext.\nWewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\nmakethewebversionofthebook,andforo\ufb00eringsupporttoimprovethequality\noftheresultingHTML.\nWe\u00a0would\u00a0liketothank\u00a0Ian\u2019swifeDaniela\u00a0FloriGoodfellowforpatiently\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\nWewouldliketothanktheGoogleBrainteamforprovidinganintellectual\nenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthis\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\ntothankIan\u2019sformermanager,GregCorrado,andhiscurrentmanager,Samy\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeo\ufb00rey\nHintonforencouragement whenwritingwasdi\ufb03cult.\nx", "N ot at i o n\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\nconcepts,wedescribemostoftheseideasinchapters2\u20134.\nNum b e r s and Ar r a y s\naAscalar(integerorreal)\naAvector\nAAmatrix\nAAtensor\nI nIdentitymatrixwithrowsandcolumns n n\nIIdentitymatrixwithdimensionalityimpliedby\ncontext\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\n1atposition i\ndiag()aAsquare,diagonalmatrixwithdiagonalentries\ngivenbya\naAscalarrandomvariable\naAvector-valuedrandomvariable\nAAmatrix-valuedrandomvariable\nxi", "CO NTE NT S\nSet s and G r aphs\nAAset\nRThesetofrealnumbers\n{}01 ,Thesetcontaining0and1\n{ } 01 , , . . . , nThesetofallintegersbetweenand0 n\n[] a , bTherealintervalincludingand a b\n(] a , bTherealintervalexcludingbutincluding a b\nA B\\Setsubtraction,i.e.,\u00a0thesetcontainingtheele-\nmentsofthatarenotin A B\nGAgraph\nP a G(x i)Theparentsofx iinG\nI ndexing\na iElement iofvectora,withindexingstartingat1\na \u2212 iAllelementsofvectorexceptforelementa i\nA i , jElementofmatrix i , jA\nA i , :Rowofmatrix iA\nA : , iColumnofmatrix iA\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\nA : : , , i2-Dsliceofa3-Dtensor\na iElementoftherandomvector i a\nL i near Al g e br a O p e r at i o ns\nA\ue03eTransposeofmatrixA\nA+Moore-PenrosepseudoinverseofA\nAB\ue00cElement-wise(Hadamard)productofandAB\ndet()ADeterminantofA\nx i i", "CO NTE NT S\nCal c ul usd y\nd xDerivativeofwithrespectto y x\n\u2202 y\n\u2202 xPartialderivativeofwithrespectto y x\n\u2207 x yGradientofwithrespectto y x\n\u2207 X yMatrixderivativesofwithrespectto y X\n\u2207 X yTensorcontainingderivativesof ywithrespectto\nX\n\u2202 f\n\u2202xJacobianmatrixJ\u2208 Rm n \u00d7of f: Rn\u2192 Rm\n\u22072\nx f f f () (xorH)()xTheHessianmatrixofatinputpointx\ue05a\nf d()xxDe\ufb01niteintegralovertheentiredomainofx\n\ue05a\nSf d()xx x De\ufb01niteintegralwithrespecttoovertheset S\nP r o babil i t y and I nf o r m at i o n T heor y\nabTherandomvariablesaandbareindependent \u22a5\nabcTheyareconditionallyindependentgivenc \u22a5|\nP()aAprobabilitydistributionoveradiscretevariable\np()aAprobabilitydistributionoveracontinuousvari-\nable,oroveravariablewhosetypehasnotbeen\nspeci\ufb01ed\na Randomvariableahasdistribution \u223c P P\nE x \u223c P[()] () () () f xor E f xExpectationof f xwithrespectto Px\nVar(()) f xVarianceofunderx f x() P()\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\nH()xShannonentropyoftherandomvariablex\nD K L( ) P Q\ue06bKullback-LeiblerdivergenceofPandQ\nN(; )x\u00b5 ,\u03a3Gaussiandistributionoverxwithmean\u00b5and\ncovariance\u03a3\nx i i i", "CO NTE NT S\nF unc t i o ns\nf f : A B\u2192Thefunctionwithdomainandrange A B\nf g f g \u25e6Compositionofthefunctionsand\nf(;)x\u03b8Afunctionofxparametrized by\u03b8.\u00a0(Sometimes\nwewrite f(x)andomittheargument\u03b8tolighten\nnotation)\nlog x x Naturallogarithmof\n\u03c3 x()Logisticsigmoid,1\n1+exp()\u2212 x\n\u03b6 x x () log(1+exp( Softplus, ))\n||||x p Lpnormofx\n||||x L2normofx\nx+Positivepartof,i.e., x max(0) , x\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa\nvector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\ntothearrayelement-wise. Forexample,if C= \u03c3( X),then C i , j , k= \u03c3( X i , j , k)forall\nvalidvaluesof,and. i j k\nD at aset s and D i st r i but i o n s\np da t aThedatageneratingdistribution\n\u02c6 p da t aTheempiricaldistributionde\ufb01nedbythetraining\nset\nXAsetoftrainingexamples\nx( ) iThe-thexample(input)fromadataset i\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\ning\nXThe m n\u00d7matrixwithinputexamplex( ) iinrow\nX i , :\nx i v", "C h a p t e r 1\nI n t ro d u ct i on\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\nbacktoatleastthetimeofancientGreece.Themythical\ufb01guresPygmalion,\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\nGalatea,Talos,andPandoramayallberegardedasarti\ufb01ciallife( , OvidandMartin\n2004Sparkes1996Tandy1997 ;,;,).\nWhenprogrammable computerswere\ufb01rstconceived,peoplewonderedwhether\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas\nbuilt(Lovelace1842,).Today, ar t i \ufb01c i al i n t e l l i g e nc e(AI)isathriving\ufb01eldwith\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\ninmedicineandsupportbasicscienti\ufb01cresearch.\nIntheearlydaysofarti\ufb01cialintelligence,the\ufb01eldrapidlytackledandsolved\nproblemsthatareintellectually di\ufb03cultforhumanbeingsbutrelativelystraight-\nforwardforcomputers\u2014problemsthatcanbedescribedbyalistofformal,math-\nematicalrules.\u00a0Thetruechallengetoarti\ufb01cialintelligenceprovedtobesolving\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\nformally\u2014probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\nspokenwordsorfacesinimages.\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\nhierarchyofconcepts,witheachconceptde\ufb01nedintermsofitsrelationtosimpler\nconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\n1", "CHAPTER1.INTRODUCTION\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\ntheworld.Forexample,IBM\u2019sDeepBluechess-playingsystemdefeatedworld\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove\ninonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis\u00a0a\ntremendousaccomplishment,\u00a0butthechallengeisnotduetothedi\ufb03cultyof\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\nprovidedaheadoftimebytheprogrammer.\nIronically,abstractandformaltasksthatareamongthemostdi\ufb03cultmental\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers\nhavelongbeenabletodefeateventhebesthumanchessplayer,butareonly\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\norspeech.Aperson\u2019severydayliferequiresanimmenseamountofknowledge\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\ndi\ufb03culttoarticulateinaformalway.Computersneedtocapturethissame\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\narti\ufb01cialintelligenceishowtogetthisinformalknowledgeintoacomputer.\nSeveralarti\ufb01cialintelligenceprojectshavesoughttohard-codeknowledgeabout\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\nk no wl e dge baseapproachtoarti\ufb01cialintelligence.Noneoftheseprojectshasled\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\ncalledCycL.Thesestatementsareenteredbyasta\ufb00ofhumansupervisors.Itisan\nunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\nenginedetectedaninconsistencyinthestory:\u00a0itknewthatpeopledonothave\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\nentity\u201cFredWhileShaving\u201dcontainedelectricalparts.Itthereforeaskedwhether\nFredwasstillapersonwhilehewasshaving.\nThedi\ufb03cultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\n2", "CHAPTER1.INTRODUCTION\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily\nonthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\nvariousoutcomes.However,itcannotin\ufb02uencethewaythatthefeaturesare\nde\ufb01nedinanyway.\u00a0IflogisticregressionwasgivenanMRIscanofthepatient,\nratherthanthedoctor\u2019sformalizedreport,itwouldnotbeabletomakeuseful\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\ncomplications thatmightoccurduringdelivery.\nThisdependenceonrepresentationsisageneralphenomenon thatappears\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif\nthecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\narithmeticonArabicnumerals,but\ufb01ndarithmeticonRomannumeralsmuch\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\nenormouse\ufb00ectontheperformanceofmachinelearningalgorithms.Forasimple\nvisualexample,see\ufb01gure.1.1\nManyarti\ufb01cialintelligencetaskscanbesolvedbydesigningtherightsetof\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine\nlearningalgorithm.Forexample,ausefulfeatureforspeakeridenti\ufb01cationfrom\nsoundisanestimateofthesizeofspeaker\u2019svocaltract.Itthereforegivesastrong\nclueastowhetherthespeakerisaman,woman,orchild.\nHowever,formanytasks,itisdi\ufb03culttoknowwhatfeaturesshouldbeextracted.\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\nofawheelasafeature.Unfortunately,itisdi\ufb03culttodescribeexactlywhata\nwheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringo\ufb00\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\nobscuringpartofthewheel,andsoon.\n3", "CHAPTER1.INTRODUCTION\n\ue078\ue079\ue043 \ue061 \ue072 \ue074 \ue065 \ue073\ue069\ue061\ue06e\ue020 \ue063 \ue06f \ue06f \ue072 \ue064 \ue069 \ue06e \ue061 \ue074 \ue065 \ue073\n\ue072\ue0b5\ue050 \ue06f \ue06c \ue061 \ue072 \ue020 \ue063 \ue06f \ue06f \ue072 \ue064 \ue069 \ue06e \ue061 \ue074 \ue065 \ue073\nFigure1.1:Exampleofdi\ufb00erentrepresentations:supposewewanttoseparatetwo\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.\nOnesolutiontothisproblemistousemachinelearningtodiscovernotonly\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng.\u00a0Learnedrepresentations\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto\nmonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\nhumantimeande\ufb00ort;itcantakedecadesforanentirecommunityofresearchers.\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\nconvertstheinputdataintoadi\ufb00erentrepresentation,anda dec o derfunction\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun\nthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\nrepresentationhavevariousniceproperties.Di\ufb00erentkindsofautoencodersaimto\nachievedi\ufb00erentkindsofproperties.\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\ncontext,weusetheword\u201cfactors\u201dsimplytorefertoseparatesourcesofin\ufb02uence;\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\n4", "CHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthata\ufb00ectobservablequantities.\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker\u2019s\nage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\nandtheangleandbrightnessofthesun.\nAmajorsourceofdi\ufb03cultyinmanyreal-worldarti\ufb01cialintelligenceapplications\nisthatmanyofthefactorsofvariationin\ufb02uenceeverysinglepieceofdataweare\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\ntoblackatnight.Theshapeofthecar\u2019ssilhouettedependsontheviewingangle.\nMostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\nonesthatwedonotcareabout.\nOfcourse,itcanbeverydi\ufb03culttoextractsuchhigh-level,abstractfeatures\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker\u2019saccent,\ncanbeidenti\ufb01edonlyusingsophisticated,nearlyhuman-levelunderstandingof\nthedata.Whenitisnearlyasdi\ufb03culttoobtainarepresentationastosolvethe\noriginalproblem,representationlearningdoesnot,at\ufb01rstglance,seemtohelpus.\nD e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\nwhichareinturnde\ufb01nedintermsofedges.\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep\nnetworkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\napplicationofadi\ufb00erentmathematical functionasprovidinganewrepresentation\noftheinput.\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe\ncomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\ncanbethoughtofasthestateofthecomputer\u2019smemoryafterexecutinganother\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\ninstructionsinsequence.Sequentialinstructionso\ufb00ergreatpowerbecauselater\n5", "CHAPTER1.INTRODUCTION\nVisible\u00a0layer\n(input\u00a0pixels)1st\u00a0hidden\u00a0layer\n(edges)2nd\u00a0hidden\u00a0layer\n(corners\u00a0and\ncontours)3rd\u00a0hidden\u00a0layer\n(object\u00a0parts)CARPERSONANIMALOutput\n(object\u00a0identity)\nFigure1.2:Illustrationofadeeplearningmodel.Itisdi\ufb03cultforacomputertounderstand\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.\nDeeplearningresolvesthisdi\ufb03cultybybreakingthedesiredcomplicatedmappingintoa\nseriesofnestedsimplemappings,eachdescribedbyadi\ufb00erentlayerofthemodel.The\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\nfeaturesfromtheimage.Theselayersarecalled\u201chidden\u201dbecausetheirvaluesarenotgiven\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining\ntherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,the\ufb01rstlayercaneasily\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventhe\ufb01rsthidden\nlayer\u2019sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\nlayer\u2019sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer\ncandetectentirepartsofspeci\ufb01cobjects,by\ufb01ndingspeci\ufb01ccollectionsofcontoursand\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\nfromZeilerandFergus2014().\n6", "CHAPTER1.INTRODUCTION\nx 1 x 1\u03c3\nw 1 w 1\u00d7\nx 2 x 2 w 2 w 2\u00d7+El e me n t\nS e t\n+\n\u00d7\n\u03c3\nxx wwEl e me n t\nS e t\nL ogi s t i c\nR e gr e s s i onL ogi s t i c\nR e gr e s s i on\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\noutputbutdependsonthede\ufb01nitionofwhatconstitutesapossiblecomputationalstep.\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,\n\u03c3 ( wTx ),where\u03c3isthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\nviewofdeeplearning,notalloftheinformationinalayer\u2019sactivationsnecessarily\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores\nstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeci\ufb01cally,\nbutithelpsthemodeltoorganizeitsprocessing.\nTherearetwomainwaysofmeasuringthedepthofamodel.The\ufb01rstviewis\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough\na\ufb02owchartthatdescribeshowtocomputeeachofthemodel\u2019soutputsgiven\nitsinputs.Justastwoequivalentcomputerprogramswillhavedi\ufb00erentlengths\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\nbedrawnasa\ufb02owchartwithdi\ufb00erentdepthsdependingonwhichfunctionswe\nallowtobeusedasindividualstepsinthe\ufb02owchart.Figureillustrateshowthis 1.3\nchoiceoflanguagecangivetwodi\ufb00erentmeasurementsforthesamearchitecture.\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa\nmodelasbeingnotthedepthofthecomputational graphbutthedepthofthe\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n7", "CHAPTER1.INTRODUCTION\nofthe\ufb02owchartofthecomputations neededtocomputetherepresentationof\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\nThisisbecausethesystem\u2019sunderstandingofthesimplerconceptscanbere\ufb01ned\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\npresentaswell.\u00a0Inthiscase,thegraphofconceptsonlyincludestwolayers\u2014a\nlayerforeyesandalayerforfaces\u2014butthegraphofcomputations includes 2n\nlayersifwere\ufb01neourestimateofeachconceptgiventheothertimes. n\nBecauseitisnotalwaysclearwhichofthesetwoviews\u2014thedepthofthe\ncomputational graph,orthedepthoftheprobabilisticmodelinggraph\u2014ismost\nrelevant,andbecausedi\ufb00erentpeoplechoosedi\ufb00erentsetsofsmallestelements\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof\nacomputerprogram.\u00a0Nor isthereaconsensusabouthowmuchdepthamodel\nrequirestoqualifyas\u201cdeep.\u201dHowever,deeplearningcansafelyberegardedasthe\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\nSpeci\ufb01cally,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata.\u00a0Accordingtotheauthorsofthis\nbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\nkindofmachinelearningthatachievesgreatpowerand\ufb02exibilitybylearningto\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptde\ufb01nedin\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\noflessabstractones.Figureillustratestherelationshipbetweenthesedi\ufb00erent 1.4\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5\n1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\narebeginningacareerindeeplearningandarti\ufb01cialintelligenceresearch.The\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep\nlearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\n8", "CHAPTER1.INTRODUCTION\nAIMachine\u00a0learningRepresentation\u00a0learningDeep\u00a0learning\nExample:\nKnowledge\nbasesExample:\nLogistic\nregressionExample:\nShallow\nautoencoders Example:\nMLPs\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\n9", "CHAPTER1.INTRODUCTION\nInputHand-\ndesigned\u00a0\nprogramOutput\nInputHand-\ndesigned\u00a0\nfeaturesMapping\u00a0from\u00a0\nfeaturesOutput\nInputFeaturesMapping\u00a0from\u00a0\nfeaturesOutput\nInputSimple\u00a0\nfeaturesMapping\u00a0from\u00a0\nfeaturesOutput\nAdditional\u00a0\nlayers\u00a0of\u00a0more\u00a0\nabstract\u00a0\nfeatures\nRule-based\nsystemsClassic\nmachine\nlearning Representation\nlearningDeep\nlearning\nFigure1.5:\u00a0Flowchartsshowinghowthedi\ufb00erentpartsofanAIsystemrelatetoeach\notherwithindi\ufb00erentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\nlearnfromdata.\n1 0", "CHAPTER1.INTRODUCTION\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\nsearchengines,onlineadvertisingand\ufb01nance.\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III\nwidelybelievedtobeimportantforfutureresearchindeeplearning.\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\nchapterstoread,\ufb01gureprovidesa\ufb02owchartshowingthehigh-levelorganization 1.6\nofthebook.\nWedoassumethatallreaderscomefromacomputersciencebackground. We\nassumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory.\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\n\u2022Deeplearninghashadalongandrichhistory,buthasgonebymanynames\nre\ufb02ectingdi\ufb00erentphilosophicalviewpoints,andhaswaxedandwanedin\npopularity.\n\u2022Deeplearninghasbecomemoreusefulastheamountofavailabletraining\ndatahasincreased.\n\u2022Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n(bothhardwareandsoftware)fordeeplearninghasimproved.\n\u2022Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\naccuracyovertime.\n1 1", "CHAPTER1.INTRODUCTION\n1.\u00a0Introduction\nPart\u00a0I:\u00a0Applied\u00a0Math\u00a0and\u00a0Machine\u00a0Learning\u00a0Basics\n2.\u00a0Linear\u00a0Algebra3.\u00a0Probability\u00a0and\u00a0\nInformation\u00a0Theory\n4.\u00a0Numerical\u00a0\nComputation5.\u00a0Machine\u00a0Learning\u00a0\nBasics\nPart\u00a0II:\u00a0Deep\u00a0Networks:\u00a0Modern\u00a0Practices\n6.\u00a0Deep\u00a0Feedforward\u00a0\nNetworks\n7.\u00a0Regularization8.\u00a0Optimization 9.\u00a0\u00a0CNNs10.\u00a0\u00a0RNNs\n11.\u00a0Practical\u00a0\nMethodology12.\u00a0Applications\nPart\u00a0III:\u00a0Deep\u00a0Learning\u00a0Research\n13.\u00a0Linear\u00a0Factor\u00a0\nModels14.\u00a0Autoencoders15.\u00a0Representation\u00a0\nLearning\n16.\u00a0Structured\u00a0\nProbabilistic\u00a0Models17.\u00a0Monte\u00a0Carlo\u00a0\nMethods\n18.\u00a0Partition\u00a0\nFunction19.\u00a0Inference\n20.\u00a0Deep\u00a0Generative\u00a0\nModels\nFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\nindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\n1 2", "CHAPTER1.INTRODUCTION\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\nw o rks\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\nexcitingnewtechnology,andaresurprisedtoseeamentionof\u201chistory\u201dinabook\naboutanemerging\ufb01eld.Infact,deeplearningdatesbacktothe1940s.Deep\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany\ndi\ufb00erentnames,andhasonlyrecentlybecomecalled\u201cdeeplearning.\u201dThe\ufb01eld\nhasbeenrebrandedmanytimes,re\ufb02ectingthein\ufb02uenceofdi\ufb00erentresearchers\nanddi\ufb00erentperspectives.\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\nlearning\u00a0known\u00a0as c y b e r net i c sin\u00a0the\u00a01940s\u20131960s,\u00a0deep\u00a0learning\u00a0knownas\nc o nnec t i o n i s minthe1980s\u20131990s,andthecurrentresurgenceunderthename\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedin\ufb01gure.1.7\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\nhappensorcouldhappeninthebrain.\u00a0Asaresult,oneofthenamesthatdeep\nlearninghasgonebyis ar t i \ufb01c i al neur al net w o r k s(ANNs).Thecorresponding\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired\nbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe\ncomputational principlesbehindthebrainandduplicateitsfunctionality.Another\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\nlightonthesebasicscienti\ufb01cquestionsareusefulapartfromtheirabilitytosolve\nengineeringapplications.\nThemodernterm\u201cdeeplearning\u201dgoesbeyondtheneuroscienti\ufb01cperspective\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral\nprincipleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\nlearningframeworksthatarenotnecessarilyneurallyinspired.\n1 3", "CHAPTER1.INTRODUCTION\n1940 1950 1960 1970 1980 1990 2000\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\nc y b e r n e t i c s\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\nFigure1.7:The\ufb01gureshowstwoofthethreehistoricalwavesofarti\ufb01cialneuralnets\nresearch,asmeasuredbythefrequencyofthephrases\u201ccybernetics\u201dand\u201cconnectionism\u201dor\n\u201cneuralnetworks\u201daccordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The\n\ufb01rstwavestartedwithcyberneticsinthe1940s\u20131960s, withthedevelopmentoftheories\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\nthe\ufb01rstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980\u20131995period,\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton\ne t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\nthecorrespondingscienti\ufb01cactivityoccurred.\n1 4", "CHAPTER1.INTRODUCTION\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\nmotivatedfromaneuroscienti\ufb01cperspective.Thesemodelsweredesignedto\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\nf ( x w, ) =x 1w 1 + \u00b7 \u00b7 \u00b7 +x nw n.This\ufb01rstwaveofneuralnetworksresearchwas\nknownascybernetics,asillustratedin\ufb01gure.1.7\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943\nofbrainfunction.Thislinearmodelcouldrecognizetwodi\ufb00erentcategoriesof\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\ntocorrespondtothedesiredde\ufb01nitionofthecategories,theweightsneededtobe\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.\u00a0Inthe1950s,\ntheperceptron(Rosenblatt19581962,,)becamethe\ufb01rstmodelthatcouldlearn\ntheweightsde\ufb01ningthecategoriesgivenexamplesofinputsfromeachcategory.\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame\ntime,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\nandHo\ufb001960,),andcouldalsolearntopredictthesenumbersfromdata.\nThesesimplelearningalgorithmsgreatlya\ufb00ectedthemodernlandscapeofma-\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\nmodi\ufb01edversionsofthestochasticgradientdescentalgorithmremainthedominant\ntrainingalgorithmsfordeeplearningmodelstoday.\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\nlearningmodels,thoughinmanycasestheyare t r a i ne dindi\ufb00erentwaysthanthe\noriginalmodelsweretrained.\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0\nandf ( [ 0, 0], w ) = 0.Criticswhoobservedthese\ufb02awsinlinearmodelscaused\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n1969).Thiswasthe\ufb01rstmajordipinthepopularityofneuralnetworks.\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\nlearningresearchers,butitisnolongerthepredominant guideforthe\ufb01eld.\nThemainreasonforthediminishedrole\u00a0ofneuroscienceindeeplearning\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain\ntouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n1 5", "CHAPTER1.INTRODUCTION\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\ncansolvemanydi\ufb00erenttasks.Neuroscientistshavefoundthatferretscanlearnto\n\u201csee\u201dwiththeauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\ndi\ufb00erenttasksthatthebrainsolves.Beforethishypothesis,machinelearning\nresearchwasmorefragmented,withdi\ufb00erentcommunitiesofresearchersstudying\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)\nintroducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\nthe r e c t i \ufb01ed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\nfunction.Thesimpli\ufb01edmodernversionwasdevelopedincorporatingideasfrom\nmanyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\nneuroscienceasanin\ufb02uence,and ()citingmoreengineering- Jarrett e t a l .2009\norientedin\ufb02uences.Whileneuroscienceisanimportantsourceofinspiration,it\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\ndi\ufb00erentfunctionsthanmodernrecti\ufb01edlinearunits,butgreaterneuralrealism\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while\nneurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\ndonotyetknowenoughaboutbiologicallearningforneurosciencetoo\ufb00ermuch\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\nin\ufb02uencethanresearchersworkinginothermachinelearning\ufb01eldssuchaskernel\nmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommany\ufb01elds,\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n1 6", "CHAPTER1.INTRODUCTION\nneuroscienceatall.\nItis\u00a0worth\u00a0notingthat\u00a0thee\ufb00orttounderstandhowthe\u00a0brainworkson\nan\u00a0algorithmic\u00a0lev el\u00a0is\u00a0alive\u00a0andwell.This\u00a0endeavor\u00a0is\u00a0primarily\u00a0knownas\n\u201ccomputational neuroscience\u201dandisaseparate\ufb01eldofstudyfromdeeplearning.\nItiscommonforresearcherstomovebackandforthbetweenboth\ufb01elds.The\n\ufb01eldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\nthatareabletosuccessfullysolvetasksrequiringintelligence,whilethe\ufb01eldof\ncomputational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\nmodelsofhowthebrainactuallyworks.\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\ni ng( ,; ,).\u00a0Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\ntounderstandingthemind,combiningmultipledi\ufb00erentlevelsofanalysis.During\ntheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\nDespitetheirpopularity,symbolicmodelsweredi\ufb03culttoexplainintermsof\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\nThecentralideainconnectionism isthatalargenumberofsimplecomputational\nunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\ncomputational models.\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotoday\u2019sdeeplearning.\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany\npossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\nbird,greentruck,andsoon.Thisrequiresninedi\ufb00erentneurons,andeachneuron\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons\ndescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n1 7", "CHAPTER1.INTRODUCTION\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\nofonespeci\ufb01ccategoryofobjects.\u00a0Theconceptofdistributedrepresentationis\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart\ne t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\nwithneuralnetworks.()and ()identi\ufb01edsomeof Hochreiter1991Bengio e t a l .1994\nthefundamentalmathematical di\ufb03cultiesinmodelinglongsequences,describedin\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\nmemoryorLSTMnetworktoresolvesomeofthesedi\ufb03culties.Today,theLSTM\niswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\nprocessingtasksatGoogle.\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotful\ufb01ll\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\nother\ufb01eldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .\n1992CortesandVapnik1995Sch\u00f6lkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.\nThisprogramunitedmachinelearningresearchgroupsledbyGeo\ufb00reyHinton\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\nandcomputervision.\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydi\ufb03cult\ntotrain.\u00a0Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat\nthesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\nwiththehardwareavailableatthetime.\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\n1 8", "CHAPTER1.INTRODUCTION\n2006.Geo\ufb00reyHintonshowedthatakindofneuralnetworkcalledadeepbelief\nnetworkcouldbee\ufb03cientlytrainedusingastrategycalledgreedylayer-wisepre-\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\nTheotherCIFAR-a\ufb03liatedresearchgroupsquicklyshowedthatthesamestrategy\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on\ntestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\nterm\u201cdeeplearning\u201dtoemphasizethatresearcherswerenowabletotraindeeper\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity\nofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\nmodelstoleveragelargelabeleddatasets.\n1 . 2 . 2 In creasin g D a t a s et S i zes\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa\ncrucialtechnologythoughthe\ufb01rstexperimentswitharti\ufb01cialneuralnetworkswere\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks\ntodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,\nmoreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n1 9", "CHAPTER1.INTRODUCTION\nintoadatasetappropriateformachinelearningapplications.Theageof\u201cBig\nData\u201dhasmademachinelearningmucheasierbecausethekeyburdenofstatistical\nestimation\u2014generalizingwelltonewdataafterobservingonlyasmallamount\nofdata\u2014hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10\nmillionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\nlearning.\n1 . 2 . 3 In creasin g Mo d el S i zes\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-\nismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\nBiologicalneuronsarenotespeciallydenselyconnected.Asseenin\ufb01gure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\nsmalluntilquiterecently,asshownin\ufb01gure.Sincetheintroductionofhidden 1.11\nunits,arti\ufb01cialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\nallowfasterscaling,arti\ufb01cialneuralnetworkswillnothavethesamenumberof\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\nrepresentmorecomplicatedfunctionsthancurrentarti\ufb01cialneurons,sobiological\nneuralnetworksmaybeevenlargerthanthisplotportrays.\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\nneuronsthanaleechwereunabletosolvesophisticatedarti\ufb01cialintelligenceprob-\nlems.Eventoday\u2019snetworks,whichweconsiderquitelargefromacomputational\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\nprimitivevertebrateanimalslikefrogs.\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n2 0", "CHAPTER1.INTRODUCTION\n1900 1950 198520002015\nYear100101102103104105106107108109Datasetsize(numberexamples)\nIrisMNISTPublicSVHN\nImageNet\nCIFAR-10ImageNet10k\nILSVRC  2014Sports-1M\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\nWMT\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers\nofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\ndemonstratethatneuralnetworkswereabletolearnspeci\ufb01ckindsoffunctions(Widrow\nandHo\ufb001960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\nofthousandsofexamplessuchastheMNISTdataset(shownin\ufb01gure)ofscans 1.9\nofhandwrittennumbers( ,).Inthe\ufb01rstdecadeofthe2000s,more LeCun e t a l .1998b\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\nthe\ufb01rsthalfofthe2010s,signi\ufb01cantlylargerdatasets,containinghundredsofthousands\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .\n2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBM\u2019sdatasetconstructed\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\n2 1", "CHAPTER1.INTRODUCTION\nFigure1.9:ExampleinputsfromtheMNISTdataset.The\u201cNIST\u201dstandsforNational\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\nThe\u201cM\u201dstandsfor\u201cmodi\ufb01ed,\u201dsincethedatahasbeenpreprocessedforeasierusewith\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\nandassociatedlabelsdescribingwhichdigit0\u20139iscontainedineachimage.Thissimple\nclassi\ufb01cationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning\nresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\nGeo\ufb00reyHintonhasdescribeditas\u201cthe d r o s o p h i l aofmachinelearning,\u201dmeaningthat\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\nconditions,muchasbiologistsoftenstudyfruit\ufb02ies.\n2 2", "CHAPTER1.INTRODUCTION\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\nexpectedtocontinuewellintothefuture.\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide\naccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\nappliedwithsuccesstobroaderandbroadersetsofapplications.\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized\n( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000di\ufb00erent\ncategoriesofobjects.\u00a0ThelargestcontestinobjectrecognitionistheImageNet\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\nwonthischallengeforthe\ufb01rsttimeandbyawidemargin,bringingdownthe\nstate-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\nforeachimageandthecorrectcategoryappearedinthe\ufb01rst\ufb01veentriesofthis\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\nasshownin\ufb01gure.1.12\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\nthishistoryinmoredetailinsection.12.3\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand\nimagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\n2013)andyieldedsuperhumanperformanceintra\ufb03csignclassi\ufb01cation(Ciresan\n2 3", "CHAPTER1.INTRODUCTION\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\n12\n34\n567\n89\n1 0\nF r ui t \ufb02yMo useC a tH um a n\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinarti\ufb01cialneural\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\nneuronsismostlyadesignconsideration.Somearti\ufb01cialneuralnetworkshavenearlyas\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks\ntohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\nnetworksizesfrom (). Wikipedia2015\n1.Adaptivelinearelement( ,) WidrowandHo\ufb001960\n2.Neocognitron(Fukushima1980,)\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n7.Distributedautoencoder(,) Le e t al.2012\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n10.GoogLeNet( ,) Szegedy e t al.2014a\n2 4", "CHAPTER1.INTRODUCTION\ne t a l .,).2012\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\nelementsofthesequence( ,).Recurrentneuralnetworks, G\u00fcl\u00e7ehreandBengio2013\nsuchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjust\ufb01xedinputs.\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015).\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn\ntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\nfuturecouldinprinciplebeappliedtonearlyanytask.\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous\nagentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\nalsosigni\ufb01cantlyimprovedtheperformanceofreinforcementlearningforrobotics\n(,). Finn e t a l .2015\nManyoftheseapplicationsofdeeplearningarehighlypro\ufb01table.Deeplearning\nisnowused\u00a0bymanytoptechnologycompanies\u00a0includi ngGoogle,\u00a0Microsoft,\nFacebook,IBM,Baidu,Apple,Adobe,Net\ufb02ix,NVIDIAandNEC.\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\nDistBelief(,),Ca\ufb00e(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015\nTensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\ncommercialproducts.\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\n2 5", "CHAPTER1.INTRODUCTION\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\nscienti\ufb01c\ufb01elds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-\nBarley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescienti\ufb01c\n\ufb01eldsinthefuture.\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand\nbringittonewfrontiers.\n2 6", "CHAPTER1.INTRODUCTION\n1950 198520002015 2056\nYear10\u2212 210\u2212 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\n123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure1.11:Sincetheintroductionofhiddenunits,arti\ufb01cialneuralnetworkshavedoubled\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\n1.Perceptron(,,) Rosenblatt19581962\n2.Adaptivelinearelement( ,) WidrowandHo\ufb001960\n3.Neocognitron(Fukushima1980,)\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\n7.Mean\ufb01eldsigmoidbeliefnetwork(,) Saul e t al.1996\n8.LeNet-5( ,) LeCun e t al.1998b\n9.Echostatenetwork( ,) JaegerandHaas2004\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n16.OMP-1network( ,) CoatesandNg2011\n17.Distributedautoencoder(,) Le e t al.2012\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n20.GoogLeNet( ,) Szegedy e t al.2014a\n2 7", "CHAPTER1.INTRODUCTION\n2010 2011 2012 2013 2014 2015\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classi\ufb01cationerrorrate\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\neveryyear,andyieldedlowerandlowererrorrateseachtime.\u00a0DatafromRussakovsky\ne t a l . e t a l . ()and2014b He().2015\n2 8", "P a rt I\nAppliedMathandMachine\nLearningBasics\n29", "This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\nallo w us t o de\ufb01ne f unctions of many v ariables , \ufb01 nd t he highes t and low e s t p oints\non t hes e f unctions and q uantify degrees of b e lief.\nN e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how\nt o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction.\nThis e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep.\u00a0In t he\ns ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\nf r amew ork.\n3 0", "C h a p t e r 2\nL i n e ar A l ge b ra\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\nthekeylinearalgebraprerequisites.\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas\nShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\ntopicsthatarenotessentialforunderstandingdeeplearning.\n2.1Scalars,Vectors,MatricesandTensors\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\n\u2022Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\n31", "CHAPTER2.LINEARALGEBRA\nexample,wemightsay\u201cLet s\u2208 Rbetheslopeoftheline,\u201dwhilede\ufb01ninga\nreal-valuedscalar,or\u201cLet n\u2208 Nbethenumberofunits,\u201dwhilede\ufb01ninga\nnaturalnumberscalar.\n\u2022Vectors:\u00a0Avectorisanarrayofnumbers.Thenumbersarearrangedin\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\nasx.Theelementsofthevectorareidenti\ufb01edbywritingitsnameinitalic\ntypeface,withasubscript.The\ufb01rstelementofxis x 1,thesecondelement\nis x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\nwewritethemasacolumnenclosedinsquarebrackets:\nx=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0x 1\nx 2\n...\nx n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb. (2.1)\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\ngivingthecoordinatealongadi\ufb00erentaxis.\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we\nde\ufb01neasetcontainingtheindicesandwritethesetasasubscript.For\nexample,toaccess x 1, x 3and x 6,wede\ufb01netheset S={1 ,3 ,6}andwrite\nx S.Weusethe\u2212signtoindexthecomplementofaset.Forexamplex \u2212 1is\nthevectorcontainingallelementsofxexceptfor x 1,andx \u2212 Sisthevector\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\n\u2022Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidenti\ufb01ed\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas\naheightof mandawidthof n,thenwesaythatA\u2208 Rm n \u00d7.\u00a0Weusually\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\nofthenumberswithverticalcoordinate ibywritinga\u201c\u201dforthehorizontal :\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\n3 2", "CHAPTER2.LINEARALGEBRA\nA =\uf8ee\n\uf8f0A 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\nA 3 1 , A 3 2 ,\uf8f9\n\uf8fb \u21d2 A\ue021=\ue025A 1 1 , A 2 1 , A 3 1 ,\nA 1 2 , A 2 2 , A 3 2 ,\ue026\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\nmaindiagonal.\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\n\ue014A 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\ue015\n. (2.2)\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust\nasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\nofthematrixcomputedbyapplyingthefunctionto. fA\n\u2022Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamed\u201cA\u201d\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\nbywriting A i , j , k.\nOneimportantoperationonmatricesisthetranspose.\u00a0Thetransposeofa\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\n\ufb01gureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\nmatrixasAA\ue03e,anditisde\ufb01nedsuchthat\n(A\ue03e) i , j= A j , i . (2.3)\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n3 3", "CHAPTER2.LINEARALGEBRA\nde\ufb01neavectorbywritingoutitselementsinthetextinlineasarowmatrix,\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\nx= [ x 1 , x 2 , x 3]\ue03e.\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\ncanseethatascalarisitsowntranspose: a a= \ue03e.\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just\nbyperformingthatoperationoneachelementofamatrix:D= a\u00b7B+ cwhere\nD i , j= a B\u00b7 i , j+ c.\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\nmatrix.Thisshorthandeliminatestheneedtode\ufb01neamatrixwithbcopiedinto\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\niscalled .broadcasting\n2.2MultiplyingMatricesandVectors\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\norderforthisproducttobede\ufb01ned,Amusthavethesamenumberofcolumnsas\nBhasrows.IfAisofshape m n\u00d7andBisofshape n p\u00d7,thenCisofshape\nm p\u00d7.Wecanwritethematrixproductjustbyplacingtwoormorematrices\ntogether,e.g.\nCAB= . (2.4)\nTheproductoperationisde\ufb01nedby\nC i , j=\ue058\nkA i , k B k, j . (2.5)\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\ue00c\nThedotproductbetweentwovectorsxandyofthesamedimensionality\nisthematrixproductx\ue03ey.WecanthinkofthematrixproductC=ABas\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\n3 4", "CHAPTER2.LINEARALGEBRA\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\nanalysis\u00a0ofmatrices\u00a0moreconvenient.For\u00a0example,\u00a0matrix\u00a0m ultiplication\u00a0is\ndistributive:\nABCABAC (+) = + . (2.6)\nItisalsoassociative:\nABCABC ( ) = ( ) . (2.7)\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\nvectorsiscommutative:\nx\ue03eyy= \ue03ex . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )AB\ue03e= B\ue03eA\ue03e. (2.9)\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\nx\ue03ey=\ue010\nx\ue03ey\ue011\ue03e\n= y\ue03ex . (2.10)\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\nthereadershouldbeawarethatmanymoreexist.\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\nequations:\nAxb= (2.11)\nwhereA\u2208 Rm n \u00d7isaknownmatrix,b\u2208 Rmisaknownvector,andx\u2208 Rnisa\nvectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\nconstraint.Wecanrewriteequationas:2.11\nA 1 : ,x= b 1 (2.12)\nA 2 : ,x= b 2 (2.13)\n. . . (2.14)\nA m , :x= b m (2.15)\nor,evenmoreexplicitly,as:\nA 1 1 , x 1+A 1 2 , x 2+ +\u00b7\u00b7\u00b7A 1 , n x n= b 1 (2.16)\n3 5", "CHAPTER2.LINEARALGEBRA\n\uf8ee\n\uf8f0100\n010\n001\uf8f9\n\uf8fb\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\nA 2 1 , x 1+A 2 2 , x 2+ +\u00b7\u00b7\u00b7A 2 , n x n= b 2 (2.17)\n. . . (2.18)\nA m , 1 x 1+A m , 2 x 2+ +\u00b7\u00b7\u00b7A m , n x n= b m . (2.19)\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\nequationsofthisform.\n2.3IdentityandInverseMatrices\nLinearalgebrao\ufb00ersapowerfultoolcalledmatrixinversionthatallowsusto\nanalyticallysolveequationformanyvaluesof. 2.11 A\nTodescribematrixinversion,we\ufb01rstneedtode\ufb01netheconceptofanidentity\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\nn-dimensionalvectorsasI n.Formally,I n\u2208 Rn n \u00d7,and\n\u2200\u2208x Rn,I nxx= . (2.20)\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\ndiagonalare1,whilealloftheotherentriesarezero.See\ufb01gureforanexample.2.2\nThematrixinverseofAisdenotedasA\u2212 1,anditisde\ufb01nedasthematrix\nsuchthat\nA\u2212 1AI= n . (2.21)\nWecannowsolveequationbythefollowingsteps: 2.11\nAxb= (2.22)\nA\u2212 1AxA= \u2212 1b (2.23)\nI nxA= \u2212 1b (2.24)\n3 6", "CHAPTER2.LINEARALGEBRA\nxA= \u2212 1b . (2.25)\nOfcourse,thisprocessdependsonitbeingpossibleto\ufb01ndA\u2212 1.Wediscuss\ntheconditionsfortheexistenceofA\u2212 1inthefollowingsection.\nWhenA\u2212 1exists,severaldi\ufb00erentalgorithmsexistfor\ufb01ndingitinclosedform.\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\ntimesfordi\ufb00erentvaluesofb.However,A\u2212 1isprimarilyusefulasatheoretical\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.\nBecauseA\u2212 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\nestimatesof.x\n2.4LinearDependenceandSpan\nInorderforA\u2212 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\nsolutionsorin\ufb01nitelymanysolutionsforsomevaluesofb.\u00a0Itisnotpossibleto\nhavemorethanonebutlessthanin\ufb01nitelymanysolutionsforaparticularb;if\nbothandaresolutionsthen xy\nzxy = \u03b1+(1 )\u2212 \u03b1 (2.26)\nisalsoasolutionforanyreal. \u03b1\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\nofAasspecifyingdi\ufb00erentdirectionswecantravelfromtheorigin(thepoint\nspeci\ufb01edbythevectorofallzeros),anddeterminehowmanywaysthereareof\nreachingb.Inthisview,eachelementofxspeci\ufb01eshowfarweshouldtravelin\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\ncolumn: i\nAx=\ue058\nix iA : , i . (2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\neachvectorv( ) ibyacorrespondingscalarcoe\ufb03cientandaddingtheresults:\n\ue058\nic iv( ) i. (2.28)\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\noftheoriginalvectors.\n3 7", "CHAPTER2.LINEARALGEBRA\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\nspacerangeortheof.A\nInorderforthesystemAx=btohaveasolutionforallvaluesofb\u2208 Rm,\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\nimmediately thatAmusthaveatleast mcolumns,i.e., n m\u2265.\u00a0Otherwise, the\ndimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\n3\u00d72matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\nifandonlyifliesonthatplane.b\nHaving n m\u2265isonlyanecessaryconditionforeverypointtohaveasolution.\nItisnotasu\ufb03cientcondition,becauseitispossibleforsomeofthecolumnsto\nberedundant.Considera2\u00d72matrixwherebothofthecolumnsareidentical.\nThishasthesamecolumnspaceasa2\u00d71matrixcontainingonlyonecopyofthe\nreplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\nencompassallof R2,eventhoughtherearetwocolumns.\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors.\u00a0Ifweaddavectortoasetthatisalinearcombinationof\ntheothervectorsintheset,thenewvectordoesnotaddanypointstotheset\u2019s\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,\nthematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\nconditionisbothnecessaryandsu\ufb03cientforequationtohaveasolutionfor 2.11\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\nmcolumnsmayhavemorethanonesuchset.\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat\nequationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\nwayofparametrizing eachsolution.\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\nwithlinearlydependentcolumnsisknownas.singular\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\nequation.However,wecannotusethemethodofmatrixinversionto\ufb01ndthe\n3 8", "CHAPTER2.LINEARALGEBRA\nsolution.\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\nalsopossibletode\ufb01neaninversethatismultipliedontheright:\nAA\u2212 1= I . (2.29)\nForsquarematrices,theleftinverseandrightinverseareequal.\n2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=\ue020\ue058\ni| x i|p\ue021 1\np\n(2.30)\nfor p , p . \u2208 R\u22651\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatis\ufb01es\nthefollowingproperties:\n\u2022 \u21d2 f() = 0 xx= 0\n\u2022 \u2264 f(+) xy f f ()+x ()y(thetriangleinequality)\n\u2022\u2200\u2208 || \u03b1 R , f \u03b1(x) = \u03b1 f()x\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\nEuclideandistancefromtheorigintothepointidenti\ufb01edbyx.The L2normis\nusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\nthesquared L2norm,whichcanbecalculatedsimplyasx\ue03ex.\nThesquared L2normismoreconvenienttoworkwithmathematically and\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend\nontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\n3 9", "CHAPTER2.LINEARALGEBRA\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\nthe L1norm.The L1normmaybesimpli\ufb01edto\n||||x 1=\ue058\ni| x i| . (2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthedi\ufb00erencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the \ue00f L1normincreasesby. \ue00f\nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\nelements.Someauthorsrefertothisfunctionasthe\u201c L0norm,\u201dbutthisisincorrect\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\nscalingthevectorby \u03b1doesnotchangethenumberofnonzeroentries.\u00a0The L1\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\nOneothernormthatcommonlyarisesinmachinelearningisthe L\u221enorm,\nalsoknownasthemaxnorm.Thisnormsimpli\ufb01estotheabsolutevalueofthe\nelementwiththelargestmagnitudeinthevector,\n||||x \u221e= max\ni| x i| . (2.32)\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\nFrobeniusnorm:\n|||| A F=\ue073\ue058\ni , jA2\ni , j , (2.33)\nwhichisanalogoustothe L2normofavector.\nThedotproductoftwovectorscanberewrittenintermsofnorms.Speci\ufb01cally,\nx\ue03eyx= |||| 2||||y 2cos \u03b8 (2.34)\nwhereistheanglebetweenand. \u03b8 xy\n2.6SpecialKindsofMatricesandVectors\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\nthemaindiagonal.\u00a0Formally,amatrixDisdiagonalifandonlyif D i , j=0for\n4 0", "CHAPTER2.LINEARALGEBRA\nall i\ue036= j.\u00a0Wehavealreadyseenoneexampleofadiagonalmatrix:\u00a0theidentity\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\nisverycomputationally e\ufb03cient.Tocomputediag(v)x,weonlyneedtoscaleeach\nelement x iby v i.Inotherwords,diag(v)x=vx\ue00c.Invertingasquarediagonal\nmatrixisalsoe\ufb03cient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\nandinthatcase,diag(v)\u2212 1=diag([1 /v 1 , . . . ,1 /v n]\ue03e).Inmanycases,wemay\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\nmatricestobediagonal.\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill\npossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\nelementsofthevectorifiswiderthanitistall. D\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\nAA= \ue03e. (2.35)\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,\nifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\nA isavectorwith : unitvectorunitnorm\n||||x 2= 1 . (2.36)\nAvectorxandavectoryareorthogonaltoeachotherifx\ue03ey= 0.Ifboth\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\northonormal.\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\nmalandwhosecolumnsaremutuallyorthonormal:\nA\ue03eAAA= \ue03e= I . (2.37)\n4 1", "CHAPTER2.LINEARALGEBRA\nThisimpliesthat\nA\u2212 1= A\ue03e, (2.38)\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\nPaycarefulattentiontothede\ufb01nitionoforthogonalmatrices.Counterintuitively,\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\n2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,or\ufb01ndingsomepropertiesofthemthatareuniversal,notcaused\nbythewaywechoosetorepresentthem.\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\norinbinary,butitwillalwaysbetruethat12 = 2\u00d72\u00d73.Fromthisrepresentation\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\nintegermultipleofwillbedivisibleby. 12 3\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat\nshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\nrepresentationofthematrixasanarrayofelements.\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\neigenvalues.\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv= \u03bb . (2.39)\nThescalar \u03bbisknownastheeigenvaluecorrespondingtothiseigenvector.(One\ncanalso\ufb01ndalefteigenvectorsuchthatv\ue03eA= \u03bbv\ue03e,\u00a0butweareusually\nconcernedwithrighteigenvectors).\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s \u2208 R\ue036= 0.\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\nforuniteigenvectors.\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues { \u03bb 1 , . . . , \u03bb n}.Wemayconcatenateallofthe\n4 2", "CHAPTER2.LINEARALGEBRA\n\u0000\ue033 \u0000\ue032 \u0000\ue031 \ue030 \ue031 \ue032 \ue033\n\ue078\ue030\u0000\ue033\u0000\ue032\u0000\ue031\ue030\ue031\ue032\ue033\ue078\ue031\ue076\ue028\ue031 \ue029\n\ue076\ue028\ue032 \ue029\ue042 \ue065 \ue066 \ue06f \ue072 \ue065 \ue020 \ue06d \ue075 \ue06c \ue074 \ue069 \ue070 \ue06c \ue069 \ue063 \ue061 \ue074 \ue069 \ue06f \ue06e\n\u0000\ue033 \u0000\ue032 \u0000\ue031 \ue030 \ue031 \ue032 \ue033\n\ue078\ue030\n\ue030\u0000\ue033\u0000\ue032\u0000\ue031\ue030\ue031\ue032\ue033\ue078\ue030\n\ue031\ue076\ue028\ue031 \ue029\ue0b8\ue031 \ue076\ue028\ue031 \ue029\n\ue076\ue028\ue032 \ue029\ue0b8\ue032\ue076\ue028\ue032 \ue029\ue041 \ue066 \ue074 \ue065 \ue072 \ue020 \ue06d \ue075 \ue06c \ue074 \ue069 \ue070 \ue06c \ue069 \ue063 \ue061 \ue074 \ue069 \ue06f \ue06e\ue045 \ue066 \ue066 \ue065 \ue063 \ue074 \ue020 \ue06f\ue066 \ue020 \ue065 \ue069 \ue067 \ue065 \ue06e \ue076 \ue065 \ue063 \ue074 \ue06f\ue072 \ue073 \ue020 \ue061\ue06e \ue064 \ue020 \ue065 \ue069 \ue067\ue065 \ue06e \ue076 \ue061\ue06c \ue075 \ue065 \ue073\nFigure2.3:Anexampleofthee\ufb00ectofeigenvectorsandeigenvalues.Here,wehave\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue \u03bb 1andv( 2 )with\neigenvalue \u03bb 2. ( L e f t )Weplotthesetofallunitvectorsu\u2208 R2asaunitcircle. ( R i g h t )We\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\ncanseethatitscalesspaceindirectionv( ) iby \u03bb i.\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavector\u03bb= [ \u03bb 1 , . . . ,\n\u03bb n]\ue03e.The ofisthengivenby eigendecompositionA\nAV\u03bbV = diag()\u2212 1. (2.40)\nWehaveseenthatconstructingmatriceswithspeci\ufb01ceigenvaluesandeigenvec-\ntorsallowsustostretchspaceindesireddirections.\u00a0Ho wever,weoftenwantto\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3", "CHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspeci\ufb01cclassof\nmatricesthathaveasimpledecomposition.Speci\ufb01cally,everyrealsymmetric\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\nandeigenvalues:\nAQQ = \u039b\ue03e, (2.41)\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and \u039bisa\ndiagonalmatrix.Theeigenvalue\u039b i , iisassociatedwiththeeigenvectorincolumn i\nofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\nscalingspaceby \u03bb iindirectionv( ) i.See\ufb01gureforanexample.2.3\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof \u039b\nindescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\nifalloftheeigenvaluesareunique.\nTheeigendecompositionof\u00a0amatrix\u00a0tellsus\u00a0many\u00a0usefulfactsabout\u00a0the\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\nquadraticexpressionsoftheform f(x) =x\ue03eAxsubjectto||||x 2= 1.Wheneverx\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.\nThemaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivede\ufb01nite.A\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemide\ufb01-\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativede\ufb01nite,and\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemide\ufb01nite.Positive\nsemide\ufb01nitematricesareinterestingbecausetheyguaranteethat\u2200xx ,\ue03eAx\u22650.\nPositivede\ufb01nitematricesadditionallyguaranteethatx\ue03eAxx = 0 \u21d2 = 0.\n2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\n4 4", "CHAPTER2.LINEARALGEBRA\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\nexample,ifamatrixisnotsquare,theeigendecompositionisnotde\ufb01ned,andwe\nmustuseasingularvaluedecompositioninstead.\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\namatrixVofeigenvectorsandavectorofeigenvalues\u03bbsuchthatwecanrewrite\nAas\nAV\u03bbV = diag()\u2212 1. (2.42)\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA\nasaproductofthreematrices:\nAUDV = \ue03e. (2.43)\nSupposethatAisan m n\u00d7matrix.ThenUisde\ufb01nedtobean m m\u00d7matrix,\nD V tobeanmatrix,and m n\u00d7 tobeanmatrix. n n\u00d7\nEachofthesematricesisde\ufb01nedtohaveaspecialstructure.ThematricesU\nandVarebothde\ufb01nedtobeorthogonalmatrices.ThematrixDisde\ufb01nedtobe\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\ncolumnsofareknownasasthe V right-singularvectors.\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\neigenvectorsofAA\ue03e.Theright-singularvectorsofAaretheeigenvectorsofA\ue03eA.\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofA\ue03eA.\nThesameistrueforAA\ue03e.\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection.\n2.9TheMoore-PenrosePseudoinverse\nMatrixinversionisnotde\ufb01nedformatricesthatarenotsquare.Supposewewant\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\nAxy= (2.44)\n4 5", "CHAPTER2.LINEARALGEBRA\nbyleft-multiplyingeachsidetoobtain\nxBy= . (2.45)\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\nuniquemappingfromto.AB\nIfAistallerthanitiswide,\u00a0thenitispossibleforthisequationtohave\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\nsolutions.\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisde\ufb01nedasamatrix A\nA+=lim\n\u03b1 \ue026 0(A\ue03eAI+ \u03b1)\u2212 1A\ue03e. (2.46)\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisde\ufb01ni-\ntion,butrathertheformula\nA+= VD+U\ue03e, (2.47)\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\nelementsthentakingthetransposeoftheresultingmatrix.\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\npseudoinverseprovidesoneofthemanypossiblesolutions.Speci\ufb01cally,itprovides\nthesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\nsolutions.\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\npossibletointermsofEuclideannorm y ||\u2212||Axy 2.\n2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =A\ue058\niA i , i . (2.48)\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\ndi\ufb03culttospecifywithoutresortingtosummationnotationcanbespeci\ufb01edusing\n4 6", "CHAPTER2.LINEARALGEBRA\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\nanalternativewayofwritingtheFrobeniusnormofamatrix:\n|||| A F=\ue071\nTr(AA\ue03e) . (2.49)\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\nmanipulatetheexpressionusingmanyusefulidentities.\u00a0Forexample,thetrace\noperatorisinvarianttothetransposeoperator:\nTr() = Tr(AA\ue03e) . (2.50)\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto\nmovingthelastfactorintothe\ufb01rstposition,iftheshapesofthecorresponding\nmatricesallowtheresultingproducttobede\ufb01ned:\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\normoregenerally,\nTr(n\ue059\ni = 1F( ) i) = Tr(F( ) nn \u2212 1\ue059\ni = 1F( ) i) . (2.52)\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\ndi\ufb00erentshape.Forexample,forA\u2208 Rm n \u00d7andB\u2208 Rn m \u00d7,wehave\nTr( ) = Tr( )ABBA (2.53)\neventhoughAB\u2208 Rm m \u00d7andBA\u2208 Rn n \u00d7.\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\n2.11TheDeterminant\nThedeterminant ofa\u00a0squarematrix,\u00a0denoted det(A),\u00a0isa\u00a0functionmapping\nmatricesto\u00a0realscalars.Thedeterminant isequal\u00a0totheproductof\u00a0allthe\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\nthetransformationpreservesvolume.\n4 7", "CHAPTER2.LINEARALGEBRA\n2.12Example:PrincipalComponentsAnalysis\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\nWewouldliketoloseaslittleprecisionaspossible.\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\nofthem.Foreachpointx( ) i\u2208 Rnwewill\ufb01ndacorrespondingcodevectorc( ) i\u2208 Rl.\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\noriginaldata.Wewillwantto\ufb01ndsomeencodingfunctionthatproducesthecode\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\ninputgivenitscode, .xx \u2248 g f(())\nPCAisde\ufb01nedbyourchoiceofthedecodingfunction.Speci\ufb01cally,tomakethe\ndecoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\ninto Rn.Let,where g() = cDcD\u2208 Rn l \u00d7isthematrixde\ufb01ningthedecoding.\nComputingtheoptimalcodeforthisdecodercouldbeadi\ufb03cultproblem.To\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\ntoeachother.(NotethatDisstillnottechnically\u201canorthogonalmatrix\u201dunless\nl n= )\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive\ntheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\nnorm.\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,the\ufb01rst\nthingweneedtodois\ufb01gureouthowtogeneratetheoptimalcodepointc\u2217for\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\ninputpointxanditsreconstruction, g(c\u2217).Wecanmeasurethisdistanceusinga\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\nc\u2217= argmin\nc||\u2212 ||x g()c 2 . (2.54)\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because\nbothareminimizedbythesamevalueofc.Bothareminimizedbythesame\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\n4 8", "CHAPTER2.LINEARALGEBRA\nmonotonically increasingfornon-negative arguments.\nc\u2217= argmin\nc||\u2212 ||x g()c2\n2 . (2.55)\nThefunctionbeingminimizedsimpli\ufb01esto\n( ())x\u2212 gc\ue03e( ())x\u2212 gc (2.56)\n(bythede\ufb01nitionofthe L2norm,equation)2.30\n= x\ue03exx\u2212\ue03eg g ()c\u2212()c\ue03exc+( g)\ue03eg()c (2.57)\n(bythedistributiveproperty)\n= x\ue03exx\u22122\ue03eg g ()+c ()c\ue03eg()c (2.58)\n(becausethescalar g()c\ue03exisequaltothetransposeofitself).\nWecannowchangethefunctionbeingminimizedagain,toomitthe\ufb01rstterm,\nsincethistermdoesnotdependon:c\nc\u2217= argmin\nc\u22122x\ue03eg g ()+c ()c\ue03eg .()c (2.59)\nTomakefurtherprogress,wemustsubstituteinthede\ufb01nitionof: g()c\nc\u2217= argmin\nc\u22122x\ue03eDcc+\ue03eD\ue03eDc (2.60)\n= argmin\nc\u22122x\ue03eDcc+\ue03eI lc (2.61)\n(bytheorthogonalityandunitnormconstraintson)D\n= argmin\nc\u22122x\ue03eDcc+\ue03ec (2.62)\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\nyoudonotknowhowtodothis):\n\u2207 c(2\u2212x\ue03eDcc+\ue03ec) = 0 (2.63)\n\u22122D\ue03exc+2= 0 (2.64)\ncD= \ue03ex . (2.65)\n4 9", "CHAPTER2.LINEARALGEBRA\nThismakesthealgorithme\ufb03cient:\u00a0wecanoptimallyencodexjustusinga\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\nf() = xD\ue03ex . (2.66)\nUsingafurthermatrixmultiplication, wecanalsode\ufb01nethePCAreconstruction\noperation:\nr g f () = x (()) = xDD\ue03ex . (2.67)\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe\npointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\noferrorscomputedoveralldimensionsandallpoints:\nD\u2217= argmin\nD\ue073\ue058\ni , j\ue010\nx( ) i\nj\u2212 r(x( ) i) j\ue0112\nsubjecttoD\ue03eDI= l(2.68)\nToderivethealgorithmfor\ufb01ndingD\u2217,wewillstartbyconsideringthecase\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\nd\u2217= argmin\nd\ue058\ni||x( ) i\u2212dd\ue03ex( ) i||2\n2subjectto||||d 2= 1 .(2.69)\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\nscalarvalued\ue03ex( ) iontherightofthevectord.Itismoreconventionaltowrite\nscalarcoe\ufb03cientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\nsuchaformulaas\nd\u2217= argmin\nd\ue058\ni||x( ) i\u2212d\ue03ex( ) id||2\n2subjectto||||d 2= 1 ,(2.70)\nor,exploitingthefactthatascalarisitsowntranspose,as\nd\u2217= argmin\nd\ue058\ni||x( ) i\u2212x( ) i \ue03edd||2\n2subjectto||||d 2= 1 .(2.71)\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\n5 0", "CHAPTER2.LINEARALGEBRA\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\nThiswillallowustousemorecompactnotation.LetX\u2208 Rm n \u00d7bethematrix\nde\ufb01nedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i\ue03e.\nWecannowrewritetheproblemas\nd\u2217= argmin\nd||\u2212XXdd\ue03e||2\nFsubjecttod\ue03ed= 1 .(2.72)\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\nportionasfollows:\nargmin\nd||\u2212XXdd\ue03e||2\nF (2.73)\n= argmin\ndTr\ue012\ue010\nXXdd \u2212\ue03e\ue011\ue03e\ue010\nXXdd \u2212\ue03e\ue011\ue013\n(2.74)\n(byequation)2.49\n= argmin\ndTr(X\ue03eXX\u2212\ue03eXdd\ue03e\u2212dd\ue03eX\ue03eXdd+\ue03eX\ue03eXdd\ue03e)(2.75)\n= argmin\ndTr(X\ue03eX)Tr(\u2212X\ue03eXdd\ue03e)Tr(\u2212dd\ue03eX\ue03eX)+Tr(dd\ue03eX\ue03eXdd\ue03e)\n(2.76)\n= argmin\nd\u2212Tr(X\ue03eXdd\ue03e)Tr(\u2212dd\ue03eX\ue03eX)+Tr(dd\ue03eX\ue03eXdd\ue03e)(2.77)\n(becausetermsnotinvolvingdonota\ufb00ectthe) d argmin\n= argmin\nd\u22122Tr(X\ue03eXdd\ue03e)+Tr(dd\ue03eX\ue03eXdd\ue03e)(2.78)\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n= argmin\nd\u22122Tr(X\ue03eXdd\ue03e)+Tr(X\ue03eXdd\ue03edd\ue03e)(2.79)\n(usingthesamepropertyagain)\nAtthispoint,were-introducetheconstraint:\nargmin\nd\u22122Tr(X\ue03eXdd\ue03e)+Tr(X\ue03eXdd\ue03edd\ue03e)subjecttod\ue03ed= 1(2.80)\n= argmin\nd\u22122Tr(X\ue03eXdd\ue03e)+Tr(X\ue03eXdd\ue03e)subjecttod\ue03ed= 1(2.81)\n(duetotheconstraint)\n= argmin\nd\u2212Tr(X\ue03eXdd\ue03e)subjecttod\ue03ed= 1(2.82)\n5 1", "CHAPTER2.LINEARALGEBRA\n= argmax\ndTr(X\ue03eXdd\ue03e)subjecttod\ue03ed= 1(2.83)\n= argmax\ndTr(d\ue03eX\ue03eXdd )subjectto\ue03ed= 1(2.84)\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speci\ufb01cally,\ntheoptimaldisgivenbytheeigenvectorofX\ue03eXcorrespondingtothelargest\neigenvalue.\nThisderivationisspeci\ufb01ctothecaseof l=1andrecoversonlythe\ufb01rst\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe\nlargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\nwritingthisproofasanexercise.\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\n5 2", "C h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\nnewuncertainstatements.Inarti\ufb01cialintelligenceapplications,weuseprobability\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various\nexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\nunderstandthematerialinthisbook.\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin\nthepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\nofuncertaintyinaprobabilitydistribution.\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\nbesu\ufb03cienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo\nsuggestthatyouconsultanadditionalresource,suchasJaynes2003().\n53", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability?\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\nexecuteeachmachineinstruction\ufb02awlessly.Errorsinhardwaredooccur,butare\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning\nmakesheavyuseofprobabilitytheory.\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\nbyPearl1988().\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.\nInfact,beyondmathematical statementsthataretruebyde\ufb01nition,itisdi\ufb03cult\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\nguaranteedtooccur.\nTherearethreepossiblesourcesofuncertainty:\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame\nwhereweassumethatthecardsaretrulyshu\ufb04edintoarandomorder.\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.\u00a0Theoutcome\ngiventhecontestant\u2019schoiceisdeterministic,butfromthecontestant\u2019spoint\nofview,theoutcomeisuncertain.\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\nthe\u00a0information wehave\u00a0observed,\u00a0the\u00a0discarded\u00a0i nformationresults\u00a0in\nuncertaintyinthemodel\u2019spredictions. Forexample,supposewebuilda\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n54", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\ntheprecisepositionofobjects:\u00a0eachobjectcouldbeanywherewithinthe\ndiscretecellthatitwasobservedtooccupy.\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\nmodelingsystemhasthe\ufb01delitytoaccommodateacomplexrule.Forexample,the\nsimplerule\u201cMostbirds\ufb02y\u201discheaptodevelopandisbroadlyuseful,whilearule\noftheform,\u201cBirds\ufb02y,exceptforveryyoungbirdsthathavenotyetlearnedto\n\ufb02y,sickorinjuredbirdsthathavelosttheabilityto\ufb02y,\ufb02ightlessspeciesofbirds\nincludingthecassowary,ostrichandkiwi...\u201d\u00a0isexpensivetodevelop,maintainand\ncommunicate,andafterallofthise\ufb00ortisstillverybrittleandpronetofailure.\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide\nallofthetoolswewantforarti\ufb01cialintelligenceapplications.Probabilitytheory\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.\u00a0Whenwe\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\ntheexperiment(e.g.,drawahandofcards)in\ufb01nitelymanytimes,thenproportion\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot\nseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingthe\ufb02u,\nthismeanssomethingverydi\ufb00erent\u2014wecannotmakein\ufb01nitelymanyreplicasof\nthepatient,noristhereanyreasontobelievethatdi\ufb00erentreplicasofthepatient\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthasthe\ufb02u\nand0indicatingabsolutecertaintythatthepatientdoesnothavethe\ufb02u.\u00a0The\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\nofcertainty,isknownas B ay e si an pr o babili t y.\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.\nForexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n55", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\nsee(). Ramsey1926\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe\nlikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\n3.2RandomVariables\nA r andom v ar i abl eisavariablethatcantakeondi\ufb00erentvaluesrandomly.We\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On\nitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\nmustbecoupledwithaprobabilitydistributionthatspeci\ufb01eshowlikelyeachof\nthesestatesare.\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\nisonethathasa\ufb01niteorcountablyin\ufb01nitenumberofstates.Notethatthese\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\nassociatedwitharealvalue.\n3.3ProbabilityDistributions\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\ncontinuous.\n3.3.1DiscreteVariablesandProbabilityMassFunctions\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith\nacapitalP.Oftenweassociateeachrandomvariablewithadi\ufb00erentprobability\n56", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\nP P ()xisusuallynotthesameas()y.\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes\ntodisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\nexplicitly:P(x=x).Sometimeswede\ufb01neavariable\ufb01rst,thenuse\u223cnotationto\nspecifywhichdistributionitfollowslater:xx. \u223cP()\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust\nsatisfythefollowingproperties:\n\u2022Thedomainofmustbethesetofallpossiblestatesofx. P\n\u2022\u2200\u2208xx,0\u2264P(x)\u22641.Animpossibleeventhasprobabilityandnostatecan 0 \nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\n\u2022\ue050\nx \u2208 xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\ntheprobabilityofoneofmanyeventsoccurring.\nForexample,considerasinglediscreterandomvariablexwithkdi\ufb00erent\nstates.Wecanplacea uni f o r m di st r i but i o nonx\u2014thatis,makeeachofits\nstatesequallylikely\u2014bysettingitsprobabilitymassfunctionto\nPx (= x i) =1\nk(3.1)\nforalli.Wecanseethatthis\ufb01tstherequirementsforaprobabilitymassfunction.\nThevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\n\ue058\niPx (= x i) =\ue058\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized.\n57", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\nfollowingproperties:\n\u2022Thedomainofmustbethesetofallpossiblestatesofx. p\n\u2022\u2200\u2208 \u2265 \u2264 xx,px() 0 () . p Notethatwedonotrequirex 1.\n\u2022\ue052\npxdx()= 1.\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeci\ufb01c\nstatedirectly,insteadtheprobabilityoflandinginsideanin\ufb01nitesimalregionwith\nvolumeisgivenby. \u03b4x px\u03b4x()\nWecanintegratethedensityfunctionto\ufb01ndtheactualprobabilitymassofa\nsetofpoints.Speci\ufb01cally,theprobabilitythatxliesinsomeset Sisgivenbythe\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\nliesintheintervalisgivenby []a,b\ue052\n[ ] a , bpxdx().\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeci\ufb01c\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\nwhereaandbaretheendpointsoftheinterval,withb>a.The\u201c;\u201dnotationmeans\n\u201cparametrized by\u201d;weconsiderxtobetheargumentofthefunction,whileaand\nbareparametersthatde\ufb01nethefunction.Toensurethatthereisnoprobability\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx\ue036\u2208[a,b] [.Withina,b],\nuxa,b (;) =1\nb a \u2212.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\nbywritingx. \u223cUa,b()\n3.4MarginalProbability\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\nForexample,supposewehavediscreterandomvariablesxandy,andweknow\nP,(xy.Wecan\ufb01ndxwiththe : ) P() sum r ul e\n\u2200\u2208xxx,P(= ) =x\ue058\nyPx,y. (= xy= ) (3.3)\n58", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThename\u201cmarginalprobability\u201dcomesfromtheprocessofcomputingmarginal\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\ndi\ufb00erentvaluesofxinrowsanddi\ufb00erentvaluesofyincolumns,itisnaturalto\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\ntherightoftherow.\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\npx() =\ue05a\npx,ydy. () (3.4)\n3.5ConditionalProbability\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\nconditionalprobabilitycanbecomputedwiththeformula\nPyx (= y |x= ) =Py,x (= yx= )\nPx (= x ). (3.5)\nTheconditionalprobabilityisonlyde\ufb01nedwhenP(x=x)>0.Wecannotcompute\ntheconditionalprobabilityconditionedonaneventthatneverhappens.\nItisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\nwhichwedonotexploreinthisbook.\n3.6TheChainRuleofConditionalProbabilities\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\nintoconditionaldistributionsoveronlyonevariable:\nP(x( 1 ),...,x( ) n) = (Px( 1 ))\u03a0n\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i \u2212).(3.6)\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\nItfollowsimmediatelyfromthede\ufb01nitionofconditionalprobabilityinequation.3.5\n59", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nForexample,applyingthede\ufb01nitiontwice,weget\nP,,P,P, (abc)= (ab|c)(bc)\nP,PP (bc)= ( )bc| ()c\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\n3.7IndependenceandConditionalIndependence\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\nonlyy:\n\u2200\u2208 \u2208xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom\nvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\nwayforeveryvalueofz:\n\u2200\u2208 \u2208 \u2208 | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\n(3.8)\nWe\u00a0candenoteindependence\u00a0andconditionalindependence\u00a0with compact\nnotation:xy\u22a5meansthatxandyareindependent,whilexyz \u22a5|meansthatx\nandyareconditionallyindependentgivenz.\n3.8Expectation,VarianceandCovariance\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa\nprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\nE x \u223c P[()] =fx\ue058\nxPxfx, ()() (3.9)\nwhileforcontinuousvariables,itiscomputedwithanintegral:\nE x \u223c p[()] =fx\ue05a\npxfxdx. ()() (3.10)\n60", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[\u00b7]averagesover\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\nnoambiguity,wemayomitthesquarebrackets.\nExpectationsarelinear,forexample,\nE x[()+ ()] = \u03b1fx\u03b2gx\u03b1 E x[()]+fx\u03b2 E x[()]gx, (3.11)\nwhenandarenotdependenton. \u03b1\u03b2 x\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\nvariablexvaryaswesampledi\ufb00erentvaluesofxfromitsprobabilitydistribution:\nVar(()) = fx E\ue068\n(() [()]) fx\u2212 Efx2\ue069\n. (3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n\nThe c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\ntoeachother,aswellasthescaleofthesevariables:\nCov(()()) = [(() [()])(() [()])] fx,gy Efx\u2212 Efxgy\u2212 Egy.(3.13)\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto\ntakeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\nrelated,ratherthanalsobeinga\ufb00ectedbythescaleoftheseparatevariables.\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-\never,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\nzerocovariance.Forexample,supposewe\ufb01rstsamplearealnumberxfroma\nuniformdistributionovertheinterval[\u22121,1].Wenextsamplearandomvariable\n61", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ns.Withprobability1\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\nthevalueofstobe\u22121.Wecanthengeneratearandomvariableybyassigning\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\nthemagnitudeof.However,y Cov() = 0x,y.\nThe c o v ar i anc e m at r i xofarandomvector x\u2208 Rnisannn\u00d7matrix,such\nthat\nCov() x i , j= Cov(x i,x j). (3.14)\nThediagonalelementsofthecovariancegivethevariance:\nCov(x i,x i) = Var(x i). (3.15)\n3.9CommonProbabilityDistributions\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\nlearning.\n3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\nItiscontrolledbyasingleparameter\u03c6\u2208[0,1],whichgivestheprobabilityofthe\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\nP \u03c6 (= 1) = x (3.16)\nP \u03c6 (= 0) = 1x \u2212 (3.17)\nPx\u03c6 (= x ) = x(1 )\u2212\u03c61 \u2212 x(3.18)\nE x[] = x\u03c6 (3.19)\nVar x() = (1 )x\u03c6\u2212\u03c6 (3.20)\n3.9.2MultinoulliDistribution\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\nvariablewithkdi\ufb00erentstates,wherekis\ufb01nite.1Themultinoullidistributionis\n1\u201cMultinoulli\u201disatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany\ntimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\nManytextsusetheterm\u201cmultinomial\u201dtorefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase. n= 1\n62", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nparametrized byavector p\u2208[0,1]k \u2212 1,wherep igivestheprobabilityofthei-th\nstate.The\ufb01nal,k-thstate\u2019sprobabilityisgivenby1\u2212 1\ue03ep.Notethatwemust\nconstrain 1\ue03ep\u22641.Multinoullidistributionsareoftenusedtorefertodistributions\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\norvarianceofmultinoulli-dis tributedrandomvariables.\nTheBernoulliandmultinoullidistributionsaresu\ufb03cienttodescribeanydistri-\nbutionovertheirdomain.\u00a0They areabletodescribeanydistributionovertheir\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\nparametersmustimposestrictlimitsonthedistribution.\n3.9.3GaussianDistribution\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\nt i o n,alsoknownasthe : G aussian di st r i but i o n\nN(;x\u00b5,\u03c32) =\ue072\n1\n2\u03c0\u03c32exp\ue012\n\u22121\n2\u03c32( )x\u00b5\u22122\ue013\n.(3.21)\nSee\ufb01gureforaplotofthedensityfunction. 3.1\nThetwoparameters \u00b5\u2208 Rand\u03c3\u2208(0,\u221e)controlthenormaldistribution.\nTheparameter\u00b5givesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =\u00b5.Thestandarddeviationofthedistributionisgivenby\n\u03c3,andthevarianceby\u03c32.\nWhenweevaluatethePDF,weneedtosquareandinvert\u03c3.Whenweneedto\nfrequentlyevaluatethePDFwithdi\ufb00erentparametervalues,amoree\ufb03cientway\nofparametrizing thedistributionistouseaparameter\u03b2\u2208(0,\u221e)tocontrolthe\npr e c i si o norinversevarianceofthedistribution:\nN(;x\u00b5,\u03b2\u2212 1) =\ue072\n\u03b2\n2\u03c0exp\ue012\n\u22121\n2\u03b2x\u00b5 (\u2212)2\ue013\n. (3.22)\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould\ntake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\n63", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n\u2212 \u2212 \u2212 \u2212 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x \u00b5\nIn\ufb02ectionpointsat\nx \u00b5 \u03c3 = \u00b1\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;\u00b5,\u03c32)exhibits\naclassic\u201cbellcurve\u201dshape,withthexcoordinateofitscentralpeakgivenby\u00b5,and\nthewidthofitspeakcontrolledby\u03c3.Inthisexample,wedepictthestandardnormal\ndistribution,withand. \u00b5= 0\u03c3= 1\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal\ndistributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\nstructuredbehavior.\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone\nthatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\nsection.19.4.2\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\nde\ufb01nitesymmetricmatrix: \u03a3\nN(; ) = x \u00b5, \u03a3\ue073\n1\n(2)\u03c0ndet() \u03a3exp\ue012\n\u22121\n2( ) x \u00b5\u2212\ue03e\u03a3\u2212 1( ) x \u00b5\u2212\ue013\n.(3.23)\n64", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nTheparameter \u00b5stillgivesthemeanofthedistribution,thoughnowitis\nvector-valued.Theparameter \u03a3givesthecovariancematrixofthedistribution.\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\nmanydi\ufb00erentvaluesoftheparameters,thecovarianceisnotacomputationally\ne\ufb03cientwaytoparametrizethedistribution,sinceweneedtoinvert \u03a3toevaluate\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x \u03b2\nN(; x \u00b5 \u03b2,\u2212 1) =\ue073\ndet() \u03b2\n(2)\u03c0nexp\ue012\n\u22121\n2( ) x \u00b5\u2212\ue03e\u03b2 x \u00b5 (\u2212)\ue013\n.(3.24)\nWeoften\ufb01xthecovariancematrixtobeadiagonalmatrix.Anevensimpler\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\ntimestheidentitymatrix.\n3.9.4ExponentialandLaplaceDistributions\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\ndi st r i but i o n:\npx\u03bb\u03bb (;) = 1 x \u2265 0exp( )\u2212\u03bbx. (3.25)\nTheexponentialdistributionusestheindicatorfunction 1 x \u2265 0toassignprobability\nzerotoallnegativevaluesof.x\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\nofprobabilitymassatanarbitrarypointisthe\u00b5 L apl ac e di st r i but i o n\nLaplace(;) =x\u00b5,\u03b31\n2\u03b3exp\ue012\n\u2212|\u2212|x\u00b5\n\u03b3\ue013\n. (3.26)\n3.9.5TheDiracDistributionandEmpiricalDistribution\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution\nclustersaroundasinglepoint.Thiscanbeaccomplishedbyde\ufb01ningaPDFusing\ntheDiracdeltafunction,:\u03b4x()\npx\u03b4x\u00b5. () = (\u2212) (3.27)\nTheDiracdeltafunctionisde\ufb01nedsuchthatitiszero-valuedeverywhereexcept\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\nassociateseachvaluexwithareal-valuedoutput,insteaditisadi\ufb00erentkindof\n65", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisde\ufb01nedintermsofits\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\nthanzero.\nByde\ufb01ningp(x)tobe\u03b4shiftedby\u2212\u00b5weobtainanin\ufb01nitelynarrowand\nin\ufb01nitelyhighpeakofprobabilitymasswhere.x\u00b5= \nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\ndi st r i but i o n,\n\u02c6p() = x1\nmm\ue058\ni = 1\u03b4( x x\u2212( ) i) (3.28)\nwhichputsprobabilitymass1\nmoneachofthempoints x( 1 ),..., x( ) mforminga\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\ntode\ufb01netheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue\nthatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset.\u00a0Anotherimportantperspectiveontheempiricaldistributionis\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n(seesection).5.5\n3.9.6MixturesofDistributions\nItisalsocommontode\ufb01neprobabilitydistributionsbycombiningothersimpler\nprobabilitydistributions.Onecommon\u00a0wayof\u00a0combining\u00a0distributionsis\u00a0to\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\nmultinoullidistribution:\nP() =x\ue058\niPiPi (= c )( = xc| ) (3.29)\nwherecisthemultinoullidistributionovercomponentidentities. P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical\ndistributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample.\n66", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\nprobabilitydistributionsfromsimpleonesinmoredetail.\nThemixturemodelallowsustobrie\ufb02yglimpseaconceptthatwillbeof\nparamountimportancelater\u2014the l at e n t v ar i abl e.Alatentvariableisarandom\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe\nmixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\nvariablesarediscussedfurtherinsection.16.5\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e\nmodel,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\naseparatelyparametrized mean \u00b5( ) iandcovariance \u03a3( ) i.Somemixturescanhave\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\nviatheconstraint \u03a3( ) i= \u03a3,i\u2200.AswithasingleGaussiandistribution,themixture\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\ndiagonalorisotropic.\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture\nspecifythe pr i o r pr o babili t y\u03b1 i=P(c=i) giventoeachcomponenti.Theword\n\u201cprior\u201dindicatesthatitexpressesthemodel\u2019sbeliefsaboutc b e f o r eithasobserved\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\nspeci\ufb01c,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\ncomponents.\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\n3.10UsefulPropertiesofCommonFunctions\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\ntheprobabilitydistributionsusedindeeplearningmodels.\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\n\u03c3x() =1\n1+exp()\u2212x. (3.30)\nThelogisticsigmoidiscommonlyusedtoproducethe\u03c6parameterofaBernoulli\n67", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx 1x 2\nFigure3.2:\u00a0SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\ncomponents.Fromlefttoright,the\ufb01rstcomponenthasanisotropiccovariancematrix,\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance\nseparatelyalonganarbitrarybasisofdirections.\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\nforthe\u03c6parameter.See\ufb01gureforagraphofthesigmoidfunction.The 3.3\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\nmeaningthatthefunctionbecomesvery\ufb02atandinsensitivetosmallchangesinits\ninput.\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\n2001):\n\u03b6x x. () = log(1+exp()) (3.31)\nThesoftplusfunctioncanbeusefulforproducingthe\u03b2or\u03c3parameterofanormal\ndistributionbecauseitsrangeis(0,\u221e).Italsoarisescommonlywhenmanipulating\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\nfactthatitisasmoothedor\u201csoftened\u201dversionof\nx+= max(0),x. (3.32)\nSee\ufb01gureforagraphofthesoftplusfunction. 3.4\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\nthem:\n68", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n\u2212 \u2212 1 0 5 0 5 1 0\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .\u03c3 x ( )\nFigure3.3:Thelogisticsigmoidfunction.\n\u2212 \u2212 1 0 5 0 5 1 0\nx024681 0\u03b6 x ( )\nFigure3.4:Thesoftplusfunction.\n69", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n\u03c3x() =exp()x\nexp()+exp(0)x(3.33)\nd\ndx\u03c3x\u03c3x\u03c3x () = ()(1\u2212()) (3.34)\n1 () = () \u2212\u03c3x\u03c3\u2212x (3.35)\nlog() = () \u03c3x \u2212\u03b6\u2212x (3.36)\nd\ndx\u03b6x\u03c3x () = () (3.37)\n\u2200\u2208x(01),,\u03c3\u2212 1() = logx\ue012x\n1\u2212x\ue013\n(3.38)\n\u2200x>,\u03b60\u2212 1() = log(exp()1) x x\u2212 (3.39)\n\u03b6x() =\ue05ax\n\u2212 \u221e\u03c3ydy() (3.40)\n\u03b6x\u03b6xx ()\u2212(\u2212) = (3.41)\nThefunction\u03c3\u2212 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\nusedinmachinelearning.\nEquationprovidesextrajusti\ufb01cationforthename\u201csoftplus.\u201dThesoftplus 3.41\nfunctionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\nfunction,x\u2212=max{0,x\u2212}.Toobtainasmoothfunctionthatisanalogoustothe\nnegativepart,onecanuse\u03b6(\u2212x).Justasxcanberecoveredfromitspositivepart\nandnegativepartviatheidentityx+\u2212x\u2212=x,itisalsopossibletorecoverx\nusingthesamerelationshipbetweenand,asshowninequation. \u03b6x()\u03b6x(\u2212) 3.41\n3.11Bayes\u2019Rule\nWeoften\ufb01ndourselvesinasituationwhereweknowP(yx|)andneedtoknow\nP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\nusing B a y e s\u2019 r ul e:\nP( ) =xy|PP()x( )yx|\nP()y. (3.42)\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\nP() =y\ue050\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\n70", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nBayes\u2019ruleis\u00a0straightforwardto\u00a0derivefrom\u00a0thede\ufb01nitionofconditional\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,who\ufb01rst\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\nindependentlydiscoveredbyPierre-SimonLaplace.\n3.12TechnicalDetailsofContinuousVariables\nAproperformalunderstandingofcontinuousrandomvariablesandprobability\ndensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\nthistextbook,butwecanbrie\ufb02ysketchsomeoftheissuesthatmeasuretheoryis\nemployedtoresolve.\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets\nS 1and S 2suchthatp( x\u2208 S 1) +p( x\u2208 S 2)>1but S 1\u2229 S 2=\u2205.Thesesets\naregenerallyconstructedmakingveryheavyuseofthein\ufb01niteprecisionofreal\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthatarede\ufb01nedby\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\nprobabilityofwithoutencounteringparadoxes.\u00a0Inthisbook,weonlyintegrate\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever\nbecomesarelevantconcern.\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\nasetissaidtohave m e asur e z e r o.Wedonotformallyde\ufb01nethisconceptinthis\ntextbook.Forourpurposes,itissu\ufb03cienttounderstandtheintuitionthataset\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,\nwithin R2,alinehasmeasurezero,whilea\ufb01lledpolygonhaspositivemeasure.\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\nnumbershasmeasurezero,forinstance).\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\n71", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\ntheoryholdforalldiscretevaluesbutonlyhold\u201calmosteverywhere\u201dforcontinuous\nvalues.\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-\ntinuous,di\ufb00erentiabletransformation.Onemightexpectthatp y( y) =p x(g\u2212 1( y)).\nThisisactuallynotthecase.\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\ny=x\n2andx\u223cU(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\neverywhereexcepttheinterval[0,1\n2] 1 ,anditwillbeonthisinterval.Thismeans\n\ue05a\np y()=ydy1\n2, (3.43)\nwhichviolatesthede\ufb01nitionofaprobabilitydistribution.Thisisacommonmistake.\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof\nspaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\nin\ufb01nitesimallysmallregionwithvolume\u03b4 xisgivenbyp( x)\u03b4 x.Sincegcanexpand\norcontractspace,thein\ufb01nitesimalvolumesurrounding xin xspacemayhave\ndi\ufb00erentvolumeinspace. y\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\npreservetheproperty\n|p y(())= gxdy||p x()xdx.| (3.44)\nSolvingfromthis,weobtain\np y() = yp x(g\u2212 1())y\ue00c\ue00c\ue00c\ue00c\u2202x\n\u2202y\ue00c\ue00c\ue00c\ue00c(3.45)\norequivalently\np x() = xp y(())gx\ue00c\ue00c\ue00c\ue00c\u2202gx()\n\u2202x\ue00c\ue00c\ue00c\ue00c. (3.46)\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\nm at r i x\u2014thematrixwithJ i , j=\u2202 x i\n\u2202 y j.Thus,forreal-valuedvectorsand, x y\np x() = xp y(())g x\ue00c\ue00c\ue00c\ue00cdet\ue012\u2202g() x\n\u2202 x\ue013 \ue00c\ue00c\ue00c\ue00c. (3.47)\n72", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.13InformationTheory\nInformationtheory\u00a0isa\u00a0branchof\u00a0appliedmathematics\u00a0thatrevolvesaround\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\nspeci\ufb01cprobabilitydistributionsusingvariousencodingschemes.Inthecontextof\nmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\nwheresomeofthesemessagelengthinterpretations donotapply.This\ufb01eldis\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\n().2003\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely\neventhas\u00a0occurredismoreinformativethanlearningthata\u00a0likely\u00a0eventhas\noccurred.Amessagesaying\u201cthesunrosethismorning\u201dissouninformative as\ntobeunnecessarytosend,butamessagesaying\u201ctherewasasolareclipsethis\nmorning\u201disveryinformative.\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\nSpeci\ufb01cally,\n\u2022Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\nwhatsoever.\n\u2022Lesslikelyeventsshouldhavehigherinformationcontent.\n\u2022Independenteventsshouldhaveadditiveinformation. Forexample,\ufb01nding\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\nmuchinformationas\ufb01ndingoutthatatossedcoinhascomeupasheads\nonce.\nInordertosatisfyallthreeoftheseproperties,wede\ufb01nethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx. () = log\u2212 () (3.48)\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our\nde\ufb01nitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\n73", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ninformationgainedbyobservinganeventofprobability1\ne.Othertextsusebase-2\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\njustarescalingofinformationmeasuredinnats.\nWhenxiscontinuous,weusethesamede\ufb01nitionofinformationbyanalogy,\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\nguaranteedtooccur.\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x \u223c P[()] = Ix \u2212 E x \u223c P[log()]Px. (3.49)\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\naredi\ufb00erent)neededonaveragetoencodesymbolsdrawnfromadistributionP.\nDistributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\n\ufb01gureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\nknownasthe di \ufb00 e r e n t i al e nt r o p y.\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\nrandomvariablex,wecanmeasurehowdi\ufb00erentthesetwodistributionsareusing\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\nD K L( ) = PQ\ue06b E x \u223c P\ue014\nlogPx()\nQx()\ue015\n= E x \u223c P[log()log()] Px\u2212Qx.(3.50)\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-\nnegative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\nthecaseofdiscretevariables,orequal\u201calmosteverywhere\u201dinthecaseofcontinuous\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthedi\ufb00erence\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\nbecauseitisnotsymmetric:D K L(PQ\ue06b)\ue036=D K L(QP\ue06b)forsomePandQ.\u00a0This\n74", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\nto.Theentropyisgivenby 1 (p\u22121)log(1\u2212p)\u2212pplog.Whenpisnear0,thedistribution\nisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\noutcomes.\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\ntouseD K L( )PQ\ue06borD K L( )QP\ue06b.See\ufb01gureformoredetail.3.6\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y\nH(P,Q) =H(P)+D K L(PQ\ue06b),whichissimilartotheKLdivergencebutlacking\nthetermontheleft:\nHP,Q( ) = \u2212 E x \u223c Plog()Qx. (3.51)\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\ntreattheseexpressionsaslim x \u2192 0xxlog= 0.\n3.14StructuredProbabilisticModels\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n75", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nxProbability Densityq\u2217= argminq D K L() p q \ue06b\np x()\nq\u2217() x\nxProbability Densityq\u2217= argminq D K L() q p \ue06b\np() x\nq\u2217() x\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\neitherD KL(pq\ue06b)orD KL(qp\ue06b).Weillustratethee\ufb00ectofthischoiceusingamixtureof\ntwoGaussiansforp,andasingleGaussianforq.\u00a0Thechoiceofwhichdirectionofthe\nKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\ndirectionoftheKLdivergencere\ufb02ectswhichoftheseconsiderationstakespriorityforeach\napplication. ( L e f t )Thee\ufb00ectofminimizingD KL(pq\ue06b).Inthiscase,weselectaqthathas\nhighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\ne\ufb00ectofminimizingD KL(qp\ue06b).Inthiscase,weselectaqthathaslowprobabilitywhere\nphaslowprobability.Whenphasmultiplemodesthataresu\ufb03cientlywidelyseparated,\nasinthis\ufb01gure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we\nillustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\narenotseparatedbyasu\ufb03cientlystronglowprobabilityregion,thenthisdirectionofthe\nKLdivergencecanstillchoosetoblurthemodes.\n76", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ndescribetheentirejointprobabilitydistributioncanbeveryine\ufb03cient(both\ncomputationally andstatistically).\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\nain\ufb02uencesthevalueofbandbin\ufb02uencesthevalueofc,butthataandcare\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree\nvariablesasaproductofprobabilitydistributionsovertwovariables:\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\nThesefactorizationscangreatlyreducethenumberofparametersneeded\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\nreducethecostofrepresentingadistributionifweareableto\ufb01ndafactorization\nintodistributionsoverfewervariables.\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword\n\u201cgraph\u201dinthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\ninthegraphcorrespondstoarandomvariable,\u00a0and anedgeconnectingtwo\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect\ninteractionsbetweenthosetworandomvariables.\nD i r e c t e dmodelsuse\u00a0graphswithdirectededges,\u00a0andtheyrepresentfac-\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\nSpeci\ufb01cally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\ngiventheparentsofx i,denotedPa G(x i):\np() = x\ue059\nip(x i|Pa G(x i)). (3.53)\nSee\ufb01gureforanexampleofadirectedgraphandthefactorizationofprobability 3.7\ndistributionsitrepresents.\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n77", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall\nconnectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\nmodelisassociatedwithafactor\u03c6( ) i(C( ) i).Thesefactorsarejustfunctions,not\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\ndistribution.\nTheprobabilityofacon\ufb01gurationofrandomvariablesis pr o p o r t i o naltothe\nproductofallofthesefactors\u2014assignmentsthatresultinlargerfactorvaluesare\nmorelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\nthereforedividebyanormalizingconstantZ,de\ufb01nedtobethesumorintegral\noverallstatesoftheproductofthe\u03c6functions,inordertoobtainanormalized\nprobabilitydistribution:\np() = x1\nZ\ue059\ni\u03c6( ) i\ue010\nC( ) i\ue011\n. (3.55)\nSee\ufb01gureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents.\nKeep\u00a0inmind\u00a0thatthese\u00a0graphicalrepresentationsof\u00a0factorizations are\u00a0a\nlanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\n78", "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,, (abcde) =1\nZ\u03c6( 1 )( )abc,,\u03c6( 2 )()bd,\u03c6( 3 )()ce,. (3.56)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways.\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\ndi\ufb00erentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\ndetail.\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\nmostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods.\n79", "C h a p t e r 4\nNumericalComputation\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\nlution.Commonoperationsincludeoptimization (\ufb01ndingthevalueofanargument\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.\nEvenjustevaluatingamathematical functiononadigitalcomputercanbedi\ufb03cult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusinga\ufb01niteamountofmemory.\n4. 1 O v er\ufb02 o w an d Un d er\ufb02 o w\nThefundamentaldi\ufb03cultyinperformingcontinuousmathonadigitalcomputer\nisthatweneedtorepresentin\ufb01nitelymanyrealnumberswitha\ufb01nitenumber\nofbitpatterns.Thismeansthatforalmostallrealnumbers,\u00a0weincursome\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany\ncases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\nroundingerror.\nOneformofroundingerrorthatisparticularlydevastatingis under \ufb02o w.\nUnder\ufb02owoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\nbehavequalitativelydi\ufb00erentlywhentheirargumentiszeroratherthanasmall\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\n80", "CHAPTER4.NUMERICALCOMPUTATION\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\nisusuallytreatedas\u2212\u221e,whichthenbecomesnot-a-numberifitisusedformany\nfurtherarithmeticoperations).\nAnotherhighlydamagingformofnumericalerroris o v e r \ufb02o w.Over\ufb02owoccurs\nwhennumberswithlargemagnitudeareapproximatedas\u221eor\u2212\u221e.Further\narithmeticwillusuallychangethesein\ufb01nitevaluesintonot-a-numbervalues.\nOneexampleofafunctionthatmustbestabilizedagainstunder\ufb02owand\nover\ufb02owisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\nde\ufb01nedtobe\nsoftmax() x i=exp( x i)\ue050n\nj = 1exp( x j). (4.1)\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\nwecanseethatalloftheoutputsshouldbeequalto1\nn.Numerically,thismay\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will\nunder\ufb02ow.Thismeansthedenominator ofthesoftmaxwillbecome0,sothe\ufb01nal\nresultisunde\ufb01ned.When cisverylargeandpositive,exp( c)willover\ufb02ow,again\nresultingintheexpressionasawholebeingunde\ufb01ned.Bothofthesedi\ufb03culties\ncanberesolvedbyinsteadevaluating softmax( z)where z= x\u2212max i x i.Simple\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofover\ufb02ow.\nLikewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunder\ufb02owinthedenominator leadingtoadivisionbyzero.\nThereisstillonesmallproblem.Under\ufb02owinthenumeratorcanstillcause\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\nlogsoftmax( x)by\ufb01rstrunningthesoftmaxsubroutinethenpassingtheresultto\nthelogfunction,wecoulderroneouslyobtain \u2212\u221e.Instead,wemustimplement\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The\nlogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\nthefunction. softmax\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible\ntoimplementanewalgorithmandhavethenewimplementation automatically\n8 1", "CHAPTER4.NUMERICALCOMPUTATION\nstabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012\nofasoftwarepackagethatautomatically detectsandstabilizesmanycommon\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\n4. 2 P o or C on d i t i o n i n g\nConditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges\ninitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\ncanbeproblematicforscienti\ufb01ccomputationbecauseroundingerrorsintheinputs\ncanresultinlargechangesintheoutput.\nConsiderthefunction f( x)= A\u2212 1x.When A\u2208 Rn n \u00d7hasaneigenvalue\ndecomposition,its c o ndi t i o n num beris\nmax\ni , j\ue00c\ue00c\ue00c\ue00c\u03bb i\n\u03bb j\ue00c\ue00c\ue00c\ue00c. (4.2)\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\nThissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\nofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify\npre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\n4. 3 Gradi en t - Bas e d O p t i m i z a t i o n\nMostdeeplearningalgorithmsinvolveoptimization ofsomesort.\u00a0Optimization\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\nx.\u00a0Weusuallyphrasemostoptimization problemsintermsofminimizing f( x).\nMaximization maybeaccomplishedviaaminimization algorithmbyminimizing\n\u2212 f() x.\nThefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -\nt i o nor c r i t e r i o n.Whenweareminimizingit,\u00a0wemayalsocallitthe c o st\nf unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n.\u00a0Inthisbook,weusetheseterms\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\ntosomeoftheseterms.\nWeoftendenotethevaluethatminimizesormaximizesafunctionwitha\nsuperscript.Forexample,wemightsay \u2217 x\u2217= argmin() f x.\n8 2", "CHAPTER4.NUMERICALCOMPUTATION\n\u2212 \u2212 \u2212 \u2212 20. 15. 10. 05 00 05 10 15 20 ......\nx\u221220.\u221215.\u221210.\u221205.00.05.10.15.20.\nGlobalminimumat= 0.x\nSincef\ue030() = 0,gradient x\ndescent haltshere.\nFor 0,wehave x< f\ue030() 0,x<\nsowecandecreasebyf\nmoving rightward.For 0,wehave x> f\ue030() 0,x>\nsowecandecreasebyf\nmoving leftward.\nf x() =1\n2x2\nf\ue030() = x x\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.\nWeassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here.\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\nThe der i v at i v eofthisfunctionisdenotedas f\ue030( x)orasd y\nd x.Thederivative f\ue030( x)\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeci\ufb01eshowtoscale\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\noutput: f x \ue00f f x \ue00f f (+) \u2248()+\ue030() x.\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\nweknowthat f( x \ue00f\u2212sign( f\ue030( x)))islessthan f( x)forsmallenough \ue00f.Wecan\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).See\ufb01gureforan4.1\nexampleofthistechnique.\nWhen f\ue030( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection\ntomove.Pointswhere f\ue030( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\npoints,soitisnolongerpossibletodecrease f( x)bymakingin\ufb01nitesimalsteps.\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\n8 3", "CHAPTER4.NUMERICALCOMPUTATION\nMinimum Maximum Saddlepoint\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\nsoitisnotpossibletoincrease f( x)bymakingin\ufb01nitesimalsteps.Somecritical\npointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\n\ufb01gureforexamplesofeachtypeofcriticalpoint. 4.2\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby\nvery\ufb02atregions.Allofthismakesoptimization verydi\ufb03cult,especiallywhenthe\ninputtothefunctionismultidimensional.Wethereforeusuallysettlefor\ufb01ndinga\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\n\ufb01gureforanexample.4.3\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn\u2192 R.Forthe\nconceptof\u201cminimization\u201d\u00a0to makesense,theremuststillbeonlyone(scalar)\noutput.\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\nder i v at i v e s.Thepartialderivative\u2202\n\u2202 x if( x)measureshow fchangesasonlythe\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\nvectorcontainingallofthepartialderivatives,denoted \u2207 x f( x).Element iofthe\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\n8 4", "CHAPTER4.NUMERICALCOMPUTATION\nxf x()\nIdeally,wewouldlike\ntoarriveattheglobal\nminimum, butthis\nmight notbepossible.Thislocalminimum\nperformsnearlyaswellas\ntheglobalone,\nsoitisanacceptable\nhaltingpoint.\nThislocalminimumperforms\npoorlyandshouldbeavoided.\nFigure4.3:Optimizationalgorithmsmayfailto\ufb01ndaglobalminimumwhenthereare\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond\ntosigni\ufb01cantlylowvaluesofthecostfunction.\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\nofthefunction f( x+ \u03b1 u)withrespectto \u03b1,evaluatedat \u03b1= 0.Usingthechain\nrule,wecanseethat\u2202\n\u2202 \u03b1f \u03b1 (+ x u)evaluatesto u\ue03e\u2207 x f \u03b1 () xwhen = 0.\nTominimize f,wewouldliketo\ufb01ndthedirectioninwhich fdecreasesthe\nfastest.Wecandothisusingthedirectionalderivative:\nmin\nu u ,\ue03e u = 1u\ue03e\u2207 x f() x (4.3)\n=min\nu u ,\ue03e u = 1|||| u 2||\u2207 x f() x|| 2cos \u03b8 (4.4)\nwhere \u03b8istheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\nignoringfactorsthatdonotdependon u,thissimpli\ufb01estomin ucos \u03b8.Thisis\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.\nThisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\nSteepestdescentproposesanewpoint\nx\ue030= x\u2212\u2207 \ue00f x f() x (4.5)\n8 5", "CHAPTER4.NUMERICALCOMPUTATION\nwhere \ue00fisthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\nWecanchoose \ue00finseveraldi\ufb00erentways.Apopularapproachistoset \ue00ftoasmall\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\nderivativevanish.Anotherapproachistoevaluate f \ue00f ( x\u2212\u2207 x f()) xforseveral\nvaluesof \ue00fandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\nThislaststrategyiscalleda l i ne se ar c h.\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\nequation \u2207 x f() = 0 xfor. x\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\nsmallmove)towardsbettercon\ufb01gurations canbegeneralizedtodiscretespaces.\nAscendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\n( ,). RusselandNorvig2003\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\nSometimesweneedto\ufb01ndallofthepartialderivativesofafunctionwhoseinput\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\nknownasa J ac o bi an m at r i x.Speci\ufb01cally,ifwehaveafunction f: Rm\u2192 Rn,\nthentheJacobianmatrix J\u2208 Rn m \u00d7ofisde\ufb01nedsuchthat f J i , j=\u2202\n\u2202 x jf() x i.\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn\u2192 R,thederivative\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas\u22022\n\u2202 x i \u2202 x jf.\nInasingledimension,wecandenoted2\nd x2 fby f\ue030 \ue030( x).Thesecondderivativetells\nushowthe\ufb01rstderivativewillchangeaswevarytheinput.Thisisimportant\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement\naswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\nthenthereisnocurvature.Itisaperfectly\ufb02atline,anditsvaluecanbepredicted\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \ue00f\nalongthenegativegradient,andthecostfunctionwilldecreaseby \ue00f.Ifthesecond\nderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\nactuallydecreasebymorethan \ue00f.Finally,ifthesecondderivativeispositive,the\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan \ue00f.See\n8 6", "CHAPTER4.NUMERICALCOMPUTATION\nxf x()N e g a t i v e c u r v a t u r e\nxf x()N o c u r v a t u r e\nxf x()P o s i t i v e c u r v a t u r e\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster\nthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\nfunctioninadvertently.\n\ufb01guretoseehowdi\ufb00erentformsofcurvaturea\ufb00ecttherelationshipbetween 4.4\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond\nderivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\nHessian m at r i x.TheHessianmatrix isde\ufb01nedsuchthat H x()( f)\nH x()( f) i , j=\u22022\n\u2202 x i \u2202 x jf .() x (4.6)\nEquivalently,theHessianistheJacobianofthegradient.\nAnywherethatthesecondpartialderivativesarecontinuous,thedi\ufb00erential\noperatorsarecommutative,i.e.theirordercanbeswapped:\n\u22022\n\u2202 x i \u2202 x jf() = x\u22022\n\u2202 x j \u2202 x if .() x (4.7)\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.\nMostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere.\u00a0Because theHessianmatrixisrealandsymmetric,\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n8 7", "CHAPTER4.NUMERICALCOMPUTATION\neigenvectors.Thesecondderivativeinaspeci\ufb01cdirectionrepresentedbyaunit\nvector disgivenby d\ue03eH d.When disaneigenvectorof H,thesecondderivative\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond\nderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\ntothefunction aroundthecurrentpoint f() x x( 0 ):\nf f () x\u2248( x( 0 ))+( x x\u2212( 0 ))\ue03eg+1\n2( x x\u2212( 0 ))\ue03eH x x (\u2212( 0 )) .(4.8)\nwhere gisthegradientand HistheHessianat x( 0 ).\u00a0Ifweusealearningrate\nof \ue00f,thenthenewpoint xwillbegivenby x( 0 )\u2212 \ue00f g.Substitutingthisintoour\napproximation,weobtain\nf( x( 0 )\u2212 \u2248 \ue00f g) f( x( 0 ))\u2212 \ue00f g\ue03eg+1\n2\ue00f2g\ue03eH g . (4.9)\nTherearethree\u00a0termshere:theoriginalvalue\u00a0ofthefunction,\u00a0the expected\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\ngradientdescentstepcanactuallymoveuphill.When g\ue03eH giszeroornegative,\ntheTaylorseriesapproximationpredictsthatincreasing \ue00fforeverwilldecrease f\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge \ue00f,so\nonemustresorttomoreheuristicchoicesof \ue00finthiscase.When g\ue03eH gispositive,\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\nthefunctionthemostyields\n\ue00f\u2217=g\ue03eg\ng\ue03eH g. (4.10)\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\nmaximaleigenvalue \u03bb m a x,thenthisoptimalstepsizeisgivenby1\n\u03bbmax.Tothe\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\nrate.\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\npoint, f\ue030( x) = 0.Whenthesecondderivative f\ue030 \ue030( x) >0,the\ufb01rstderivative f\ue030( x)\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n8 8", "CHAPTER4.NUMERICALCOMPUTATION\nf\ue030( x \ue00f\u2212) <0and f\ue030( x+ \ue00f) >0forsmallenough \ue00f.Inotherwords,aswemove\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\nbeginstopointuphilltotheleft.\u00a0Thus,when f\ue030( x)=0and f\ue030 \ue030( x) >0,wecan\nconcludethat xisalocalminimum.Similarly,when f\ue030( x) = 0and f\ue030 \ue030( x) <0,we\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\nt e st.Unfortunately,when f\ue030 \ue030( x) = 0,thetestisinconclusive.Inthiscase xmay\nbeasaddlepoint,orapartofa\ufb02atregion.\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\n\u2207 x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\nHessianispositivede\ufb01nite(allitseigenvaluesarepositive),thepointisalocal\nminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\nderivativetest.Likewise,whentheHessianisnegativede\ufb01nite(allitseigenvalues\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\npossibleto\ufb01ndpositiveevidenceofsaddlepointsinsomecases.\u00a0Whenatleast\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother\ncrosssection.See\ufb01gureforanexample.Finally,themultidimensionalsecond 4.5\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\nInmultipledimensions,thereisadi\ufb00erentsecondderivativeforeachdirection\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures\nhowmuchthesecondderivativesdi\ufb00erfromeachother.WhentheHessianhasa\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\nremainsnegativeforlonger.Italsomakesitdi\ufb03culttochooseagoodstepsize.\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing\nuphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\nstepsizeistoosmalltomakesigni\ufb01cantprogressinotherdirectionswithless\ncurvature.See\ufb01gureforanexample.4.6\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n8 9", "CHAPTER4.NUMERICALCOMPUTATION\n\ue078\ue031\u0000\ue031 \ue035\ue030\ue031 \ue035\ue078 \ue032\u0000\ue031 \ue035\ue030\ue031 \ue035\ue066\ue078\ue028\ue031\ue03b \ue078\ue032\ue029\n\u0000\ue035 \ue030 \ue030\ue030\ue035 \ue030 \ue030\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\ninthisexampleis f( x)= x2\n1\u2212 x2\n2.Alongtheaxiscorrespondingto x 1,thefunction\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\neigenvectoroftheHessianwithnegativeeigenvalue.Thename\u201csaddlepoint\u201dderivesfrom\nthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\n9 0", "CHAPTER4.NUMERICALCOMPUTATION\n\u2212 \u2212 \u2212 3 0 2 0 1 0 0 1 0 2 0\nx 1\u2212 3 0\u2212 2 0\u2212 1 001 02 0x 2\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\nhas\ufb01vetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\ncurvatureisinthedirection[1 ,1]\ue03eandtheleastcurvatureisinthedirection[1 ,\u22121]\ue03e.The\nredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat\nthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\ndirectioninthiscontext.\n9 1", "CHAPTER4.NUMERICALCOMPUTATION\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n\u2019 s m e t ho d.\nNewton\u2019smethodisbasedonusingasecond-orderTaylorseriesexpansionto\napproximatenearsomepoint f() x x( 0 ):\nf f () x\u2248( x( 0 ))+( x x\u2212( 0 ))\ue03e\u2207 x f( x( 0 ))+1\n2( x x\u2212( 0 ))\ue03eH x()( f( 0 ))( x x\u2212( 0 )) .(4.11)\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\nx\u2217= x( 0 )\u2212 H x()( f( 0 ))\u2212 1\u2207 x f( x( 0 )) . (4.12)\nWhen fisapositivede\ufb01nitequadraticfunction,Newton\u2019smethodconsistsof\napplyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\nde\ufb01nitequadratic,Newton\u2019smethodconsistsofapplyingequationmultiple4.12\ntimes.\u00a0Iterativelyupdatingtheapproximation andjumpingtotheminimumof\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\npropertynearasaddlepoint.Asdiscussedinsection,Newton\u2019smethodis 8.2.3\nonlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\npointsunlessthegradientpointstowardthem.\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\narecalled \ufb01r st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\nalsousetheHessianmatrix,suchasNewton\u2019smethod,arecalled se c o nd-or d e r\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).\nThe\u00a0optimization algorithms\u00a0employedin\u00a0mostcontextsin\u00a0this\u00a0book\u00a0are\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\nusedindeeplearningisquitecomplicated.Inmanyother\ufb01elds,thedominant\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\noffunctions.\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-\ningourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\nofchangeisboundedbya L i psc hi t z c o nst antL:\n\u2200\u2200| \u2212 |\u2264L||\u2212|| x , y , f() x f() y x y 2 . (4.13)\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n9 2", "CHAPTER4.NUMERICALCOMPUTATION\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\nwithrelativelyminormodi\ufb01cations.\nPerhapsthemostsuccessful\ufb01eldofspecializedoptimization is c o n v e x o p-\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\napplicableonlytoconvexfunctions\u2014functionsforwhichtheHessianispositive\nsemide\ufb01niteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\nproblemsindeeplearningaredi\ufb03culttoexpressintermsofconvexoptimization.\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For\nmoreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\norRockafellar1997().\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\npossible\u00a0values\u00a0of x.Insteadwemay\u00a0wishto\u00a0\ufb01nd\u00a0themaximal\u00a0or\u00a0minimal\nvalue\u00a0of f( x)for\u00a0valuesof xinsome\u00a0set S.Thisis\u00a0known\u00a0as c o nst r ai n e d\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\nconstrainedoptimization terminology.\nWeoftenwishto\ufb01ndasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||\u2264 x 1\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize \ue00f,\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\nalinesearch,wecansearchonlyoverstepsizes \ue00fthatyieldnew xpointsthatare\nfeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\nWhenpossible,thismethodcanbemademoree\ufb03cientbyprojectingthegradient\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\nthelinesearch(,).Rosen1960\nAmoresophisticatedapproachistodesignadi\ufb00erent,unconstrainedopti-\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\n9 3", "CHAPTER4.NUMERICALCOMPUTATION\nx\u2208 R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\ng( \u03b8) = f([cossin \u03b8 , \u03b8]\ue03e)withrespectto \u03b8,thenreturn[cossin \u03b8 , \u03b8]asthesolution\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\nbetweenoptimization problemsmustbedesignedspeci\ufb01callyforeachcasewe\nencounter.\nThe K ar ush\u2013 K u h n \u2013 T uc k e r(KKT)approach1providesaverygeneralso-\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea\nnewfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n.\nTode\ufb01netheLagrangian,we\ufb01rstneedtodescribe Sintermsofequations\nandinequalities.\u00a0W ewantadescriptionof Sintermsof mfunctions g( ) iand n\nfunctions h( ) jsothat S={|\u2200 x i , g( ) i( x) = 0and\u2200 j , h( ) j( x)\u22640}.Theequations\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s\nWeintroducenewvariables \u03bb iand \u03b1 jforeachconstraint,thesearecalledthe\nKKTmultipliers.ThegeneralizedLagrangianisthende\ufb01nedas\nL , , f ( x \u03bb \u03b1) = ()+ x\ue058\ni\u03bb i g( ) i()+ x\ue058\nj\u03b1 j h( ) j() x .(4.14)\nWecannowsolveaconstrainedminimization problemusingunconstrained\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x \u221e\nmin\nxmax\n\u03bbmax\n\u03b1 \u03b1 , \u2265 0L , , . ( x \u03bb \u03b1) (4.15)\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x\nmin\nx \u2208 Sf .() x (4.16)\nThisfollowsbecauseanytimetheconstraintsaresatis\ufb01ed,\nmax\n\u03bbmax\n\u03b1 \u03b1 , \u2265 0L , , f , ( x \u03bb \u03b1) = () x (4.17)\nwhileanytimeaconstraintisviolated,\nmax\n\u03bbmax\n\u03b1 \u03b1 , \u2265 0L , , . ( x \u03bb \u03b1) = \u221e (4.18)\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\n9 4", "CHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged.\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\ngrangefunctionof,whichleadstothisoptimization problem: \u2212 f() x\nmin\nxmax\n\u03bbmax\n\u03b1 \u03b1 , \u2265 0\u2212 f()+ x\ue058\ni\u03bb i g( ) i()+ x\ue058\nj\u03b1 j h( ) j() x .(4.19)\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\nmax\nxmin\n\u03bbmin\n\u03b1 \u03b1 , \u2265 0f()+ x\ue058\ni\u03bb i g( ) i() x\u2212\ue058\nj\u03b1 j h( ) j() x .(4.20)\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemayde\ufb01neit\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\nanysignforeach \u03bb i.\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\nh( ) i( x)is ac t i v eif h( ) i( x\u2217) = 0.Ifaconstraintisnotactive,thenthesolutionto\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes\nothersolutions.Forexample,aconvexproblemwithanentireregionofglobally\noptimalpoints(awide,\ufb02at,regionofequalcost)couldhaveasubsetofthis\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\nthesolutiontomin xmax \u03bbmax \u03b1 \u03b1 , \u2265 0 L( x \u03bb \u03b1 , ,)willhave \u03b1 i=0.Wecanthus\nobservethatatthesolution, \u03b1 h\ue00c( x)= 0.Inotherwords,forall i,weknow\nthatatleastoneoftheconstraints \u03b1 i\u22650and h( ) i( x)\u22640mustbeactiveatthe\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\ntoin\ufb02uencethesolutionto x,ortheinequalityhasnoin\ufb02uenceonthesolution\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-\nmizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\nbutnotalwayssu\ufb03cientconditions,forapointtobeoptimal.Theconditionsare:\n\u2022ThegradientofthegeneralizedLagrangianiszero.\n\u2022AllconstraintsonbothandtheKKTmultipliersaresatis\ufb01ed. x\n9 5", "CHAPTER4.NUMERICALCOMPUTATION\n\u2022Theinequalityconstraintsexhibit\u201ccomplementary slackness\u201d: \u03b1 h\ue00c( x) = 0.\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\n4. 5 E x am p l e: L i n ear L eas t S q u are s\nSupposewewantto\ufb01ndthevalueofthatminimizes x\nf() = x1\n2||\u2212|| A x b2\n2 . (4.21)\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisprobleme\ufb03ciently.\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork.\nFirst,weneedtoobtainthegradient:\n\u2207 x f() = x A\ue03e( ) = A x b\u2212 A\ue03eA x A\u2212\ue03eb . (4.22)\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\nfordetails.\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\n2||\u2212|| A x b2\n2withrespectto x\nusinggradientdescent,startingfromanarbitraryvalueof. x\nSetthestepsize()andtolerance()tosmall,positivenumbers. \ue00f \u03b4\nwhi l e|| A\ue03eA x A\u2212\ue03eb|| 2 > \u03b4 do\nx x\u2190 \u2212 \ue00f\ue000\nA\ue03eA x A\u2212\ue03eb\ue001\ne nd whi l e\nOnecanalsosolvethisproblemusingNewton\u2019smethod.Inthiscase,because\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewton\u2019s\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\nstep.\nNowsuppose\u00a0we\u00a0wishto\u00a0minimizethesame\u00a0function,butsubjectto\u00a0the\nconstraint x\ue03ex\u22641.Todoso,weintroducetheLagrangian\nL , \u03bb f \u03bb ( x) = ()+ x\ue010\nx\ue03ex\u22121\ue011\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\n\u03bb , \u03bb \u2265 0L , \u03bb . ( x) (4.24)\n9 6", "CHAPTER4.NUMERICALCOMPUTATION\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemust\ufb01nda\nsolutionwheretheconstraintisactive.Bydi\ufb00erentiating theLagrangianwith\nrespectto,weobtaintheequation x\nA\ue03eA x A\u2212\ue03eb x+2 \u03bb= 0 . (4.25)\nThistellsusthatthesolutionwilltaketheform\nx A= (\ue03eA I+2 \u03bb)\u2212 1A\ue03eb . (4.26)\nThemagnitudeof \u03bbmustbechosensuchthattheresultobeystheconstraint.We\ncan\ufb01ndthisvaluebyperforminggradientascenton.Todoso,observe \u03bb\n\u2202\n\u2202 \u03bbL , \u03bb( x) = x\ue03ex\u22121 . (4.27)\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\nuphillandincreasetheLagrangianwithrespectto \u03bb,weincrease \u03bb.Becausethe\ncoe\ufb03cientonthe x\ue03expenaltyhasincreased,solvingthelinearequationfor xwill\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation\nandadjusting \u03bbcontinuesuntil xhasthecorrectnormandthederivativeon \u03bbis\n0.\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-\ufb02edged\nlearningsystems.\n9 7", "C h a p t e r 5\nMac h i n e L e ar n i n g B asics\nDeeplearningisaspeci\ufb01ckindofmachinelearning.Inordertounderstand\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy\n()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\non\u00a0traditional machinelearning\u00a0techniques\u00a0thathavestrongly\u00a0in\ufb02uenced the\ndevelopmentofdeeplearningalgorithms.\nWebeginwithade\ufb01nitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm.\u00a0W ethenproceedtodescribehowthe\nchallengeof\ufb01ttingthetrainingdatadi\ufb00ersfromthechallengeof\ufb01ndingpatterns\nthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\nonprovingcon\ufb01denceintervalsaroundthesefunctions;wethereforepresentthe\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.\nMostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory.\u00a0Mostdeeplearning\nalgorithmsare\u00a0basedonan\u00a0optimization algorithmcalled\u00a0stochasticgradient\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\n98", "CHAPTER5.MACHINELEARNINGBASICS\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles.\n5.1LearningAlgorithms\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\nwhatdowemeanbylearning?Mitchell1997()providesthede\ufb01nition\u201cAcomputer\nprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\nimproveswithexperienceE.\u201dOnecanimagineaverywidevarietyofexperiences\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\nbooktoprovideaformalde\ufb01nitionofwhatmaybeusedforeachoftheseentities.\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndi\ufb00erentkindsoftasks,performance measuresandexperiencesthatcanbeused\ntoconstructmachinelearningalgorithms.\n5.1.1TheTask, T\nMachinelearningallowsustotackletasksthataretoodi\ufb03culttosolvewith\n\ufb01xedprogramswrittenanddesignedbyhumanbeings.Fromascienti\ufb01cand\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\nprinciplesthatunderlieintelligence.\nInthisrelativelyformalde\ufb01nitionoftheword\u201ctask,\u201dtheprocessoflearning\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe\ntask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspeci\ufb01eshowtowalkmanually.\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa\nvectorx\u2208 Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\n9 9", "CHAPTER5.MACHINELEARNINGBASICS\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\ncommonmachinelearningtasksincludethefollowing:\n\u2022Classi\ufb01cation:Inthistypeoftask,thecomputerprogramisaskedtospecify\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\nalgorithmisusuallyaskedtoproduceafunctionf: Rn\u2192{1,...,k}.When\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\nidenti\ufb01edbynumericcodey.Thereareothervariantsoftheclassi\ufb01cation\ntask,forexample,wherefoutputsaprobabilitydistributionoverclasses.\nAnexampleofaclassi\ufb01cationtaskisobjectrecognition,wheretheinput\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\ndi\ufb00erentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith\ndeeplearning( ,; ,).Object Krizhevskyetal.2012Io\ufb00eandSzegedy2015\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\ntheirusers.\n\u2022Classi\ufb01cationwithmissinginputs:Classi\ufb01cationbecomesmorechal-\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassi\ufb01cation\ntask,thelearningalgorithmonlyhastode\ufb01neafunctionmapping single\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\nbemissing,ratherthanprovidingasingleclassi\ufb01cationfunction,thelearning\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\nfyingxwithadi\ufb00erentsubsetofitsinputsmissing.Thiskindofsituation\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\nareexpensiveorinvasive.Onewaytoe\ufb03cientlyde\ufb01nesuchalargeset\noffunctionsistolearnaprobabilitydistributionoveralloftherelevant\nvariables,thensolvetheclassi\ufb01cationtaskbymarginalizing outthemissing\nvariables.Withninputvariables,wecannowobtainall2ndi\ufb00erentclassi\ufb01-\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis\nsectioncanalsobegeneralizedtoworkwithmissinginputs;classi\ufb01cation\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\n1 0 0", "CHAPTER5.MACHINELEARNINGBASICS\n\u2022Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\nisaskedtooutputafunctionf: Rn\u2192 R.Thistypeoftaskissimilarto\nclassi\ufb01cation,exceptthattheformatofoutputisdi\ufb00erent.Anexampleof\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\nalgorithmictrading.\n\u2022Transcription:Inthistypeoftask,themachinelearningsystemisasked\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\nrecognition,thecomputerprogramisshownaphotographcontainingan\nimageoftextandisaskedtoreturnthistextintheformofasequence\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.\n2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\n2012b).\n\u2022Machinetranslation:Inamachinetranslationtask,theinputalready\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram\nmustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\n\u2022Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\nimportantrelationshipsbetweenthedi\ufb00erentelements.Thisisabroad\ncategory,andsubsumesthetranscriptionandtranslationtasksdescribed\nabove,butalsomanyothertasks.Oneexampleisparsing\u2014mappinga\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\ntask.Anotherexampleispixel-wisesegmentationofimages,\u00a0wherethe\ncomputerprogramassignseverypixelinanimagetoaspeci\ufb01ccategory.For\n1 0 1", "CHAPTER5.MACHINELEARNINGBASICS\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare\ncalledstructuredoutputtasksbecausetheprogrammustoutputseveral\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\nanimagecaptioningprogrammustformavalidsentence.\n\u2022Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\nthroughasetofeventsorobjects,and\ufb02agssomeofthemasbeingunusual\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard\ninformation,thethief\u2019spurchaseswilloftencomefromadi\ufb00erentprobability\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\nsurveyofanomalydetectionmethods.\n\u2022Synthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata.\u00a0Synthesisandsamplingviamachinelearningcanbeuseful\nformediaapplicationswhereitcanbeexpensiveorboringforanartistto\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeci\ufb01c\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform\ncontainingaspokenversionofthatsentence.\u00a0Thisisakindofstructured\noutputtask,butwiththeaddedquali\ufb01cationthatthereisnosinglecorrect\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\n\u2022Imputationofmissingvalues:Inthistypeoftask,themachinelearning\nalgorithmisgivenanewexamplex\u2208 Rn,butwithsomeentriesx iofx\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\nentries.\n1 0 2", "CHAPTER5.MACHINELEARNINGBASICS\n\u2022Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\ninputacorruptedexample\u02dcx\u2208 Rnobtainedbyanunknowncorruptionprocess\nfromacleanexamplex\u2208 Rn.Thelearnermustpredictthecleanexample\nxfromitscorruptedversion\u02dcx,ormoregenerallypredicttheconditional\nprobabilitydistributionp(x|\u02dcx).\n\u2022Densityestimationorprobabilitymassfunctionestimation:In\nthedensityestimationproblem,themachinelearningalgorithmisasked\ntolearnafunctionpmodel: Rn\u2192 R,wherepmodel(x)canbeinterpreted\nasaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe\nprobabilitydistribution.Densityestimationallowsustoexplicitlycapture\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\navaluex iismissingandalloftheothervalues,denotedx \u2212 i,aregiven,\nthenweknowthedistributionoveritisgivenbyp(x i|x \u2212 i).Inpractice,\ndensityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\nintractable.\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\ndo,nottode\ufb01nearigidtaxonomyoftasks.\n5.1.2ThePerformanceMeasure, P\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis\nspeci\ufb01ctothetaskbeingcarriedoutbythesystem. T\nFortaskssuchasclassi\ufb01cation,classi\ufb01cationwithmissinginputs,andtran-\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n1 0 3", "CHAPTER5.MACHINELEARNINGBASICS\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\nifitiscorrectlyclassi\ufb01edand1ifitisnot.Fortaskssuchasdensityestimation,\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\nloss.Instead,wemustuseadi\ufb00erentperformancemetricthatgivesthemodel\nacontinuous-valuedscoreforeachexample.Themostcommonapproachisto\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\nlearningsystem.\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,\nbutitisoftendi\ufb03culttochooseaperformancemeasurethatcorrespondswellto\nthedesiredbehaviorofthesystem.\nInsomecases,thisisbecauseitisdi\ufb03culttodecidewhatshouldbemeasured.\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\nofthesystemattranscribingentiresequences,orshouldweuseamore\ufb01ne-grained\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe\nsystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\naspeci\ufb01cpointinspaceinmanysuchmodelsisintractable.Inthesecases,one\nmustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\nordesignagoodapproximationtothedesiredcriterion.\n5.1.3TheExperience, E\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\nlearningprocess.\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\n1 0 4", "CHAPTER5.MACHINELEARNINGBASICS\nde\ufb01nedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\ndi\ufb00erentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset\nalsorecordswhichspecieseachplantbelongedto.Threedi\ufb00erentspeciesare\nrepresentedinthedataset.\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms\nperformotherroles,likeclustering,whichconsistsofdividingthedatasetinto\nclustersofsimilarexamples.\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\ndi\ufb00erentspeciesbasedontheirmeasurements.\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples\nofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror\nteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\nUnsupervisedlearningandsupervisedlearningarenotformallyde\ufb01nedterms.\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\nthatforavector x\u2208 Rn,thejointdistributioncanbedecomposedas\np() = xn\ue059\ni=1p(x i|x1,...,x i \u22121). (5.1)\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof\nmodelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\n1 0 5", "CHAPTER5.MACHINELEARNINGBASICS\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\nunsupervised\u00a0learningtechnologiesto\u00a0learn\u00a0thejointdistributionp( x,y)and\ninferring\npy(| x) =p,y( x)\ue050\ny\ue030p,y( x\ue030). (5.2)\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classi\ufb01cation\nandstructuredoutputproblemsassupervisedlearning.Densityestimationin\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning\nwithdeepmodels,seeKotzias 2015etal.().\nSomemachinelearningalgorithmsdonotjustexperiencea\ufb01xeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.\u00a0Such\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013\nMostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures.\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\nmatrixisamatrixcontainingadi\ufb00erentexampleineachrow.Eachcolumnofthe\nmatrixcorrespondstoadi\ufb00erentfeature.Forinstance,theIrisdatasetcontains\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent\nthedatasetwithadesignmatrixX\u2208 R1504 \u00d7,whereX i ,1isthesepallengthof\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\nwithdi\ufb00erentwidthsandheights,thendi\ufb00erentphotographswillcontaindi\ufb00erent\nnumbersofpixels,sonotallofthephotographs maybedescribedwiththesame\nlengthofvector.Sectionandchapterdescribehowtohandledi\ufb00erent 9.7 10\n1 0 6", "CHAPTER5.MACHINELEARNINGBASICS\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\nx() iandx() jhavethesamesize.\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject\nappearsineachofthephotos.Wemightdothiswithanumericcode,with0\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.\nJustasthereisnoformalde\ufb01nitionofsupervisedandunsupervisedlearning,\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\n5.1.4Example:LinearRegression\nOurde\ufb01nitionofamachinelearningalgorithmasanalgorithmthatiscapable\nofimprovingacomputerprogram\u2019sperformanceatsometaskviaexperienceis\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa\nsimplemachinelearningalgorithm:linearregression.Wewillreturntothis\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\nunderstanditsbehavior.\nAsthenameimplies,linearregressionsolvesaregressionproblem.\u00a0Inother\nwords,thegoalistobuildasystemthatcantakeavectorx\u2208 Rnasinputand\npredictthevalueofascalary\u2208 Rasitsoutput.Inthecaseoflinearregression,\ntheoutputisalinearfunctionoftheinput.Let\u02c6ybethevaluethatourmodel\npredictsshouldtakeon.Wede\ufb01netheoutputtobe y\n\u02c6y= w\ue03ex (5.3)\nwherew\u2208 Rnisavectorof .parameters\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\nthecoe\ufb03cientthatwemultiplybyfeaturex ibeforesummingupthecontributions\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\neachfeaturea\ufb00ectstheprediction.\u00a0If afeaturex ireceivesapositiveweightw i,\n1 0 7", "CHAPTER5.MACHINELEARNINGBASICS\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction \u02c6y.\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\ndecreasesthevalueofourprediction.Ifafeature\u2019sweightislargeinmagnitude,\nthenithasalargee\ufb00ectontheprediction.Ifafeature\u2019sweightiszero,ithasno\ne\ufb00ectontheprediction.\nWethushaveade\ufb01nitionofourtaskT:\u00a0topredictyfromxbyoutputting\n\u02c6y= w\ue03ex.Nextweneedade\ufb01nitionofourperformancemeasure,.P\nSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\ntargetsasy()test.\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\nsquarederrorofthemodelonthetestset.If\u02c6y()testgivesthepredictionsofthe\nmodelonthetestset,thenthemeansquarederrorisgivenby\nMSEtest=1\nm\ue058\ni(\u02c6y()test\u2212y()test)2\ni. (5.4)\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when \u02c6y()test=y()test.\nWecanalsoseethat\nMSEtest=1\nm||\u02c6y()test\u2212y()test||2\n2, (5.5)\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\nandthetargetsincreases.\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm\nisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\nminimizethemeansquarederroronthetrainingset,MSEtrain.\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\n\u2207 wMSEtrain= 0 (5.6)\n\u21d2\u2207 w1\nm||\u02c6y()train\u2212y()train||2\n2= 0 (5.7)\n\u21d21\nm\u2207 w||X()trainwy\u2212()train||2\n2= 0 (5.8)\n1 0 8", "CHAPTER5.MACHINELEARNINGBASICS\n\u2212 \u2212 1 0 . 0 5 0 0 0 5 1 0 . . . .\nx1\u2212 3\u2212 2\u2212 10123yL i n ea r r eg r es s i o n ex a m p l e\n0 5 1 0 1 5 . . .\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns\ntosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\n\u21d2\u2207 w\ue010\nX()trainwy\u2212()train\ue011\ue03e\ue010\nX()trainwy\u2212()train\ue011\n= 0(5.9)\n\u21d2\u2207 w\ue010\nw\ue03eX()train \ue03eX()trainww\u22122\ue03eX()train \ue03ey()train+y()train \ue03ey()train\ue011\n= 0\n(5.10)\n\u21d22X()train \ue03eX()trainwX\u22122()train \ue03ey()train= 0(5.11)\n\u21d2w=\ue010\nX()train \ue03eX()train\ue011\u22121\nX()train \ue03ey()train(5.12)\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\nsee\ufb01gure.5.1\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\naslightlymoresophisticatedmodelwithoneadditionalparameter\u2014an intercept\nterm.Inthismodelb\n\u02c6y= w\ue03ex+b (5.13)\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\nmappingfromfeaturestopredictionsisnowana\ufb03nefunction.Thisextensionto\na\ufb03nefunctionsmeansthattheplotofthemodel\u2019spredictionsstilllookslikea\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n1 0 9", "CHAPTER5.MACHINELEARNINGBASICS\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\nplaystheroleofthebiasparameter.Wewillfrequentlyusetheterm\u201clinear\u201dwhen\nreferringtoa\ufb03nefunctionsthroughoutthisbook.\nTheintercepttermbisoftencalledthebiasparameterofthea\ufb03netransfor-\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm\nisdi\ufb00erentfromtheideaofastatisticalbias,inwhichastatisticalestimation\nalgorithm\u2019sexpectedestimateofaquantityisnotequaltothetruequantity.\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\nlearningalgorithms.\n5.2Capacity,Over\ufb01ttingandUnder\ufb01tting\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\npreviouslyunseeninputs\u2014notjustthoseonwhichourmodelwastrained.\u00a0The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply\nanoptimization problem.Whatseparatesmachinelearningfromoptimization is\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\nwell.\u00a0Thegeneralization errorisde\ufb01nedastheexpectedvalueoftheerrorona\nnewinput.Heretheexpectationistakenacrossdi\ufb00erentpossibleinputs,drawn\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately\nfromthetrainingset.\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwy\u2212()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwy\u2212()test||2\n2.\nHowcanwea\ufb00ectperformanceonthetestsetwhenwegettoobserveonlythe\n1 1 0", "CHAPTER5.MACHINELEARNINGBASICS\ntrainingset?The\ufb01eldofstatisticallearningtheoryprovidessomeanswers.If\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\ndo.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\nsetarecollected,thenwecanmakesomeprogress.\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\ncalledthedatageneratingprocess.Wetypicallymakeasetofassumptions\nknowncollectivelyasthei.i.d.\u00a0assumptions.\u00a0Theseassumptionsarethatthe\nexamplesineachdatasetareindependentfromeachother,andthatthetrain\nsetandtestsetareidenticallydistributed,drawnfromthesameprobability\ndistributionaseachother.\u00a0Thisassumptionallowsustodescribethedatagen-\neratingprocesswithaprobabilitydistributionoverasingleexample.Thesame\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\nWecallthatsharedunderlyingdistributionthedatageneratingdistribution,\ndenotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto\nmathematically studytherelationshipbetweentrainingerrorandtesterror.\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\nexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\np(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset.Forsome\ufb01xedvaluew,theexpectedtrainingseterrorisexactlythesameas\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame\ndatasetsamplingprocess.Theonlydi\ufb00erencebetweenthetwoconditionsisthe\nnameweassigntothedatasetwesample.\nOfcourse,\u00a0when\u00a0weuseamachinelearning\u00a0algorithm,\u00a0w edonot\ufb01xthe\nparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\ntestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\ntheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\nlearningalgorithmwillperformareitsabilityto:\n1.\u00a0Makethetrainingerrorsmall.\n2.\u00a0Makethegapbetweentrainingandtesterrorsmall.\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunder\ufb01ttingandover\ufb01tting.Under\ufb01ttingoccurswhenthemodelisnotableto\nobtainasu\ufb03cientlylowerrorvalueonthetrainingset.Over\ufb01ttingoccurswhen\nthegapbetweenthetrainingerrorandtesterroristoolarge.\nWecancontrolwhetheramodelismorelikelytoover\ufb01torunder\ufb01tbyaltering\nitscapacity.Informally,amodel\u2019scapacityisitsabilityto\ufb01tawidevarietyof\n1 1 1", "CHAPTER5.MACHINELEARNINGBASICS\nfunctions.Modelswithlowcapacitymaystruggleto\ufb01tthetrainingset.Models\nwithhighcapacitycanover\ufb01tbymemorizingpropertiesofthetrainingsetthatdo\nnotservethemwellonthetestset.\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize\nlinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\nhypothesisspace.Doingsoincreasesthemodel\u2019scapacity.\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\narealreadyfamiliar,withprediction\n\u02c6ybwx. = + (5.15)\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\ncanlearnamodelthatisquadraticasafunctionof:x\n\u02c6ybw = +1xw+2x2. (5.16)\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input\nstillalinearfunctionoftheparameters,sowecanstillusethenormalequations\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\n\u02c6yb= +9\ue058\ni=1w ixi. (5.17)\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\namountoftrainingdatatheyareprovidedwith.Modelswithinsu\ufb03cientcapacity\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex\ntasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\nmayover\ufb01t.\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\nanddegree-9predictorattemptingto\ufb01taproblemwherethetrueunderlying\nfunctionisquadratic.\u00a0Thelinearfunctionisunabletocapturethecurvaturein\nthetrueunderlyingproblem,soitunder\ufb01ts.Thedegree-9predictoriscapableof\nrepresentingthecorrectfunction,butitisalsocapableofrepresentingin\ufb01nitely\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n1 1 2", "CHAPTER5.MACHINELEARNINGBASICS\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\nasolutionthatgeneralizeswellwhensomanywildlydi\ufb00erentsolutionsexist.In\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\nthetasksoitgeneralizeswelltonewdata.\n\ue078\ue030\ue079\ue055\ue06e \ue064 \ue065 \ue072 \ue066 \ue069 \ue074 \ue074 \ue069 \ue06e \ue067\n\ue078\ue030\ue079\ue041\ue070 \ue070 \ue072 \ue06f \ue070 \ue072 \ue069 \ue061 \ue074 \ue065 \ue020 \ue063 \ue061 \ue070 \ue061 \ue063 \ue069 \ue074 \ue079\n\ue078\ue030\ue079\ue04f \ue076 \ue065 \ue072 \ue066 \ue069 \ue074 \ue074 \ue069 \ue06e \ue067\nFigure5.2:We\ufb01tthreemodelstothisexampletrainingset.Thetrainingdatawas\ngeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\nbyevaluatingaquadraticfunction.\u00a0 ( L e f t )Alinearfunction\ufb01ttothedatasu\ufb00ersfrom\nunder\ufb01tting\u2014itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\nquadraticfunction\ufb01ttothedatageneralizeswelltounseenpoints.Itdoesnotsu\ufb00erfrom\nasigni\ufb01cantamountofover\ufb01ttingorunder\ufb01tting.Apolynomialofdegree9\ufb01tto ( R i g h t )\nthedatasu\ufb00ersfromover\ufb01tting.HereweusedtheMoore-Penrosepseudoinversetosolve\ntheunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea.\nSofarwehavedescribedonlyonewayofchangingamodel\u2019scapacity:by\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew\nparametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\namodel\u2019scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\nmodelspeci\ufb01eswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\ntherepresentationalcapacityofthemodel.Inmanycases,\ufb01ndingthebest\nfunctionwithinthisfamilyisaverydi\ufb03cultoptimization problem.Inpractice,\nthelearningalgorithmdoesnotactually\ufb01ndthebestfunction,butmerelyone\nthatsigni\ufb01cantlyreducesthetrainingerror.Theseadditionallimitations,suchas\n1 1 3", "CHAPTER5.MACHINELEARNINGBASICS\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm\u2019s\ne\ufb00ectivecapacitymaybelessthantherepresentationalcapacityofthemodel\nfamily.\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\nmodelsarere\ufb01nementsofthoughtdatingbacktophilosophersatleastasearly\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\nmostwidelyknownasOccam\u2019srazor(c.1287-1347).Thisprinciplestatesthat\namongcompetinghypothesesthatexplainknownobservationsequallywell,one\nshouldchoosethe\u201csimplest\u201done.Thisideawasformalizedandmademoreprecise\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassi\ufb01er.The\nVCdimensionisde\ufb01nedasbeingthelargestpossiblevalueofmforwhichthere\nexistsatrainingsetofmdi\ufb00erentxpointsthattheclassi\ufb01ercanlabelarbitrarily.\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,\n1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\nintellectualjusti\ufb01cationthatmachinelearningalgorithmscanwork,buttheyare\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\ndi\ufb03culttodeterminethecapacityofdeeplearningalgorithms.\u00a0Theproblemof\ndeterminingthecapacityofadeeplearningmodelisespeciallydi\ufb03cultbecausethe\ne\ufb00ectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and\nwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\nproblemsinvolvedindeeplearning.\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\nsu\ufb03cientlycomplexhypothesistoachievelowtrainingerror.Typically,training\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,\ngeneralization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\nillustratedin\ufb01gure.5.3\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\n1 1 4", "CHAPTER5.MACHINELEARNINGBASICS\n0 O pti m a l C a pa c i t y\nC a pa c i t yE r r o rU nde r \ufb01tti ng z o ne O v e r \ufb01tti ng z o ne\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\nG e n e r a l i z a t i o n e r r o r\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\nbehavedi\ufb00erently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\narebothhigh.Thisistheunder\ufb01ttingregime.Asweincreasecapacity,trainingerror\ndecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheover\ufb01tting\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\nbyaparametervectorwhosesizeis\ufb01niteand\ufb01xedbeforeanydataisobserved.\nNon-parametric modelshavenosuchlimitation.\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\nwhichhasa\ufb01xed-lengthvectorofweights,thenearestneighborregressionmodel\nsimplystorestheXandyfromthetrainingset.\u00a0Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,\u02c6y=y iwherei=argmin||X i ,:\u2212||x2\n2.\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which\nmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdi\ufb00erent\noutputs)onanyregressiondataset.\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\n1 1 5", "CHAPTER5.MACHINELEARNINGBASICS\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\npolynomialexpansionoftheinput.\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata.\u00a0Evensuchamodelwillstillincursomeerroronmany\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase\nofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\ndistributioniscalledthe p,y(x)Bayeserror.\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until\nthebestpossibleerrorisachieved.Any\ufb01xedparametricmodelwithlessthan\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\n\ufb01gureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\nexamples.\n5.2.1TheNoFreeLunchTheorem\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom\na\ufb01nitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\nisnotlogicallyvalid.\u00a0Tologicallyinferaruledescribingeverymemberofaset,\nonemusthaveinformationabouteverymemberofthatset.\nInpart,machinelearningavoidsthisproblembyo\ufb00eringonlyprobabilisticrules,\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning.\u00a0Machine\nlearningpromisesto\ufb01ndrulesthatareprobably most correctaboutmembersof\nthesettheyconcern.\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\nallpossibledatageneratingdistributions,everyclassi\ufb01cationalgorithmhasthe\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n1 1 6", "CHAPTER5.MACHINELEARNINGBASICS\n\ue031 \ue030\ue030\ue031 \ue030\ue031\ue031 \ue030\ue032\ue031 \ue030\ue033\ue031 \ue030\ue034\ue031 \ue030\ue035\n\ue04e\ue075\ue06d \ue062 \ue065\ue072\ue020 \ue06f \ue066 \ue020 \ue074 \ue072\ue061 \ue069 \ue06e \ue069 \ue06e \ue067 \ue020 \ue065\ue078 \ue061 \ue06d \ue070 \ue06c \ue065 \ue073\ue030 \ue02e \ue030\ue030 \ue02e \ue035\ue031 \ue02e \ue030\ue031 \ue02e \ue035\ue032 \ue02e \ue030\ue032 \ue02e \ue035\ue033 \ue02e \ue030\ue033 \ue02e \ue035\ue045 \ue072\ue072\ue06f \ue072\ue020 \ue028 \ue04d \ue053 \ue045 \ue029\ue042 \ue061 \ue079 \ue065 \ue073\ue020 \ue065 \ue072 \ue072 \ue06f \ue072\n\ue054 \ue072 \ue061 \ue069 \ue06e \ue020 \ue028 \ue071 \ue075 \ue061 \ue064 \ue072 \ue061 \ue074 \ue069 \ue063 \ue029\n\ue054 \ue065 \ue073\ue074 \ue020 \ue028 \ue071 \ue075 \ue061 \ue064 \ue072 \ue061 \ue074 \ue069 \ue063 \ue029\n\ue054 \ue065 \ue073\ue074 \ue020 \ue028 \ue06f \ue070 \ue074 \ue069 \ue06d \ue061 \ue06c \ue020 \ue063 \ue061 \ue070 \ue061 \ue063 \ue069 \ue074 \ue079 \ue029\n\ue054 \ue072 \ue061 \ue069 \ue06e \ue020 \ue028 \ue06f \ue070 \ue074 \ue069 \ue06d \ue061 \ue06c \ue020 \ue063 \ue061 \ue070 \ue061 \ue063 \ue069 \ue074 \ue079 \ue029\n\ue031 \ue030\ue030\ue031 \ue030\ue031\ue031 \ue030\ue032\ue031 \ue030\ue033\ue031 \ue030\ue034\ue031 \ue030\ue035\n\ue04e\ue075\ue06d \ue062 \ue065\ue072\ue020 \ue06f \ue066 \ue020 \ue074 \ue072\ue061 \ue069 \ue06e \ue069 \ue06e \ue067 \ue020 \ue065\ue078 \ue061 \ue06d \ue070 \ue06c \ue065 \ue073\ue030\ue035\ue031 \ue030\ue031 \ue035\ue032 \ue030\ue04f \ue070 \ue074 \ue069 \ue06d \ue061 \ue06c \ue020 \ue063\ue061 \ue070 \ue061 \ue063\ue069\ue074\ue079 \ue020 \ue028 \ue070 \ue06f \ue06c \ue079 \ue06e \ue06f \ue06d \ue069 \ue061 \ue06c \ue020 \ue064 \ue065 \ue067 \ue072\ue065 \ue065 \ue029\nFigure5.4:Thee\ufb00ectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\nandthengeneratedseveraldi\ufb00erentsizesoftrainingset.Foreachsize,wegenerated40\ndi\ufb00erenttrainingsetsinordertoploterrorbarsshowing95percentcon\ufb01denceintervals.\n( T o p )TheMSEonthetrainingandtestsetfortwodi\ufb00erentmodels:aquadraticmodel,\nandamodelwithdegreechosentominimizethetesterror.Bothare\ufb01tinclosedform.For\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\nThisisbecauselargerdatasetsareharderto\ufb01t.Simultaneously,thetesterrordecreases,\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The\ntrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\ntomemorizespeci\ufb01cinstancesofthetrainingset.Asthetrainingsizeincreasestoin\ufb01nity,\nthetrainingerrorofany\ufb01xed-capacitymodel(here,thequadraticmodel)mustrisetoat\nleasttheBayeserror.\u00a0Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases.\u00a0Theoptimal\ncapacityplateausafterreachingsu\ufb03cientcomplexitytosolvethetask.\n1 1 7", "CHAPTER5.MACHINELEARNINGBASICS\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\ntothesameclass.\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\nalgorithmsthatperformwellonthesedistributions.\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal\nlearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\nunderstandwhatkindsofdistributionsarerelevanttothe\u201crealworld\u201dthatanAI\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\n5.2.2Regularization\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\nalgorithmstoperformwellonaspeci\ufb01ctask.Wedosobybuildingasetof\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith\nthelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\nconcretelyistoincreaseordecreasethemodel\u2019srepresentationalcapacitybyadding\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\nisabletochoose.Wegavethespeci\ufb01cexampleofincreasingordecreasingthe\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\nfarisoversimpli\ufb01ed.\nThebehaviorofouralgorithmisstronglya\ufb00ectednotjustbyhowlargewe\nmakethesetoffunctionsallowedinitshypothesisspace,butbythespeci\ufb01cidentity\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus\ncontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\nfunctions.\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\nispreferred.Theunpreferredsolutionwillbechosenonlyifit\ufb01tsthetraining\n1 1 8", "CHAPTER5.MACHINELEARNINGBASICS\ndatasigni\ufb01cantlybetterthanthepreferredsolution.\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speci\ufb01cally,\nJ() = wMSEtrain+\u03bbw\ue03ew, (5.18)\nwhere\u03bbisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference\nforsmallerweights.When\u03bb= 0,weimposenopreference,andlarger\u03bbforcesthe\nweightstobecomesmaller.\u00a0MinimizingJ(w)resultsinachoiceofweightsthat\nmakeatradeo\ufb00between\ufb01ttingthetrainingdataandbeingsmall.Thisgivesus\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\nexampleofhowwecancontrolamodel\u2019stendencytoover\ufb01torunder\ufb01tviaweight\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdi\ufb00erentvalues\nof.See\ufb01gurefortheresults. \u03bb 5.5\n\ue078\ue030\ue079\ue055 \ue06e \ue064 \ue065 \ue072 \ue066 \ue069 \ue074 \ue074 \ue069 \ue06e \ue067\n\ue028 \ue045 \ue078 \ue063 \ue065 \ue073\ue073\ue069\ue076 \ue065 \ue020 \ue0b8 \ue029\n\ue078\ue030\ue079\ue041 \ue070 \ue070 \ue072 \ue06f \ue070 \ue072 \ue069 \ue061 \ue074 \ue065 \ue020 \ue077 \ue065 \ue069 \ue067 \ue068 \ue074 \ue020 \ue064 \ue065 \ue063 \ue061 \ue079\n\ue028 \ue04d \ue065 \ue064 \ue069 \ue075 \ue06d \ue020 \ue0b8 \ue029\n\ue078\ue030\ue079\ue04f \ue076 \ue065 \ue072 \ue066 \ue069 \ue074 \ue074 \ue069 \ue06e \ue067\n\ue028 \ue030 \ue029 \ue0b8 \ue021\nFigure5.5:We\ufb01tahigh-degreepolynomialregressionmodeltoourexampletrainingset\nfrom\ufb01gure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromover\ufb01tting.\n( L e f t )Withverylarge\u03bb,wecanforcethemodeltolearnafunctionwithnoslopeat\nall.Thisunder\ufb01tsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. \u03bb\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\ncoe\ufb03cients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the\ndegree-9polynomialover\ufb01tssigni\ufb01cantly,aswesawin\ufb01gure.5.2\n1 1 9", "CHAPTER5.MACHINELEARNINGBASICS\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;\u03b8)by\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\ndecay,theregularizeris\u2126(w) =w\ue03ew.Inchapter,wewillseethatmanyother 7\nregularizersarepossible.\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\nofcontrollingamodel\u2019scapacitythanincludingorexcludingmembersfromthe\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas\nexpressinganin\ufb01nitelystrongpreferenceagainstthatfunction.\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\nde\ufb01nedwithsmallerweightsexplicitly,\u00a0viaanextraterminthecriterionwe\nminimize.Thereare\u00a0many\u00a0otherwaysof\u00a0expressing preferencesfor\u00a0di\ufb00erent\nsolutions,bothimplicitlyandexplicitly.Together,thesedi\ufb00erentapproaches\nareknownasregularization.\u00a0Regularizationisanymodi\ufb01cationwemaketoa\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\ntrainingerror.Regularizationisoneofthecentralconcernsofthe\ufb01eldofmachine\nlearning,rivaledinitsimportanceonlybyoptimization.\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks\nthatpeoplecando)mayallbesolvede\ufb00ectivelyusingverygeneral-purposeforms\nofregularization.\n5.3HyperparametersandValidationSets\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\nitself(thoughwecan\u00a0designa\u00a0nestedlearning\u00a0procedure\u00a0where one\u00a0learning\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).\nInthepolynomialregressionexamplewesawin\ufb01gure,thereisasingle 5.2\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\nparameter.The\u03bbvalueusedtocontrolthestrengthofweightdecayisanother\nexampleofahyperparameter.\nSometimesasettingischosentobeahyperparameter thatthelearningal-\ngorithmdoesnotlearnbecauseitisdi\ufb03culttooptimize.Morefrequently,the\n1 2 0", "CHAPTER5.MACHINELEARNINGBASICS\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\nalwayschoosethemaximumpossiblemodelcapacity,resultinginover\ufb01tting(refer\nto\ufb01gure).Forexample,wecanalways\ufb01tthetrainingsetbetterwithahigher 5.3\ndegreepolynomialandaweightdecaysettingof\u03bb= 0thanwecouldwithalower\ndegreepolynomialandapositiveweightdecaysetting.\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve.\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\nitshyperparameters .\u00a0Forthisreason,noexamplefromthetestsetcanbeused\ninthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\ntrainingdata.Speci\ufb01cally,wesplitthetrainingdataintotwodisjointsubsets.One\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.\nThesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\n20%forvalidation.Sincethevalidationsetisusedto\u201ctrain\u201dthehyperparameters ,\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\nInpractice,\u00a0when thesametestsethasbeenusedrepeatedlytoevaluate\nperformanceofdi\ufb00erentalgorithmsovermanyyears,andespeciallyifweconsider\nalltheattemptsfromthescienti\ufb01ccommunityatbeatingthereportedstate-of-\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotre\ufb02ectthe\ntrue\ufb01eldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\n1 2 1", "CHAPTER5.MACHINELEARNINGBASICS\n5.3.1Cross-Validation\nDividingthedatasetintoa\ufb01xedtrainingsetanda\ufb01xedtestsetcanbeproblematic\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\naroundtheestimatedaveragetesterror,makingitdi\ufb03culttoclaimthatalgorithm\nAworksbetterthanalgorithmonthegiventask. B\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone\ntousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\nthetrainingandtestingcomputationondi\ufb00erentrandomlychosensubsetsorsplits\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe\ndataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\nused.\n5.4Estimators,BiasandVariance\nThe\ufb01eldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful\ntoformallycharacterizenotionsofgeneralization, under\ufb01ttingandover\ufb01tting.\n5.4.1PointEstimation\nPointestimationistheattempttoprovidethesingle\u201cbest\u201dpredictionofsome\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterby\u03b8 \u02c6\u03b8.\nLet{x(1),...,x() m}beasetofmindependentandidenticallydistributed\n1 2 2", "CHAPTER5.MACHINELEARNINGBASICS\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)\ninthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\nofunsupervisedlearning.\u00a0The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error.\u00a0Theerrorson\nindividualexamplescanbeusedtocomputeacon\ufb01denceintervalaroundthemean\n(equation).\u00a0Whilethesecon\ufb01denceintervalsarenotwell-justi\ufb01edafterthe 5.47\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\nalgorithmAisbetterthanalgorithmBonlyifthecon\ufb01denceintervaloftheerror\nofalgorithmAliesbelowanddoesnotintersectthecon\ufb01denceintervalofalgorithm\nB.\nDe\ufb01neKFoldXV(): D,A,L,k\nRequire: D,thegivendataset,withelementsz() i\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\ninputandoutputsalearnedfunction\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\nanexamplez() i\u2208 \u2208 Dtoascalar R\nRequire:k,thenumberoffolds\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\nfordoikfromto1\nf i= (A D D\\ i)\nforz() jin D ido\ne j= (Lf i,z() j)\nendfor\nendfor\nReturne\n1 2 3", "CHAPTER5.MACHINELEARNINGBASICS\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\n\u02c6\u03b8 m= (gx(1),...,x() m). (5.19)\nThede\ufb01nitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\n\u03b8oreventhattherangeofgisthesameasthesetofallowablevaluesof\u03b8.\nThisde\ufb01nitionofapointestimatorisverygeneralandallowsthedesignerofan\nestimatorgreat\ufb02exibility.Whilealmostanyfunctionthusquali\ufb01esasanestimator,\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlying\u03b8that\ngeneratedthetrainingdata.\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\nthatthetrueparametervalue\u03b8is\ufb01xedbutunknown,whilethepointestimate\n\u02c6\u03b8isafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\nfunctionofthedataisrandom.Therefore \u02c6\u03b8isarandomvariable.\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators.\nFunctionEstimationAswementionedabove,sometimesweareinterestedin\nperformingfunctionestimation(orfunctionapproximation).Herewearetryingto\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\nwemayassumethaty=f(x)+\ue00f,where\ue00fstandsforthepartofythatisnot\npredictablefromx.\u00a0Infunctionestimation,weareinterestedinapproximating\nfwithamodelorestimate \u02c6f.Functionestimationisreallyjustthesameas\nestimatingaparameter\u03b8;thefunctionestimator \u02c6fissimplyapointestimatorin\nfunctionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\nafunction \u02c6f y mappingfromtox.\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\ndiscusswhattheytellusabouttheseestimators.\n5.4.2Bias\nThebiasofanestimatorisde\ufb01nedas:\nbias(\u02c6\u03b8 m) = ( E\u02c6\u03b8 m)\u2212\u03b8 (5.20)\n1 2 4", "CHAPTER5.MACHINELEARNINGBASICS\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\nand\u03b8isthetrueunderlyingvalueof\u03b8usedtode\ufb01nethedatageneratingdistri-\nbution.Anestimator \u02c6\u03b8 missaidtobeunbiasedifbias(\u02c6\u03b8 m) = 0,whichimplies\nthat E(\u02c6\u03b8 m)=\u03b8.Anestimator \u02c6\u03b8 missaidtobeasymptoticallyunbiasedif\nlim m \u2192 \u221ebias(\u02c6\u03b8 m) = 0,whichimpliesthatlim m \u2192 \u221e E(\u02c6\u03b8 m) = \u03b8.\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}\nthatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\nbutionwithmean:\u03b8\nPx(() i;) = \u03b8\u03b8x() i(1 )\u2212\u03b8(1 \u2212 x() i). (5.21)\nAcommonestimatorforthe\u03b8parameterofthisdistributionisthemeanofthe\ntrainingsamples:\n\u02c6\u03b8 m=1\nmm\ue058\ni=1x() i. (5.22)\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\nintoequation:5.20\nbias(\u02c6\u03b8 m) = [ E\u02c6\u03b8 m]\u2212\u03b8 (5.23)\n= E\ue022\n1\nmm\ue058\ni=1x() i\ue023\n\u2212\u03b8 (5.24)\n=1\nmm\ue058\ni=1E\ue068\nx() i\ue069\n\u2212\u03b8 (5.25)\n=1\nmm\ue058\ni=11\ue058\nx() i=0\ue010\nx() i\u03b8x() i(1 )\u2212\u03b8(1 \u2212 x() i)\ue011\n\u2212\u03b8(5.26)\n=1\nmm\ue058\ni=1()\u03b8\u2212\u03b8 (5.27)\n= = 0\u03b8\u03b8\u2212 (5.28)\nSince bias(\u02c6\u03b8) = 0,wesaythatourestimator \u02c6\u03b8isunbiased.\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\naccordingtoaGaussiandistributionp(x() i) =N(x() i;\u00b5,\u03c32),wherei\u2208{1,...,m}.\n1 2 5", "CHAPTER5.MACHINELEARNINGBASICS\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\npx(() i;\u00b5,\u03c32) =1\u221a\n2\u03c0\u03c32exp\ue020\n\u22121\n2(x() i\u2212\u00b5)2\n\u03c32\ue021\n.(5.29)\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\nmean:\n\u02c6\u00b5 m=1\nmm\ue058\ni=1x() i(5.30)\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\nitsexpectation:\nbias(\u02c6\u00b5 m) = [\u02c6 E\u00b5 m]\u2212\u00b5 (5.31)\n= E\ue022\n1\nmm\ue058\ni=1x() i\ue023\n\u2212\u00b5 (5.32)\n=\ue020\n1\nmm\ue058\ni=1E\ue068\nx() i\ue069\ue021\n\u2212\u00b5 (5.33)\n=\ue020\n1\nmm\ue058\ni=1\u00b5\ue021\n\u2212\u00b5 (5.34)\n= = 0\u00b5\u00b5\u2212 (5.35)\nThuswe\ufb01ndthatthesamplemeanisanunbiasedestimatorofGaussianmean\nparameter.\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodi\ufb00erentestimatorsofthevarianceparameter\u03c32ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\nThe\ufb01rstestimatorof\u03c32weconsiderisknownasthesamplevariance:\n\u02c6\u03c32\nm=1\nmm\ue058\ni=1\ue010\nx() i\u2212\u02c6\u00b5 m\ue0112\n, (5.36)\nwhere \u02c6\u00b5 misthesamplemean,de\ufb01nedabove.Moreformally,weareinterestedin\ncomputing\nbias(\u02c6\u03c32\nm) = [\u02c6 E\u03c32\nm]\u2212\u03c32(5.37)\n1 2 6", "CHAPTER5.MACHINELEARNINGBASICS\nWebeginbyevaluatingtheterm E[\u02c6\u03c32\nm]:\nE[\u02c6\u03c32\nm] = E\ue022\n1\nmm\ue058\ni=1\ue010\nx() i\u2212\u02c6\u00b5 m\ue0112\ue023\n(5.38)\n=m\u22121\nm\u03c32(5.39)\nReturningtoequation,weconcludethatthebiasof 5.37 \u02c6\u03c32\nmis\u2212\u03c32/m.Therefore,\nthesamplevarianceisabiasedestimator.\nTheunbiasedsamplevarianceestimator\n\u02dc\u03c32\nm=1\nm\u22121m\ue058\ni=1\ue010\nx() i\u2212\u02c6\u00b5 m\ue0112\n(5.40)\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\nThatis,we\ufb01ndthat E[\u02dc\u03c32\nm] = \u03c32:\nE[\u02dc\u03c32\nm] = E\ue022\n1\nm\u22121m\ue058\ni=1\ue010\nx() i\u2212\u02c6\u00b5 m\ue0112\ue023\n(5.41)\n=m\nm\u22121E[\u02c6\u03c32\nm] (5.42)\n=m\nm\u22121\ue012m\u22121\nm\u03c32\ue013\n(5.43)\n= \u03c32. (5.44)\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\nestimatorsareclearlydesirable,theyarenotalwaysthe\u201cbest\u201destimators.Aswe\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\n5.4.3VarianceandStandardError\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.\nThevarianceofanestimatorissimplythevariance\nVar(\u02c6\u03b8) (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE(\u02c6\u03b8).\n1 2 7", "CHAPTER5.MACHINELEARNINGBASICS\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\nlowvariance.\nWhenwecomputeanystatisticusinga\ufb01nitenumberofsamples,ourestimate\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave\nobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\nbeendi\ufb00erent.Theexpecteddegreeofvariationinanyestimatorisasourceof\nerrorthatwewanttoquantify.\nThestandarderrorofthemeanisgivenby\nSE(\u02c6\u00b5 m) =\ue076\ue075\ue075\ue074Var\ue022\n1\nmm\ue058\ni=1x() i\ue023\n=\u03c3\u221am, (5.46)\nwhere\u03c32isthetruevarianceofthesamplesxi.Thestandarderrorisoften\nestimatedbyusinganestimateof\u03c3.Unfortunately,neitherthesquarerootof\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance\nprovideanunbiasedestimateofthestandarddeviation.Bothapproachestend\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\nForlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe\naccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\nfallsinanychoseninterval.Forexample,the95%con\ufb01denceintervalcenteredon\nthemean \u02c6\u00b5 mis\n(\u02c6\u00b5 m\u2212196SE( \u02c6.\u00b5 m)\u02c6,\u00b5 m+196SE( \u02c6.\u00b5 m)), (5.47)\nunderthenormaldistributionwithmean \u02c6\u00b5 mandvariance SE(\u02c6\u00b5 m)2.Inmachine\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm\nBiftheupperboundofthe95%con\ufb01denceintervalfortheerrorofalgorithmAis\nlessthanthelowerboundofthe95%con\ufb01denceintervalfortheerrorofalgorithm\nB.\n1 2 8", "CHAPTER5.MACHINELEARNINGBASICS\nExample:\u00a0BernoulliDistributionWeonceagainconsiderasetofsamples\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\n(recallP(x() i;\u03b8) =\u03b8x() i(1\u2212\u03b8)(1 \u2212 x() i)).Thistimeweareinterestedincomputing\nthevarianceoftheestimator \u02c6\u03b8 m=1\nm\ue050m\ni=1x() i.\nVar\ue010\n\u02c6\u03b8 m\ue011\n= Var\ue020\n1\nmm\ue058\ni=1x() i\ue021\n(5.48)\n=1\nm2m\ue058\ni=1Var\ue010\nx() i\ue011\n(5.49)\n=1\nm2m\ue058\ni=1\u03b8\u03b8 (1\u2212) (5.50)\n=1\nm2m\u03b8\u03b8 (1\u2212) (5.51)\n=1\nm\u03b8\u03b8 (1\u2212) (5.52)\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\nreturntowhenwediscussconsistency(seesection).5.4.5\n5.4.4Tradingo\ufb00BiasandVariancetoMinimizeMeanSquared\nError\nBiasandvariancemeasuretwodi\ufb00erentsourcesoferrorinanestimator.Bias\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected\nestimatorvaluethatanyparticularsamplingofthedataislikelytocause.\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\n\ufb01gureandweareonlyo\ufb00eredthechoicebetweenamodelwithlargebiasand 5.2\nonethatsu\ufb00ersfromlargevariance.Howdowechoosebetweenthem?\nThemostcommonwaytonegotiatethistrade-o\ufb00istousecross-validation.\nEmpirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\nMSE = [( E\u02c6\u03b8 m\u2212\u03b8)2] (5.53)\n= Bias(\u02c6\u03b8 m)2+Var(\u02c6\u03b8 m) (5.54)\n1 2 9", "CHAPTER5.MACHINELEARNINGBASICS\nTheMSEmeasurestheoverallexpecteddeviation\u2014in asquarederrorsense\u2014\nbetweentheestimatorandthetruevalueoftheparameter\u03b8.Asisclearfrom\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\nC apac i t yB i as Ge ne r al i z at i on\ne r r orV ar i anc e\nO pt i m al\nc apac i t yO v e r \ufb01t t i ng\u00a0z o n e U nde r \ufb01t t i ng\u00a0z o n e\nFigure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunder\ufb01tting\nwhenthecapacityisbelowthisoptimumandover\ufb01ttingwhenitisabove.Thisrelationship\nissimilartotherelationshipbetweencapacity,under\ufb01tting,andover\ufb01tting,discussedin\nsectionand\ufb01gure. 5.2 5.3\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine\nlearningconceptsofcapacity,under\ufb01ttingandover\ufb01tting.Inthecasewheregen-\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\nanddecreasebias.Thisisillustratedin\ufb01gure,whereweseeagaintheU-shaped 5.6\ncurveofgeneralization errorasafunctionofcapacity.\n5.4.5Consistency\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\n\ufb01xedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe\namountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\n1 3 0", "CHAPTER5.MACHINELEARNINGBASICS\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\nplimm \u2192 \u221e\u02c6\u03b8 m= \u03b8. (5.55)\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany\ue00f>0,\nP(|\u02c6\u03b8 m\u2212|\u03b8>\ue00f)\u21920asm\u2192\u221e.Theconditiondescribedbyequationis5.55\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\nstrongconsistencyreferringtothealmostsureconvergenceof\u02c6\u03b8to\u03b8.Almost\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\noccurswhenp(lim m \u2192 \u221e x() m= ) = 1x.\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\nnumberofdataexamplesgrows.However,thereverseisnottrue\u2014asymptotic\nunbiasednessdoesnotimplyconsistency.\u00a0Forexample,considerestimatingthe\nmeanparameter\u00b5ofanormaldistributionN(x;\u00b5,\u03c32),withadatasetconsisting\nofmsamples:{x(1),...,x() m}.Wecouldusethe\ufb01rstsamplex(1)ofthedataset\nasanunbiasedestimator:\u02c6\u03b8=x(1).Inthatcase, E(\u02c6\u03b8 m)=\u03b8sotheestimator\nisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\nestimatorasitisthecasethat not \u02c6\u03b8 m\u2192 \u2192\u221e\u03b8mas.\n5.5MaximumLikelihoodEstimation\nPreviously,wehaveseensomede\ufb01nitionsofcommonestimatorsandanalyzed\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeci\ufb01c\nfunctionsthataregoodestimatorsfordi\ufb00erentmodels.\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\nthetruebutunknowndatageneratingdistributionpdata() x.\nLetpmodel( x;\u03b8)beaparametricfamilyofprobabilitydistributionsoverthe\nsamespaceindexedby\u03b8.Inotherwords,pmodel(x;\u03b8)mapsanycon\ufb01gurationx\ntoarealnumberestimatingthetrueprobabilitypdata()x.\nThemaximumlikelihoodestimatorforisthende\ufb01nedas \u03b8\n\u03b8ML= argmax\n\u03b8pmodel(;) X\u03b8 (5.56)\n= argmax\n\u03b8m\ue059\ni=1pmodel(x() i;)\u03b8 (5.57)\n1 3 1", "CHAPTER5.MACHINELEARNINGBASICS\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\nForexample,itispronetonumericalunder\ufb02ow.Toobtainamoreconvenient\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\nintoasum:\n\u03b8ML= argmax\n\u03b8m\ue058\ni=1logpmodel(x() i;)\u03b8. (5.58)\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation\nwithrespecttotheempiricaldistribution\u02c6pdatade\ufb01nedbythetrainingdata:\n\u03b8ML= argmax\n\u03b8E x \u223c\u02c6 pdatalogpmodel(;)x\u03b8. (5.59)\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\nthedissimilaritybetweentheempiricaldistribution\u02c6pdatade\ufb01nedbythetraining\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\nDKL(\u02c6pdata\ue06bpmodel) = E x \u223c\u02c6 pdata[log \u02c6pdata()logx\u2212pmodel()]x.(5.60)\nThetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\nneedonlyminimize\n\u2212 E x \u223c\u02c6 pdata[logpmodel()]x (5.61)\nwhichisofcoursethesameasthemaximization inequation.5.59\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\nentropybetweenthedistributions.Manyauthorsusetheterm\u201ccross-entropy\u201dto\nidentifyspeci\ufb01callythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,\nbutthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\nentropybetweentheempiricaldistributionde\ufb01nedbythetrainingsetandthe\nprobabilitydistributionde\ufb01nedbymodel.Forexample,meansquarederroristhe\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\ntributionmatchtheempiricaldistribution\u02c6pdata.Ideally,wewouldliketomatch\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\ndistribution.\nWhiletheoptimal\u03b8isthesameregardlessofwhetherwearemaximizingthe\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n1 3 2", "CHAPTER5.MACHINELEARNINGBASICS\naredi\ufb00erent.Insoftware,weoftenphrasebothasminimizingacostfunction.\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\nourgoalistoestimateaconditionalprobabilityP( y x|;\u03b8)inordertopredict y\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\ntargets,thentheconditionalmaximumlikelihoodestimatoris\n\u03b8ML= argmax\n\u03b8P. ( ;)YX|\u03b8 (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\n\u03b8ML= argmax\n\u03b8m\ue058\ni=1log(Py() i|x() i;)\u03b8. (5.63)\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\nintroducedearlierinsection,maybejusti\ufb01edasamaximumlikelihood 5.1.4\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\ntotakeaninputxandproduceanoutputvalue \u02c6y.Themappingfromxto\u02c6yis\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum\nlikelihoodestimation.Insteadofproducingasingleprediction \u02c6y,wenowthink\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\nthatwithanin\ufb01nitelylargetrainingset,wemightseeseveraltrainingexamples\nwiththesameinputvaluexbutdi\ufb00erentvaluesofy.\u00a0Thegoalofthelearning\nalgorithmisnowto\ufb01tthedistributionp(y|x)toallofthosedi\ufb00erentyvalues\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\nweobtainedbefore,wede\ufb01nep(y|x) =N(y;\u02c6y(x;w),\u03c32).Thefunction \u02c6y(x;w)\ngivesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\nthevarianceis\ufb01xedtosomeconstant\u03c32chosenbytheuser.Wewillseethatthis\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\n1 3 3", "CHAPTER5.MACHINELEARNINGBASICS\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\ngivenby\nm\ue058\ni=1log(py() i|x() i;)\u03b8 (5.64)\n= log \u2212m\u03c3\u2212m\n2log(2)\u03c0\u2212m\ue058\ni=1\ue00d\ue00d\u02c6y() i\u2212y() i\ue00d\ue00d2\n2\u03c32,(5.65)\nwhere \u02c6y() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\nsquarederror,\nMSEtrain=1\nmm\ue058\ni=1||\u02c6y() i\u2212y() i||2, (5.66)\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields\nthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\nThetwocriteriahavedi\ufb00erentvaluesbutthesamelocationoftheoptimum.This\njusti\ufb01estheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\n5.5.2PropertiesofMaximumLikelihood\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\nbethebestestimatorasymptotically,asthenumberofexamplesm\u2192\u221e,interms\nofitsrateofconvergenceasincreases.m\nUnderappropriate\u00a0conditions,\u00a0the maximumlikelihood\u00a0estimatorhas\u00a0the\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\noftrainingexamplesapproachesin\ufb01nity,themaximumlikelihoodestimateofa\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\n\u2022Thetruedistributionpdatamustliewithinthemodelfamilypmodel(\u00b7;\u03b8).\nOtherwise,noestimatorcanrecoverpdata.\n\u2022Thetruedistributionpdatamustcorrespondtoexactlyonevalueof\u03b8.Other-\nwise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. \u03b8\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\ntor,manyofwhichsharethepropertyofbeingconsistentestimators.\u00a0However,\n1 3 4", "CHAPTER5.MACHINELEARNINGBASICS\nconsistentestimatorscandi\ufb00erintheirstatistice\ufb03ciency,meaningthatone\nconsistentestimatormayobtainlowergeneralization errorfora\ufb01xednumberof\nsamplesm,orequivalently,mayrequirefewerexamplestoobtaina\ufb01xedlevelof\ngeneralization error.\nStatisticale\ufb03ciencyistypicallystudiedintheparametriccase(likeinlinear\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto\nmeasurehowclosewearetothetrueparameterisbytheexpectedmeansquared\nerror,computingthesquareddi\ufb00erencebetweentheestimatedandtrueparameter\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\nformlarge,theCram\u00e9r-Raolowerbound(,;,)showsthatno Rao1945Cram\u00e9r1946\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\nestimator.\nForthesereasons(consistencyande\ufb03ciency),maximumlikelihoodisoften\nconsideredthepreferredestimatortouseformachinelearning.Whenthenumber\nofexamplesissmallenoughtoyieldover\ufb01ttingbehavior,regularizationstrategies\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\nthathaslessvariancewhentrainingdataislimited.\n5.6BayesianStatistics\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\ningasinglevalueof\u03b8,thenmakingallpredictionsthereafterbasedonthatone\nestimate.Anotherapproachistoconsiderallpossiblevaluesof\u03b8whenmakinga\nprediction.ThelatteristhedomainofBayesianstatistics.\nAsdiscussed\u00a0insection\u00a0,\u00a0the\u00a0frequen tist\u00a0perspective\u00a0isthat\u00a0thetrue 5.4.1\nparametervalue\u03b8is\ufb01xedbutunknown,whilethepointestimate \u02c6\u03b8isarandom\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\nTheBayesianperspectiveonstatisticsisquitedi\ufb00erent.\u00a0The Bayesianuses\nprobabilitytore\ufb02ectdegreesofcertaintyofstatesofknowledge.Thedatasetis\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameter\u03b8\nisunknownoruncertainandthusisrepresentedasarandomvariable.\nBeforeobservingthedata,werepresentourknowledgeof\u03b8usingtheprior\nprobabilitydistribution,p(\u03b8)(sometimesreferredtoassimply\u201ctheprior\u201d).\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\nquitebroad(i.e.withhighentropy)tore\ufb02ectahighdegreeofuncertaintyinthe\n1 3 5", "CHAPTER5.MACHINELEARNINGBASICS\nvalueof\u03b8beforeobservinganydata.Forexample,onemightassume that apriori\n\u03b8liesinsome\ufb01niterangeorvolume,withauniformdistribution.\u00a0Manypriors\ninsteadre\ufb02ectapreferencefor\u201csimpler\u201d\u00a0solutions(suchassmallermagnitude\ncoe\ufb03cients,orafunctionthatisclosertobeingconstant).\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\nrecoverthee\ufb00ectofdataonourbeliefabout\u03b8bycombiningthedatalikelihood\npx((1),...,x() m|\u03b8)withthepriorviaBayes\u2019rule:\npx(\u03b8|(1),...,x() m) =px((1),...,x() m|\u03b8\u03b8)(p)\npx((1),...,x() m)(5.67)\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\nfewhighlylikelyvaluesoftheparameters.\nRelativetomaximumlikelihoodestimation,Bayesianestimationo\ufb00erstwo\nimportantdi\ufb00erences.First,unlikethemaximumlikelihoodapproachthatmakes\npredictionsusingapointestimateof\u03b8,theBayesianapproachistomakepredictions\nusingafulldistributionover\u03b8.Forexample,afterobservingmexamples,the\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\npx((+1) m|x(1),...,x() m) =\ue05a\npx((+1) m| |\u03b8\u03b8)(px(1),...,x() m)d.\u03b8(5.68)\nHereeachvalueof\u03b8withpositiveprobabilitydensitycontributestotheprediction\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.\nAfterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueof\u03b8,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake.\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\ntaintyinagivenpointestimateof\u03b8byevaluatingitsvariance.Thevarianceof\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto\nprotectwellagainstover\ufb01tting.\u00a0Thisintegralisofcoursejustanapplicationof\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\nestimate.\nThesecondimportantdi\ufb00erencebetweentheBayesianapproachtoestimation\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n1 3 6", "CHAPTER5.MACHINELEARNINGBASICS\npriordistribution.Thepriorhasanin\ufb02uencebyshiftingprobabilitymassdensity\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\njudgmentimpactingthepredictions.\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\nisavailable,buttypicallysu\ufb00erfromhighcomputational costwhenthenumberof\ntrainingexamplesislarge.\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\nwelearnalinearmappingfromaninputvectorx\u2208 Rntopredictthevalueofa\nscalar.Thepredictionisparametrized bythevector y\u2208 R w\u2208 Rn:\n\u02c6y= w\ue03ex. (5.69)\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\nofovertheentiretrainingsetas: y\n\u02c6y()train= X()trainw. (5.70)\nExpressedasaGaussianconditionaldistributionony()train,wehave\np(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\n\u221dexp\ue012\n\u22121\n2(y()train\u2212X()trainw)\ue03e(y()train\u2212X()trainw)\ue013\n,\n(5.72)\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\n(X()train,y()train) ( ) assimplyXy,.\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\n\ufb01rstneedtospecifyapriordistribution.Thepriorshouldre\ufb02ectournaivebelief\naboutthevalueoftheseparameters.Whileitissometimesdi\ufb03cultorunnatural\ntoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\nabout\u03b8.\u00a0Forreal-valuedparametersitiscommontouseaGaussianasaprior\ndistribution:\np() = (;w Nw\u00b50, \u039b0) exp\u221d\ue012\n\u22121\n2(w\u00b5\u22120)\ue03e\u039b\u22121\n0(w\u00b5\u22120)\ue013\n,(5.73)\n1 3 7", "CHAPTER5.MACHINELEARNINGBASICS\nwhere\u00b50and \u039b0arethepriordistributionmeanvectorandcovariancematrix\nrespectively.1\nWiththepriorthusspeci\ufb01ed,wecannowproceedindeterminingtheposterior\ndistributionoverthemodelparameters.\np,p,p (wX|y) \u221d(yX|w)()w (5.74)\n\u221dexp\ue012\n\u22121\n2( )yXw\u2212\ue03e( )yXw\u2212\ue013\nexp\ue012\n\u22121\n2(w\u00b5\u22120)\ue03e\u039b\u22121\n0(w\u00b5\u22120)\ue013\n(5.75)\n\u221dexp\ue012\n\u22121\n2\ue010\n\u22122y\ue03eXww+\ue03eX\ue03eXww+\ue03e\u039b\u22121\n0w\u00b5\u22122\ue03e\n0 \u039b\u22121\n0w\ue011\ue013\n.\n(5.76)\nWenowde\ufb01ne \u039b m=\ue000\nX\ue03eX+ \u039b\u22121\n0\ue001 \u22121and\u00b5 m= \u039b m\ue000\nX\ue03ey+ \u039b\u22121\n0\u00b50\ue001\n.Using\nthesenewvariables,we\ufb01ndthattheposteriormayberewrittenasaGaussian\ndistribution:\np, (wX|y) exp\u221d\ue012\n\u22121\n2(w\u00b5\u2212 m)\ue03e\u039b\u22121\nm(w\u00b5\u2212 m)+1\n2\u00b5\ue03e\nm \u039b\u22121\nm\u00b5 m\ue013\n(5.77)\n\u221dexp\ue012\n\u22121\n2(w\u00b5\u2212 m)\ue03e\u039b\u22121\nm(w\u00b5\u2212 m)\ue013\n. (5.78)\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\ne\ufb00ectofBayesianinference.Inmostsituations,weset\u00b50to 0.Ifweset \u039b0=1\n\u03b1I,\nthen\u00b5 mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\nweightdecaypenaltyof\u03b1w\ue03ew.Onedi\ufb00erenceisthattheBayesianestimateis\nunde\ufb01nedif\u03b1issettozero\u2014-wearenotallowedtobegintheBayesianlearning\nprocesswithanin\ufb01nitelywideprioronw.Themoreimportantdi\ufb00erenceisthat\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\ndi\ufb00erentvaluesofare,ratherthanprovidingonlytheestimate w \u00b5 m.\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\nposteriordistributionovertheparameter\u03b8,itisstilloftendesirabletohavea\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\nd i a g o n a l c o v a ria n c e m a t rix \u039b0= diag( \u03bb0) .\n1 3 8", "CHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate.\u00a0Onecommonreasonfordesiringapointestimateisthat\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\nintractable,andapointestimateo\ufb00ersatractableapproximation.Ratherthan\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\nthebene\ufb01toftheBayesianapproachbyallowingthepriortoin\ufb02uencethechoice\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof\nmaximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\ncaseofcontinuous):\u03b8\n\u03b8MAP= argmax\n\u03b8p( ) = argmax\u03b8x|\n\u03b8log( )+log() px\u03b8|p\u03b8.(5.79)\nWerecognize,aboveontherighthandside,logp(x\u03b8|),i.e.thestandardlog-\nlikelihoodterm,and,correspondingtothepriordistribution. log()p\u03b8\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\n\u03bbI2),thenthelog-priortermin\nequationisproportional tothefamiliar 5.79 \u03bbw\ue03ewweightdecaypenalty,plusa\ntermthatdoesnotdependonwanddoesnota\ufb00ectthelearningprocess.MAP\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\ndecay.\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\nthepriceofincreasedbias.\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning\nregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(\u03b8).Not\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\ndistributionisnotallowedtodo.\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\ndistribution,astheprior(NowlanandHinton1992,).\n1 3 9", "CHAPTER5.MACHINELEARNINGBASICS\n5.7SupervisedLearningAlgorithms\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\ntrainingsetofexamplesofinputsxandoutputsy.\u00a0Inmanycasestheoutputs\nymaybedi\ufb03culttocollectautomatically andmustbeprovidedbyahuman\n\u201csupervisor,\u201dbutthetermstillappliesevenwhenthetrainingsettargetswere\ncollectedautomatically .\n5.7.1ProbabilisticSupervisedLearning\nMost\u00a0supervised\u00a0learning\u00a0algorithms\u00a0inthis\u00a0book\u00a0are\u00a0based\u00a0on estimating\u00a0a\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\nlikelihoodestimationto\ufb01ndthebestparametervector\u03b8foraparametricfamily\nofdistributions .py(|x\u03b8;)\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\npyy (| Nx\u03b8;) = (;\u03b8\ue03exI,). (5.80)\nWecangeneralizelinearregressiontotheclassi\ufb01cationscenariobyde\ufb01ninga\ndi\ufb00erentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and\nclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\nmustaddupto1.\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe\ninterval(0,1)andinterpretthatvalueasaprobability:\npy \u03c3 (= 1 ;) = |x\u03b8 (\u03b8\ue03ex). (5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassi\ufb01cationratherthanregression).\nInthecaseoflinearregression,wewereableto\ufb01ndtheoptimalweightsby\nsolvingthenormalequations.Logisticregressionissomewhatmoredi\ufb03cult.There\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative\nlog-likelihood(NLL)usinggradientdescent.\n1 4 0", "CHAPTER5.MACHINELEARNINGBASICS\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\ntherightkindofinputandoutputvariables.\n5.7.2SupportVectorMachines\nOneofthemostin\ufb02uentialapproachestosupervisedlearningisthesupportvector\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\nlogisticregressioninthatitisdrivenbyalinearfunctionw\ue03ex+b.Unlikelogistic\nregression,thesupportvectormachinedoesnotprovideprobabilities, butonly\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\nw\ue03ex+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\nw\ue03ex+bisnegative.\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan\nbere-writtenas\nw\ue03ex+= +bbm\ue058\ni=1\u03b1 ix\ue03ex() i(5.82)\nwherex() iisatrainingexampleand\u03b1isavectorofcoe\ufb03cients.Rewritingthe\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\nfunction\u03c6(x) andthedotproductwithafunctionk(xx,() i) =\u03c6(x)\u00b7\u03c6(x() i) called\nakernel.The \u00b7operatorrepresentsaninnerproductanalogousto\u03c6(x)\ue03e\u03c6(x() i).\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\nsomein\ufb01nitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for\nexample,innerproductsbasedonintegrationratherthansummation.Acomplete\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +\ue058\ni\u03b1 ik,(xx() i). (5.83)\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetween\u03c6(x)\nandf(x)islinear.Also,therelationshipbetween\u03b1andf(x)islinear.The\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying\n\u03c6()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\n1 4 1", "CHAPTER5.MACHINELEARNINGBASICS\nguaranteedtoconvergee\ufb03ciently.Thisispossiblebecauseweconsider\u03c6\ufb01xedand\noptimizeonly\u03b1,i.e.,theoptimization algorithmcanviewthedecisionfunction\nasbeinglinearinadi\ufb00erentspace.Second,thekernelfunctionkoftenadmits\nanimplementationthatissigni\ufb01cantlymorecomputational e\ufb03cientthannaively\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. \u03c6()x\nInsomecases,\u03c6(x)canevenbein\ufb01nitedimensional,whichwouldresultin\nanin\ufb01nitecomputational costforthenaive,explicitapproach.Inmanycases,\nk(xx,\ue030)isanonlinear,tractablefunctionofxevenwhen\u03c6(x)isintractable.As\nanexampleofanin\ufb01nite-dimens ionalfeaturespacewithatractablekernel,we\nconstructafeaturemapping\u03c6(x)overthenon-negativeintegersx.Supposethat\nthismappingreturnsavectorcontainingxonesfollowedbyin\ufb01nitelymanyzeros.\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\ntothecorrespondingin\ufb01nite-dimens ionaldotproduct.\nThemostcommonlyusedkernelistheGaussiankernel\nk, ,\u03c3 (uvuv ) = (N \u2212;02I) (5.84)\nwhere N(x;\u00b5, \u03a3)isthestandardnormaldensity.Thiskernelisalsoknownas\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\nproductinanin\ufb01nite-dimens ionalspace,butthederivationofthisspaceisless\nstraightforwardthaninourexampleofthekernelovertheintegers. min\nWecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\nforclassy.Whenatestpointx\ue030isnearxaccordingtoEuclideandistance,the\nGaussiankernelhasalargeresponse,indicatingthatx\ue030isverysimilartothex\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\nsimilarityofthecorrespondingtrainingexamples.\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\norkernelmethods( ,; WilliamsandRasmussen1996Sch\u00f6lkopf1999etal.,).\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample\ncontributesaterm\u03b1 ik(xx,() i)tothedecisionfunction.Supportvectormachines\nareabletomitigatethisbylearningan\u03b1vectorthatcontainsmostlyzeros.\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\nthetrainingexamplesthathavenon-zero\u03b1 i.Thesetrainingexamplesareknown\n1 4 2", "CHAPTER5.MACHINELEARNINGBASICS\nassupportvectors.\nKernelmachinesalsosu\ufb00erfromahighcomputational costoftrainingwhen\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark.\n5.7.3OtherSimpleSupervisedLearningAlgorithms\nWehavealreadybrie\ufb02yencounteredanothernon-probabilis ticsupervisedlearning\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\nafamilyoftechniquesthatcanbeusedforclassi\ufb01cationorregression.Asa\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoa\ufb01xed\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe\ntrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\nwe\ufb01ndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\nanykindofsupervisedlearningwherewecande\ufb01neanaverageoveryvalues.In\nthecaseofclassi\ufb01cation,wecanaverageoverone-hotcodevectorscwithc y= 1\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese\none-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\nsupposewehaveamulticlassclassi\ufb01cationtaskandmeasureperformancewith0-1\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\nnumberoftrainingexamplesapproachesin\ufb01nity.TheerrorinexcessoftheBayes\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\ndistantneighborsrandomly.Whenthereisin\ufb01nitetrainingdata,alltestpointsx\nwillhavein\ufb01nitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\nofthem,theprocedureconvergestotheBayeserrorrate.\u00a0Thehighcapacityof\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\ngivenasmall,\ufb01nitetrainingset.Oneweaknessofk-nearestneighborsisthatit\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,\nimaginewehavearegressiontaskwithx\u2208 R100drawnfromanisotropicGaussian\n1 4 3", "CHAPTER5.MACHINELEARNINGBASICS\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\nfeaturesx2throughx100,notbythelonefeaturex1.\u00a0Thustheoutputonsmall\ntrainingsetswillessentiallyberandom.\n1 4 4", "CHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\ndisplayedwithabinarystringidenti\ufb01ercorrespondingtoitspositioninthetree,obtained\nbyappendingabittoitsparentidenti\ufb01er(0=chooseleftortop,1=chooserightorbottom).\n( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletode\ufb01ne,soitis\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\nnumberoftrainingexamples.\n1 4 5", "CHAPTER5.MACHINELEARNINGBASICS\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\n1984)anditsmanyvariants.Asshownin\ufb01gure,eachnodeofthedecision 5.7\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\ncut).\u00a0Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one\ncorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,\nstruggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\nwithaxis-alignedsteps.\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational\nresourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\nlearningalgorithmsbythinkingaboutthesimilaritiesanddi\ufb00erencesbetween\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\nSee (),\u00a0(),\u00a0 ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\n5.8UnsupervisedLearningAlgorithms\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3\nonly\u201cfeatures\u201dbutnotasupervisionsignal.Thedistinctionbetweensupervised\nandunsupervisedalgorithmsisnotformallyandrigidlyde\ufb01nedbecausethereisno\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,\n\ufb01ndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n1 4 6", "CHAPTER5.MACHINELEARNINGBASICS\nrelatedexamples.\nAclassicunsupervisedlearningtaskisto\ufb01ndthe\u201cbest\u201drepresentationofthe\ndata.By\u2018best\u2019wecanmeandi\ufb00erentthings,butgenerallyspeakingwearelooking\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\nmoreaccessiblethanitself.x\nTherearemultiplewaysofde\ufb01ningarepresentation.Threeofthe simpler\u00a0\nmostcommonincludelowerdimensionalrepresentations,sparserepresentations\nandindependentrepresentations.Low-dimensionalrepresentationsattemptto\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan\noverallstructureoftherepresentationthattendstodistributedataalongtheaxes\noftherepresentationspace.Independentrepresentationsattempttodisentangle\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\noftherepresentationarestatisticallyindependent.\nOf\u00a0coursethese\u00a0three\u00a0criteriaare\u00a0certainly\u00a0notmutuallyexclusive.Low-\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto\nreducethesizeofarepresentationisto\ufb01ndandremoveredundancies.Identifying\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\nachievemorecompressionwhilediscardinglessinformation.\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe\nremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\ndevelopthesecriteriaindi\ufb00erentwaysorintroduceothercriteria.\n5.8.1PrincipalComponentsAnalysis\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n1 4 7", "CHAPTER5.MACHINELEARNINGBASICS\n\u2212 \u2212 2 0 1 0 0 1 0 2 0\nx 1\u2212 2 0\u2212 1 001 02 0x 2\n\u2212 \u2212 2 0 1 0 0 1 0 2 0\nz 1\u2212 2 0\u2212 1 001 02 0z 2\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.\u00a0 ( R i g h t )The\ntransformeddataz=x\ue03eWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\nmostvarianceisnowalongz 2.\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\nisa\ufb01rststeptowardthecriterionoflearningrepresentationswhoseelementsare\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan\ninputxtoarepresentationzasshownin\ufb01gure.Insection,wesawthat 5.8 2.12\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\ncorrespondstothe\ufb01rstprincipalcomponentofthedata.ThuswecanusePCA\nasasimpleande\ufb00ectivedimensionalityreductionmethodthatpreservesasmuch\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation\ndecorrelatestheoriginaldatarepresentation.X\nLetusconsiderthemn\u00d7-dimensionaldesignmatrixX.Wewillassumethat\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nm\u22121X\ue03eX. (5.85)\n1 4 8", "CHAPTER5.MACHINELEARNINGBASICS\nPCA\ufb01ndsarepresentation(throughlineartransformation)z=x\ue03eWwhere\nVar[]zisdiagonal.\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\naregivenbytheeigenvectorsofX\ue03eX.Fromthisview,\nX\ue03eXWW = \u039b\ue03e. (5.86)\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\nSpeci\ufb01cally,theyaretherightsingularvectorsofX.Toseethis,letWbethe\nrightsingularvectorsinthedecompositionX=UW \u03a3\ue03e.\u00a0Wethenrecoverthe\noriginaleigenvectorequationwithastheeigenvectorbasis: W\nX\ue03eX=\ue010\nUW \u03a3\ue03e\ue011\ue03e\nUW \u03a3\ue03e= W \u03a32W\ue03e.(5.87)\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\nSVDof,wecanexpressthevarianceofas: X X\nVar[] =x1\nm\u22121X\ue03eX (5.88)\n=1\nm\u22121(UW \u03a3\ue03e)\ue03eUW \u03a3\ue03e(5.89)\n=1\nm\u22121W \u03a3\ue03eU\ue03eUW \u03a3\ue03e(5.90)\n=1\nm\u22121W \u03a32W\ue03e, (5.91)\nwhereweusethefactthatU\ue03eU=IbecausetheUmatrixofthesingularvalue\ndecompositionisde\ufb01nedtobeorthogonal.Thisshowsthatifwetakez=x\ue03eW,\nwecanensurethatthecovarianceofisdiagonalasrequired: z\nVar[] =z1\nm\u22121Z\ue03eZ (5.92)\n=1\nm\u22121W\ue03eX\ue03eXW (5.93)\n=1\nm\u22121W\ue03eW \u03a32W\ue03eW (5.94)\n=1\nm\u22121\u03a32, (5.95)\nwherethistimeweusethefactthatW\ue03eW=I,againfromthede\ufb01nitionofthe\nSVD.\n1 4 9", "CHAPTER5.MACHINELEARNINGBASICS\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\n(asgivenby \u03a32)whichimmediatelyimpliesthattheindividualelementsofzare\nmutuallyuncorrelated.\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof\nvariationunderlyingthedata.\u00a0InthecaseofPCA,thisdisentanglingtakesthe\nformof\ufb01ndingarotationoftheinputspace(describedbyW)thatalignsthe\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\nwith.z\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\ncanbedonewithasimplelineartransformation.\n5.8.2-meansClustering k\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\nThek-meansclusteringalgorithmdividesthetrainingsetintokdi\ufb00erentclusters\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\nzero.\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse\nrepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\nwewilldevelopotheralgorithmsthatlearnmore\ufb02exiblesparserepresentations,\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\nareanextremeexampleofsparserepresentationsthatlosemanyofthebene\ufb01ts\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\nsimilartoeachother)anditconfersthecomputational advantagethattheentire\nrepresentationmaybecapturedbyasingleinteger.\nThek-meansalgorithmworksbyinitializingkdi\ufb00erentcentroids{\u00b5(1),...,\u00b5() k}\ntodi\ufb00erentvalues,thenalternatingbetweentwodi\ufb00erentstepsuntilconvergence.\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\nthenearestcentroid\u00b5() i.Intheotherstep,eachcentroid\u00b5() iisupdatedtothe\nmeanofalltrainingexamplesx() jassignedtocluster.i\n1 5 0", "CHAPTER5.MACHINELEARNINGBASICS\nOnedi\ufb03cultypertainingtoclusteringisthattheclusteringproblemisinherently\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe\nclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\nmaybemanydi\ufb00erentclusteringsthatallcorrespondwelltosomepropertyof\ntherealworld.Wemayhopeto\ufb01ndaclusteringthatrelatestoonefeaturebut\nobtainadi\ufb00erent,equallyvalidclusteringthatisnotrelevanttoourtask.For\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\ncars.Ifweaskeachclusteringalgorithmto\ufb01ndtwoclusters,onealgorithmmay\n\ufb01ndaclusterofcarsandaclusteroftrucks,whileanothermay\ufb01ndaclusterof\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\nlostinformationaboutsimilarity.Redcarsareinadi\ufb00erentclusterfromgray\ncars,justastheyareinadi\ufb00erentclusterfromgraytrucks.\u00a0Theoutputofthe\nclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\nthantheyaretograytrucks.Theyaredi\ufb00erentfromboththings,andthatisall\nweknow.\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\ntwoattributesforeachvehicle\u2014onerepresentingitscolorandonerepresenting\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe\ntwoattributesweareinterestedinarecolorandcar-versus-truckratherthan\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\nsimilaritybetweenobjectsina\ufb01ne-grainedwaybycomparingmanyattributes\ninsteadofjusttestingwhetheroneattributematches.\n5.9StochasticGradientDescent\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\n1 5 1", "CHAPTER5.MACHINELEARNINGBASICS\ngradientdescentalgorithmintroducedinsection.4.3\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\nexpensive.\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\nJ() = \u03b8 E x ,y \u223c\u02c6 pdataL,y,(x\u03b8) =1\nmm\ue058\ni=1L(x() i,y() i,\u03b8)(5.96)\nwhereistheper-exampleloss L L,y,py. (x\u03b8) = log\u2212 (|x\u03b8;)\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\n\u2207 \u03b8J() =\u03b81\nmm\ue058\ni=1\u2207 \u03b8L(x() i,y() i,.\u03b8) (5.97)\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\nlong.\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.\nSpeci\ufb01cally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\nB={x(1),...,x( m\ue030)}drawnuniformlyfromthetrainingset.Theminibatchsize\nm\ue030istypicallychosentobearelativelysmallnumberofexamples,rangingfrom\n1toafewhundred.Crucially,m\ue030isusuallyheld\ufb01xedasthetrainingsetsizem\ngrows.Wemay\ufb01tatrainingsetwithbillionsofexamplesusingupdatescomputed\nononlyahundredexamples.\nTheestimateofthegradientisformedas\ng=1\nm\ue030\u2207 \u03b8m\ue030\ue058\ni=1L(x() i,y() i,.\u03b8) (5.98)\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\nthenfollowstheestimatedgradientdownhill:\n\u03b8\u03b8g \u2190 \u2212\ue00f, (5.99)\nwhereisthelearningrate. \ue00f\n1 5 2", "CHAPTER5.MACHINELEARNINGBASICS\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\nlocalminimuminareasonableamountoftime,butitoften\ufb01ndsaverylowvalue\nofthecostfunctionquicklyenoughtobeuseful.\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\ndatasets.Fora\ufb01xedmodelsize,thecostperSGDupdatedoesnotdependonthe\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\nconvergenceusuallyincreaseswithtrainingsetsize.\u00a0However,asmapproaches\nin\ufb01nity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\nextendtheamountoftrainingtimeneededtoreachthemodel\u2019sbestpossibletest\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\namodelwithSGDisasafunctionof. O(1) m\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\nalgorithmsrequireconstructinganmm\u00d7matrixG i , j=k(x() i,x() j).Constructing\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\nwith\u00a0billions of\u00a0examples. In\u00a0academia, starting\u00a0in2006,deep\u00a0learning was\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\nindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\ndatasets.\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\ninchapter.8\n5.10BuildingaMachineLearningAlgorithm\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\nafairlysimplerecipe:combineaspeci\ufb01cationofadataset,acostfunction,an\noptimization procedureandamodel.\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n1 5 3", "CHAPTER5.MACHINELEARNINGBASICS\nXyand,thecostfunction\nJ,b(w) = \u2212 E x ,y \u223c\u02c6 pdatalogpmodel( )y|x, (5.100)\nthemodelspeci\ufb01cationpmodel(y|x) =N(y;x\ue03ew+b,1),and,inmostcases,the\noptimization algorithmde\ufb01nedbysolvingforwherethegradientofthecostiszero\nusingthenormalequations.\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning\nprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthe\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\nlikelihoodestimation.\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\ntoobtain\nJ,b\u03bb (w) = ||||w2\n2\u2212 E x ,y \u223c\u02c6 pdatalogpmodel( )y|x.(5.101)\nThisstillallowsclosed-formoptimization.\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger\nbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\noptimization procedure,suchasgradientdescent.\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\nlearningcanbesupportedbyde\ufb01ningadatasetthatcontainsonlyXandproviding\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtainthe\ufb01rst\nPCAvectorbyspecifyingthatourlossfunctionis\nJ() = w E x \u223c\u02c6 pdata||\u2212 ||xr(;)xw2\n2 (5.102)\nwhileourmodelisde\ufb01nedtohavewwithnormoneandreconstructionfunction\nr() = xw\ue03exw.\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\napproximatingitsgradients.\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot\nimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n1 5 4", "CHAPTER5.MACHINELEARNINGBASICS\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\ntheircostfunctionshave\ufb02atregionsthatmaketheminappropriate forminimization\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\ncanbedescribedusingthisrecipehelpstoseethedi\ufb00erentalgorithmsaspartofa\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather\nthanasalonglistofalgorithmsthateachhaveseparatejusti\ufb01cations.\n5.11ChallengesMotivatingDeepLearning\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes\nexponentiallymoredi\ufb03cultwhenworkingwithhigh-dimensionaldata,andhow\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\nareinsu\ufb03cienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\novercometheseandotherobstacles.\n5.11.1TheCurseofDimensionality\nManymachinelearningproblemsbecomeexceedinglydi\ufb03cultwhenthenumber\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof\ndimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\ncon\ufb01gurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\nincreases.\n1 5 5", "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofcon\ufb01gurationsofinterestmaygrowexponentially. ( L e f t )Inthis\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin\neachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\ndimensionsitismoredi\ufb03culttodistinguish10di\ufb00erentvaluesofeachvariable.\u00a0Weneed\ntokeeptrackofupto10\u00d710=100regions,andweneedatleastthatmanyexamplesto\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\naxis,weseemtoneedO(vd)regionsandexamples.\u00a0Thisisaninstanceofthecurseof\ndimensionality.FiguregraciouslyprovidedbyNicolasChapados.\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\nespeciallysoinmachinelearning.\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\nAsillustratedin\ufb01gure,astatisticalchallengearisesbecausethenumberof 5.9\npossiblecon\ufb01gurations ofxismuchlargerthanthenumberoftrainingexamples.\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\ngrid,likeinthe\ufb01gure.Wecandescribelow-dimensional spacewithalownumber\nofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell.\u00a0Ifwearedoingregressionwecanaveragethetarget\nvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\ncon\ufb01gurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n1 5 6", "CHAPTER5.MACHINELEARNINGBASICS\nmeaningfulaboutthesenewcon\ufb01gurations? Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint.\n5.11.2LocalConstancyandSmoothnessRegularization\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions\noverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\ndirectlyin\ufb02uencingtheitselfandonlyindirectlyactingontheparameters function\nviatheire\ufb00ectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\ndegreeofbeliefinvariousfunctions.\nAmongthemostwidelyusedoftheseimplicit\u201cpriors\u201d\u00a0isthesmoothness\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\nshouldnotchangeverymuchwithinasmallregion.\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\nadditional(explicit\u00a0andimplicit)priorsinorder\u00a0toreducethegeneralization\nerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\ninsu\ufb03cientforthesetasks.\nTherearemanydi\ufb00erentwaystoimplicitlyorexplicitlyexpressapriorbelief\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesedi\ufb00erent\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf\u2217that\nsatis\ufb01esthecondition\nf\u2217() x\u2248f\u2217(+)x\ue00f (5.103)\nformostcon\ufb01gurationsxandsmallchange\ue00f.Inotherwords,ifweknowagood\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat\nanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\npossible.\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n1 5 7", "CHAPTER5.MACHINELEARNINGBASICS\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\nthanthenumberoftrainingexamples.\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther\napartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\nresembleseachtrainingexamplex() i.\u00a0Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,). Bengioetal.2006b\nDecisiontreesalsosu\ufb00erfromthelimitationsofexclusivelysmoothness-based\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare\nleavesanduseaseparateparameter(orsometimesmanyparametersforextensions\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\nrequiredto\ufb01tthetree.Amultipleofnisneededtoachievesomelevelofstatistical\ncon\ufb01denceinthepredictedoutput.\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters\nassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\nwhereeachtrainingexamplecanbeusedtode\ufb01neatmostoneregion,isillustrated\nin\ufb01gure.5.10\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\nFor\u00a0example,\u00a0imagine that\u00a0thetargetfunctionis\u00a0akind\u00a0ofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.\nImaginewhathappenswhenthenumberoftrainingexamplesissubstantially\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan\nexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\n1 5 8", "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.10:\u00a0Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintoregions.\u00a0Anexample(representedherebyacircle)withineachregionde\ufb01nesthe\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\nde\ufb01neswhattheoutputshouldbeforallpointswithinthecorrespondingregion.\u00a0The\nregionsde\ufb01nedbynearestneighbormatchingformageometricpatterncalledaVoronoi\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber\noftrainingexamples.Whilethis\ufb01gureillustratesthebehaviorofthenearestneighbor\nalgorithmspeci\ufb01cally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\nsurroundingthatexample.\n1 5 9", "CHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina\ndi\ufb00erentwayalongeachdimension.Ifthefunctionadditionallybehavesdi\ufb00erently\nindi\ufb00erentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\ngeneralizewell?\nTheanswertobothofthesequestions\u2014whetheritispossibletorepresent\nacomplicatedfunctione\ufb03ciently,andwhetheritispossiblefortheestimated\nfunctiontogeneralizewelltonewinputs\u2014isyes.Thekeyinsightisthatavery\nlargenumberofregions,e.g.,O(2k),canbede\ufb01nedwithO(k)examples,solong\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\ndi\ufb00erentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\nOtherapproachestomachinelearningoftenmakestronger,task-speci\ufb01cas-\nsumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\nstrong,task-speci\ufb01cassumptionsintoneuralnetworkssothattheycangeneralize\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\ncomplextobelimitedtosimple,manuallyspeci\ufb01edpropertiessuchasperiodicity,\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby\nthecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms.\u00a0Theseapparentlymildassumptionsallowanexponentialgaininthe\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse\nofdimensionality.\n1 6 0", "CHAPTER5.MACHINELEARNINGBASICS\n5.11.3ManifoldLearning\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\nmanifold.\nAmanifoldisaconnected\u00a0region. Mathematically ,\u00a0it\u00a0isasetofpoints,\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n3-Dspace.\nThede\ufb01nitionofaneighborhoodsurroundingeachpointimpliestheexistence\noftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\ntoaneighboringone.Intheexampleoftheworld\u2019ssurfaceasamanifold,onecan\nwalknorth,south,east,orwest.\nAlthoughthereisaformalmathematical meaningtotheterm\u201cmanifold,\u201din\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\ndimensioncorrespondstoalocaldirectionofvariation.See\ufb01gureforan5.11\nexampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\nofthemanifoldtovaryfromonepointtoanother.\u00a0This oftenhappenswhena\nmanifoldintersectsitself.Forexample,a\ufb01gureeightisamanifoldthathasasingle\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .\u2212 1 0 .\u2212 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .\nFigure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\ntheunderlyingmanifoldthatthelearnershouldinfer.\n1 6 1", "CHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\nof Rnconsistsofinvalidinputs,\u00a0andthatinterestinginputsoccuronlyalong\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections\nthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\nhighlyconcentrated.\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas\nthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\noftwocategoriesofobservations.\nThe\ufb01rstobservationinfavorofthemanifoldhypothesisisthattheproba-\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\nfromthesedomains.\u00a0Figureshowshow,instead,uniformlysampledpoints 5.12\nlooklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\n1 6 2", "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\nvolumeofimagespace.\nOfcourse,concentratedprobabilitydistributionsarenotsu\ufb03cienttoshow\nthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\n1 6 3", "CHAPTER5.MACHINELEARNINGBASICS\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually\nmoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\nconnectedtothemanifoldofimagesofcatfaces.\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\ntuitivereasonssupportingit.Morerigorousexperiments\u00a0(Cayton2005Narayanan,;\nandMitter2010Sch\u00f6lkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,\n2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\ninterestinAI.\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\nspeci\ufb01caddressesintermsofaddressnumbersalongthese1-Droads,notinterms\nofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\nmethodsnecessarytolearnsuchamanifoldstructure.In\ufb01gure,wewillsee 20.6\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\nandmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\n1 6 4", "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\nfeat.\n1 6 5", "P a rt I I\nD e e p N e t w orks: Mo d e rn\nPractices\n166", "Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\nusedtosolvepracticalapplications.\nDeeplearninghasalonghistoryandmanyaspirations.Severalapproaches\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\nthe\ufb01nalpartofthebook.\nThispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\nnologiesthatarealreadyusedheavilyinindustry.\nModern\u00a0deeplearning\u00a0provides\u00a0avery\u00a0powerful\u00a0framework\u00a0forsupervised\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\nbeaccomplishedviadeeplearning,givensu\ufb03cientlylargemodelsandsu\ufb03ciently\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed\nasassociatingonevectortoanother,orthataredi\ufb03cultenoughthataperson\nwouldrequiretimetothinkandre\ufb02ectinordertoaccomplishthetask,remain\nbeyondthescopeofdeeplearningfornow.\nThispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\nWe\u00a0begin\u00a0by\u00a0describingthe\u00a0feedforward\u00a0deepnetworkmodelthatisusedto\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization\nandoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\nforthepracticalmethodologyinvolvedindesigning,building,andcon\ufb01guringan\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\nlearning.\nThesechaptersarethemostimportantforapractitioner\u2014someone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday.\n1 6 7", "C h a p t e r 6\nD e e p F e e d f orw ard N e t w orks\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\nThegoalofafeedforwardnetworkistoapproximatesomefunction f\u2217.Forexample,\nforaclassi\ufb01er, y= f\u2217(x)mapsaninputxtoacategory y.Afeedforwardnetwork\nde\ufb01nesamappingy= f(x;\u03b8)andlearnsthevalueoftheparameters\u03b8thatresult\ninthebestfunctionapproximation.\nThesemodelsarecalledfeedforwardbecauseinformation\ufb02owsthroughthe\nfunctionbeingevaluatedfromx,throughtheintermediate computations usedto\nde\ufb01ne f,and\ufb01nallytotheoutputy.Therearenofeedbackconnectionsinwhich\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\nnetworks,presentedinchapter.10\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For\nexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\nlanguageapplications.\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\nrepresentedbycomposingtogethermanydi\ufb00erentfunctions.Themodelisasso-\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed\ntogether.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledthe\ufb01rst\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\n168", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\nthename\u201cdeeplearning\u201darises.The\ufb01nallayerofafeedforwardnetworkiscalled\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f\u2217(x).\nThetrainingdataprovidesuswithnoisy,approximateexamplesof f\u2217(x) evaluated\natdi\ufb00erenttrainingpoints.Eachexamplexisaccompanied byalabel y f\u2248\u2217(x).\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint\nx;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\nnotdirectlyspeci\ufb01edbythetrainingdata.\u00a0Thelearningalgorithmmustdecide\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\ndecidehowtousetheselayerstobestimplementanapproximation of f\u2217.Because\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\nlayersarecalledhiddenlayers.\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,\neachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\nactivationvalue.\u00a0Theideaofusingmanylayersofvector-valuedrepresentation\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\ntheserepresentationsisalsolooselyguidedbyneuroscienti\ufb01cobservationsabout\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe\ngoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations.\u00a0Linearmodels,suchaslogistic\nregressionandlinearregression,areappealingbecausetheymaybe\ufb01te\ufb03ciently\nandreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\nthelinearmodelnottoxitselfbuttoatransformedinput \u03c6(x),where \u03c6isa\n1 6 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\nthe \u03c6mapping.Wecanthinkof \u03c6asprovidingasetoffeaturesdescribingx,or\nasprovidinganewrepresentationfor.x\nThequestionisthenhowtochoosethemapping. \u03c6\n1.Oneoptionistouseaverygeneric \u03c6,suchasthein\ufb01nite-dimens ional \u03c6that\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.\u00a0If \u03c6(x)is\nofhighenoughdimension,wecanalwayshaveenoughcapacityto\ufb01tthe\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\nproblems.\n2.Anotheroptionistomanuallyengineer \u03c6.Untiltheadventofdeeplearning,\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\ne\ufb00ortfor\u00a0eachseparate\u00a0task,\u00a0withpractitioners\u00a0specializing\u00a0in di\ufb00erent\ndomainssuchasspeech\u00a0recognition or\u00a0computer vision,\u00a0and\u00a0with little\ntransferbetweendomains.\n3.Thestrategyofdeeplearningistolearn \u03c6.Inthisapproach,wehaveamodel\ny= f(x;\u03b8w ,) = \u03c6(x;\u03b8)\ue03ew.Wenowhaveparameters\u03b8thatweusetolearn\n\u03c6fromabroadclassoffunctions,andparameterswthatmapfrom \u03c6(x)to\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\n\u03c6de\ufb01ningahiddenlayer.\u00a0Thisapproachistheonlyoneofthethreethat\ngivesupontheconvexityofthetrainingproblem,butthebene\ufb01tsoutweigh\ntheharms.Inthisapproach,weparametrizetherepresentationas \u03c6(x;\u03b8)\nandusetheoptimization algorithmto\ufb01ndthe\u03b8thatcorrespondstoagood\nrepresentation.Ifwewish,thisapproachcancapturethebene\ufb01tofthe\ufb01rst\napproachbybeinghighlygeneric\u2014wedosobyusingaverybroadfamily\n\u03c6(x;\u03b8).Thisapproachcanalsocapturethebene\ufb01tofthesecondapproach.\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\ndesigningfamilies \u03c6(x;\u03b8)thattheyexpectwillperformwell.Theadvantage\nisthatthehumandesigneronlyneedsto\ufb01ndtherightgeneralfunction\nfamilyratherthan\ufb01ndingpreciselytherightfunction.\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\n1 7 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmappingsfromxtoythatlackfeedbackconnections.\u00a0Othermodelspresented\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign\ndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese\nlayersshould\u00a0beconnectedto\u00a0each\u00a0other,\u00a0and howmanyunitsshould\u00a0bein\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\nmoderngeneralizations ,whichcanbeusedtoe\ufb03cientlycomputethesegradients.\nFinally,weclosewithsomehistoricalperspective.\n6. 1 E x am p l e: L earni n g X O R\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning\ntheXORfunction.\nTheXORfunction(\u201cexclusiveor\u201d)isanoperationontwobinaryvalues, x 1\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\ny= f\u2217(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;\u03b8)and\nourlearningalgorithmwilladapttheparameters\u03b8tomake fassimilaraspossible\nto f\u2217.\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.\nWewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0]\ue03e,[0 ,1]\ue03e,\n[1 ,0]\ue03e,and[1 ,1]\ue03e}.\u00a0Wewilltrainthenetworkonallfourofthesepoints.\u00a0The\nonlychallengeisto\ufb01tthetrainingset.\nWecantreatthisproblemasaregressionproblemanduseameansquared\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n1 7 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\naredescribedinsection.6.2.2.2\nEvaluatedonourwholetrainingset,theMSElossfunctionis\nJ() =\u03b81\n4\ue058\nx\u2208 X( f\u2217() (;))x\u2212 fx\u03b82. (6.1)\nNowwemustchoosetheformofourmodel, f(x;\u03b8).Supposethatwechoose\nalinearmodel,withconsistingofand.Ourmodelisde\ufb01nedtobe \u03b8w b\nf , b (;xw) = x\ue03ew+ b . (6.2)\nWecanminimize J(\u03b8)inclosedformwithrespecttowand busingthenormal\nequations.\nAftersolvingthenormalequations,weobtainw= 0and b=1\n2.\u00a0Thelinear\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\nthisproblemistouseamodelthatlearnsadi\ufb00erentfeaturespaceinwhicha\nlinearmodelisabletorepresentthesolution.\nSpeci\ufb01cally,wewillintroduceaverysimplefeedforwardnetworkwithone\nhiddenlayercontainingtwohiddenunits.See\ufb01gureforanillustrationof 6.2\nthismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =W\ue03ex\nand f( 2 )(h) =h\ue03ew.Then f(x) =w\ue03eW\ue03ex.Wecouldrepresentthisfunctionas\nf() = xx\ue03ew\ue030wherew\ue030= Ww.\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural\nnetworksdosousingana\ufb03netransformationcontrolledbylearnedparameters,\nfollowedbya\ufb01xed,nonlinearfunctioncalledanactivationfunction.Weusethat\nstrategyhere,byde\ufb01ningh= g(W\ue03ex+c) ,whereWprovidestheweightsofa\nlineartransformationandcthebiases.Previously,todescribealinearregression\n1 7 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 1\nx 101x 2O r i g i n a l s p a c e x\n0 1 2\nh 101h 2L e a r n e d s p a c e h\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\nfunction.When x1= 0,themodel\u2019soutputmustincreaseas x2increases.When x1= 1,\nthemodel\u2019soutputmustdecreaseas x 2increases.Alinearmodelmustapplya\ufb01xed\ncoe\ufb03cient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\nthecoe\ufb03cienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\nmappedbothx= [1 ,0]\ue03eandx= [0 ,1]\ue03etoasinglepointinfeaturespace,h= [1 ,0]\ue03e.\nThelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcan\ufb01tthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize.\n1 7 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nyy\nhh\nx xWwyy\nh 1 h 1\nx 1 x 1h 2 h 2\nx 2 x 2\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodi\ufb00erentstyles.Speci\ufb01cally,\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )\neachentirevectorrepresentingalayer\u2019sactivations.\u00a0Thisstyleismuchmorecompact.\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\nofdrawing.\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean\na\ufb03netransformationfromaninputvectortoanoutputscalar.Now,wedescribe\nana\ufb03netransformationfromavectorxtoavectorh,soanentirevectorofbias\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\nthatisappliedelement-wise,with h i= g(x\ue03eW : , i+ c i).Inmodernneuralnetworks,\nthedefaultrecommendation istousetherecti\ufb01edlinearunitorReLU(Jarrett\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)de\ufb01nedbytheactivation 2011a\nfunction depictedin\ufb01gure. g z , z () = max0{} 6.3\nWecannowspecifyourcompletenetworkas\nf , , , b (;xWcw) = w\ue03emax0{ ,W\ue03exc+}+ b . (6.3)\nWecannowspecifyasolutiontotheXORproblem.Let\nW=\ue01411\n11\ue015\n, (6.4)\nc=\ue014\n0\n\u22121\ue015\n, (6.5)\n1 7 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0\nz0g z ( ) = m a x 0{ , z}\nFigure6.3:Therecti\ufb01edlinearactivationfunction.Thisactivationfunctionisthedefault\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\nfunctionwithtwolinearpieces.Becauserecti\ufb01edlinearunitsarenearlylinear,they\npreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\ncomplicatedsystemsfromminimalcomponents.\u00a0MuchasaTuringmachine\u2019smemory\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\nfromrecti\ufb01edlinearfunctions.\n1 7 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nw=\ue0141\n\u22122\ue015\n, (6.6)\nand. b= 0\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\nwithoneexampleperrow:\nX=\uf8ee\n\uf8ef\uf8ef\uf8f000\n01\n10\n11\uf8f9\n\uf8fa\uf8fa\uf8fb. (6.7)\nThe\ufb01rststepintheneuralnetworkistomultiplytheinputmatrixbythe\ufb01rst\nlayer\u2019sweightmatrix:\nXW=\uf8ee\n\uf8ef\uf8ef\uf8f000\n11\n11\n22\uf8f9\n\uf8fa\uf8fa\uf8fb. (6.8)\nNext,weaddthebiasvector,toobtainc\n\uf8ee\n\uf8ef\uf8ef\uf8f00 1\u2212\n10\n10\n21\uf8f9\n\uf8fa\uf8fa\uf8fb. (6.9)\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1\nthisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\nAlinearmodelcannotimplementsuchafunction.To\ufb01nishcomputingthevalue\nofforeachexample,weapplytherecti\ufb01edlineartransformation: h\n\uf8ee\n\uf8ef\uf8ef\uf8f000\n10\n10\n21\uf8f9\n\uf8fa\uf8fa\uf8fb. (6.10)\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\nlongerlieonasingleline.Asshownin\ufb01gure,theynowlieinaspacewherea 6.1\nlinearmodelcansolvetheproblem.\nWe\ufb01nishbymultiplyingbytheweightvector:w\n\uf8ee\n\uf8ef\uf8ef\uf8f00\n1\n1\n0\uf8f9\n\uf8fa\uf8fa\uf8fb. (6.11)\n1 7 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\nInthisexample,wesimplyspeci\ufb01edthesolution,thenshowedthatitobtained\nzeroerror.\u00a0Inarealsituation,theremightbebillionsofmodelparametersand\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\nhere.Instead,agradient-basedoptimization algorithmcan\ufb01ndparametersthat\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata\nglobalminimumofthelossfunction,sogradientdescentcouldconvergetothis\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\ndescentcouldalso\ufb01nd.Theconvergencepointofgradientdescentdependsonthe\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\n\ufb01ndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\nhere.\n6. 2 Gradi en t - Bas e d L earni n g\nDesigningandtraininganeuralnetworkisnotmuchdi\ufb00erentfromtrainingany\nothermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\nacostfunction,andamodelfamily.\nThelargestdi\ufb00erencebetweenthelinearmodelswehaveseensofarandneural\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost\nfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\nconvergesstartingfromanyinitialparameters(intheory\u2014inpracticeitisvery\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis\nimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\nparticulardiscussedinsection.Forthemoment,itsu\ufb03cestounderstandthat 8.4\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\ncostfunctioninonewayoranother.\u00a0The speci\ufb01calgorithmsareimprovements\nandre\ufb01nementsontheideasofgradientdescent,introducedinsection,and,4.3\n1 7 7", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmorespeci\ufb01cally,aremostoftenimprovementsofthestochasticgradientdescent\nalgorithm,introducedinsection.5.9\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\nmuchdi\ufb00erentfromtraininganyothermodel.Computingthegradientisslightly\nmorecomplicatedforaneuralnetwork,butcanstillbedonee\ufb03cientlyandexactly.\nSectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\ntheneuralnetworksscenario.\n6.2.1CostFunctions\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe\ncostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\nInmostcases,ourparametricmodelde\ufb01nesadistribution p(yx|;\u03b8)and\nwesimplyuse\u00a0theprinciple\u00a0ofmaximumlikelihood.Thismeansweusethe\ncross-entropybetweenthetrainingdataandthemodel\u2019spredictionsasthecost\nfunction.\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned\non.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe\ndescribedinchapter.7\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n1 7 8", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\ncostfunctionisgivenby\nJ() = \u03b8 \u2212 E x y ,\u223c \u02c6 pdatalog p m o de l( )yx| . (6.12)\nThespeci\ufb01cformofthecostfunctionchangesfrommodeltomodel,depending\nonthespeci\ufb01cformoflog p m o de l.Theexpansionoftheaboveequationtypically\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;\u03b8) ,I),\nthenwerecoverthemeansquarederrorcost,\nJ \u03b8() =1\n2E x y ,\u223c \u02c6 pdata||\u2212 ||y f(;)x\u03b82+const , (6.13)\nuptoascalingfactorof1\n2andatermthatdoesnotdependon.Thediscarded\u03b8\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)x\u03b8\nAnadvantageofthisapproachofderivingthecostfunctionfrommaximum\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\nforthelearningalgorithm.Functionsthatsaturate(becomevery\ufb02at)undermine\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases\nthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\nhiddenunitsortheoutputunitssaturate.\u00a0Thenegativelog-likelihoodhelpsto\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\nsection.6.2.2\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum\nlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n1 7 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\ncross-entropyapproachingnegativein\ufb01nity.Regularizationtechniquesdescribed\ninchapterprovideseveraldi\ufb00erentwaysofmodifyingthelearningproblemso 7\nthatthemodelcannotreapunlimitedrewardinthisway.\n6.2.1.2LearningConditionalStatistics\nInsteadoflearningafullprobabilitydistribution p(yx|;\u03b8)weoftenwanttolearn\njustoneconditionalstatisticofgiven.yx\nForexample,wemayhaveapredictor f(x;\u03b8) thatwewishtopredictthemean\nof.y\nIfweuseasu\ufb03cientlypowerfulneuralnetwork,wecanthinkoftheneural\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\nratherthanbyhavingaspeci\ufb01cparametricform.Fromthispointofview,we\ncanviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespeci\ufb01c\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical\ntoolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\nusedtoderivethefollowingtworesults.\nOur\ufb01rstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\ntionproblem\nf\u2217= argmin\nfE x y ,\u223c pdata||\u2212 ||y f()x2(6.14)\nyields\nf\u2217() = x E y\u223c pdata ( ) y x|[]y , (6.15)\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe\ncouldtrainonin\ufb01nitelymanysamplesfromthetruedatageneratingdistribution,\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\nmeanofforeachvalueof. y x\n1 8 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nDi\ufb00erentcostfunctionsgivedi\ufb00erentstatistics.Asecondresultderivedusing\ncalculusofvariationsisthat\nf\u2217= argmin\nfE x y ,\u223c pdata||\u2212 ||y f()x 1 (6.16)\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\nfunctioniscommonlycalled . meanabsoluteerror\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat\nsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\nentiredistribution. p( )yx|\n6.2.2OutputUnits\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\nmodeldistribution.\u00a0Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction.\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\nsetofhiddenfeaturesde\ufb01nedbyh= f(x;\u03b8).Theroleoftheoutputlayeristhen\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask\nthatthenetworkmustperform.\n6.2.2.1LinearUnitsforGaussianOutputDistributions\nOnesimplekindofoutputunitisanoutputunitbasedonana\ufb03netransformation\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\nGivenfeaturesh,alayeroflinearoutputunitsproducesavector\u02c6y=W\ue03eh+b.\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| Ny\u02c6yI ,) . (6.17)\n1 8 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\nerror.\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\nde\ufb01nitematrixforallinputs.Itisdi\ufb03culttosatisfysuchconstraintswithalinear\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.\nApproachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\nBecauselinearunitsdonotsaturate,theyposelittledi\ufb03cultyforgradient-\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\nalgorithms.\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\nManytasksrequirepredictingthevalueofabinaryvariable y.Classi\ufb01cation\nproblemswithtwoclassescanbecastinthisform.\nThemaximum-likelihoodapproachistode\ufb01neaBernoullidistributionover y\nconditionedon.x\nABernoullidistributionisde\ufb01nedbyjustasinglenumber.Theneuralnet\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\nmustlieintheinterval[0,1].\nSatisfyingthisconstraintrequiressomecarefuldesigne\ufb00ort.Supposewewere\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\nP y(= 1 ) = max |x\ue06e\n0min ,\ue06e\n1 ,w\ue03eh+ b\ue06f\ue06f\n.(6.18)\nThiswouldindeedde\ufb01neavalidconditionaldistribution,butwewouldnotbeable\ntotrainitverye\ufb00ectivelywithgradientdescent.Anytimethatw\ue03eh+ bstrayed\noutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\nparameters.\nInstead,itisbettertouseadi\ufb00erentapproachthatensuresthereisalwaysa\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\nAsigmoidoutputunitisde\ufb01nedby\n\u02c6 y \u03c3= \ue010\nw\ue03eh+ b\ue011\n(6.19)\n1 8 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwhereisthelogisticsigmoidfunctiondescribedinsection. \u03c3 3.10\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\nusesalinearlayertocompute z=w\ue03eh+ b.Next,itusesthesigmoidactivation\nfunctiontoconvertintoaprobability. z\nWeomitthedependenceonxforthemomenttodiscusshowtode\ufb01nea\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\nbyconstructinganunnormalized probabilitydistribution\u02dc P( y),whichdoesnot\nsumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\ncontrolledbyasigmoidaltransformationof: z\nlog\u02dc P y y z () = (6.20)\n\u02dc P y y z () = exp() (6.21)\nP y() =exp() y z\ue0501\ny\ue030= 0exp( y\ue030z)(6.22)\nP y \u03c3 y z . () = ((2\u22121)) (6.23)\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\nthroughoutthestatisticalmodelingliterature.The zvariablede\ufb01ningsucha\ndistributionoverbinaryvariablesiscalleda.logit\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\nlikelihoodis\u2212log P( y|x),theloginthecostfunctionundoestheexpofthe\nsigmoid.Withoutthise\ufb00ect,thesaturationofthesigmoidcouldpreventgradient-\nbased\u00a0learningfrom\u00a0makinggoodprogress.Theloss\u00a0functionfor\u00a0maximum\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\nJ P y () = log\u03b8 \u2212 (|x) (6.24)\n= log((2 1)) \u2212 \u03c3 y\u2212 z (6.25)\n= ((12)) \u03b6 \u2212 y z . (6.26)\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n(1\u22122 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\nhastherightanswer\u2014when y= 1and zisverypositive,or y= 0and zisvery\nnegative.When zhasthewrongsign,theargumenttothesoftplusfunction,\n1 8 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n(1\u22122 y) z,maybesimpli\ufb01edto|| z.As|| zbecomeslargewhile zhasthewrongsign,\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\ncorrectamistaken. z\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan\nsaturateanytime \u03c3( z)saturates.Thesigmoidactivationfunctionsaturatesto0\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\noutputunits.\nAnalytically,thelogarithmofthesigmoidisalwaysde\ufb01nedand\ufb01nite,because\nthesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\nfunctionof z,ratherthanasafunctionof\u02c6 y= \u03c3( z).Ifthesigmoidfunction\nunder\ufb02owstozero,thentakingthelogarithmof\u02c6 yyieldsnegativein\ufb01nity.\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable\nwith npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\ndistributionoverabinaryvariable.\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassi\ufb01er,torepresent\ntheprobabilitydistributionover ndi\ufb00erentclasses.Morerarely,softmaxfunctions\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\nndi\ufb00erentoptionsforsomeinternalvariable.\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber\n\u02c6 y P y . = (= 1 )|x (6.27)\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\nthelog-likelihood,wechosetoinsteadpredictanumber z=log\u02dc P( y=1|x).\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction.\n1 8 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\ntoproduceavector\u02c6y,with \u02c6 y i= P( y= i|x).Werequirenotonlythateach\nelementof\u02c6 y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\nlayerpredictsunnormalized logprobabilities:\nzW= \ue03ehb+ , (6.28)\nwhere z i=log\u02dc P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\nnormalizetoobtainthedesired z \u02c6y.Formally,thesoftmaxfunctionisgivenby\nsoftmax()z i=exp( z i)\ue050\njexp( z j). (6.29)\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.De\ufb01ningthe\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\ntheofthesoftmax: exp\nlogsoftmax()z i= z i\u2212log\ue058\njexp( z j) . (6.30)\nThe\ufb01rsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,the\ufb01rst 6.30\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\npusheddown.Togainsomeintuitionforthesecondterm,log\ue050\njexp( z j),observe\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\nbasedontheideathatexp( z k) isinsigni\ufb01cantforany z kthatisnoticeablylessthan\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\nthe\u2212 z itermandthelog\ue050\njexp( z j)\u2248max j z j= z itermswillroughlycancel.\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassi\ufb01ed.\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n1 8 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\nsoftmax((;))zx\u03b8 i\u2248\ue050m\nj = 1 1y() j= i , x() j= x\ue050m\nj = 1 1x() j = x. (6.31)\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\nmodelisonlyabletoapproximatethesefractions.\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell\nwiththesoftmaxfunction.Speci\ufb01cally,objectivefunctionsthatdonotusealogto\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\noutput,evenwhenthemodelmakeshighlycon\ufb01dentincorrectpredictions(,Bridle\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\nthesoftmaxfunctionitself.\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\noutputvaluescansaturatewhenthedi\ufb00erencesbetweeninputvaluesbecome\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\nToseethatthesoftmaxfunctionrespondstothedi\ufb00erencebetweenitsinputs,\nobservethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\ninputs:\nsoftmax() = softmax(+) zz c . (6.32)\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\nsoftmax() = softmax( max zz\u2212\niz i) . (6.33)\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\nbytheamountthatitsargumentsdeviatefrommax i z i.\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\n1 8 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancausesimilardi\ufb03cultiesforlearningifthelossfunctionisnotdesignedto\ncompensateforit.\nTheargumentztothesoftmaxfunctioncanbeproducedintwodi\ufb00erentways.\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\neveryelementofz,asdescribedaboveusingthelinearlayerz=W\ue03eh+b.While\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n\u22121parametersare\nnecessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\n\ufb01rst n\u22121 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\nofzbe\ufb01xed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\nwhatthesigmoidunitdoes.De\ufb01ning P( y= 1|x) = \u03c3( z)isequivalenttode\ufb01ning\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n\u22121\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\nsetofprobabilitydistributions,buthavedi\ufb00erentlearningdynamics.Inpractice,\nthereisrarelymuchdi\ufb00erencebetweenusingtheoverparametrized versionorthe\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\nFromaneuroscienti\ufb01cpointofview,itisinterestingtothinkofthesoftmaxas\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe\nextreme(whenthedi\ufb00erencebetweenthemaximal a iandtheothersislargein\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\nandtheothersarenearly0).\nThename\u201csoftmax\u201dcanbesomewhatconfusing.Thefunctionismoreclosely\nrelatedtotheargmaxfunctionthanthemaxfunction.\u00a0Theterm\u201csoft\u201dderives\nfromthefactthatthesoftmaxfunctioniscontinuousanddi\ufb00erentiable. The\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\nordi\ufb00erentiable. Thesoftmaxfunctionthusprovidesa\u201csoftened\u201dversionofthe\nargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)\ue03ez.\nItwouldperhapsbebettertocallthesoftmaxfunction\u201csoftargmax,\u201d\u00a0butthe\ncurrentnameisanentrenchedconvention.\n6.2.2.4OtherOutputTypes\nThelinear,\u00a0sigmoid,\u00a0andsoftmaxoutputunitsdescribedabovearethemost\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n1 8 7", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nagoodcostfunctionfornearlyanykindofoutputlayer.\nIngeneral,ifwede\ufb01neaconditionaldistribution p(yx|;\u03b8),theprincipleof\nmaximumlikelihoodsuggestsweuse asourcostfunction. \u2212 | log( pyx\u03b8;)\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;\u03b8).\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;\u03b8) =\u03c9providestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas . \u2212log(;()) p y\u03c9x\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\ngiven x.Inthesimplecase,wherethevariance \u03c32isaconstant,thereisaclosed\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\nempiricalmeanofthesquareddi\ufb00erencebetweenobservations yandtheirexpected\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe\ndistribution p( y|x)thatiscontrolledby\u03c9= f(x;\u03b8).Thenegativelog-likelihood\n\u2212log p(y;\u03c9(x))willthenprovideacostfunctionwiththeappropriateterms\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\ncanmakeanewparameterinthenetworkthatiscopieddirectlyinto\u03c9.Thisnew\nparametermightbe \u03c3itselforcouldbeaparameter vrepresenting \u03c32oritcould\nbeaparameter \u03b2representing1\n\u03c32,dependingonhowwechoosetoparametrize\nthedistribution.Wemaywishourmodeltopredictadi\ufb00erentamountofvariance\nin yfordi\ufb00erentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\nheteroscedasticcase,wesimplymakethespeci\ufb01cationofthevariancebeoneof\nthevaluesoutputby f( x;\u03b8).AtypicalwaytodothisistoformulatetheGaussian\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\ndiag (6.34) ()\u03b2 .\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\nlog-likelihoodoftheGaussiandistributionparametrized by\u03b2involvesonlymul-\ntiplicationby \u03b2 iandadditionoflog\u03b2 i.Thegradientofmultiplication, addition,\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,\narbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\ncanvanishnearzero,makingitdi\ufb03culttolearnparametersthataresquared.\n1 8 8", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivede\ufb01nite.\u00a0Because\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\npositivede\ufb01nite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.\nIfwesupposethataistherawactivationofthemodelusedtodeterminethe\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\nvector:\u03b2= \u03b6(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\ndiagonalmatrix.\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\ndiagonal.\u00a0Ifthecovarianceisfullandconditional,thenaparametrization must\nbechosenthatguaranteespositive-de\ufb01nitenessofthepredictedcovariancematrix.\nThiscanbeachievedbywriting \u03a3() = ()xBxB\ue03e()x,whereBisanunconstrained\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\nlikelihoodisexpensive,witha d d\u00d7matrixrequiring O( d3)computationforthe\ndeterminantandinverseof \u03a3(x)(orequivalently,andmorecommonlydone,its\neigendecompositionorthatof).Bx()\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues\nthatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldi\ufb00erent\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisde\ufb01nedby\ntheconditionalprobabilitydistribution\np( ) =yx|n\ue058\ni = 1p i (= c |Nx)(;y\u00b5( ) i()x , \u03a3( ) i())x .(6.35)\nTheneuralnetworkmusthavethreeoutputs:avectorde\ufb01ning p(c= i|x),a\nmatrixproviding\u00b5( ) i(x)forall i,andatensorproviding \u03a3( ) i(x)forall i.These\noutputsmustsatisfydi\ufb00erentconstraints:\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\noverthe ndi\ufb00erentcomponentsassociatedwithlatentvariable1c,andcan\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t\ny , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\nra n d o m v a ria b l e .\n1 8 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\nthattheseoutputsarepositiveandsumto1.\n2.Means\u00b5( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\nan n d\u00d7matrixcontainingall nofthese d-dimensionalvectors.\u00a0Learning\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan\nlearningthemeansofadistributionwithonlyoneoutputmode.Weonly\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\neachexample\u2019scontributiontothelossforeachcomponentbytheprobability\nthatthecomponentproducedtheexample.\n3.Covariances \u03a3( ) i(x):thesespecifythecovariancematrixforeachcomponent\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal\nmatrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\nspeci\ufb01cationofthenegativelog-likelihoodunderthemixturemodel.\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone\ngetsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,). MurrayandLarochelle2014\nGaussianmixtureoutputsareparticularlye\ufb00ectiveingenerativemodelsof\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput\nmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\ndensitynetworkisshownin\ufb01gure.6.4\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\nthatformsasentence.Inthese\u00a0cases,wemaycontinuetousetheprinciple\nofmaximumlikelihoodappliedtoourmodel p(y;\u03c9(x)),butthemodelweuse\n1 9 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nxy\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith\npredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\nChapterdescribeshowtouserecurrentneuralnetworkstode\ufb01nesuchmodels 10\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\nprobabilitydistributions.\n6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\narecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\nmodel.\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\nyethavemanyde\ufb01nitiveguidingtheoreticalprinciples.\nRecti\ufb01edlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\ntypesofhiddenunitsareavailable.Itcanbedi\ufb03culttodeterminewhentouse\nwhichkind(thoughrecti\ufb01edlinearunitsareusuallyanacceptablechoice).\u00a0We\n1 9 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\nonavalidationset.\nSomeofthehiddenunitsincludedinthislistarenotactuallydi\ufb00erentiableat\nallinputpoints.Forexample,therecti\ufb01edlinearfunction g( z) =max{0 , z}isnot\ndi\ufb00erentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\nforthesemodelstobeusedformachinelearningtasks.\u00a0Thisisinpartbecause\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\nthecostfunction,butinsteadmerelyreduceitsvaluesigni\ufb01cantly,asshownin\n\ufb01gure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8\nexpecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\nfortheminimaofthecostfunctiontocorrespondtopointswithunde\ufb01nedgradient.\nHiddenunitsthatarenotdi\ufb00erentiableareusuallynon-di\ufb00erentiable atonlya\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativede\ufb01ned\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\nde\ufb01nedbytheslopeofthefunctionimmediately totherightof z.Afunction\nisdi\ufb00erentiableat zonlyifboththeleftderivativeandtherightderivativeare\nde\ufb01nedandequaltoeachother.Thefunctionsusedinthecontextofneural\nnetworksusuallyhavede\ufb01nedleftderivativesandde\ufb01nedrightderivatives.Inthe\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\ntheone-sidedderivativesratherthanreportingthatthederivativeisunde\ufb01nedor\nraisinganerror.\u00a0Thismaybeheuristicallyjusti\ufb01edbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.\nWhenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 \ue00fthatwasrounded\nto.Insomecontexts,moretheoreticallypleasingjusti\ufb01cationsareavailable,but 0\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\ninpracticeonecansafelydisregardthenon-di\ufb00erentiabilityofthehiddenunit\nactivationfunctionsdescribedbelow.\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting\navectorofinputsx,computingana\ufb03netransformationz=W\ue03ex+b,and\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\nfunction. g()z\n1 9 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1Recti\ufb01edLinearUnitsandTheirGeneralizations\nRecti\ufb01edlinearunitsusetheactivationfunction . g z , z () = max0{}\nRecti\ufb01edlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\nunits.Theonlydi\ufb00erencebetweenalinearunitandarecti\ufb01edlinearunitis\nthatarecti\ufb01edlinearunitoutputszeroacrosshalfitsdomain.\u00a0This makesthe\nderivativesthrougharecti\ufb01edlinearunitremainlargewhenevertheunitisactive.\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe\nrectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\nthatintroducesecond-ordere\ufb00ects.\nRecti\ufb01edlinearunitsaretypicallyusedontopofana\ufb03netransformation:\nhW= ( g\ue03exb+) . (6.36)\nWheninitializingtheparametersofthea\ufb03netransformation,itcanbeagood\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes\nitverylikelythattherecti\ufb01edlinearunitswillbeinitiallyactiveformostinputs\ninthetrainingsetandallowthederivativestopassthrough.\nSeveralgeneralizations ofrecti\ufb01edlinearunitsexist.Mostofthesegeneral-\nizationsperformcomparablytorecti\ufb01edlinearunitsandoccasionallyperform\nbetter.\nOnedrawbacktorecti\ufb01edlinearunitsisthattheycannotlearnviagradient-\nbased\u00a0methods\u00a0onexamples\u00a0for\u00a0which\u00a0their\u00a0activ ation\u00a0iszero.Avariety\u00a0of\ngeneralizations ofrecti\ufb01edlinearunitsguaranteethattheyreceivegradientevery-\nwhere.\nThreegeneralizations ofrecti\ufb01edlinearunitsarebasedonusinganon-zero\nslope \u03b1 iwhen z i <0: h i= g(z\u03b1 ,) i=max(0 , z i)+ \u03b1 imin(0 , z i).Absolutevalue\nrecti\ufb01cation\ufb01xes \u03b1 i=\u22121toobtain g( z) =|| z.Itisusedforobjectrecognition\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\nofrecti\ufb01edlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .\n2013)\ufb01xes \u03b1 itoasmallvaluelike0.01whileaparametricReLUorPReLU\ntreats \u03b1 iasalearnableparameter(,). He e t a l .2015\nMaxoutunits( ,)generalizerecti\ufb01edlinearunits Goodfellow e t a l .2013a\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\n1 9 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\noneofthesegroups:\ng()z i=max\nj\u2208 G() iz j (6.37)\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i\u22121) k+1 , . . . , i k}.\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\ndirectionsintheinputspace.x\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan\nlearntoapproximateanyconvexfunctionwitharbitrary\ufb01delity.Inparticular,\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\ninputxasatraditionallayerusingtherecti\ufb01edlinearactivationfunction,absolute\nvaluerecti\ufb01cationfunction,ortheleakyorparametricReLU,orcanlearnto\nimplementatotallydi\ufb00erentfunctionaltogether.Themaxoutlayerwillofcourse\nbeparametrized di\ufb00erentlyfromanyoftheseotherlayertypes,sothelearning\ndynamicswillbedi\ufb00erenteveninthecaseswheremaxoutlearnstoimplementthe\nsamefunctionofasoneoftheotherlayertypes. x\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\nsomaxoutunitstypicallyneedmoreregularizationthanrecti\ufb01edlinearunits.They\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\npiecesperunitiskeptlow(,). Cai e t a l .2013\nMaxoutunitshaveafewotherbene\ufb01ts.Insomecases,onecangainsomesta-\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speci\ufb01cally,\nifthefeaturescapturedby ndi\ufb00erentlinear\ufb01lterscanbesummarizedwithout\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\nlayercangetbywithtimesfewerweights. k\nBecauseeachunitisdrivenbymultiple\ufb01lters,maxoutunitshavesomeredun-\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\nthepast( ,). Goodfellow e t a l .2014a\nRecti\ufb01edlinearunitsandallofthesegeneralizations ofthemarebasedonthe\nprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork\n1 9 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\narchitectures,theLSTM,propagatesinformationthroughtimeviasummation\u2014a\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\ninsection.10.10\n6.3.2LogisticSigmoidandHyperbolicTangent\nPriortotheintroduction ofrecti\ufb01edlinearunits,mostneuralnetworksusedthe\nlogisticsigmoidactivationfunction\ng z \u03c3 z () = () (6.38)\northehyperbolictangentactivationfunction\ng z z . () = tanh( ) (6.39)\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z \u03c3 z\u2212\nWe\u00a0havealready\u00a0seen sigmoid\u00a0unitsasoutput\u00a0units,\u00a0usedto\u00a0predictthe\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\nunitssaturateacrossmostoftheirdomain\u2014they saturatetoahighvaluewhen\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof\nsigmoidalunitscanmakegradient-basedlearningverydi\ufb03cult.Forthisreason,\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\nlayer.\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while \u03c3(0) =1\n2.\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\nnetwork\u02c6 y=w\ue03etanh(U\ue03etanh(V\ue03ex))resemblestrainingalinearmodel\u02c6 y=\nw\ue03eU\ue03eV\ue03exsolongastheactivationsofthenetworkcanbekeptsmall.This\nmakestrainingthenetworkeasier. tanh\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise\nlinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\ndrawbacksofsaturation.\n1 9 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.3OtherHiddenUnits\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\nIngeneral,awidevarietyofdi\ufb00erentiable functionsperformperfectlywell.\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation\nfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommon\ntotestmanydi\ufb00erentactivationfunctionsand\ufb01ndthatseveralvariationson\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigni\ufb01cant\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\ntypesaresocommonastobeuninteresting.\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared\nintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider\naneuralnetworklayerwith ninputsand poutputs,h= g(W\ue03ex+b).Wemay\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\nusingweightmatrixV.Ifthe\ufb01rstlayerhasnoactivationfunction,thenwehave\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\nfactoredapproachistocomputeh= g(V\ue03eU\ue03ex+b).IfUproduces qoutputs,\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It\ncomesatthecostofconstrainingthelineartransformationtobelow-rank,but\ntheselow-rankrelationshipsareoftensu\ufb03cient.Linearhiddenunitsthuso\ufb00eran\ne\ufb00ectivewayofreducingthenumberofparametersinanetwork.\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden\nunitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\nmanipulatememory,describedinsection.10.12\n1 9 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAfewotherreasonablycommonhiddenunittypesinclude:\n\u2022RadialbasisfunctionorRBFunit: h i=exp\ue010\n\u22121\n\u03c32\ni||W : , i\u2212||x2\ue011\n.This\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\nsaturatestoformost,itcanbedi\ufb03culttooptimize. 0x\n\u2022Softplus: g( a) = \u03b6( a) =log(1+ ea).Thisisasmoothversionoftherecti\ufb01er,\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic\nmodels. ()comparedthesoftplusandrecti\ufb01erandfound Glorot e t a l .2011a\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\nbeverycounterintuitive\u2014onemightexpectittohaveanadvantageover\ntherecti\ufb01erduetobeingdi\ufb00erentiableeverywhereorduetosaturatingless\ncompletely,butempiricallyitdoesnot.\n\u2022Hardtanh:thisisshapedsimilarlytothetanhandtherecti\ufb01erbutunlike\nthelatter,itisbounded, g( a)=max(\u22121 ,min(1 , a)).Itwasintroduced\nby(). Collobert2004\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\nunittypesremaintobediscovered.\n6. 4 A rc h i t ec t u re D es i gn\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers.\u00a0Most\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach\nlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,the\ufb01rstlayer\nisgivenby\nh( 1 )= g( 1 )\ue010\nW( 1 )\ue03exb+( 1 )\ue011\n, (6.40)\nthesecondlayerisgivenby\nh( 2 )= g( 2 )\ue010\nW( 2 )\ue03eh( 1 )+b( 2 )\ue011\n, (6.41)\nandsoon.\n1 9 7", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\nanetworkwithevenonehiddenlayerissu\ufb03cientto\ufb01tthetrainingset.Deeper\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize.\u00a0The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror.\n6.4.1UniversalApproximationPropertiesandDepth\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\nbyde\ufb01nitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\nAt\ufb01rstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.\nFortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\nmationframework.Speci\ufb01cally,theuniversalapproximationtheorem(Hornik\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\nlayerandatleastonehiddenlayerwithany\u201csquashing\u201dactivationfunction(such\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\nfunctionfromone\ufb01nite-dimensional spacetoanotherwithanydesirednon-zero\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The\nderivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\nisbeyondthescopeofthisbook;\u00a0forourpurposesitsu\ufb03cestosaythatany\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\nalsoapproximateanyfunctionmappingfromany\ufb01nitedimensionaldiscretespace\ntoanother.Whiletheoriginaltheoremswere\ufb01rststatedintermsofunitswith\nactivationfunctionsthatsaturatebothforverynegativeandforverypositive\narguments,universalapproximation theoremshavealsobeenprovedforawider\nclassofactivationfunctions,whichincludesthenowcommonlyusedrecti\ufb01edlinear\nunit( ,). Leshno e t a l .1993\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable\nto l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\ncanfailfortwodi\ufb00erentreasons.First,theoptimizationalgorithmusedfortraining\n1 9 8", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmaynotbeableto\ufb01ndthevalueoftheparametersthatcorrespondstothedesired\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\nover\ufb01tting.Recallfromsectionthatthe\u201cnofreelunch\u201dtheoremshowsthat 5.2.1\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There\nisnouniversalprocedureforexaminingatrainingsetofspeci\ufb01cexamplesand\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly\nwithonehiddenunitcorrespondingtoeachinputcon\ufb01gurationthatneedstobe\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\nnumberofpossiblebinaryfunctionsonvectorsv\u2208{0 ,1}nis22nandselecting\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\nfreedom.\nInsummary,afeedforwardnetworkwithasinglelayerissu\ufb03cienttorepresent\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe\nnumberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\namountofgeneralization error.\nThereexistfamiliesoffunctionswhichcanbeapproximated e\ufb03cientlybyan\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n.\u00a0Suchresults\nwere\ufb01rstprovedformodelsthatdonotresemblethecontinuous,di\ufb00erentiable\nneuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\nmodels.The\ufb01rstresultswereforcircuitsoflogicgates(,).Later H\u00e5stad1986\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\n( ,; ,),andthentonetworkswith H\u00e5stadandGoldmann1991Hajnal e t a l .1993\ncontinuous-valuedactivations(,; ,).\u00a0Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserecti\ufb01edlinearunits. ()demonstrated Leshno e t a l .1993\nthatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\nincludingrecti\ufb01edlinearunits,haveuniversalapproximation properties,butthese\nresultsdonotaddressthequestionsofdepthore\ufb03ciency\u2014theyspecifyonlythat\nasu\ufb03cientlywiderecti\ufb01ernetworkcouldrepresentanyfunction.Montufar e t a l .\n1 9 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprecti\ufb01ernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\nfromrecti\ufb01ernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\nanetworkwithabsolutevaluerecti\ufb01cationcreatesmirrorimagesofthefunction\ncomputedontopofsomehiddenunit,withrespecttotheinputofthathidden\nunit.Eachhiddenunitspeci\ufb01eswheretofoldtheinputspaceinordertocreate\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\nrecti\ufb01ernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerecti\ufb01cation\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\nofsymmetryisgivenbythehyperplanede\ufb01nedbytheweightsandbiasoftheunit.A\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )\nbefoldedontopofthe\ufb01rst(byanotherdownstreamunit)toobtainanothersymmetry\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\npermissionfrom (). Montufar e t a l .2014\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\nnumberoflinearregionscarvedoutbyadeeprecti\ufb01ernetworkwith dinputs,\ndepth,andunitsperhiddenlayer,is l n\nO\ue020\ue012n\nd\ue013d l (\u2212 1 )\nnd\ue021\n, (6.42)\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswith\ufb01ltersper l k\nunit,thenumberoflinearregionsis\nO\ue010\nk( 1 ) + l\u2212 d\ue011\n. (6.43)\n2 0 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\nWemayalsowanttochooseadeepmodelforstatisticalreasons.\u00a0Anytime\nwechooseaspeci\ufb01cmachinelearningalgorithm,weareimplicitlystatingsome\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe\nwanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\nmultiplesteps,whereeachstepmakesuseofthepreviousstep\u2019soutput.\u00a0These\nintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;\ne t a l . e t a l . ,;2014dSzegedy ,).See\ufb01gureand\ufb01gureforexamplesof 2014a 6.6 6.7\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\n6.4.2OtherArchitecturalConsiderations\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\nInpractice,neuralnetworksshowconsiderablymorediversity.\nManyneuralnetworkarchitectures havebeendevelopedforspeci\ufb01ctasks.\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\nhavetheirownarchitecturalconsiderations.\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra\narchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradientto\ufb02owfrom\noutputlayerstolayersnearertheinput.\n2 0 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n3 4 5 6 7 8 9 1 0 1 1\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\ne t a l .().\u00a0Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth.\u00a0See 2014d\n\ufb01gureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesamee\ufb00ect.\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin\ntheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\nthatareverye\ufb00ectiveforcomputervisionproblems.Inthischapter,itisdi\ufb03cult\ntogivemuchmorespeci\ufb01cadviceconcerningthearchitectureofagenericneural\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat\nhavebeenfoundtoworkwellfordi\ufb00erentapplicationdomains.\n2 0 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\nN u m b e r o f p a r a m e t e r s \u00d7 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\n3,fullyconnected\n11,convolutional\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot\nnearlyase\ufb00ectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\ncontextover\ufb01tataround20millionparameterswhiledeeponescanbene\ufb01tfromhaving\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\nthespaceoffunctionsthemodelcanlearn.Speci\ufb01cally,itexpressesabeliefthatthe\nfunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\ncornersde\ufb01nedintermsofedges)orinlearningaprogramwithsequentiallydependent\nsteps(e.g.,\ufb01rstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\nthem).\n2 0 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i \ufb00 eren t i at i on A l go-\nri t h m s\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\noutput \u02c6y,information\ufb02owsforwardthroughthenetwork.Theinputsxprovide\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\nand\ufb01nallyproduces \u02c6y.Thisiscalledforwardpropagation.Duringtraining,\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(\u03b8).\nTheback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\nbackprop,allowstheinformationfromthecosttothen\ufb02owbackwardsthrough\nthenetwork,inordertocomputethegradient.\nComputingananalyticalexpressionforthegradientisstraightforward,but\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\nThetermback-propagation isoften\u00a0misunders toodasmeaningthewhole\nlearningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\nFurthermore,back-propagation isoftenmisunderstoodasbeingspeci\ufb01ctomulti-\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\nfunctionisunde\ufb01ned).Speci\ufb01cally,wewilldescribehowtocomputethegradient\n\u2207 x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\n\u2207 \u03b8 J(\u03b8).Manymachinelearningtasksinvolvecomputingotherderivatives,either\naspartof\u00a0thelearning\u00a0process,\u00a0or\u00a0to analyzethelearned\u00a0model. The\u00a0back-\npropagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\nusedcasewherehasasingleoutput. f\n2 0 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.1ComputationalGraphs\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\nmoreprecise language. computationalgraph\nManywaysofformalizingcomputationasgraphsarepossible.\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.\nAnoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\ntogether.\nWithoutlossofgenerality,\u00a0wede\ufb01neanoperationtoreturnonlyasingle\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour\ndescriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\nconceptualunderstanding.\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\nwedrawadirectededgefrom xto y.\u00a0Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext.\nExamplesofcomputational graphsareshownin\ufb01gure.6.8\n6.5.2ChainRuleofCalculus\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is\nusedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\nchainrule,withaspeci\ufb01corderofoperationsthatishighlye\ufb03cient.\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x. (6.44)\nWecangeneralizethisbeyondthescalarcase.Supposethatx\u2208 Rm,y\u2208 Rn,\n2 0 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxx yy\n( a)\u00d7\nx x ww\n( b)u( 1 )u( 1 )\nd o t\nbbu( 2 )u( 2 )\n+\u02c6 y \u02c6 y\n\u03c3\n( c )XX WWU( 1 )U( 1 )\nm a t m u l\nb bU( 2 )U( 2 )\n+HH\nr e l u\nx x ww\n( d)\u02c6 y\u02c6 y\nd o t\n\u03bb \u03bbu( 1 )u( 1 )\ns q ru( 2 )u( 2 )\ns u mu( 3 )u( 3 )\n\u00d7\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) \u00d7operationto\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) \u02c6 y= \u03c3\ue000\nx\ue03ew+ b\ue001\n.\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression\nbutneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\nmatrixofrecti\ufb01edlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\nofinputsX.Examplesa\u2013cappliedatmostoneoperationtoeachvariable,butit ( d )\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\nweightsareusedtomakeboththeprediction\u02c6 yandtheweightdecaypenalty \u03bb\ue050\niw2\ni.\n2 0 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\n\u2202 z\n\u2202 x i=\ue058\nj\u2202 z\n\u2202 y j\u2202 y j\n\u2202 x i. (6.45)\nInvectornotation,thismaybeequivalentlywrittenas\n\u2207 x z=\ue012\u2202y\n\u2202x\ue013\ue03e\n\u2207 y z , (6.46)\nwhere\u2202 y\n\u2202 xistheJacobianmatrixof. n m\u00d7 g\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\naJacobianmatrix\u2202 y\n\u2202 xbyagradient\u2207 y z.Theback-propagation algorithmconsists\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.\nUsuallywedonotapplytheback-propagationalgorithmmerelytovectors,\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\nsameasback-propagation withvectors.Theonlydi\ufb00erenceishowthenumbers\narearrangedinagridtoformatensor.Wecouldimagine\ufb02atteningeachtensor\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\nback-propagationisstilljustmultiplyingJacobiansbygradients.\nTodenotethegradientofavalue zwithrespecttoatensor X,wewrite \u2207 X z,\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates\u2014for\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\npossibleindextuples i,(\u2207 X z) igives\u2202 z\n\u2202 X i.Thisisexactlythesameashowforall\npossibleintegerindices iintoavector,(\u2207 x z) igives\u2202 z\n\u2202 x i.Usingthisnotation,we\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\n\u2207 X z=\ue058\nj(\u2207 X Y j)\u2202 z\n\u2202 Y j. (6.47)\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\nintroducessomeextraconsiderations.\nSpeci\ufb01cally,manysubexpressionsmayberepeatedseveraltimeswithinthe\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n2 0 7", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\n\ufb01gure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\nbewasteful.\u00a0Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime.\nWe\ufb01rstbeginbyaversionoftheback-propagationalgorithmthatspeci\ufb01esthe\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeci\ufb01cation\nofthecomputational graphforcomputingtheback-propagation. However,this\nformulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\nsymbolicgraphthatperformsthegradientcomputation.\u00a0Such aformulationis\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\nnodesthatcontainarbitrarytensors.\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).\u00a0In\notherwordswewishtocompute\u2202 u() n\n\u2202 u() iforall i\u2208{1 ,2 , . . . , n i}.Intheapplication\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\ncorrespondtotheparametersofthemodel.\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\ngoingupto u( ) n.Asde\ufb01nedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan\noperation f( ) iandiscomputedbyevaluatingthefunction\nu( ) i= ( f A( ) i) (6.48)\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\nThatalgorithmspeci\ufb01estheforwardpropagationcomputation,whichwecould\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\nthederivative\u2202 u() n\n\u2202 u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\n2 0 8", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\nu( 1 )to u( n i )toanoutput u( ) n.Thisde\ufb01nesacomputational graphwhereeachnode\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a\u2208 ( u( ) i).The\ninputtothecomputational graphisthevectorx,andissetintothe\ufb01rst n inodes\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreado\ufb00thelast(output)\nnode u( ) n.\nfor i , . . . , n = 1 ido\nu( ) i\u2190 x i\nendfor\nfor i n= i+1 , . . . , ndo\nA( ) i\u2190{ u( ) j|\u2208 j P a u(( ) i)}\nu( ) i\u2190 f( ) i( A( ) i)\nendfor\nreturn u( ) n\nusingthechainrulewithrespecttoscalaroutput u( ) n:\n\u2202 u( ) n\n\u2202 u( ) j=\ue058\ni j P a u :\u2208 (() i )\u2202 u( ) n\n\u2202 u( ) i\u2202 u( ) i\n\u2202 u( ) j(6.49)\nasspeci\ufb01edbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\nthecomputationof\u2202 u() i\n\u2202 u() j.Inaddition,adotproductisperformedforeachnode,\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\nof u( ) jandthevectorcontainingthepartialderivatives\u2202 u() i\n\u2202 u() jforthesamechildren\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone\nnodewithrespecttooneofitsparents)aswellasperformingonemultiplication\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\ne\ufb03cientimplementations.\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\nsubexpressionswithoutregardtomemory.Speci\ufb01cally,itperformsontheorder\nofoneJacobianproductpernodeinthegraph.\u00a0Thiscanbeseenfromthefact\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof\nthegraphexactlyonceinordertoobtaintheassociatedpartialderivative\u2202 u() i\n\u2202 u() j.\n2 0 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.2Simpli\ufb01edversionoftheback-propagation algorithmforcomputing\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\nintendedtofurtherunderstandingbyshowingasimpli\ufb01edcasewhereallvariables\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\nThissimpli\ufb01edversioncomputesthederivativesofallnodesinthegraph.\u00a0The\ncomputational costofthisalgorithmisproportional tothenumberofedgesin\nthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\ntheforwardpropagation. Each\u2202 u() i\n\u2202 u() jisafunctionoftheparents u( ) jof u( ) i,thus\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\ngraph.\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\ntionsofthenetwork\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave\nbeencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\n\u2202 u() n\n\u2202 u() i.\ng r a d t a b l e_ [ u( ) n] 1\u2190\nfor do j n= \u22121downto1\nThenextlinecomputes\u2202 u() n\n\u2202 u() j=\ue050\ni j P a u :\u2208 (() i )\u2202 u() n\n\u2202 u() i\u2202 u() i\n\u2202 u() jusingstoredvalues:\ng r a d t a b l e_ [ u( ) j] \u2190\ue050\ni j P a u :\u2208 (() i ) g r a d t a b l e_ [ u( ) i]\u2202 u() i\n\u2202 u() j\nendfor\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.\nHowever,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\nsimpli\ufb01cationsonthecomputational graph,ormaybeabletoconservememoryby\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\nafterdescribingtheback-propagation algorithmitself.\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\nToclarifytheabovede\ufb01nitionoftheback-propagation computation,letusconsider\nthespeci\ufb01cgraphassociatedwithafully-connected multi-layerMLP.\nAlgorithm\ufb01rstshowstheforwardpropagation, whichmapsparametersto 6.3\nthesupervisedloss L(\u02c6yy ,)associatedwithasingle(input,target) trainingexample\n( )xy ,,with \u02c6ytheoutputoftheneuralnetworkwhenisprovidedininput. x\nAlgorithm\u00a0 then\u00a0shows\u00a0thecorresponding\u00a0computation\u00a0to be\u00a0donefor 6.4\n2 1 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfff\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\nthegradient.Let w\u2208 Rbetheinputtothegraph.Weusethesamefunction f: R R\u2192\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\nTocompute\u2202 z\n\u2202 w,weapplyequationandobtain: 6.44\n\u2202 z\n\u2202 w(6.50)\n=\u2202 z\n\u2202 y\u2202 y\n\u2202 x\u2202 x\n\u2202 w(6.51)\n= f\ue030() y f\ue030() x f\ue030() w (6.52)\n= f\ue030((())) f f w f\ue030(()) f w f\ue030() w (6.53)\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52\nruntime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\nusefulwhenmemoryislimited.\n2 1 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napplyingtheback-propagation algorithmtothisgraph.\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However,\u00a0theyarespecializedtoonespeci\ufb01c\nproblem.\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\ncomputation.\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\nthecomputationofthecostfunction.Theloss L(\u02c6yy ,)dependsontheoutput\n\u02c6yandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer \u2126( \u03b8),where \u03b8\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould\nuseaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . . , l , \u2208{1 }theweightmatricesofthemodel\nRequire:b( ) i, i , . . . , l , \u2208{1 }thebiasparametersofthemodel\nRequire:x,theinputtoprocess\nRequire:y,thetargetoutput\nh( 0 )= x\nfordo k , . . . , l = 1\na( ) k= b( ) k+W( ) kh( 1 ) k\u2212\nh( ) k= ( fa( ) k)\nendfor\n\u02c6yh= ( ) l\nJ L= (\u02c6yy ,)+\u2126() \u03bb \u03b8\n6.5.5Symbol-to-SymbolDerivatives\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or\nvariables\u00a0thatdo\u00a0not\u00a0havespeci\ufb01c\u00a0values.Thesealgebraic\u00a0and graph-based\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\ntrainaneuralnetwork,wemustassignspeci\ufb01cvaluestothesesymbols.We\nreplaceasymbolicinputtothenetworkxwithaspeci\ufb01cnumericvalue,suchas\n[123765 18] . , . ,\u2212 .\ue03e.\n2 1 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\noutputlayerandgoingbackwardstothe\ufb01rsthiddenlayer.Fromthesegradients,\nwhichcanbeinterpretedasanindicationofhoweachlayer\u2019soutputshouldchange\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The\ngradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\ncomputed)orusedwithothergradient-basedoptimization methods.\nAftertheforwardcomputation,computethegradientontheoutputlayer:\ng\u2190\u2207 \u02c6 y J= \u2207 \u02c6 y L(\u02c6yy ,)\nfor do k l , l , . . . , = \u22121 1\nConvert\u00a0thegradienton\u00a0thelayer\u2019s\u00a0output\u00a0into\u00a0a\u00a0gradient\u00a0into\u00a0thepre-\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\ng\u2190\u2207a() k J f = g\ue00c\ue030(a( ) k)\nComputegradientsonweightsandbiases(includingtheregularizationterm,\nwhereneeded):\n\u2207b() k J \u03bb = +g \u2207b() k\u2126() \u03b8\n\u2207W() k J= gh( 1 ) k\u2212\ue03e+ \u03bb\u2207W() k\u2126() \u03b8\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer\u2019sactivations:\ng\u2190\u2207h(1) k \u2212 J= W( ) k\ue03eg\nendfor\n2 1 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfffz z\nxxyy\nw wfff\nd z\nd yd z\nd yf\ue021\nd y\nd xd y\nd xf\ue021\nd z\nd xd z\nd x\u00d7\nd x\nd wd x\nd wf\ue021\nd z\nd wd z\nd w\u00d7\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\nspeci\ufb01cnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe\nderivativesforanyspeci\ufb01cnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\nittoconstructthegraphfortheexpressioncorrespondingtod z\nd w.Inthisexample,wedo\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\nderivative.\nSomeapproachestoback-propagationtakeacomputational graphandaset\nofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach\u201csymbol-\nto-number\u201ddi\ufb00erentiation. ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCa\ufb00e(,). Collobert e t a l .2011b Jia2013\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012\nandTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\nisillustratedin\ufb01gure.Theprimaryadvantageofthisapproachisthat 6.10\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\nback-propagationagain,di\ufb00erentiating thederivativesinordertoobtainhigher\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin\n2 1 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\ngraphmaythenbeevaluatedusingspeci\ufb01cnumericalvaluesatalatertime.This\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\nparents\u2019valuesareavailable.\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas\nperformingexactlythesamecomputations asaredoneinthegraphbuiltbythe\nsymbol-to-symbolapproach.Thekeydi\ufb00erenceisthatthesymbol-to-number\napproachdoesnotexposethegraph.\n6.5.6GeneralBack-Propagation\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\nthatthegradientwithrespectto zisgivenbyd z\nd z=1.Wecanthencompute\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe\ncurrentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\ntwoormorepaths,wesimplysumthegradientsarrivingfromdi\ufb00erentpathsat\nthatnode.\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\nmaximumgenerality,wedescribethisvariableasbeingatensor V.\u00a0Tensorcan\ningeneralhaveanynumberofdimensions.\u00a0Theysubsumescalars,vectors,and\nmatrices.\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\n\u2022 g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\noperation,andtheget_operationfunction.Supposewehaveavariablethat\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\nreturnsapointertoaninstanceofthecorrespondingC++class.\n\u2022 g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\nVinthecomputational graph.G\n\u2022 G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\ninthecomputational graph.G\n2 1 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient\nofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\nisresponsibleforde\ufb01ningtwoback-propagation rules,oneforeachofitsinput\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\nisgivenbyGB\ue03e.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the\nb p r o pmethodandspecifyingthatthedesiredgradientisgivenbyA\ue03eG.The\nback-propagationalgorithmitselfdoesnotneedtoknowanydi\ufb00erentiation rules.It\nonlyneedstocalleachoperation\u2019sbpropruleswiththerightarguments.Formally,\no p b p r o p i n p u t s . ( , , X G)mustreturn\n\ue058\ni(\u2207 X o p f i n p u t s .( ) i) G i , (6.54)\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe\nmathematical functionthattheoperationimplements, Xistheinputwhosegradient\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\nderivativeon. x\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\nanynewoperationsmanually.\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\n2 1 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\ninthe subroutineofalgorithm build_grad 6.6.\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\nRequire:G,thecomputational graph\nRequire: z,thevariabletobedi\ufb00erentiated\nLetG\ue030beGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin. T\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\ng r a d t a b l e_ [] 1 z\u2190\nfordo Vin T\nb u i l d g r a d_ ( V , ,GG\ue030, g r a d t a b l e_ )\nendfor\nReturn restrictedto grad_table T\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\nNowthatwehavespeci\ufb01edtheback-propagationalgorithm,wecanunderstandits\ncomputational cost.Ifweassumethateachoperationevaluationhasroughlythe\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan\nO( n2) operations.Herewearecountingoperationsinthecomputational graph,not\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,\nwemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n2 1 7", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG\ue030, g r a d t a b l e_ )of\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmde\ufb01ned\ninalgorithm .6.5\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\nRequire:G,thegraphtomodify.\nRequire:G\ue030,therestrictionoftonodesthatparticipateinthegradient. G\nRequire:grad_table,adatastructuremappingnodestotheirgradients\nif then Visingrad_table\nReturn_ g r a d t a b l e[] V\nendif\ni\u21901\nfor C V in_ g e t c o n s u m e r s( ,G\ue030)do\no p g e t o p e r a t i o n \u2190_ () C\nD C \u2190 b u i l d g r a d_ ( , ,GG\ue030, g r a d t a b l e_ )\nG( ) i\u2190 G o p b p r o p g e t i n p u t s . (_ ( C ,\ue030) ) , , V D\ni i\u2190+1\nendfor\nG\u2190\ue050\ni G( ) i\ng r a d t a b l e_ [] = V G\nInsertandtheoperationscreatingitinto G G\nReturn G\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany\nnodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\ntherecursivechainrule(equation)non-recursively: 6.49\n\u2202 u( ) n\n\u2202 u( ) j=\ue058\npa t h ( u( \u03c01), u( \u03c02), . . . , u( \u03c0 t)) ,\nf r o m \u03c01 = t o j \u03c0 t = nt\ue059\nk = 2\u2202 u( \u03c0 k )\n\u2202 u( \u03c0 k \u22121 ). (6.55)\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation\ngraph.Thislargecostwouldbeincurredbecausethesamecomputationfor\n\u2202 u() i\n\u2202 u() jwouldberedonemanytimes.\u00a0Toavoidsuchrecomputation, wecanthink\nofback-propagation asatable-\ufb01llingalgorithmthattakesadvantageofstoring\nintermediateresults\u2202 u() n\n\u2202 u() i.Eachnodeinthegraphhasacorrespondingslotina\ntabletostorethegradientforthatnode.By\ufb01llinginthesetableentriesinorder,\n2 1 8", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-\ufb01lling\nstrategyissometimescalled . dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron.\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona\nsingleminibatch.Speci\ufb01cally,weuseaminibatchofexamplesfromthetraining\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy\noperationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\ndistributionde\ufb01nedbytheseunnormalized logprobabilities. Theresultingcross-\nentropyde\ufb01nesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\nlikelihoodestimationoftheclassi\ufb01er.However,tomakethisexamplemorerealistic,\nwealsoincludearegularizationterm.Thetotalcost\nJ J= M LE+ \u03bb\uf8eb\n\uf8ed\ue058\ni , j\ue010\nW( 1 )\ni , j\ue0112\n+\ue058\ni , j\ue010\nW( 2 )\ni , j\ue0112\uf8f6\n\uf8f8 (6.56)\nconsistsofthecross-entropyandaweightdecaytermwithcoe\ufb03cient \u03bb.The\ncomputational graphisillustratedin\ufb01gure.6.11\nThecomputational graphforthegradientofthisexampleislargeenoughthat\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebene\ufb01ts\noftheback-propagation algorithm,whichisthatitcanautomatically generate\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\nderivemanually.\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\nbylookingattheforwardpropagationgraphin\ufb01gure.Totrain,wewish 6.11\ntocomputeboth\u2207W(1) Jand \u2207W(2) J.Therearetwodi\ufb00erentpathsleading\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\nalwayscontribute 2 \u03bbW( ) itothegradientonW( ) i.\n2 1 9", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nXXW( 1 )W( 1 )U( 1 )U( 1 )\nm a t m u lHH\nr e l u\nU( 3 )U( 3 )\ns q ru( 4 )u( 4 )\ns u m\u03bb \u03bb u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\nm a t m u ly yJ M L E J M L E\nc r o s s _ e n t r o p y\nU( 5 )U( 5 )\ns q ru( 6 )u( 6 )\ns u mu( 8 )u( 8 )J J\n+\n\u00d7\n+\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.\nLetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\nexploretwodi\ufb00erentbranches.Ontheshorterbranch,itaddsH\ue03eGtothe\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\ncomputes \u2207 H J=GW( 2 )\ue03eusingtheback-propagationruleforthe\ufb01rstargument\ntothematrixmultiplication operation.Next,thereluoperationusesitsback-\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G\ue030.Thelaststepofthe\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\nargumentoftheoperationtoadd matmul X\ue03eG\ue030tothegradientonW( 1 ).\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto\nupdatetheparameters.\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\n2 2 0", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe\nnumberofexamplesintheminibatchand n histhenumberofhiddenunits.\n6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice.\nAsnotedabove,wehaverestrictedthede\ufb01nitionofanoperationtobea\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis\nbesttocomputebothinasinglepassthroughmemory,soitismoste\ufb03cientto\nimplementthisprocedureasasingleoperationwithtwooutputs.\nWe\u00a0havenot\u00a0described\u00a0how\u00a0tocontrolthememoryconsumption\u00a0ofback-\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebu\ufb00erand\naddingeachvaluetothatbu\ufb00erasitiscomputed.\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\ndatatypes,suchas32-bit\ufb02oatingpoint,64-bit\ufb02oatingpoint,andintegervalues.\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\nSomeoperationshaveunde\ufb01nedgradients,anditisimportanttotrackthese\ncasesanddeterminewhetherthegradientrequestedbytheuserisunde\ufb01ned.\nVariousothertechnicalitiesmakereal-worlddi\ufb00erentiation morecomplicated.\nThesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\nthatmanymoresubtletiesexist.\n6.5.9Di\ufb00erentiationoutsidetheDeepLearningCommunity\nThe\u00a0deeplearning\u00a0comm unityhas\u00a0beensomewhat\u00a0isolatedfrom\u00a0the\u00a0broader\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n2 2 1", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nconcerninghowtoperformdi\ufb00erentiation. Moregenerally,the\ufb01eldofautomatic\ndi\ufb00erentiationisconcernedwithhowtocomputederivativesalgorithmically .\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\ndi\ufb00erentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\nruleindi\ufb00erentorders.Ingeneral,\u00a0determining theorderofevaluationthat\nresultsinthelowestcomputational costisadi\ufb03cultproblem.Findingtheoptimal\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\nexpensiveform.\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\nwede\ufb01ne\nq i=exp( z i)\ue050\niexp( z i), (6.57)\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\noperations,\u00a0and\u00a0construct\u00a0a cross-entropyloss J=\u2212\ue050\ni p ilog q i.Ahuman\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\nsimpleform: q i\u2212 p i.Theback-propagation algorithmisnotcapableofsimplifying\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware\nlibrariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\nbythepureback-propagation algorithm.\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\n\u2202 u() i\n\u2202 u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\nthesameorderasthenumberofcomputations fortheforwardcomputation: this\ncanbeseeninalgorithm becauseeachlocalpartialderivative 6.2\u2202 u() i\n\u2202 u() jneedsto\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\ntask.\u00a0ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon\nmatchingknownsimpli\ufb01cationpatternsinordertoiterativelyattempttosimplify\nthegraph.Wede\ufb01nedback-propagation onlyforthecomputationofagradientofa\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\nof kdi\ufb00erentscalarnodesinthegraph,orofatensor-valuednodecontaining k\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\n2 2 2", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\nanotherformofautomaticdi\ufb00erentiationcalledforwardmodeaccumulation.\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989\nalsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\no\ufb00computational e\ufb03ciencyformemory.Therelationshipbetweenforwardmode\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\nright-multiplyingasequenceofmatrices,suchas\nABCD , (6.58)\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend\nandgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\nthemultiplications left-to-right,correspondingtotheforwardmode.\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\nplementdi\ufb00erentiationsoftwarethatactsdirectlyontraditionalprogramming\nlanguagecode,suchasPythonorCcode,andautomatically generatesprograms\nthatdi\ufb00erentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\nrequiringthelibrarydevelopertode\ufb01nethebpropmethodsforeveryoperation\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeende\ufb01ned.\nHowever,thespecializedapproachalsohasthebene\ufb01tofallowingcustomized\nback-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\nwouldpresumablybeunabletoreplicate.\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\nlearningcommunityverywell.Inthefuture,di\ufb00erentiation technologyfordeep\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\ninthebroader\ufb01eldofautomaticdi\ufb00erentiation.\n2 2 3", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.10Higher-OrderDerivatives\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\nderivativesastheyusetodescribetheoriginalfunctionbeingdi\ufb00erentiated.This\nmeansthatthesymbolicdi\ufb00erentiation machinerycanbeappliedtoderivatives.\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative\nofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\nmatrix.Ifwehaveafunction f: Rn\u2192 R,thentheHessianmatrixisofsize n n\u00d7.\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\nthusinfeasibletoevenrepresent.\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor\nperformingvariousoperationslikeapproximately invertingamatrixor\ufb01nding\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\notherthanmatrix-vector products.\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\nHv= \u2207 x\ue068\n(\u2207 x f x())\ue03ev\ue069\n. (6.59)\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-\ncallybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\ntakesthegradientofafunctionoftheinnergradientexpression.\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\nspecifythattheautomaticdi\ufb00erentiationsoftwareshouldnotdi\ufb00erentiatethrough\nthegraphthatproduced.v\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\ne( ) iistheone-hotvectorwith e( ) i\ni= 1andallotherentriesequalto0.\n6. 6 Hi s t or i c a l Not es\nFeedforwardnetworkscanbeseenase\ufb03cientnonlinearfunctionapproximators\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\n2 2 4", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask.\nThechainrulethatunderliestheback-propagation algorithmwasinvented\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L\u2019H\u00f4pital1696\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\noptimization problemsuntilthe19thcentury(Cauchy1847,).\nBeginninginthe1940s,thesefunctionapproximation techniqueswereusedto\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\nseveralofthe\ufb02awsofthelinearmodelfamily,suchasitsinabilitytolearnthe\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\nceptronandameansofcomputingthegradientthroughsuchamodel.E\ufb03cient\napplicationsofthechainrulebasedondynamicprogramming begantoappear\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\ntechniquestotrainingarti\ufb01cialneuralnetworks.Theideawas\ufb01nallydeveloped\ninpracticeafterbeingindependentlyrediscoveredindi\ufb00erentways(,;LeCun1985\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-\ncessingpresentedtheresultsofsomeofthe\ufb01rstsuccessfulexperimentswith\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\nresearchinmulti-layerneuralnetworks.\u00a0However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation.\u00a0Theyincludecrucialideasaboutthepossiblecomputational\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame\nunderthenameof\u201cconnectionism\u201d becauseoftheimportancethisschoolofthought\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\ne t a l .,).1986\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\nbeganin2006.\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s.\u00a0Thesameback-propagationalgorithmandthesame\n2 2 5", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\nnetworksnoticeably.\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\nhadpreviouslysu\ufb00eredfromsaturationandslowlearningwhenusingthemean\nsquarederrorloss.\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\nlinearhiddenunits,suchasrecti\ufb01edlinearunits.Recti\ufb01cationusingthemax{0 , z}\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\nmodelsdid\u00a0notuserecti\ufb01ed\u00a0linearunits,\u00a0but\u00a0insteadappliedrecti\ufb01cation\u00a0to\nnonlinearfunctions.Despitetheearlypopularityofrecti\ufb01cation,recti\ufb01cationwas\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\nwhenneuralnetworksareverysmall.Asoftheearly2000s,recti\ufb01edlinearunits\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\nnon-di\ufb00erentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\nJarrett2009 e t a l .()observedthat\u201cusingarectifyingnonlinearityisthesinglemost\nimportantfactorinimprovingtheperformanceofarecognitionsystem\u201damong\nseveraldi\ufb00erentfactorsofneuralnetworkarchitecturedesign.\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\nRandomweightsaresu\ufb03cienttopropagateusefulinformationthrougharecti\ufb01ed\nlinearnetwork,allowingtheclassi\ufb01erlayeratthetoptolearnhowtomapdi\ufb00erent\nfeaturevectorstoclassidentities.\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\nshowedthatlearningisfareasierindeeprecti\ufb01edlinearnetworksthanindeep\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\n2 2 6", "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRecti\ufb01edlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohave\u00a0anin\ufb02uenceonthe\u00a0developmentofdeep\nlearningalgorithms. ()motivaterecti\ufb01edlinearunitsfrom Glorot e t a l .2011a\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\ncompletelyinactive.2)Forsomeinputs,abiologicalneuron\u2019soutputisproportional\ntoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.\nToday,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it\nismorecommontousesupervisedlearningtosupportunsupervisedlearning.\nFeedforwardnetworkscontinuetohaveunful\ufb01lledpotential.Inthefuture,we\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\nsubsequentchapters,weturntohowtousethesemodels\u2014howtoregularizeand\ntrainthem.\n2 2 7", "C h a p t e r 7\nRegularization f or D e e p L e ar n i n g\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\nasregularization.\u00a0As wewillseethereareagreatmanyformsofregularization\navailabletothedeeplearningpractitioner. Infact,\u00a0developingmoree\ufb00ective\nregularizationstrategieshasbeenoneofthemajorresearche\ufb00ortsinthe\ufb01eld.\nChapterintroducedthebasicconceptsofgeneralization, under\ufb01tting,over\ufb01t- 5\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\ntoformdeepmodels.\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.\nIfyouarealreadyfamiliarwiththeseconcepts,\u00a0feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks.\nInsection,wede\ufb01nedregularizationas\u201canymodi\ufb01cationwemaketo 5.2.2\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\nitstrainingerror.\u201dTherearemanyregularizationstrategies.Someputextra\nconstraints\u00a0ona\u00a0machine\u00a0learning\u00a0model, such\u00a0asadding\u00a0restrictionson\u00a0the\nparametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\n228", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\nspeci\ufb01ckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.\nInthecontextofdeeplearning,mostregularizationstrategiesarebasedon\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\nbiasforreducedvariance.Ane\ufb00ectiveregularizerisonethatmakesapro\ufb01table\ntrade,reducingvariancesigni\ufb01cantlywhilenotoverlyincreasingthebias.Whenwe\ndiscussedgeneralization andover\ufb01ttinginchapter,wefocusedonthreesituations, 5\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\nprocess\u2014correspondingtounder\ufb01ttingandinducingbias,or(2)matchedthetrue\ndatageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\notherpossiblegeneratingprocesses\u2014theover\ufb01ttingregimewherevariancerather\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\nmodelfromthethirdregimeintothesecondregime.\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso\nwecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\nextent,wearealwaystryingto\ufb01tasquarepeg(thedatageneratingprocess)into\naroundhole(ourmodelfamily).\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\nsimplematterof\ufb01ndingthemodeloftherightsize,withtherightnumberof\nparameters.Instead,wemight\ufb01nd\u2014andindeedinpracticaldeeplearningscenarios,\nwealmostalwaysdo\ufb01nd\u2014thatthebest\ufb01ttingmodel(inthesenseofminimizing\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\nmodel.\n2 2 9", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.1ParameterNormPenalties\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\nande\ufb00ectiveregularizationstrategies.\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\nrameternormpenalty \u2126(\u03b8)totheobjectivefunction J.Wedenotetheregularized\nobjectivefunctionby\u02dc J:\n\u02dc J , J , \u03b1 (;\u03b8Xy) = (;\u03b8Xy)+\u2126()\u03b8 (7.1)\nwhere \u03b1\u2208[0 ,\u221e)isahyperparameter thatweightstherelativecontributionofthe\nnormpenaltyterm,,relativetothestandardobjectivefunction \u2126 J.Setting \u03b1to0\nresultsinnoregularization. Largervaluesof \u03b1correspondtomoreregularization.\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction \u02dc Jit\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\nofthesizeoftheparameters\u03b8(orsomesubsetoftheparameters).Di\ufb00erent\nchoicesfortheparameternormcanresultindi\ufb00erentsolutionsbeingpreferred. \u2126\nInthissection,wediscussthee\ufb00ectsofthevariousnormswhenusedaspenalties\nonthemodelparameters.\nBeforedelvingintotheregularizationbehaviorofdi\ufb00erentnorms,wenotethat\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythat\u2126\npenalizes ofthea\ufb03netransformationateachlayerandleaves onlytheweights\nthebiasesunregularized. Thebiasestypicallyrequirelessdatato\ufb01taccurately\nthantheweights.\u00a0Eachweightspeci\ufb01eshowtwovariablesinteract.\u00a0Fittingthe\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\ncanintroduceasigni\ufb01cantamountofunder\ufb01tting. Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbea\ufb00ectedbyanormpenalty,whilethe\nvector\u03b8denotesalloftheparameters,includingbothwandtheunregularized\nparameters.\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\npenaltywithadi\ufb00erent \u03b1coe\ufb03cientforeachlayerofthenetwork.Becauseitcan\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\nsearchspace.\n2 3 0", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\nbyaddingaregularizationterm\u2126(\u03b8) =1\n2\ue06b\ue06bw2\n2totheobjectivefunction.Inother\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\nTikhonovregularization.\nWecangainsomeinsightintothebehaviorofweightdecayregularization\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\npresentation,weassumenobiasparameter,so\u03b8isjustw.Suchamodelhasthe\nfollowingtotalobjectivefunction:\n\u02dc J , (;wXy) =\u03b1\n2w\ue03ewwXy +( J; ,) , (7.2)\nwiththecorrespondingparametergradient\n\u2207 w\u02dc J , \u03b1 (;wXy) = w+\u2207 w J , . (;wXy) (7.3)\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\nwww \u2190 \u2212 \ue00f \u03b1( +\u2207 w J , . (;wXy)) (7.4)\nWrittenanotherway,theupdateis:\nww \u2190 \u2212(1 \ue00f \u03b1)\u2212\u2207 \ue00f w J , . (;wXy) (7.5)\nWecanseethattheadditionoftheweightdecaytermhasmodi\ufb01edthelearning\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\nasinglestep.Butwhathappensovertheentirecourseoftraining?\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation\ntotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\nobtainsminimalunregularized trainingcost,w\u2217=argminw J(w).Iftheobjective\nfunctionistrulyquadratic,asinthecaseof\ufb01ttingalinearregressionmodelwith\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i \ufb01 c p o i n t i n s p a c e\na n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e \ufb00 e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e\nm o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\n2 3 1", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nmeansquarederror,thentheapproximationisperfect.Theapproximation \u02c6 Jis\ngivenby\n\u02c6 J J () = \u03b8 (w\u2217)+1\n2(ww\u2212\u2217)\ue03eHww (\u2212\u2217) , (7.6)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw\u2217.Thereis\nno\ufb01rst-orderterminthisquadraticapproximation, becausew\u2217isde\ufb01nedtobea\nminimum,wherethegradientvanishes.Likewise,becausew\u2217isthelocationofa\nminimumof,wecanconcludethatispositivesemide\ufb01nite. J H\nTheminimumof\u02c6 Joccurswhereitsgradient\n\u2207 w\u02c6 J() = (wHww\u2212\u2217) (7.7)\nisequalto. 0\nTostudythee\ufb00ectofweightdecay,wemodifyequationbyaddingthe 7.7\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\nversionof\u02c6 J.Weusethevariable \u02dcwtorepresentthelocationoftheminimum.\n\u03b1\u02dcwH+ (\u02dcww\u2212\u2217) = 0 (7.8)\n(+ )H \u03b1I\u02dcwHw = \u2217(7.9)\n\u02dcwHI = (+ \u03b1)\u2212 1Hw\u2217. (7.10)\nAs \u03b1approaches0,theregularizedsolution \u02dcwapproachesw\u2217.Butwhat\nhappensas \u03b1grows?BecauseHisrealandsymmetric,wecandecomposeit\nintoadiagonalmatrix \u039bandanorthonormal basisofeigenvectors,Q,suchthat\nHQQ = \u039b\ue03e.Applyingthedecompositiontoequation,weobtain:7.10\n\u02dcwQQ = ( \u039b\ue03e+ ) \u03b1I\u2212 1QQ \u039b\ue03ew\u2217(7.11)\n=\ue068\nQIQ (+ \u039b \u03b1)\ue03e\ue069\u2212 1\nQQ \u039b\ue03ew\u2217(7.12)\n= (+ )Q \u039b \u03b1I\u2212 1\u039bQ\ue03ew\u2217. (7.13)\nWeseethatthee\ufb00ectofweightdecayistorescalew\u2217alongtheaxesde\ufb01nedby\ntheeigenvectorsofH.Speci\ufb01cally,thecomponentofw\u2217thatisalignedwiththe\ni-theigenvectorofHisrescaledbyafactorof\u03bb i\n\u03bb i + \u03b1.(Youmaywishtoreview\nhowthiskindofscalingworks,\ufb01rstexplainedin\ufb01gure).2.3\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,\nwhere \u03bb i\ue01d \u03b1,thee\ufb00ectofregularizationisrelativelysmall.However,components\nwith \u03bb i\ue01c \u03b1willbeshrunktohavenearlyzeromagnitude.Thise\ufb00ectisillustrated\nin\ufb01gure.7.1\n2 3 2", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w\u2217\n\u02dc w\nFigure7.1:Anillustrationofthee\ufb00ectof L2(orweightdecay)regularizationonthevalue\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\nthepoint\u02dcw,thesecompetingobjectivesreachanequilibrium.Inthe\ufb01rstdimension,the\neigenvalueoftheHessianof Jissmall.\u00a0Theobjectivefunctiondoesnotincreasemuch\nwhenmovinghorizontallyawayfromw\u2217.Becausetheobjectivefunctiondoesnotexpress\nastrongpreferencealongthisdirection,theregularizerhasastronge\ufb00ectonthisaxis.\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\nisverysensitivetomovementsawayfromw\u2217.Thecorrespondingeigenvalueislarge,\nindicatinghighcurvature.Asaresult,weightdecaya\ufb00ectsthepositionof w2relatively\nlittle.\nOnlydirectionsalongwhichtheparameterscontributesigni\ufb01cantlytoreducing\ntheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\ntellsusthatmovementinthisdirectionwillnotsigni\ufb01cantlyincreasethegradient.\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\nSofarwehavediscussedweightdecayintermsofitse\ufb00ectontheoptimization\nofanabstract,general,quadraticcostfunction.Howdothesee\ufb00ectsrelateto\nmachinelearninginparticular?Wecan\ufb01ndoutbystudyinglinearregression,a\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n2 3 3", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthesumofsquarederrors:\n( )Xwy\u2212\ue03e( )Xwy\u2212 . (7.14)\nWhenweadd L2regularization, theobjectivefunctionchangesto\n( )Xwy\u2212\ue03e( )+Xwy\u22121\n2\u03b1w\ue03ew . (7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (\ue03eX)\u2212 1X\ue03ey (7.16)\nto\nwX= (\ue03eXI+ \u03b1)\u2212 1X\ue03ey . (7.17)\nThematrixX\ue03eXinequationisproportionaltothecovariancematrix 7.161\nmX\ue03eX.\nUsing L2regularizationreplacesthismatrixwith\ue000\nX\ue03eXI+ \u03b1\ue001\u2212 1inequation.7.17\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof \u03b1tothe\ndiagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\nto\u201cperceive\u201dtheinputXashavinghighervariance,whichmakesitshrinkthe\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\nthisaddedvariance.\n7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters.\u00a0Anotheroptionistouse L1\nregularization.\nFormally, L1regularizationonthemodelparameter isde\ufb01nedas:w\n\u2126() = \u03b8 ||||w 1=\ue058\ni| w i| , (7.18)\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\nnowdiscussthee\ufb00ectof L1regularizationonthesimplelinearregressionmodel,\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\nparticular,weareinterestedindelineatingthedi\ufb00erencesbetween L1and L2forms\n2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rm\u2126() = \u03b8 ||\u2212 w w( ) o|| 1=\ue050\ni| w i\u2212 w( ) o\ni| .\n2 3 4", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter \u2126 \u03b1.\nThus,theregularizedobjectivefunction \u02dc J , (;wXy)isgivenby\n\u02dc J , \u03b1 (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\nwiththecorrespondinggradient(actually,sub-gradient):\n\u2207 w\u02dc J , \u03b1 (;wXy) = sign( )+w \u2207 w J ,(Xyw;) (7.20)\nwhere issimplythesignofappliedelement-wise. sign( )w w\nByinspectingequation,wecanseeimmediately thatthee\ufb00ectof 7.20 L1\nregularizationisquitedi\ufb00erentfromthatof L2regularization. Speci\ufb01cally,wecan\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\nregularization.\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent\nviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\ninthissettingisgivenby\n\u2207 w\u02c6 J() = (wHww\u2212\u2217) , (7.21)\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww\u2217.\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.\nThisassumptionholdsifthedataforthelinearregressionproblemhasbeen\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\naccomplishedusingPCA.\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\nposesintoasumovertheparameters:\n\u02c6 J , J (;wXy) = (w\u2217; )+Xy ,\ue058\ni\ue0141\n2H i , i(w i\u2212w\u2217\ni)2+ \u03b1 w| i|\ue015\n.(7.22)\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n(foreachdimension),withthefollowingform: i\nw i= sign( w\u2217\ni)max\ue01a\n| w\u2217\ni|\u2212\u03b1\nH i , i,0\ue01b\n. (7.23)\n2 3 5", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConsiderthesituationwhere w\u2217\ni > i 0forall.Therearetwopossibleoutcomes:\n1.Thecasewhere w\u2217\ni\u2264\u03b1\nH i , i.Heretheoptimalvalueof w iundertheregularized\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\ntotheregularizedobjective\u02dc J(w;Xy ,)isoverwhelmed\u2014indirection i\u2014by\nthe L1regularizationwhichpushesthevalueof w itozero.\n2.Thecasewhere w\u2217\ni >\u03b1\nH i , i.Inthiscase,theregularizationdoesnotmovethe\noptimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\ndistanceequalto\u03b1\nH i , i.\nAsimilarprocesshappenswhen w\u2217\ni <0,butwiththe L1penaltymaking w iless\nnegativeby\u03b1\nH i , i,or0.\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\ndi\ufb00erentbehaviorthanariseswith L2regularization. Equationgavethe7.13\nsolution \u02dc wfor L2regularization. Ifwerevisitthatequationusingtheassumption\nofadiagonalandpositivede\ufb01niteHessianHthatweintroducedforouranalysisof\nL1regularization,we\ufb01ndthat\u02dc w i=H i , i\nH i , i + \u03b1w\u2217\ni.If w\u2217\niwasnonzero,then \u02dc w iremains\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\ntobecomesparse,while L1regularizationmaydosoforlargeenough. \u03b1\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively\nasafeatureselectionmechanism.Featureselectionsimpli\ufb01esamachinelearning\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\nasMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights.\u00a0For L1regu-\nlarization,thepenalty \u03b1\u2126(w)= \u03b1\ue050\ni| w i|usedtoregularizeacostfunctionis\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w\u2208 Rn:\nlog() = pw\ue058\nilogLaplace( w i;0 ,1\n\u03b1) = \u2212|||| \u03b1w 1+log log2 n \u03b1 n\u2212 .(7.24)\n2 3 6", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\nignorethe termsbecausetheydonotdependon. log log2 \u03b1\u2212 w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\n\u02dc J , J , \u03b1 . (;\u03b8Xy) = (;\u03b8Xy)+\u2126()\u03b8 (7.25)\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective\nfunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoe\ufb03cient,\ncalledaKarush\u2013Kuhn\u2013Tucker(KKT)multiplier,andafunctionrepresenting\nwhethertheconstraintissatis\ufb01ed.Ifwewantedtoconstrain\u2126(\u03b8)tobelessthan\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\nL \u2212 (; ) = (; )+(\u2126() \u03b8 , \u03b1Xy , J\u03b8Xy , \u03b1\u03b8 k .) (7.26)\nThesolutiontotheconstrainedproblemisgivenby\n\u03b8\u2217= argmin\n\u03b8max\n\u03b1 , \u03b1\u2265 0L()\u03b8 , \u03b1 . (7.27)\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 \u03b8\nand \u03b1.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\nconstraint.Manydi\ufb00erentproceduresarepossible\u2014somemayusegradientdescent,\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszero\u2014butinall\nprocedures \u03b1mustincreasewhenever\u2126(\u03b8) > kanddecreasewhenever\u2126(\u03b8) < k.\nAllpositive \u03b1encourage \u2126(\u03b8)toshrink.Theoptimalvalue \u03b1\u2217willencourage \u2126(\u03b8)\ntoshrink,butnotsostronglytomakebecomelessthan. \u2126()\u03b8 k\nTogainsomeinsightintothee\ufb00ectoftheconstraint,wecan\ufb01x \u03b1\u2217andview\ntheproblemasjustafunctionof:\u03b8\n\u03b8\u2217= argmin\n\u03b8L(\u03b8 , \u03b1\u2217) = argmin\n\u03b8J , \u03b1 (;\u03b8Xy)+\u2217\u2126()\u03b8 .(7.28)\nThisisexactlythesameastheregularizedtrainingproblemofminimizing \u02dc J.\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe \u2126 L2norm,thentheweightsareconstrainedtolieinan L2\nball.\u00a0Ifisthe \u2126 L1norm,thentheweightsareconstrainedtolieinaregionof\n2 3 7", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\nimposebyusingweightdecaywithcoe\ufb03cient \u03b1\u2217becausethevalueof \u03b1\u2217doesnot\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\nbetween kand \u03b1\u2217dependsontheformof J.Whilewedonotknowtheexactsize\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing \u03b1\ninordertogroworshrinktheconstraintregion.Larger \u03b1willresultinasmaller\nconstraintregion.Smallerwillresultinalargerconstraintregion. \u03b1\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\ndescenttotakeastepdownhillon J(\u03b8)andthenproject\u03b8backtothenearest\npointthatsatis\ufb01es\u2126(\u03b8) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof \u03b1that\ncorrespondstothis. k\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing\nconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\nprocedurestogetstuckinlocalminimacorrespondingtosmall\u03b8.Whentraining\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\n\u201cdeadunits.\u201dTheseareunitsthatdonotcontributemuchtothebehaviorofthe\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\nallverysmall.\u00a0Whentrainingwithapenaltyonthenormoftheweights,these\ncon\ufb01gurations canbelocallyoptimal,evenifitispossibletosigni\ufb01cantlyreduce\nJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\nhaveane\ufb00ectwhentheweightsbecomelargeandattempttoleavetheconstraint\nregion.\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce\nlargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\nconsistentlyincreasethesizeoftheweights,then\u03b8rapidlymovesawayfrom\ntheoriginuntilnumericalover\ufb02owoccurs.Explicitconstraintswithreprojection\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\nsomestability.\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n2 3 8", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit\nobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\nanexplicitconstraintwithreprojection.\n7.3RegularizationandUnder-ConstrainedProblems\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\nerlyde\ufb01ned.Manylinearmodelsinmachinelearning,includinglinearregression\nandPCA,dependoninvertingthematrixX\ue03eX.Thisisnotpossiblewhenever\nX\ue03eXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-\nbutiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\nX\ue03eXI+ \u03b1instead.Thisregularizedmatrixisguaranteedtobeinvertible.\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere\ntheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassi\ufb01cation,then2wwillalsoachieveperfectclassi\ufb01cationandhigherlikelihood.\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\nimplementationofgradientdescentwilleventuallyreachsu\ufb03cientlylargeweights\ntocausenumericalover\ufb02ow,atwhichpointitsbehaviorwilldependonhowthe\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.\nMostformsofregularizationareabletoguaranteetheconvergenceofiterative\nmethodsappliedtounderdetermined problems.\u00a0Forexample,weightdecaywill\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\nslopeofthelikelihoodisequaltotheweightdecaycoe\ufb03cient.\nTheideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems.\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\n2 3 9", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMoore-Penrosepseudoinverse.Recallthatonede\ufb01nitionofthepseudoinverse\nX+ofamatrixisX\nX+=lim\n\u03b1\ue026 0(X\ue03eXI+ \u03b1)\u2212 1X\ue03e. (7.29)\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\ndecay.Speci\ufb01cally,equationisthelimitofequationastheregularization 7.29 7.17\ncoe\ufb03cientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\nunderdetermined problemsusingregularization.\n7.4DatasetAugmentation\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata.\nThisapproachiseasiestforclassi\ufb01cation.Aclassi\ufb01erneedstotakeacompli-\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.\nThismeansthatthemaintaskfacingaclassi\ufb01eristobeinvarianttoawidevariety\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\ntheinputsinourtrainingset. x\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\nisdi\ufb03culttogeneratenewfakedataforadensityestimationtaskunlesswehave\nalreadysolvedthedensityestimationproblem.\nDatasetaugmentationhasbeenaparticularlye\ufb00ectivetechniqueforaspeci\ufb01c\nclassi\ufb01cationproblem:objectrecognition.Imagesarehighdimensionalandinclude\nanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\ntheimagehavealsoprovenquitee\ufb00ective.\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect\nclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\ndi\ufb00erencebetween\u2018b\u2019and\u2018d\u2019andthedi\ufb00erencebetween\u20186\u2019and\u20189\u2019,sohorizontal\n\ufb02ipsand180\u25e6rotationsarenotappropriatewaysofaugmentingdatasetsforthese\ntasks.\n2 4 0", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTherearealsotransformationsthatwewouldlikeourclassi\ufb01erstobeinvariant\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\nbeimplementedasasimplegeometricoperationontheinputpixels.\nDatasetaugmentationise\ufb00ectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,).\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\ncanalsobeseenasaformofdataaugmentation.Formanyclassi\ufb01cationand\nevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset\naugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\nthatthisapproachcanbehighlye\ufb00ectiveprovidedthatthemagnitudeofthe\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\nmultiplyingbynoise.\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\nthee\ufb00ectofdatasetaugmentationintoaccount.Often,hand-designeddataset\naugmentationschemescandramaticallyreducethegeneralization errorofamachine\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith\nnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,\nwhileoperationsthatarespeci\ufb01ctooneapplicationdomain(suchasrandomly\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\n2 4 1", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.5NoiseRobustness\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\naugmentationstrategy.Forsomemodels,theadditionofnoisewithin\ufb01nitesimal\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise\nappliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\nofthatapproach.\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,).\u00a0Thiscan\nbeinterpretedasa\u00a0stochasticimplementation of\u00a0Bayesianinference\u00a0overthe\nweights.\u00a0TheBayesiantreatmentoflearningwouldconsiderthemodelweights\ntobeuncertainandrepresentableviaaprobabilitydistributionthatre\ufb02ectsthis\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytore\ufb02ect\nthisuncertainty.\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\nafunction \u02c6 y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares\ncostfunctionbetweenthemodelpredictions \u02c6 y()xandthetruevalues: y\nJ= E p x , y ( )\ue002(\u02c6 y y ()x\u2212)2\ue003\n. (7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\nWenowassumethatwitheachinputpresentationwealsoincludearandom\nperturbation \ue00f W\u223cN(\ue00f; 0 , \u03b7I)ofthenetworkweights.Letusimaginethatwe\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelas\u02c6 y \ue00f W(x).Despite\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe\noutputofthenetwork.Theobjectivefunctionthusbecomes:\n\u02dc J W= E p , y , ( x \ue00f W )\ue068\n(\u02c6 y \ue00f W() )x\u2212 y2\ue069\n(7.31)\n= E p , y , ( x \ue00f W )\ue002\n\u02c6 y2\n\ue00f W()2\u02c6x\u2212 y y \ue00f W()+x y2\ue003\n.(7.32)\nForsmall \u03b7,theminimization of Jwithaddedweightnoise(withcovariance\n\u03b7I)isequivalenttominimization of Jwithanadditionalregularizationterm:\n2 4 2", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\u03b7 E p , y ( x )\ue002\ue06b\u2207 W\u02c6 y()x\ue06b2\ue003\n.Thisformofregularizationencouragestheparametersto\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\narelativelysmallin\ufb02uenceontheoutput.Inotherwords,itpushesthemodel\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\nweights,\ufb01ndingpointsthatarenotmerelyminima,butminimasurroundedby\n\ufb02atregions(HochreiterandSchmidhuber1995,).Inthesimpli\ufb01edcaseoflinear\nregression(where,forinstance, \u02c6 y(x) =w\ue03ex+ b),thisregularizationtermcollapses\ninto \u03b7 E p ( ) x\ue002\n\ue06b\ue06bx2\ue003\n,whichisnotafunctionofparametersandthereforedoesnot\ncontributetothegradientof\u02dc J Wwithrespecttothemodelparameters.\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall\nconstant \ue00f,thetrainingsetlabel yiscorrectwithprobability 1\u2212 \ue00f,andotherwise\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\nsoftmaxwith koutputvaluesbyreplacingthehardandclassi\ufb01cationtargets 0 1\nwithtargetsof\ue00f\nk\u2212 1and1\u2212 \ue00f,respectively.Thestandardcross-entropylossmay\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax\nclassi\ufb01erandhardtargetsmayactuallyneverconverge\u2014thesoftmaxcannever\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\ndiscouragingcorrectclassi\ufb01cation.Thisstrategyhasbeenusedsincethe1980s\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy\netal.,).2015\n7.6Semi-SupervisedLearning\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\nx.\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\n2 4 3", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\nsimilarrepresentations.Alinearclassi\ufb01erinthenewspacemayachievebetter\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents\nanalysisasapre-processingstepbeforeapplyingaclassi\ufb01er(ontheprojected\ndata).\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\nthentrade-o\ufb00thesupervisedcriterion \u2212log P( y x|)withtheunsupervisedor\ngenerativeone(suchas\u2212log P( x)or\u2212log P( x y ,)).Thegenerativecriterionthen\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised\nlearningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\ninthetotalcriterion,onecan\ufb01ndabettertrade-o\ufb00thanwithapurelygenerative\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\nBengio2008,).\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel\nfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesigni\ufb01cantly. P() x P( ) y x|\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\n7.7Multi-TaskLearning\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\narisingoutofseveraltasks.\u00a0Inthesamewaythatadditionaltrainingexamples\nputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\nconstrainedtowardsgoodvalues(assumingthesharingisjusti\ufb01ed),oftenyielding\nbettergeneralization.\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\ndi\ufb00erentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\n2 4 4", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\nparameters:\n1.Task-speci\ufb01cparameters(whichonlybene\ufb01tfromtheexamplesoftheirtask\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\nnetworkin\ufb01gure.7.2\n2.Genericparameters,sharedacrossallthetasks(whichbene\ufb01tfromthe\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\nin\ufb01gure.7.2\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )\nh( s h a r e d )h( s h a r e d )\nxx\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\nandthis\ufb01gureillustratesthecommonsituationwherethetasksshareacommoninputbut\ninvolvedi\ufb00erenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\ncanbesharedacrosssuchtasks,whiletask-speci\ufb01cparameters(associatedrespectively\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga\nsharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe\nassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n2 4 5", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n0 50 100 150 200 250\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\nV a l i d a t i o n s e t l o s s\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto\nincreaseagain,forminganasymmetricU-shapedcurve.\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\nthedi\ufb00erenttasksarevalid,meaningthatthereissomethingsharedacrosssome\nofthetasks.\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\nfollowing:amongthefactorsthat\u00a0explainthevariations\u00a0observed\u00a0inthedata\nassociatedwiththedi\ufb00erenttasks,somearesharedacrosstwoormoretasks.\n7.8EarlyStopping\nWhentraininglargemodelswithsu\ufb03cientrepresentationalcapacitytoover\ufb01t\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\nvalidationseterrorbeginstoriseagain.See\ufb01gureforanexampleofthis 7.3\nbehavior.Thisbehavioroccursveryreliably.\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin\ntimewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\n2 4 6", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\nvalidationerrorforsomepre-speci\ufb01ednumberofiterations.Thisprocedureis\nspeci\ufb01edmoreformallyinalgorithm .7.1\nAlgorithm\u00a07.1Theearlystopping\u00a0meta-algorithmfor\u00a0determiningthe\u00a0best\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\nvalidationset.\nLetbethenumberofstepsbetweenevaluations. n\nLet pbethe\u201cpatience,\u201dthenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup.\nLet\u03b8 obetheinitialparameters.\n\u03b8\u03b8\u2190 o\ni\u21900\nj\u21900\nv\u2190\u221e\n\u03b8\u2217\u2190\u03b8\ni\u2217\u2190 i\nwhiledo j < p\nUpdatebyrunningthetrainingalgorithmforsteps. \u03b8 n\ni i n \u2190+\nv\ue030\u2190ValidationSetError ()\u03b8\nif v\ue030< vthen\nj\u21900\n\u03b8\u2217\u2190\u03b8\ni\u2217\u2190 i\nv v\u2190\ue030\nelse\nj j\u2190+1\nendif\nendwhile\nBestparametersare\u03b8\u2217,bestnumberoftrainingstepsis i\u2217\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly\nusedformofregularizationindeeplearning.Itspopularityisduebothtoits\ne\ufb00ectivenessanditssimplicity.\nOnewaytothinkofearlystoppingisasaverye\ufb03cienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\nWecanseein\ufb01gurethatthishyperparameter hasaU-shapedvalidationset 7.3\n2 4 7", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\nU-shapedvalidationsetperformancecurve,asillustratedin\ufb01gure.Inthecaseof 5.3\nearlystopping,wearecontrollingthee\ufb00ectivecapacityofthemodelbydetermining\nhowmanystepsitcantaketo\ufb01tthetrainingset.Mosthyperparametersmustbe\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\natthestartoftraining,thenruntrainingforseveralstepstoseeitse\ufb00ect.The\n\u201ctrainingtime\u201d\u00a0hyperparam eterisuniqueinthatbyde\ufb01nitionasinglerunof\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigni\ufb01cantcost\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis\nsmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring\ntraining,theseoccasionalslowwriteshavelittlee\ufb00ectonthetotaltrainingtime.\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\nsmallweights.\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\noccuratalocalminimumofthetrainingobjective.\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra\ntrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\nonecanuseforthissecondtrainingprocedure.\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\n2 4 8", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\ntheearlystoppingproceduredeterminedwasoptimalinthe\ufb01rstpass.Thereare\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\ntrainingsetisbigger.\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\ntotrain,thenretrainingonallthedata.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 \u03b8usingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nreturns i\u2217,theoptimalnumberofsteps.\nSettorandomvaluesagain. \u03b8\nTrainonX( ) t r a i nandy( ) t r a i nfor i\u2217steps.\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\nfromthe\ufb01rstroundoftrainingandthencontinuetrainingbutnowusingallof\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\nofanumberofsteps.\u00a0Instead,wecanmonitortheaveragelossfunctiononthe\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining\nsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\nThisprocedureispresentedmoreformallyinalgorithm .7.3\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber\noftrainingiterations,italsohasthebene\ufb01tofprovidingregularizationwithout\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\nthegradientsofsuchadditionalterms.\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n2 4 9", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\ntivevaluewestarttoover\ufb01t,thencontinuetraininguntilthatvalueisreached.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 \u03b8usingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nupdates.\u03b8\n\ue00f J , \u2190(\u03b8X( ) s ubtr a i n,y( ) s ubtr a i n)\nwhile J ,(\u03b8X( v a l i d ),y( v a l i d )) > \ue00fdo\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\nendwhile\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n()and ()arguedthatearlystoppinghasthee\ufb00ectof 1995aSj\u00f6bergandLjung1995\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter\nspaceintheneighborhoodoftheinitialparametervalue\u03b8 o,asillustratedin\n\ufb01gure.Morespeci\ufb01cally,imaginetaking 7.4 \u03c4optimization steps(corresponding\nto \u03c4trainingiterations)andwithlearningrate \ue00f.Wecanviewtheproduct \ue00f \u03c4\nasameasureofe\ufb00ectivecapacity.Assumingthegradientisbounded,restricting\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\nspacereachablefrom\u03b8 o.Inthissense, \ue00f \u03c4behavesasifitwerethereciprocalof\nthecoe\ufb03cientusedforweightdecay.\nIndeed,wecanshowhow\u2014inthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescent\u2014earlystoppingisequivalentto L2\nregularization.\nInordertocomparewithclassical L2regularization, weexamineasimple\nsettingwheretheonlyparametersarelinearweights(\u03b8=w).Wecanmodel\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\nempiricallyoptimalvalueoftheweightsw\u2217:\n\u02c6 J J () = \u03b8 (w\u2217)+1\n2(ww\u2212\u2217)\ue03eHww (\u2212\u2217) , (7.33)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw\u2217.Giventhe\nassumptionthatw\u2217isaminimumof J(w),weknowthatHispositivesemide\ufb01nite.\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\n\u2207 w\u02c6 J() = (wHww\u2212\u2217) . (7.34)\n2 5 0", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w\u2217\n\u02dc w\nw 1w 2w\u2217\n\u02dc w\nFigure7.4:Anillustrationofthee\ufb00ectofearlystopping. ( L e f t )Thesolidcontourlines\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw\u2217that\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint\u02dcw.\n( R i g h t )Anillustrationofthee\ufb00ectof L2regularizationforcomparison.Thedashedcircles\nindicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\nnearertheoriginthantheminimumoftheunregularizedcost.\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\nanalyzinggradientdescenton\u02c6 J:\nw( ) \u03c4= w( 1 ) \u03c4\u2212\u2212\u2207 \ue00f w\u02c6 J(w( 1 ) \u03c4\u2212) (7.35)\n= w( 1 ) \u03c4\u2212\u2212 \ue00fHw(( 1 ) \u03c4\u2212\u2212w\u2217) (7.36)\nw( ) \u03c4\u2212w\u2217= ( )(IH\u2212 \ue00fw( 1 ) \u03c4\u2212\u2212w\u2217) . (7.37)\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\ntheeigendecompositionofH:H=QQ \u039b\ue03e,where \u039bisadiagonalmatrixandQ\nisanorthonormalbasisofeigenvectors.\nw( ) \u03c4\u2212w\u2217= (IQQ \u2212 \ue00f \u039b\ue03e)(w( 1 ) \u03c4\u2212\u2212w\u2217)(7.38)\nQ\ue03e(w( ) \u03c4\u2212w\u2217) = ( )I\u2212 \ue00f \u039bQ\ue03e(w( 1 ) \u03c4\u2212\u2212w\u2217) (7.39)\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e\na l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\ni n i t i a l v a l u e w( 0 ).\n2 5 1", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAssumingthatw( 0 )=0andthat \ue00fischosentobesmallenoughtoguarantee\n|1\u2212 \ue00f \u03bb i| <1,theparametertrajectoryduringtrainingafter \u03c4parameterupdates\nisasfollows:\nQ\ue03ew( ) \u03c4= [ ( )I\u2212I\u2212 \ue00f \u039b\u03c4]Q\ue03ew\u2217. (7.40)\nNow,theexpressionforQ\ue03e\u02dcwinequationfor7.13 L2regularizationcanberear-\nrangedas:\nQ\ue03e\u02dcwI = (+ \u039b \u03b1)\u2212 1\u039bQ\ue03ew\u2217(7.41)\nQ\ue03e\u02dcwII = [\u2212(+ \u039b \u03b1)\u2212 1\u03b1]Q\ue03ew\u2217(7.42)\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 \ue00f,\n\u03b1 \u03c4,andarechosensuchthat\n( )I\u2212 \ue00f \u039b\u03c4= (+ ) \u039b \u03b1I\u2212 1\u03b1 , (7.43)\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\nthatifall \u03bb iaresmall(thatis, \ue00f \u03bb i\ue01c1and \u03bb i /\u03b1\ue01c1)then\n\u03c4\u22481\n\ue00f \u03b1, (7.44)\n\u03b1\u22481\n\u03c4 \ue00f. (7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations \u03c4playsarole\ninverselyproportionaltothe L2regularizationparameter,andtheinverseof \u03c4 \ue00f\nplaystheroleoftheweightdecaycoe\ufb03cient.\nParametervaluescorrespondingtodirectionsofsigni\ufb01cantcurvature(ofthe\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\ntodirectionsofsigni\ufb01cantcurvaturetendtolearnearlyrelativetoparameters\ncorrespondingtodirectionsoflesscurvature.\nThederivationsinthissectionhaveshownthatatrajectoryoflength \u03c4ends\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically\ndeterminesthecorrectamountofregularizationwhileweightdecayrequiresmany\ntrainingexperimentswithdi\ufb00erentvaluesofitshyperparameter.\n2 5 2", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.9ParameterTyingandParameterSharing\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\ntotheparameters,wehavealwaysdonesowithrespecttoa\ufb01xedregionorpoint.\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\ndeviatingfromthe\ufb01xedvalueofzero.However,sometimeswemayneedother\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake\nbutweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\nshouldbesomedependencies betweenthemodelparameters.\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\nhavetwomodelsperformingthesameclassi\ufb01cationtask(withthesamesetof\nclasses)butwithsomewhatdi\ufb00erentinputdistributions.Formally,wehavemodel\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels\nmaptheinput\u00a0totwo\u00a0di\ufb00erent,\u00a0but\u00a0related outputs:\u02c6 y( ) A= f(w( ) A,x)and\n\u02c6 y( ) B= ( gw( ) B,x).\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\ntoeachother: \u2200 i, w( ) A\nishouldbecloseto w( ) B\ni.Wecanleveragethisinformation\nthroughregularization. Speci\ufb01cally,wecanuseaparameternormpenaltyofthe\nform: \u2126(w( ) A,w( ) B)=\ue06bw( ) A\u2212w( ) B\ue06b2\n2.\u00a0Hereweusedan L2penalty,butother\nchoicesarealsopossible.\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\ntheparametersofonemodel,trainedasaclassi\ufb01erinasupervisedparadigm,to\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\nconstructedsuchthatmanyoftheparametersintheclassi\ufb01ermodelcouldbe\npairedtocorrespondingparametersintheunsupervisedmodel.\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe\nclosetooneanother,themorepopularwayistouseconstraints:toforcesets\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\nassharingauniquesetofparameters.Asigni\ufb01cantadvantageofparametersharing\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\nmodels\u2014suchastheconvolutionalneuralnetwork\u2014thiscanleadtosigni\ufb01cant\nreductioninthememoryfootprintofthemodel.\n2 5 3", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\ntocomputervision.\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)\niscomputedoverdi\ufb00erentlocationsintheinput.Thismeansthatwecan\ufb01nda\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\ni+1intheimage.\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\nmodelparametersandtosigni\ufb01cantlyincreasenetworksizeswithoutrequiringa\ncorrespondingincreaseintrainingdata.\u00a0Itremainsoneofthebestexamplesof\nhowtoe\ufb00ectivelyincorporatedomainknowledgeintothenetworkarchitecture.\nCNNswillbediscussedinmoredetailinchapter.9\n7.10SparseRepresentations\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\npenaltyonthemodelparameters.\nWehave\u00a0alreadydiscussed\u00a0(insection)how7.1.2 L1penalizationinduces\nasparseparametrization\u2014meaning thatmanyoftheparametersbecomezero\n(orcloseto\u00a0zero).Representationalsparsity,\u00a0on\u00a0theother\u00a0hand,\u00a0des cribesa\nrepresentationwheremanyoftheelementsoftherepresentationarezero(orclose\ntozero).Asimpli\ufb01edviewofthisdistinctioncanbeillustratedinthecontextof\nlinearregression:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f018\n5\n15\n\u22129\n\u22123\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0400 20 0 \u2212\n00 10 3 0 \u2212\n050 0 0 0\n100 10 4 \u2212 \u2212\n100 0 50 \u2212\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f02\n3\n\u22122\n\u22125\n1\n4\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ny\u2208 RmA\u2208 Rm n\u00d7x\u2208 Rn(7.46)\n2 5 4", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\u221214\n1\n19\n2\n23\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f03 12 54 1 \u2212 \u2212\n4 2 3 11 3 \u2212 \u2212\n\u2212 \u2212 \u2212 15 4 2 3 2\n3 1 2 30 3 \u2212 \u2212\n\u2212 \u2212 \u2212 \u2212 54 22 5 1\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f00\n2\n0\n0\n\u22123\n0\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ny\u2208 RmB\u2208 Rm n\u00d7h\u2208 Rn(7.47)\nInthe\ufb01rstexpression,wehaveanexampleofasparselyparametrized linear\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\ntheinformationpresentin,butdoessowithasparsevector. x\nRepresentationalregularizationisaccomplishedbythesamesortsofmechanisms\nthatwehaveusedinparameterregularization.\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\n\u2126()h.Asbefore,wedenotetheregularizedlossfunctionby\u02dc J:\n\u02dc J , J , \u03b1 (;\u03b8Xy) = (;\u03b8Xy)+\u2126()h (7.48)\nwhere \u03b1\u2208[0 ,\u221e)weightstherelativecontributionofthenormpenaltyterm,with\nlargervaluesofcorrespondingtomoreregularization. \u03b1\nJustasan L1penaltyontheparametersinducesparametersparsity,an L1\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\n\u2126(h) =||||h 1=\ue050\ni| h i|.\u00a0Ofcourse,the L1penaltyisonlyonechoiceofpenalty\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.\nLee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\nm\ue050\nih( ) i,to\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\noptimization problem\nargmin\nh h ,\ue06b\ue06b 0 < k\ue06b\u2212 \ue06bxWh2, (7.49)\nwhere \ue06b\ue06bh 0isthenumberofnon-zeroentriesofh.\u00a0Thisproblemcanbesolved\ne\ufb03cientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\n2 5 5", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nOMP- kwiththevalueof kspeci\ufb01edtoindicatethenumberofnon-zerofeatures\nallowed. ()demonstratedthatOMP-canbeaverye\ufb00ective CoatesandNg2011 1\nfeatureextractorfordeeparchitectures.\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts.\n7.11BaggingandOtherEnsembleMethods\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-\neralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\ntrainseveraldi\ufb00erentmodelsseparately,thenhaveallofthemodelsvoteonthe\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\nasensemblemethods.\nThereasonthatmodelaveragingworksisthatdi\ufb00erentmodelswillusually\nnotmakeallthesameerrorsonthetestset.\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel\nmakesanerror \ue00f ioneachexample,\u00a0withtheerrorsdrawnfromazero-mean\nmultivariatenormaldistributionwithvariances E[ \ue00f2\ni] = vandcovariances E[ \ue00f i \ue00f j] =\nc.\u00a0Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n1\nk\ue050\ni \ue00f i.Theexpectedsquarederroroftheensemblepredictoris\nE\uf8ee\n\uf8f0\ue020\n1\nk\ue058\ni\ue00f i\ue0212\uf8f9\n\uf8fb=1\nk2E\uf8ee\n\uf8f0\ue058\ni\uf8eb\n\uf8ed \ue00f2\ni+\ue058\nj i\ue036=\ue00f i \ue00f j\uf8f6\n\uf8f8\uf8f9\n\uf8fb(7.50)\n=1\nkv+k\u22121\nkc . (7.51)\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared\nerrorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\nensembleisonly1\nkv.Thismeansthattheexpectedsquarederroroftheensemble\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\nindependenterrors,theensemblewillperformsigni\ufb01cantlybetterthanitsmembers.\nDi\ufb00erentensemblemethodsconstructtheensembleofmodelsindi\ufb00erentways.\nForexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n2 5 6", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n8\n8F i r s t \u00a0 e nse m b l e \u00a0 m e m b e r\nSe c ond\u00a0e nse m b l e \u00a0 m e m b e rO r i gi nal \u00a0 data s e t\nF i r s t \u00a0 r e s am pl e d \u00a0 d a t a s e t\nSe c ond\u00a0re s am p l e d \u00a0 d a t a s e t\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodi\ufb00erent\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets\nbysamplingwithreplacement.The\ufb01rstdatasetomitsthe9andrepeatsthe8.Onthis\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\nclassi\ufb01cationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\nachievingmaximalcon\ufb01denceonlywhenbothloopsofthe8arepresent.\ndi\ufb00erentkindofmodelusingadi\ufb00erentalgorithmorobjectivefunction.Bagging\nisamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\nfunctiontobereusedseveraltimes.\nSpeci\ufb01cally,bagginginvolvesconstructing kdi\ufb00erentdatasets.Eachdataset\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound\n2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\ni.Thedi\ufb00erencesbetweenwhichexamplesareincludedineachdatasetresultin\ndi\ufb00erencesbetweenthetrainedmodels.See\ufb01gureforanexample.7.5\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\noftenbene\ufb01tfrommodelaveragingevenifallofthemodelsaretrainedonthesame\ndataset.Di\ufb00erencesinrandominitialization, randomselectionofminibatches,\ndi\ufb00erencesinhyperparameters,ordi\ufb00erentoutcomesofnon-determinis ticimple-\nmentationsofneuralnetworksareoftenenoughtocausedi\ufb00erentmembersofthe\n2 5 7", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nensembletomakepartiallyindependenterrors.\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\nforscienti\ufb01cpapers,becauseanymachinelearningalgorithmcanbene\ufb01tsubstan-\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-\ningoverdozensofmodels.ArecentprominentexampleistheNet\ufb02ixGrand\nPrize(Koren2009,).\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual\nneuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\nunitstotheneuralnetwork.\n7.12Dropout\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\npowerfulmethodofregularizingabroadfamilyofmodels.Toa\ufb01rstapproximation,\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical\nwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\nof\ufb01vetotenneuralnetworks\u2014 ()usedsixtowintheILSVRC\u2014 Szegedy etal.2014a\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\nneuralnetworks.\nSpeci\ufb01cally,dropouttrainstheensembleconsistingofallsub-networksthat\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,\nasillustratedin\ufb01gure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\na\ufb03netransformationsandnonlinearities, wecane\ufb00ectivelyremoveaunitfroma\nnetworkbymultiplyingitsoutputvaluebyzero.\u00a0Thisprocedurerequiressome\nslightmodi\ufb01cationformodelssuchasradialbasisfunctionnetworks,whichtake\n2 5 8", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthedi\ufb00erencebetweentheunit\u2019sstateandsomereferencevalue.Here,wepresent\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\nbetriviallymodi\ufb01edtoworkwithotheroperationsthatremoveaunitfromthe\nnetwork.\nRecallthattolearnwithbagging,wede\ufb01ne kdi\ufb00erentmodels,construct k\ndi\ufb00erentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan\nexponentiallylargenumberofneuralnetworks.Speci\ufb01cally,totrainwithdropout,\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\nrandomlysampleadi\ufb00erentbinarymasktoapplytoalloftheinputandhidden\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\nincluded)isahyperparameter \ufb01xedbeforetrainingbegins.\u00a0Itisnotafunction\nofthecurrentvalueofthemodelparametersortheinputexample.Typically,\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\nwithdropout.\nMoreformally,supposethatamaskvector\u00b5speci\ufb01eswhichunitstoinclude,\nand J(\u03b8\u00b5 ,)de\ufb01nesthecostofthemodelde\ufb01nedbyparameters\u03b8andmask\u00b5.\nThendropouttrainingconsistsinminimizing E \u00b5 J(\u03b8\u00b5 ,).Theexpectationcontains\nexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\nbysamplingvaluesof.\u00b5\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\nparameters,witheachmodelinheritingadi\ufb00erentsubsetofparametersfromthe\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe\ncaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall\u2014usually,\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\naretheonlydi\ufb00erences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof\ntheoriginaltrainingsetsampledwithreplacement.\n2 5 9", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1yy\nh 2 h 2\nx 1 x 1 x 2 x 2\nyy\nh 1 h 1\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2yy\nx 1 x 1 x 2 x 2yy\nh 2 h 2\nx 2 x 2\nyy\nh 1 h 1\nx 1 x 1yy\nh 1 h 1\nx 2 x 2yy\nh 2 h 2\nx 1 x 1yy\nx 1 x 1\nyy\nx 2 x 2yy\nh 2 h 2yy\nh 1 h 1yyB ase \u00a0 ne t w or k\nE nse m bl e \u00a0 of \u00a0 s u b n e t w or k s\nFigure\u00a07.6:Dropout\u00a0trainsan\u00a0ensemble\u00a0consistingof\u00a0allsub-networks\u00a0that\u00a0canbe\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\nbydroppingoutdi\ufb00erentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting\ntheinputtotheoutput.Thisproblembecomesinsigni\ufb01cantfornetworkswithwider\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\nsmaller.\n2 6 0", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\u02c6 x 1\u02c6 x 1\n\u00b5 x 1 \u00b5 x 1 x 1 x 1\u02c6 x 2\u02c6 x 2\nx 2 x 2 \u00b5 x 2 \u00b5 x 2h 1 h 1 h 2 h 2\u00b5 h 1 \u00b5 h 1 \u00b5 h 2 \u00b5 h 2\u02c6 h 1\u02c6 h 1\u02c6 h 2\u02c6 h 2yyyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\npropagationwithdropout,werandomlysampleavector\u00b5withoneentryforeachinput\norhiddenunitinthenetwork.Theentriesof\u00b5arebinaryandaresampledindependently\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\n\ufb01gureandrunningforwardpropagationthroughit. 7.6\n2 6 1", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\nitsmembers.Werefertothisprocessasinferenceinthiscontext.\u00a0Sofar,our\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\nprobabilistic.Now,weassumethatthemodel\u2019sroleistooutputaprobability\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\nofthesedistributions,\n1\nkk\ue058\ni = 1p( ) i( ) y|x . (7.52)\nInthecaseofdropout,eachsub-modelde\ufb01nedbymaskvector\u00b5de\ufb01nesaprob-\nabilitydistribution p( y ,|x\u00b5).Thearithmeticmeanoverallmasksisgiven\nby\ue058\n\u00b5p p y , ()\u00b5(|x\u00b5) (7.53)\nwhere p(\u00b5)istheprobabilitydistributionthatwasusedtosample\u00b5attraining\ntime.\nBecausethissumincludesanexponentialnumberofterms,itisintractable\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\nofsimpli\ufb01cation.Sofar,deepneuralnetsarenotknowntopermitanytractable\nsimpli\ufb01cation.Instead,\u00a0wecan\u00a0approximatetheinferencewithsampling,\u00a0by\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\nsu\ufb03cienttoobtaingoodperformance.\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\nthearithmeticmeanoftheensemblemembers\u2019predicteddistributions.Warde-\nFarley2014etal.()presentargumentsandempiricalevidencethatthegeometric\nmeanperformscomparablytothearithmeticmeaninthiscontext.\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\ndistributionde\ufb01neddirectlybythegeometricmeanisgivenby\n\u02dc p e nse m bl e( ) = y|x 2d\ue073\ue059\n\u00b5p y , (|x\u00b5) (7.54)\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\ndistributionover\u00b5tosimplifythepresentation,butnon-uniformdistributionsare\n2 6 2", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\np e nse m bl e( ) = y|x\u02dc p e nse m bl e( ) y|x\ue050\ny\ue030\u02dc p e nse m bl e( y\ue030|x). (7.55)\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\ni.Themotivationforthismodi\ufb01cationistocapturetherightexpectedvalueofthe\noutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\nBecauseweusuallyuseaninclusionprobabilityof1\n2,theweightscalingrule\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2\ntheexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\nmissingonaverage.\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\nclassi\ufb01erwithinputvariablesrepresentedbythevector: n v\nP y (= y | v) = softmax\ue010\nW\ue03ev+b\ue011\ny. (7.56)\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\ninputwithabinaryvector: d\nP y (= y | v;) = dsoftmax\ue010\nW\ue03e( )+d\ue00c vb\ue011\ny.(7.57)\nTheensemblepredictorisde\ufb01nedbyre-normalizingthegeometricmeanoverall\nensemblemembers\u2019predictions:\nP e nse m bl e(= ) =y y| v\u02dc P e nse m bl e(= )y y| v\ue050\ny\ue030\u02dc P e nse m bl e(= y y\ue030| v)(7.58)\nwhere\n\u02dc P e nse m bl e(= ) =y y| v2n\ue073\ue059\nd\u2208{} 0 1 ,nP y . (= y | v;)d (7.59)\n2 6 3", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nToseethattheweightscalingruleisexact,wecansimplify \u02dc P e nse m bl e:\n\u02dc P e nse m bl e(= ) =y y| v2n\ue073\ue059\nd\u2208{} 0 1 ,nP y (= y | v;)d(7.60)\n= 2n\ue073\ue059\nd\u2208{} 0 1 ,nsoftmax (W\ue03e( )+)d\ue00c vby (7.61)\n= 2n\ue076\ue075\ue075\ue074\ue059\nd\u2208{} 0 1 ,nexp\ue000\nW\ue03ey , :( )+d\ue00c v b y\ue001\n\ue050\ny\ue030exp\ue010\nW\ue03e\ny\ue030 , :( )+d\ue00c v b y\ue030\ue011 (7.62)\n=2n\ue071\ue051\nd\u2208{} 0 1 ,nexp\ue000\nW\ue03ey , :( )+d\ue00c v b y\ue001\n2n\ue072\ue051\nd\u2208{} 0 1 ,n\ue050\ny\ue030exp\ue010\nW\ue03e\ny\ue030 , :( )+d\ue00c v b y\ue030\ue011(7.63)\nBecause\u02dc Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat\nareconstantwithrespectto: y\n\u02dc P e nse m bl e(= ) y y| v\u221d2n\ue073\ue059\nd\u2208{} 0 1 ,nexp\ue000\nW\ue03ey , :( )+d\ue00c v b y\ue001\n(7.64)\n= exp\uf8eb\n\uf8ed1\n2n\ue058\nd\u2208{} 0 1 ,nW\ue03e\ny , :( )+d\ue00c v b y\uf8f6\n\uf8f8 (7.65)\n= exp\ue0121\n2W\ue03e\ny , : v+ b y\ue013\n. (7.66)\nSubstitutingthisbackintoequationweobtainasoftmaxclassi\ufb01erwithweights 7.58\n1\n2W.\nTheweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-\nmationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\nbetter(intermsofclassi\ufb01cationaccuracy)thanMonteCarloapproximations tothe\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\nthatsomemodelsobtainbetterclassi\ufb01cationaccuracyusingtwentysamplesand\n2 6 4", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\napproximationisproblem-dependent.\nSrivastava2014etal.()showedthatdropoutismoree\ufb00ectivethanother\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,\ufb01lter\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\nwithotherformsofregularizationtoyieldafurtherimprovement.\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using\ndropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\nexamples.\nAnothersigni\ufb01cantadvantageofdropoutisthatitdoesnotsigni\ufb01cantlylimit\nthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\nthearchitectureofthemodel.\nThoughthecostper-stepofapplyingdropouttoaspeci\ufb01cmodelisnegligible,\nthecostofusingdropoutinacompletesystemcanbesigni\ufb01cant.Becausedropout\nisaregularizationtechnique,itreducesthee\ufb00ectivecapacityofamodel.Too\ufb00set\nthise\ufb00ect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge\ndatasets,regularizationconferslittlereductioningeneralization error.\u00a0Inthese\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\nthebene\ufb01tofregularization.\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\ne\ufb00ective.Bayesian\u00a0neuralnetworks(,\u00a0)outperform\u00a0dropout\u00a0onthe Neal1996\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,\nunsupervisedfeaturelearningcangainanadvantageoverdropout.\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\nisequivalentto L2weightdecay,withadi\ufb00erentweightdecaycoe\ufb03cientfor\n2 6 5", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\neachinputfeature.Themagnitudeofeachfeature\u2019sweightdecaycoe\ufb03cientis\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\nmodels,dropoutisnotequivalenttoweightdecay.\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\napproach\u2019ssuccess.Itisjustameansofapproximating thesumoverallsub-\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster\nconvergencetimeduetothereducedstochasticityinthecomputationofthe\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\n(butalsomorecomputationally expensive)approximation totheaverageoverall\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\nproblems,buthasnotyetyieldedasigni\ufb01cantimprovementorbeenappliedtoa\nlargeproblem.\nJustasstochasticityisnotnecessarytoachievetheregularizing\u00a0e\ufb00ect of\ndropout,itisalsonotsu\ufb03cient.Todemonstratethis,Warde-Farley2014etal.()\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\nitsregularizinge\ufb00ect.Dropoutboostingtrainstheentireensembletojointly\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\ndropoutisanalogoustobagging,\u00a0this approachisanalogoustoboosting.As\nintended,experimentswithdropoutboostingshowalmostnoregularizatione\ufb00ect\ncomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\ndropoutasrobustnesstonoise.Theregularizatione\ufb00ectofthebaggedensembleis\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\nperformwellindependently ofeachother.\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights.\u00a0DropConnectisaspecialcaseof\ndropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodi\ufb00erent\nspatiallocationsofeachfeaturemap.\u00a0Sofar,dropoutremainsthemostwidely\nusedimplicitensemblemethod.\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic\nbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\nimplementsaformofbaggingwithparametersharing.Earlier,\u00a0wedescribed\n2 6 6", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ndropoutas\u00a0bagginganensembleofmodelsformedbyincludingor\u00a0excluding\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\ninclusionandexclusion.Inprinciple,anykindofrandommodi\ufb01cationisadmissible.\nInpractice,wemustchoosemodi\ufb01cationfamiliesthatneuralnetworksareable\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\napproximateinferencerule.Wecanthinkofanyformofmodi\ufb01cationparametrized\nbyavector\u00b5astraininganensembleconsistingof p( y ,|x\u00b5)forallpossible\nvaluesof\u00b5.Thereisnorequirementthat\u00b5havea\ufb01nitenumberofvalues.For\nexample,\u00b5canbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\nweightsby\u00b5\u223cN( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\nE[\u00b5] = 1thestandardnetworkautomatically implementsapproximate inference\nintheensemble,withoutneedinganyweightscaling.\nSofarwehavedescribeddropoutpurelyasameansofperforminge\ufb03cient,\napproximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\nswappinggenesbetweentwodi\ufb00erentorganisms,createsevolutionarypressurefor\ngenestobecomenotjustgood,buttobecomereadilyswappedbetweendi\ufb00erent\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.\u00a0Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropouto\ufb00ersadditionalimprovementstogeneralization error\nbeyondthoseobtainedbyensemblesofindependentmodels.\nItisimportanttounderstandthatalargeportionofthepowerofdropout\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\nexample,ifthemodellearnsahiddenunit h ithatdetectsafaceby\ufb01ndingthenose,\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein\ntheimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\n2 6 7", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\ndistributionthatthemodelhasacquiredsofar.\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\nnoisewereadditivewith\ufb01xedscale,thenarecti\ufb01edlinearhiddenunit h iwith\naddednoise \ue00fcouldsimplylearntohave h ibecomeverylargeinordertomake\ntheaddednoise \ue00finsigni\ufb01cantbycomparison.Multiplicativenoisedoesnotallow\nsuchapathologicalsolutiontothenoiserobustnessproblem.\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\noptimization, butthenoisecanhavearegularizinge\ufb00ect,andsometimesmakes\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\n7.13AdversarialTraining\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen\nevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\nsearchforexamplesthatthemodelmisclassi\ufb01es. ()foundthat Szegedy etal.2014b\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\nproceduretosearchforaninputx\ue030nearadatapointxsuchthatthemodel\noutputisverydi\ufb00erentatx\ue030.Inmanycases,x\ue030canbesosimilartoxthata\nhumanobservercannottellthedi\ufb00erencebetweentheoriginalexampleandthe\nadversarialexample,butthenetworkcanmakehighlydi\ufb00erentpredictions.See\n\ufb01gureforanexample.7.8\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\nthatarebeyondthescopeofthischapter.\u00a0However,theyareinterestinginthe\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.\ntestsetviaadversarialtraining\u2014trainingonadversariallyperturbedexamples\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarial\u00a0examplesis\u00a0excessive\u00a0linearity.Neural\u00a0networks\u00a0arebuilt\u00a0out\u00a0of\nprimarilylinearbuildingblocks.\u00a0Insomeexperimentstheoverallfunctionthey\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n2 6 8", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n+ .007\u00d7 =\nx sign(\u2207 x J(\u03b8x , , y))x+\n\ue00fsign(\u2207 x J(\u03b8x , , y))\ny=\u201cpanda\u201d \u201cnematode\u201d\u201cgibbon\u201d\nw/57.7%\ncon\ufb01dencew/8.2%\ncon\ufb01dencew/99.3%\ncon\ufb01dence\nFigure7.8:\u00a0AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\nrespecttotheinput,wecanchangeGoogLeNet\u2019sclassi\ufb01cationoftheimage.Reproduced\nwithpermissionfrom (). Goodfellow e t a l .2014b\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\nifithasnumerousinputs.Ifwechangeeachinputby \ue00f,thenalinearfunction\nwithweightswcanchangebyasmuchas \ue00f||||w 1,whichcanbeaverylarge\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly\nintroducingalocalconstancypriorintosupervisedneuralnets.\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\nfromnearlylineartonearlylocallyconstantandthushavethe\ufb02exibilitytocapture\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.\nAdversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\nmodelitselfassignssomelabel \u02c6 y.Themodel\u2019slabel \u02c6 ymaynotbethetruelabel,\nbutifthemodelishighquality,then\u02c6 yhasahighprobabilityofprovidingthe\ntruelabel.Wecanseekanadversarialexamplex\ue030thatcausestheclassi\ufb01erto\noutputalabel y\ue030with y\ue030\ue036=\u02c6 y.Adversarialexamplesgeneratedusingnotthetrue\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial\nexamples(Miyato2015etal.,).Theclassi\ufb01ermaythenbetrainedtoassignthe\nsamelabeltoxandx\ue030.Thisencouragestheclassi\ufb01ertolearnafunctionthatis\n2 6 9", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\nlies.Theassumptionmotivatingthisapproachisthatdi\ufb00erentclassesusuallylie\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\nfromoneclassmanifoldtoanotherclassmanifold.\n7.14Tangent\u00a0Distance,\u00a0TangentProp,and\u00a0Manifold\nTangentClassi\ufb01er\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin\nsection.5.11.3\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassi\ufb01er\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement\nonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\nrespectivelybelong.Althoughthatmaybecomputationally di\ufb03cult(itwould\nrequiresolvinganoptimization problem,to\ufb01ndthenearestpairofpointson M 1\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional\nlinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\nonetospecifythetangentvectors.\nInarelatedspirit,thetangentpropalgorithm( ,)(\ufb01gure) Simardetal.19927.9\ntrainsaneuralnetclassi\ufb01erwithanextrapenaltytomakeeachoutput f(x)of\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\nsameclassconcentrate.Localinvarianceisachievedbyrequiring \u2207 x f(x)tobe\northogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\nregularizationpenalty:\u2126\n\u2126() = f\ue058\ni\ue010\n(\u2207 x f())x\ue03ev( ) i\ue0112\n. (7.67)\n2 7 0", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\nthee\ufb00ectoftransformationssuchastranslation,rotation,andscalinginimages.\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992\nbutalsointhecontextofreinforcementlearning(,). Thrun1995\nTangentpropagation is\u00a0closelyrelated\u00a0todataset\u00a0augmentation.In\u00a0both\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\nnetwork.Thedi\ufb00erenceisthatinthecaseofdatasetaugmentation, thenetworkis\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\nmorethananin\ufb01nitesimalamountofthesetransformations.Tangentpropagation\ndoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\nthe\u00a0speci\ufb01ed\u00a0transformation.While\u00a0thisanalytical\u00a0approac h\u00a0isintellectually\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\nin\ufb01nitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\nlargerperturbations.Second,thein\ufb01nitesimalapproachposesdi\ufb03cultiesformodels\nbasedonrecti\ufb01edlinearunits.Thesemodelscanonlyshrinktheirderivatives\nbyturningunitso\ufb00orshrinkingtheirweights.Theyarenotabletoshrinktheir\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\nunitscan.Datasetaugmentation workswellwithrecti\ufb01edlinearunitsbecause\ndi\ufb00erentsubsetsofrecti\ufb01edunitscanactivatefordi\ufb00erenttransformedversionsof\neachoriginalinput.\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,\n1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\n\ufb01ndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\naugmentationusingmanuallyspeci\ufb01edtransformationsbothrequirethatthe\nmodelshouldbeinvarianttocertainspeci\ufb01eddirectionsofchangeintheinput.\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe\ninvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\nasdatasetaugmentationisthenon-in\ufb01nitesimalversionoftangentpropagation,\nadversarialtrainingisthenon-in\ufb01nitesimalversionofdoublebackprop.\nThemanifoldtangentclassi\ufb01er(,),eliminatestheneedto Rifaietal.2011c\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n2 7 1", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nx 1x 2N o r m a lT a ng e nt\nFigure7.9:\u00a0Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\n1992 Rifai2011c )andmanifoldtangentclassi\ufb01er( e t a l .,),whichbothregularizethe\nclassi\ufb01eroutputfunction f(x).Eachcurverepresentsthemanifoldforadi\ufb00erentclass,\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe\nclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\ntangentdirectionsandmanynormaldirections.Weexpecttheclassi\ufb01cationfunctionto\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\nclassi\ufb01erregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent\npropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\nmanifold)whilethemanifoldtangentclassi\ufb01erestimatesthemanifoldtangentdirections\nbytraininganautoencoderto\ufb01tthetrainingdata.Theuseofautoencoderstoestimate\nmanifoldswillbedescribedinchapter.14\nestimatethemanifoldtangentvectors.Themanifoldtangentclassi\ufb01ermakesuse\nofthistechniquetoavoidneedinguser-speci\ufb01edtangentvectors.\u00a0Asillustrated\nin\ufb01gure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\nandincludefactorsthatmustbelearnedbecausetheyareobject-speci\ufb01c(suchas\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassi\ufb01er\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassi\ufb01er\nasintangentprop(equation).7.67\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\n2 7 2", "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\nthemeofmachinelearningisoptimization, describednext.\n2 7 3", "C h a p t e r 8\nOptimizationforTrainingDeep\nModels\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\ndi\ufb03cultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural\nnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral.\nThischapterfocusesononeparticularcaseofoptimization: \ufb01ndingtheparam-\neters\u03b8ofaneuralnetworkthatsigni\ufb01cantlyreduceacostfunction J(\u03b8),which\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\nwellasadditionalregularizationterms.\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdi\ufb00ersfrompureoptimization. Next,wepresentseveral\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdi\ufb03cult.We\nthende\ufb01neseveralpracticalalgorithms,includingbothoptimization algorithms\nthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\n274", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\nalgorithmsintohigher-levelprocedures.\n8.1HowLearningDi\ufb00ersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdi\ufb00erfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure\nP,thatisde\ufb01nedwithrespecttothetestsetandmayalsobeintractable.We\nthereforeoptimize Ponlyindirectly.Wereduceadi\ufb00erentcostfunction J(\u03b8)in\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\ndeepmodelsalsotypicallyincludesomespecializationonthespeci\ufb01cstructureof\nmachinelearningobjectivefunctions.\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\nsuchas\nJ() = \u03b8 E ( ) \u02c6 x ,y \u223c pdataL f , y , ((;)x\u03b8) (8.1)\nwhere Listheper-examplelossfunction, f(x;\u03b8)isthepredictedoutputwhen\ntheinputisx,\u02c6 p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\nsupervisedcase,wheretheargumentsto Lare f(x;\u03b8)and y.However,itistrivial\ntoextendthisdevelopment,forexample,toinclude\u03b8orxasarguments,orto\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\nunsupervisedlearning.\nEquationde\ufb01nesanobjectivefunctionwithrespecttothetrainingset.We 8.1\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\noverthe\ufb01nitetrainingset:\nJ\u2217() = \u03b8 E ( ) x ,y \u223c pdataL f , y . ((;)x\u03b8) (8.2)\n8.1.1EmpiricalRiskMinimization\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere\nthattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\n2 7 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistribution\u02c6 p(x , y)\nde\ufb01nedbythetrainingset.Wenowminimizetheempiricalrisk\nE x ,y \u223c \u02c6 pdata ( ) x , y[((;))] = L fx\u03b8 , y1\nmm \ue058\ni = 1L f((x( ) i;)\u03b8 , y( ) i)(8.3)\nwhereisthenumberoftrainingexamples. m\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigni\ufb01cantlyas\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\ncanbeexpectedtodecreasebyvariousamounts.\nHowever,empiricalriskminimization ispronetoover\ufb01tting.Modelswith\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\nriskminimization isnotreallyfeasible.Themoste\ufb00ectivemodernoptimization\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorunde\ufb01ned\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we\nrarelyuseempiricalriskminimization. Instead,wemustuseaslightlydi\ufb00erent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmoredi\ufb00erent\nfromthequantitythatwetrulywanttooptimize.\n8.1.2SurrogateLossFunctionsandEarlyStopping\nSometimes,thelossfunctionweactuallycareabout(sayclassi\ufb01cationerror)isnot\nonethatcanbeoptimizede\ufb03ciently.Forexample,exactlyminimizingexpected0-1\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear\nclassi\ufb01er(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassi\ufb01cationerrorin\nexpectation.\n2 7 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\nonecanimprovetherobustnessoftheclassi\ufb01erbyfurtherpushingtheclassesapart\nfromeachother,obtainingamorecon\ufb01dentandreliableclassi\ufb01er,thusextracting\nmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\nminimizingtheaverage0-1lossonthetrainingset.\nAveryimportantdi\ufb00erencebetweenoptimization ingeneralandoptimization\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\nstopping(section)issatis\ufb01ed.Typicallytheearlystoppingcriterionisbased 7.8\nonthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\nandisdesignedtocausethealgorithmtohaltwheneverover\ufb01ttingbeginstooccur.\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\nwhichisverydi\ufb00erentfromthepureoptimization setting,whereanoptimization\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\n8.1.3BatchandMinibatchAlgorithms\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral\noptimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\nspace,decomposeintoasumovereachexample:\n\u03b8 M L= argmax\n\u03b8m \ue058\ni = 1log p m o de l(x( ) i, y( ) i;)\u03b8 . (8.4)\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\nempiricaldistributionde\ufb01nedbythetrainingset:\nJ() = \u03b8 E x ,y \u223c \u02c6 pdatalog p m o de l(;)x , y\u03b8 . (8.5)\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n2 7 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmostcommonlyusedpropertyisthegradient:\n\u2207 \u03b8 J() = \u03b8 E x ,y \u223c \u02c6 pdata\u2207 \u03b8log p m o de l(;)x , y\u03b8 . (8.6)\nComputing\u00a0this expectation\u00a0exactly\u00a0isvery\u00a0expensive\u00a0because\u00a0it\u00a0requires\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n\nsamplesisgivenby \u03c3 /\u221an ,where \u03c3isthetruestandarddeviationofthevalueof\nthesamples.Thedenominator of\u221anshowsthattherearelessthanlinearreturns\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof\nnumberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\nofthegradientratherthanslowlycomputingtheexactgradient.\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we\nareunlikelytotrulyencounterthisworst-casesituation,butwemay\ufb01ndlarge\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\nbecausetheword\u201cbatch\u201disalsooftenusedtodescribetheminibatchusedby\nminibatchstochasticgradientdescent.Typicallytheterm\u201cbatchgradientdescent\u201d\nimpliestheuseofthefulltrainingset,whiletheuseoftheterm\u201cbatch\u201dtodescribe\nagroupofexamplesdoesnot.\u00a0Forexample,itisverycommontousetheterm\n\u201cbatchsize\u201dtodescribethesizeofaminibatch.\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\ncreatedexamplesratherthanfroma\ufb01xed-sizetrainingsetoverwhichseveral\npassesaremade.\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n2 7 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\ncallthemstochasticmethods.\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\npresentedindetailinsection.8.3.1\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\n\u2022Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\nlessthanlinearreturns.\n\u2022Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\nisnoreductioninthetimetoprocessaminibatch.\n\u2022Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\nhardwaresetupsthisisthelimitingfactorinbatchsize.\n\u2022Somekindsofhardwareachievebetterruntimewithspeci\ufb01csizesofarrays.\nEspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestoo\ufb00er\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\nsometimesbeingattemptedforlargemodels.\n\u2022Smallbatchescano\ufb00eraregularizinge\ufb00ect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1.\u00a0Trainingwithsuchasmallbatch\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh\nduetotheneedtomakemoresteps,bothbecauseofthereducedlearning\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\nDi\ufb00erentkindsofalgorithmsusedi\ufb00erentkindsofinformationfromthemini-\nbatchindi\ufb00erentways.Somealgorithmsaremoresensitivetosamplingerrorthan\nothers,eitherbecausetheyuseinformationthatisdi\ufb03culttoestimateaccurately\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare\nusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\nH\u2212 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\nsizesarerequiredtominimize\ufb02uctuationsintheestimatesofH\u2212 1g.Suppose\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\n2 7 9", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nHoritsinverseampli\ufb01espre-existingerrors,inthiscase,estimationerrorsing.\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\nH\u2212 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\napproximately,sotheupdateH\u2212 1gwillcontainevenmoreerrorthanwewould\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan\nunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\nlistmightbearrangedsothat\ufb01rstwehave\ufb01vebloodsamplestakenatdi\ufb00erent\ntimesfromthe\ufb01rstpatient,thenwehavethreebloodsamplestakenfromthe\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\nholdssomesigni\ufb01cance,itisnecessarytoshu\ufb04etheexamplesbeforeselecting\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof\nexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\nitisusuallysu\ufb03cienttoshu\ufb04etheorderofthedatasetonceandthenstoreitin\nshu\ufb04edfashion.Thiswillimposea\ufb01xedsetofpossibleminibatchesofconsecutive\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea\nsigni\ufb01cantdetrimentale\ufb00ect.Failingtoevershu\ufb04etheexamplesinanywaycan\nseriouslyreducethee\ufb00ectivenessofthealgorithm.\nManyoptimization problemsinmachinelearningdecomposeoverexamples\nwellenoughthatwecancomputeentireseparateupdatesoverdi\ufb00erentexamples\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\ndiscussedfurtherinsection.12.1.3\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\n2 8 0", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ndescentshu\ufb04ethedatasetonceandthenpassthroughitmultipletimes.Onthe\n\ufb01rstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\nnewfairsamplesfromthedatageneratingdistribution.\nThefactthatstochasticgradientdescentminimizesgeneralization erroris\neasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\nfromastreamofdata.Inotherwords,insteadofreceivinga\ufb01xed-sizetraining\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\nfrom p da t a.\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete.\u00a0Inthis\ncase,thegeneralization error(equation)canbewrittenasasum 8.2\nJ\u2217() =\u03b8\ue058\nx\ue058\nyp da t a()((;)) x , y L fx\u03b8 , y , (8.7)\nwiththeexactgradient\ng= \u2207 \u03b8 J\u2217() =\u03b8\ue058\nx\ue058\nyp da t a()x , y\u2207 \u03b8 L f , y . ((;)x\u03b8)(8.8)\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\nundermildassumptionsregarding p da t aand. L\nHence,\u00a0wecanobtainanunbiasedestimatoroftheexactgradientof\u00a0the\ngeneralization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\n\u02c6g=1\nm\u2207 \u03b8\ue058\niL f((x( ) i;)\u03b8 , y( ) i) . (8.9)\nUpdatinginthedirectionof \u03b8 \u02c6gperformsSGDonthegeneralization error.\nOfcourse,\u00a0thisinterpretation only\u00a0applies whenexamplesarenotreused.\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,\nunlessthetrainingsetisextremelylarge.\u00a0When multiplesuchepochsareused,\nonlythe\ufb01rstepochfollowstheunbiasedgradientofthegeneralization error,but\n2 8 1", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofcourse,theadditionalepochsusuallyprovideenoughbene\ufb01tduetodecreased\ntrainingerrortoo\ufb00settheharmtheycausebyincreasingthegapbetweentraining\nerrorandtesterror.\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\nset.Whenusinganextremelylargetrainingset,over\ufb01ttingisnotanissue,so\nunder\ufb01ttingandcomputational e\ufb03ciencybecomethepredominant concerns.See\nalso ()foradiscussionofthee\ufb00ectofcomputational BottouandBousquet2008\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\n8.2ChallengesinNeuralNetworkOptimization\nOptimization ingeneralisanextremelydi\ufb03culttask.Traditionally,machine\nlearninghasavoidedthedi\ufb03cultyofgeneraloptimization bycarefullydesigning\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis\nconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels.\n8.2.1Ill-Conditioning\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\nmoredetailinsection.4.3.1\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\n\u201cstuck\u201dinthesensethatevenverysmallstepsincreasethecostfunction.\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\ncostfunctionpredictsthatagradientdescentstepofwilladd \u2212 \ue00fg\n1\n2\ue00f2g\ue03eHgg\u2212 \ue00f\ue03eg (8.10)\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n2\ue00f2g\ue03eHg\nexceeds \ue00fg\ue03eg.\u00a0Todeterminewhetherill-conditioning isdetrimentaltoaneural\nnetwork\u00a0training task,\u00a0one\u00a0canmonitorthe\u00a0squaredgradientnormg\ue03egand\n2 8 2", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u221250050100150200250\nTrainingtime(epochs)\u221220246810121416Gradient norm\n0 50100150200250\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classi\ufb01cationerrorrate\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient\nevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassi\ufb01cation\nerrordecreasestoalowlevel.\ntheg\ue03eHgterm.Inmanycases,thegradientnormdoesnotshrinksigni\ufb01cantly\nthroughoutlearning,buttheg\ue03eHgtermgrowsbymorethananorderofmagnitude.\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\ncurvature.Figureshowsanexampleofthegradientincreasingsigni\ufb01cantly 8.1\nduringthesuccessfultrainingofaneuralnetwork.\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\ntraining,someofthetechniquesusedtocombatitinothercontextsareless\napplicabletoneuralnetworks.Forexample,Newton\u2019smethodisanexcellenttool\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\nthesubsequentsectionswewillarguethatNewton\u2019smethodrequiressigni\ufb01cant\nmodi\ufb01cationbeforeitcanbeappliedtoneuralnetworks.\n8.2.2LocalMinima\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\ncanbereducedtotheproblemof\ufb01ndingalocalminimum.Anylocalminimumis\n2 8 3", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nguaranteedtobeaglobalminimum.Someconvexfunctionshavea\ufb02atregionat\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\na\ufb02atregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\nknowthatwehavereachedagoodsolutionifwe\ufb01ndacriticalpointofanykind.\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave\nanextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\nnecessarilyamajorproblem.\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\nvariablesallhavemultiplelocalminimabecauseofthemodelidenti\ufb01ability\nproblem.Amodelissaidtobeidenti\ufb01ableifasu\ufb03cientlylargetrainingsetcan\nruleoutallbutonesettingofthemodel\u2019sparameters.Modelswithlatentvariables\nareoftennotidenti\ufb01ablebecausewecanobtainequivalentmodelsbyexchanging\nlatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\nunits.Thiskindofnon-identi\ufb01abilit yisknownasweightspacesymmetry.\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\nadditionalcausesofnon-identi\ufb01abilit y.Forexample,inanyrecti\ufb01edlinearor\nmaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\n\u03b1ifwealsoscaleallofitsoutgoingweightsby1\n\u03b1.Thismeansthat\u2014ifthecost\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\nweightsratherthanthemodels\u2019outputs\u2014everylocalminimumofarecti\ufb01edlinear\normaxoutnetworkliesonan( m n\u00d7)-dimensionalhyperbolaofequivalentlocal\nminima.\nThesemodelidenti\ufb01abilityissuesmeanthattherecanbeanextremelylarge\norevenuncountablyin\ufb01niteamountoflocalminimainaneuralnetworkcost\nfunction.However,alloftheselocalminimaarisingfromnon-identi\ufb01abilit yare\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\nnotaproblematicformofnon-convexity.\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima\nwithhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\noptimization algorithms.\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\n2 8 4", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\nnowsuspectthat,forsu\ufb03cientlylargeneuralnetworks,mostlocalminimahavea\nlowcostfunctionvalue,andthatitisnotimportantto\ufb01ndatrueglobalminimum\nratherthanto\ufb01ndapointinparameterspacethathaslowbutnotminimalcost\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\netal.,).2014\nManypractitioners attributenearlyalldi\ufb03cultywithneuralnetworkoptimiza-\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeci\ufb01c\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\ninsigni\ufb01cantsize,theproblemisneitherlocalminimanoranyotherkindofcritical\npoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\nspaces,itcanbeverydi\ufb03culttopositivelyestablishthatlocalminimaarethe\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,\nwhileothershavealowercost.\u00a0Atasaddlepoint,theHessianmatrixhasboth\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\nmaximumalonganothercross-section.See\ufb01gureforanillustration. 4.5\nManyclasses\u00a0ofrandomfunctionsexhibitthefollowingbehavior:inlow-\ndimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn\u2192 Rof\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.\u00a0The\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\nImaginethatthesignofeacheigenvalueisgeneratedby\ufb02ippingacoin.Inasingle\ndimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\n2 8 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.\u00a0In\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\nheads ntimesifweareatacriticalpointwithlowcost.\u00a0Thismeansthatlocal\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith\nhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\nhighcostaremorelikelytobelocalmaxima.\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout\nproofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof\ndeepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\nfunctionsrelatedtoneuralnetworksdoessoaswell.\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\nrithms?For\ufb01rst-orderoptimization algorithmsthatuseonlygradientinformation,\nthesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\ngivenin\ufb01gure.Thesevisualizationsshowa\ufb02atteningofthecostfunctionnear 8.2\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015\nalsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\nmaybedi\ufb00erentformorerealisticusesofgradientdescent.\nForNewton\u2019smethod,\u00a0itisclearthatsaddlepointsconstituteaproblem.\n2 8 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nP r o j e c t i o n2o f \u03b8\nP r o j e c t i o n 1 o f \u03b8J(\n)\u03b8\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\nwithpermissionfromGoodfellow2015 e t a l .().\u00a0Thesevisualizationsappearsimilarfor\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\nvisualizationsusuallydonotshowmanyconspicuousobstacles.\u00a0Priortothesuccessof\nstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\nstructurethanisrevealedbytheseprojections.\u00a0Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\nMostoftrainingtimeisspenttraversingtherelatively\ufb02atvalleyofthecostfunction,\nwhichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\ninthisregion,orsimplytheneedtocircumnavigatethetall\u201cmountain\u201dvisibleinthe\n\ufb01gureviaanindirectarcingpath.\n2 8 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nGradientdescentisdesignedtomove\u201cdownhill\u201dandisnotexplicitlydesigned\ntoseekacriticalpoint.Newton\u2019smethod,however,isdesignedtosolvefora\npointwherethegradientiszero.Withoutappropriatemodi\ufb01cation,itcanjump\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014\nsaddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\nimprovessigni\ufb01cantlyoverthetraditionalversion.Second-order methodsremain\ndi\ufb03culttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\npromiseifitcouldbescaled.\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\npoints.Therearealsomaxima,\u00a0whic haremuchlikesaddlepointsfromthe\nperspectiveofoptimization\u2014many algorithmsarenotattractedtothem,\u00a0but\nunmodi\ufb01edNewton\u2019smethodis.Maximaofmanyclassesofrandomfunctions\nbecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\nTheremayalsobewide,\ufb02atregionsofconstantvalue.Intheselocations,the\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\n\ufb02atregionmustconsistentirelyofglobalminima,butinageneraloptimization\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\n8.2.4Cli\ufb00sandExplodingGradients\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\ncli\ufb00s,asillustratedin\ufb01gure.Theseresultfromthemultiplicationofseveral 8.3\nlargeweightstogether.Onthefaceofanextremelysteepcli\ufb00structure,the\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingo\ufb00\nofthecli\ufb00structurealtogether.\n2 8 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\ue077\n\ue062\ue04a\ue077\ue03b \ue062\n\ue028\ue029\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacli\ufb00region,a\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe\noptimizationworkthathadbeendone.\u00a0FigureadaptedwithpermissionfromPascanu\ne t a l .().2013\nThecli\ufb00canbedangerouswhetherweapproachitfromaboveorfrombelow,\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\nwithinanin\ufb01nitesimalregion.Whenthetraditionalgradientdescentalgorithm\nproposestomakeaverylargestep,thegradientclippingheuristicintervenesto\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cli\ufb00\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\nmultiplication.\n8.2.5Long-TermDependencies\nAnotherdi\ufb03cultythatneuralnetworkoptimization algorithmsmustovercome\narises\u00a0when\u00a0thecomputational\u00a0gra ph\u00a0becomes\u00a0extremely\u00a0deep.Feedforward\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\n2 8 9", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\npronounceddi\ufb03culties.\nForexample,supposethatacomputational graphcontainsapaththatconsists\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(\u03bb)V\u2212 1.\nInthissimplecase,itisstraightforwardtoseethat\nWt=\ue000\nV\u03bbVdiag()\u2212 1\ue001t= ()Vdiag\u03bbtV\u2212 1. (8.11)\nAnyeigenvalues \u03bb ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\nvanishingandexplodinggradientproblemreferstothefactthatgradients\nthroughsuchagrapharealsoscaledaccordingtodiag(\u03bb)t.Vanishinggradients\nmakeitdi\ufb03culttoknowwhichdirectiontheparametersshouldmovetoimprove\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecli\ufb00\nstructuresdescribedearlierthatmotivategradientclippingareanexampleofthe\nexplodinggradientphenomenon.\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\nsimilartothepowermethodalgorithmusedto\ufb01ndthelargesteigenvalueof\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\nnotsurprisingthatx\ue03eWtwilleventuallydiscardallcomponentsofxthatare\northogonaltotheprincipaleigenvectorof.W\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward\nnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\nvanishingandexplodinggradientproblem(,). Sussillo2014\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n8.2.6InexactGradients\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\nanoisyorevenbiasedestimateofthesequantities.\u00a0Nearlyeverydeeplearning\nalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient.\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\n2 9 0", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\nofaBoltzmannmachine.\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure\nManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\nlossfunctionatasinglepoint\u2014itcanbedi\ufb03culttomakeasinglestepif J(\u03b8)is\npoorlyconditionedatthecurrentpoint\u03b8,orif\u03b8liesonacli\ufb00,orif\u03b8isasaddle\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\nnotpointtowarddistantregionsofmuchlowercost.\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\nmountain-shapedstructure.\nMuchofresearchintothedi\ufb03cultiesofoptimization hasfocusedonwhether\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1\nshowsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\n\u2212log p( y|x;\u03b8)canlackaglobalminimumpointandinsteadasymptotically\napproachsomevalueasthemodelbecomesmorecon\ufb01dent.Foraclassi\ufb01erwith\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof\nzero.Likewise,amodelofrealvalues p( y|x) =N( y; f(\u03b8) , \u03b2\u2212 1)canhavenegative\nlog-likelihoodthatasymptotestonegativein\ufb01nity\u2014if f(\u03b8)isabletocorrectly\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\n\u03b2withoutbound.See\ufb01gureforanexampleofafailureoflocaloptimization to 8.4\n\ufb01ndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\npoints.\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat\nin\ufb02uencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n2 9 1", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u03b8J ( ) \u03b8\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdi\ufb03cultyin\nthiscaseisbeinginitializedonthewrongsideofthe\u201cmountain\u201dandnotbeingableto\ntraverseit.\u00a0Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate\nsuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\nexcessivetrainingtime,asillustratedin\ufb01gure.8.2\noftheprocess.\nManyexistingresearchdirectionsareaimedat\ufb01ndinggoodinitialpointsfor\nproblemsthathavedi\ufb03cultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves.\nGradientdescentandessentiallyalllearningalgorithmsthataree\ufb00ectivefor\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves\ncanbedi\ufb03culttocompute.Wemaybeabletocomputesomepropertiesofthe\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\nnotde\ufb01neareasonablyshortpathtoavalidsolution,butwearenotactually\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In\nthesecases,localdescentwithstepsofsize \ue00fmayde\ufb01neareasonablyshortpath\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\nstepsofsize \u03b4 \ue00f\ue01c.Inthesecases,localdescentmayormaynotde\ufb01neapath\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\n2 9 2", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\nthefunctionhasawide\ufb02atregion,orifwemanagetolandexactlyonacritical\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\nforcriticalpoints,suchasNewton\u2019smethod).Inthesecases,localdescentdoes\nnotde\ufb01neapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin\n\ufb01gure,oralonganunnecessarilylongtrajectorytothesolution,asin\ufb01gure. 8.4 8.2\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\nmakingneuralnetworkoptimization di\ufb03cult,andthisisanactiveareaofresearch.\nRegardlessofwhichoftheseproblemsaremostsigni\ufb01cant,allofthemmightbe\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\nwithinthatwell-behavedregion.\u00a0Thislastviewsuggestsresearchintochoosing\ngoodinitialpointsfortraditionaloptimization algorithmstouse.\n8.2.8TheoreticalLimitsofOptimization\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\nlittlebearingontheuseofneuralnetworksinpractice.\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\nnetworkoutput\u00a0discretevalues.However,\u00a0most\u00a0neuralnetworkunitsoutput\nsmoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\nitcanbedi\ufb03culttotellwhetheraparticularproblemfallsintothatclass.Other\nresultsshowthat\ufb01ndingasolutionforanetworkofagivensizeisintractable,but\ninpracticewecan\ufb01ndasolutioneasilybyusingalargernetworkforwhichmany\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\ncontextofneuralnetworktraining,weusuallydonotcareabout\ufb01ndingtheexact\nminimumofafunction,butseekonlytoreduceitsvaluesu\ufb03cientlytoobtaingood\ngeneralization error.\u00a0Theoretical analysisofwhetheranoptimization algorithm\ncanaccomplishthisgoalisextremelydi\ufb03cult.Developingmorerealisticbounds\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\ngoalformachinelearningresearch.\n2 9 3", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.3BasicAlgorithms\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n8.3.1StochasticGradientDescent\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused\noptimization algorithmsformachinelearningingeneralandfordeeplearning\ninparticular.\u00a0As discussedinsection,itispossibletoobtainanunbiased 8.1.3\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\nexamplesdrawni.i.dfromthedatageneratingdistribution.\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\nRequire:Learningrate \ue00f k.\nRequire:Initialparameter\u03b8\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate: \u02c6g\u2190+1\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nApplyupdate:\u03b8\u03b8\u2190 \u2212 \ue00f\u02c6g\nendwhile\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\nhavedescribedSGDasusinga\ufb01xedlearningrate \ue00f.Inpractice,itisnecessaryto\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\natiterationas k \ue00f k.\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\ndescent,sobatchgradientdescentcanusea\ufb01xedlearningrate.Asu\ufb03cient\nconditiontoguaranteeconvergenceofSGDisthat\n\u221e\ue058\nk = 1\ue00f k= and \u221e , (8.12)\n2 9 4", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u221e\ue058\nk = 1\ue00f2\nk < .\u221e (8.13)\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: \u03c4\n\ue00f k= (1 )\u2212 \u03b1 \ue00f 0+ \u03b1 \ue00f \u03c4 (8.14)\nwith \u03b1=k\n\u03c4.Afteriteration,itiscommontoleaveconstant. \u03c4 \ue00f\nThelearningratemaybechosenbytrialanderror,butitisusuallybest\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,\ntheparameterstochooseare \ue00f 0, \ue00f \u03c4,and \u03c4.Usually \u03c4maybesettothenumberof\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\n\ue00f \u03c4shouldbesettoroughlythevalueof 1% \ue00f 0.Themainquestionishowtoset \ue00f 0.\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\nfunctionoftenincreasingsigni\ufb01cantly.Gentleoscillationsare\ufb01ne,especiallyif\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe\nuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\n\ufb01nalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\nafterthe\ufb01rst100iterationsorso.Therefore,itisusuallybesttomonitorthe\ufb01rst\nseveraliterationsandusealearningratethatishigherthanthebest-performing\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.\nThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\nconvergetowithinsome\ufb01xedtoleranceofits\ufb01naltestseterrorbeforeithas\nprocessedtheentiretrainingset.\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\nmeasuretheexcesserror J(\u03b8)\u2212min \u03b8 J(\u03b8),whichistheamountthatthecurrent\ncostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\nproblem,theexcesserroris O(1\u221a\nk)after kiterations,whileinthestronglyconvex\ncaseitis O(1\nk).Theseboundscannotbeimprovedunlessextraconditionsare\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\ngradientdescentintheory.However,theCram\u00e9r-Raobound(,;, Cram\u00e9r1946Rao\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\nk).Bottou\n2 9 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\nanoptimization algorithmthatconvergesfasterthan O(1\nk)formachinelearning\ntasks\u2014fasterconvergencepresumablycorrespondstoover\ufb01tting.Moreover,the\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples\noutweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\ntheremainderofthischapterachievebene\ufb01tsthatmatterinpracticebutarelost\nintheconstantfactorsobscuredbythe O(1\nk)asymptoticanalysis.Onecanalso\ntradeo\ufb00thebene\ufb01tsofbothbatchandstochasticgradientdescentbygradually\nincreasingtheminibatchsizeduringthecourseoflearning.\nFormoreinformationonSGD,see(). Bottou1998\n8.3.2Momentum\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,\nlearningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\nintheirdirection.Thee\ufb00ectofmomentumisillustratedin\ufb01gure.8.5\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\nofvelocity\u2014itisthedirectionandspeedatwhichtheparametersmovethrough\nparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\naccordingtoNewton\u2019slawsofmotion.Momentuminphysicsismasstimesvelocity.\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter \u03b1\u2208[0 ,1)\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.\nTheupdateruleisgivenby:\nvv\u2190 \u03b1\u2212\u2207 \ue00f \u03b8\ue020\n1\nmm \ue058\ni = 1L((fx( ) i;)\u03b8 ,y( ) i)\ue021\n, (8.15)\n\u03b8\u03b8v \u2190 + . (8.16)\nThevelocityvaccumulatesthegradientelements\u2207 \u03b8\ue0001\nm\ue050m\ni = 1 L((fx( ) i;)\u03b8 ,y( ) i)\ue001\n.\nThelarger \u03b1isrelativeto \ue00f,themorepreviousgradientsa\ufb00ectthecurrentdirection.\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\n2 9 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u2212 \u2212 \u2212 3 0 2 0 1 0 0 1 0 2 0\u2212 3 0\u2212 2 0\u2212 1 001 02 0\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\novercomesthe\ufb01rstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis\nfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\nnarrowaxisofthecanyon.Comparealso\ufb01gure,whichshowsthebehaviorofgradient 4.6\ndescentwithoutmomentum.\n2 9 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\nobservesgradientg,thenitwillaccelerateinthedirectionof\u2212g,untilreachinga\nterminalvelocitywherethesizeofeachstepis\n\ue00f||||g\n1\u2212 \u03b1. (8.17)\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\n1 \u2212 \u03b1.For\nexample, \u03b1= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\nthegradientdescentalgorithm.\nCommonvaluesof \u03b1usedinpracticeinclude .5, .9,and .99.Likethelearning\nrate, \u03b1mayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\nislaterraised.Itislessimportanttoadapt \u03b1overtimethantoshrink \ue00fovertime.\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter. \ue00f \u03b1\nRequire:Initialparameter,initialvelocity. \u03b8 v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate:g\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nComputevelocityupdate:vvg \u2190 \u03b1\u2212 \ue00f\nApplyupdate:\u03b8\u03b8v \u2190 +\nendwhile\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild\nintuitionforhowthemomentumandgradientdescentalgorithmsbehave.\nThepositionoftheparticleatanypointintimeisgivenby\u03b8( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = t\u22022\n\u2202 t2\u03b8() t . (8.18)\nRatherthanviewingthisasasecond-orderdi\ufb00erentialequationoftheposition,\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\ntandrewritetheNewtoniandynamicsasa\ufb01rst-orderdi\ufb00erentialequation:\nv() = t\u2202\n\u2202 t\u03b8() t , (8.19)\n2 9 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nf() = t\u2202\n\u2202 tv() t . (8.20)\nThemomentumalgorithmthenconsistsofsolvingthedi\ufb00erentialequationsvia\nnumericalsimulation.Asimplenumericalmethodforsolvingdi\ufb00erentialequations\nisEuler\u2019smethod,whichsimplyconsistsofsimulatingthedynamicsde\ufb01nedby\ntheequationbytakingsmall,\ufb01nitestepsinthedirectionofeachgradient.\nThisexplainsthebasicformofthemomentumupdate,butwhatspeci\ufb01callyare\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:\n\u2212\u2207 \u03b8 J(\u03b8).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\nuntilitbeginstogouphillagain.\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\notherforce,proportionalto\u2212v( t).Inphysicsterminology,thisforcecorresponds\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas\nsyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\nconvergetoalocalminimum.\nWhydoweuse\u2212v( t)andviscousdraginparticular?\u00a0Partofthereasonto\nuse\u2212v( t)ismathematical convenience\u2014anintegerpowerofthevelocityiseasy\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe\nvelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.\nIfweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\nalocalminimum.Viscousdragavoidsbothoftheseproblems\u2014itisweakenough\n2 9 9", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\n8.3.3NesterovMomentum\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\ninspiredbyNesterov\u2019sacceleratedgradientmethod(,,).The Nesterov19832004\nupdaterulesinthiscasearegivenby:\nvv\u2190 \u03b1\u2212\u2207 \ue00f \u03b8\ue022\n1\nmm \ue058\ni = 1L\ue010\nfx(( ) i;+ )\u03b8 \u03b1v ,y( ) i\ue011\ue023\n,(8.21)\n\u03b8\u03b8v \u2190 + , (8.22)\nwheretheparameters \u03b1and \ue00fplayasimilarroleasinthestandardmomentum\nmethod.Thedi\ufb00erencebetweenNesterovmomentumandstandardmomentumis\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof\nconvergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\nbyNesterov1983().Unfortunately,\u00a0inthestochasticgradientcase,Nesterov\nmomentumdoesnotimprovetherateofconvergence.\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\nRequire:Learningrate,momentumparameter. \ue00f \u03b1\nRequire:Initialparameter,initialvelocity. \u03b8 v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondinglabelsy( ) i.\nApplyinterimupdate: \u02dc\u03b8\u03b8v \u2190 + \u03b1\nComputegradient(atinterimpoint):g\u21901\nm\u2207 \u02dc \u03b8\ue050\ni L f((x( ) i;\u02dc\u03b8y) ,( ) i)\nComputevelocityupdate:vvg \u2190 \u03b1\u2212 \ue00f\nApplyupdate:\u03b8\u03b8v \u2190 +\nendwhile\n3 0 0", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.4ParameterInitializationStrategies\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep\nlearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\nmodelsisasu\ufb03cientlydi\ufb03culttaskthatmostalgorithmsarestronglya\ufb00ectedby\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\nencountersnumericaldi\ufb03cultiesandfailsaltogether.Whenlearningdoesconverge,\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit\nconvergestoapointwithhigh\u00a0orlowcost.Also,\u00a0pointsofcomparablecost\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcana\ufb00ectthe\ngeneralization aswell.\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\ninitialization strategiesisadi\ufb03culttaskbecauseneuralnetworkoptimization is\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood\nunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\nafterlearningbeginstoproceed.Afurtherdi\ufb03cultyisthatsomeinitialpoints\nmaybebene\ufb01cialfromtheviewpointofoptimization butdetrimentalfromthe\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointa\ufb00ects\ngeneralization isespeciallyprimitive,o\ufb00eringlittletonoguidanceforhowtoselect\ntheinitialpoint.\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial\nparametersneedto\u201cbreaksymmetry\u201d\u00a0betweendi\ufb00erentunits.Iftwohidden\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\ntheseunitsmusthavedi\ufb00erentinitialparameters.\u00a0Iftheyhavethesameinitial\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputedi\ufb00erent\nupdatesfordi\ufb00erentunits(forexample,ifonetrainswithdropout),itisusually\nbesttoinitializeeachunittocomputeadi\ufb00erentfunctionfromalloftheother\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\nofback-propagation. Thegoalofhavingeachunitcomputeadi\ufb00erentfunction\n3 0 1", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\nforalargesetofbasisfunctionsthatareallmutuallydi\ufb00erentfromeachother,\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\ndi\ufb00erentfunctionfromeachotherunit.Randominitialization fromahigh-entropy\ndistributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\ntoassignanyunitstocomputethesamefunctionaseachother.\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\nchosenconstantsmuchlikethebiasesare.\nWealmostalwaysinitializealltheweightsin\u00a0themodel\u00a0tovalues\u00a0drawn\nrandomly\u00a0froma\u00a0Gaussian\u00a0oruniform\u00a0distribution.Thechoiceof\u00a0Gaussian\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\nlargee\ufb00ectonboththeoutcomeoftheoptimization procedureandontheability\nofthenetworktogeneralize.\nLargerinitialweightswillyieldastrongersymmetrybreakinge\ufb00ect,helping\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor\nback-propagationthroughthelinearcomponentofeachlayer\u2014largervaluesinthe\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\nofthedeterministicforwardpropagationprocedureappearsrandom).\u00a0Tosome\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping\n(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\nTheperspectivesofregularizationandoptimization cangiveverydi\ufb00erent\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-\nfully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\n3 0 2", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nduetotriggeringsomeearlystoppingcriterionbasedonover\ufb01tting)expressesa\npriorthatthe\ufb01nalparametersshouldbeclosetotheinitialparameters.Recall\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\nthee\ufb00ectofinitialization. Wecanthinkofinitializingtheparameters\u03b8to\u03b8 0as\nbeingsimilartoimposingaGaussianprior p(\u03b8)withmean\u03b8 0.Fromthispoint\nofview,itmakessensetochoose\u03b8 0tobenear0.Thispriorsaysthatitismore\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\npreferenceforthemtointeract.Ontheotherhand,ifweinitialize\u03b8 0tolarge\nvalues,thenourpriorspeci\ufb01eswhichunitsshouldinteractwitheachother,and\nhowtheyshouldinteract.\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\nnoutputsbysamplingeachweightfrom U(\u22121\u221am,1\u221am),whileGlorotandBengio\n()suggestusingthe 2010 normalizedinitialization\nW i , j\u223c U\ue020\n\u2212\ue072\n6\nm n+,\ue072\n6\nm n+\ue021\n. (8.23)\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall\nlayerstohavethesamegradientvariance.Theformulaisderivedusingthe\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\nnonlinearcounterparts.\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied\nateachlayer.Theyderivespeci\ufb01cvaluesofthescalingfactorfordi\ufb00erenttypesof\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand\ngradientsincreaseinnormastheypropagatebackward.\u00a0 ()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissu\ufb03cienttotrainnetworksasdeepas\n3 0 3", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations.\u00a0A keyinsightof\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\nbehavior.Thisisbecausefeedforwardnetworksuseadi\ufb00erentweightmatrixat\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat\nariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\noptimalperformance.Thismaybeforthreedi\ufb00erentreasons.First,wemay\nbeusingthewrongcriteria\u2014itmaynotactuallybebene\ufb01cialtopreservethe\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently\nincreasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\nnotexactlyequaltothetheoreticalpredictions.\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\nsamestandarddeviation,suchas1\u221am,isthateveryindividualweightbecomes\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis\ninitializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\nvalues.Becauseittakesalongtimeforgradientdescenttoshrink\u201cincorrect\u201dlarge\nvalues,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\nthathaveseveral\ufb01ltersthatmustbecarefullycoordinatedwitheachother.\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor\nthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\nByrepeatedlyidentifyingthe\ufb01rstlayerwithunacceptably smallactivationsand\n3 0 4", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\nactivations.\u00a0Thisprocedurecaninprinciplebeautomatedandisgenerallyless\ncomputationally costlythanhyperparameter optimization basedonvalidationset\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona\nsinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeci\ufb01edmore\nformallyandstudiedby (). MishkinandMatas2015\nSo\u00a0far\u00a0we\u00a0have\u00a0focused\u00a0on\u00a0the\u00a0initialization\u00a0ofthe\u00a0weights.Fortunately,\ninitialization ofotherparametersistypicallyeasier.\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight\ninitialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\nnon-zerovalues:\n\u2022Ifabiasisforanoutputunit,thenitisoftenbene\ufb01cialtoinitializethebiasto\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\nonlybythebias.Thisjusti\ufb01essettingthebiastotheinverseoftheactivation\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\nForexample,iftheoutputisadistributionoverclassesandthisdistribution\nisahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassi\ufb01ersbutalsoto\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\nmatchthemarginaldistributionover.x\n\u2022Sometimeswemay\u00a0wanttochoosethebiastoavoidcausingtoo\u00a0much\nsaturationatinitialization. Forexample,wemaysetthebiasofaReLU\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample,\u00a0itisnot\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\n\u2022Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit\nh\u2208[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h.\u00a0We\n3 0 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h u\u2248or u h\u22480.\u00a0Inthese\nsituations,wewanttosetthebiasfor hsothat h\u22481mostofthetimeat\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\ntheLSTMmodel,describedinsection.10.10\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\nusingthemodel\np y y (| Nx) = (|wTx+1) b , /\u03b2 (8.24)\nwhere \u03b2isaprecisionparameter.Wecanusuallyinitializevarianceorprecision\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\nenoughtozerothatthebiasesmaybesetwhileignoringthee\ufb00ectoftheweights,\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-\nters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\no\ufb00ersfasterconvergencethanarandominitialization. Someoftheseinitialization\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey\nencodeinformationaboutthedistributionintheinitialparametersofthemodel.\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdi\ufb00erentunitstocomputedi\ufb00erentfunctionsfromeachother.\n8.5AlgorithmswithAdaptiveLearningRates\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\nofthehyperparameters thatisthemostdi\ufb03culttosetbecauseithasasigni\ufb01cant\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2\ncostisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\n3 0 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\nthecourseoflearning.\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,\nthenthelearningrateshoulddecrease.\u00a0Ofcourse,thiskindofrulecanonlybe\nappliedtofullbatchoptimization.\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrie\ufb02yreviewafewofthesealgorithms.\n8.5.1AdaGrad\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011\nparameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\nhavearelativelysmalldecreaseintheirlearningrate.Thenete\ufb00ectisgreater\nprogressinthemoregentlyslopeddirectionsofparameterspace.\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat\u2014for\ntrainingdeepneuralnetworkmodels\u2014theaccumulation ofsquaredgradientsfrom\nthebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\ne\ufb00ectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\nmodels.\n8.5.2RMSProp\nTheRMSPropalgorithm(,)modi\ufb01esAdaGradtoperformbetterin Hinton2012\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\ntoaconvexfunction.\u00a0When appliedtoanon-convexfunctiontotrainaneural\nnetwork,thelearningtrajectorymaypassthroughmanydi\ufb00erentstructuresand\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\n3 0 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.4TheAdaGradalgorithm\nRequire:Globallearningrate \ue00f\nRequire:Initialparameter\u03b8\nRequire:Smallconstant,perhaps \u03b4 10\u2212 7,fornumericalstability\nInitializegradientaccumulationvariabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nAccumulatesquaredgradient:rrgg \u2190+\ue00c\nComputeupdate: \u2206\u03b8\u2190\u2212\ue00f\n\u03b4 +\u221ar\ue00cg.(Divisionandsquarerootapplied\nelement-wise)\nApplyupdate:\u03b8\u03b8\u03b8 \u2190 +\u2206\nendwhile\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\nextremepastsothatitcanconvergerapidlyafter\ufb01ndingaconvexbowl,asifit\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6\nmovingaverageintroducesanewhyperparameter, \u03c1,thatcontrolsthelengthscale\nofthemovingaverage.\nEmpirically,RMSProphasbeenshowntobeane\ufb00ectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\n8.5.3Adam\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\nalgorithmandispresentedinalgorithm .Thename\u201cAdam\u201d\u00a0derivesfrom 8.7\nthephrase\u201cadaptivemoments.\u201dInthecontextoftheearlieralgorithms,itis\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\ndirectlyasanestimateofthe\ufb01rstordermoment(withexponentialweighting)of\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n3 0 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.5TheRMSPropalgorithm\nRequire:Globallearningrate,decayrate. \ue00f \u03c1\nRequire:Initialparameter\u03b8\nRequire:Smallconstant \u03b4,\u00a0usually 10\u2212 6,\u00a0usedtostabilizedivision\u00a0bysmall\nnumbers.\nInitializeaccumulation variablesr= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nAccumulatesquaredgradient:rrgg \u2190 \u03c1+(1 )\u2212 \u03c1\ue00c\nComputeparameterupdate: \u2206\u03b8=\u2212\ue00f\u221a\n\u03b4 + r\ue00cg.(1\u221a\n\u03b4 + rappliedelement-wise)\nApplyupdate:\u03b8\u03b8\u03b8 \u2190 +\u2206\nendwhile\nbiascorrectionstotheestimatesofboththe\ufb01rst-ordermoments(themomentum\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias\nearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\nthesuggesteddefault.\n8.5.4ChoosingtheRightOptimizationAlgorithm\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\nchoose?\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014\npresentedavaluablecomparisonofalargenumberofoptimization algorithms\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n3 0 9", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\nRequire:Globallearningrate,decayrate,momentumcoe\ufb03cient. \ue00f \u03c1 \u03b1\nRequire:Initialparameter,initialvelocity. \u03b8 v\nInitializeaccumulation variabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputeinterimupdate: \u02dc\u03b8\u03b8v \u2190 + \u03b1\nComputegradient:g\u21901\nm\u2207 \u02dc \u03b8\ue050\ni L f((x( ) i;\u02dc\u03b8y) ,( ) i)\nAccumulategradient:rrgg \u2190 \u03c1+(1 )\u2212 \u03c1\ue00c\nComputevelocityupdate:vv\u2190 \u03b1\u2212\ue00f\u221ar\ue00cg.(1\u221arappliedelement-wise)\nApplyupdate:\u03b8\u03b8v \u2190 +\nendwhile\nlargelyontheuser\u2019sfamiliaritywiththealgorithm(foreaseofhyperparameter\ntuning).\n8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\nrisk:\nJ() = \u03b8 E x ,y \u223c \u02c6 pdata ( ) x , y[((;))] = L fx\u03b8 , y1\nmm \ue058\ni = 1L f((x( ) i;)\u03b8 , y( ) i) .(8.25)\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\ndiscussedinchapter.7\n8.6.1Newton\u2019sMethod\nInsection,weintroducedsecond-ordergradientmethods.Incontrastto\ufb01rst- 4.3\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\noptimization. Themostwidelyusedsecond-ordermethodisNewton\u2019smethod.We\nnowdescribeNewton\u2019smethodinmoredetail,withemphasisonitsapplicationto\nneuralnetworktraining.\n3 1 0", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: ) \ue00f 0001 .\nRequire:Exponentialdecayratesformomentestimates, \u03c1 1and \u03c1 2in[0 ,1).\n(Suggesteddefaults:andrespectively) 09 . 0999 .\nRequire:Smallconstant \u03b4usedfornumericalstabilization.(Suggesteddefault:\n10\u2212 8)\nRequire:Initialparameters\u03b8\nInitialize1stand2ndmomentvariables ,s= 0r= 0\nInitializetimestep t= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nt t\u2190+1\nUpdatebiased\ufb01rstmomentestimate:s\u2190 \u03c1 1s+(1\u2212 \u03c1 1)g\nUpdatebiasedsecondmomentestimate:r\u2190 \u03c1 2r+(1\u2212 \u03c1 2)gg\ue00c\nCorrectbiasin\ufb01rstmoment:\u02c6s\u2190s\n1 \u2212 \u03c1t\n1\nCorrectbiasinsecondmoment:\u02c6r\u2190r\n1 \u2212 \u03c1t\n2\nComputeupdate: \u2206= \u03b8\u2212 \ue00f\u02c6s\u221a\n\u02c6 r + \u03b4(operationsappliedelement-wise)\nApplyupdate:\u03b8\u03b8\u03b8 \u2190 +\u2206\nendwhile\nNewton\u2019smethodisanoptimization schemebasedonusingasecond-orderTay-\nlorseriesexpansiontoapproximate J(\u03b8)nearsomepoint\u03b8 0,ignoringderivatives\nofhigherorder:\nJ J () \u03b8\u2248(\u03b8 0)+(\u03b8\u03b8\u2212 0)\ue03e\u2207 \u03b8 J(\u03b8 0)+1\n2(\u03b8\u03b8\u2212 0)\ue03eH\u03b8\u03b8 (\u2212 0) ,(8.26)\nwhereHistheHessianof Jwithrespectto\u03b8evaluatedat\u03b8 0.Ifwethensolvefor\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\n\u03b8\u2217= \u03b8 0\u2212H\u2212 1\u2207 \u03b8 J(\u03b8 0) (8.27)\nThusforalocallyquadraticfunction(withpositivede\ufb01niteH),byrescaling\nthegradientbyH\u2212 1,Newton\u2019smethodjumpsdirectlytotheminimum.\u00a0If the\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton\u2019s\nmethod,giveninalgorithm .8.8\n3 1 1", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.8Newton\u2019smethodwithobjective J(\u03b8) =\n1\nm\ue050m\ni = 1 L f((x( ) i;)\u03b8 , y( ) i).\nRequire:Initialparameter\u03b8 0\nRequire:Trainingsetofexamples m\nwhile do stoppingcriterionnotmet\nComputegradient:g\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nComputeHessian:H\u21901\nm\u22072\n\u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nComputeHessianinverse:H\u2212 1\nComputeupdate: \u2206= \u03b8\u2212H\u2212 1g\nApplyupdate:\u03b8\u03b8\u03b8 = +\u2206\nendwhile\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive\nde\ufb01nite,Newton\u2019smethodcanbeappliediteratively.Thisimpliesatwo-step\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\ningthequadraticapproximation).\u00a0Second, updatetheparametersaccordingto\nequation.8.27\nInsection,wediscussedhowNewton\u2019smethodisappropriateonlywhen 8.2.3\ntheHessianispositivede\ufb01nite.Indeeplearning,thesurfaceoftheobjective\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\nareproblematicforNewton\u2019smethod.\u00a0IftheeigenvaluesoftheHessianarenot\nallpositive,forexample,nearasaddlepoint,thenNewton\u2019smethodcanactually\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes \u03b1\n\u03b8\u2217= \u03b8 0\u2212[(( H f\u03b8 0))+ ] \u03b1I\u2212 1\u2207 \u03b8 f(\u03b8 0) . (8.28)\nThisregularizationstrategyisusedinapproximations toNewton\u2019smethod,such\nastheLevenberg\u2013Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and\nworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\nvalueof \u03b1wouldhavetobesu\ufb03cientlylargetoo\ufb00setthenegativeeigenvalues.\nHowever,as \u03b1increasesinsize,theHessianbecomesdominatedbythe \u03b1Idiagonal\nandthedirectionchosenbyNewton\u2019smethodconvergestothestandardgradient\ndividedby \u03b1.\u00a0Whenstrongnegativecurvatureispresent, \u03b1mayneedtobeso\nlargethatNewton\u2019smethodwouldmakesmallerstepsthangradientdescentwith\naproperlychosenlearningrate.\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n3 1 2", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsuchassaddlepoints,theapplicationofNewton\u2019smethodfortraininglargeneural\nnetworksislimitedbythesigni\ufb01cantcomputational burdenitimposes.The\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\nkcanbeinthemillions),Newton\u2019smethodwouldrequiretheinversionofa k k\u00d7\nmatrix\u2014with computational complexityof O( k3).Also,sincetheparameterswill\nchangewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\ncanbepracticallytrainedviaNewton\u2019smethod.Intheremainderofthissection,\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton\u2019s\nmethodwhileside-steppingthecomputational hurdles.\n8.6.2ConjugateGradients\nConjugategradientsisamethodtoe\ufb03cientlyavoidthecalculationoftheinverse\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis\napproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherine\ufb00ective\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\nsearchdirection.\nLettheprevioussearchdirectionbed t \u2212 1.Attheminimum,wheretheline\nsearchterminates,thedirectionalderivativeiszeroindirectiond t \u2212 1:\u2207 \u03b8 J(\u03b8)\u00b7\nd t \u2212 1=0.Sincethegradientatthispointde\ufb01nesthecurrentsearchdirection,\nd t=\u2207 \u03b8 J(\u03b8) willhavenocontributioninthedirectiond t \u2212 1.Thusd tisorthogonal\ntod t \u2212 1.Thisrelationshipbetweend t \u2212 1andd tisillustratedin\ufb01gurefor8.6\nmultipleiterationsofsteepestdescent.Asdemonstratedinthe\ufb01gure,thechoiceof\northogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\nseekstoaddressthisproblem.\nInthemethodofconjugategradients,weseekto\ufb01ndasearchdirectionthat\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\n3 1 3", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u0000\ue033 \ue030 \u0000\ue032 \ue030 \u0000\ue031 \ue030 \ue030 \ue031 \ue030 \ue032 \ue030\u0000\ue033 \ue030\u0000\ue032 \ue030\u0000\ue031 \ue030\ue030\ue031 \ue030\ue032 \ue030\nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\nmethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\nde\ufb01nedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\nseenwithusinga\ufb01xedlearningratein\ufb01gure,butevenwiththeoptimalstepsize 4.6\nthealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Byde\ufb01nition,at\ntheminimumoftheobjectivealongagivendirection,thegradientatthe\ufb01nalpointis\northogonaltothatdirection.\ntheform:\nd t= \u2207 \u03b8 J \u03b2 ()+\u03b8 td t \u2212 1 (8.29)\nwhere \u03b2 tisacoe\ufb03cientwhosemagnitudecontrolshowmuchofthedirection,d t \u2212 1,\nweshouldaddbacktothecurrentsearchdirection.\nTwodirections,d tandd t \u2212 1,arede\ufb01nedasconjugateifd\ue03e\ntHd t \u2212 1= 0,where\nHistheHessianmatrix.\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\neigenvectorsofHtochoose \u03b2 t,whichwouldnotsatisfyourgoalofdeveloping\namethodthatismorecomputationally viablethanNewton\u2019smethodforlarge\nproblems.\u00a0Canwecalculatetheconjugatedirectionswithoutresortingtothese\ncalculations?Fortunatelytheanswertothatisyes.\nTwopopularmethodsforcomputingthe \u03b2 tare:\n1.\u00a0Fletcher-Reeves:\n\u03b2 t=\u2207 \u03b8 J(\u03b8 t)\ue03e\u2207 \u03b8 J(\u03b8 t)\n\u2207 \u03b8 J(\u03b8 t \u2212 1)\ue03e\u2207 \u03b8 J(\u03b8 t \u2212 1)(8.30)\n3 1 4", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n2.\u00a0Polak-Ribi\u00e8re:\n\u03b2 t=(\u2207 \u03b8 J(\u03b8 t)\u2212\u2207 \u03b8 J(\u03b8 t \u2212 1))\ue03e\u2207 \u03b8 J(\u03b8 t)\n\u2207 \u03b8 J(\u03b8 t \u2212 1)\ue03e\u2207 \u03b8 J(\u03b8 t \u2212 1)(8.31)\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\nthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\nminimumalongthepreviousdirections.Asaconsequence,ina k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9\nAlgorithm8.9Theconjugategradientmethod\nRequire:Initialparameters\u03b8 0\nRequire:Trainingsetofexamples m\nInitialize\u03c1 0= 0\nInitialize g 0= 0\nInitialize t= 1\nwhile do stoppingcriterionnotmet\nInitializethegradientg t= 0\nComputegradient:g t\u21901\nm\u2207 \u03b8\ue050\ni L f((x( ) i;)\u03b8 ,y( ) i)\nCompute \u03b2 t=( g t \u2212 g t \u22121 )\ue03eg t\ng\ue03e\nt \u22121g t \u22121(Polak-Ribi\u00e8re)\n(Nonlinearconjugategradient:optionallyreset \u03b2 ttozero,forexampleif tis\namultipleofsomeconstant,suchas) k k= 5\nComputesearchdirection:\u03c1 t= \u2212g t+ \u03b2 t\u03c1 t \u2212 1\nPerformlinesearchto\ufb01nd: \ue00f\u2217= argmin \ue00f1\nm\ue050m\ni = 1 L f((x( ) i;\u03b8 t+ \ue00f\u03c1 t) ,y( ) i)\n(Onatrulyquadraticcostfunction,analyticallysolvefor \ue00f\u2217ratherthan\nexplicitlysearchingforit)\nApplyupdate:\u03b8 t + 1= \u03b8 t+ \ue00f\u2217\u03c1 t\nt t\u2190+1\nendwhile\nNonlinearConjugateGradients:Sofarwehavediscussedthemethodof\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.\u00a0Ofcourse,\nourprimaryinterestinthischapteristoexploreoptimization methodsfortraining\nneuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\nobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodi\ufb01cation.\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n3 1 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\nsearchalongtheunalteredgradient.\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbene\ufb01cialto\ninitializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\n2011).\u00a0Adaptationsofconjugategradientsspeci\ufb01callyforneuralnetworkshave\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\n1993).\n8.6.3BFGS\nTheBroyden\u2013Fletcher\u2013Goldfarb\u2013Shanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewton\u2019smethodwithoutthecomputational\nburden.In\u00a0thatrespect,\u00a0BFGSissimilartotheconjugategradientmethod.\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewton\u2019s\nupdate.RecallthatNewton\u2019supdateisgivenby\n\u03b8\u2217= \u03b8 0\u2212H\u2212 1\u2207 \u03b8 J(\u03b8 0) , (8.32)\nwhereHistheHessianof Jwithrespectto\u03b8evaluatedat\u03b8 0.Theprimary\ncomputational di\ufb03cultyinapplyingNewton\u2019supdateisthecalculationofthe\ninverseHessianH\u2212 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\namatrixM tthatisiterativelyre\ufb01nedbylowrankupdatestobecomeabetter\napproximationofH\u2212 1.\nThespeci\ufb01cationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984().\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\n\u03c1 tisdeterminedby\u03c1 t=M tg t.Alinesearchisperformedinthisdirectionto\ndeterminethesizeofthestep, \ue00f\u2217,takeninthisdirection.The\ufb01nalupdatetothe\nparametersisgivenby:\n\u03b8 t + 1= \u03b8 t+ \ue00f\u2217\u03c1 t . (8.33)\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\n3 1 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\nonthelinesearch\ufb01ndingapointveryclosetothetrueminimumalongtheline.\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\nlesstimere\ufb01ningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\nparameters.\nLimited\u00a0Memory\u00a0BFGS\u00a0(or\u00a0L-BFGS)The\u00a0memory costs\u00a0ofthe\u00a0BFGS\nalgorithmcanbesigni\ufb01cantlydecreasedbyavoidingstoringthecompleteinverse\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\nthatM( 1 ) t \u2212istheidentitymatrix,ratherthanstoringtheapproximation fromone\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsde\ufb01nedbyL-BFGS\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this\nprocedureremainswellbehavedwhentheminimumofthelinesearchisreached\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n8.7OptimizationStrategiesandMeta-Algorithms\nManyoptimization techniquesarenotexactlyalgorithms,\u00a0butrathergeneral\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\nincorporatedintomanydi\ufb00erentalgorithms.\n8.7.1BatchNormalization\nBatchnormalization ( ,)isoneofthemostexcitingrecent Io\ufb00eandSzegedy2015\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\nbythedi\ufb03cultyoftrainingverydeepmodels.\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother\nlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\n3 1 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\nanddoesnotuseanactivationfunctionateachhiddenlayer:\u02c6 y= x w 1 w 2 w 3 . . . w l.\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i \u2212 1 w i.\nTheoutput \u02c6 yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\nweights w i.Supposeourcostfunctionhasputagradientofon1 \u02c6 y,sowewishto\ndecrease\u02c6 yslightly.Theback-propagationalgorithmcanthencomputeagradient\ng=\u2207 w\u02c6 y.Considerwhathappenswhenwemakeanupdatewwg \u2190 \u2212 \ue00f.The\n\ufb01rst-orderTaylorseriesapproximation of\u02c6 ypredictsthatthevalueof\u02c6 ywilldecrease\nby \ue00fg\ue03eg.Ifwewantedtodecrease\u02c6 yby .1,this\ufb01rst-orderinformationavailablein\nthegradientsuggestswecouldsetthelearningrate \ue00fto. 1\ng\ue03eg.However,theactual\nupdatewillincludesecond-orderandthird-ordere\ufb00ects,onuptoe\ufb00ectsoforder l.\nThenewvalueof\u02c6 yisgivenby\nx w( 1\u2212 \ue00f g 1)( w 2\u2212 \ue00f g 2)( . . . w l\u2212 \ue00f g l) . (8.34)\nAnexampleofonesecond-ordertermarisingfromthisupdateis \ue00f2g 1 g 2\ue051l\ni = 3 w i.\nThistermmightbenegligibleif\ue051l\ni = 3 w iissmall,ormightbeexponentiallylarge\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\ntochooseanappropriatelearningrate,becausethee\ufb00ectsofanupdatetothe\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese\nsecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\nevenhigher-orderinteractionscanbesigni\ufb01cant.Evensecond-orderoptimization\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\nthemfromtrulyaccountingforallsigni\ufb01cantsecond-orderinteractions. Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead?\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep\nnetwork.Thereparametrization signi\ufb01cantlyreducestheproblemofcoordinating\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\nH\ue030=H\u00b5\u2212\n\u03c3, (8.35)\nwhere\u00b5isavectorcontainingthemeanofeachunitand\u03c3isavectorcontaining\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting\nthevector\u00b5andthevector\u03c3tobeappliedtoeveryrowofthematrixH.Within\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting \u00b5 j\n3 1 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nanddividingby \u03c3 j.TherestofthenetworkthenoperatesonH\ue030inexactlythe\nsamewaythattheoriginalnetworkoperatedon.H\nAttrainingtime,\n\u00b5=1\nm\ue058\niH i , : (8.36)\nand\n\u03c3=\ue073\n\u03b4+1\nm\ue058\ni( )H\u00b5\u22122\ni , (8.37)\nwhere \u03b4isasmallpositivevaluesuchas10\u2212 8imposedtoavoidencountering\ntheunde\ufb01nedgradientof\u221azat z=0.Crucially,\u00a0weback-propagatethrough\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose\nanoperation\u00a0that actssimplytoincreasethestandard\u00a0deviationormeanof\nh i;thenormalization operationsremovethee\ufb00ectofsuchanactionandzero\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\nnormalization approach.\u00a0Previous approacheshadinvolvedaddingpenaltiesto\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter\nusuallyresultedinsigni\ufb01cantwastedtimeasthelearningalgorithmrepeatedly\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\nunitsalwaysbestandardizedbyde\ufb01nition,deftlysidesteppingbothproblems.\nAttesttime,\u00b5and\u03c3maybereplacedbyrunningaveragesthatwerecollected\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\nwithoutneedingtousede\ufb01nitionsof\u00b5and\u03c3thatdependonanentireminibatch.\nRevisitingthe\u02c6 y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\ndi\ufb03cultiesinlearningthismodelbynormalizing h l \u2212 1.Supposethat xisdrawn\nfromaunitGaussian.Then h l \u2212 1willalsocomefromaGaussian,becausethe\ntransformationfrom xto h lislinear.However, h l \u2212 1willnolongerhavezeromean\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\n\u02c6h l \u2212 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany\nupdatetothelowerlayers,\u02c6h l \u2212 1willremainaunitGaussian.Theoutput \u02c6 ymay\nthenbelearnedasasimplelinearfunction \u02c6 y= w l\u02c6 h l \u2212 1.Learninginthismodelis\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\ne\ufb00ectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.\u00a0In\nsomecornercases,thelowerlayerscanhaveane\ufb00ect.Changingoneofthelower\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\n3 1 9", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofoneofthelowerweightscan\ufb02iptherelationshipbetween\u02c6 h l \u2212 1and y.\u00a0These\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\nanextremee\ufb00ectonthestatisticsof h l \u2212 1.Batchnormalization hasthusmade\nthismodelsigni\ufb01cantlyeasiertolearn.\u00a0Inthisexample,theeaseoflearningof\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\nthelowerlayersnolongerhaveanyharmfule\ufb00ect,buttheyalsonolongerhave\nanybene\ufb01ciale\ufb00ect.Thisisbecausewehavenormalizedoutthe\ufb01rstandsecond\norderstatistics,whichisallthatalinearnetworkcanin\ufb02uence.Inadeepneural\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\nunittochange.\nBecausethe\ufb01nallayerofthenetworkisabletolearnalineartransformation,\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\ntheinspirationforbatchnormalization. Unfortunately,\u00a0eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach.\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\npowerofthe\u00a0neuralnetworkcontainingthatunit.Inordertomaintainthe\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\nactivationsHwith\u03b3H\ue030+\u03b2ratherthansimplythenormalizedH\ue030.Thevariables\n\u03b3and\u03b2arelearnedparametersthatallowthenewvariabletohaveanymean\nandstandarddeviation.At\ufb01rstglance,thismayseemuseless\u2014whydidweset\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto\nanyarbitraryvalue\u03b2?Theansweristhatthenewparametrization canrepresent\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\nparametrization hasdi\ufb00erentlearningdynamics.Intheoldparametrization, the\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\ninthelayersbelowH.Inthenewparametrization, themeanof\u03b3H\ue030+\u03b2is\ndeterminedsolelyby\u03b2.Thenewparametrization ismucheasiertolearnwith\ngradientdescent.\nMostneuralnetworklayerstaketheformof \u03c6(XW+b)where \u03c6issome\n\ufb01xednonlinearactivationfunctionsuchastherecti\ufb01edlineartransformation.It\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\nX,ortothetransformedvalueXW+b. ()recommend Io\ufb00eandSzegedy2015\n3 2 0", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthelatter.Morespeci\ufb01cally,XW+bshouldbereplacedbyanormalizedversion\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\nthe \u03b2parameterappliedbythebatchnormalization reparametrization. Theinput\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\nrecti\ufb01edlinearfunctioninapreviouslayer.\u00a0Thestatisticsoftheinputarethus\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.\nInconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\nsamenormalizing \u00b5and \u03c3ateveryspatiallocationwithinafeaturemap,sothat\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\n8.7.2CoordinateDescent\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\nvariable x i,\u00a0then\u00a0minimize\u00a0it with\u00a0respect\u00a0to\u00a0another variable x jand\u00a0soon,\nrepeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\nonecoordinateatatime.\u00a0Moregenerally,blockcoordinatedescentrefersto\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\n\u201ccoordinatedescent\u201disoftenusedtorefertoblockcoordinatedescentaswellas\nthestrictlyindividualcoordinatedescent.\nCoordinatedescentmakesthemostsensewhenthedi\ufb00erentvariablesinthe\noptimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\nsigni\ufb01cantlymoree\ufb03cientthanoptimization withrespecttoallofthevariables.\nForexample,considerthecostfunction\nJ ,(HW) =\ue058\ni , j| H i , j|+\ue058\ni , j\ue010\nXW\u2212\ue03eH\ue0112\ni , j.(8.38)\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\nto\ufb01ndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues\nHtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\nThefunction Jisnotconvex.However,\u00a0wecandividetheinputstothe\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n3 2 1", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nusanoptimization strategythatallowsustousee\ufb03cientconvexoptimization\nalgorithms,byalternatingbetweenoptimizingWwithH\ufb01xed,thenoptimizing\nHWwith\ufb01xed.\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\nstronglyin\ufb02uencestheoptimalvalueofanothervariable,asinthefunction f(x) =\n( x 1\u2212 x 2)2+ \u03b1\ue000\nx2\n1+ x2\n2\ue001\nwhere \u03b1isapositiveconstant.The\ufb01rsttermencourages\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem\ntobenearzero.Thesolutionistosetbothtozero.Newton\u2019smethodcansolve\ntheprobleminasinglestepbecauseitisapositivede\ufb01nitequadraticproblem.\nHowever,forsmall \u03b1,coordinatedescentwillmakeveryslowprogressbecausethe\n\ufb01rsttermdoesnotallowasinglevariabletobechangedtoavaluethatdi\ufb00ers\nsigni\ufb01cantlyfromthecurrentvalueoftheothervariable.\n8.7.3PolyakAveraging\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\npoints\u00a0inthe\u00a0trajectory\u00a0through parameter\u00a0spacevisited\u00a0by\u00a0anoptimization\nalgorithm.\u00a0If titerationsofgradientdescentvisitpoints\u03b8( 1 ), . . . ,\u03b8( ) t,thenthe\noutputofthePolyakaveragingalgorithmis\u02c6\u03b8( ) t=1\nt\ue050\ni\u03b8( ) i.\u00a0Onsomeproblem\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjusti\ufb01cation\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof\nthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\nverycomplicatedandvisitmanydi\ufb00erentregions.Includingpointsinparameter\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\nexponentiallydecayingrunningaverage:\n\u02c6\u03b8( ) t= \u03b1\u02c6\u03b8( 1 ) t \u2212+(1 )\u2212 \u03b1\u03b8( ) t. (8.39)\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\netal.()forarecentexample. 2015\n3 2 2", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.4SupervisedPretraining\nSometimes,directlytrainingamodeltosolveaspeci\ufb01ctaskcanbetooambitious\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydi\ufb03cult.Itis\nsometimesmoree\ufb00ectivetotrainasimplermodeltosolvethetask,thenmake\nthemodelmorecomplex.Itcanalsobemoree\ufb00ectivetotrainthemodeltosolve\nasimplertask,thenmoveontoconfrontthe\ufb01naltask.Thesestrategiesthat\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof\ntrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\npretraining.\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya\n\ufb01ne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\n\ufb01nds.\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\ndeeplearning.Inthissection,wedescribespeci\ufb01callythosepretrainingalgorithms\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\nproblems.Thisapproachisknownas . greedysupervisedpretraining\nIntheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\nthelayersinthe\ufb01nalneuralnetwork.Anexampleofgreedysupervisedpretraining\nisillustratedin\ufb01gure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015\nthe\ufb01rstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\nWhy\u00a0would\u00a0greedy\u00a0sup ervised\u00a0pretraining help?The\u00a0hypothesis \u00a0initially\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n3 2 3", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ny y\nh( 1 )h( 1 )\nx x\n( a )U( 1 )U( 1 )\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\nx x\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\ny yh( 1 )h( 1 )\nx x\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\ny yh( 1 )h( 1 )\nx x\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\nU( 2 )U( 2 )\nW( 2 )W( 2 )\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\n( a )Westartbytrainingasu\ufb03cientlyshallowarchitecture.Anotherdrawingofthe ( b )\nsamearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\ndiscardthehidden-to-outputlayer.Wesendtheoutputofthe\ufb01rsthiddenlayerasinput\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\nasthe\ufb01rstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\nTofurtherimprovetheoptimization,wecanjointly\ufb01ne-tuneallthelayers,eitheronlyat\ntheendorateachstageofthisprocess.\n3 2 4", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization.\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\nandtheninitializeasame-sizenetworkwiththe\ufb01rst klayersofthe\ufb01rstnet.All\nthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\nthenjointlytrainedtoperformadi\ufb00erentsetoftasks(anothersubsetofthe1000\nImageNetobjectcategories),withfewertrainingexamplesthanforthe\ufb01rstsetof\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\nsection.15.2\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen\nbecomesateacherforasecondnetwork,designatedthestudent.Thestudent\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\ndi\ufb03culttotrainwithSGDundernormalcircumstances.Thetrainingofthe\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional\nparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\nthe\ufb01nalclassi\ufb01cationtarget,theobjectiveistopredictthemiddlehiddenlayer\noftheteachernetwork.\u00a0Thelowerlayersofthestudentnetworksthushavetwo\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\nanddeepnetworkappearstobemoredi\ufb03culttotrainthanawideandshallow\nnetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdi\ufb03cultto\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\nsolvetheproblem.\n3 2 5", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.5DesigningModelstoAidOptimization\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\ncomefromdesigningthemodelstobeeasiertooptimize.\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\ndi\ufb03cult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto\noptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\nSpeci\ufb01cally,modernneuralnetworksre\ufb02ectadesignchoicetouselineartrans-\nformationsbetweenlayersandactivationfunctionsthataredi\ufb00erentiable almost\neverywhereandhavesigni\ufb01cantslopeinlargeportionsoftheirdomain.\u00a0Inpar-\nticular,modelinnovationsliketheLSTM,recti\ufb01edlinearunitsandmaxoutunits\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\noptimization easier.Thegradient\ufb02owsthroughmanylayersprovidedthatthe\nJacobianofthelineartransformationhasreasonablesingularvalues.\u00a0Moreover,\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel\u2019s\noutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\ncorrespondsreasonablywelltomovingtowardadistantsolution.\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\ntheshortestpathfromthelower\u00a0layer\u2019sparameters\u00a0totheoutput,\u00a0and thus\nmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\nanddeeply-supervisednets(,).These\u201cauxiliaryheads\u201daretrained Leeetal.2014\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\ntheauxiliaryheadsmaybediscarded.\u00a0Thisisanalternativetothepretraining\nstrategies,whichwereintroducedintheprevioussection.Inthisway,onecan\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n3 2 6", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\n8.7.6ContinuationMethodsandCurriculumLearning\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\nproblemistoattempttoinitializetheparametersinaregionthatisconnected\ntothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\ndiscover.\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\nminimizeacostfunction J(\u03b8),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.\nThesecostfunctionsaredesignedtobeincreasinglydi\ufb03cult,with J( 0 )beingfairly\neasytominimize,and J( ) n,themostdi\ufb03cult,being J(\u03b8),thetruecostfunction\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\nmeanthatitiswellbehavedovermoreof\u03b8space.Arandominitialization ismore\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby\nsolvinganeasyproblemthenre\ufb01nethesolutiontosolveincrementally harder\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\nwhichaddsnoisetotheparameters(Kirkpatrick\u00a01983etal.,).Continuation\nmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\novercomingthechallengeoflocalminima.Speci\ufb01cally,theyweredesignedto\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\nthesecontinuationmethodswouldconstructeasiercostfunctionsby\u201cblurring\u201dthe\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating\nJ( ) i() = \u03b8 E\u03b8\ue030\u223c N ( \u03b8\ue030; \u03b8 , \u03c3()2 i) J(\u03b8\ue030) (8.40)\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\n3 2 7", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\nenoughinformationaboutthelocationofaglobalminimumthatwecan\ufb01ndthe\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\napproachcanbreakdowninthreedi\ufb00erentways.First,itmightsuccessfullyde\ufb01ne\naseriesofcostfunctionswherethe\ufb01rstisconvexandtheoptimumtracksfrom\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso\nmanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(\u03b8) =\u2212\u03b8\ue03e\u03b8.\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe\noriginalcostfunction.\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\neliminate\ufb02atregions,decreasevarianceingradientestimates,improveconditioning\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates\neasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\nandprogresstowardaglobalsolution.\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal\ntraining(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\nlearning(,;,;,). () Solomono\ufb001989Elman1993Sanger1994Bengioetal.2009\njusti\ufb01edthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\nincreasingthein\ufb02uenceofsimplerexamples(eitherbyassigningtheircontributions\ntothecostfunctionlargercoe\ufb03cients,orbysamplingthemmorefrequently),and\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning\nhasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\ntasks.Curriculumlearningwasalsoveri\ufb01edasbeingconsistentwiththewayin\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\n3 2 8", "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmoreprototypicalexamplesandthenhelpthelearnerre\ufb01nethedecisionsurface\nwiththelessobviouscases.Curriculum-based strategiesaremoree\ufb00ectivefor\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\nalsoincreasethee\ufb00ectivenessofotherteachingstrategies( , BasuandChristensen\n2013).\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:\nZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\nstochasticcurriculum,inwhicharandommixofeasyanddi\ufb03cultexamplesisalways\npresentedtothelearner,butwheretheaverageproportionofthemoredi\ufb03cult\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\nfromthefulltrainingset)wasobserved.\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto\nregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\nlittleornomodi\ufb01cation.\n3 2 9", "C h a p t e r 9\nC on v ol u t i on al N e t w orks\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\ntremendouslysuccessfulinpracticalapplications.Thename\u201cconvolutionalneural\nnetwork\u201dindicatesthatthenetworkemploysamathematical operationcalled\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\nmultiplicationinatleastoneoftheirlayers.\nInthis\u00a0chapter,\u00a0wewill\ufb01rst\u00a0describewhatconvolutionis.Next,\u00a0wewill\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks\nemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\ncorrespondpreciselytothede\ufb01nitionofconvolutionasusedinother\ufb01eldssuch\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\nwillalso\u00a0show\u00a0how\u00a0convolutionmaybeappliedtomanykindsofdata,\u00a0with\ndi\ufb00erentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\nmoree\ufb03cient.Convolutionalnetworksstandoutasanexampleofneuroscienti\ufb01c\nprinciplesin\ufb02uencingdeeplearning.Wewilldiscusstheseneuroscienti\ufb01cprinciples,\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\n330", "CHAPTER9.CONVOLUTIONALNETWORKS\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\nhere.\n9.1TheConvolutionOperation\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-\nvaluedargument.Tomotivatethede\ufb01nitionofconvolution,westartwithexamples\noftwofunctionswemightuse.\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\nt.Both xand tarereal-valued,i.e.,wecangetadi\ufb00erentreadingfromthelaser\nsensoratanyinstantintime.\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\nestimateofthespaceship\u2019sposition,wewouldliketoaveragetogetherseveral\nmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\ns t() =\ue05a\nx a w t a d a ()( \u2212) (9.1)\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\ndenotedwithanasterisk:\ns t x w t () = ( \u2217)() (9.2)\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisde\ufb01ned\nforanyfunctionsforwhichtheaboveintegralisde\ufb01ned,andmaybeusedforother\npurposesbesidestakingweightedaverages.\nInconvolutionalnetworkterminology,the\ufb01rstargument(inthisexample,the\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\n3 3 1", "CHAPTER9.CONVOLUTIONALNETWORKS\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\nreferredtoasthe . f e at ur e m ap\nInourexample,theideaofalasersensorthatcanprovidemeasurements\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly\nintegervalues.Ifwenowassumethat xand warede\ufb01nedonlyoninteger t,we\ncande\ufb01nethediscreteconvolution:\ns t x w t () = ( \u2217)() =\u221e\ue058\na = \u2212 \u221ex a w t a ()( \u2212) (9.3)\nInmachinelearningapplications,theinputisusuallyamultidimensional array\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored\nseparately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\n\ufb01nitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\ncanimplementthein\ufb01nitesummationasasummationovera\ufb01nitenumberof\narrayelements.\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\ntouseatwo-dimensionalkernel: K\nS i , j I K i , j () = ( \u2217)() =\ue058\nm\ue058\nnI m , n K i m , j n . ( )( \u2212 \u2212)(9.4)\nConvolutioniscommutative,meaningwecanequivalentlywrite:\nS i , j K I i , j () = ( \u2217)() =\ue058\nm\ue058\nnI i m , j n K m , n . ( \u2212 \u2212)( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand. n\nThecommutativepropertyofconvolutionarisesbecausewehave \ufb02i pp e dthe\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe\ninputincreases,buttheindexintothekerneldecreases.Theonlyreasonto\ufb02ip\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n3 3 2", "CHAPTER9.CONVOLUTIONALNETWORKS\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\nbutwithout\ufb02ippingthekernel:\nS i , j I K i , j () = ( \u2217)() =\ue058\nm\ue058\nnI i m , j n K m , n . (+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,\nandspecifywhetherwemeanto\ufb02ipthekernelornotincontextswherekernel\n\ufb02ippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\nbasedonconvolutionwithkernel\ufb02ippingwilllearnakernelthatis\ufb02ippedrelative\ntothekernellearnedbyanalgorithmwithoutthe\ufb02ipping.Itisalsorarefor\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes\nnotcommuteregardlessofwhethertheconvolutionoperation\ufb02ipsitskernelor\nnot.\nSee\ufb01gureforanexampleofconvolution(withoutkernel\ufb02ipping)applied 9.1\ntoa2-Dtensor.\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z\nm at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\nmatrixmultiplication anddoesnotdependonspeci\ufb01cpropertiesofthematrix\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges\ntotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\nfurtherspecializationsinordertodealwithlargeinputse\ufb03ciently,buttheseare\nnotstrictlynecessaryfromatheoreticalperspective.\n3 3 3", "CHAPTER9.CONVOLUTIONALNETWORKS\na b c d\ne f g h\ni j k lw x\ny z\na w + b x +\ne y + f za w + b x +\ne y + f zb w + c x +\nf y + g zb w + c x +\nf y + g zc w + d x +\ng y + h zc w + d x +\ng y + h z\ne w + f x +\ni y + j ze w + f x +\ni y + j zf w + g x +\nj y + k zf w + g x +\nj y + k zg w + h x +\nk y + l zg w + h x +\nk y + l zI nput\nK e r ne l\nO ut put\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-\ufb02ipping.Inthiscasewerestrict\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called\u201cvalid\u201d\nconvolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\nupper-leftregionoftheinputtensor.\n3 3 4", "CHAPTER9.CONVOLUTIONALNETWORKS\n9.2Motivation\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\nr e pr e se n t at i o ns.Moreover,\u00a0convolutionprovidesameansforworkingwith\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput\nunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof\npixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\nmemoryrequirementsofthemodelandimprovesitsstatisticale\ufb03ciency.Italso\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\nine\ufb03ciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\nmatrixmultiplication requires m n \u00d7parametersandthealgorithmsusedinpractice\nhave O( m n \u00d7)runtime(perexample).Ifwelimitthenumberofconnections\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly\nk n \u00d7parametersand O( k n \u00d7)runtime.Formanypracticalapplications,itis\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\nkseveralordersofmagnitudesmallerthan m.\u00a0Forgraphicaldemonstrationsof\nsparseconnectivity,see\ufb01gureand\ufb01gure.Inadeepconvolutionalnetwork, 9.2 9.3\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\nasshownin\ufb01gure.Thisallowsthenetworktoe\ufb03cientlydescribecomplicated 9.4\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple\nbuildingblocksthateachdescribeonlysparseinteractions.\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In\naconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\noftheinput(exceptperhapssomeoftheboundarypixels,\u00a0dependingonthe\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n3 3 5", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\nandalsohighlighttheoutputunitsin sthatarea\ufb00ectedbythisunit. ( T o p )When sis\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsarea\ufb00ectedby 3 x.\n( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\nalloftheoutputsarea\ufb00ectedby x 3.\n3 3 6", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : \u00a0Wehighlightoneoutputunit, s 3,\nandalsohighlighttheinputunitsin xthata\ufb00ectthisunit.Theseunitsareknown\nasthereceptive\ufb01eldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof\nwidth,onlythreeinputsa\ufb00ect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsa\ufb00ect s 3.\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\nx 4 x 4h 4 h 4\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\nFigure9.4:Thereceptive\ufb01eldoftheunitsinthedeeperlayersofaconvolutionalnetwork\nislargerthanthereceptive\ufb01eldoftheunitsintheshallowlayers.Thise\ufb00ectincreasesif\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(\ufb01gure)orpooling 9.12\n(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\ninputimage.\n3 3 7", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\nparameterintwodi\ufb00erentmodels.\u00a0 ( T o p )Theblackarrowsindicateusesofthecentral\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this\nsingleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\nhasnoparametersharingsotheparameterisusedonlyonce.\nforeverylocation,welearnonlyoneset.Thisdoesnota\ufb00ecttheruntimeof\nforwardpropagation\u2014it isstill O( k n \u00d7)\u2014butitdoesfurtherreducethestorage\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis\npracticallyinsigni\ufb01cantcomparedto m n \u00d7.Convolutionisthusdramatically more\ne\ufb03cientthandensematrixmultiplication intermsofthememoryrequirements\nandstatisticale\ufb03ciency.Foragraphicaldepictionofhowparametersharingworks,\nsee\ufb01gure.9.5\nAsanexampleofbothofthese\ufb01rsttwoprinciplesinaction,\ufb01gureshows9.6\nhowsparseconnectivityandparametersharingcandramatically improvethe\ne\ufb03ciencyofalinearfunctionfordetectingedgesinanimage.\nInthecaseofconvolution,theparticularformofparametersharingcausesthe\nlayertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\nSpeci\ufb01cally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\n3 3 8", "CHAPTER9.CONVOLUTIONALNETWORKS\nmappingoneimagefunctiontoanotherimagefunction,suchthat I\ue030= g( I)is\ntheimagefunctionwith I\ue030( x , y)= I( x \u22121 , y).Thisshiftseverypixelof Ione\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\ntheresultwillbethesameasifweappliedconvolutionto I\ue030,thenappliedthe\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\nthatconvolutionproducesasortoftimelinethatshowswhendi\ufb00erentfeatures\nappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin\nthe\ufb01rstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\nonanindividual\u2019sface,weprobablywanttoextractdi\ufb00erentfeaturesatdi\ufb00erent\nlocations\u2014thepartofthenetworkprocessingthetopofthefaceneedstolookfor\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\nlookforachin.\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\nforhandlingthesekindsoftransformations.\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksde\ufb01nedby\nmatrixmultiplication witha\ufb01xed-shapematrix.Convolutionenablesprocessing\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n9.3Pooling\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(see\ufb01gure).9.7\nInthe\ufb01rststage,thelayerperformsseveralconvolutionsinparalleltoproducea\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\nanonlinearactivationfunction,suchastherecti\ufb01edlinearactivationfunction.\nThisstageissometimescalledthe det e c t o rstage.\u00a0Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\n3 3 9", "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.6: E \ufb03 c i e n c y o f e d g e d e t e c t i o n.\u00a0Theimageontherightwasformedbytaking\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\nleft.\u00a0Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and\nrequires319 \u00d7280 \u00d73=267 ,960\ufb02oatingpointoperations(twomultiplicationsand\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\ntransformationwithamatrixmultiplicationwouldtake320 \u00d7280 \u00d7319 \u00d7280,orover\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoree\ufb03cientfor\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\nperformsoversixteenbillion\ufb02oatingpointoperations,makingconvolutionroughly60,000\ntimesmoree\ufb03cientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\nandconvolutionwouldrequirethesamenumberof\ufb02oatingpointoperationstocompute.\nThematrixwouldstillneedtocontain2 \u00d7319 \u00d7280=178 ,640entries.Convolution\nisanextremelye\ufb03cientwayofdescribingtransformationsthatapplythesamelinear\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\nGoodfellow)\n3 4 0", "CHAPTER9.CONVOLUTIONALNETWORKS\nConvolutional\u00a0Layer\nInput\u00a0to\u00a0layerConvolution\u00a0stage:\nAne\u00a0transform \ufb03Detector\u00a0stage:\nNonlinearity\ne.g.,\u00a0recti\ufb01ed\u00a0linearPooling\u00a0stageNext\u00a0layer\nInput\u00a0to\u00a0layersConvolution\u00a0layer:\nAne\u00a0transform\u00a0 \ufb03Detector\u00a0layer:\u00a0Nonlinearity\ne.g.,\u00a0recti\ufb01ed\u00a0linearPooling\u00a0layerNext\u00a0layerComplex\u00a0layer\u00a0terminology Simple\u00a0layer\u00a0terminology\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo\ncommonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\neachlayerhavingmany\u201cstages.\u201dInthisterminology,thereisaone-to-onemapping\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\nnotevery\u201clayer\u201dhasparameters.\n3 4 1", "CHAPTER9.CONVOLUTIONALNETWORKS\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\nbasedonthedistancefromthecentralpixel.\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled\noutputsdonotchange.See\ufb01gureforanexampleofhowthisworks. 9.8 Invariance\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\nimportanttopreservethelocationofafeature.Forexample,ifwewantto\ufb01nda\ncornerde\ufb01nedbytwoedgesmeetingataspeci\ufb01corientation,weneedtopreserve\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\nTheuseofpoolingcanbeviewedasaddinganin\ufb01nitelystrongpriorthat\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\nassumptioniscorrect,itcangreatlyimprovethestatisticale\ufb03ciencyofthenetwork.\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn\nwhichtransformationstobecomeinvariantto(see\ufb01gure).9.9\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\n\ufb01gureforanexample.Thisimprovesthecomputational e\ufb03ciencyofthe 9.10\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas\nwhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\nreductionintheinputsizecanalsoresultinimprovedstatisticale\ufb03ciencyand\nreducedmemoryrequirementsforstoringtheparameters.\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize.\u00a0For\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassi\ufb01cation\nlayermusthavea\ufb01xedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\no\ufb00setbetweenpoolingregionssothattheclassi\ufb01cationlayeralwaysreceivesthe\nsamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\n\ufb01nalpoolinglayerofthenetworkmaybede\ufb01nedtooutputfoursetsofsummary\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\n3 4 2", "CHAPTER9.CONVOLUTIONALNETWORKS\n0. 1 1. 0. 21. 1. 1.\n0. 10. 2\n. . . . . .. . . . . .\n0. 3 0. 1 1.1. 0. 3 1.\n0. 21.\n. . . . . .. . . . . .D E T E C T O R \u00a0 S T A GEP O O L I N G\u00a0 ST A GE\nP O O L I N G\u00a0 ST A GE\nD E T E C T O R \u00a0 S T A GE\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions\nandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\n3 4 3", "CHAPTER9.CONVOLUTIONALNETWORKS\nL ar ge \u00a0 r e s pon s e\ni n\u00a0 po ol i ng\u00a0uni tL ar ge \u00a0 r e s pon s e\ni n\u00a0 po ol i ng\u00a0uni t\nL ar ge\nr e s ponse\ni n\u00a0 de t e c t or\nuni t \u00a0 1L ar ge\nr e s ponse\ni n\u00a0 de t e c t or\nuni t \u00a0 3\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\ntheinput.Hereweshowhowasetofthreelearned\ufb01ltersandamaxpoolingunitcanlearn\ntobecomeinvarianttorotation.Allthree\ufb01ltersareintendedtodetectahand-written5.\nEach\ufb01lterattemptstomatchaslightlydi\ufb00erentorientationofthe5.Whena5appearsin\ntheinput,thecorresponding\ufb01lterwillmatchitandcausealargeactivationinadetector\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\nwasactivated.Weshowherehowthenetworkprocessestwodi\ufb00erentinputs,resulting\nintwodi\ufb00erentdetectorunitsbeingactivated.Thee\ufb00ectonthepoolingunitisroughly\nthesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\ntransformations.\n0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0. 1\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor\noftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\nwanttoignoresomeofthedetectorunits.\n3 4 4", "CHAPTER9.CONVOLUTIONALNETWORKS\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\ndi\ufb00erentsetofpoolingregionsforeachimage.Anotherapproachistolearna\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012\nPoolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection.\u00a0The20.6\ninverse-likeoperationsonpoolingunitsneededinsomedi\ufb00erentiablenetworkswill\nbecoveredinsection.20.10.6\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassi\ufb01cation\nusingconvolutionandpoolingareshownin\ufb01gure.9.11\n9.4Convolutionand\u00a0Pooling\u00a0asan\u00a0In\ufb01nitelyStrong\nPrior\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh\nentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\nmoreactiveroleindeterminingwheretheparametersendup.\nAnin\ufb01nitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues.\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,\nbutwithanin\ufb01nitelystrongprioroveritsweights.Thisin\ufb01nitelystrongprior\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\nexceptforinthesmall,spatiallycontiguousreceptive\ufb01eldassignedtothathidden\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganin\ufb01nitely\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n3 4 5", "CHAPTER9.CONVOLUTIONALNETWORKS\nInput\u00a0image:\u00a0\n256x256x3Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a0256x256x64Output\u00a0of\u00a0pooling\u00a0\nwith\u00a0stride\u00a04:\u00a0\n64x64x64Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a064x64x64Output\u00a0of\u00a0pooling\u00a0\nwith\u00a0stride\u00a04:\u00a0\n16x16x64Output\u00a0of\u00a0reshape\u00a0to\u00a0\nvector:\n16,384\u00a0unitsOutput\u00a0of\u00a0matrix\u00a0\nmultiply:\u00a01,000\u00a0unitsOutput\u00a0of\u00a0softmax:\u00a0\n1,000\u00a0class\u00a0\nprobabilities\nInput\u00a0image:\u00a0\n256x256x3Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a0256x256x64Output\u00a0of\u00a0pooling\u00a0\nwith\u00a0stride\u00a04:\u00a0\n64x64x64Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a064x64x64Output\u00a0of\u00a0pooling\u00a0to\u00a0\n3x3\u00a0grid:\u00a03x3x64Output\u00a0of\u00a0reshape\u00a0to\u00a0\nvector:\n576\u00a0unitsOutput\u00a0of\u00a0matrix\u00a0\nmultiply:\u00a01,000\u00a0unitsOutput\u00a0of\u00a0softmax:\u00a0\n1,000\u00a0class\u00a0\nprobabilities\nInput\u00a0image:\u00a0\n256x256x3Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a0256x256x64Output\u00a0of\u00a0pooling\u00a0\nwith\u00a0stride\u00a04:\u00a0\n64x64x64Output\u00a0of\u00a0\nconvolution\u00a0+\u00a0\nReLU:\u00a064x64x64Output\u00a0of\u00a0\nconvolution:\n16x16x1,000Output\u00a0of\u00a0average\u00a0\npooling:\u00a01x1x1,000Output\u00a0of\u00a0softmax:\u00a0\n1,000\u00a0class\u00a0\nprobabilities\nOutput\u00a0of\u00a0pooling\u00a0\nwith\u00a0stride\u00a04:\u00a0\n16x16x64\nFigure9.11:Examplesofarchitecturesforclassi\ufb01cationwithconvolutionalnetworks.The\nspeci\ufb01cstridesanddepthsusedinthis\ufb01gurearenotadvisableforrealuse;theyare\ndesignedtobeveryshallowinorderto\ufb01tontothepage.\u00a0Realconvolutionalnetworks\nalsoofteninvolvesigni\ufb01cantamountsofbranching,unlikethechainstructuresused\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesa\ufb01xedimagesize.\nAfteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\nconvolutionalfeaturemapisreshapedto\ufb02attenoutthespatialdimensions.Therest\nofthenetworkisanordinaryfeedforwardnetworkclassi\ufb01er,asdescribedinchapter.6\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\nbuta\ufb01xednumberofpools,inordertoprovidea\ufb01xed-sizevectorof576unitstothe\nfullyconnectedportionofthenetwork.\u00a0Aconvolutionalnetworkthatdoesnot ( R i g h t )\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\ntheargumenttothesoftmaxclassi\ufb01eratthetop.\n3 4 6", "CHAPTER9.CONVOLUTIONALNETWORKS\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\nequivarianttotranslation.Likewise,theuseofpoolingisanin\ufb01nitelystrongprior\nthateachunitshouldbeinvarianttosmalltranslations.\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\nin\ufb01nitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\nofaconvolutionalnetasafullyconnectednetwithanin\ufb01nitelystrongpriorcan\ngiveussomeinsightsintohowconvolutionalnetswork.\nOnekeyinsightisthatconvolutionandpoolingcancauseunder\ufb01tting. Like\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth\nhighlyinvariantfeaturesandfeaturesthatwillnotunder\ufb01twhenthetranslation\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\ninappropriate.\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there\nareseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\nofspatialrelationshipshard-codedintothembytheirdesigner.\n9.5VariantsoftheBasicConvolutionFunction\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\nunderstoodinthemathematical literature.Thefunctionsusedinpracticedi\ufb00er\nslightly.Herewedescribethesedi\ufb00erencesindetail,andhighlightsomeuseful\npropertiesofthefunctionsusedinneuralnetworks.\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\nnetworktoextractmanykindsoffeatures,atmanylocations.\n3 4 7", "CHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations.\u00a0Forexample,acolorimagehasared,green\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\ntothesecondlayeristheoutputofthe\ufb01rstlayer,whichusuallyhastheoutput\nofmanydi\ufb00erentconvolutionsateachposition.Whenworkingwithimages,we\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with\noneindexintothedi\ufb00erentchannelsandtwoindicesintothespatialcoordinates\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdi\ufb00erentexamplesin\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\nkernel-\ufb02ippingisused.Thesemulti-channeloperationsareonlycommutativeif\neachoperationhasthesamenumberofoutputchannelsasinputchannels.\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\ninput,withano\ufb00setof krowsand lcolumnsbetweentheoutputunitandthe\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K\nacrosswithout\ufb02ipping,then V K\nZ i , j , k=\ue058\nl , m , nV l , j m , k n + \u2212 1 + \u2212 1 K i , l , m , n (9.7)\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\narraysusingaforthe\ufb01rstentry.Thisnecessitatesthe 1 \u22121intheaboveformula.\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler.\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe\ncomputational cost(attheexpenseofnotextractingourfeaturesas\ufb01nely).We\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\nde\ufb01neadownsampledconvolutionfunctionsuchthat c\nZ i , j , k= ( ) c K V , , s i , j , k=\ue058\nl , m , n\ue002\nVl , j s m , k s n ( \u2212 \u00d7 1 ) + ( \u2212 \u00d7 1 ) + K i , l , m , n\ue003\n.(9.8)\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\n3 4 8", "CHAPTER9.CONVOLUTIONALNETWORKS\ntode\ufb01neaseparatestrideforeachdirectionofmotion.See\ufb01gureforan9.12\nillustration.\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\nateachlayer.\u00a0Zeropaddingtheinputallowsustocontrolthekernelwidthand\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto\nchoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\nkernels\u2014bothscenariosthatsigni\ufb01cantlylimittheexpressivepowerofthenetwork.\nSee\ufb01gureforanexample. 9.13\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In\nthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\nthekernelhaswidth k,theoutputwillbeofwidth m k \u2212+1.\u00a0Therateofthis\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill\neventuallydropto1 \u00d71,atwhichpointadditionallayerscannotmeaningfully\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder\nin\ufb02uencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\ninanoutputimageofwidth m+ k \u22121.Inthiscase,theoutputpixelsnearthe\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\ncanmakeitdi\ufb03culttolearnasinglekernelthatperformswellatallpositionsin\ntheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\ntermsoftestsetclassi\ufb01cationaccuracy)liessomewherebetween\u201cvalid\u201dand\u201csame\u201d\nconvolution.\n3 4 9", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\nx 4 x 4 x 5 x 5s 3 s 3\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\nx 4 x 4z 4 z 4\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\nc onv ol ut i on\nD ow nsampl i n g\nC onv ol ut i on\nFigure\u00a09.12:Convolution\u00a0witha\u00a0stride.Inthisexample,we\u00a0use\u00a0astride\u00a0oftwo.\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation.\u00a0 ( Bot-\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto\nconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\nthatarethendiscarded.\n3 5 0", "CHAPTER9.CONVOLUTIONALNETWORKS\n. . . . . .. . .\n. . . . . .. . . . . .. . . . . .\nFigure9.13: T h e e \ufb00 e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\nshrinkby\ufb01vepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly\nabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\nshrinkingisinevitableinthiskindofarchitecture. Byadding\ufb01veimplicitzeroes ( Bottom )\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\nmakeanarbitrarilydeepconvolutionalnetwork.\n3 5 1", "CHAPTER9.CONVOLUTIONALNETWORKS\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speci\ufb01ed\nbya6-Dtensor W.\u00a0Theindicesinto Warerespectively: i,theoutputchannel,\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowo\ufb00set\nwithintheinput,and n,thecolumno\ufb00setwithintheinput.Thelinearpartofa\nlocallyconnectedlayeristhengivenby\nZ i , j , k=\ue058\nl , m , n[ V l , j m , k n + \u2212 1 + \u2212 1 w i , j , k, l , m , n] . (9.9)\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\nconnections.\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame\nfeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\nimage.\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\nwaytodothisistomakethe\ufb01rst moutputchannelsconnecttoonlythe\ufb01rst\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n\ninputchannels,andsoon.See\ufb01gureforanexample.Modelinginteractions 9.15\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\nreducememoryconsumptionandincreasestatisticale\ufb03ciency,andalsoreduces\ntheamountofcomputationneededtoperformforwardandback-propagation. It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\nT i l e d c o n v o l ut i o n( ,;,)o\ufb00ersacom- GregorandLeCun2010aLeetal.2010\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan\nlearningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\nneighboringlocationswillhavedi\ufb00erent\ufb01lters,likeinalocallyconnectedlayer,\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\nfeaturemap.See\ufb01gureforacomparisonoflocallyconnectedlayers,tiled 9.16\nconvolution,andstandardconvolution.\n3 5 2", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b c \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 d e \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f g\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 h \u00a0 i \u00a0 \u00a0\nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\nFigure9.14:Comparisonoflocalconnections,convolution,andfullconnections.\n( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\nauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\n( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\nthesameconnectivityasthelocallyconnectedlayer.Thedi\ufb00erenceliesnotinwhichunits\ninteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\nhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly\nacrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\n( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\nedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\ndiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected\nlayer.\n3 5 3", "CHAPTER9.CONVOLUTIONALNETWORKS\nI nput\u00a0T e nsorO ut put\u00a0T e nsor\nS p a t i a l \u00a0 c o o r d i n a t e sC h a n n e l \u00a0 c o o r d i n a t e s\nFigure9.15:\u00a0Aconvolutionalnetworkwiththe\ufb01rsttwooutputchannelsconnectedto\nonlythe\ufb01rsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\nthesecondtwoinputchannels.\n3 5 4", "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b c \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 d e \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f g\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 h \u00a0 i \u00a0 \u00a0\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b c \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 d a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 b c \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 d a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\nconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\nsizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\nThedi\ufb00erencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally\nconnectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight\nbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )\ntdi\ufb00erentkernels.Hereweillustratethecaseof t= 2.\u00a0Oneofthesekernelshasedges\nlabeled\u201ca\u201dand\u201cb,\u201dwhiletheotherhasedgeslabeled\u201cc\u201dand\u201cd.\u201d\u00a0Eachtimewemoveone\npixeltotherightintheoutput,wemoveontousingadi\ufb00erentkernel.Thismeansthat,\nlikethelocallyconnectedlayer,neighboringunitsintheoutputhavedi\ufb00erentparameters.\nUnlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,\nwecyclebacktothe\ufb01rstkernel.Iftwooutputunitsareseparatedbyamultipleof t\nsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )\nconvolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated\ninthediagrambyusingthekernelwithweightslabeled\u201ca\u201dand\u201cb\u201deverywhere.\n3 5 5", "CHAPTER9.CONVOLUTIONALNETWORKS\nTode\ufb01netiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\nthedimensionscorrespondtodi\ufb00erentlocationsintheoutputmap.Ratherthan\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\nthroughasetof tdi\ufb00erentchoicesofkernelstackineachdirection.If tisequalto\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\nZ i , j , k=\ue058\nl , m , nV l , j m , k n + \u2212 1 + \u2212 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)\nwhereis\u00a0themodulooperation,with % t% t=0(, t+1)% t=1,etc.It\u00a0is\nstraightforwardtogeneralizethisequationtouseadi\ufb00erenttilingrangeforeach\ndimension.\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\ndi\ufb00erent\ufb01lters.Ifthese\ufb01lterslearntodetectdi\ufb00erenttransformedversionsof\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\nlearnedtransformation(see\ufb01gure).Convolutionallayersarehard-codedtobe 9.9\ninvariantspeci\ufb01callytotranslation.\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\nInsomesimplecases,\u00a0thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty.\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa\nmatrixmultiplication (ifwe\ufb01rstreshapetheinputtensorintoa\ufb02atvector).The\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\nnetwork.\nMultiplication bythetransposeofthematrixde\ufb01nedbyconvolutionisone\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks\nthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\n3 5 6", "CHAPTER9.CONVOLUTIONALNETWORKS\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation\u2019s\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan\nresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\ntoldwhatthesizeoftheoriginalinputwas.\nThesethreeoperations\u2014convolution,backpropfromoutputtoweights,and\nbackpropfromoutputtoinputs\u2014aresu\ufb03cienttocomputeallofthegradients\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\nconvolution.\u00a0See ()forafullderivationoftheequationsinthe Goodfellow2010\nfullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\nde\ufb01nedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto\ncomputethecostfunction J.Duringback-propagation, wewillreceiveatensor G\nsuchthat G i , j , k=\u2202\n\u2202 Z i , j , kJ , . ( V K)\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\nweightsinthekernel.Todoso,wecanuseafunction\ng , , s ( G V) i , j , k, l=\u2202\n\u2202 K i , j , k, lJ ,( V K) =\ue058\nm , nG i , m , n V j , m s k, n s l ( \u2212 \u00d7 1 ) + ( \u2212 \u00d7 1 ) + .(9.11)\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.\nTodoso,wecanuseafunction\nh , , s ( K G) i , j , k=\u2202\n\u2202 V i , j , kJ ,( V K) (9.12)\n=\ue058\nl , m\ns . t .\n( 1 ) + = l \u2212 \u00d7 s m j\ue058\nn , p\ns . t .\n( 1 ) + = n \u2212 \u00d7 s p k\ue058\nqK q , i , m , p G q , l , n .(9.13)\nAutoencodernetworks,\u00a0describedinchapter,\u00a0arefeedforwardnetworks 14\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n3 5 7", "CHAPTER9.CONVOLUTIONALNETWORKS\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\nW\ue03eW x.Itiscommonformore\u00a0general autoencoders\u00a0tousemultiplication\nbythetransposeoftheweightmatrixjustasPCAdoes.\u00a0Tomakesuchmodels\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwede\ufb01ne\nareconstruction\nR K H = ( h , , s .) (9.14)\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect\nto Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\ndi\ufb00erentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\nback-propagationalgorithmonanystandardnetworkarchitectures.\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome\nbiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\nofhowtoshareparametersamongthebiases.\u00a0Forlocallyconnectedlayersitis\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,\ufb01xed\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.\nSeparatingthebiasesmayslightlyreducethestatisticale\ufb03ciencyofthemodel,but\nalsoallowsthemodeltocorrectfordi\ufb00erencesintheimagestatisticsatdi\ufb00erent\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\n9.6StructuredOutputs\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\nobject,ratherthanjustpredictingaclasslabelforaclassi\ufb01cationtaskorareal\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya\nstandardconvolutionallayer.Forexample,themodelmightemitatensor S,where\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\nthatfollowtheoutlinesofindividualobjects.\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\n3 5 8", "CHAPTER9.CONVOLUTIONALNETWORKS\n\u02c6 Y( 1 )\u02c6 Y( 1 )\u02c6 Y( 2 )\u02c6 Y( 2 )\u02c6 Y( 3 )\u02c6 Y( 3 )\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\nXXU U UV V V W W\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\nchannels(red,green,blue).Thegoalistooutputatensoroflabels\u02c6 Y,withaprobability\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,\nimagecolumns,andthedi\ufb00erentclasses.Ratherthanoutputting\u02c6 Yinasingleshot,the\nrecurrentnetworkiterativelyre\ufb01nesitsestimate\u02c6 Ybyusingapreviousestimateof\u02c6 Y\nasinputforcreatinganewestimate.\u00a0Thesameparametersareusedforeachupdated\nestimate,andtheestimatecanbere\ufb01nedasmanytimesaswewish.Thetensorof\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe\nhiddenvalues.Onallbutthe\ufb01rststep,thekernels Wareconvolvedover\u02c6 Ytoprovide\ninputtothehiddenlayer.Onthe\ufb01rsttimestep,thistermisreplacedbyzero.Because\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\ndescribedinchapter.10\ninputplane,asshownin\ufb01gure.Inthekindsofarchitectures typicallyusedfor 9.13\nclassi\ufb01cationofasingleobjectinanimage,thegreatestreductioninthespatial\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In\nordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\nuseapoolingoperatorwithunitstride.\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\noftheimagelabels,thenre\ufb01nethisinitialguessusingtheinteractionsbetween\nneighboringpixels.Repeatingthisre\ufb01nementstepseveraltimescorrespondsto\nusingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\nthearchitectureofsucharecurrentconvolutionalnetwork.\n3 5 9", "CHAPTER9.CONVOLUTIONALNETWORKS\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork\ncanbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\n9.7DataTypes\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\neachchannelbeingtheobservationofadi\ufb00erentquantityatsomepointinspace\nortime.Seetableforexamplesofdatatypeswithdi\ufb00erentdimensionalities 9.1\nandnumberofchannels.\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\n().2010\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest\ndatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\nevenwhencomputational costandover\ufb01ttingarenotsigni\ufb01cantissues.\nForexample,consideracollectionofimages,whereeachimagehasadi\ufb00erent\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof\n\ufb01xedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\ndi\ufb00erentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\nmultiplication; thesameconvolutionkernelinducesadi\ufb00erentsizeofdoublyblock\ncirculantmatrixforeachsizeofinput.\u00a0Sometimes theoutputofthenetworkis\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis\nnecessary.Inothercases,thenetworkmustproducesome\ufb01xed-sizeoutput,for\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\nmaintaina\ufb01xednumberofpooledoutputs.Someexamplesofthiskindofstrategy\nareshownin\ufb01gure.9.11\n3 6 0", "CHAPTER9.CONVOLUTIONALNETWORKS\nSinglechannel Multi-channel\n1-DAudio\u00a0waveform:The\u00a0axis\u00a0we\nconvolveovercorrespondsto\ntime.Wediscretizetimeand\nmeasuretheamplitudeofthe\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\ntionsof3-Dcomputer-rendered\ncharactersaregeneratedbyalter-\ningtheposeofa\u201cskeleton\u201dover\ntime.Ateachpointintime,the\nposeofthecharacterisdescribed\nbyaspeci\ufb01cationoftheanglesof\neachofthejointsinthecharac-\nter\u2019sskeleton.Eachchannelin\nthedatawefeedtotheconvolu-\ntionalmodelrepresentstheangle\naboutoneaxisofonejoint.\n2-DAudiodatathathasbeenprepro-\ncessedwithaFouriertransform:\nWecantransformtheaudiowave-\nformintoa2Dtensorwithdif-\nferentrowscorrespondingtodif-\nferentfrequencies\u00a0anddi\ufb00erent\ncolumnscorrespondingtodi\ufb00er-\nentpointsintime.Usingconvolu-\ntioninthetimemakesthemodel\nequivarianttoshiftsintime.Us-\ningconvolutionacrossthefre-\nquencyaxismakesthemodel\nequivarianttofrequency,sothat\nthesamemelodyplayedinadif-\nferentoctaveproducesthesame\nrepresentationbutatadi\ufb00erent\nheightinthenetwork\u2019soutput.Colorimagedata:Onechannel\ncontainstheredpixels,onethe\ngreen\u00a0pixels,\u00a0and\u00a0one\u00a0theblue\npixels.Theconvolutionkernel\nmovesoverboththehorizontal\nandverticalaxesofthe\u00a0image,\nconferringtranslationequivari-\nanceinbothdirections.\n3-DVolumetricdata:Acommon\nsourceofthiskindofdataismed-\nicalimagingtechnology,suchas\nCTscans.Colorvideodata:Oneaxiscorre-\nspondstotime,onetotheheight\nofthevideoframe,andoneto\nthewidthofthevideoframe.\nTable9.1:Examplesofdi\ufb00erentformatsofdatathatcanbeusedwithconvolutional\nnetworks.\n3 6 1", "CHAPTER9.CONVOLUTIONALNETWORKS\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\nofobservationofthesamekindofthing\u2014di\ufb00eren tlengthsofrecordingsover\ntime,di\ufb00erentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludedi\ufb00erent\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and\nourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\ncorrespondingtothetestscores.\n9.8E\ufb03cientConvolutionAlgorithms\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\nresources,asdiscussedinsection,areessential.\u00a0However,inmanycasesit 12.1\nisalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\nalgorithm.\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\nofthetwosignals,\u00a0andconvertingbacktothetimedomainusinganinverse\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\nimplementationofdiscreteconvolution.\nWhena d-dimensionalkernelcanbeexpressedas\u00a0theouterproductof d\nvectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\nkernelisseparable,naiveconvolutionisine\ufb03cient.Itisequivalenttocompose d\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\nissigni\ufb01cantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable\nconvolutionrequires O( w d \u00d7)runtimeandparameterstoragespace.Ofcourse,\nnoteveryconvolutioncanberepresentedinthisway.\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\nniquesthatimprovethee\ufb03ciencyofonlyforwardpropagationareusefulbecause\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\nanetworkthantoitstraining.\n3 6 2", "CHAPTER9.CONVOLUTIONALNETWORKS\n9.9RandomorUnsupervisedFeatures\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork\ntrainingistousefeaturesthatarenottrainedinasupervisedfashion.\nTherearethreebasicstrategiesforobtaining\u00a0con volutionkernelswithout\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.\u00a0PartIII\ndescribesmanymoreunsupervisedlearningapproaches.Learningthefeatures\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\nclassi\ufb01erlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\nRandom\ufb01ltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett\netal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\naconvolutionalnetwork:\ufb01rstevaluatetheperformanceofseveralconvolutional\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.\nAnintermediate approachistolearnthefeatures,butusingmethodsthatdo\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totrainthe\ufb01rstlayer\ninisolation,thenextractallfeaturesfromthe\ufb01rstlayeronlyonce,thentrainthe\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The\ncanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworkso\ufb00er Leeetal.2009\n3 6 3", "CHAPTER9.CONVOLUTIONALNETWORKS\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\nWecanthenusetheparametersfromthispatch-basedmodeltode\ufb01nethekernels\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining\nprocess.Usingthisapproach,wecantrainverylargemodelsandincurahigh\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\nfromroughly2007\u20132013,whenlabeleddatasetsweresmallandcomputational\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\nentirenetworkoneachtrainingiteration.\nAswithotherapproachestounsupervisedpretraining,itremainsdi\ufb03cultto\nteaseapartthecauseofsomeofthebene\ufb01tsseenwiththisapproach.Unsupervised\npretrainingmayo\ufb00ersomeregularizationrelativetosupervisedtraining,oritmay\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\ncostofthelearningrule.\n9.10TheNeuroscienti\ufb01cBasisforConvolutionalNet-\nworks\nConvolutional\u00a0networksare\u00a0perhaps\u00a0the greatest\u00a0successstory\u00a0ofbiologically\ninspiredarti\ufb01cialintelligence.Thoughconvolutionalnetworkshavebeenguided\nbymanyother\ufb01elds,someofthekeydesignprinciplesofneuralnetworkswere\ndrawnfromneuroscience.\nThehistoryofconvolutionalnetworksbeginswithneuroscienti\ufb01cexperiments\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland\nWiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\naNobelprize.Their\ufb01ndingsthathavehadthegreatestin\ufb02uenceoncontemporary\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\ncats.Theyobservedhowneuronsinthecat\u2019sbrainrespondedtoimagesprojected\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeci\ufb01c\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto\notherpatterns.\n3 6 4", "CHAPTER9.CONVOLUTIONALNETWORKS\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\nfocusonasimpli\ufb01ed,cartoonviewofbrainfunction.\nInthissimpli\ufb01edview,wefocusonapartofthebraincalledV1,alsoknown\nasthe pr i m ar y v i sual c o r t e x.\u00a0V1isthe\ufb01rstareaofthebrainthatbeginsto\nperformsigni\ufb01cantlyadvancedprocessingofvisualinput.\u00a0Inthiscartoonview,\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the\nlight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\ncalledthelateralgeniculatenucleus.\u00a0Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead.\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:\n1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\nmirroring\u00a0the structure\u00a0of\u00a0theimage\u00a0in\u00a0the retina.For\u00a0example,\u00a0light\narrivingatthelowerhalfoftheretinaa\ufb00ectsonlythecorrespondinghalfof\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\nde\ufb01nedintermsoftwodimensionalmaps.\n2.V1containsmany si m pl e c e l l s.Asimplecell\u2019sactivitycantosomeextent\nbecharacterizedbyalinear\u00a0function oftheimagein\u00a0asmall,\u00a0spatially\nlocalizedreceptive\ufb01eld.Thedetectorunitsofaconvolutionalnetworkare\ndesignedtoemulatethesepropertiesofsimplecells.\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.\nTheseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\nlayersofthebrain,weeventually\ufb01ndcellsthatrespondtosomespeci\ufb01cconcept\nandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n3 6 5", "CHAPTER9.CONVOLUTIONALNETWORKS\nnicknamed\u201cgrandmother cells\u201d\u2014theideaisthatapersoncouldhaveaneuronthat\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\nshadow,etc.\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005\ntestedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\nTheyfoundwhathascometobecalledthe\u201cHalleBerryneuron\u201d:anindividual\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuron\ufb01reswhena\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\nthewords\u201cHalleBerry.\u201dOfcourse,thishasnothingtodowithHalleBerryherself;\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern\nconvolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\nnetwork\u2019slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\n(IT).Whenviewinganobject,information\ufb02owsfromtheretina,throughthe\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithinthe\ufb01rst\n100msofglimpsinganobject.\u00a0Ifapersonisallowedtocontinuelookingatthe\nobjectformoretime,theninformationwillbeginto\ufb02owbackwardsasthebrain\nusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\nHowever,ifweinterrupttheperson\u2019sgaze,andobserveonlythe\ufb01ringratesthat\nresultfromthe\ufb01rst100msofmostlyfeedforwardactivation,thenITprovestobe\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\n\ufb01ringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\nrecognitiontasks(,). DiCarlo2013\nThatbeingsaid,therearemanydi\ufb00erencesbetweenconvolutionalnetworks\nandthemammalianvisionsystem.Someofthesedi\ufb00erencesarewellknown\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\nthesedi\ufb00erencesarenotyetknown,becausemanybasicquestionsabouthowthe\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\n\u2022Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,\nthisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\n3 6 6", "CHAPTER9.CONVOLUTIONALNETWORKS\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).\n\u2022Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\nsofararepurelyvisual.\n\u2022Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\nabletounderstandentirescenesincludingmanyobjectsandrelationships\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.\n\u2022EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntoo\ufb00eracompellingimprovement.\n\u2022WhilefeedforwardIT\ufb01ringratescapturemuchofthesameinformationas\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\ncomputations are.Thebrainprobablyusesverydi\ufb00erentactivationand\npoolingfunctions.Anindividualneuron\u2019sactivationprobablyisnotwell-\ncharacterizedbyasinglelinear\ufb01lterresponse.ArecentmodelofV1involves\nmultiplequadratic\ufb01ltersforeachneuron(,).Indeedour Rustetal.2005\ncartoonpictureof\u201csimplecells\u201d\u00a0and\u201ccomplexcells\u201d\u00a0mightcreateanon-\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\nkindofcellbutwiththeir\u201cparameters\u201denablingacontinuumofbehaviors\nrangingfromwhatwecall\u201csimple\u201dtowhatwecall\u201ccomplex.\u201d\nItis\u00a0alsoworthmentioningthatneuroscience\u00a0hastold\u00a0usrelativelylittle\nabouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\nalgorithm.\n3 6 7", "CHAPTER9.CONVOLUTIONALNETWORKS\nLang\u00a0andHinton\u00a01988()introducedthe\u00a0use\u00a0ofback-propagation\u00a0totrain\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscienti\ufb01cobserva-\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989\nthemodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\nconvolutionappliedtoimages.\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeci\ufb01c\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.\nInadeep,nonlinearnetwork,itcanbedi\ufb03culttounderstandthefunctionof\nindividualcells.Simplecellsinthe\ufb01rstlayerareeasiertoanalyze,becausetheir\nresponsesaredrivenbyalinearfunction.Inanarti\ufb01cialneuralnetwork,wecan\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal\u2019s\nretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\nthen\ufb01talinearmodeltotheseresponsesinordertoobtainanapproximation of\ntheneuron\u2019sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\nandShapley2004,).\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns.\u00a0TheGaborfunctiondescribestheweightata2-Dpoint\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,\nI( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\nlocations,de\ufb01nedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\nofview,theresponseofasimplecelltoanimageisgivenby\ns I() =\ue058\nx \u2208 X\ue058\ny \u2208 Yw x , y I x , y . ()() (9.15)\nSpeci\ufb01cally,takestheformofaGaborfunction: w x , y()\nw x , y \u03b1 , \u03b2 (; x , \u03b2 y , f , \u03c6 , x 0 , y 0 , \u03c4 \u03b1) = exp\ue000\u2212 \u03b2 x x\ue030 2\u2212 \u03b2 y y\ue030 2\ue001\ncos( f x\ue030+) \u03c6 ,(9.16)\nwhere\nx\ue030= ( x x \u2212 0)cos()+( \u03c4 y y \u2212 0)sin() \u03c4 (9.17)\n3 6 8", "CHAPTER9.CONVOLUTIONALNETWORKS\nand\ny\ue030= ( \u2212 x x \u2212 0)sin()+( \u03c4 y y \u2212 0)cos() \u03c4 . (9.18)\nHere, \u03b1, \u03b2 x, \u03b2 y, f, \u03c6, x 0, y 0,and \u03c4areparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndi\ufb00erentsettingsoftheseparameters.\nTheparameters x 0, y 0,and \u03c4de\ufb01neacoordinatesystem.\u00a0Wetranslateand\nrotate xand ytoform x\ue030and y\ue030.Speci\ufb01cally,thesimplecellwillrespondtoimage\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness\naswemovealongalinerotatedradiansfromthehorizontal. \u03c4\nViewedasafunctionof x\ue030and y\ue030,thefunction wthenrespondstochangesin\nbrightnessaswemovealongthe x\ue030axis.\u00a0Ithastwoimportantfactors:oneisa\nGaussianfunctionandtheotherisacosinefunction.\nTheGaussianfactor \u03b1exp\ue000\n\u2212 \u03b2 x x\ue030 2\u2212 \u03b2 y y\ue030 2\ue001\ncanbeseenasagatingtermthat\nensuresthesimplecellwillonlyrespondtovaluesnearwhere x\ue030and y\ue030areboth\nzero,inotherwords,nearthecenterofthecell\u2019sreceptive\ufb01eld.Thescalingfactor\n\u03b1adjuststhetotalmagnitudeofthesimplecell\u2019sresponse,while \u03b2 xand \u03b2 ycontrol\nhowquicklyitsreceptive\ufb01eldfallso\ufb00.\nThecosinefactor cos( f x\ue030+ \u03c6) controlshowthesimplecellrespondstochanging\nbrightnessalongthe x\ue030axis.Theparameter fcontrolsthefrequencyofthecosine\nandcontrolsitsphaseo\ufb00set. \u03c6\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\ntoaspeci\ufb01cspatialfrequencyofbrightnessinaspeci\ufb01cdirectionataspeci\ufb01c\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage\nhasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights\u2014when\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\nnegative.\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecells\u2019responses: c( I)=\ue070\ns 0() I2+ s 1() I2.\u00a0An\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except\nfor \u03c6,and \u03c6issetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcellde\ufb01nedinthisway\nrespondswhentheGaussianreweightedimage I( x , y)exp( \u2212 \u03b2 x x\ue030 2\u2212 \u03b2 y y\ue030 2) contains\nahighamplitudesinusoidalwavewithfrequency findirection \u03c4near ( x 0 , y 0),\nregardlessofthephaseo\ufb00setofthiswave.Inotherwords,thecomplexcellis\ninvarianttosmalltranslationsoftheimageindirection \u03c4,ortonegatingtheimage\n3 6 9", "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdi\ufb00erentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and \u03c4.\u00a0EachGaborfunctioninthisgridis\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and \u03c4ischosenso\nthateachGabor\ufb01lterissensitivetothedirectionradiatingoutfromthecenterofthegrid.\nFortheothertwoplots, x 0, y 0,and \u03c4are\ufb01xedtozero.\u00a0 Gaborfunctionswith ( C e n t e r )\ndi\ufb00erentGaussianscaleparameters \u03b2 xand \u03b2 y.Gaborfunctionsarearrangedinincreasing\nwidth(decreasing \u03b2 x)aswemovelefttorightthroughthegrid,andincreasingheight\n(decreasing \u03b2 y)aswemovetoptobottom.Fortheothertwoplots,the \u03b2valuesare\ufb01xed\nto1.5 \u00d7theimagewidth.Gaborfunctionswithdi\ufb00erentsinusoidparameters ( R i g h t ) f\nand \u03c6.Aswemovetoptobottom, fincreases,andaswemovelefttoright, \u03c6increases.\nFortheothertwoplots,is\ufb01xedto0andis\ufb01xedto5theimagewidth. \u03c6 f \u00d7\n(replacingblackwithwhiteandviceversa).\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\nasimpleunsupervisedlearningalgorithm,\u00a0sparse coding,learnsfeatureswith\nreceptive\ufb01eldssimilartothoseofsimplecells.Sincethen,wehavefoundthat\nanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\nlearningalgorithms,whichlearnthesefeaturesintheir\ufb01rstlayer.Figure9.19\nshowssomeexamples.Becausesomanydi\ufb00erentlearningalgorithmslearnedge\ndetectors,itisdi\ufb03culttoconcludethatanyspeci\ufb01clearningalgorithmisthe\n\u201cright\u201dmodelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not\nwhenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydi\ufb00erent\napproachestostatisticalmodeling.SeeHyv\u00e4rinen 2009etal.()forareviewofthe\n\ufb01eldofnaturalimagestatistics.\n3 7 0", "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeci\ufb01c\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\nimagepatches. ( R i g h t )Convolutionkernelslearnedbythe\ufb01rstlayerofafullysupervised\nconvolutionalmaxoutnetwork.Neighboringpairsof\ufb01ltersdrivethesamemaxoutunit.\n9.11ConvolutionalNetworksandtheHistoryofDeep\nLearning\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\nthe\ufb01rstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\nconsideredviable.Convolutionalnetworkswerealsosomeofthe\ufb01rstneural\nnetworkstosolveimportantcommercialapplicationsandremainattheforefront\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12\nandmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\n3 7 1", "CHAPTER9.CONVOLUTIONALNETWORKS\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\nlessimpactforyearsearlier.\nConvolutionalnetsweresomeofthe\ufb01rstworkingdeepnetworkstrainedwith\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\nsimplybethatconvolutionalnetworksweremorecomputationally e\ufb03cientthan\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem\nandtunetheirimplementation andhyperparameters.Largernetworksalsoseem\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouse\ufb00orttouse\nneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto\nanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\nnetworks.\n3 7 2", "C h a p t e r 1 0\nS e qu e n ce Mo d e l i n g: Recurren t\nan d Recursiv e N e t s\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) \u03c4.Justasconvolutionalnetworks\ncanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\nlength.\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\nthe1980s:sharingparametersacrossdi\ufb00erentpartsofamodel.Parametersharing\nmakesitpossibletoextendandapplythemodeltoexamplesofdi\ufb00erentforms\n(di\ufb00erentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\nseenduringtraining,norsharestatisticalstrengthacrossdi\ufb00erentsequencelengths\nandacrossdi\ufb00erentpositionsintime.Suchsharingisparticularlyimportantwhen\naspeci\ufb01cpieceofinformationcanoccuratmultiplepositionswithinthesequence.\nForexample,considerthetwosentences\u201cIwenttoNepalin2009\u201dand\u201cIn2009,\nIwenttoNepal.\u201dIfweaskamachinelearningmodeltoreadeachsentenceand\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n373", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\nnetworkthatprocessessentencesof\ufb01xedlength.Atraditionalfullyconnected\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\nacrossseveraltimesteps.\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This\nconvolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\nstep.Recurrentnetworksshareparametersinadi\ufb00erentway.Eachmemberofthe\noutputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph.\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 \u03c4.In\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\nwithadi\ufb00erentsequencelength \u03c4foreachmemberoftheminibatch.Wehave\nomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\nThischapterextendstheideaofacomputational graphtoincludecycles.These\ncyclesrepresentthein\ufb02uenceofthepresentvalueofavariableonitsownvalue\natafuturetimestep.Suchcomputational graphsallowustode\ufb01nerecurrent\nneuralnetworks.Wethendescribemanydi\ufb00erentwaystoconstruct,train,and\nuserecurrentneuralnetworks.\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\nchapter,wereferthereadertothetextbookofGraves2012().\n3 7 4", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.1UnfoldingComputationalGraphs\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.\nUnfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure.\nForexample,considertheclassicalformofadynamicalsystem:\ns( ) t= ( fs( 1 ) t \u2212;)\u03b8 , (10.1)\nwheres( ) tiscalledthestateofthesystem.\nEquationisrecurrentbecausethede\ufb01nitionof 10.1 sattime trefersbackto\nthesamede\ufb01nitionattime. t\u22121\nFora\ufb01nitenumberoftimesteps \u03c4,thegraphcanbeunfoldedbyapplying\nthede\ufb01nition \u03c4\u22121times.Forexample,ifweunfoldequationfor10.1 \u03c4= 3time\nsteps,weobtain\ns( 3 )=( fs( 2 );)\u03b8 (10.2)\n=(( f fs( 1 ););)\u03b8\u03b8 (10.3)\nUnfoldingtheequationbyrepeatedlyapplyingthede\ufb01nitioninthiswayhas\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\nnowberepresentedbyatraditionaldirectedacycliccomputational graph.\u00a0The\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\n\ufb01gure.10.1\ns( t \u2212 1 )s( t \u2212 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\nf f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph.\u00a0Eachnoderepresentsthestateatsometime tandthe\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\nofusedtoparametrize)areusedforalltimesteps. \u03b8 f\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\nsignalx( ) t,\ns( ) t= ( fs( 1 ) t \u2212,x( ) t;)\u03b8 , (10.4)\n3 7 5", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\nRecurrentneuralnetworkscanbebuiltinmanydi\ufb00erentways.Muchas\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\nde\ufb01nethevaluesoftheirhiddenunits.\u00a0Toindicatethatthestateisthehidden\nunitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\nthestate:\nh( ) t= ( fh( 1 ) t \u2212,x( ) t;)\u03b8 , (10.5)\nillustratedin\ufb01gure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This\nsummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n(x( ) t,x( 1 ) t \u2212,x( 2 ) t \u2212, . . . ,x( 2 ),x( 1 ))toa\ufb01xedlengthvectorh( ) t.Dependingonthe\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence\nuptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\n(chapter).14\nf fhh\nx xh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\nf f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\ninformationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance.\nEquationcanbedrawnintwodi\ufb00erentways.OnewaytodrawtheRNN 10.5\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n3 7 6", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\nview,thenetworkde\ufb01nesacircuitthatoperatesinrealtime,withphysicalparts\nwhosecurrentstatecanin\ufb02uencetheirfuturestate,asintheleftof\ufb01gure.10.2\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan\nunfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\ndi\ufb00erentvariables,withonevariablepertimestep,representingthestateofthe\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\nseparatenodeofthecomputational graph,asintherightof\ufb01gure.Whatwe10.2\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideofthe\ufb01gure\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\ngraphnowhasasizethatdependsonthesequencelength.\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\nh( ) t= g( ) t(x( ) t,x( 1 ) t \u2212,x( 2 ) t \u2212, . . . ,x( 2 ),x( 1 )) (10.6)\n=( fh( 1 ) t \u2212,x( ) t;)\u03b8 (10.7)\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t \u2212,x( 2 ) t \u2212, . . . ,x( 2 ),x( 1 ))\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding\nprocessthusintroducestwomajoradvantages:\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\ninputsize,becauseitisspeci\ufb01edintermsoftransitionfromonestateto\nanotherstate,ratherthanspeci\ufb01edintermsofavariable-length historyof\nstates.\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep.\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate\nmodel g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\nrequiredwithoutparametersharing.\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\n3 7 7", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninformation\ufb02owforwardintime(computingoutputsandlosses)andbackward\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\ninformation\ufb02ows.\n10.2RecurrentNeuralNetworks\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\ncandesignawidevarietyofrecurrentneuralnetworks.\nUUV V\nWWo( t \u2212 1 )o( t \u2212 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\nthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\ncomputes\u02c6y=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby\naweightmatrixV.Equationde\ufb01nesforwardpropagationinthismodel. 10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance.\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\nincludethefollowing:\n3 7 8", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n\u2022Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsbetweenhiddenunits,illustratedin\ufb01gure.10.3\n\u2022Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\nunitsatthenexttimestep,illustratedin\ufb01gure10.4\n\u2022Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\n\ufb01gure.10.5\n\ufb01gureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\nmostofthechapter.\nTherecurrentneuralnetworkof\ufb01gureandequationisuniversalinthe 10.3 10.8\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\narecurrentnetworkofa\ufb01nitesize.TheoutputcanbereadfromtheRNNafter\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput\n(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\nfunctionsinthissettingusingasinglespeci\ufb01cRNNof\ufb01nitesize(Siegelmannand\nSontag1995()use886units).The\u201cinput\u201doftheTuringmachineisaspeci\ufb01cation\nofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\nmachineissu\ufb03cientforallproblems.ThetheoreticalRNNusedfortheproof\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\nrationalnumbersofunboundedprecision.\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\n\ufb01gure.The\ufb01guredoesnotspecifythechoiceofactivationfunctionforthe 10.3\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,\nthe\ufb01guredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\nobtainavector\u02c6yofnormalizedprobabilitiesovertheoutput.Forwardpropagation\nbeginswithaspeci\ufb01cationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\n3 7 9", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nUV\nWo( t \u2212 1 )o( t \u2212 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\nh( ) . . .h( ) . . .V V V\nU U UU nf ol d\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput\ntothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\nsmallersetoffunctions)thanthoseinthefamilyrepresentedby\ufb01gure.TheRNN 10.3\nin\ufb01gurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\nrepresentationhandtransmithtothefuture.TheRNNinthis\ufb01gureistrainedto\nputaspeci\ufb01coutputvalueintoo,andoistheonlyinformationitisallowedtosend\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\nfromthepast.ThismakestheRNNinthis\ufb01gurelesspowerful,butitmaybeeasierto\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\nparallelizationduringtraining,asdescribedinsection.10.2.1\n3 8 0", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nt t \u03c4 = 1to= ,weapplythefollowingupdateequations:\na( ) t= +bWh( 1 ) t \u2212+Ux( ) t(10.8)\nh( ) t=tanh(a( ) t) (10.9)\no( ) t= +cVh( ) t(10.10)\n\u02c6y( ) t=softmax(o( ) t) (10.11)\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora\ngivensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\nL\ue010\n{x( 1 ), . . . ,x( ) \u03c4}{ ,y( 1 ), . . . ,y( ) \u03c4}\ue011\n(10.12)\n=\ue058\ntL( ) t(10.13)\n=\u2212\ue058\ntlog p m o de l\ue010\ny( ) t|{x( 1 ), . . . ,x( ) t}\ue011\n, (10.14)\nwhere p m o de l\ue000\ny( ) t|{x( 1 ), . . . ,x( ) t}\ue001\nisgivenbyreadingtheentryfor y( ) tfromthe\nmodel\u2019soutputvector\u02c6y( ) t.Computingthegradientofthislossfunctionwithrespect\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\noftheunrolledgraphin\ufb01gure,followedbyabackwardpropagationpass 10.3\nmovingrighttoleftthroughthegraph.Theruntimeis O( \u03c4) andcannotbereduced\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;\neachtimestepmayonlybecomputedafterthepreviousone.\u00a0Statescomputed\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\npass,sothememorycostisalso O( \u03c4).Theback-propagation algorithmapplied\ntotheunrolledgraphwith O( \u03c4)costiscalledback-propagationthroughtime\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\nalternative?\n10.2.1TeacherForcingandNetworkswithOutputRecurrence\nThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\nthehiddenunitsatthenexttimestep(shownin\ufb01gure)isstrictlylesspowerful 10.4\n3 8 1", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser\nknowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\nneedtocomputetheoutputfortheprevioustimestep\ufb01rst,becausethetraining\nsetprovidestheidealvalueofthatoutput.\nh( t \u2212 1 )h( t \u2212 1 )\nWh( ) th( ) t . . . . . .\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\nU U Uh( ) \u03c4h( ) \u03c4\nx( ) \u03c4x( ) \u03c4W\nUo( ) \u03c4o( ) \u03c4y( ) \u03c4y( ) \u03c4L( ) \u03c4L( ) \u03c4\nV\n. . . . . .\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\n\ufb01xed-sizerepresentationusedasinputforfurtherprocessing.\u00a0Theremightbeatarget\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby\nback-propagatingfromfurtherdownstreammodules.\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.\u00a0Wecansee\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n3 8 2", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t \u2212 1 )o( t \u2212 1 )o( ) to( ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tW\nV V\nU Uo( t \u2212 1 )o( t \u2212 1 )o( ) to( ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tW\nV V\nU U\nT r ai n\u00a0 t i m e T e s t \u00a0 t i m e\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis\napplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel\u2019soutput\no( ) t,andfeedtheoutputbackintothemodel.\n3 8 3", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nlikelihoodcriterionis\nlog p\ue010\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\ue011\n(10.15)\n=log p\ue010\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\ue011\n+log p\ue010\ny( 1 )|x( 1 ),x( 2 )\ue011\n(10.16)\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\nvaluefromthetrainingset.Maximumlikelihoodthusspeci\ufb01esthatduringtraining,\nratherthanfeedingthemodel\u2019sownoutputbackintoitself,theseconnections\nshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\nThisisillustratedin\ufb01gure.10.6\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier\ntimesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\nwithbothteacherforcingandBPTT.\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\ntheoutputdistribution)fedbackasinput.\u00a0Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitedi\ufb00erentfromthekindofinputs\nthatitwillseeattesttime.\u00a0Onewaytomitigatethisproblemistotrainwith\nbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b\ninputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\nusemoreofthegeneratedvaluesasinput.\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n3 8 4", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\ngradient-basedtechniquestotrainanRNN.\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby\ntforx( ) t,h( ) t,o( ) tand L( ) t.\u00a0Foreachnode Nweneedtocomputethegradient\n\u2207 N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingthe\ufb01nalloss\n\u2202 L\n\u2202 L( ) t= 1 . (10.17)\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\nsoftmaxfunctiontoobtainthevector\u02c6yofprobabilitiesovertheoutput.Wealso\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe\ninputsofar.Thegradient\u2207o( ) t Lontheoutputsattimestep t,forall i , t,isas\nfollows:\n(\u2207o( ) t L)i=\u2202 L\n\u2202 o( ) t\ni=\u2202 L\n\u2202 L( ) t\u2202 L( ) t\n\u2202 o( ) t\ni=\u02c6 y( ) t\ni\u2212 1i , y( ) t .(10.18)\nWeworkourwaybackwards,startingfromtheendofthesequence.Atthe\ufb01nal\ntimestep, \u03c4h( ) \u03c4onlyhaso( ) \u03c4asadescendent,soitsgradientissimple:\n\u2207h( ) \u03c4 L= V\ue03e\u2207o( ) \u03c4 L. (10.19)\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\nfrom t= \u03c4\u22121downto t= 1,notingthath( ) t(for t < \u03c4)hasasdescendentsboth\no( ) tandh( + 1 ) t.Itsgradientisthusgivenby\n\u2207h( ) t L=\ue020\n\u2202h( + 1 ) t\n\u2202h( ) t\ue021\ue03e\n(\u2207h( +1) t L)+\ue020\n\u2202o( ) t\n\u2202h( ) t\ue021\ue03e\n(\u2207o( ) t L) (10.20)\n= W\ue03e(\u2207h( +1) t L)diag\ue012\n1\u2212\ue010\nh( + 1 ) t\ue0112\ue013\n+V\ue03e(\u2207o( ) t L)(10.21)\nwhere diag\ue010\n1\u2212\ue000\nh( + 1 ) t\ue0012\ue011\nindicatesthediagonalmatrixcontainingtheelements\n1\u2212( h( + 1 ) t\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\nhiddenunitattime. i t+1\n3 8 5", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nOncethegradientsonthe\u00a0internalnodesofthe\u00a0computational graphare\nobtained,\u00a0wecanobtainthegradientsontheparameternodes.Becausethe\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\nofasingleedgeinthecomputational graphtothegradient.However,the\u2207 W f\noperatorusedincalculustakesintoaccountthecontributionofWtothevalue\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\nintroducedummyvariablesW( ) tthatarede\ufb01nedtobecopiesofWbutwitheach\nW( ) tusedonlyattimestep t.Wemaythenuse\u2207W( ) ttodenotethecontribution\noftheweightsattimesteptothegradient. t\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\n\u2207 c L=\ue058\nt\ue020\n\u2202o( ) t\n\u2202c\ue021\ue03e\n\u2207o( ) t L=\ue058\nt\u2207o( ) t L (10.22)\n\u2207 b L=\ue058\nt\ue020\n\u2202h( ) t\n\u2202b( ) t\ue021\ue03e\n\u2207h( ) t L=\ue058\ntdiag\ue012\n1\u2212\ue010\nh( ) t\ue0112\ue013\n\u2207h( ) t L(10.23)\n\u2207 V L=\ue058\nt\ue058\ni\ue020\n\u2202 L\n\u2202 o( ) t\ni\ue021\n\u2207 V o( ) t\ni=\ue058\nt(\u2207o( ) t L)h( ) t\ue03e(10.24)\n\u2207 W L=\ue058\nt\ue058\ni\ue020\n\u2202 L\n\u2202 h( ) t\ni\ue021\n\u2207W( ) t h( ) t\ni (10.25)\n=\ue058\ntdiag\ue012\n1\u2212\ue010\nh( ) t\ue0112\ue013\n(\u2207h( ) t L)h( 1 ) t \u2212\ue03e(10.26)\n\u2207 U L=\ue058\nt\ue058\ni\ue020\n\u2202 L\n\u2202 h( ) t\ni\ue021\n\u2207U( ) t h( ) t\ni (10.27)\n=\ue058\ntdiag\ue012\n1\u2212\ue010\nh( ) t\ue0112\ue013\n(\u2207h( ) t L)x( ) t\ue03e(10.28)\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\nitdoesnothaveanyparametersasancestorsinthecomputational graphde\ufb01ning\ntheloss.\n3 8 6", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.2.3RecurrentNetworksasDirectedGraphicalModels\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and\nweusuallyusethecross-entropyassociatedwiththatdistributiontode\ufb01netheloss.\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\nWhen\u00a0we\u00a0use\u00a0apredictivelog-likelihood\u00a0trainingobjective,such\u00a0asequa-\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\nthelog-likelihood\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t \u2212) . (10.30)\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges\nfromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\nvaluesinthepasttothecurrent y( ) tvalue.\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) \u03c4},withnoadditionalinputs\nx.Theinputattimestep tissimplytheoutputattimestep t\u22121.TheRNNthen\nde\ufb01nesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\nprobabilities:\nP P () = Y ( y( 1 ), . . . , y( ) \u03c4) =\u03c4\ue059\nt = 1P( y( ) t| y( 1 ) t \u2212, y( 2 ) t \u2212, . . . , y( 1 ))(10.31)\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) \u03c4}accordingtosuchamodel\n3 8 7", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\npastobservation y( ) imayin\ufb02uencetheconditionaldistributionofsome y( ) t(for t > i),\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\ngraph(asinequation)mightbeveryine\ufb03cient,withanevergrowingnumberof 10.6\ninputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\nconnectivitybute\ufb03cientparametrization,asillustratedin\ufb01gure.10.8\nis\nL=\ue058\ntL( ) t(10.32)\nwhere\nL( ) t= log( \u2212 Py( ) t= y( ) t| y( 1 ) t \u2212, y( 2 ) t \u2212, . . . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even\nthoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\ne\ufb03cientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\nsharethesameparameterswiththeotherstages.\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\ne\ufb03ciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\n3 8 8", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\nshouldonlycontainedgesfrom{y( ) t k \u2212, . . . ,y( 1 ) t \u2212}toy( ) t,ratherthancontaining\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\ninputsshouldhaveanin\ufb02uenceonthenextelementofthesequence.RNNsare\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\nfromthedistantpastinawaythatisnotcapturedbythee\ufb00ectofy( ) iony( 1 ) t \u2212.\nOnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\nde\ufb01ningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\nvalueswiththecompletegraphstructureisshownin\ufb01gure.Thecomplete10.7\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\nmarginalizing themoutofthemodel.\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe\nhiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaverye\ufb03cient\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\nrepresentation\u2014anarraycontainingaseparateentryforeachpossibleassignment\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\noccurring.\u00a0If ycantakeon kdi\ufb00erentvalues,thetabularrepresentationwould\nhave O( k\u03c4)parameters.Bycomparison,duetoparametersharing,thenumberof\nparametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\nlong-termrelationshipsbetweenvariablese\ufb03ciently,usingrecurrentapplications\nofthesamefunction fandsameparameters\u03b8ateachtimestep.Figure10.8\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate\nquantitybetweenthem.Avariable y( ) iinthedistantpastmayin\ufb02uenceavariable\ny( ) tviaitse\ufb00ectonh.Thestructureofthisgraphshowsthatthemodelcanbe\ne\ufb03cientlyparametrized byusingthesameconditionalprobabilitydistributionsat\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\njointassignmentofallvariablescanbeevaluatede\ufb03ciently.\nEvenwiththee\ufb03cientparametrization ofthegraphicalmodel,someoperations\nremaincomputationally challenging.Forexample,itisdi\ufb03culttopredictmissing\n1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\nh i d d e n u n i t s .\n3 8 9", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence.\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybedi\ufb03cult. o p t i m i z i ng\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\nthatthesameparameterscanbeusedfordi\ufb00erenttimesteps.Equivalently,the\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship\nbetweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\ndi\ufb00erenttimesteps.Thiswouldalreadybemuchbetterthanusingadi\ufb00erent\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\nextrapolatewhenfacedwithnewvaluesof. t\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow\ntodrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\nsimplytosamplefromtheconditionaldistributionateachtimestep.\u00a0However,\nthereisoneadditionalcomplication.\u00a0The RNNmusthavesomemechanismfor\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,\nweinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) \u03c4\nineachtrainingexample.\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya\nsigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\nsequenceendsorcontinuesateachtimestep.\nAnotherwaytodeterminethesequencelength \u03c4istoaddanextraoutputto\nthemodelthatpredictstheinteger \u03c4itself.Themodelcansampleavalueof \u03c4\nandthensample \u03c4stepsworthofdata.Thisapproachrequiresaddinganextra\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis\nawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\ncaneitherconsistofthevalueof \u03c4orcanconsistof \u03c4 t\u2212,thenumberofremaining\n3 9 0", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\nendabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\nbasedonthedecomposition\nP(x( 1 ), . . . ,x( ) \u03c4) = ()( P \u03c4 Px( 1 ), . . . ,x( ) \u03c4| \u03c4 .)(10.34)\nThestrategyofpredicting \u03c4directlyisusedforexamplebyGoodfellow e t a l .\n().2014d\n10.2.4ModelingSequencesConditionedonContextwithRNNs\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected\ngraphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of\ncourse,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8\ninputsx( 1 ),x( 2 ), . . . ,x( ) \u03c4.Ingeneral,RNNsallowtheextensionofthegraphical\nmodelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut\nalsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof\nfeedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;\u03b8)\ncanbereinterpretedasamodelrepresentingaconditionaldistribution P(y\u03c9|)\nwith\u03c9=\u03b8.Wecanextendsuchamodeltorepresentadistribution P(yx|)by\nusingthesame P(y\u03c9|)asbefore,butmaking\u03c9afunctionofx.Inthecaseof\nanRNN,thiscanbeachievedindi\ufb00erentways.Wereviewherethemostcommon\nandobviouschoices.\nPreviously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\nt=1 , . . . , \u03c4asinput.\u00a0Anotheroptionistotakeonlyasinglevectorxasinput.\nWhenxisa\ufb01xed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto\nanRNNare:\n1.\u00a0asanextrainputateachtimestep,or\n2.\u00a0astheinitialstateh( 0 ),or\n3.\u00a0both.\nThe\ufb01rstandmostcommonapproachisillustratedin\ufb01gure.Theinteraction10.9\nbetweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly\nintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequence\nof yvalues.\u00a0Thesameproductx\ue03eRisaddedasadditionalinputtothehidden\nunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue\n3 9 1", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nofx\ue03eRthatise\ufb00ectivelyanewbiasparameterusedforeachofthehiddenunits.\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparameters\u03b8ofthenon-conditional modelandturningtheminto\u03c9,where\nthebiasparameterswithinarenowafunctionoftheinput. \u03c9\no( t \u2212 1 )o( t \u2212 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\nx xy( ) . . .y( ) . . .\nR R R R R\nFigure10.9:AnRNNthatmapsa\ufb01xed-lengthvectorxintoadistributionoversequences\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).\nRatherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) \u03c4|x( 1 ), . . . ,x( ) \u03c4)thatmakesa\nconditionalindependence assumptionthatthisdistributionfactorizesas\n\ue059\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\ntheoutputattime ttothehiddenunitattime t+1,asshownin\ufb01gure.The10.10\nmodelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\n3 9 2", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t \u2212 1 )o( t \u2212 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\nU U U\nx( t \u2212 1 )x( t \u2212 1 )R\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\nFigure10.10:\u00a0Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence\nofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\n\ufb01gure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\ngivensequencesofxofthesamelength.TheRNNof\ufb01gureisonlyabletorepresent 10.3\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\nthevalues.x\n3 9 3", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\no( t \u2212 1 )o( t \u2212 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t \u2212 1 )L( t \u2212 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t \u2212 1 )y( t \u2212 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t \u2212 1 )x( t \u2212 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t \u2212 1 )g( t \u2212 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t\nFigure10.11:\u00a0Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\npoint t,theoutputunitso( ) tcanbene\ufb01tfromarelevantsummaryofthepastinitsh( ) t\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\n10.3BidirectionalRNNs\nAlloftherecurrentnetworkswehaveconsidereduptonowhavea\u201ccausal\u201dstruc-\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\nx( 1 ), . . . ,x( 1 ) t \u2212,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoa\ufb00ectthecurrentstatewhenthey\nvaluesareavailable.\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\n3 9 4", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis\nalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\ntasks,describedinthenextsection.\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\ne t a l .,).1999\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\nsub-RNNthatmovesbackwardthroughtime.\u00a0Thisallowstheoutputunitso( ) t\ntocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\n\ufb01xed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\naconvolutionalnetwork,oraregularRNNwitha\ufb01xed-sizelook-aheadbu\ufb00er).\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\nhavingRNNs,eachonegoinginoneofthefourdirections:\u00a0up, down,left, f o u r\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea\nrepresentationthatwouldcapturemostlylocalinformationbutcouldalsodepend\non\u00a0long-range inputs,ifthe\u00a0RNN\u00a0isable\u00a0tolearn\u00a0tocarry\u00a0that\u00a0information.\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior\ntotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\ninteractions.\n3 9 5", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\ntures\nWehaveseenin\ufb01gurehowanRNNcanmapaninputsequencetoa\ufb01xed-size 10.5\nvector.Wehaveseenin\ufb01gurehowanRNNcanmapa\ufb01xed-sizevectortoa 10.9\nsequence.\u00a0Wehaveseenin\ufb01gures,,andhowanRNNcan 10.310.410.1010.11\nmapaninputsequencetoanoutputsequenceofthesamelength.\nE nc ode r\n\u2026\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\n\u2026\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\nFigure10.12:\u00a0Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\ngivenoutputsequence).The\ufb01nalhiddenstateoftheencoderRNNisusedtocomputea\ngenerally\ufb01xed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\nsequenceandisgivenasinputtothedecoderRNN.\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength.\u00a0This comesupin\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\n3 9 6", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\nnotofthesamelength(althoughtheirlengthsmightberelated).\nWeoftencalltheinputtotheRNNthe\u201ccontext.\u201dWewanttoproducea\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\nThesimplestRNNarchitectureformappingavariable-length sequenceto\nanothervariable-length sequencewas\ufb01rstproposedby ()and Cho e t a l .2014a\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\ntectureandwerethe\ufb01rsttoobtainstate-of-the-art translationusingthisapproach.\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\nthetranslations.\u00a0Theseauthorsrespectivelycalledthisarchitecture, illustrated\nin\ufb01gure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\n\ufb01nalhiddenstate.\u00a0(2)adecoderorwriteroroutputRNNisconditionedon\nthat\ufb01xed-lengthvector(justlikein\ufb01gure)togeneratetheoutputsequence 10.9\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan\nvaryfromeachother,whilepreviousarchitectures constrained n x= n y= \u03c4.Ina\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\ndecoderRNN.\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-\nsequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\nateachtimestep.Thesetwowayscanalsobecombined.\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder.\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong\nsequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\nthana\ufb01xed-sizevector.Additionally,theyintroducedanattentionmechanism\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\n3 9 7", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequence.Seesectionformoredetails. 12.4.5.1\n10.5DeepRecurrentNetworks\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\nandassociatedtransformations:\n1.\u00a0fromtheinputtothehiddenstate,\n2.\u00a0fromtheprevioushiddenstatetothenexthiddenstate,and\n3.\u00a0fromthehiddenstatetotheoutput.\nWiththeRNNarchitectureof\ufb01gure,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each\nofthesecorrespondstoashallowtransformation.\u00a0Byashallowtransformation,\nwemeanatransformationthatwouldberepresentedbyasinglelayerwithin\nadeepMLP.Typicallythisisatransformationrepresentedbyalearneda\ufb03ne\ntransformationfollowedbya\ufb01xednonlinearity.\nWoulditbeadvantageoustointroducedepthineachoftheseoperations?\nExperimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests\nso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough\ndepthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),\nElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs.\nGraves2013 e t a l .()werethe\ufb01rsttoshowasigni\ufb01cantbene\ufb01tofdecomposing\nthestateofanRNNintomultiplelayersasin\ufb01gure(left).Wecanthink 10.13\nofthelowerlayersinthehierarchydepictedin\ufb01gureaasplayingarole 10.13\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\nthehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther\nandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\nenumeratedabove,asillustratedin\ufb01gureb.Considerationsofrepresentational 10.13\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\nsobyaddingdepthmayhurtlearningbymakingoptimization di\ufb03cult.Ingeneral,\nitiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\n\ufb01gurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable\nintimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden\nlayerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\nshortestpathbetweenvariablesinanytwodi\ufb00erenttimesteps,comparedwiththe\nordinaryRNNof\ufb01gure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this\n3 9 8", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhy\nxz\n( a) ( b) ( c )xhy\nxhy\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\nhidden,hidden-to-hiddenandhidden-to-outputparts.\u00a0Thismaylengthentheshortest\npathlinkingdi\ufb00erenttimesteps.Thepath-lengtheninge\ufb00ectcanbemitigatedby ( c )\nintroducingskipconnections.\n3 9 9", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\nillustratedin\ufb01gurec.10.13\n10.6RecursiveNeuralNetworks\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\nx( 4 )x( 4 )Voo\nU W U WUW\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\nbemappedtoa\ufb01xed-sizerepresentation(theoutputo),witha\ufb01xedsetofparameters\n(theweightmatricesU,V,W).The\ufb01gureillustratesasupervisedlearningcaseinwhich\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\nnetworks,withadi\ufb00erentkindofcomputational graph,whichisstructuredasa\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\ngraphforarecursivenetworkisillustratedin\ufb01gure.Recursiveneural 10.14\n2W e s u g g e s t t o n o t a b b re v i a t e \u201c re c u rs i v e n e u ra l n e t w o rk \u201d a s \u201c R NN\u201d t o a v o i d c o n f u s i o n with\n\u201c re c u rre n t n e u ra l n e t w o rk . \u201d\n4 0 0", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\ncomputervision( ,). Socher e t a l .2011b\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence\nofthesamelength \u03c4,thedepth(measuredasthenumberofcompositionsof\nnonlinearoperations)canbedrasticallyreducedfrom \u03c4to O(log \u03c4),whichmight\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\nlanguagesentences,thetreestructurefortherecursivenetworkcanbe\ufb01xedto\nthestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\nparser( ,,).\u00a0Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby(). Bottou2011\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\nandassociatethe\u00a0inputsandtargetswith\u00a0individualnodesofthe\u00a0tree.The\ncomputationperformedbyeachnodedoesnothavetobethetraditionalarti\ufb01cial\nneuroncomputation(a\ufb03netransformationofallinputsfollowedbyamonotone\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\nrepresentedbycontinuousvectors(embeddings).\n10.7TheChallengeofLong-TermDependencies\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\nwithgradientsnotexploding),thedi\ufb03cultywithlong-termdependenciesarises\nfromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\n4 0 1", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n\u2212 \u2212 \u2212 6 0 4 0 2 0 0 2 0 4 0 6 0\nI nput c o o r di na t e\u2212 4\u2212 3\u2212 2\u2212 101234P r o j e c t i o n o f o utput0\n1\n2\n3\n4\n5\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate\ndowntoasingledimension,plottedonthe y-axis.\u00a0The x-axisisthecoordinateofthe\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\nhasbeencomposed.\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.\nRecurrentnetworksinvolvethecompositionofthesamefunctionmultiple\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\nbehavior,asillustratedin\ufb01gure.10.15\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\nh( ) t= W\ue03eh( 1 ) t \u2212(10.36)\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\nandlackinginputsx.As\u00a0described\u00a0insection\u00a0,\u00a0thisrecurrencerelation 8.2.5\nessentiallydescribesthepowermethod.Itmaybesimpli\ufb01edto\nh( ) t=\ue000\nWt\ue001\ue03eh( 0 ), (10.37)\nandifadmitsaneigendecompositionoftheform W\nWQQ = \u039b\ue03e, (10.38)\n4 0 2", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwithorthogonal ,therecurrencemaybesimpli\ufb01edfurtherto Q\nh( ) t= Q\ue03e\u039btQh( 0 ). (10.39)\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\nwilleventuallybediscarded.\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine\nmultiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\nnetworkthathasadi\ufb00erentweight w( ) tateachtimestep,thesituationisdi\ufb00erent.\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\ue051\nt w( ) t.Suppose\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\ndesiredvariance v\u2217wemaychoosetheindividualweightswithvariance v=n\u221a\nv\u2217.\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN\nmustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\n1994).Speci\ufb01cally,wheneverthemodelisabletorepresentlongtermdependencies,\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\nthegradientofashortterminteraction.\u00a0It doesnotmeanthatitisimpossible\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\n\ufb02uctuationsarisingfromshort-termdependencies.Inpractice,theexperiments\nin ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydi\ufb03cult,\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\nreaching0forsequencesofonlylength10or20.\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious\napproachesthathavebeenproposedtoreducethedi\ufb03cultyoflearninglong-\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\n4 0 3", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\noneofthemainchallengesindeeplearning.\n10.8EchoStateNetworks\nTherecurrentweightsmappingfromh( 1 ) t \u2212toh( ) tandtheinputweightsmapping\nfromx( ) ttoh( ) taresomeofthemostdi\ufb03cultparameterstolearninarecurrent\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\nJaeger2007b,)approachtoavoidingthisdi\ufb03cultyistosettherecurrentweights\nsuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed\nreservoircomputing(Luko\u0161evi\u010diusandJaeger2009,)todenotethefactthat\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturedi\ufb00erent\naspectsofthehistoryofinputs.\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\nhistoryofinputsuptotime t)intoa\ufb01xed-lengthvector(therecurrentstateh( ) t),\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve\ntheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\nsimplelearningalgorithms(,). Jaeger2003\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural\nnetworkstate?\u00a0Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\nJ( ) t=\u2202 s( ) t\n\u2202 s( 1 ) t \u2212.OfparticularimportanceisthespectralradiusofJ( ) t,de\ufb01nedto\nbethemaximumoftheabsolutevaluesofitseigenvalues.\n4 0 4", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTounderstandthee\ufb00ectofthespectralradius,considerthesimplecaseof\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\naneigenvectorvwithcorrespondingeigenvalue \u03bb.Considerwhathappensaswe\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n\nstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\naperturbedversionofg.Ifwebeginwithg+ \u03b4v,thenafteronestep,wewill\nhaveJ(g+ \u03b4v).After nsteps,wewillhaveJn(g+ \u03b4v).Fromthiswecansee\nthatback-propagationstartingfromgandback-propagationstartingfromg+ \u03b4v\ndivergeby \u03b4Jnvafter nstepsofback-propagation.Ifvischosentobeaunit\neigenvectorofJwitheigenvalue \u03bb,thenmultiplicationbytheJacobiansimply\nscalesthedi\ufb00erenceateachstep.Thetwoexecutionsofback-propagationare\nseparatedbyadistanceof \u03b4 \u03bb||n.Whenvcorrespondstothelargestvalueof|| \u03bb,\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\nofsize. \u03b4\nWhen || \u03bb >1,thedeviationsize \u03b4 \u03bb||ngrowsexponentiallylarge.When || \u03bb <1,\nthedeviationsizebecomesexponentiallysmall.\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon\nmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\nradius.\u00a0Indeed,themostrecentworkonechostatenetworksadvocatesusinga\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\nwherethestateh( + 1 ) t= h( ) t \ue03eW.\nWhenalinearmapW\ue03ealwaysshrinkshasmeasuredbythe L2norm,then\nwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\nthepastwhenweusea\ufb01nitelevelofprecision(suchas32bitintegers)tostore\nthestatevector.\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,\nduringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n4 0 5", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\ncoe\ufb03cients,\u00a0whenwemultiplythematrixbythevector.Aneigenvaluewith\nmagnitudegreaterthanonecorrespondstomagni\ufb01cation (exponentialgrowth,if\nappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\nWithanonlinearmap,\u00a0theJacobianisfreetochangeateachstep.The\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\ndi\ufb00erencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\nbounded.Notethat\u00a0itispossible\u00a0forback-propagation to\u00a0retainunbounded\ndynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\nThestrategyofechostatenetworksissimplyto\ufb01xtheweightstohavesome\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\ndoesnotexplodeduetothestabilizinge\ufb00ectofsaturatingnonlinearities liketanh.\nMorerecently,ithasbeenshownthatthetechniquesusedtosettheweights\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\nwiththesparseinitialization schemedescribedinsection.8.4\n10.9LeakyUnitsandOtherStrategiesforMultiple\nTimeScales\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\natmultipletimescales,sothatsomepartsofthemodeloperateat\ufb01ne-grained\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\nscalesandtransferinformationfromthedistantpasttothepresentmoree\ufb03ciently.\nVariousstrategiesforbuildingboth\ufb01neandcoarsetimescalesarepossible.These\nincludetheadditionofskipconnectionsacrosstime,\u201cleakyunits\u201dthatintegrate\nsignalswithdi\ufb00erenttimeconstants,andtheremovalofsomeoftheconnections\n4 0 6", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nusedtomodel\ufb01ne-grainedtimescales.\n10.9.1AddingSkipConnectionsthroughTime\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.\nItispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\ndiminishexponentiallyasafunctionof\u03c4\ndratherthan \u03c4.Sincethereareboth\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin \u03c4.\nThisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\nlong-termdependencies mayberepresentedwellinthisway.\n10.9.2LeakyUnitsandaSpectrumofDi\ufb00erentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\nWhenweaccumulatearunningaverage \u00b5( ) tofsomevalue v( ) tbyapplyingthe\nupdate \u00b5( ) t\u2190 \u03b1 \u00b5( 1 ) t \u2212+(1\u2212 \u03b1) v( ) tthe \u03b1parameterisanexampleofalinearself-\nconnectionfrom \u00b5( 1 ) t \u2212to \u00b5( ) t.When \u03b1isnearone,therunningaverageremembers\ninformationaboutthepastforalongtime,andwhen \u03b1isnearzero,information\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\nunits.\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\nalwayslearntobein\ufb02uencedbyavaluefrom dtimestepsearlier.Theuseofa\nlinearself-connectionwithaweightnearoneisadi\ufb00erentwayofensuringthatthe\nunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\nthise\ufb00ecttobeadaptedmoresmoothlyand\ufb02exiblybyadjustingthereal-valued\n\u03b1ratherthanbyadjustingtheinteger-valuedskiplength.\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n(,). Jaeger e t a l .2007\n4 0 7", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\nunits.\u00a0Onestrategyistomanually\ufb01xthemtovaluesthatremainconstant,for\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\nHavingsuchleakyunitsatdi\ufb00erenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\n10.9.3RemovingConnections\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\ninformation\ufb02owingmoreeasilythroughlongdistancesattheslowertimescales.\nThisideadi\ufb00ersfromtheskipconnectionsthroughtimediscussedearlier\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\nwithlongerconnections.Unitsmodi\ufb01edinsuchawayareforcedtooperateona\nlongtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\nfocusontheirothershort-termconnections.\nTherearedi\ufb00erentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdi\ufb00erenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavedi\ufb00erentgroupsofunitsassociatedwithdi\ufb00erent\ufb01xedtimescales.\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu\ne t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\natdi\ufb00erenttimes,withadi\ufb00erentfrequencyfordi\ufb00erentgroupsofunits.Thisis\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\nwellonanumberofbenchmarkdatasets.\n10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themoste\ufb00ectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe . gatedrecurrentunit\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\ntimethathavederivativesthatneithervanishnorexplode.Leakyunits\u00a0did\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n4 0 8", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nateachtimestep.\n\u00d7\ni nput i nput\u00a0gate f or ge t \u00a0 gate output\u00a0gateoutput\ns t at es e l f - l oop\u00d7\n+ \u00d7\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork\u201ccell.\u201dCellsareconnected\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\nAninputfeatureiscomputedwitharegulararti\ufb01cialneuronunit.Itsvaluecanbe\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa\nlinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\nbeshuto\ufb00bytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\ninputunitcanhaveanysquashingnonlinearity.\u00a0Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\nforaparticularfeatureorcategory)overalongduration.However,oncethat\ninformationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n4 0 9", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\niswhatgatedRNNsdo.\n10.10.1LSTM\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\ncan\ufb02owforlongdurationsisacorecontributionoftheinitiallongshort-term\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\n\ufb01xed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000\nbyanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\nInthiscase,wemeanthatevenforanLSTMwith\ufb01xedparameters,thetimescale\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\ninmanyapplications,\u00a0suchasunconstrainedhandwriting recognition(Graves\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),\nhandwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\nparsing(Vinyals2014a e t a l .,).\nTheLSTMblockdiagramisillustratedin\ufb01gure.Thecorresponding 10.16\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-\nwisenonlinearitytothea\ufb03netransformationofinputsandrecurrentunits,LSTM\nrecurrentnetworkshave\u201cLSTMcells\u201dthathaveaninternalrecurrence(aself-loop),\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\nsystemofgatingunitsthatcontrolsthe\ufb02owofinformation. Themostimportant\ncomponentisthestateunit s( ) t\nithathasalinearself-loopsimilartotheleaky\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe\nassociatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\ni(fortimestep t\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\nf( ) t\ni= \u03c3\uf8eb\n\uf8ed bf\ni+\ue058\njUf\ni , j x( ) t\nj+\ue058\njWf\ni , j h( 1 ) t \u2212\nj\uf8f6\n\uf8f8 ,(10.40)\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\n4 1 0", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\nf( ) t\ni:\ns( ) t\ni= f( ) t\ni s( 1 ) t \u2212\ni + g( ) t\ni \u03c3\uf8eb\n\uf8ed b i+\ue058\njU i , j x( ) t\nj+\ue058\njW i , j h( 1 ) t \u2212\nj\uf8f6\n\uf8f8 ,(10.41)\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\niiscomputed\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n0and1),butwithitsownparameters:\ng( ) t\ni= \u03c3\uf8eb\n\uf8ed bg\ni+\ue058\njUg\ni , j x( ) t\nj+\ue058\njWg\ni , j h( 1 ) t \u2212\nj\uf8f6\n\uf8f8 .(10.42)\nTheoutput h( ) t\nioftheLSTMcellcanalsobeshuto\ufb00,viatheoutputgate q( ) t\ni,\nwhichalsousesasigmoidunitforgating:\nh( ) t\ni= tanh\ue010\ns( ) t\ni\ue011\nq( ) t\ni (10.43)\nq( ) t\ni= \u03c3\uf8eb\n\uf8ed bo\ni+\ue058\njUo\ni , j x( ) t\nj+\ue058\njWo\ni , j h( 1 ) t \u2212\nj\uf8f6\n\uf8f8 (10.44)\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\ni\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\nin\ufb01gure.Thiswouldrequirethreeadditionalparameters. 10.16\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\nthanthesimplerecurrentarchitectures,\ufb01rstonarti\ufb01cialdatasetsdesignedfor\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;\nGraves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\nhavebeenstudiedandusedandarediscussednext.\n10.10.2OtherGatedRNNs\nWhichpieces\u00a0ofthe\u00a0LSTMarchitecture are\u00a0actually necessary?Whatother\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\ncontrolthetimescaleandforgettingbehaviorofdi\ufb00erentunits?\n4 1 1", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\ndi\ufb00erencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\narethefollowing:\nh( ) t\ni= u( 1 ) t \u2212\ni h( 1 ) t \u2212\ni+(1\u2212 u( 1 ) t \u2212\ni) \u03c3\uf8eb\n\uf8ed b i+\ue058\njU i , j x( 1 ) t \u2212\nj +\ue058\njW i , j r( 1 ) t \u2212\nj h( 1 ) t \u2212\nj\uf8f6\n\uf8f8 ,\n(10.45)\nwhereustandsfor\u201cupdate\u201dgateandrfor\u201creset\u201dgate.Theirvalueisde\ufb01nedas\nusual:\nu( ) t\ni= \u03c3\uf8eb\n\uf8ed bu\ni+\ue058\njUu\ni , j x( ) t\nj+\ue058\njWu\ni , j h( ) t\nj\uf8f6\n\uf8f8 (10.46)\nand\nr( ) t\ni= \u03c3\uf8eb\n\uf8ed br\ni+\ue058\njUr\ni , j x( ) t\nj+\ue058\njWr\ni , j h( ) t\nj\uf8f6\n\uf8f8 .(10.47)\nTheresetandupdatesgatescanindividually\u201cignore\u201dpartsofthestatevector.\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany\ndimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\nignoreit(attheotherextreme)byreplacingitbythenew\u201ctargetstate\u201dvalue\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\nadditionalnonlineare\ufb00ectintherelationshipbetweenpaststateandfuturestate.\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.\nAlternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\nacrossawiderangeoftasks(,; Gre\ufb00 e t a l .2015Jozefowicz2015Gre\ufb00 e t a l .,).\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015\nadvocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\nexploredarchitecturalvariants.\n4 1 2", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.11OptimizationforLong-TermDependencies\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\nderivativesmayvanishatthesametimethat\ufb01rstderivativesvanish.Second-order\noptimization algorithmsmayroughlybeunderstoodasdividingthe\ufb01rstderivative\nbythesecondderivative(inhigherdimension,multiplyingthegradientbythe\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetothe\ufb01rst\nderivative,thentheratioof\ufb01rstandsecondderivativesmayremainrelatively\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler\nmethodssuchasNesterovmomentumwithcarefulinitialization couldachieve\nsimilarresults.SeeSutskever2012()formoredetail.\u00a0Bothoftheseapproaches\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\npowerfuloptimization algorithm.\n10.11.1ClippingGradients\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4\nbyarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\neitherverylargeorverysmallinmagnitude.Thisisillustratedin\ufb01gureand8.3\n\ufb01gure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\nparameters)hasa\u201clandscape\u201d\u00a0inwhichone\ufb01nds\u201ccli\ufb00s\u201d:wideandrather\ufb02at\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\nformingakindofcli\ufb00.\nThedi\ufb03cultythatarisesisthatwhentheparametergradientisverylarge,a\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa\nregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\ncorrespondstothesteepestdescentwithinanin\ufb01nitesimalregionsurroundingthe\ncurrentparameters.Outsideofthisin\ufb01nitesimalregion,thecostfunctionmay\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n4 1 3", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\nlandscapeonthenextstep.\n\ue077\n\ue062\ue04a\ue077\ue03b \ue062\n\ue028\ue029\ue057 \ue069 \ue074 \ue068 \ue06f \ue075 \ue074 \ue020 \ue063 \ue06c \ue069 \ue070 \ue070 \ue069 \ue06e \ue067\n\ue077\n\ue062\ue04a\ue077\ue03b \ue062\n\ue028\ue029\ue057 \ue069 \ue074 \ue068 \ue020 \ue063 \ue06c \ue069 \ue070 \ue070 \ue069 \ue06e \ue067\nFigure10.17:Exampleofthee\ufb00ectofgradientclippinginarecurrentnetworkwith\ntwoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\nreasonablyinthevicinityofextremelysteepcli\ufb00s.Thesesteepcli\ufb00scommonlyoccur\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\nThecli\ufb00isexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient\nfromthecli\ufb00face.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\nreactiontothecli\ufb00.Whileitdoesascendthecli\ufb00face,thestepsizeisrestrictedsothat\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\npermissionfromPascanu2013 e t a l .().\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\nclippingthegradient.Therearedi\ufb00erentinstancesofthisidea(Mikolov2012,;\nPascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\nupdate:\nif||||g > v (10.48)\ng\u2190g v\n||||g(10.49)\n4 1 4", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\ngradientofalltheparameters(includingdi\ufb00erentgroupsofparameters,suchas\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient\nnormclipping,theparameterupdatevectornormisnowbounded.Thisbounded\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\ngradientisnumerically InforNan(consideredin\ufb01niteornot-a-number),then\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\nnumericallyunstablecon\ufb01guration. Clippingthegradientnormper-minibatchwill\nnotchangethedirectionofthegradientforanindividualminibatch.However,\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\ntothe\ufb01naldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall\nminibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we\nconjecturethatallthesemethodsbehavesimilarly.\n10.11.2RegularizingtoEncourageInformationFlow\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-\nloopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\ntoregularizeorconstraintheparameterssoastoencourage\u201cinformation\ufb02ow.\u201d\nInparticular,wewouldlikethegradientvector\u2207h( ) t Lbeingback-propagatedto\n4 1 5", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\nendofthesequence.Formally,wewant\n(\u2207h( ) t L)\u2202h( ) t\n\u2202h( 1 ) t \u2212(10.50)\ntobeaslargeas\n\u2207h( ) t L. (10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\n\u2126 =\ue058\nt\uf8eb\n\uf8ed\ue00c\ue00c\ue00c|\u2207(h( ) t L)\u2202 h( ) t\n\u2202 h( 1 ) t \u2212\ue00c\ue00c\ue00c|\n||\u2207h( ) t L||\u22121\uf8f6\n\uf8f82\n. (10.52)\nComputingthegradientofthisregularizermayappeardi\ufb03cult,butPascanu\ne t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\nvectors\u2207h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\nthedependenciesthatanRNNcanlearn.\u00a0BecauseitkeepstheRNNdynamics\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.\nWithoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\nAkeyweaknessofthisapproachisthatitisnotase\ufb00ectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling.\n10.12ExplicitMemory\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\ntherearedi\ufb00erentkindsofknowledge.Someknowledgecanbeimplicit,sub-\nconscious,anddi\ufb03culttoverbalize\u2014suchashowtowalk,orhowadoglooks\ndi\ufb00erentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\nstraightforwardtoputintowords\u2014everydaycommonsense knowledge,like\u201cacat\nisakindofanimal,\u201dorveryspeci\ufb01cfactsthatyouneedtoknowtoaccomplish\nyourcurrentgoals,like\u201cthemeetingwiththesalesteamisat3:00PMinroom\n141.\u201d\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\nmemorizefacts.\u00a0Stochasticgradientdescentrequiresmanypresentationsofthe\n4 1 6", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nT ask \u00a0 ne t w or k ,\nc ontrol l i ng\u00a0th e \u00a0 m e m o r yMe m or y \u00a0 c e l l s\nW r i t i ng\nm e c hani s mR e adi ng\nm e c hani s m\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\ndistinguishthe\u201crepresentation\u201dpartofthemodel(the\u201ctasknetwork,\u201dherearecurrent\nnetinthebottom)fromthe\u201cmemory\u201dpartofthemodel(thesetofcells),whichcan\nstorefacts.Thetasknetworklearnsto\u201ccontrol\u201dthememory,decidingwheretoreadfrom\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\n4 1 7", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\ninformationthat\u00a0arerelevantto\u00a0achieving\u00a0some goal.Suchexplicit\u00a0memory\ncomponentswouldallowoursystemsnotonlytorapidlyand\u201cintentionally\u201dstore\nandretrievespeci\ufb01cfactsbutalsotosequentiallyreasonwiththem.Theneed\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\nresponsestotheinput(,). Hinton1990\nToresolvethisdi\ufb03culty,Weston2014 e t a l .()introducedmemorynetworks\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-\nnism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1\nrelatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\nLSTMsandGRUs.Thedi\ufb00erenceisthatthenetworkoutputsaninternalstate\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\ndigitalcomputerreadfromorwritetoaspeci\ufb01caddress.\nItisdi\ufb03culttooptimizefunctionsthatproduceexact,integeraddresses.To\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\nmodifymultiplecellsbydi\ufb00erentamounts.Thecoe\ufb03cientsfortheseoperations\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows\nthefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\ndescent.Thegradientonthesecoe\ufb03cientsindicateswhethereachofthemshould\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\nmemoryaddressesreceivingalargecoe\ufb03cient.\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\n4 1 8", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\ncostofaccessingamemorycell.\u00a0Wepaythecomputational costofproducinga\ncoe\ufb03cientformanycells,butweexpectthesecoe\ufb03cientstoclusteraroundasmall\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\no\ufb00setsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat\ntheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\nreadinstructionassaying,\u201cRetrievethelyricsofthesongthathasthechorus\u2018We\nallliveinayellowsubmarine.\u2019\u201dContent-basedaddressingismoreusefulwhenwe\nmaketheobjectstoberetrievedlarge\u2014ifeveryletterofthesongwasstoredina\nseparatememorycell,wewouldnotbeableto\ufb01ndthemthisway.Bycomparison,\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\nWecanthinkofalocation-basedreadinstructionassaying\u201cRetrievethelyricsof\nthesonginslot347.\u201dLocation-basedaddressingcanoftenbeaperfectlysensible\nmechanismevenwhenthememorycellsaresmall.\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then\ntheinformationitcontainscanbepropagatedforwardintimeandthegradients\npropagatedbackwardintimewithouteithervanishingorexploding.\nTheexplicitmemoryapproachisillustratedin\ufb01gure,whereweseethat 10.18\na\u201ctaskneuralnetwork\u201d\u00a0iscoupledwithamemory.Althoughthattaskneural\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\nThetasknetworkcanchoosetoreadfromorwritetospeci\ufb01cmemoryaddresses.\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM\nRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\nforverylongdurations.\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\ncells,wecaninterpretthememoryaddressingcoe\ufb03cientsasprobabilities and\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described\ninsection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\ndecisions.\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\n4 1 9", "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmechanism\u00a0forchoosing\u00a0anaddress\u00a0isin\u00a0itsform\u00a0identical\u00a0totheattention\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\ntranslation( ,)anddiscussedinsection.\u00a0Theidea Bahdanau e t a l .2015 12.4.5.1\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In\nthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\nattentioncanmovetoacompletelydi\ufb00erentplace,comparedtothepreviousstep.\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\ntasks.\n4 2 0", "C h a p t e r 1 1\nPractical Methodology\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide\nwhethertogathermoredata,increaseordecreasemodelcapacity,addorremove\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.\nMostofthisbookisaboutdi\ufb00erentmachinelearningmodels,trainingalgo-\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost\nimportantingredienttobeingamachinelearningexpertisknowingawidevariety\nofmachinelearningtechniquesandbeinggoodatdi\ufb00erentkindsofmath.Inprac-\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\nrecommendations inthischapterareadaptedfrom().Ng2015\nWerecommendthefollowingpracticaldesignprocess:\n\u2022Determineyourgoals\u2014whaterrormetrictouse,andyourtargetvaluefor\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\nproblemthattheapplicationisintendedtosolve.\n\u2022Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n421", "CHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics.\n\u2022Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\nisduetoover\ufb01tting,under\ufb01tting, oradefectinthedataorsoftware.\n\u2022Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\nhyperparameters,orchangingalgorithms,basedonspeci\ufb01c\ufb01ndingsfrom\nyourinstrumentation.\nAsarunningexample,wewilluseStreetViewaddressnumbertranscription\nsystem( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\nmethodologyweadvocate.\nWenowdescribeeachofthestepsinthisprocess.\n11.1PerformanceMetrics\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessary\ufb01rst\nstepbecauseyourerrormetricwillguideallofyourfutureactions.\u00a0Youshould\nalsohaveanideaofwhatlevelofperformanceyoudesire.\nKeepinmindthatformostapplications,itisimpossibletoachieveabsolute\nzeroerror.TheBayeserrorde\ufb01nestheminimumerrorratethatyoucanhopeto\nachieve,evenifyouhavein\ufb01nitetrainingdataandcanrecoverthetrueprobability\ndistribution.This\u00a0isbecause\u00a0your\u00a0inputfeatures\u00a0maynot\u00a0contain\u00a0complete\ninformationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\nstochastic.Youwillalsobelimitedbyhavinga\ufb01niteamountoftrainingdata.\nTheamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\nmoney,orhumansu\ufb00ering(forexample,ifyourdatacollectionprocessinvolves\nperforminginvasivemedicaltests).Whenyourgoalistoanswerascienti\ufb01cquestion\naboutwhichalgorithmperformsbetterona\ufb01xedbenchmark,thebenchmark\n4 2 2", "CHAPTER11.PRACTICALMETHODOLOGY\nspeci\ufb01cationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\nmoredata.\nHowcanonedetermineareasonablelevelofperformancetoexpect?Typically,\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\ncost-e\ufb00ective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic\ndesirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\nAnotherimportantconsiderationbesidesthetargetvalueoftheperformance\nmetricisthechoiceofwhichmetrictouse.Severaldi\ufb00erentperformancemetrics\nmaybeusedtomeasurethee\ufb00ectivenessofacompleteapplicationthatincludes\nmachinelearningcomponents.Theseperformancemetricsareusuallydi\ufb00erent\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.\nHowever,manyapplicationsrequiremoreadvancedmetrics.\nSometimesitismuchmorecostlytomakeonekindofamistakethananother.\nForexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\nmeasuringtheerrorrateofaspamclassi\ufb01er,wemaywishtomeasuresomeform\noftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\nofallowingspammessages.\nSometimeswewishtotrainabinaryclassi\ufb01erthatisintendedtodetectsome\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassi\ufb01er\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis\ntoinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,\nwithprecisiononthe y-axisandrecallonthe x-axis.Theclassi\ufb01ergeneratesascore\nthatishigheriftheeventtobedetectedoccurred.\u00a0Forexample,afeedforward\n4 2 3", "CHAPTER11.PRACTICALMETHODOLOGY\nnetworkdesignedtodetectadiseaseoutputs \u02c6 y= P( y=1| x),estimatingthe\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\nthreshold.\u00a0Byvaryingthethreshold,wecantradeprecisionforrecall.\u00a0Inmany\ncases,wewishtosummarizetheperformanceoftheclassi\ufb01erwithasinglenumber\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\nF-scor egivenby\nF=2 pr\np r+. (11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\nInsomeapplications,itispossibleforthemachinelearningsystemtorefuseto\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\nhowcon\ufb01dentitshouldbeaboutadecision,especiallyifawrongdecisioncan\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto\ntranscribetheaddressnumberfromaphotographinordertoassociatethelocation\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.\nOfcourse,themachinelearningsystemisonlyusefulifitisabletodramatically\nreducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.\nItispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\nis98%accuracy.\nManyothermetricsarepossible.Wecanforexample,measureclick-through\nrates,collectusersatisfactionsurveys,andsoon.\u00a0Manyspecializedapplication\nareashaveapplication-speci\ufb01ccriteriaaswell.\nWhatisimportantistodeterminewhichperformancemetrictoimproveahead\noftime,thenconcentrateonimprovingthismetric.Withoutclearlyde\ufb01nedgoals,\nitcanbedi\ufb03culttotellwhetherchangestoamachinelearningsystemmake\nprogressornot.\n4 2 4", "CHAPTER11.PRACTICALMETHODOLOGY\n11.2DefaultBaselineModels\nAfterchoosingperformancemetricsandgoals,\u00a0thenextstepinanypractical\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\nthissection,weproviderecommendations forwhichalgorithmstouseasthe\ufb01rst\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\nafterthiswriting.\nDependingonthecomplexityofyourproblem,youmayevenwanttobegin\nwithoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\nstatisticalmodellikelogisticregression.\nIfyouknowthatyourproblemfallsintoan\u201cAI-complete\u201dcategorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel.\nFirst,choosethegeneralcategoryofmodelbasedonthestructureofyour\ndata.Ifyouwanttoperformsupervisedlearningwith\ufb01xed-sizevectorsasinput,\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\nAreasonablechoiceofoptimization algorithmisSGDwithmomentumwitha\ndecayinglearningrate(populardecayschemesthatperformbetterorworseon\ndi\ufb00erentproblemsincludedecayinglinearlyuntilreachinga\ufb01xedminimumlearning\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.\nBatchnormalization canhaveadramatice\ufb00ectonoptimization performance,\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\nWhileitisreasonabletoomitbatchnormalization fromthevery\ufb01rstbaseline,it\nshouldbeintroducedquicklyifoptimization appearstobeproblematic.\nUnlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\neachvariable.\n4 2 5", "CHAPTER11.PRACTICALMETHODOLOGY\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\nwillprobablydowellby\ufb01rstcopyingthemodelandalgorithmthatisalready\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\ntasks( ,). Girshicketal.2015\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-\nscribedfurtherinpart.Thisissomewhatdomainspeci\ufb01c.Somedomains,such III\nasnaturallanguageprocessing,areknowntobene\ufb01ttremendouslyfromunsuper-\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\nnotbringabene\ufb01t,exceptinthesemi-supervisedsetting,whenthenumberof\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,\nthenincludeitinyour\ufb01rstend-to-endbaseline.Otherwise,onlyuseunsupervised\nlearninginyour\ufb01rstattemptifthetaskyouwanttosolveisunsupervised.You\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\nbaselineover\ufb01ts.\n11.3DeterminingWhethertoGatherMoreData\nAfterthe\ufb01rstend-to-endsystemisestablished,itistimetomeasuretheperfor-\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\nnovicesaretemptedtomakeimprovementsbytryingoutmanydi\ufb00erentalgorithms.\nHowever,itisoftenmuchbettertogathermoredatathantoimprovethelearning\nalgorithm.\nHowdoesonedecidewhethertogathermoredata?First,determinewhether\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\nAlso,tryimprovingthelearningalgorithm,forexamplebytuningthelearning\nratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\nrichersetoffeatures.\nIftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n4 2 6", "CHAPTER11.PRACTICALMETHODOLOGY\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\ntrainingsetperformance,thengatheringmoredataisoneofthemoste\ufb00ective\nsolutions.\u00a0Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance\nsigni\ufb01cantly.\u00a0Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove\nregularization, byadjustinghyperparameters suchasweightdecaycoe\ufb03cients,\norbyaddingregularizationstrategiessuchasdropout.Ifyou\ufb01ndthatthegap\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\nregularizationhyperparameters ,thengatheringmoredataisadvisable.\nWhendecidingwhethertogathermoredata,itisalsonecessarytodecide\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\ntrainingsetsizeandgeneralization error,likein\ufb01gure.Byextrapolatingsuch 5.4\ncurves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.\nIfgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe\ndomainofresearchandnotthedomainofadviceforappliedpractitioners.\n11.4SelectingHyperparameters\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\naspectsofthealgorithm\u2019sbehavior.Someofthesehyperparametersa\ufb00ectthetime\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters a\ufb00ect\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\ncorrectresultswhendeployedonnewinputs.\nTherearetwobasicapproachestochoosingthesehyperparameters :choosing\nthemmanuallyandchoosingthemautomatically .Choosingthehyperparameters\n4 2 7", "CHAPTER11.PRACTICALMETHODOLOGY\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly.\n1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\nhyperparameters,trainingerror,generalization errorandcomputational resources\n(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\ndamentalideasconcerningthee\ufb00ectivecapacityofalearningalgorithmfrom\nchapter.5\nThegoalofmanualhyperparametersearchisusuallyto\ufb01ndthelowestgeneral-\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\nbecausethisishighlyplatform-dependent.\nTheprimarygoalofmanualhyperparametersearchistoadjustthee\ufb00ective\ncapacityofthemodeltomatchthecomplexityofthetask.E\ufb00ectivecapacity\nisconstrainedbythreefactors:\u00a0therepresentationalcapacityofthemodel,the\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\nhigherrepresentationalcapacity\u2014itiscapableofrepresentingmorecomplicated\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if\nthetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\nsomeofthesefunctions.\nThegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asin\ufb01gure.\u00a0Atoneextreme,the 5.3\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\nbecausetrainingerrorishigh.Thisistheunder\ufb01ttingregime.Attheotherextreme,\nthehyperparameter valuecorrespondstohighcapacity,andthegeneralization\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\noftrainingerror.\nForsomehyperparameters,over\ufb01ttingoccurswhenthevalueofthehyper-\nparameterislarge.\u00a0Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8", "CHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\nForsomehyperparameters ,over\ufb01ttingoccurswhenthevalueofthehyperparame-\nterissmall.Forexample,thesmallestallowableweightdecaycoe\ufb03cientofzero\ncorrespondstothegreateste\ufb00ectivecapacityofthelearningalgorithm.\nNoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\nManyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe\nnumberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\nareswitchesthat\u00a0specify\u00a0whetherornotto\u00a0usesomeoptionalcomponentof\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome\npartofthecurve.Forexample,theminimumweightdecaycoe\ufb03cientiszero.This\nmeansthatifthemodelisunder\ufb01ttingwhenweightdecayiszero,wecannotenter\ntheover\ufb01ttingregionbymodifyingtheweightdecaycoe\ufb03cient.Inotherwords,\nsomehyperparameters canonlysubtractcapacity.\nThelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhave\u00a0timeto\u00a0tuneonly\u00a0onehyperparameter, tune\u00a0thelearning\u00a0rate. It\u00a0con-\ntrolsthee\ufb00ectivecapacityofthemodelinamorecomplicatedwaythanother\nhyperparameters\u2014thee\ufb00ectivecapacityofthemodelishighestwhenthelearning\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\nillustratedin\ufb01gure.Whenthelearningrateistoolarge,gradientdescent 11.1\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits\noptimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.\nThise\ufb00ectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\nTuningtheparametersotherthanthelearningraterequiresmonitoringboth\ntrainingandtesterrortodiagnosewhetheryourmodelisover\ufb01ttingorunder\ufb01tting,\nthenadjustingitscapacityappropriately .\nIfyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave\nnochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\ncon\ufb01dentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\nincreasesthecomputational costsassociatedwiththemodel.\nIfyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\n4 2 9", "CHAPTER11.PRACTICALMETHODOLOGY\n1 0\u2212 21 0\u2212 11 00\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisfora\ufb01xed\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\nfactorproportionaltothelearningratereduction.\u00a0Generalizationerrorcanfollowthis\ncurveorbecomplicatedbyregularizatione\ufb00ectsarisingoutofhavingatoolargeor\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\nover\ufb01tting,andevenpointswithequivalenttrainingerrorcanhavedi\ufb00erentgeneralization\nerror.\nnowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\no\ufb00thesequantities.Neuralnetworkstypicallyperformbestwhenthetraining\nerrorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\ndrivenbythegapbetweentrainandtesterror.\u00a0Yourgoalistoreducethisgap\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\nchangeregularizationhyperparameters toreducee\ufb00ectivemodelcapacity,suchas\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\nlargemodelthatisregularizedwell,forexamplebyusingdropout.\nMosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor\ndecreasemodelcapacity.SomeexamplesareincludedinTable.11.1\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational\ncostoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n4 3 0", "CHAPTER11.PRACTICALMETHODOLOGY\nHyperparameterIncreases\ncapacity\nwhen...Reason Caveats\nNumberofhid-\ndenunitsincreasedIncreasingthenumberof\nhiddenunitsincreasesthe\nrepresentationalcapacity\nofthemodel.Increasingthenumber\nofhiddenunits\u00a0increases\nboththetimeandmemory\ncostofessentiallyeveryop-\nerationonthemodel.\nLearningratetunedop-\ntimallyAnimproperlearningrate,\nwhether\u00a0toohigh\u00a0ortoo\nlow,resultsinamodel\nwithlowe\ufb00ectivecapacity\nduetooptimizationfailure\nConvolutionker-\nnelwidthincreasedIncreasingthekernelwidth\nincreasesthenumberofpa-\nrametersinthemodelAwiderkernelresultsin\nanarroweroutputdimen-\nsion,reducingmodelca-\npacityunlessyouuseim-\nplicitzeropaddingtore-\nducethise\ufb00ect.Wider\nkernelsrequiremoremem-\noryforparameterstorage\nandincreaseruntime,but\nanarroweroutputreduces\nmemorycost.\nImplicitzero\npaddingincreasedAddingimplicitzerosbe-\nforeconvolutionkeepsthe\nrepresentationsizelargeIncreasedtimeandmem-\norycostofmostopera-\ntions.\nWeightdecayco-\ne\ufb03cientdecreasedDecreasingtheweightde-\ncaycoe\ufb03cientfreesthe\nmodelparameterstobe-\ncomelarger\nDropoutratedecreasedDroppingunitslessoften\ngivestheunitsmoreoppor-\ntunitiesto\u201cconspire\u201dwith\neachotherto\ufb01tthetrain-\ningset\nTable11.1:Thee\ufb00ectofvarioushyperparametersonmodelcapacity.\n4 3 1", "CHAPTER11.PRACTICALMETHODOLOGY\nprinciple,thisapproachcouldfailduetooptimization di\ufb03culties,butformany\nproblemsoptimization doesnotseemtobeasigni\ufb01cantbarrier,providedthatthe\nmodelischosenappropriately .\n1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning\nalgorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\noftenbene\ufb01tsigni\ufb01cantlyfromtuningoffortyormorehyperparameters .Manual\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring\nhyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\nautomatedalgorithmscan\ufb01ndusefulvaluesofthehyperparameters .\nIfwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\nwearetryingto\ufb01ndavalueofthehyperparametersthatoptimizesanobjective\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget\nfortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\nto\u00a0develop h y p e r par am e t e r \u00a0 o p t i m i z a t i o nalgorithms\u00a0thatwrap\u00a0a\u00a0learnin g\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\nshouldbeexploredforeachofthelearningalgorithm\u2019shyperparameters .However,\nthesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\nsecondaryhyperparameters foralltasks.\n1 1 . 4 . 3 G ri d S ea rch\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\ng r i d se ar c h.Foreachhyperparameter,\u00a0the userselectsasmall\ufb01nitesetof\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\nspeci\ufb01cationofhyperparametervaluesintheCartesianproductofthesetofvalues\nforeachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n4 3 2", "CHAPTER11.PRACTICALMETHODOLOGY\nGrid Random\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )\nhyperparametercon\ufb01gurations.Usuallymostofthesehyperparametersareindependent\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\nhyperparametercon\ufb01gurationsandrunstrainingwitheachofthem.Bothgridsearch\nandrandomsearchevaluatethevalidationseterrorandreturnthebestcon\ufb01guration.\nThe\ufb01gureillustratesthetypicalcasewhereonlysomehyperparametershaveasigni\ufb01cant\nin\ufb02uenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\nhasasigni\ufb01cante\ufb00ect.Gridsearchwastesanamountofcomputationthatisexponential\ninthenumberofnon-in\ufb02uentialhyperparameters,whilerandomsearchtestsaunique\nvalueofeveryin\ufb02uentialhyperparameteronnearlyeverytrial.Figurereproducedwith\npermissionfrom (). BergstraandBengio2012\n4 3 3", "CHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\n\ufb01gureforanillustrationofagridofhyperparameter values. 11.2\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid\nsearchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\nratetakenwithintheset{ .1 , .01 ,10\u22123,10\u22124,10\u22125},oranumberofhiddenunits\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter \u03b1usingvaluesof{\u22121 ,0 ,1}.\nIfthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 \u03b1\nliesandweshouldshiftthegridandrunanothersearchwith \u03b1in,forexample,\n{1 ,2 ,3}.Ifwe\ufb01ndthatthebestvalueof \u03b1is,thenwemaywishtore\ufb01neour 0\nestimatebyzoominginandrunningagridsearchover. {\u2212 } . , , .101\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\nparallelism(withalmostnoneedforcommunication betweendi\ufb00erentmachines\ncarryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\nevenparallelization maynotprovideasatisfactorysizeofsearch.\n1 1 . 4 . 4 Ra n d o m S ea rch\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\nrandomsearch( ,). BergstraandBengio2012\nArandomsearchproceedsasfollows.Firstwede\ufb01neamarginaldistribution\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete\nhyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\nhyperparameters.Forexample,\nl o g l e a r n i n g r a t e __ \u223c\u2212\u2212 u(1 ,5) (11.2)\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b).\nSimilarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\nlog(2000) ).\n4 3 4", "CHAPTER11.PRACTICALMETHODOLOGY\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\nnotincuradditionalcomputational cost.\u00a0Infact,asillustratedin\ufb01gure,a11.2\nrandomsearchcanbeexponentiallymoree\ufb03cientthanagridsearch,whenthere\nareseveralhyperparametersthatdonotstronglya\ufb00ecttheperformancemeasure.\nThisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012\nsearchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\nthenumberoftrialsrunbyeachmethod.\nAswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,tore\ufb01nethesearchbasedontheresultsofthe\ufb01rstrun.\nThemainreasonwhyrandomsearch\ufb01ndsgoodsolutionsfasterthangridsearch\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters\nwouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\nwouldusuallyhavedi\ufb00erentvalues.Henceifthechangebetweenthesetwovalues\ndoesnotmarginallymakemuchdi\ufb00erenceintermsofvalidationseterror,grid\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters .\n1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem.\nThedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\nsimpli\ufb01edsettingswhereitisfeasibletocomputethegradientofsomedi\ufb00erentiable\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal.\n2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving\nintrinsicallynon-di\ufb00erentiable interactionswiththevalidationseterror,asinthe\ncaseofdiscrete-valuedhyperparameters .\nTocompensateforthislackofagradient,wecanbuildamodelofthevalidation\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-\nmizationthusinvolvesatradeo\ufb00betweenexploration(proposinghyperparameters\n4 3 5", "CHAPTER11.PRACTICALMETHODOLOGY\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\niscon\ufb01dentwillperformaswellasanyhyperparameters ithasseensofar\u2014usually\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\nTPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011\nCurrently,wecannotunambiguously recommendBayesianhyperparameter\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\nforobtainingthoseresultswithlesse\ufb00ort.Bayesianhyperparameteroptimization\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\naparticularproblembutisnotyetsu\ufb03cientlymatureorreliable.Thatbeing\nsaid,hyperparameter optimization isanimportant\ufb01eldofresearchthat,while\noftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobene\ufb01t\nnotonlytheentire\ufb01eldofmachinelearningbutthedisciplineofengineeringin\ngeneral.\nOnedrawbackcommontomosthyperparameter optimization algorithmswith\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\nfromtheexperiment.Thisismuchlesse\ufb03cient,inthesenseofhowmuchinfor-\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman\npractitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\nexperiment,to\u201cfreeze\u201darunningexperimentthatisnotpromising,orto\u201cthaw\u201d\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\nmoreinformation.\n11.5DebuggingStrategies\nWhenamachinelearningsystemperformspoorly,itisusuallydi\ufb03culttotell\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\nisabugintheimplementation ofthealgorithm.\u00a0Machine learningsystemsare\ndi\ufb03culttodebugforavarietyofreasons.\nInmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n4 3 6", "CHAPTER11.PRACTICALMETHODOLOGY\nneuralnetworkonaclassi\ufb01cationtaskanditachieves5%testerror,wehave new\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\nbehavior.\nAfurtherdi\ufb03cultyisthatmostmachinelearningmodelshavemultipleparts\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose\nfurtherthatwehavemanuallyimplemented thegradientdescentruleforeach\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\nb b\u2190\u2212 \u03b1 (11.4)\nwhere \u03b1isthelearningrate.Thiserroneousupdatedoesnotusethegradientat\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\nDependingonthedistributionoftheinput,theweightsmaybeabletoadaptto\ncompensateforthenegativebiases.\nMostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\nbothofthesetwodi\ufb03culties.Eitherwedesignacasethatissosimplethatthe\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\npartoftheneuralnetimplementationinisolation.\nSomeimportantdebuggingtestsinclude:\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed\nsuperimposedontheimage.Whentrainingagenerativemodelofspeech,listento\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis\nperformingwellwhenitisnot.\nVisualizetheworstmistakes:\u00a0Mostmodelsareabletooutputsomesortof\ncon\ufb01dencemeasureforthetasktheyperform.Forexample,classi\ufb01ersbasedona\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\ntothemostlikelyclassthusgivesanestimateofthecon\ufb01dencethemodelhasin\nitsclassi\ufb01cationdecision.Typically,maximumlikelihoodtrainingresultsinthese\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n4 3 7", "CHAPTER11.PRACTICALMETHODOLOGY\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.\nForexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit\nsomeofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\ncon\ufb01dentmistakesshowedthattherewasasystematicproblemwiththecropping.\nModifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\nnumbers.\nReasoningaboutsoftwareusingtrainandtesterror:Itisoftendi\ufb03cultto\ndeterminewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\nmodelisover\ufb01ttingforfundamentalalgorithmicreasons.Analternativepossibility\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\nwasprepareddi\ufb00erentlyfromthetrainingdata.Ifbothtrainandtesterrorare\nhigh,thenitisdi\ufb03culttodeterminewhetherthereisasoftwaredefectorwhether\nthemodelisunder\ufb01ttingduetofundamentalalgorithmicreasons.Thisscenario\nrequiresfurthertests,describednext.\nFitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\nitisduetogenuineunder\ufb01ttingorduetoasoftwaredefect.Usuallyevensmall\nmodelscanbeguaranteedtobeable\ufb01tasu\ufb03cientlysmalldataset.Forexample,\naclassi\ufb01cationdatasetwithonlyoneexamplecanbe\ufb01tjustbysettingthebiases\noftheoutputlayercorrectly.Usuallyifyoucannottrainaclassi\ufb01ertocorrectly\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\nwithhigh\ufb01delity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\nCompareback-propagatedderivativestonumericalderivatives:Ifyouareusing\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-\nputations,orifyouareaddinganewoperationtoadi\ufb00erentiation libraryand\nmustde\ufb01neitsbpropmethod,thenacommonsourceoferrorisimplementingthis\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n4 3 8", "CHAPTER11.PRACTICALMETHODOLOGY\nistocomparethederivativescomputedbyyourimplementation ofautomatic\ndi\ufb00erentiationtothederivativescomputedbya .Because \ufb01ni t e di \ufb00 e r e nc e s\nf\ue030() =lim x\n\ue00f \u21920f x \ue00f f x (+)\u2212()\n\ue00f, (11.5)\nwecanapproximate thederivativebyusingasmall,\ufb01nite: \ue00f\nf\ue030() x\u2248f x \ue00f f x (+)\u2212()\n\ue00f. (11.6)\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di \ufb00 e r -\ne nc e:\nf\ue030() x\u2248f x(+1\n2\ue00f f x )\u2212(\u22121\n2 \ue00f)\n\ue00f. (11.7)\nTheperturbationsize \ue00fmustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchby\ufb01nite-precisionnumericalcomputations.\nUsually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\ng: Rm\u2192 Rn.Unfortunately,\ufb01nitedi\ufb00erencingonlyallowsustotakeasingle\nderivativeatatime.Wecaneitherrun\ufb01nitedi\ufb00erencing m ntimestoevaluateall\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses\nrandomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\nwhere uand varerandomlychosenvectors.Computing f\ue030( x)correctlyrequires\nbeingabletoback-propagatethrough gcorrectly,yetise\ufb03cienttodowith\ufb01nite\ndi\ufb00erencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\nprojection.\nIfonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\naverye\ufb03cientwaytonumericallyestimatethegradientbyusingcomplexnumbers\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\nobservationthat\nf x i \ue00f f x i \ue00f f (+) = ()+\ue030()+( x O \ue00f2) (11.8)\nreal((+)) = ()+( f x i \ue00f f x O \ue00f2)imag( ,f x i \ue00f (+)\n\ue00f) = f\ue030()+( x O \ue00f2) ,(11.9)\nwhere i=\u221a\n\u22121.Unlikeinthereal-valuedcaseabove,thereisnocancellatione\ufb00ect\nduetotakingthedi\ufb00erencebetweenthevalueof fatdi\ufb00erentpoints.Thisallows\ntheuseoftinyvaluesof \ue00flike \ue00f= 10\u2212150,whichmakethe O( \ue00f2)errorinsigni\ufb01cant\nforallpracticalpurposes.\n4 3 9", "CHAPTER11.PRACTICALMETHODOLOGY\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrecti\ufb01ers,\nhowoftenaretheyo\ufb00?Arethereunitsthatarealwayso\ufb00?Fortanhunits,\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated\ntheunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\nAssuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare\nstalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\nevolution.\nFinally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\nproblems.\u00a0Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\nSomeguaranteesthatsomeoptimizationalgorithmso\ufb00erincludethattheobjective\nfunctionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\nUsuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter.\n11.6Example:Multi-DigitNumberRecognition\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology\ninpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\ndatabaseinfrastructure,andsoon,wereofparamountimportance.\nFromthepointofviewofthemachinelearningtask,theprocessbeganwith\ndatacollection.\u00a0The carscollectedtherawdataandhumanoperatorsprovided\nlabels.Thetranscriptiontaskwasprecededbyasigni\ufb01cantamountofdataset\ncuration,includingusingothermachinelearningtechniquestodetectthehouse\n4 4 0", "CHAPTER11.PRACTICALMETHODOLOGY\nnumberspriortotranscribingthem.\nThetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics.\u00a0Animportantgeneralprincipleistotailorthe\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\nforthisproject.\u00a0Speci\ufb01cally,thegoalwastoobtainhuman-level,98%accuracy.\nThislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach\nthislevelofaccuracy,theStreetViewtranscriptionsystemsacri\ufb01cescoverage.\nCoveragethusbecamethemainperformancemetricoptimizedduringtheproject,\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\npossibletoreducethecon\ufb01dencethresholdbelowwhichthenetworkrefusesto\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.\nAfterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa\nconvolutionalnetworkwithrecti\ufb01edlinearunits.Thetranscriptionprojectbegan\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\nbaseline,the\ufb01rstimplementation oftheoutputlayerofthemodelconsistedof n\ndi\ufb00erentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\nweretrainedexactlythesameasifthetaskwereclassi\ufb01cation,witheachsoftmax\nunittrainedindependently.\nOurrecommendedmethodologyistoiterativelyre\ufb01nethebaselineandtest\nwhethereachchangemakesanimprovement.The\ufb01rstchangetotheStreetView\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\nmetricandthestructureofthedata.Speci\ufb01cally,thenetworkrefusestoclassify\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\nsomethreshold t.Initially,thede\ufb01nitionof p( y x|)wasad-hoc,basedonsimply\nmultiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\nmuchmoree\ufb00ectively.\nAtthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem\nisunder\ufb01ttingorover\ufb01tting.Inthiscase,trainandtestseterrorwerenearly\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\n4 4 1", "CHAPTER11.PRACTICALMETHODOLOGY\ntounder\ufb01ttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\nstrategieswerecommendistovisualizethemodel\u2019sworsterrors.Inthiscase,that\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\nhighestcon\ufb01dence.Theseprovedtomostlyconsistofexampleswheretheinput\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\nremovedbythecroppingoperation.Forexample,aphotoofanaddress\u201c1849\u201d\nmightbecroppedtootightly,withonlythe\u201c849\u201dremainingvisible.Thisproblem\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\nsystem\u2019scoverage.\nFinally,thelastfewpercentagepointsofperformancecamefromadjusting\nhyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\nremainedroughlyequal,itwasalwaysclearthatanyperformancede\ufb01citsweredue\ntounder\ufb01tting, aswellasduetoafewremainingproblemswiththedatasetitself.\nOverall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\nhavebeenpossibleviahumane\ufb00ort.\nWehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\nothersimilarsuccesses.\n4 4 2", "C h a p t e r 1 2\nA p p l i cat i on s\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\nspeci\ufb01capplicationareasthatdeeplearninghasbeenusedtosolve.\u00a0Whileone\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad\nvarietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\nLanguagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature.\n12. 1 L arge- S c a l e D eep L earni n g\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan\nexhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\nimprovementinneuralnetwork\u2019saccuracyandtheimprovementofthecomplexity\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\ngrownexponentiallyforthepastthreedecades,yetarti\ufb01cialneuralnetworksare\nonlyaslargeasthenervoussystemsofinsects.\nBecausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443", "CHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure.\n12.1.1FastCPUImplementations\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\nToday,thisapproachisgenerallyconsideredinsu\ufb03cient.WenowmostlyuseGPU\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.\nAdescriptionofhowtoimplemente\ufb03cientnumericalCPUcodeisbeyond\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\nspeci\ufb01cCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusing\ufb01xed-point\narithmeticratherthan\ufb02oating-pointarithmetic.Bycreatingacarefullytuned\ufb01xed-\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\nastrong\ufb02oating-pointsystem.EachnewmodelofCPUhasdi\ufb00erentperformance\ncharacteristics,sosometimes\ufb02oating-pointimplementations canbefastertoo.\nTheimportantprincipleisthatcarefulspecializationofnumericalcomputation\nroutinescanyieldalargepayo\ufb00.Otherstrategies,besideschoosingwhethertouse\n\ufb01xedor\ufb02oatingpoint,includeoptimizingdatastructurestoavoidcachemisses\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\nimplementationdetails,butwhentheperformanceofanimplementation restricts\nthesizeofthemodel,theaccuracyofthemodelsu\ufb00ers.\n12.1.2GPUImplementations\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\nbene\ufb01cialforneuralnetworksaswell.\nVideogamerenderingrequiresperformingmanyoperationsinparallelquickly.\nModelsof\u00a0characters\u00a0and environments\u00a0arespeci\ufb01ed\u00a0intermsof\u00a0listsof\u00a03-D\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\nateachpixelinparalleltodeterminethecolorofeachpixel.\u00a0Inbothcases,the\n4 4 4", "CHAPTER12.APPLICATIONS\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebu\ufb00ersof\nmemory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\nCPUs.\nNeuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\nlargeandnumerousbu\ufb00ersofparameters,activationvalues,andgradientvalues,\neachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\nbu\ufb00ersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.\nGPUso\ufb00eracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.\nNeuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\nnetworkscanbedividedintomultipleindividual\u201cneurons\u201dthatcanbeprocessed\nindependentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\nbene\ufb01tfromtheparallelismofGPUcomputing.\nGPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\ngraphicstasks.Overtime,GPUhardwarebecamemore\ufb02exible,allowingcustom\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\nbasedonarenderingtask.TheseGPUscouldbeusedforscienti\ufb01ccomputingby\nwritingtheoutputofacomputationtoabu\ufb00erofpixelvalues.Steinkrau e t a l .\n()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\nacceleratesupervisedconvolutionalnetworks.\nThepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary\ncode,notjustrenderingsubroutines.\u00a0NVIDIA\u2019sCUDAprogramming language\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\n4 4 5", "CHAPTER12.APPLICATIONS\nbandwidth,GP-GPUsnowo\ufb00eranidealplatformforneuralnetworkprogramming.\nThisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\nWritinge\ufb03cientcodeforGP-GPUsremainsadi\ufb03culttaskbestlefttospe-\ncialists.\u00a0ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\ndi\ufb00erentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most\nwritablememorylocationsarenotcached,soitcanactuallybefastertocompute\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.\nGPUcodeisalsoinherentlymulti-threaded andthedi\ufb00erentthreadsmustbe\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\ntransaction.Di\ufb00erentmodelsofGPUsareabletocoalescedi\ufb00erentkindsofread\norwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\nof2.\u00a0Theexactspeci\ufb01cationsdi\ufb00erbetweenmodelsofGPU.Anothercommon\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\nsameinstructionsimultaneously.Thismeansthatbranchingcanbedi\ufb03culton\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\nexecutesthesameinstructionduringeachcycle,soifdi\ufb00erentthreadswithinthe\nsamewarpneedtoexecutedi\ufb00erentcodepaths,thesedi\ufb00erentcodepathsmust\nbetraversedsequentiallyratherthaninparallel.\nDuetothedi\ufb03cultyofwritinghighperformanceGPUcode,researchersshould\nstructuretheirwork\ufb02owtoavoidneedingtowritenewGPUcodeinordertotest\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the\nmachinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speci\ufb01esallofits\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.\nOtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l .\n2011b)providesimilarfeatures.\n4 4 6", "CHAPTER12.APPLICATIONS\n12.1.3Large-ScaleDistributedImplementations\nInmanycases,thecomputational resourcesavailableonasinglemachineare\ninsu\ufb03cient.Wethereforewanttodistributetheworkloadoftrainingandinference\nacrossmanymachines.\nDistributinginferenceissimple,becauseeachinputexamplewewanttoprocess\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\ntogetheronasingledatapoint,witheachmachinerunningadi\ufb00erentpartofthe\nmodel.Thisisfeasibleforbothinferenceandtraining.\nDataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\nthestandardde\ufb01nitionofgradientdescentisasacompletelysequentialalgorithm:\nthegradientatstepisafunctionoftheparametersproducedbystep. t t\u22121\nThiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.\nThisreducestheaverageamountofimprovementthateachgradientdescentstep\nyields,becausesomeofthecoresoverwriteeachother\u2019sprogress,buttheincreased\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean\ne t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\n2015).Academicdeeplearningresearcherstypicallycannota\ufb00ordthesamescale\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild\ndistributednetworkswithrelativelylow-costhardwareavailableintheuniversity\nsetting( ,). Coates e t a l .2013\n12.1.4ModelCompression\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\n4 4 7", "CHAPTER12.APPLICATIONS\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\npowerfulcomputercluster,thendeployitonmobilephones.\nAkeystrategyforreducingthecostofinferenceismodelcompression(Bu-\ncilu\u02c7a2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto\nstoreandevaluate.\nModelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\nprimarilybyaneedtopreventover\ufb01tting.Inmostcases,themodelwiththe\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.\nEvaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\nTheselargemodelslearnsomefunction f(x),butdosousingmanymore\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto\nthelimitednumberoftrainingexamples.Assoonaswehave\ufb01tthisfunction\nf(x),wecangenerateatrainingsetcontainingin\ufb01nitelymanyexamples,simply\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\nmodeltomatch f(x)onthesepoints.Inordertomoste\ufb03cientlyusethecapacity\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative\nmodeltrainedontheoriginaltrainingset.\nAlternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).\n12.1.5DynamicStructure\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich\nsubsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\ncomputation(,; ,).\u00a0Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n4 4 8", "CHAPTER12.APPLICATIONS\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\nDynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\ngenerallythroughoutthesoftwareengineeringdiscipline.\u00a0Thesimplestversions\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\nbeappliedtoaparticularinput.\nAvenerablestrategyforacceleratinginferenceinaclassi\ufb01eristouseacascade\nofclassi\ufb01ers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\nwemustuseasophisticatedclassi\ufb01erwithhighcapacity,thatisexpensivetorun.\nHowever,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\nasequenceofclassi\ufb01ers.The\ufb01rstclassi\ufb01ersinthesequencehavelowcapacity,\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure\nwedonotwronglyrejectaninputwhentheobjectispresent.The\ufb01nalclassi\ufb01er\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\nclassi\ufb01ersinasequence,abandoninganyexampleassoonasanyoneelementin\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\nhighcon\ufb01dence,usingahighcapacitymodel,butdoesnotforceustopaythecost\noffullinferenceforeveryexample.Therearetwodi\ufb00erentwaysthatthecascade\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade\nindividuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\nhighcapacity,becausesomeofitsindividualmembersdo.\u00a0Itisalsopossibleto\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassi\ufb01er\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows\nareexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\nthat\ufb01rstlocatestheaddressnumberwithonemachinelearningmodelandthen\ntranscribesitwithanother(Goodfellow2014d e t a l .,).\nDecisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\nAsimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n4 4 9", "CHAPTER12.APPLICATIONS\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\ndonewiththeprimarygoalofacceleratinginferencecomputations.\nInthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\ngiventhecurrentinput.The\ufb01rstversionofthisideaiscalledthemixtureof\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset\nofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\nandthe\ufb01naloutputisobtainedbytheweightedcombinationoftheoutputof\ntheexperts.Inthatcase,\u00a0theuseofthegaterdoesnoto\ufb00erareductionin\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But\nwhenwewanttoselectdi\ufb00erentsubsetsofunitsorparameters,itisnotpossible\ntousea\u201csoftswitch\u201dbecauseitrequiresenumerating(andcomputingoutputsfor)\nallthegatercon\ufb01gurations. Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l .\n()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a\ngradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\nqualityoftheapproximation.\nAnother\u00a0kindof\u00a0dynamicstructure\u00a0isa\u00a0switch,\u00a0where\u00a0ahidden\u00a0unit can\nreceiveinputfromdi\ufb00erentunitsdependingonthecontext.Thisdynamicrouting\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotprovene\ufb00ectiveonlarge-scaleapplications.\nContemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\nandthusdonotachieveallofthepossiblecomputational bene\ufb01tsofdynamic\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\ndegreeofparallelismthatresultsfromthesystemfollowingdi\ufb00erentcodebranches\nfordi\ufb00erentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We\ncanwritemorespecializedsub-routinesthatconvolveeachexamplewithdi\ufb00erent\nkernelsormultiplyeachrowofadesignmatrixbyadi\ufb00erentsetofcolumns\nofweights.Unfortunately,\u00a0thesemorespecializedsubroutinesaredi\ufb03cultto\nimplemente\ufb03ciently.CPUimplementations willbeslowduetothelackofcache\n4 5 0", "CHAPTER12.APPLICATIONS\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\ndi\ufb00erentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\nofexamplessimultaneously.\u00a0Thiscanbeanacceptablestrategyforminimizing\nthetimerequiredtoprocessa\ufb01xedamountofexamplesinano\ufb04inesetting.In\nareal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\nmachinetoprocessthe\ufb01rststepinacascadeandanothermachinetoprocess\nthelaststepinacascade,thenthe\ufb01rstwilltendtobeoverloadedandthelast\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\nimplementdi\ufb00erentnodesofaneuraldecisiontree.\n12.1.6SpecializedHardwareImplementationsofDeepNetworks\nSincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l .\n2003MisraandSaha2010 ; ,).\nDi\ufb00erentformsofspecializedhardware(GrafandJackel1989Meadand ,;\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have\nbeendevelopedoverthelastdecades,eitherwithASICs(application-speci\ufb01cinte-\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\n(combiningdigitalandanalogcomponents).Inrecentyearsmore\ufb02exibleFPGA\n(\ufb01eldprogrammable gatedarray)implementations(wheretheparticularsofthe\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.\nThoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\nandGPUs)typicallyuse32or64bitsofprecisiontorepresent\ufb02oatingpoint\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster\nhardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n4 5 1", "CHAPTER12.APPLICATIONS\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\nGPUs).Thisisverydi\ufb00erentfromthesituationofthe1990s(thepreviousneural\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware\ndesignsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\nnaturallanguage).\nRecentworkonlow-precisionimplementationsofbackprop-based neuralnets\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\nthatbetween8and16bitsofprecisioncansu\ufb03ceforusingortrainingdeep\nneuralnetworkswithback-propagation.\u00a0Whatisclearisthatmoreprecisionis\nrequiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\n\ufb01xedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\nrequiredpernumber.Traditional\ufb01xedpointnumbersarerestrictedtoa\ufb01xed\nrange(whichcorrespondstoagivenexponentina\ufb02oatingpointrepresentation).\nDynamic\ufb01xedpointrepresentationssharethatrangeamongasetofnumbers\n(suchasalltheweightsinonelayer).Using\ufb01xedpointratherthan\ufb02oatingpoint\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,\npowerrequirementsandcomputingtimeneededforperformingmultiplications,\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\ntrainamoderndeepnetworkwithbackprop.\n12. 2 C om p u t er V i s i on\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\ndeeplearningapplications,becausevisionisataskthatise\ufb00ortlessforhumans\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms\nofobjectrecognitionoropticalcharacterrecognition.\nComputervisionisaverybroad\ufb01eldencompassingawidevarietyofways\nofprocessingimages,andanamazingdiversityofapplications.\u00a0Applicationsof\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .\n2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\n4 5 2", "CHAPTER12.APPLICATIONS\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple\nofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\nremovingobjectsfromimages.\n12.2.1Preprocessing\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\ninputcomesinaformthatisdi\ufb03cultformanydeeplearningarchitecturesto\nrepresent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\nsame,reasonablerange,like[0,1]or[-1,1].\u00a0Mixingimagesthatliein[0,1]with\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\ncroppedorscaledto\ufb01tthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.\nSomeconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\nimage( ,). Hadsell e t a l .2007\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error\nofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\nthemodelmanydi\ufb00erentversionsofthesameinput(forexample,thesameimage\ncroppedatslightlydi\ufb00erentlocations)andhavethedi\ufb00erentinstantiationsofthe\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\nensembleapproach,andhelpstoreducegeneralization error.\nOtherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe\namountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n4 5 3", "CHAPTER12.APPLICATIONS\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\nthemodelneededto\ufb01tthetrainingset.Simplertasksmaybesolvedbysmaller\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\niscon\ufb01denthasnorelevancetothetask.Whentrainingwithlargedatasetsand\nlargemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\ne t a l .,).2012\n12.2.1.1ContrastNormalization\nOneofthemostobvioussourcesofvariationthatcanbesafelyremoved\u00a0for\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe\nmagnitudeofthedi\ufb00erencebetweenthebrightandthedarkpixelsinanimage.\nTherearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\nX\u2208 Rr c\u00d7\u00d73,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\nentireimageisgivenby\n\ue076\ue075\ue075\ue0741\n3 r cr \ue058\ni=1c \ue058\nj=13 \ue058\nk=1\ue000\nX i , j , k\u2212\u00af X\ue0012(12.1)\nwhere \u00af Xisthemeanintensityoftheentireimage:\n\u00af X=1\n3 r cr \ue058\ni=1c \ue058\nj=13 \ue058\nk=1X i , j , k . (12.2)\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage,\u00a0then\nrescalingitsothatthe\u00a0standarddeviation\u00a0across its\u00a0pixelsis\u00a0equaltosome\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal\nintensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n4 5 4", "CHAPTER12.APPLICATIONS\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\nmotivatesintroducingasmall,positiveregularizationparameter \u03bbtobiasthe\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\ntobeatleast \ue00f.Givenaninputimage X,GCNproducesanoutputimage X\ue030,\nde\ufb01nedsuchthat\nX\ue030\ni , j , k= sX i , j , k\u2212\u00af X\nmax\ue01a\n\ue00f ,\ue071\n\u03bb+1\n3 r c\ue050r\ni=1\ue050c\nj=1\ue0503\nk=1\ue000\nX i , j , k\u2212\u00af X\ue0012\ue01b .(12.3)\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely\ntocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\ntopracticallyignorethesmalldenominator problembysetting \u03bb= 0andavoid\ndivisionby0inextremelyrarecasesbysetting \ue00ftoanextremelylowvaluelike\n10\u22128.\u00a0Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\n\ue00f \u03bb = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.\nThescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\npreferabletode\ufb01neGCNintermsofstandarddeviationratherthan L2norm\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN\nbasedonstandarddeviationallowsthesame stobeusedregardlessofimage\nsize.However,theobservationthatthe L2normisproportionaltothestandard\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\nexamplestoasphericalshell.See\ufb01gureforanillustration.Thiscanbea 12.1\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdi\ufb00erent\nbiases.Suchcoordinationcanbedi\ufb03cultforthelearningalgorithmtodiscover.\nAdditionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\nreducingeachexampletoadirectionratherthanadirectionandadistance.\nCounterintuitively,thereisapreprocessingoperationknownasspheringand\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\n4 5 5", "CHAPTER12.APPLICATIONS\n\u2212 1 5 0 0 1 5 . . .\nx 0\u2212 1 5 .0 0 .1 5 .x 1Rawinput\n\u2212 1 5 0 0 1 5 . . .\nx 0GCN, = 10 \u03bb\u2212 2\n\u2212 1 5 0 0 1 5 . . .\nx 0GCN, = 0 \u03bb\nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\n( C e n t e r )GCNwith \u03bb= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\ns= 1and \ue00f= 10\u2212 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized\nGCN,with \u03bb >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore. s \ue00f\nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\nsphericalcontours.Spheringismorecommonlyknownas .whitening\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein\ntheshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\nlargedi\ufb00erencebetweenthebrightnessofthedarkareaandthebrightnessofthe\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.\nThismotivateslocalcontrastnormalization.Localcontrastnormalization\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\ntheimageasawhole.See\ufb01gureforacomparisonofglobalandlocalcontrast 12.2\nnormalization.\nVariousde\ufb01nitionsoflocalcontrastnormalization arepossible.Inallcases,\nonemodi\ufb01eseachpixelbysubtractingameanofnearbypixelsanddividingby\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\npixeltobemodi\ufb01ed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\nbemodi\ufb01ed.\u00a0Inthecaseofcolorimages,somestrategiesprocessdi\ufb00erentcolor\n4 5 6", "CHAPTER12.APPLICATIONS\nInputimage GCN LCN\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,thee\ufb00ects\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\ncontrastnormalizationmodi\ufb01estheimagemuchmore,discardingallregionsofconstant\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsof\ufb01netexture,\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe\nnormalizationkernelbeingtoohigh.\nchannelsseparatelywhileotherscombineinformationfromdi\ufb00erentchannelsto\nnormalizeeachpixel( ,). Sermanet e t a l .2012\nLocalcontrastnormalization canusuallybeimplemented e\ufb03cientlybyusing\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\ndivisionondi\ufb00erentfeaturemaps.\nLocalcontrastnormalization isadi\ufb00erentiable operationandcanalsobeusedas\nanonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\noperationappliedtotheinput.\nAswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.\n4 5 7", "CHAPTER12.APPLICATIONS\n12.2.1.2DatasetAugmentation\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassi\ufb01er 7.4\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\nexamplesthathavebeenmodi\ufb01edwithtransformationsthatdonotchangethe\nclass.Objectrecognitionisaclassi\ufb01cationtaskthatisespeciallyamenableto\nthisform\u00a0ofdataset\u00a0augmentationbecause\u00a0theclass\u00a0isinvariant\u00a0toso\u00a0many\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric\noperations.Asdescribedbefore,classi\ufb01erscanbene\ufb01tfromrandomtranslations,\nrotations,andinsomecases,\ufb02ipsoftheinputtoaugmentthedataset.Inspecialized\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12. 3 S p eec h R ec ogn i t i o n\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken\nnaturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually\nasequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\ntaskconsistsofcreatingafunction f\u2217\nASRthatcomputesthemostprobablelinguistic\nsequencegiventheacousticsequence: y X\nf\u2217\nASR() = argmaxX\nyP\u2217( = ) y X|X (12.4)\nwhere P\u2217isthetrueconditionaldistributionrelatingtheinputsXtothetargets\ny.\nSincethe1980sanduntilabout2009\u20132012,state-of-theartspeechrecognition\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand\nphonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987\nTheGMM-HMM\u00a0modelfamilytreats\u00a0acousticwaveformsasbeinggenerated\nbythefollowingprocess:\u00a0\ufb01rstanHMMgeneratesasequenceofphonemesand\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\n4 5 8", "CHAPTER12.APPLICATIONS\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\nspeechrecognitionwasactuallyoneofthe\ufb01rstareaswhereneuralnetworkswere\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the\nperformanceofASRbasedonneuralnetsapproximately matchedtheperformance\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\nphonemestodiscriminatebetween),\u00a0whichwasbetterthanorcomparableto\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\nHowever,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor\nspeechrecognitionandthee\ufb00ortthathadbeeninvestedinbuildingthesesystems\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\nLater,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs\nforthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).\nStartingin2009,speechresearchersappliedaformofdeeplearningbasedon\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.\nThesenetworkstakespectralacousticrepresentationsina\ufb01xed-sizeinputwindow\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosigni\ufb01cantlyimprove\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone\nrecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n4 5 9", "CHAPTER12.APPLICATIONS\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\nontechniquessuchasrecti\ufb01edlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\ne t a l .,).\u00a0Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\narenowdeployedinproductssuchasmobilephones.\nLater,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\nunnecessaryordidnotbringanysigni\ufb01cantimprovement.\nThesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga\nlongperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\ntrainingsets(see\ufb01gure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\nalgorithmsandarchitectures forASR,whichisstillongoingtoday.\nOneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l .\n2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\nfrequencyofspectralcomponents.\nAnotherimportantpush,\u00a0stillongoing,hasbeentowardsend-to-enddeep\nlearningspeechrecognitionsystemsthatcompletelyremovetheHMM.The\ufb01rst\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding.\u00a0This\nworkbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\nappliedinothersettings.\nAnothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\nsystemlearnhowto\u201calign\u201dtheacoustic-levelinformationwiththephonetic-level\n4 6 0", "CHAPTER12.APPLICATIONS\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\nspecializedlanguagesdesignedtoallowe\ufb03cientandunambiguousparsingbysimple\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\ndescription.\u00a0Naturallanguageprocessingincludesapplicationssuchasmachine\ntranslation,inwhichthelearnermustreadasentenceinonehumanlanguageand\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\narebasedonlanguagemodelsthatde\ufb01neaprobabilitydistributionoversequences\nofwords,charactersorbytesinanaturallanguage.\nAswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\nHowever,toachieveexcellentperformanceandtoscalewelltolargeapplications,\nsomedomain-speci\ufb01cstrategiesbecomeimportant.Tobuildane\ufb03cientmodelof\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\nbeendevelopedtomakemodelsofsuchaspacee\ufb03cient,bothinacomputational\nandinastatisticalsense.\n12.4.1-grams n\nAlanguagemodelde\ufb01nesaprobabilitydistributionoversequencesoftokens\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\nearliestsuccessfullanguagemodelswerebasedonmodelsof\ufb01xed-lengthsequences\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\nModelsbasedon n-gramsde\ufb01netheconditionalprobabilityofthe n-thtoken\ngiventhepreceding n\u22121tokens.Themodelusesproductsoftheseconditional\ndistributionstode\ufb01netheprobabilitydistributionoverlongersequences:\nP x(1 , . . . , x \u03c4) = ( P x1 , . . . , x n\u22121)\u03c4\ue059\nt n=P x( t| x t n\u2212+1 , . . . , x t\u22121) .(12.5)\n4 6 1", "CHAPTER12.APPLICATIONS\nThisdecompositionisjusti\ufb01edbythechainruleofprobability.Theprobability\ndistributionovertheinitialsequence P( x1 , . . . , x n\u22121)maybemodeledbyadi\ufb00erent\nmodelwithasmallervalueof. n\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand\nMercer1980Katz1987ChenandGoodman1999 ,;,; ,).\nForsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\nfor n=2,andtrigramfor n=3.\u00a0ThesenamesderivefromtheLatinpre\ufb01xesfor\nthecorrespondingnumbersandtheGreeksu\ufb03x\u201c-gram\u201ddenotingsomethingthat\niswritten.\nUsuallywetrainbothan n-grammodelandan n\u22121 grammodelsimultaneously.\nThismakesiteasytocompute\nP x( t| x t n\u2212+1 , . . . , x t\u22121) =P n( x t n\u2212+1 , . . . , x t)\nP n\u22121( x t n\u2212+1 , . . . , x t\u22121)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomitthe\ufb01nalcharacterfromeachsequencewhenwe\ntrain P n\u22121.\nAsanexample,wedemonstratehowatrigrammodelcomputestheprobability\nofthesentence\u201cTHEDOGRANAWAY.\u201dThe\ufb01rstwordsofthesentencecannotbe\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-\nabilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N).\nFinally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\nweobtain:\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N .\n(12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even\nthoughthetuple ( x t n\u2212+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\ndi\ufb00erentkindsofcatastrophicoutcomes.When P n\u22121iszero,theratioisunde\ufb01ned,\nsothemodeldoesnotevenproduceasensibleoutput.When P n\u22121isnon-zerobut\nP niszero,thetestlog-likelihoodis\u2212\u221e.\u00a0Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2", "CHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\nSee ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\nsymbolvalues.Thismethodcanbejusti\ufb01edasBayesianinferencewithauniform\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe\nhigher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\nmorelikelytoavoidcountsofzero.Back-o\ufb00methodslook-upthelower-order\nn-gramsifthefrequencyofthecontext x t\u22121 , . . . , x t n\u2212+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k \u2212+ , . . . , x t\u22121,forincreasing k,untilasu\ufb03cientlyreliableestimateis\nfound.\nClassical n-grammodelsareparticularlyvulnerabletothecurseofdimension-\nality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset.\nOnewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\nisevenmoreseverethanusual,becauseanytwodi\ufb00erentwordshavethesamedis-\ntancefromeachotherinone-hotvectorspace.Itisthusdi\ufb03culttoleveragemuch\ninformationfromany\u201cneighbors\u201d\u2014onlytrainingexamplesthatrepeatliterallythe\nsamecontextareusefulforlocalgeneralization.\u00a0T oovercometheseproblems,a\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\nsemanticallysimilarwords.\nToimprovethestatisticale\ufb03ciencyof n-grammodels,class-basedlanguage\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce\nthenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-o\ufb00are\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences\ninwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\nlostinthisrepresentation.\n4 6 3", "CHAPTER12.APPLICATIONS\n12.4.2NeuralLanguageModels\nNeurallanguagemodelsorNLMsare\u00a0aclassoflanguagemodeldesigned\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\nsequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone\nword(anditscontext)andothersimilarwordsandcontexts.Thedistributed\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,\ntransferringinformationfromeachtrainingsentencetoanexponentiallylarge\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\nexponentialnumberofsimilarsentences.\nWesometimescallthesewordrepresentationswordembeddings.Inthis\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal\ntothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\naone-hotvector,soeverypairofwordsisatEuclideandistance\u221a\n2fromeach\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n(oranypairofwordssharingsome\u201cfeatures\u201dlearnedbythemodel)arecloseto\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\nFigurezoomsinonspeci\ufb01careasofalearnedwordembeddingspacetoshow 12.3\nhowsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\nNeuralnetworksinotherdomainsalsode\ufb01neembeddings.Forexample,a\nhiddenlayerofaconvolutionalnetworkprovidesan\u201cimageembedding.\u201dUsually\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\nrepresented.\nThebasicideaofusingdistributedrepresentationstoimprovemodelsfor\nnaturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n4 6 4", "CHAPTER12.APPLICATIONS\nmultiplelatentvariables(MnihandHinton2007,).\n\u2212 \u2212 \u2212 \u2212 \u2212 3432302826\u221214\u221213\u221212\u221211\u221210\u22129\u22128\u22127\u22126\nCanadaEuropeOntario\nNorthEnglish\nCanadianUnionAfricanAfrica\nBritishFrance\nRussianChina\nGermanyFrench\nAssemblyEU JapanIraq\nSouthEuropean\n350355360365370375380 . . . . . . .171819202122\n1995199619971998199920002001\n200220032004\n20052006200720082009\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural\nmachinetranslationmodel( ,),zoominginonspeci\ufb01careaswhere Bahdanau e t a l .2015\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\n12.4.3High-DimensionalOutputs\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce\nwords(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\nrepresentingsuchadistributionistoapplyana\ufb03netransformationfromahidden\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear\ncomponentofthisa\ufb03netransformationisverylarge,becauseitsoutputdimension\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\ntimeaswellastesttime\u2014wecannotcalculateonlythedotproductwiththeweight\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and\nattesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\n4 6 5", "CHAPTER12.APPLICATIONS\nlossfunctions,thegradientcanbecomputede\ufb03ciently( ,),but Vincent e t a l .2015\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\nmanydi\ufb03culties.\nSupposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\n\u02c6y.Ifweparametrizethetransformationfromhto\u02c6ywithlearnedweightsW\nandlearnedbiasesb,thenthea\ufb03ne-softmaxoutputlayerperformsthefollowing\ncomputations:\na i= b i+\ue058\njW i j h j\u2200\u2208{ ||} i1 , . . . , V , (12.8)\n\u02c6 y i=ea i\n\ue050|| V\ni\ue030=1 eai \ue030. (12.9)\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\ncomputationofmostneurallanguagemodels.\n12.4.3.1UseofaShortList\nThe\ufb01rstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()\nbuiltuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\nfrequentwords(handledbytheneuralnet)andatail T= V L\\ofmorerarewords\n(handledbyan n-grammodel).\u00a0Tobeabletocombinethetwopredictions,the\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\nunittoprovideanestimateof P( i C \u2208| T ).Theextraoutputcanthenbeusedto\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V\nP y i C (= |) =1 i\u2208 L P y i C, i P i C (= | \u2208 \u2212 L)(1 (\u2208| T ))\n+1 i\u2208 T P y i C, i P i C (= | \u2208 T)(\u2208| T )(12.10)\nwhere P( y= i C, i| \u2208 L)isprovidedbytheneurallanguagemodeland P( y= i|\nC, i\u2208 T) isprovidedbythe n-grammodel.Withslightmodi\ufb01cation,thisapproach\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodel\u2019ssoftmax\nlayer,ratherthanaseparatesigmoidunit.\nAnobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent\n4 6 6", "CHAPTER12.APPLICATIONS\nwords,where,arguably,itistheleastuseful.\u00a0Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow.\n12.4.3.2HierarchicalSoftmax\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),\nthe|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\nmodels.\nOnecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.\nThesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\nthetreehasdepth O(log|| V).\u00a0Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat\neverynodeonapathfromtherootofthetreetotheleafcontainingtheword.\nFigureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\nsummationoverallofthepathsthatleadtothatword.\nTopredicttheconditionalprobabilities requiredateachnodeofthetree,we\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe\nsamecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\nBecausetheoutputlog-likelihoodcanbecomputede\ufb03ciently(aslowaslog|| V\nratherthan|| V),itsgradientsmayalsobecomputede\ufb03ciently.Thisincludesnot\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients\nwithrespecttothehiddenlayeractivations.\nItispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\n4 6 7", "CHAPTER12.APPLICATIONS\n( 1) ( 0)\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeci\ufb01cwords.\nInternalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence\nofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissu\ufb03cientlybalanced,\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\nthenumberofwords|| V:\u00a0thechoiceofoneoutof|| Vwordscanbeobtainedbydoing\nO(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach\nnodeindexedbythepre\ufb01xofthesebits.Forexample,node(1 ,0)correspondstothe\npre\ufb01x( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\n4 6 8", "CHAPTER12.APPLICATIONS\npractice,thecomputational savingsaretypicallynotworththee\ufb00ortbecausethe\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\nactivationsgrowsasas O( l n2\nh)whiletheoutputcomputations growas O( n h n b).\nAslongas n b\u2264 l n h,wecanreducecomputationmorebyshrinking n hthanby\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\nexceedsamillionwordsandlog2(106)\u224820,itispossibletoreduce n btoabout,20\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\natreewithabranchingfactorof,onecaninsteadde\ufb01neatreewithdepthtwo 2\nandabranchingfactorof\ue070\n|| V.Suchatreecorrespondstosimplyde\ufb01ningaset\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\ntwocapturesmostofthecomputational bene\ufb01tofthehierarchicalstrategy.\nOnequestionthatremainssomewhatopenishowtobestde\ufb01netheseword\nclasses,orhowtode\ufb01nethewordhierarchyingeneral.Earlyworkusedexisting\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\njointlywiththeneurallanguagemodel.Learningthehierarchyisdi\ufb03cult.Anexact\noptimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization. However,\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\nwordsintowordclasses.\nAnimportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\ntionalbene\ufb01tsbothattrainingtimeandattesttime,ifattesttimewewantto\ncomputetheprobabilityofspeci\ufb01cwords.\nOfcourse,computingtheprobabilityofall|| Vwordswillremainexpensive\nevenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\nprovideane\ufb03cientandexactsolutiontothisproblem.\nAdisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses.\n12.4.3.3ImportanceSampling\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly\ncomputingthecontributionofthegradientfromallofthewordsthatdonotappear\n4 6 9", "CHAPTER12.APPLICATIONS\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\ninequation,thegradientcanbewrittenasfollows: 12.8\n\u2202 P y C log(|)\n\u2202 \u03b8=\u2202logsoftmax y()a\n\u2202 \u03b8(12.13)\n=\u2202\n\u2202 \u03b8logea y\n\ue050\ni ea i(12.14)\n=\u2202\n\u2202 \u03b8( a y\u2212log\ue058\niea i) (12.15)\n=\u2202 a y\n\u2202 \u03b8\u2212\ue058\niP y i C (= |)\u2202 a i\n\u2202 \u03b8(12.16)\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\nperword.The\ufb01rsttermisthepositivephaseterm(pushing a yup)whilethe\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\nSamplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\nwhichispreciselywhatwearetryingtoavoid.\nInsteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\nS\u00e9n\u00e9cal2003BengioandS\u00e9n\u00e9cal2008 ,; ,).Thisisanapplicationofamoregeneral\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\ninsection.Unfortunately,evenexactimportancesamplingisnote\ufb03cient 17.2\nbecauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\nassociatedgradientisweightedby\nw i=p n i /q n i\ue050N\nj=1 p n j /q n j. (12.17)\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\n4 7 0", "CHAPTER12.APPLICATIONS\ngradient:\n|| V\ue058\ni=1P i C(|)\u2202 a i\n\u2202 \u03b8\u22481\nmm \ue058\ni=1w i\u2202 a n i\n\u2202 \u03b8. (12.18)\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\ntheparameters,itisalsopossibletosamplefromsuchadistributionverye\ufb03ciently.\nImportancesamplingisnotonlyusefulforspeedingupmodelswithlarge\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge\nsparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.\nMachinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight\nmostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\ncomputational bene\ufb01ttousingsparseoutputs,becausethemodelmaychooseto\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\nDauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\nimportancesampling.Thee\ufb03cientalgorithmminimizesthelossreconstructionfor\nthe\u201cpositivewords\u201d(thosethatarenon-zerointhetarget)andanequalnumber\nof\u201cnegativewords.\u201dThenegativewordsarechosenrandomly,usingaheuristicto\nsamplewordsthataremorelikelytobemistaken.\u00a0Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.\nInallofthesecases,thecomputational complexityofgradientestimationfor\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\nratherthanproportionaltothesizeoftheoutputvector.\n12.4.3.4Noise-ContrastiveEstimationandRankingLoss\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\n4 7 1", "CHAPTER12.APPLICATIONS\nscores a i.Therankinglossproposedthenis\nL=\ue058\nimax(01 ,\u2212 a y+ a i) . (12.19)\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n(includingconditionaltextgenerationtaskssuchastranslation).\nAmorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\nandKavukcuoglu2013,).\n12.4.4CombiningNeuralLanguageModelswith-grams n\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup\nonlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\nofcapacity.Incomparison,doublinganeuralnetwork\u2019snumberofparameters\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional\nnetworks,canaddparameterswhilereducingthedegreeofparametersharing\ninordertomaintainthesameamountofcomputation. However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters.\nOneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003\ntheensemblemembersmakeindependentmistakes.The\ufb01eldofensemblelearning\nprovidesmanywaysofcombiningtheensemblemembers\u2019predictions,including\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\nItisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\n4 7 2", "CHAPTER12.APPLICATIONS\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\nishuge\u2014thenewportionofthearchitecturecontainsupto|| s Vnparameters\u2014but\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\ntheextrainputsareverysparse.\n12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage.\u00a0Mac hine\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\ntranslationswillnotbegrammaticalduetodi\ufb00erencesbetweenthelanguages.For\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish\ndirectlytheyyieldphrasessuchas\u201capplered.\u201dTheproposalmechanismsuggests\nmanyvariantsofthesuggestedtranslation,ideallyincluding\u201credapple.\u201dAsecond\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\ntranslations,andcanscore\u201credapple\u201dasbetterthan\u201capplered.\u201d\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad\nusedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\nmachinetranslationincludenotjusttraditionalback-o\ufb00 n-grammodels(Jelinek\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\nentropylanguagemodels(,),inwhichana\ufb03ne-softmaxlayer Berger e t a l .1996\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven\naninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\nthatde\ufb01nesamarginaldistributionoversomevariabletode\ufb01neaconditional\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k\ninthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels. n\n4 7 3", "CHAPTER12.APPLICATIONS\nD e c ode rO ut put\u00a0ob j e c t \u00a0 ( E ngl i s h\u00a0\ns e nt e nc e )\nI nt e r m e di at e , \u00a0 s e m a n t i c \u00a0 r e pr e s e nt a t i o n\nSourc e \u00a0 ob j e c t \u00a0 ( F r e nc h\u00a0 s e n t e nc e \u00a0 or \u00a0 i m a g e )E nc ode r\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\nByusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping\nfromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\ntomachinetranslationbutalsotocaptiongenerationfromimages.\nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\npreprocessedtobeof\ufb01xedlength.Tomakethetranslationmore\ufb02exible,wewould\nliketouseamodelthatcanaccommodatevariablelengthinputsandvariable\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\nwhentheinputisasequence.Inallcases,onemodel\ufb01rstreadstheinputsequence\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\nsummarythe\u201ccontext\u201d C.Thecontext Cmaybealistofvectors,oritmaybea\nvectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\nnetwork(KalchbrennerandBlunsom2013,).\u00a0Asecondmodel,usuallyanRNN,\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\nin\ufb01gure.12.5\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\nmodelmusthaveawaytorepresenttheentiresourcesentence.\u00a0Earliermodels\nwereonlyabletorepresentindividualwordsorphrases.\u00a0Fromarepresentation\n4 7 4", "CHAPTER12.APPLICATIONS\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\nexplored\ufb01rstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever\ne t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\n\u03b1( t \u2212 1 )\u03b1( t \u2212 1 )\u03b1( ) t\u03b1( ) t\u03b1( + 1 ) t\u03b1( + 1 ) t\nh( t \u2212 1 )h( t \u2212 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7+\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\noffeaturevectorsh( ) twithweights \u03b1( ) t.Insomeapplications,thefeaturevectorshare\nhiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\nweights \u03b1( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\napproximatesreadingthatonespeci\ufb01ctimestepprecisely.Theweights \u03b1( ) tareusually\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly\nindexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\nattentionmechanismbasedonweightedaveragesisasmooth,di\ufb00erentiableapproximation\nthatcanbetrainedwithexistingoptimizationalgorithms.\nUsinga\ufb01xed-sizerepresentationtocaptureallthesemanticdetailsofavery\nlongsentenceofsay60wordsisverydi\ufb03cult.\u00a0Itcanbeachievedbytraininga\nsu\ufb03cientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\ne t a l .()and2014aSutskever2014 e t a l .().However,amoree\ufb03cientapproachis\ntoreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n4 7 5", "CHAPTER12.APPLICATIONS\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\nfocusingonadi\ufb00erentpartoftheinputsentenceinordertogatherthesemantic\ndetailsthatarerequiredtoproducethenextoutputword.\u00a0Thatisexactlythe\nideathat ()\ufb01rstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\ntofocusonspeci\ufb01cpartsoftheinputsequenceateachtimestepisillustratedin\n\ufb01gure.12.6\nWecanthinkofanattention-basedsystemashavingthreecomponents:\n1.Aprocessthat\u201c r e a d s\u201drawdata(suchassourcewordsinasourcesentence),\nandconvertsthemintodistributedrepresentations,withonefeaturevector\nassociatedwitheachwordposition.\n2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\nunderstoodasa\u201c\u201d\u00a0containingasequenceoffacts,whichcanbe m e m o r y\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\nofthem.\n3.Aprocessthat\u201c\u201dthecontentofthememorytosequentiallyperform e x p l o i t s\natask,ateachtimestephavingtheabilityputattentiononthecontentof\nonememoryelement(orafew,withadi\ufb00erentweight).\nThethirdcomponentgeneratesthetranslatedsentence.\nWhenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe\nwordembeddingsinanother(Ko\u010disk\u00fd2014 e t a l .,),yieldingloweralignmenterror\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\nThereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\n2012).Manyextensionstothisapproacharepossible.Forexample,moree\ufb03cient\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\n12.4.6HistoricalPerspective\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart\ne t a l .()inoneofthe\ufb01rstexplorationsofback-propagation, withsymbols 1986a\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\nsuchas(Colin,Mother,Victoria).\u00a0The \ufb01rstlayeroftheneuralnetworklearned\narepresentationofeachfamilymember.Forexample,\u00a0thefeaturesforColin\n4 7 6", "CHAPTER12.APPLICATIONS\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\nthemotherofColin.\nTheideaofforminganembeddingforasymbolwasextendedtotheideaofan\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned\nusingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\nThehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\npopularityofdi\ufb00erentwaysofrepresentingtheinputtothemodel.Following\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\ntheinputasasequenceofcharacters.\nBengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These\nneuralmodelshavescaledupfromde\ufb01ningrepresentationsofasmallsetofsymbols\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\nmodernapplications.Thiscomputational scalinge\ufb00ortledtotheinventionofthe\ntechniquesdescribedaboveinsection.12.4.3\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\nimprovedlanguage\u00a0modeling performance( ,).Tothisday, Bengio e t a l .2001\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,\n2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\nmodelingindividualbytesofUnicodecharacters.\nTheideasbehindneurallanguagemodelshavebeenextendedintoseveral\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\ntasks.\nTwo-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-pro\ufb01leappli-\ncationtovisualizationwordembeddingsbyJosephTurianin2009.\n4 7 7", "CHAPTER12.APPLICATIONS\n12. 5 O t h er A p p l i c a t i o n s\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\naredi\ufb00erentfromthestandardobjectrecognition,speechrecognitionandnatural\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\nscopeevenfurthertotasksthatremainprimarilyresearchareas.\n12.5.1RecommenderSystems\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential\nusersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\nmaderegardingthatproducttothatuser.Theinternetiscurrently\ufb01nancedin\ngreatpartbyvariousformsofonlineadvertising.\u00a0Therearemajorpartsofthe\neconomythatrelyononlineshopping.\u00a0CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .\nSometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\nforvideogames,ormatchingpeopleindatingservices.\nOften,thisassociationproblemishandledlikeasupervisedlearningproblem:\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\ninterest(userclicksonad,userentersarating,userclicksona\u201clike\u201dbutton,user\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic\nclassi\ufb01cationproblem(predictingtheconditionalprobabilityofsomediscrete\nevent).\nTheearlyworkonrecommendersystemsreliedonminimalinformationas\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\nthetargetvariablefordi\ufb00erentusersorfordi\ufb00erentitems.Supposethatuser1\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n4 7 8", "CHAPTER12.APPLICATIONS\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\nthenameofcollaborative\ufb01ltering.Bothnon-parametric approaches(suchas\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser\nandforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\ndependonlyoneithertheuserIDortheitemID).Let\u02c6Rbethematrixcontaining\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively\nakindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\npredictionisthusobtainedasfollows:\n\u02c6 R u , i= b u+ c i+\ue058\njA u , j B j , i . (12.20)\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\n\u02c6 R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\nconvenientlyvisualizedwhentheyare\ufb01rstreducedtoalowdimension(twoor\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just\nlikewordembeddings.\u00a0One waytoobtaintheseembeddingsisbyperforminga\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings).\nThiscorrespondstofactorizingR=UDV\ue030(oranormalizedvariant)intothe\nproductoftwofactors,thelowerrankmatricesA=UDandB=V\ue030.One\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum\nofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\nbasedoptimization. TheSVDandthebilinearpredictionofequation both12.20\nperformedverywellinthecompetitionfortheNet\ufb02ixprize( , BennettandLanning\n2007),aimingatpredictingratingsfor\ufb01lms,basedonlyonpreviousratingsby\nalargesetofanonymoususers.\u00a0Manymachinelearningexpertsparticipatedin\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded\nimprovementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\n4 7 9", "CHAPTER12.APPLICATIONS\npresentedbymostofthecompetitors,includingthewinners( ,; T\u00f6scher e t a l .2009\nKoren2009,).\nBeyondthesebilinearmodelswithdistributedrepresentations,oneofthe\ufb01rst\nusesofneuralnetworksforcollaborative\ufb01lteringisbasedontheRBMundirected\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\noftheensembleofmethodsthatwontheNet\ufb02ixcompetition(T\u00f6scher2009 e t a l .,;\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix\nhavealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\nMnih2008,).\nHowever,thereisabasiclimitationofcollaborative\ufb01lteringsystems:whena\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\nthecold-startrecommendation problemistointroduceextrainformationabout\ntheindividualusersanditems.Forexample,thisextrainformationcouldbeuser\npro\ufb01leinformationorfeaturesofeachitem.\u00a0Systems thatusesuchinformation\narecalledcontent-basedrecommendersystems.Themappingfromarich\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,).\nSpecializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical\naudiotracks,formusicrecommendation (vandenO\u00f6rd2013 e t a l .,).Inthatwork,\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.\n12.5.1.1ExplorationVersusExploitation\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-\nmendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\ntheywererecommendedandnottotheotheritems.\u00a0Inaddition,insomecases\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\n4 8 0", "CHAPTER12.APPLICATIONS\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\nwouldbeliketrainingaclassi\ufb01erbypickingoneclass\u02c6 yforeachtrainingexample\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue\nlabel yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement\nlearningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof\nsomeinputvariablethatcaninformthedecision.Forexample,weatleastknow\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\nissueinthereinforcementlearningandbanditsliterature.\nReinforcementlearningrequireschoosingatradeo\ufb00betweenexplorationand\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,\nbestversionofthelearnedpolicy\u2014actionsthatweknowwillachieveahighreward.\nExplorationreferstotakingactionsspeci\ufb01callyinordertoobtainmoretraining\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\nrewardof1.However,wemayalsowanttoexplorebytryingaction a\ue030.Wedonot\nknowwhatwillhappenifwetryaction a\ue030.Wehopetogetarewardof,butwe 2\nruntheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\nrewardandthemodel\u2019samountofuncertaintyaboutthatreward.\n4 8 1", "CHAPTER12.APPLICATIONS\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\nOneofthemostprominentfactorsisthetimescaleweareinterestedin.\u00a0Ifthe\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\nmoreexplorationsothatfutureactionscanbeplannedmoree\ufb00ectivelywithmore\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\nmoreexploitation.\nSupervised\u00a0learninghas\u00a0notradeo\ufb00\u00a0between\u00a0explorationand\u00a0exploitation\nbecausethesupervisionsignalalwaysspeci\ufb01eswhichoutputiscorrectforeach\ninput.Thereisnoneedtotryoutdi\ufb00erentoutputstodetermineifoneisbetter\nthanthemodel\u2019scurrentoutput\u2014wealwaysknowthatthelabelisthebestoutput.\nAnotherdi\ufb03cultyarisinginthecontextofreinforcementlearning,besidesthe\nexploration-exploitationtrade-o\ufb00,isthedi\ufb03cultyofevaluatingandcomparing\ndi\ufb00erentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\nevaluatethelearner\u2019sperformanceusinga\ufb01xedsetoftestsetinputvalues.The\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits.\n12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\nswering\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine\ntranslationandnaturallanguageprocessingduetotheuseofembeddingsfor\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\nrepresentations.\n12.5.2.1Knowledge,RelationsandQuestionAnswering\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\nInmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\n4 8 2", "CHAPTER12.APPLICATIONS\ndonot.Forexample,wecande\ufb01netherelation\u201cislessthan\u201donthesetofentities\n{1 ,2 ,3}byde\ufb01ningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\nrelationisde\ufb01ned,wecanuseitlikeaverb.Because(1 ,2)\u2208 S,wesaythat1is\nlessthan2.Because(2 ,1)\ue036\u2208 S,wecannotsaythat2islessthan1.Ofcourse,the\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecouldde\ufb01nea\nrelation containingtupleslike(,). is_a_type_of dogmammal\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically\nsimpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\nsentencestaketheformofatripletoftokens\n(subjectverbobject) , , (12.21)\nwithvalues\n(entityi ,relation j ,entityk) . (12.22)\nWecanalsode\ufb01neanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) . (12.23)\nForexample,wecouldde\ufb01nethehas_furattribute,andapplyittoentitieslike\ndog.\nManyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\nHowshouldwebestdothiswithinthecontextofneuralnetworks?\nMachinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\nTherearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\nkindofinformation,\u00a0alb eit\u00a0notformattedasthreetokensentences.Whena\ndatabaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\nexpertknowledgeaboutanapplicationareatoanarti\ufb01cialintelligencesystem,\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes\ne t a l .,).2013a\n1R e s p e c t i v e l y a v a i l a b l e \u00a0 f ro m t h e s e \u00a0 w e b \u00a0 s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t .\np r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3", "CHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtode\ufb01neamodelfamilytotrain.\nAcommonapproachistoextendneurallanguagemodelstomodelentitiesand\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling\nlanguageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.\nEarlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton\n2000)positedhighlyconstrainedparametricforms(\u201clinearrelationalembeddings\u201d),\noftenusingadi\ufb00erentformofrepresentationfortherelationthanfortheentities.\nForexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\ne t a l .,),allowingustomakestatementsaboutrelations,butmore\ufb02exibilityis 2012\nputinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\nApracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .\n(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015\napplication.\nEvaluatingtheperformanceofamodelonalinkpredictiontaskisdi\ufb03cult\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\nbetrue).\u00a0Ifthemodelproposesafactthatisnotinthedataset,weareunsure\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples\nthatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\nintherelationwithadi\ufb00erententityselectedatrandom.Thepopularprecisionat\n10%metriccountshowmanytimesthemodelranksa\u201ccorrect\u201dfactamongthe\ntop10%ofallcorruptedversionsofthatfact.\n4 8 4", "CHAPTER12.APPLICATIONS\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\none,insomecontext.\nEventually,knowledgeofrelationscombinedwithareasoningprocessand\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess\ninputinformationandrememberimportantfacts,organizedinawaythatenables\nittoretrieveandreasonaboutthemlater.Thisremainsadi\ufb03cultopenproblem\nwhichcanonlybesolvedinrestricted\u201ctoy\u201denvironments.Currently,thebest\napproachtorememberingandretrievingspeci\ufb01cdeclarativefactsistousean\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\n\ufb01rstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).\ne t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015\ntheinputintothememoryandtoproducetheanswergiventhecontentsofthe\nmemory.\nDeeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\nasofthiswriting.\nThisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II\nnetworks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\nmethodsinvolveusingthegradientofacostfunctionto\ufb01ndtheparametersofa\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\nterritoryofresearch\u2014methodsthataredesignedtoworkwithlesstrainingdata\nortoperformagreatervarietyoftasks,wherethechallengesaremoredi\ufb03cult\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\n4 8 5", "P a rt I I I\nDeepLearningResearch\n486", "This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes\nt o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y .\nIn t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d\nlearning problems \u2014 how t o learn t o map one v e c t or t o another, given e nough\ne x amples of t he mapping.\nN ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y\nwis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle\nmis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples\nf r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial\napplications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d\ndata t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of\nt he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary\nf or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of\nt as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or\ns e mi-s up e r v is e d learning.\nMan y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d\nlearning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat\ndeep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of\nt as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d\nlearning and s ome of t he p opular t hought ab out how w e c an make progres s in t his\n\ufb01 e ld.\nA c e ntral c aus e of t he di\ufb03culties with uns upervis e d learning is t he high di-\nmens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct\nc hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l\nc h a l l e ng e r e gards generalization: t he num b e r of c on\ufb01gurations we may wan t t o\ndis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and\nt his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly\nha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e\nas s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or\nlearning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit\nprobabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially\nwith t he n um b e r of dimens ion s .\nWith probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o\np e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on.\n\u2022 I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19\nt he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other\nv ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r\n4 8 7", "a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs\nt o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization\nc ons t an t whic h s ums o v e r t he v alues of a and c .\n\u2022 I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition\nf unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18\nbilit y f unctions c ome up in inference ( ab o v e ) as well as in learning.\u00a0Man y\nprobabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately ,\nlearning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-\nr ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That\nc omputation is generally as intractable as c omputing t he partition f unction\nits e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17\nt e n us e d t o deal with t he partition f unction ( c omputing it or its gradient).\nU nfortun ately , MCMC metho ds s u\ufb00er when t he mo des of t he mo del dis t r ibu-\nt ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces\n( s e c t ion ) . 17.5\nOne wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,\nand many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he\nb o ok. A nother interes t in g w ay ,\u00a0als o dis c us s e d here,\u00a0w ould b e t o av oid t hes e\nin t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire\ns uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n\nprop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary\napproac hes t o generativ e mo deling are dis c us s e d in c hapter . 20\nP art is t he mos t imp ortant f or a r e s e arc her\u2014s om e one who wan t s t o un- I I I\nders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he \ufb01 e ld of deep\nlearning, and pus h t he \ufb01 e ld f orward t ow ards t r ue arti\ufb01cial intelligence.\n4 8 8", "C h a p t e r 1 3\nL i n e ar F act or Mo d e l s\nManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel\noftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto\npredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many\nofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|.\nTheselatentvariablesprovideanothermeansofrepresentingthedata.Distributed\nrepresentationsbased\u00a0onlatent\u00a0variablescanobtain\u00a0alloftheadvantagesof\nrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrent\nnetworks.\nInthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\nlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\nblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;\nRoweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They\nalsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat\nthemoreadvanceddeepmodelswillextendfurther.\nAlinearfactormodelisde\ufb01nedbytheuseofastochastic,lineardecoder\nfunctionthatgeneratesbyaddingnoisetoalineartransformationof. x h\nThesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\nfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\nmadethesemodelssomeofthe\ufb01rstlatentvariablemodelstobeextensivelystudied.\nAlinearfactormodeldescribesthedatagenerationprocessasfollows.First,\nwesampletheexplanatoryfactorsfromadistribution h\nh\u223c p ,() h (13.1)\nwhere p( h)isafactorialdistribution,with p( h) =\ue051\ni p( h i),sothatitiseasyto\n489", "CHAPTER13.LINEARFACTORMODELS\nsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:\nx W h b = ++noise (13.2)\nwherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\nThisisillustratedin\ufb01gure.13.1\nh 1 h 1 h 2 h 2 h 3 h 3\nx 1 x 1 x 2 x 2 x 3 x 3\nx h n  o i s  e x h n  o i s  e = W + + b = W + + b\nFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\nwhichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof\nindependentlatentfactors h,plussomenoise.Di\ufb00erentmodels,suchasprobabilistic\nPCA,factoranalysisorICA,makedi\ufb00erentchoicesabouttheformofthenoiseandof\ntheprior. p() h\n13.1ProbabilisticPCAandFactorAnalysis\nProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\nfactormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\ndi\ufb00erinthechoicesmadeforthenoisedistributionandthemodel\u2019spriorover\nlatentvariablesbeforeobserving. h x\nIn f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994\npriorisjusttheunitvarianceGaussian\nh 0 \u223cN(; h , I) (13.3)\nwhiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,\ngiven h.Speci\ufb01cally,\u00a0the\u00a0noiseisassumed\u00a0tobedrawnfroma\u00a0diagonalco-\nvariance\u00a0Gaussian distribution,with\u00a0covariancematrix \u03c8=diag( \u03c32),with\n\u03c32= [ \u03c32\n1 , \u03c32\n2 , . . . , \u03c32\nn]\ue03eavectorofper-variablevariances.\nTheroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween\nthedi\ufb00erentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta\nmultivariatenormalrandomvariable,with\nx\u223cN(; x b W W ,\ue03e+) \u03c8 . (13.4)\n490", "CHAPTER13.LINEARFACTORMODELS\nInordertocastPCAinaprobabilisticframework,\u00a0wecanmakeaslight\nmodi\ufb01cationtothefactoranalysismodel,makingtheconditionalvariances \u03c32\ni\nequaltoeachother.Inthatcasethecovarianceof xisjust W W\ue03e+ \u03c32I,where\n\u03c32isnowascalar.Thisyieldstheconditionaldistribution\nx\u223cN(; x b W W ,\ue03e+ \u03c32I) (13.5)\norequivalently\nx h z = W ++ b \u03c3 (13.6)\nwhere z\u223cN( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999\niterativeEMalgorithmforestimatingtheparameters and W \u03c32.\nThis pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost\nvariationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall\nresidual r e c o nst r u c t i o n e r r o r \u03c32.Asshownby (), TippingandBishop1999\nprobabilisticPCAbecomesPCAas \u03c3\u21920.Inthatcase,theconditionalexpected\nvalueof hgiven xbecomesanorthogonalprojectionof x b\u2212ontothespace\nspannedbythecolumnsof,likeinPCA. d W\nAs \u03c3\u21920,thedensitymodelde\ufb01nedbyprobabilisticPCAbecomesverysharp\naroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe\nmodelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster\nnearahyperplane.\n13.2IndependentComponentAnalysis(ICA)\nIndependentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\nalgorithms( ,; ,;\u00a0,; Herault\u00a0andAns1984Jutten\u00a0andHerault1991Comon1994\nHyv\u00e4rinen1999Hyv\u00e4rinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,).\nItisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved\nsignalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\ntheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\nmerelydecorrelatedfromeachother.1\nManydi\ufb00erentspeci\ufb01cmethodologiesarereferredtoasICA.Thevariant\nthatismostsimilartotheothergenerativemodelswehavedescribedhereisa\nvariant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992\npriordistributionovertheunderlyingfactors, p( h),mustbe\ufb01xedaheadoftimeby\ntheuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma\n1Seesectionforadiscussionofthedi\ufb00erencebetweenuncorrelatedvariablesandindepen- 3.8\ndentvariables.\n491", "CHAPTER13.LINEARFACTORMODELS\nnonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning\nthemodelthenproceedsasusual,usingmaximumlikelihood.\nThemotivationforthisapproachisthatbychoosing p( h)tobeindependent,\nwecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.\nThisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto\nrecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each\ntrainingexampleisonemomentintime,each x iisonesensor\u2019sobservationof\nthemixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For\nexample,wemighthave npeoplespeakingsimultaneously.Ifwehave ndi\ufb00erent\nmicrophonesplacedindi\ufb00erentlocations,ICAcandetectthechangesinthevolume\nbetweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso\nthateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused\ninneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical\nsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject\u2019s\nheadareusedtomeasuremanyelectricalsignalscomingfromthebody.The\nexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom\nthesubject\u2019sheartandeyesarestrongenoughtoconfoundmeasurementstaken\natthesubject\u2019sscalp.Thesignalsarriveattheelectrodesmixedtogether,so\nICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals\noriginatinginthebrain,andtoseparatesignalsindi\ufb00erentbrainregionsfrom\neachother.\nAsmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise\ninthegenerationof xratherthanusingadeterministicdecoder.Mostdonot\nusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\nh= W\u2212 1xindependentfromeachother.Manycriteriathataccomplishthisgoal\narepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe\nanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\nproblematicoperationbyconstrainingtobeorthogonal. W\nAllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)\nisanindependentpriorwithGaussiancomponents,then Wisnotidenti\ufb01able.\nWecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery\ndi\ufb00erentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\nthatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe\nmodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe\nuserexplicitlyspeci\ufb01esthedistribution,atypicalchoiceistouse p( h i) =d\nd h i\u03c3( h i).\nTypicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\ndoestheGaussiandistribution,sowecanalsoseemostimplementations ofICA\naslearningsparsefeatures.\n492", "CHAPTER13.LINEARFACTORMODELS\nManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\nphrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples\nfromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but\ndonothaveanywayofrepresenting p( h),andthusdonotimposeadistribution\nover p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\nh= W\u2212 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis\naccomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore\noftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\ndataorestimatingitsdensity.\nJustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\nchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\nweuseanonlinearfunction ftogeneratetheobserveddata.SeeHyv\u00e4rinenand\nPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith\nensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000\nAnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent\nc o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014\nofinvertibletransformations(encoderstages)thathavethepropertythatthe\ndeterminantoftheJacobianofeachtransformationcanbecomputede\ufb03ciently.\nThismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts\ntotransformthedataintoaspacewhereithasafactorizedmarginaldistribution,\nbutismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\nisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto\ngeneratesamplesfromthemodel(by\ufb01rstsamplingfrom p( h)andthenapplying\nthedecoder).\nAnothergeneralization ofICAistolearngroupsoffeatures,withstatistical\ndependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyv\u00e4rinenand\nHoyer1999Hyv\u00e4rinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen\ntobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso\npossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\ngroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\nfeatures.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns\nGabor\ufb01lters,suchthatneighboringfeatureshavesimilarorientation,locationor\nfrequency.Manydi\ufb00erentphaseo\ufb00setsofsimilarGaborfunctionsoccurwithin\neachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\n13.3SlowFeatureAnalysis\nSl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom\n493", "CHAPTER13.LINEARFACTORMODELS\ntimesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\nSlowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\nprinciple.Theideaisthattheimportantcharacteristicsofsceneschangevery\nslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\nscene.Forexample,incomputervision,individualpixelvaluescanchangevery\nrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel\nwillrapidlychangefromblacktowhiteandbackagainasthezebra\u2019sstripespass\noverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe\nimagewillnotchangeatall,andthefeaturedescribingthezebra\u2019spositionwill\nchangeslowly.\u00a0Wethereforemaywishtoregularizeourmodeltolearnfeatures\nthatchangeslowlyovertime.\nTheslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\ntoawidevarietyofmodels(,;,; ,; Hinton1989F\u00f6ldi\u00e1k1989Mobahi e t a l .2009\nBergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\ndi\ufb00erentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\nintroducedbyaddingatermtothecostfunctionoftheform\n\u03bb\ue058\ntL f(( x( + 1 ) t)( , f x( ) t)) (13.7)\nwhere \u03bbisahyperparameter determiningthestrengthoftheslownessregularization\nterm, tistheindexintoatimesequenceofexamples, fisthefeatureextractor\ntoberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)\nand f( x( + 1 ) t).Acommonchoiceforisthemeansquareddi\ufb00erence. L\nSlowfeatureanalysisisaparticularlye\ufb03cientapplicationoftheslowness\nprinciple.Itise\ufb03cientbecauseitisappliedtoalinearfeatureextractor,andcan\nthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\ngenerativemodelperse,inthesensethatitde\ufb01nesalinearmapbetweeninput\nspaceandfeaturespacebutdoesnotde\ufb01neaprioroverfeaturespaceandthus\ndoesnotimposeadistributiononinputspace. p() x\nTheSFAalgorithm(WiskottandSejnowski2002,)consistsofde\ufb01ning f( x; \u03b8)\ntobealineartransformation,andsolvingtheoptimization problem\nmin\n\u03b8E t(( f x( + 1 ) t) i\u2212 f( x( ) t) i)2(13.8)\nsubjecttotheconstraints\nE t f( x( ) t) i= 0 (13.9)\nand\nE t[( f x( ) t)2\ni] = 1 . (13.10)\n494", "CHAPTER13.LINEARFACTORMODELS\nTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\nproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\nvaluesandobtainadi\ufb00erentsolutionwithequalvalueoftheslownessobjective.\nTheconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\npathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\nareordered,withthe\ufb01rstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\nmustalsoaddtheconstraint\n\u2200 i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11)\nThisspeci\ufb01esthatthelearnedfeaturesmustbelinearlydecorrelated fromeach\nother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe\noneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\nreconstructionerror,\u00a0to\u00a0forcethe\u00a0featurestodiversify,\u00a0but\u00a0thisdecorrelation\nmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\nproblemmaybesolvedinclosedformbyalinearalgebrapackage.\nSFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\nexpansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe\nquadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear\nSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors\nbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis\nexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron\ntopofthatexpansion.\nWhentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\nquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\nthoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\nonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep\nSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\nbyneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA\nthusseemstobeareasonablybiologicallyplausiblemodel.\nAmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich\nfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical\npredictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\ncon\ufb01guration space\u00a0(e.g.,\u00a0inthe\u00a0caseofrandom\u00a0motion inthe\u00a03-Drendered\nenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\ndistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\ntheunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe\noptimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\nappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\n495", "CHAPTER13.LINEARFACTORMODELS\nThisisincomparisontootherlearningalgorithmswherethecostfunctiondepends\nhighlyonspeci\ufb01cpixelvalues,makingitmuchmoredi\ufb03culttodeterminewhat\nfeaturesthemodelwilllearn.\nDeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\nestimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome\nthebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited\nitsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and\nthat,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\nitwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\nonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\nwhethertheobject\u2019svelocityishighorlow,buttheslownessprincipleencourages\nthemodeltoignorethepositionofobjectsthathavehighvelocity.\n13.4SparseCoding\nSpar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996\nbeenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction\nmechanism.\u00a0Strictlyspeaking,theterm\u201csparsecoding\u201dreferstotheprocessof\ninferringthevalueof hinthismodel,while\u201csparsemodeling\u201dreferstotheprocess\nofdesigningandlearningthemodel,buttheterm\u201csparsecoding\u201disoftenusedto\nrefertoboth.\nLikemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\nobtainreconstructionsof x,asspeci\ufb01edinequation.Morespeci\ufb01cally,sparse 13.2\ncodingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\nisotropicprecision: \u03b2\np , ( ) = (; + x h| N x W h b1\n\u03b2I) . (13.12)\nThedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen\nandField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\nStudent- tdistributions.Forexample,theLaplacepriorparametrized intermsof\nthesparsitypenaltycoe\ufb03cientisgivenby \u03bb\np h( i) = Laplace( h i;0 ,2\n\u03bb) =\u03bb\n4e\u22121\n2\u03bb h| i|(13.13)\nandtheStudent-priorby t\np h( i) \u221d1\n(1+h2\ni\n\u03bd)\u03bd +1\n2. (13.14)\n496", "CHAPTER13.LINEARFACTORMODELS\nTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\ntrainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\nreconstructthedatagiventheencoding.Thisapproachwillbejusti\ufb01edfurtheras\naprincipledapproximation tomaximumlikelihoodlater,insection.19.3\nFormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\nthatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder\nthatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\nisanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek\nthesinglemostlikelycodevalue:\nh\u2217= () = argmax f x\nhp . ( ) h x| (13.15)\nWhencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12\noptimization problem:\nargmax\nhp( ) h x| (13.16)\n=argmax\nhlog( ) p h x| (13.17)\n=argmin\nh\u03bb|||| h 1+ \u03b2||\u2212 || x W h2\n2 , (13.18)\nwherewehavedroppedtermsnotdependingon handdividedbypositivescaling\nfactorstosimplifytheequation.\nDuetotheimpositionofan L1normon h,thisprocedurewillyieldasparse\nh\u2217(Seesection).7.1.2\nTotrainthemodelratherthanjustperforminference,wealternatebetween\nminimization withrespectto handminimization withrespectto W.Inthis\npresentation,wetreat \u03b2asahyperparameter.Typicallyitissetto1becauseits\nroleinthisoptimization problemissharedwith \u03bbandthereisnoneedforboth\nhyperparameters.Inprinciple,wecouldalsotreat \u03b2asaparameterofthemodel\nandlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\non hbutdodependon \u03b2.Tolearn \u03b2,thesetermsmustbeincluded,or \u03b2will\ncollapseto.0\nNotallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|).\nOftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\nvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\nIfwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor\nanelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially\nsparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d\n497", "CHAPTER13.LINEARFACTORMODELS\ninferenceinadi\ufb00erentmodelfamily,thespikeandslabsparsecodingmodel,for\nwhichsamplesfromthepriorusuallycontaintruezeros.\nThesparsecodingapproachcombinedwiththeuseofthenon-parametric\nencodercaninprincipleminimizethecombinationofreconstructionerrorand\nlog-priorbetterthananyspeci\ufb01cparametricencoder.Anotheradvantageisthat\nthereisnogeneralization errortotheencoder.Aparametricencodermustlearn\nhowtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble\nthetrainingdata,alearned,parametricencodermayfailto\ufb01ndan hthatresults\ninaccuratereconstructionorasparsecode.Forthevastmajorityofformulations\nofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\nprocedurewillalways\ufb01ndtheoptimalcode(unlessdegeneratecasessuchas\nreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\ncanstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe\ndecoderweights,ratherthangeneralization errorintheencoder.Thelackof\ngeneralization errorinsparsecoding\u2019soptimization-based encodingprocessmay\nresultinbettergeneralization whensparsecodingisusedasafeatureextractorfor\naclassi\ufb01erthanwhenaparametricfunctionisusedtopredictthecode.Coates\nandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\nobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\nencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l .\n()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\nextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\nlabelsperclass).\nTheprimarydisadvantageofthenon-parametric encoderisthatitrequires\ngreatertimetocompute hgiven xbecausethenon-parametric approachrequires\nrunninganiterativealgorithm.Theparametricautoencoderapproach,developed\nin\u00a0chapter\u00a0,usesonly\u00a0a\u00a0\ufb01xed\u00a0n umber\u00a0of\u00a0layers,\u00a0often\u00a0only\u00a0one.Another 14\ndisadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe\nnon-parametric encoder,whichmakesitdi\ufb03culttopretrainasparsecodingmodel\nwithanunsupervisedcriterionandthen\ufb01ne-tuneitusingasupervisedcriterion.\nModi\ufb01edversionsofsparsecodingthatpermitapproximate derivativesdoexist\nbutarenotwidelyused( ,). BagnellandBradley2009\nSparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\nshownin\ufb01gure.Thishappensevenwhenthemodelisabletoreconstruct 13.2\nthedatawellandprovideusefulfeaturesforaclassi\ufb01er.Thereasonisthateach\nindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\nresultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated\nsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-\n498", "CHAPTER13.LINEARFACTORMODELS\nFigure13.2:\u00a0Example samplesandweightsfromaspikeandslabsparsecodingmodel\ntrainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe\ntrainingexamples.At\ufb01rstglance,onemightassumethemodelispoorly\ufb01t.The ( R i g h t )\nweightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\ndigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\noverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets\nareappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\ngenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\nreproducedwithpermissionfromGoodfellow2013d e t a l .().\nfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore\nsophisticatedshallowmodels.\n13.5ManifoldInterpretationofPCA\nLinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\nlearningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997\nde\ufb01ningathinpancake-shapedregionofhighprobability\u2014aGaussiandistribution\nthatisverynarrowalongsomeaxes,justasapancakeisvery\ufb02atalongitsvertical\naxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\naxes.\u00a0Thisisillustratedin\ufb01gure.\u00a0PCAcanbeinterpretedasaligningthis 13.3\npancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation\nappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\nmatrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto\nxaspossible,\nLettheencoderbe\nh x W = ( f) = \ue03e( ) x \u00b5\u2212 . (13.19)\n499", "CHAPTER13.LINEARFACTORMODELS\nTheencodercomputesalow-dimensional representationof h.Withtheautoencoder\nview,wehaveadecodercomputingthereconstruction\n\u02c6 x h b V h = ( g) = + . (13.20)\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\nmanifold.The\ufb01gureshowstheupperhalfofthe\u201cpancake\u201dabovethe\u201cmanifoldplane\u201d\nwhichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis\nverysmall(arrowpointingoutofplane)andcanbeconsideredlike\u201cnoise,\u201dwhiletheother\nvariancesarelarge(arrowsintheplane)andcorrespondto\u201csignal,\u201dandacoordinate\nsystemforthereduced-dimensiondata.\nThechoicesoflinearencoderanddecoderthatminimizereconstructionerror\nE[||\u2212 x\u02c6 x||2] (13.21)\ncorrespondto V= W, \u00b5= b= E[ x]andthecolumnsof Wformanorthonormal\nbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\nmatrix\nC x \u00b5 x \u00b5 = [( E \u2212)(\u2212)\ue03e] . (13.22)\nInthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe\nmagnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\nOnecanalsoshowthateigenvalue \u03bb iof Ccorrespondstothevarianceof x\ninthedirectionofeigenvector v( ) i.If x\u2208 RDand h\u2208 Rdwith d < D,thenthe\n500", "CHAPTER13.LINEARFACTORMODELS\noptimalreconstructionerror(choosing,,andasabove)is \u00b5 b V W\nmin[ E||\u2212 x\u02c6 x||2] =D \ue058\ni d = + 1\u03bb i . (13.23)\nHence,ifthecovariancehasrank d,theeigenvalues \u03bb d + 1to \u03bb Dare0andrecon-\nstructionerroris0.\nFurthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\nmaximizingthevariancesoftheelementsof h,underorthogonal W,insteadof\nminimizingreconstructionerror.\nLinearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe\nsimplestmodelsthatlearnarepresentationofdata.Muchaslinearclassi\ufb01ersand\nlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\nfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\nmodelsthatperformthesametasksbutwithamuchmorepowerfuland\ufb02exible\nmodelfamily.\n501", "C h a p t e r 1 4\nA u t o e n co d e rs\nAn aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput\ntoitsoutput.\u00a0Internally ,ithasahiddenlayer hthatdescribesa c o deusedto\nrepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\nencoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h).\nThisarchitectureispresentedin\ufb01gure.Ifanautoencodersucceedsinsimply 14.1\nlearningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,\nautoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\nrestrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly\ninputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\nwhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\ndata.\nModern\u00a0autoencoders\u00a0havegeneralized\u00a0the\u00a0idea of\u00a0anencoder\u00a0and\u00a0ade-\ncoderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and\npdecoder( ) x h|.\nTheideaofautoencodershasbeenpartofthehistoricallandscapeofneural\nnetworksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\n1994).Traditionally,\u00a0autoencoderswereused\u00a0fordimensionalityreductionor\nfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersand\nlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\nmodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\naspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame\ntechniques,typicallyminibatchgradientdescentfollowinggradientscomputed\nbyback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\nalsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning\nalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\n502", "CHAPTER14.AUTOENCODERS\ntotheactivationsonthereconstructedinput.Recirculationisregardedasmore\nbiologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning\napplications.\nxx rrh h\nf g\nFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\n(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder\nhastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto\nr).\n14.1UndercompleteAutoencoders\nCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\ninterestedintheoutputofthe\u00a0decoder. Instead,\u00a0wehope\u00a0thattrainingthe\nautoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful\nproperties.\nOnewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto\nhavesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless\nthantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete\nrepresentationforcestheautoencodertocapturethemostsalientfeaturesofthe\ntrainingdata.\nThelearningprocessisdescribedsimplyasminimizingalossfunction\nL , g f ( x(())) x (14.1)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthemeansquarederror.\nWhenthedecoderislinearand Listhemeansquarederror,anundercomplete\nautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\ntrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\ntrainingdataasaside-e\ufb00ect.\nAutoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-\ntions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-\n5 0 3", "CHAPTER14.AUTOENCODERS\nnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\ncanlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\nthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\nwithaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto\nrepresenteachtrainingexample x() iwiththecode i.Thedecodercouldlearnto\nmaptheseintegerindicesbacktothevaluesofspeci\ufb01ctrainingexamples.This\nspeci\ufb01cscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\ncodertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\nthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\n14.2RegularizedAutoencoders\nUndercomplete autoencoders,withcodedimensionlessthantheinputdimension,\ncanlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\ntheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\ngiventoomuchcapacity.\nAsimilarproblemoccursifthehiddencodeisallowedtohavedimension\nequaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas\ndimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear\ndecodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\naboutthedatadistribution.\nIdeally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\nthecodedimensionandthecapacityoftheencoderanddecoderbasedonthe\ncomplexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\nabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\nanddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\nfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheability\ntocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\nrepresentation,smallnessofthederivativeoftherepresentation,androbustness\ntonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand\novercompletebutstilllearnsomethingusefulaboutthedatadistributionevenif\nthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.\nInadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted\nasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\nandequippedwithaninferenceprocedure(forcomputinglatentrepresentations\ngiveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\nmodelingapproachesthatemphasizethisconnectionwithautoencodersarethe\ndescendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b\n5 0 4", "CHAPTER14.AUTOENCODERS\nautoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12\nThesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\nanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\narenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\ntheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\n1 4 . 2 . 1 S p a rse A u t o en co d ers\nAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa\nsparsitypenalty\u2126( h)onthecodelayer h,inadditiontothereconstructionerror:\nL , g f ( x(()))+\u2126() x h (14.2)\nwhere g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder\noutput.\nSparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch\nasclassi\ufb01cation.Anautoencoderthathasbeenregularizedtobesparsemust\nrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\nthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe\ncopyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\nfeaturesasabyproduct.\nWecanthink\u00a0ofthepenalty \u2126( h)simplyasaregularizertermaddedto\nafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\n(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\n(with\u00a0asupervised\u00a0learning\u00a0ob jective)\u00a0thatdepends\u00a0on\u00a0thesesparsefeatures.\nUnlikeotherregularizerssuchasweightdecay,thereisnotastraightforward\nBayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1\nwithweightdecayandotherregularizationpenaltiescanbeinterpretedasa\nMAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\ncorrespondingtoapriorprobabilitydistributionoverthemodelparameters.In\nthisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( \u03b8 x|),\nwhichisequivalenttomaximizing log p( x \u03b8|)+log p( \u03b8).\u00a0The log p( x \u03b8|)term\nistheusualdatalog-likelihoodtermandthelog p( \u03b8)term,thelog-priorover\nparameters,incorporatesthepreferenceoverparticularvaluesof \u03b8.Thisviewwas\ndescribedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\nbecausetheregularizerdependsonthedataandisthereforebyde\ufb01nitionnota\npriorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\ntermsasimplicitlyexpressingapreferenceoverfunctions.\nRatherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\ntask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\n5 0 5", "CHAPTER14.AUTOENCODERS\nmaximumlikelihood\u00a0trainingofagenerativemodel\u00a0thathaslatentvariables.\nSupposewehaveamodelwithvisiblevariables xandlatentvariables h,with\nanexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto\npmodel( h)asthemodel\u2019spriordistributionoverthelatentvariables,representing\nthemodel\u2019sbeliefspriortoseeing x.Thisisdi\ufb00erentfromthewaywehave\npreviouslyusedtheword\u201cprior,\u201dtorefertothedistribution p( \u03b8)encodingour\nbeliefsaboutthemodel\u2019sparametersbeforewehaveseenthetrainingdata.The\nlog-likelihoodcanbedecomposedas\nlog pmodel() = log x\ue058\nhpmodel( ) h x , . (14.3)\nWecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\nforjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative\nmodel(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather\nthantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof\nview,withthischosen,wearemaximizing h\nlog pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)\nThelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,\npmodel( h i) =\u03bb\n2e\u2212| \u03bb h i|, (14.5)\ncorrespondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\nabsolutevaluepenalty,weobtain\n\u2126() = h \u03bb\ue058\ni| h i| (14.6)\n\u2212log pmodel() = h\ue058\ni\ue012\n\u03bb h| i|\u2212log\u03bb\n2\ue013\n= \u2126()+const h (14.7)\nwheretheconstanttermdependsonlyon \u03bbandnot h.Wetypicallytreat \u03bbasa\nhyperparameteranddiscardtheconstanttermsinceitdoesnota\ufb00ecttheparameter\nlearning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From\nthispointofviewofsparsityasresultingfromthee\ufb00ectof pmodel( h)onapproximate\nmaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\nall.\u00a0Itisjustaconsequenceofthemodel\u2019sdistributionoveritslatentvariables.\nThisviewprovidesadi\ufb00erentmotivationfortraininganautoencoder:itisaway\nofapproximately trainingagenerativemodel.Italsoprovidesadi\ufb00erentreasonfor\n5 0 6", "CHAPTER14.AUTOENCODERS\nwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\nvariablesthatexplaintheinput.\nEarlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008\nvariousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\nandthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected\nprobabilisticmodel p( x) =1\nZ\u02dc p( x).Theideaisthatminimizing log Zpreventsa\nprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity\non\u00a0anautoencoder\u00a0preventstheautoencoderfrom\u00a0having\u00a0lowreconstruction\nerroreverywhere.Inthiscase,\u00a0theconnectionisonthelevelofanintuitive\nunderstandingofageneralmechanismratherthanamathematical correspondence.\nTheinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina\ndirectedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward.\nOnewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders\nwasintroducedin ().Theideaistouserecti\ufb01edlinearunitsto Glorot e t a l .2011b\nproducethecodelayer.Withapriorthatactuallypushestherepresentationsto\nzero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\nnumberofzerosintherepresentation.\n1 4 . 2 . 2 D en o i s i n g A u t o en co d ers\nRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder \u2126 \nthatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\nfunction.\nTraditionally,autoencodersminimizesomefunction\nL , g f ( x(())) x (14.8)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthe L2normoftheirdi\ufb00erence.\u00a0This encourages g f\u25e6tolearntobemerelyan\nidentityfunctioniftheyhavethecapacitytodoso.\nA orDAEinsteadminimizes denoising aut o e nc o der\nL , g f ( x((\u02dc x))) , (14.9)\nwhere \u02dc xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising\nautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\ninput.\nDenoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),\nasshown\u00a0by\u00a0 ()\u00a0and ().Denoising AlainandBengio2013Bengio\u00a0 e t a l .2013c\n5 0 7", "CHAPTER14.AUTOENCODERS\nautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\nasabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\nhowovercomplete,high-capacity modelsmaybeusedasautoencoderssolong\nascareistakentopreventthemfromlearningtheidentityfunction.\u00a0Denoising\nautoencodersarepresentedinmoredetailinsection.14.5\n1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es\nAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse \u2126\nautoencoders,\nL , g f , , ( x(()))+\u2126( x h x) (14.10)\nbutwithadi\ufb00erentformof:\u2126\n\u2126( ) = h x , \u03bb\ue058\ni||\u2207 x h i||2. (14.11)\nThisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\nchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\ntheautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\ndistribution.\nAnautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der\norCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,\nmanifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail\ninsection.14.7\n14.3RepresentationalPower,LayerSizeandDepth\nAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\ndecoder.However,thisisnotarequirement.Infact,usingdeepencodersand\ndecoderso\ufb00ersmanyadvantages.\nRecallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\nwardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages\nalsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork\nasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\nbene\ufb01tfromdepth.\nOnemajoradvantageofnon-trivialdepthisthattheuniversalapproximator\ntheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\nlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan\n5 0 8", "CHAPTER14.AUTOENCODERS\narbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\nthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\nfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrom\ninputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\nconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\nleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any\nmappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.\nDepthcanexponentiallyreducethecomputational costofrepresentingsome\nfunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\nneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\ndepthinfeedforwardnetworks.\nExperimentally,deepautoencodersyieldmuchbettercompressionthancorre-\nspondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\nAcommonstrategyfortrainingadeepautoencoderistogreedilypretrain\nthedeeparchitecturebytrainingastackofshallowautoencoders,soweoften\nencountershallowautoencoders,evenwhentheultimategoalistotrainadeep\nautoencoder.\n14.4StochasticEncodersandDecoders\nAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\nunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\nautoencoders.\nAsdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\nandthelossfunctionofafeedforwardnetworkistode\ufb01neanoutputdistribution\np( y x|)andminimizethenegativelog-likelihood\u2212log p( y x|).Inthatsetting, y\nwasavectoroftargets,suchasclasslabels.\nInthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,\nwecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay\nthinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|).\u00a0We\nmaythentraintheautoencoderbyminimizing \u2212log pdecoder( ) x h|.Theexact\nformofthislossfunctionwillchangedependingontheformof pdecoder.Aswith\ntraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize\nthemeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative\nlog-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues\ncorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid\noutputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon.\n5 0 9", "CHAPTER14.AUTOENCODERS\nTypically,theoutputvariablesaretreatedasbeingconditionallyindependent\ngiven hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome\ntechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs\nwithcorrelations.\nxx rrh h\np e n c o d e r ( ) h x| p d e c o d e r ( ) x h|\nFigure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\ndecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat\ntheiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder\nand pdecoder( ) x h|forthedecoder.\nTomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\npreviously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to\nan e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedin\ufb01gure.14.2\nAnylatentvariablemodel pmodel( ) h x ,de\ufb01nesastochasticencoder\npencoder( ) = h x| pmodel( ) h x| (14.12)\nandastochasticdecoder\npdecoder( ) = x h| pmodel( ) x h| . (14.13)\nIngeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\ndistributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l .\n()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\nwilltendtomakethemcompatibleasymptotically(withenoughcapacityand\nexamples).\n14.5DenoisingAutoencoders\nThe denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted\ndatapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\nasitsoutput.\nTheDAEtrainingprocedureisillustratedin\ufb01gure.Weintroducea 14.3\ncorruptionprocess C(\u02dcx x|)whichrepresentsaconditional\u00a0distrib utionover\n5 1 0", "CHAPTER14.AUTOENCODERS\n\u02dc x \u02dc x L Lh h\nfg\nxxC ( \u02dc x x| )\nFigure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,\nwhichistrainedtoreconstructthecleandatapoint xfromitscorruptedversion\u02dc x.\nThisisaccomplishedbyminimizingtheloss L=\u2212log pdecoder( x h|= f(\u02dc x)),where\n\u02dc xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption\nprocess C(\u02dc x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean\nparametersareemittedbyafeedforwardnetwork. g\ncorruptedsamples \u02dc x,givenadatasample x.Theautoencoderthenlearnsa\nr e c o nst r u c t i o n di st r i but i o n preconstruct( x|\u02dc x)estimatedfromtrainingpairs\n( x ,\u02dc x),asfollows:\n1.\u00a0Sampleatrainingexamplefromthetrainingdata. x\n2.\u00a0Sampleacorruptedversion\u02dc xfrom C(\u02dc x x|= ) x.\n3.Use( x ,\u02dc x)asatrainingexampleforestimatingtheautoencoderreconstruction\ndistribution preconstruct( x|\u02dcx) = pdecoder( x h|)with htheoutputofencoder\nf(\u02dc x)and pdecodertypicallyde\ufb01nedbyadecoder. g() h\nTypicallywecansimplyperformgradient-basedapproximate minimization (such\nasminibatchgradientdescent)onthenegativelog-likelihood\u2212log pdecoder( x h|).\nSolongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\nnetwork\u00a0andmay\u00a0be\u00a0trainedwith\u00a0exactlythesame\u00a0techniques\u00a0as\u00a0anyother\nfeedforwardnetwork.\nWecanthereforeviewtheDAEasperformingstochasticgradientdescenton\nthefollowingexpectation:\n\u2212 E x\u223c\u02c6 p d a t a() x E\u02dc x\u223c C(\u02dcx| x)log pdecoder( = ( x h| f\u02dc x))(14.14)\nwhere \u02c6 pdata() xisthetrainingdistribution.\n5 1 1", "CHAPTER14.AUTOENCODERS\nx\u02dc x\ng f\u25e6\n\u02dc x\nC ( \u02dc x x| )\nx\nFigure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint\u02dcxbackto\ntheoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara\nlow-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption\nprocess C(\u02dcx x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\nhowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.\nWhenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\n|| g( f(\u02dc x))\u2212|| x2,thereconstruction g( f(\u02dc x)) estimates E x ,\u02dc x\u223c p dat a()( x C\u02dcx x|)[ x|\u02dc x].Thevector\ng( f(\u02dcx))\u2212\u02dc xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(\u02dcx))\nestimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenriseto\u02dc x.The\nautoencoderthuslearnsavector\ufb01eld g( f( x))\u2212 xindicatedbythegreenarrows.This\nvector\ufb01eldestimatesthescore\u2207 xlog pdata( x)uptoamultiplicativefactorthatisthe\naveragerootmeansquarereconstructionerror.\n5 1 2", "CHAPTER14.AUTOENCODERS\n1 4 . 5 . 1 E s t i m a t i n g t h e S co re\nScorematching(,)isanalternativetomaximumlikelihood.It Hyv\u00e4rinen2005\nprovidesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\nthemodeltohavethesame sc o r easthedatadistributionateverytrainingpoint\nx.Inthiscontext,thescoreisaparticulargradient\ufb01eld:\n\u2207 xlog() p x . (14.15)\nScorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4\nregardingautoencoders,itissu\ufb03cienttounderstandthatlearningthegradient\n\ufb01eldoflog pdataisonewaytolearnthestructureof pdataitself.\nAveryimportantpropertyofDAEsisthat\u00a0theirtrainingcriterion(with\nconditionallyGaussian p( x h|))makes\u00a0theautoencoder\u00a0learnavector\ufb01eld\n( g( f( x))\u2212 x)thatestimatesthescoreofthedatadistribution.Thisisillustrated\nin\ufb01gure.14.4\nDenoisingtrainingofaspeci\ufb01ckindofautoencoder(sigmoidalhiddenunits,\nlinear\u00a0reconstr uction\u00a0units) usingGaussiannoiseand\u00a0meansquared\u00a0erroras\nthereconstructioncostisequivalent(,)totrainingaspeci\ufb01ckind Vincent2011\nofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.\nThiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1\ndiscussionitsu\ufb03cestoknowthatitisamodelthatprovidesanexplicit pmodel( x; \u03b8).\nWhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun\n2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\nautoencoder.Witha\ufb01xednoiselevel,regularizedscorematchingisnotaconsistent\nestimator;itinsteadrecoversablurredversionofthedistribution.However,if\nthenoiselevelischosentoapproach0whenthenumberofexamplesapproaches\nin\ufb01nity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\nmoredetailinsection.18.5\nOtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\nappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\ncombinedwitharegularizationtermsimilartothecontractivepenaltyofthe\nCAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen-\ncodergradientprovidesanapproximationtocontrastivedivergencetrainingof\nRBMs.\nForcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand\nreconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\ngeneralencoderanddecoderparametrizations ( ,).This AlainandBengio2013\nmeansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\n5 1 3", "CHAPTER14.AUTOENCODERS\nbytrainingwiththesquarederrorcriterion\n|| g f((\u02dc x x))\u2212||2(14.16)\nandcorruption\nC(\u02dc x=\u02dcx x|) = (N\u02dc x x ;= \u00b5 , \u03c3\u03a3 = 2I) (14.17)\nwithnoisevariance \u03c32.See\ufb01gureforanillustrationofhowthisworks. 14.5\nFigure14.5:Vector\ufb01eldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold\nnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe\nreconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability\naccordingtotheimplicitlyestimatedprobabilitydistribution.Thevector\ufb01eldhaszeros\natbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima\nofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof\nlocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\nthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength\nofthearrows)islarge,itmeansthatprobabilitycanbesigni\ufb01cantlyincreasedbymoving\ninthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.\nTheautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.\nWhereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore\naccurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013\nIngeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe\ninput xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis\n5 1 4", "CHAPTER14.AUTOENCODERS\nwhytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011\nwhere g( f( x))\u2212 xmaybeobtainedbytakingthederivativeofanotherfunction.\nKamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by\nidentifyingafamilyofshallowautoencoderssuchthat g( f( x))\u2212 xcorrespondsto\nascoreforallmembersofthefamily.\nSofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\naprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas\nagenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed\nlater,insection.20.11\n1 4 . 5 . 1 . 1 Hi st o r i c a l P e r spec t i v e\nTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987\nand ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001\nimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\nHowever,thename\u201cdenoisingautoencoder\u201dreferstoamodelthatisintendednot\nmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation\nas\u00a0asidee\ufb00ect\u00a0oflearningto\u00a0denoise.This\u00a0ideacame\u00a0muchlater\u00a0(Vincent\ne t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\ndeeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\nsparsecoding,contractiveautoencodersandotherregularizedautoencoders,the\nmotivationforDAEswastoallowthelearningofaveryhigh-capacity encoder\nwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\nPriortotheintroduction ofthemodernDAE,InayoshiandKurita2005()\nexploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\nminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\nnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\ngeneralization byintroducing\u00a0the reconstructionerror\u00a0andtheinjectednoise.\nHowever,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction\nfamiliesaspowerfulascanthemodernDAE.\n14.6LearningManifoldswithAutoencoders\nLike\u00a0many\u00a0other\u00a0machine\u00a0learning\u00a0algorithms,\u00a0auto encoders\u00a0exploittheidea\nthatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\nmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3\nthisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe\nmanifoldbutmayhaveunusualbehaviorifgivenaninputthatiso\ufb00themanifold.\n5 1 5", "CHAPTER14.AUTOENCODERS\nAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\nTounderstandhowautoencodersdothis,wemustpresentsomeimportant\ncharacteristicsofmanifolds.\nAnimportantcharacterization ofamanifoldisthesetofits t angen t pl anes.\nAtapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis\nvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\nillustratedin\ufb01gure,theselocaldirectionsspecifyhowonecanchange 14.6 x\nin\ufb01nitesimallywhilestayingonthemanifold.\nAllautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\n1.Learningarepresentation hofatrainingexample xsuchthat xcanbe\napproximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn\nfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed\nnotsuccessfullyreconstructinputsthatarenotprobableunderthedata\ngeneratingdistribution.\n2.\u00a0Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-\nturalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\naregularizationtermaddedtothereconstructioncost.Thesetechniques\ngenerallyprefersolutionsthatarelesssensitivetotheinput.\nClearly,neitherforcealonewouldbeuseful\u2014copyingtheinputtotheoutput\nisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\nareusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\naboutthestructureofthedatageneratingdistribution.Theimportantprinciple\nisthattheautoencodercana\ufb00ordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d\nt o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates\nnearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture\nalocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\nmanifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder\nlearnsamappingfromtheinputspace xtoarepresentationspace,amappingthat\nisonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\nchangesorthogonaltothemanifold.\nAone-dimensional exampleisillustratedin\ufb01gure,showingthat,bymaking 14.7\nthereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\ndatapoints,wecausetheautoencodertorecoverthemanifoldstructure.\nTounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\nstructivetocomparethemtootherapproaches.Whatismostcommonlylearned\ntocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)\n5 1 6", "CHAPTER14.AUTOENCODERS\nFigure14.6:\u00a0Anillustrationoftheconceptofatangenthyperplane.Herewecreatea\none-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784\npixelsandtransformitbytranslatingitvertically.\u00a0Theamountofverticaltranslation\nde\ufb01nesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath\nthroughimagespace.Thisplotshowsafewpointsalongthismanifold.\u00a0Forvisualization,\nwehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional\nmanifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches\nthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.\nItde\ufb01nesthespaceofdirectionsinwhichitispossibletomovewhileremainingon\nthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean\nexampletangentlineatonepoint,withanimageshowinghowthistangentdirection\nappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong\nthetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels\nthatdarken.\n5 1 7", "CHAPTER14.AUTOENCODERS\nx 0 x 1 x 2\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y\nO p t i m a l r e c o n s t r u c t i o n\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\nperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\nthemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\nlineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction\nfunctioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\narrowsatthebottomoftheplotindicatethe r( x)\u2212 xreconstructiondirectionvector\natthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest\u201cmanifold\u201d\n(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake\nthederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The\ncontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is\naskedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\nspacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\nthereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints\nbackontothemanifold.\nthemanifold.Sucharepresentationforaparticularexampleisalsocalledits\nembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions\nthanthe\u201cambient\u201dspaceofwhichthemanifoldisalow-dimensionalsubset.Some\nalgorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly\nlearnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\nmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\npointintheambientspace(theinputspace)toitsembedding.\nManifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\nattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\nonlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased\nonthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample\nandedgesconnectingnearneighborstoeachother.Thesemethods(Sch\u00f6lkopf\ne t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;\n5 1 8", "CHAPTER14.AUTOENCODERS\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph\ninwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor\nrelationships.\u00a0Variousprocedurescanthusobtainthetangentplaneassociatedwitha\nneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\nexamplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize\nsucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber\nofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\napproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l .\n2000).\nandNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\nandRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha\ntangentplanethatspansthedirectionsofvariationsassociatedwiththedi\ufb00erence\nvectorsbetweentheexampleanditsneighbors,asillustratedin\ufb01gure.14.8\nAglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or\nsolvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9\nlargenumberoflocallylinearGaussian-likepatches(or\u201cpancakes,\u201dbecausethe\nGaussiansare\ufb02atinthetangentdirections).\nHowever,thereisafundamentaldi\ufb03cultywithsuchlocalnon-parametric\napproachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005\nmanifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),\nonemayneedaverylargenumberoftrainingexamplestocovereachoneof\n5 1 9", "CHAPTER14.AUTOENCODERS\nFigure14.9:Ifthetangentplanes(see\ufb01gure)ateachlocationareknown,thenthey 14.6\ncanbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\ncanbethoughtofasalocalEuclideancoordinatesystemorasalocally\ufb02atGaussian,or\n\u201cpancake,\u201dwithaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\nverylargevarianceinthedirectionsde\ufb01ningthecoordinatesystemonthepancake.A\nmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold\nParzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003\nvariant( ,). Bengio e t a l .2006c\nthesevariations,withnochancetogeneralizetounseenvariations.Indeed,these\nmethodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between\nneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan\nhaveverycomplicatedstructurethatcanbedi\ufb03culttocapturefromonlylocal\ninterpolation.Considerforexamplethemanifoldresultingfromtranslationshown\nin\ufb01gure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe\nimageistranslated,wewillobservethatonecoordinateencountersapeakora\ntroughinitsvalueonceforeverypeakortroughinbrightnessintheimage.\u00a0In\notherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage\ntemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming\nsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentations\nanddeeplearningforcapturingmanifoldstructure.\n5 2 0", "CHAPTER14.AUTOENCODERS\n14.7ContractiveAutoencoders\nThecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab\nonthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:\n\u2126() = h \u03bb\ue00d\ue00d\ue00d\ue00d\u2202 f() x\n\u2202 x\ue00d\ue00d\ue00d\ue00d2\nF. (14.18)\nThepenalty\u2126( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\nJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\nThereisaconnectionbetweenthedenoisingautoencoderandthecontractive\nautoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\ninput\u00a0noise,\u00a0the\u00a0denoising\u00a0reconstruction erroris\u00a0equivalent\u00a0toacontractive\npenaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother\nwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\n\ufb01nite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\nfeatureextractionfunctionresistin\ufb01nitesimalperturbationsoftheinput.When\nusingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse\nwithaclassi\ufb01er,thebestclassi\ufb01cationaccuracyusuallyresultsfromapplyingthe\ncontractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)\nalsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1\nThename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speci\ufb01-\ncally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\ntomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\nWecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput\nneighborhood.\nToclarify,theCAEiscontractiveonlylocally\u2014allperturbationsofatraining\npoint xaremappednearto f( x).Globally,twodi\ufb00erentpoints xand x\ue030maybe\nmappedto f( x)and f( x\ue030)pointsthatarefartherapartthantheoriginalpoints.\nItisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see\nforexamplewhathappensinthe1-Dtoyexampleof\ufb01gure).Whenthe 14.7 \u2126( h)\npenaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto\nmakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01\ninputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa\nbinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\nmostofthehypercubethatitssigmoidalhiddenunitscanspan.\nWecanthinkoftheJacobianmatrix Jatapoint xasapproximating the\nnonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword\n\u201ccontractive\u201dmoreformally.\u00a0Inthetheoryoflinearoperators,alinearoperator\n5 2 1", "CHAPTER14.AUTOENCODERS\nissaidtobecontractiveifthenormof J xremainslessthanorequaltofor1\nallunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere.\nWecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear\napproximationof f( x)ateverytrainingpoint xinordertoencourageeachof\ntheselocallinearoperatortobecomeacontraction.\nAsdescribed\u00a0insection,\u00a0regularized autoencoderslearnmanifoldsby 14.6\nbalancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare\nreconstructionerrorandthecontractivepenalty\u2126( h).Reconstructionerroralone\nwouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\nalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\nThecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\n\u2202 f() x\n\u2202 xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\nsmallnumberofdirectionsintheinput,mayhavesigni\ufb01cantderivatives.\nThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions\nxwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich\napproximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a\nand ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b\nof Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1\nsomesingularvaluesremainabove,becausethereconstructionerrorpenalty 1\nencouragestheCAEtoencodethedirectionswiththemostlocalvariance.The\ndirectionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent\ndirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent\ndirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE\nappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas\nobjectsintheimagegraduallychangepose,asshownin\ufb01gure.Visualizations 14.6\noftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful\ntransformationsoftheinputimage,asshownin\ufb01gure.14.10\nOnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\nischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\nmuchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\nRifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each\ntrainedtoreconstructthepreviousautoencoder\u2019shiddenlayer.Thecomposition\noftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\nseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive\naswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\ntheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\ncapturesmanyofthedesirablequalitativecharacteristics.\nAnotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\n5 2 2", "CHAPTER14.AUTOENCODERS\nInput\npointTangentvectors\nLocalPCA(nosharingacrossregions)\nContractiveautoencoder\nFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\nandbyacontractiveautoencoder.Thelocationonthemanifoldisde\ufb01nedbytheinput\nimageofadogdrawnfromtheCIFAR-10dataset.\u00a0Thetangentvectorsareestimated\nbytheleadingsingularvectorsoftheJacobianmatrix\u2202 h\n\u2202 xoftheinput-to-codemapping.\nAlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto\nformmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\nsharingacrossdi\ufb00erentlocationsthatshareasubsetofactivehiddenunits.\u00a0TheCAE\ntangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\ntheheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c\nifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\ncouldconsistofmultiplyingtheinputbyasmallconstant \ue00fandthedecoder\ncouldconsistofdividingthecodeby \ue00f.As \ue00fapproaches,theencoderdrivesthe 0\ncontractivepenalty\u2126( h)toapproachwithouthavinglearnedanythingaboutthe 0\ndistribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\ne t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare\nstandardneuralnetworklayersconsistingofana\ufb03netransformationfollowedby\nanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g\ntobethetransposeoftheweightmatrixof. f\n14.8PredictiveSparseDecomposition\nP r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse\ncodingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric\nencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\nappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\n(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell\nasforaudio( ,).Themodelconsistsofanencoder Hena\ufb00 e t a l .2011 f( x)anda\ndecoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe\n5 2 3", "CHAPTER14.AUTOENCODERS\noptimization algorithm.Trainingproceedsbyminimizing\n||\u2212 || x g() h2+ \u03bb|| h1+ () \u03b3 f ||\u2212 h x||2. (14.19)\nLikeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with\nrespectto handminimization withrespecttothemodelparameters.Minimization\nwithrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe\ncostfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent\ncanobtainreasonablevaluesofinasfewastensteps. h\nThetrainingprocedureusedbyPSDisdi\ufb00erentfrom\ufb01rsttrainingasparse\ncodingmodelandthentraining f( x)topredictthevaluesofthesparsecoding\nfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparameters\nforwhichcaninfergoodcodevalues. f() x\nPredictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e.\nInsection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19\nmakeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\nprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\nmodel.\nInpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring\ntraining.Theparametricencoder fisusedtocomputethelearnedfeatureswhen\nthemodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto\ninferring hviagradientdescent.Because fisadi\ufb00erentiableparametricfunction,\nPSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained\nwithanothercriterion.\n14.9ApplicationsofAutoencoders\nAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\nmationretrievaltasks.Dimensionalityreductionwasoneofthe\ufb01rstapplications\nofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\nforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\nastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\nwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The\nresultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand\nthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\nunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\nLower-dimensionalrepresentationscanimproveperformanceonmanytasks,\nsuchasclassi\ufb01cation.Modelsofsmallerspacesconsumelessmemoryandruntime.\n5 2 4", "CHAPTER14.AUTOENCODERS\nManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\neachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l .\n().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\ngeneralization.\nOnetaskthatbene\ufb01tsevenmorethanusualfromdimensionalityreductionis\ni nf o r m at i o n r e t r i e v al,thetaskof\ufb01ndingentriesinadatabasethatresemblea\nqueryentry.\u00a0Thistaskderivestheusualbene\ufb01tsfromdimensionalityreduction\nthatothertasksdo,butalsoderivestheadditionalbene\ufb01tthatsearchcanbecome\nextremelye\ufb03cientincertainkindsoflowdimensionalspaces.Speci\ufb01cally,\u00a0if\nwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\ndimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary\nmappingbinarycodevectorstoentries.Thishashtableallowsustoperform\ninformationretrievalbyreturningalldatabaseentriesthathavethesamebinary\ncodeasthe\u00a0query.Wecanalso\u00a0search\u00a0overslightlylesssimilar\u00a0entries\u00a0very\ne\ufb03ciently,justby\ufb02ippingindividualbitsfromtheencodingofthequery.\u00a0This\napproachtoinformationretrievalviadimensionalityreductionandbinarization\niscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas\nbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\nimages(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,).\nToproducebinarycodesforsemantichashing,onetypicallyusesanencoding\nfunctionwithsigmoidsonthe\ufb01nallayer.Thesigmoidunitsmustbetrainedtobe\nsaturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\nthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\ntraining.Themagnitudeofthenoiseshouldincreaseovertime.To\ufb01ghtthat\nnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\nmagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\nTheideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\ndirections,includingtheideaoftrainingtherepresentationssoastooptimize\nalossmoredirectlylinkedtothetaskof\ufb01ndingnearbyexamplesinthehash\ntable( ,). NorouziandFleet2011\n5 2 5", "C h a p t e r 1 5\nRepresen t at i on L e ar n i n g\nInthischapter,we\ufb01rstdiscusswhatitmeanstolearnrepresentationsandhow\nthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss\nhowlearningalgorithmssharestatisticalstrengthacrossdi\ufb00erenttasks,including\nusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\nrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\nlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask\nrepresentationexists.Finally,westepbackandargueaboutthereasonsforthe\nsuccessofrepresentationlearning,startingwiththetheoreticaladvantagesof\ndistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand\nendingwiththemoregeneralideaofunderlyingassumptionsaboutthedata\ngeneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.\nManyinformationprocessingtaskscanbeveryeasyorverydi\ufb03cultdepending\nonhowtheinformationisrepresented.Thisisageneralprincipleapplicableto\ndailylife,computerscienceingeneral,andtomachinelearning.Forexample,it\nisstraightforwardforapersontodivide210by6usinglongdivision.\u00a0Thetask\nbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\nnumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\nbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\npermittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\nconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing\nappropriateorinappropriate representations.Forexample,insertinganumber\nintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe\nlistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa\nred-blacktree.\nInthecontextofmachinelearning,whatmakesonerepresentationbetterthan\n526", "CHAPTER15.REPRESENTATIONLEARNING\nanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\nlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\nofthesubsequentlearningtask.\nWecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\nformingakindofrepresentationlearning.Speci\ufb01cally,thelastlayerofthenetwork\nistypicallyalinearclassi\ufb01er,suchasasoftmaxregressionclassi\ufb01er.Therestof\nthenetworklearnstoprovidearepresentationtothisclassi\ufb01er.Trainingwitha\nsupervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\nmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassi\ufb01cation\ntaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\nfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\nlastlayercouldbeanotherkindofmodel,suchasanearestneighborclassi\ufb01er\n(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\nlearndi\ufb00erentpropertiesdependingonthetypeofthelastlayer.\nSupervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\nanyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\nlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\nsomeparticularway.Forexample,supposewewanttolearnarepresentationthat\nmakesdensityestimationeasier.Distributionswithmoreindependencesareeasier\ntomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\noftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,\nunsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\nlearnarepresentationasasidee\ufb00ect.Regardlessofhowarepresentationwas\nobtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\nsupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\nrepresentation.\nMostrepresentationlearningproblemsfaceatradeo\ufb00betweenpreservingas\nmuchinformationabouttheinputaspossibleandattainingniceproperties(such\nasindependence).\nRepresentationlearningisparticularlyinterestingbecauseitprovidesone\nwaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\nlargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\ndata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\nresultsinsevereover\ufb01tting.Semi-supervisedlearningo\ufb00ersthechancetoresolve\nthisover\ufb01ttingproblembyalsolearningfromtheunlabeleddata.Speci\ufb01cally,\nwecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese\nrepresentationstosolvethesupervisedlearningtask.\nHumansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\n5 2 7", "CHAPTER15.REPRESENTATIONLEARNING\nnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\nperformance\u2014forexample,thebrainmayuseverylargeensemblesofclassi\ufb01ers\norBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\nabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\ntoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\nunlabeleddatacanbeusedtolearnagoodrepresentation.\n15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g\nUnsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\nnetworks,enablingresearchersforthe\ufb01rsttimetotrainadeepsupervisednetwork\nwithoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We\ncallthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -\nwi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow\narepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture\ntheshapeoftheinputdistribution)cansometimesbeusefulforanothertask\n(supervisedlearningwiththesameinputdomain).\nGreedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\ntationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\ncodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\npretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\nandproducingasoutputanewrepresentationofthedata,whosedistribution(or\nitsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.\nSeealgorithm foraformaldescription. 15.1\nGreedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\nbeenusedtosidestepthedi\ufb03cultyofjointlytrainingthelayersofadeepneuralnet\nforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron\n(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\nthatthisgreedylearningprocedurecouldbeusedto\ufb01ndagoodinitialization for\najointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\ntosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;\nandSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,).\nPriortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\nresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\nthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep\narchitectures,buttheunsupervisedpretrainingapproachwasthe\ufb01rstmethodto\nsucceed.\nGreedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -\n5 2 8", "CHAPTER15.REPRESENTATIONLEARNING\nr i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one\npieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se\nbecausetheseindependentpiecesarethelayersofthenetwork.Speci\ufb01cally,greedy\nlayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile\nkeepingthepreviousones\ufb01xed.Inparticular,thelowerlayers(whicharetrained\n\ufb01rst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -\nv i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning\nalgorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe\nonlya\ufb01rststepbeforeajointtrainingalgorithmisappliedto \ufb01ne-t uneallthe\nlayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\nasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\ndecreasingtrainingerror)andaformofparameterinitialization.\nItiscommontousetheword\u201cpretraining\u201dtorefernotonlytothepretraining\nstageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining\nphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\ntrainingasimpleclassi\ufb01erontopofthefeatureslearnedinthepretrainingphase,\noritmayinvolvesupervised\ufb01ne-tuningoftheentirenetworklearnedinthe\npretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\nwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining\nschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm\nwillobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining\nfollowthisbasicprotocol.\nGreedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\nforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\nandSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent\nvariables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006\nBoltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\nmodelswillbedescribedinchapter.20\nAsdiscussedinsection,\u00a0itisalsopossibletohavegreedylayer-wise 8.7.4\nsupervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\niseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\ncontexts(,). Erhanetal.2010\n1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk?\nOnmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\nimprovementsintesterrorforclassi\ufb01cationtasks.Thisobservationwasresponsible\nfortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\n5 2 9", "CHAPTER15.REPRESENTATIONLEARNING\nAl g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol.\nGiventhefollowing:\u00a0Unsupervisedfeaturelearningalgorithm L,whichtakesa\ntrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw\ninputdataisX,withonerowperexampleandf( 1 )(X)istheoutputofthe\ufb01rst\nstageencoderonX.Inthecasewhere\ufb01ne-tuningisperformed,weusealearner\nTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised\n\ufb01ne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber\nofstagesis.m\nf\u2190Identityfunction\n\u02dcXX= \nf o r dok,...,m = 1\nf( ) k= (L\u02dcX)\nff\u2190( ) k\u25e6f\n\u02dcX\u2190f( ) k(\u02dcX)\ne nd f o r\ni f\ufb01ne-tuning t he n\nff,, \u2190T(XY)\ne nd i f\nRet ur nf\n2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,\nunsupervisedpretrainingeitherdoesnotconferabene\ufb01torevencausesnoticeable\nharm. ()studiedthee\ufb00ectofpretrainingonmachinelearning Maetal.2015\nmodelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\nslightlyharmful,butformanytaskswassigni\ufb01cantlyhelpful.Becauseunsupervised\npretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand\nwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\ntask.\nAttheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\ntogreedyunsupervisedpretraininginparticular.Thereareother,completely\ndi\ufb00erentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\nsuchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13\ntrainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\nExamplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\nandBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\nobjectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly\nusingtheinput).\nUnsupervisedpretrainingcombinestwodi\ufb00erentideas.First,itmakesuseof\n5 3 0", "CHAPTER15.REPRESENTATIONLEARNING\ntheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\nasigni\ufb01cantregularizinge\ufb00ectonthemodel(and,toalesserextent,thatitcan\nimproveoptimization). Second,itmakesuseofthemoregeneralideathatlearning\nabouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto\noutputs.\nBothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral\npartsofthemachinelearningalgorithmthatarenotentirelyunderstood.\nThe\ufb01rstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\ncanhaveastrongregularizinge\ufb00ectonitsperformance, istheleastwellunderstood.\nAtthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\nmodelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\nanother.\u00a0Today,localminimaarenolongerconsideredtobeaseriousproblem\nforneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork\ntrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains\npossiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\nbeinaccessible\u2014forexample,aregionthatissurroundedbyareaswherethecost\nfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\naverynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\nHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\nverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\npretrainedparametersareretainedduringthesupervisedtrainingstageislimited.\nThisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\nlearningandsupervisedlearningratherthantwosequentialstages.Onemay\nalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe\nsupervisedlearningstagepreservesinformationfromtheunsupervisedlearning\nstagebysimplyfreezingthe\u00a0parameters for\u00a0thefeature\u00a0extractorsand\u00a0using\nsupervisedlearningonlytoaddaclassi\ufb01erontopofthelearnedfeatures.\nTheotheridea,thatalearningalgorithmcanuseinformationlearnedinthe\nunsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\nunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\ntaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\nagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout\nwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\ntherepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\nlearnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,\nsoitisnotalwayspossibletopredictwhichtaskswillbene\ufb01tfromunsupervised\nlearninginthisway.Manyaspectsofthisapproacharehighlydependenton\nthespeci\ufb01cmodelsused.Forexample,ifwewishtoaddalinearclassi\ufb01eron\n5 3 1", "CHAPTER15.REPRESENTATIONLEARNING\ntopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\nseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\nisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\npreferable\u2014theconstraintsimposedbytheoutputlayerarenaturallyincluded\nfromthestart.\nFromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\nwecanexpectunsupervisedpretrainingtobemoree\ufb00ectivewhentheinitial\nrepresentationispoor.\u00a0Onekeyexampleofthisistheuseofwordembeddings.\nWordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo\ndistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance\nof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2\ndistancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially\nusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps\nbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealow\nqualitysimilaritymetric.\nFromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\nexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\nexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\npretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\ntoperformbest\u00a0whenthe\u00a0number\u00a0ofunlabeled\u00a0examples is\u00a0very\u00a0large.The\nadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\nunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin\n2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\ncompetitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011\nnumberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\nofexamplesperclass).Thesee\ufb00ectswerealsodocumentedincarefullycontrolled\nexperimentsbyPaine2014etal.().\nOtherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\nislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.\nUnsupervisedlearningdi\ufb00ersfromregularizerslikeweightdecaybecauseitdoesnot\nbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering\nfeaturefunctionsthatareusefulfortheunsupervisedlearningtask.\u00a0Ifthetrue\nunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput\ndistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\nThesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\npretrainingisknowntocauseanimprovement,andexplainwhatisknownabout\nwhythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused\ntoimproveclassi\ufb01ers,andisusuallymostinterestingfromthepointofviewof\n5 3 2", "CHAPTER15.REPRESENTATIONLEARNING\n\u0000\ue034 \ue030 \ue030 \ue030 \u0000\ue033 \ue030 \ue030 \ue030 \u0000\ue032 \ue030 \ue030 \ue030 \u0000\ue031 \ue030 \ue030 \ue030 \ue030 \ue031 \ue030 \ue030 \ue030 \ue032 \ue030 \ue030 \ue030 \ue033 \ue030 \ue030 \ue030 \ue034 \ue030 \ue030 \ue030\u0000\ue031 \ue035 \ue030 \ue030\u0000\ue031 \ue030 \ue030 \ue030\u0000\ue035 \ue030 \ue030\ue030\ue035 \ue030 \ue030\ue031 \ue030 \ue030 \ue030\ue031 \ue035 \ue030 \ue030\n\ue057 \ue069 \ue074 \ue068 \ue020 \ue070 \ue072 \ue065 \ue074 \ue072 \ue061 \ue069 \ue06e \ue069 \ue06e \ue067\n\ue057 \ue069 \ue074 \ue068 \ue06f \ue075 \ue074 \ue020 \ue070 \ue072 \ue065 \ue074 \ue072 \ue061 \ue069 \ue06e \ue069 \ue06e \ue067\nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdi\ufb00erent\nneuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one\nmappingsfromparametervectorstofunctions),withdi\ufb00erentrandominitializations\nandwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadi\ufb00erent\nneuralnetwork,ataparticulartimeduringitstrainingprocess.This\ufb01gureisadapted\nwithpermissionfrom ().Acoordinateinfunctionspaceisanin\ufb01nite- Erhan e t a l .2010\ndimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010\nalinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeci\ufb01cx\npoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum\ne t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\n(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\novertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to\npointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\nusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.\nIsomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion\ncorrespondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\nhasreducedvariance.\n5 3 3", "CHAPTER15.REPRESENTATIONLEARNING\nreducingtestseterror.However,unsupervisedpretrainingcanhelptasksother\nthanclassi\ufb01cation,andcanacttoimproveoptimization ratherthanbeingmerely\naregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror\nfordeepautoencoders(HintonandSalakhutdinov2006,).\nErhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof\nunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements\ntotesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe\nparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\ntrainingisnon-determinis tic,andconvergestoadi\ufb00erentfunctioneverytimeit\nisrun.\u00a0Trainingmayhaltatapointwherethegradientbecomessmall,apoint\nwhereearlystoppingendstrainingtopreventover\ufb01tting,oratapointwherethe\ngradientislargebutitisdi\ufb03cultto\ufb01ndadownhillstepduetoproblemssuchas\nstochasticityorpoorconditioningoftheHessian.\u00a0Neuralnetworksthatreceive\nunsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\nwhileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\n\ufb01gureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1\nnetworksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\nestimationprocess,whichcaninturnreducetheriskofsevereover-\ufb01tting.In\notherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\naregionthattheydonotescape,andtheresultsfollowingthisinitialization are\nmoreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\nErhan2010etal.()alsoprovidesomeanswersastopretrainingworks when\nbest\u2014themeanandvarianceofthetesterrorweremostreducedbypretrainingfor\ndeepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\ninventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks\n(recti\ufb01edlinearunits,dropoutandbatchnormalization) solessisknownaboutthe\ne\ufb00ectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.\nAnimportantquestionishowunsupervisedpretrainingcanactasaregularizer.\nOnehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\nfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\nThisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\npretraining,andisdescribedfurtherinsection.15.3\nComparedtootherformsofunsupervisedlearning,unsupervisedpretraining\nhasthedisadvantagethatitoperateswithtwoseparatetrainingphases.\u00a0Many\nregularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\nstrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\nUnsupervisedpretrainingdoesnoto\ufb00eraclearwaytoadjustthethestrength\noftheregularization\u00a0arisi ngfromtheunsupervised\u00a0stage.Instead,\u00a0thereare\n5 3 4", "CHAPTER15.REPRESENTATIONLEARNING\nverymanyhyperparameters ,whosee\ufb00ectmaybemeasuredafterthefactbut\nisoftendi\ufb03culttopredictaheadoftime.Whenweperformunsupervisedand\nsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there\nisasinglehyperparameter,usuallyacoe\ufb03cientattachedtotheunsupervised\ncost,\u00a0thatdetermineshowstronglytheunsupervisedobjectivewillregularize\nthesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby\ndecreasingthiscoe\ufb03cient.Inthecaseofunsupervisedpretraining,thereisnota\nwayof\ufb02exiblyadaptingthestrengthoftheregularization\u2014either thesupervised\nmodelisinitializedtopretrainedparameters,oritisnot.\nAnotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\nhasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\nbepredictedduringthe\ufb01rstphase,sothereisalongdelaybetweenproposing\nhyperparametersforthe\ufb01rstphaseandbeingabletoupdatethemusingfeedback\nfromthesecondphase.Themostprincipledapproachistousevalidationseterror\ninthesupervisedphaseinordertoselectthehyperparameters ofthepretraining\nphase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009\nlikethenumberofpretrainingiterations,aremoreconvenientlysetduringthe\npretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis\nnotidealbutcomputationally muchcheaperthanusingthesupervisedobjective.\nToday,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe\n\ufb01eldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\none-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\nsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\nonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\nwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\nthenusethisrepresentationor\ufb01ne-tuneitforasupervisedtaskforwhichthe\ntrainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered\nbybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal.\n()andremainsincommonusetoday. 2011a\nDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\norbatchnormalization, areabletoachievehuman-levelperformanceonverymany\ntasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-\nperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and\nMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall\ndatasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform\nmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,\nthepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\npretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\n5 3 5", "CHAPTER15.REPRESENTATIONLEARNING\nandcontinuestoin\ufb02uencecontemporaryapproaches.Theideaofpretraininghas\nbeengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4\ncommonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\nispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\nnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparameters\nofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare\npublishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\n15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n\nTransferlearninganddomainadaptationrefertothesituationwherewhathasbeen\nlearnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization\ninanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe\nprevioussection,wherewetransferredrepresentationsbetweenanunsupervised\nlearningtaskandasupervisedlearningtask.\nIn t r ansf e r l e ar ni ng,thelearnermustperformtwoormoredi\ufb00erenttasks,\nbutweassumethatmanyofthefactorsthatexplainthevariationsinP 1are\nrelevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically\nunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\ntargetmaybeofadi\ufb00erentnature.Forexample,wemaylearnaboutonesetof\nvisualcategories,suchascatsanddogs,inthe\ufb01rstsetting,thenlearnabouta\ndi\ufb00erentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If\nthereissigni\ufb01cantlymoredatainthe\ufb01rstsetting(sampledfromP 1),thenthat\nmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly\nveryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions\nofedgesandvisualshapes,thee\ufb00ectsofgeometricchanges,changesinlighting,\netc.\u00a0Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7\nadaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures\nthatareusefulforthedi\ufb00erentsettingsortasks,correspondingtounderlying\nfactorsthatappearinmorethanonesetting.Thisisillustratedin\ufb01gure,with7.2\nsharedlowerlayersandtask-dependentupperlayers.\nHowever,\u00a0sometimes,\u00a0whatisshared\u00a0amongthe\u00a0di\ufb00erent\u00a0tasksisnotthe\nsemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\nrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,but\ntheearlierlayersneartheinputmayneedtorecognizeverydi\ufb00erentversionsof\nthesamephonemesorsub-phonemicvocalizationsdependingonwhichperson\nisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\n(neartheoutput)oftheneuralnetwork,andhaveatask-speci\ufb01cpreprocessing,as\n5 3 6", "CHAPTER15.REPRESENTATIONLEARNING\nillustratedin\ufb01gure.15.2\nSe l e c t i on\u00a0sw i t c h\nh(1)h(1)h(2)h(2)h(3)h(3)yy\nh(shared)h(shared)\nx(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )\nFigure15.2:\u00a0Example architectureformulti-taskortransferlearningwhentheoutput\nvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadi\ufb00erent y x \nmeaning(andpossiblyevenadi\ufb00erentdimension)foreachtask(or,forexample,each\nuser),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection\nswitch)aretask-speci\ufb01c,whiletheupperlevelsareshared.Thelowerlevelslearnto\ntranslatetheirtask-speci\ufb01cinputintoagenericsetoffeatures.\nIntherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-\noutputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\nisslightlydi\ufb00erent.Forexample,considerthetaskofsentimentanalysis,which\nconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\nCommentspostedonthewebcomefrommanycategories.Adomainadaptation\nscenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\nmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments\naboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine\nthatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\nneutralornegative,butofcoursethevocabularyandstylemayvaryfromone\ndomaintoanother,makingitmoredi\ufb03culttogeneralizeacrossdomains.Simple\nunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery\nsuccessfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\nArelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform\noftransferlearningduetogradualchangesinthedatadistributionovertime.\nBothconceptdriftandtransferlearningcanbeviewedasparticularformsof\n5 3 7", "CHAPTER15.REPRESENTATIONLEARNING\nmulti-tasklearning.Whilethephrase\u201cmulti-tasklearning\u201d\u00a0typicallyrefersto\nsupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\ntounsupervisedlearningandreinforcementlearningaswell.\nInallofthesecases,theobjectiveistotakeadvantageofdatafromthe\ufb01rst\nsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\ndirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation\nlearningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\nsamerepresentationinbothsettingsallowstherepresentationtobene\ufb01tfromthe\ntrainingdatathatisavailableforbothtasks.\nAsmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\nsuccessinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\netal.,).Inthe\ufb01rstofthesecompetitions,theexperimentalsetupisthe 2011\nfollowing.Eachparticipantis\ufb01rstgivenadatasetfromthe\ufb01rstsetting(from\ndistributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants\nmustusethistolearnagoodfeaturespace(mappingtherawinputtosome\nrepresentation),suchthatwhenweapplythislearnedtransformationtoinputs\nfromthetransfersetting(distributionP 2),alinearclassi\ufb01ercanbetrainedand\ngeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults\nfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand\ndeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected\ninthe\ufb01rstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond\n(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled\nexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\ngeneralization performance.\nTwoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t\nl e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample\nofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare\ngivenatallforthezero-shotlearningtask.\nOne-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\nlearnstocleanlyseparatetheunderlyingclassesduringthe\ufb01rststage.Duringthe\ntransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\npossibletestexamplesthatallclusteraroundthesamepointinrepresentation\nspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\ntheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned\nrepresentationspace,andwehavesomehowlearnedwhichfactorsdoanddonot\nmatterwhendiscriminatingobjectsofcertaincategories.\nAsanexampleofazero-shotlearningsetting,considertheproblemofhaving\nalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\n5 3 8", "CHAPTER15.REPRESENTATIONLEARNING\nItmaybepossibletorecognizeaspeci\ufb01cobjectclassevenwithouthavingseenan\nimageofthatobject,ifthetextdescribestheobjectwellenough.\u00a0Forexample,\nhavingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\nguessthatanimageisacat,withouthavingseenacatbefore.\nZero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\netal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation\nhasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\nasincludingthreerandomvariables:thetraditionalinputsx,thetraditional\noutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.\nThemodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where\nTisadescriptionofthetaskwewishthemodeltoperform.\u00a0Inourexampleof\nrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\nwithy= 1indicating\u201cyes\u201dandy= 0indicating\u201cno.\u201dThetaskvariableTthen\nrepresentsquestionstobeansweredsuchas\u201cIsthereacatinthisimage?\u201dIfwe\nhaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\nsamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.\nInourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\nimportantthatwehavehadunlabeledtextdatacontainingsentencessuchas\u201ccats\nhavefourlegs\u201dor\u201ccatshavepointyears.\u201d\nZero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort\nofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan\nobjectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b\nofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\nwitheachcategory.\nAsimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;\nMikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and\ntherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe\notherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith\nwordsintheother.Eventhoughwemaynothavelabeledexamplestranslating\nwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa\ntranslationforwordAbecausewehavelearnedadistributedrepresentationfor\nwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and\ncreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples\nconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe\nmostsuccessfulifallthreeingredients(thetworepresentationsandtherelations\nbetweenthem)arelearnedjointly.\nZero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\nexplainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation\n5 3 9", "CHAPTER15.REPRESENTATIONLEARNING\nh x = f x ( ) x\nx t e s t\ny t e s th y = f y ( ) y\ny \u2212 s pa ce\nR e l at i onshi p\u00a0 b e t w e e n \u00a0 e m be dde d\u00a0 p oi n t s \u00a0 w i t hi n\u00a0 one \u00a0 o f \u00a0 t h e \u00a0 d o m a i n s\nMaps\u00a0be t w e e n \u00a0 r e p r e s e n t at i on\u00a0spac e s \u00a0f x\nf y\nx \u2212 s pa ce\n( ) pa i r s i n t he t r a i ni ng s et x y ,\nf x : enco der f unctio n f o r x\nf y : enco der f unctio n f o r y\nFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.\nLabeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand\nsimilarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions\nappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\napplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints\ninxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance\ninh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both\nofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled\nexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way\nortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe\nrepresentationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning\nisthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno\nimageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)\nandimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween\nrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwerenever\npaired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach\nother.FigureinspiredfromsuggestionbyHrantKhachatrian.\n5 4 0", "CHAPTER15.REPRESENTATIONLEARNING\ninonemodality,arepresentationintheother,andtherelationship(ingeneralajoint\ndistribution)betweenpairs (xy,)consistingofoneobservationxinonemodality\nandanotherobservationyintheothermodality(SrivastavaandSalakhutdino v,\n2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from\nytoitsrepresentation,andtherelationshipbetweenthetworepresentations),\nconceptsinonerepresentationareanchoredintheother,andvice-versa,allowing\nonetomeaningfully\u00a0generalizeto\u00a0newpairs.Theprocedureis\u00a0illustratedin\n\ufb01gure.15.3\n15. 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors\nAnimportantquestionaboutrepresentationlearningis\u201cwhatmakesonerepre-\nsentationbetterthananother?\u201dOnehypothesisisthatanidealrepresentation\nisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-\nlyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature\nspacecorrespondingtodi\ufb00erentcauses,sothattherepresentationdisentanglesthe\ncausesfromoneanother.Thishypothesismotivatesapproachesinwhichwe\ufb01rst\nseekagoodrepresentationforp(x).\u00a0Sucharepresentationmayalsobeagood\nrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesof\nx.\u00a0Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\nthe1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail.\nForotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure\nsupervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006\nInotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\nwitharepresentationthatiseasytomodel\u2014forexample,onewhoseentriesare\nsparse,orindependentfromeachother.Arepresentationthatcleanlyseparates\ntheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.\nHowever,afurtherpartofthehypothesismotivatingsemi-supervisedlearning\nviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo\npropertiescoincide:\u00a0once weareabletoobtaintheunderlyingexplanationsfor\nwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\ntheothers.Speci\ufb01cally,ifarepresentationhrepresentsmanyoftheunderlying\ncausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,\nthenitiseasytopredictfrom.yh\nFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\nlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase\nwherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly,\nobservingatrainingsetofvaluesalonegivesusnoinformationabout. x p( )y x|\n5 4 1", "CHAPTER15.REPRESENTATIONLEARNING\nxp x ( )y = 1 y = 2 y = 3\nFigure15.4:Exampleofadensityoverxthatisamixtureoverthree\u00a0components.\nThecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture\ncomponents(e.g.,\u00a0naturalobjectclassesinimagedata)arestatisticallysalient,just\nmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor\ny.\nNext,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\nConsiderthesituationwhere xarisesfromamixture,withonemixturecomponent\npervalueofy,asillustratedin\ufb01gure.\u00a0Ifthemixturecomponentsarewell- 15.4\nseparated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda\nsinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).\nButmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x\nIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and\np(yx|)will\u00a0bestronglytied,\u00a0andunsupervisedrepresentationlearningthat\ntriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa\nsemi-supervisedlearningstrategy.\nConsidertheassumptionthatyisoneofthecausalfactorsofx,andlet\nhrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas\nstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\np,pp. (hx) = ( )xh|()h (15.1)\nAsaconsequence,thedatahasmarginalprobability\np() = x E hp. ( )xh| (15.2)\nFromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\nofx(fromageneralization pointofview)istheonethatuncoverstheabove\u201ctrue\u201d\n5 4 2", "CHAPTER15.REPRESENTATIONLEARNING\nstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.\nThe\u201cideal\u201drepresentationlearningdiscussedaboveshouldthusrecovertheselatent\nfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe\nveryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe\nconditionaldistributionofygivenxistiedbyBayes\u2019ruletothecomponentsin\ntheaboveequation:\np( ) = yx|pp ( )xy|()y\np()x. (15.3)\nThusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge\nofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\nsituationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\nperformance.\nAnimportantresearchproblemregardsthefactthatmostobservationsare\nformedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but\ntheunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor\nanunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\nsalientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking\niteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y\nInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible\ntocaptureallormostofthefactorsofvariationthatin\ufb02uenceanobservation.\nForexample,inavisualscene,shouldtherepresentationalwaysencodeallof\nthesmallestobjectsinthebackground? Itisawell-documented psychological\nphenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat\narenotimmediately relevanttothetasktheyareperforming\u2014see,e.g.,Simons\nandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\ndetermining toencodeineachsituation.Currently,twoofthemainstrategies what\nfordealingwithalargenumberofunderlyingcausesaretouseasupervised\nlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\nmodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch\nlargerrepresentationsifusingpurelyunsupervisedlearning.\nAnemergingstrategyforunsupervisedlearningistomodifythede\ufb01nitionof\nwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\nmodelshavebeentrainedtooptimizea\ufb01xedcriterion,oftensimilartomean\nsquarederror.These\ufb01xedcriteriadeterminewhichcausesareconsideredsalient.\nForexample,meansquarederrorappliedtothepixelsofanimageimplicitly\nspeci\ufb01esthatanunderlyingcauseisonlysalientifitsigni\ufb01cantlychangesthe\nbrightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\ntosolveinvolvesinteractingwithsmallobjects.See\ufb01gureforanexample 15.5\n5 4 3", "CHAPTER15.REPRESENTATIONLEARNING\nInput Reconstruction\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\nfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits\nspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\narerelevanttotheroboticstask.\u00a0Unfortunately,theautoencoderhaslimitedcapacity,\nandthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\nsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.\nofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\npingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\nobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\nOtherde\ufb01nitionsofsaliencearepossible.Forexample,ifagroupofpixels\nfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\nbrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\nOnewaytoimplementsuchade\ufb01nitionofsalienceistousearecentlydeveloped\napproachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c\nInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassi\ufb01er.\nThefeedforwardclassi\ufb01erattemptstorecognizeallsamplesfromthegenerative\nmodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis\nframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis\nhighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail\ninsection.Forthepurposesofthepresentdiscussion,itissu\ufb03cientto 20.10.4\nunderstandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015\nshowedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect\ntogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully\ngeneratetheearswhentrainedwiththeadversarialframework.Becausethe\nearsarenotextremelybrightordarkcomparedtothesurroundingskin,they\narenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly\n5 4 4", "CHAPTER15.REPRESENTATIONLEARNING\nGroundTruth MSE Adversarial\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\nlearningwhichfeaturesaresalient.\u00a0Inthisexample,thepredictivegenerativenetwork\nhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeci\ufb01c\nviewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould\nemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )\nsquarederroralone.Becausetheearsdonotcauseanextremedi\ufb00erenceinbrightness\ncomparedtotheneighboringskin,theywerenotsu\ufb03cientlysalientforthemodeltolearn\ntorepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof\nmeansquarederrorandadversarialloss.\u00a0Usingthislearnedcostfunction,theearsare\nsalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\nimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\ngraciouslyprovidedby (). Lotter e t a l .2015\nrecognizableshapeandconsistentpositionmeansthatafeedforwardnetwork\ncaneasilylearntodetectthem,makingthemhighlysalientunderthegenerative\nadversarialframework.See\ufb01gureforexampleimages.Generativeadversarial 15.6\nnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.\nWeexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich\nfactorstorepresent,anddevelopmechanismsforrepresentingdi\ufb00erentfactors\ndependingonthetask.\nAbene\ufb01toflearningtheunderlyingcausalfactors,aspointedoutbySch\u00f6lkopf\netal.(),isthatifthetruegenerativeprocesshas 2012 xasane\ufb00ectandyas\nacause,thenmodelingp(x y|)isrobusttochangesinp(y).\u00a0Ifthecause-e\ufb00ect\nrelationshipwasreversed,thiswouldnotbetrue,sincebyBayes\u2019rule,p(x y|)\nwouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin\ndistributionduetodi\ufb00erentdomains,temporalnon-stationarity,orchangesin\nthenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe\nuniverseareconstant)whilethemarginaldistributionovertheunderlyingcauses\ncanchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan\n5 4 5", "CHAPTER15.REPRESENTATIONLEARNING\nbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\nfactorsand. h p( )xh|\n15. 4 D i s t ri b u t ed R ep res en t at i on\nDistributedrepresentationsofconcepts\u2014representationscomposedofmanyele-\nmentsthatcanbesetseparatelyfromeachother\u2014areoneofthemostimportant\ntoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\ntheycanusenfeatureswithkvaluestodescribekndi\ufb00erentconcepts.Aswe\nhaveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits\nandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof\ndistributedrepresentation.\u00a0Wenowintroduceanadditionalobservation.\u00a0Many\ndeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\ncanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\ndiscussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\nbecauseeachdirectioninrepresentationspacecancorrespondtothevalueofa\ndi\ufb00erentunderlyingcon\ufb01gurationvariable.\nAnexampleofadistributedrepresentationisavectorofnbinaryfeatures,\nwhichcantake2ncon\ufb01gurations, eachpotentiallycorrespondingtoadi\ufb00erent\nregionininputspace,asillustratedin\ufb01gure.Thiscanbecomparedwith 15.7\nasymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\ncategory.Iftherearensymbolsinthedictionary,onecanimaginenfeature\ndetectors,eachcorrespondingtothedetectionofthepresenceoftheassociated\ncategory.Inthatcaseonlyndi\ufb00erentcon\ufb01gurations oftherepresentationspace\narepossible,carvingndi\ufb00erentregionsininputspace,asillustratedin\ufb01gure.15.8\nSuchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\nbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone\nofthemcanbeactive).Asymbolicrepresentationisaspeci\ufb01cexampleofthe\nbroaderclassofnon-distributedrepresentations,whicharerepresentationsthat\nmaycontainmanyentriesbutwithoutsigni\ufb01cantmeaningfulseparatecontrolover\neachentry.\nExamplesoflearningalgorithms\u00a0basedonnon-distributedrepresentations\ninclude:\n\u2022Clusteringmethods,includingthek-meansalgorithm:eachinputpointis\nassignedtoexactlyonecluster.\n\u2022k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\nareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple\n5 4 6", "CHAPTER15.REPRESENTATIONLEARNING\nh 1h 2 h 3\nh = [ 1 , , 1 1 ]\ue021\nh = [ 0 , , 1 1 ]\ue021h = [ 1 , , 0 1 ]\ue021h = [ 1 , , 1 0 ]\ue021\nh = [ 0 , , 1 0 ]\ue021h = [ 0 , , 0 1 ]\ue021h = [ 1 , , 0 0 ]\ue021\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\nbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\nh 1,h 2,andh 3.\u00a0Eachfeatureisde\ufb01nedbythresholdingtheoutputofalearned,linear\ntransformation.Eachfeaturedivides R2intotwohalf-planes.Leth+\nibethesetofinput\npointsforwhichh i=1andh\u2212\nibethesetofinputpointsforwhichh i=0.Inthis\nillustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding\narrowpointingtotheh+\nisideoftheboundary.Therepresentationasawholetakes\nonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\nrepresentationvalue[1,1,1]\ue03ecorrespondstotheregionh+\n1\u2229h+\n2\u2229h+\n3.Comparethistothe\nnon-distributedrepresentationsin\ufb01gure.Inthegeneralcaseof 15.8 dinputdimensions,\nadistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes.\nThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)di\ufb00erent\nregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly\nnregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\nmoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible\n(thereisnoh=0inthisexample)andthatalinearclassi\ufb01erontopofthedistributed\nrepresentationisnotabletoassigndi\ufb00erentclassidentitiestoeveryneighboringregion;\nevenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew\nisthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\nlayerandaweakclassi\ufb01erlayercanbeastrongregularizer;aclassi\ufb01ertryingtolearn\ntheconceptof\u201cperson\u201dversus\u201cnotaperson\u201ddoesnotneedtoassignadi\ufb00erentclassto\naninputrepresentedas\u201cwomanwithglasses\u201dthanitassignstoaninputrepresentedas\n\u201cmanwithoutglasses.\u201dThiscapacityconstraintencourageseachclassi\ufb01ertofocusonfew\nh iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h\n5 4 7", "CHAPTER15.REPRESENTATIONLEARNING\nvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom\neachother,sothisdoesnotqualifyasatruedistributedrepresentation.\n\u2022Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\nactivatedwhenaninputisgiven.\n\u2022Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\nexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest\nneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but\nthosevaluescannotreadilybecontrolledseparatelyfromeachother.\n\u2022KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\nalthoughthedegreeofactivationofeach\u201csupportvector\u201dortemplateexample\nisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\n\u2022Languageortranslationmodelsbasedonn-grams.Thesetofcontexts\n(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsu\ufb03xes.\nAleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample.\nSeparateparametersareestimatedforeachleafofthetree(withsomesharing\nbeingpossible).\nForsomeofthesenon-distributedalgorithms,theoutputisnotconstantby\npartsbutinsteadinterpolatesbetweenneighboringregions.Therelationship\nbetweenthenumberofparameters(orexamples)andthenumberofregionsthey\ncande\ufb01neremainslinear.\nAnimportantrelatedconceptthatdistinguishesadistributedrepresentation\nfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween\ndi\ufb00erentconcepts.Aspuresymbols,\u201ccat\u201dand\u201cdog\u201dareasfarfromeachother\nasanyothertwosymbols.However,ifoneassociatesthemwithameaningful\ndistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\ncangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation\nmaycontainentriessuchas\u201chas_fur\u201dor\u201cnumber_of_legs\u201dthathavethesame\nvaluefortheembeddingofboth\u201ccat\u201dand\u201cdog.\u201dNeurallanguagemodelsthat\noperateondistributedrepresentationsofwordsgeneralizemuchbetterthanother\nmodelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\nsection.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\nsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\nabsentfrompurelysymbolicrepresentations.\nWhenandwhycantherebeastatisticaladvantagefromusingadistributed\nrepresentationaspartofalearningalgorithm?\u00a0D istributedrepresentationscan\n5 4 8", "CHAPTER15.REPRESENTATIONLEARNING\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintodi\ufb00erentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\nalgorithmbasedonanon-distributedrepresentation.Di\ufb00erentnon-distributedalgorithms\nmayhavedi\ufb00erentgeometry,\u00a0but\u00a0theytypicallybreaktheinput\u00a0spaceintoregions,\nw i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed\napproachisthat,givenenoughparameters,itcan\ufb01tthetrainingsetwithoutsolvinga\ndi\ufb03cultoptimizationalgorithm,becauseitisstraightforwardtochooseadi\ufb00erentoutput\ni n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels\ngeneralizeonlylocallyviathesmoothnessprior,makingitdi\ufb03culttolearnacomplicated\nfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\nthiswithadistributedrepresentation,\ufb01gure.15.7\n5 4 9", "CHAPTER15.REPRESENTATIONLEARNING\nhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe\ncompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-\ndistributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,\nwhichstatesthatifuv\u2248,thenthetargetfunctionftobelearnedhasthe\npropertythatf(u)\u2248f(v),ingeneral.Therearemanywaysofformalizingsuchan\nassumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe\nknowthatf(x)\u2248y,thenwechooseanestimator \u02c6fthatapproximatelysatis\ufb01es\ntheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby\ninputx+\ue00f.Thisassumptionisclearlyveryuseful,butitsu\ufb00ersfromthecurseof\ndimensionality:\u00a0inordertolearnatargetfunctionthatincreasesanddecreases\nmanytimesinmanydi\ufb00erentregions,1wemayneedanumberofexamplesthatis\natleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof\ntheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor\neachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol\ntovalue.\u00a0However,thisdoesnotallowustogeneralizetonewsymbolsfornew\nregions.\nIfwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\nsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean\nobjectregardlessofitslocationintheimage,eventhoughspatialtranslationof\ntheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\nLetusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,\nthatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\nbinaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces,\u00a0as\nillustratedin\ufb01gure.Theexponentiallylargenumberofintersectionsof 15.7 n\nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\nrepresentationlearnercandistinguish.Howmanyregionsaregeneratedbyan\narrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe\nintersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)\nthatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\nd\ue058\nj = 0\ue012n\nj\ue013\n= (Ond). (15.4)\nTherefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\nthenumberofhiddenunits.\n1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y\nre g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i \ufb00 e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e\nm i g h t wa n t t o d i \ufb00 e r i n f 2dd i \ufb00 e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s .\n5 5 0", "CHAPTER15.REPRESENTATIONLEARNING\nThisprovidesageometricargumenttoexplainthegeneralization powerof\ndistributedrepresentation:withO(nd)parameters(fornlinear-threshold features\nin Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade\nnoassumptionatallaboutthedata,andusedarepresentationwithoneunique\nsymbolforeachregion,andseparateparametersforeachsymboltorecognizeits\ncorrespondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)\nexamples.Moregenerally,theargumentinfavorofthedistributedrepresentation\ncouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\nusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\nthedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\ntransformationwithkparameterscanlearnaboutrregionsininputspace,with\nkr\ue01c,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\nwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed\nsettingwherewewouldneedO(r)examplestoobtainthesamefeaturesand\nassociatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto\nrepresentthemodelmeansthatwehavefewerparametersto\ufb01t,andthusrequire\nfarfewertrainingexamplestogeneralizewell.\nAfurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\ntationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\ndistinctlyencodesomanydi\ufb00erentregions.Forexample,theVCdimensionofa\nneuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber\nofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\nmanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode\nspace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\nhtotheoutputyusingalinearclassi\ufb01er.Theuseofadistributedrepresentation\ncombinedwithalinearclassi\ufb01erthusexpressesapriorbeliefthattheclassesto\nberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors\ncapturedbyh.\u00a0Wewilltypicallywanttolearncategoriessuchasthesetofall\nimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\nrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition\nthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\ngreencarsandredtrucksasanotherclass.\nTheideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\nvalidated. ()\ufb01ndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015\ntrainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery\nofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\nInpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\nthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\ntoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\n5 5 1", "CHAPTER15.REPRESENTATIONLEARNING\n-+ =\nFigure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\ntheconceptofgenderfromtheconceptofwearingglasses.\u00a0Ifwebeginwiththerepre-\nsentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\nconceptofamanwithoutglasses,and\ufb01nallyaddthevectorrepresentingtheconcept\nofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\nwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto\nimagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\npermissionfrom (). Radford e t a l .2015\ncommonisthatonecouldimagine learningabouteachofthemwithouthavingto\nseeallthecon\ufb01gurationsofalltheothers. ()demonstratedthat Radfordetal.2015\nagenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\ndirectionsinrepresentationspacecapturingdi\ufb00erentunderlyingfactorsofvariation.\nFiguredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9\ntowhetherthepersonismaleorfemale,whileanothercorrespondstowhether\nthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not\n\ufb01xedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassi\ufb01ers:\ngradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\ninterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout\nthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\nglasses,withouthavingtocharacterizeallofthecon\ufb01gurations ofthen\u22121other\nfeaturesbyexamplescoveringallofthesecombinationsofvalues.\u00a0Thisformof\nstatisticalseparabilityiswhatallowsonetogeneralizetonewcon\ufb01gurations ofa\nperson\u2019sfeaturesthathaveneverbeenseenduringtraining.\n5 5 2", "CHAPTER15.REPRESENTATIONLEARNING\n15. 5 E x p on en t i al Gai n s f rom D ep t h\nWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\ntors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\nnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\nimprovedstatisticale\ufb03ciency.Inthissection,wedescribehowsimilarresultsapply\nmoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\nInsection,wesawanexampleofagenerativemodelthatlearnedabout 15.4\ntheexplanatoryfactorsunderlyingimagesoffaces,includingtheperson\u2019sgender\nandwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\nthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\nashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\nbetweentheseabstractexplanatoryfactorsandthepixelsintheimage.\u00a0Inthis\nandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\neachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery\nhigh-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\ndemands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas\nfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\nthecompositionofmanynonlinearities.\nIthasbeenproveninmanydi\ufb00erentsettingsthatorganizingcomputation\nthroughthecompositionofmanynonlinearities andahierarchyofreusedfeatures\ncangiveanexponentialboosttostatisticale\ufb03ciency,ontopoftheexponential\nboostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\nwithsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with\nasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\nfamilythatisauniversalapproximator canapproximatealargeclassoffunctions\n(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough\nhiddenunits.\u00a0However,therequirednumberofhiddenunitsmaybeverylarge.\nTheoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat\ntherearefamiliesoffunctionsthatcanberepresentede\ufb03cientlybyanarchitecture\nofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect\ntotheinputsize)withinsu\ufb03cientdepth(depth2ordepth).k\u22121\nInsection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\napproximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\nhiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\nbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux\nandBengio20082010Mont\u00fafarandAy2011Mont\u00fafar2014Krause ,,; ,;,; etal.,\n2013).\n5 5 3", "CHAPTER15.REPRESENTATIONLEARNING\nInsection,wesawthatasu\ufb03cientlydeepfeedforwardnetworkcanhave 6.4.1\nanexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\nbeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\nmodelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These\nmodelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\nsetofrandomvariables. ()showedthatthereexist DelalleauandBengio2011\nprobabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\nneedinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014\nshowedthattherearesigni\ufb01cantdi\ufb00erencesbetweeneverytwo\ufb01nitedepthsof\nSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\ntheirrepresentationalpower.\nAnotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\npoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan\nexponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\ntoonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal.\n2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\ncasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\n15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es\nToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\nrepresentationbetterthananother?Oneanswer,\ufb01rstintroducedinsection,is15.3\nthatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\nvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\napplications.Moststrategiesforrepresentationlearningarebasedonintroducing\ncluesthathelpthelearningto\ufb01ndtheseunderlyingfactorsofvariations.Theclues\ncanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\nlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually\nspeci\ufb01esthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\ntomakeuseofabundantunlabeleddata,representationlearningmakesuseof\nother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof\nimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\nordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\nregularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis\nimpossibleto\ufb01ndauniversallysuperiorregularizationstrategy,onegoalofdeep\nlearningisto\ufb01ndasetoffairlygenericregularizationstrategiesthatareapplicable\ntoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\ntosolve.\n5 5 4", "CHAPTER15.REPRESENTATIONLEARNING\nWeprovideherealistofthesegenericregularizationstrategies.Thelistis\nclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning\nalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying\nfactors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d\nbeenpartiallyexpandedhere.\n\u2022Smoothness:Thisistheassumptionthatf(x+\ue00fd)\u2248f(x)forunitdand\nsmall\ue00f.Thisassumptionallowsthelearnertogeneralizefromtraining\nexamplestonearbypointsininputspace.Manymachinelearningalgorithms\nleveragethisidea,butitisinsu\ufb03cienttoovercomethecurseofdimensionality.\n\u2022Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\nvariablesarelinear.Thisallowsthealgorithmtomakepredictionseven\nveryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\npredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\nsmoothnessassumptioninsteadmakethelinearityassumption.Theseare\ninfactdi\ufb00erentassumptions\u2014linearfunctionswithlargeweightsapplied\ntohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\n()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\n\u2022Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\nmotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\nexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\nofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3\nsupervisedlearningviarepresentationlearning.Learningthestructureofp(x)\nrequireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|\nx)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\ndescribeshowthisviewmotivatestheuseofdistributedrepresentations,with\nseparatedirectionsinrepresentationspacecorrespondingtoseparatefactors\nofvariation.\n\u2022Causalfactors:themodelisconstructedinsuchawaythatittreatsthe\nfactorsofvariationdescribedbythelearnedrepresentationhasthecauses\noftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3\nisadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\nmorerobustwhenthedistributionovertheunderlyingcauseschangesor\nwhenweusethemodelforanewtask.\n\u2022Depthahierarchical\u00a0organization\u00a0ofexplanatory\u00a0factors ,\u00a0or\u00a0 :High-level,\nabstractconceptscanbede\ufb01nedintermsofsimpleconcepts,forminga\nhierarchy.From\u00a0another\u00a0point of\u00a0view,\u00a0the\u00a0us e\u00a0ofa\u00a0deeparchitecture\n5 5 5", "CHAPTER15.REPRESENTATIONLEARNING\nexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step\nprogram,\u00a0with eachstep\u00a0referringbacktothe\u00a0outputoftheprocessing\naccomplishedviaprevioussteps.\n\u2022Sharedfactors\u00a0across\u00a0tasks:In\u00a0thecontextwherewehavemanytasks,\ncorrespondingtodi\ufb00erentyivariablessharingthesameinput xorwhere\neachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput\nx,theassumptionisthateachyiisassociatedwithadi\ufb00erentsubsetfroma\ncommonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning\nalltheP(yi|x)viaasharedintermediate representationP(h x|)allows\nsharingofstatisticalstrengthbetweenthetasks.\n\u2022Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\ncentratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\ncase,theseregionscanbeapproximatedbylow-dimensional manifoldswith\namuchsmallerdimensionalitythantheoriginalspacewherethedatalives.\nManymachinelearningalgorithmsbehavesensiblyonlyonthismanifold\n( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b\nautoencoders,attempttoexplicitlylearnthestructureofthemanifold.\n\u2022Naturalclustering:Manymachinelearningalgorithmsassumethateach\nconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\ndatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\nwithineachoneofthese.\u00a0Thisassumptionmotivatesavarietyoflearning\nalgorithms,includingtangentpropagation, doublebackprop,themanifold\ntangentclassi\ufb01erandadversarialtraining.\n\u2022Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\nmaketheassumptionthatthemostimportantexplanatoryfactorschange\nslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\nexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\nSeesectionforfurtherdescriptionofthisapproach. 13.3\n\u2022Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\ninputs\u2014thereisnoneedtouseafeaturethatdetectselephanttrunkswhen\nrepresentinganimageofacat.Itisthereforereasonabletoimposeaprior\nthatanyfeaturethatcanbeinterpretedas\u201cpresent\u201dor\u201cabsent\u201dshouldbe\nabsentmostofthetime.\n\u2022SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the\nfactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\n5 5 6", "CHAPTER15.REPRESENTATIONLEARNING\npossibleismarginalindependence,P(h) =\ue051\niP(h i),butlineardependencies\northosecapturedbyashallowautoencoderarealsoreasonableassumptions.\nThiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga\nlinearpredictororafactorizedpriorontopofalearnedrepresentation.\nTheconceptofrepresentationlearningtiestogetherallofthemanyforms\nofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep\nprobabilisticmodelsalllearnandexploitrepresentations.Learning\u00a0thebest\npossiblerepresentationremainsanexcitingavenueofresearch.\n5 5 7", "C h a p t e r 1 6\nS t ru ct u r e d Probabilis t i c Mo d e l s\nf or D e e p L e ar n i n g\nDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto\nguidetheirdesigne\ufb00ortsanddescribetheiralgorithms.Oneoftheseformalisms\nistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed\nstructuredprobabilisticmodelsbrie\ufb02yinsection.Thatbriefpresentationwas 3.14\nsu\ufb03cienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto\ndescribesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III\nmodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep\nlearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes\nstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended\ntobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction\nbeforecontinuingwiththischapter.\nAstructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,\nusingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\ninteractwitheachotherdirectly.Hereweuse\u201cgraph\u201dinthegraphtheorysense\u2014a\nsetofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\nofthemodelisde\ufb01nedbyagraph,thesemodelsareoftenalsoreferredtoas\ngraphicalmodels.\nThegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\ndi\ufb00erentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\nprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,\nwithanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning\nresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,\nyoumaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert\n558", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nmaybene\ufb01tfromreadingthe\ufb01nalsectionofthischapter,section,inwhichwe 16.7\nhighlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning\nalgorithms.Deeplearningpractitioners tendtouseverydi\ufb00erentmodelstructures,\nlearningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest\nofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese\ndi\ufb00erencesinpreferencesandexplainthereasonsforthem.\nInthischapterwe\ufb01rstdescribethechallengesofbuildinglarge-scaleproba-\nbilisticmodels.\u00a0Next,wedescribehowtouseagraphtodescribethestructure\nofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany\nchallenges,itisnotwithoutitsowncomplications. Oneofthemajordi\ufb03cultiesin\ngraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract\ndirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem.\u00a0We\noutlinetwoapproachestoresolvingthisdi\ufb03cultybylearningaboutthedependen-\nciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5\ndeeplearningpractitioners placeonspeci\ufb01capproachestographicalmodelingin\nsection.16.7\n16.1TheChallengeofUnstructuredModeling\nThegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\nneededtosolvearti\ufb01cialintelligence.Thismeansbeingabletounderstandhigh-\ndimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\nbeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and\ndocumentscontainingmultiplewordsandpunctuationcharacters.\nClassi\ufb01cationalgorithmscantakeaninputfromsucharichhigh-dimensional\ndistributionandsummarizeitwithacategoricallabel\u2014whatobjectisinaphoto,\nwhatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\nofclassi\ufb01cationdiscardsmostoftheinformationintheinputandproducesa\nsingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\nclassi\ufb01erisalsooftenabletoignoremanypartsoftheinput.Forexample,when\nrecognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof\nthephoto.\nItispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\noftenmoreexpensivethanclassi\ufb01cation.Someofthemrequireproducingmultiple\noutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\n1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry\ne n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c .\n5 5 9", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\n\u2022Densityestimation:givenaninput x,themachinelearningsystemreturns\nanestimateofthetruedensity p( x)underthedatageneratingdistribution.\nThisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-\ningoftheentireinput.Ifevenoneelementofthevectorisunusual,the\nsystemmustassignitalowprobability.\n\u2022Denoising:givenadamagedorincorrectlyobservedinput \u02dc x,themachine\nlearningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,\nthemachinelearningsystemmightbeaskedtoremovedustorscratches\nfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\nestimatedcleanexample x)andanunderstandingoftheentireinput(since\nevenonedamagedareawillstillrevealthe\ufb01nalestimateasbeingdamaged).\n\u2022Missingvalueimputation:giventheobservationsofsomeelementsof x,\nthemodelisaskedtoreturnestimatesoforaprobabilitydistributionover\nsomeoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs.\nBecausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it\nmustunderstandtheentireinput.\n\u2022Sampling:themodelgeneratesnewsamplesfromthedistribution p( x).\nApplicationsincludespeechsynthesis,i.e.producingnewwaveformsthat\nsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\ngoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\nfromthewrongdistribution,thenthesamplingprocessiswrong.\nForanexampleofasamplingtaskusingsmallnaturalimages,see\ufb01gure.16.1\nModelingarichdistributionoverthousandsormillionsofrandomvariablesisa\nchallengingtask,bothcomputationally andstatistically.Supposeweonlywanted\ntomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\nseemsoverwhelming.Forasmall, 32\u00d732 2 pixelcolor(RGB)image,thereare3 0 7 2\npossiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan\ntheestimatednumberofatomsintheuniverse.\nIngeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining\nndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof\nrepresenting P(x)bystoringalookuptablewithoneprobabilityvalueperpossible\noutcomerequires knparameters!\nThisisnotfeasibleforseveralreasons:\n5 6 0", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example32\u00d732pixelcolor\nimagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )\ndrawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\natthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\nspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,\nratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\nadjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011\n5 6 1", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n\u2022 M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues\nof nand k,representingthedistributionasatablewillrequiretoomany\nvaluestostore.\n\u2022 St a t i s t i c a l e \ufb03 c i e nc y:Asthenumberofparametersinamodelincreases,\nsodoestheamountoftrainingdataneededtochoosethevaluesofthose\nparametersusingastatisticalestimator.Becausethetable-basedmodel\nhasanastronomicalnumberofparameters,itwillrequireanastronomically\nlargetrainingsetto\ufb01taccurately.Anysuchmodelwillover\ufb01tthetraining\nsetverybadlyunlessadditionalassumptionsaremadelinkingthedi\ufb00erent\nentriesinthetable(forexample,likeinback-o\ufb00orsmoothed n-grammodels,\nsection).12.4.1\n\u2022 R u nt i m e : \u00a0 t h e c o s t o f i nfe r e nc e:\u00a0Supposewewanttoperformaninference\ntaskwhereweuseourmodelofthejointdistribution P(x)tocomputesome\notherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional\ndistribution P(x 2|x 1).Computingthesedistributionswillrequiresumming\nacrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\nintractablememorycostofstoringthemodel.\n\u2022 R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample\nfromthemodel.Thenaivewaytodothisistosamplesomevalueu\u223c U(0 ,1),\ntheniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\nexceed uandreturntheoutcomecorrespondingtothatpositioninthetable.\nThisrequiresreadingthroughthewholetableintheworstcase,soithas\nthesameexponentialcostastheotheroperations.\nTheproblemwiththetable-basedapproachisthatweareexplicitlymodeling\neverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\nprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\nUsually,mostvariablesin\ufb02uenceeachotheronlyindirectly.\nForexample,considermodelingthe\ufb01nishingtimesofateaminarelayrace.\nSupposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof\ntherace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\nherlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\nlapandhandsthebatontoCarol,whorunsthe\ufb01nallap.Wecanmodeleachof\ntheir\ufb01nishingtimesasacontinuousrandomvariable.Alice\u2019s\ufb01nishingtimedoes\nnotdependonanyoneelse\u2019s,sinceshegoes\ufb01rst.Bob\u2019s\ufb01nishingtimedepends\nonAlice\u2019s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\nhascompletedhers.\u00a0IfAlice\ufb01nishesfaster,Bobwill\ufb01nishfaster,allelsebeing\n5 6 2", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nequal.Finally,Carol\u2019s\ufb01nishingtimedependsonbothherteammates.IfAliceis\nslow,Bobwillprobably\ufb01nishlatetoo.Asaconsequence,Carolwillhavequitea\nlatestartingtimeandthusislikelytohavealate\ufb01nishingtimeaswell.However,\nCarol\u2019s\ufb01nishingtimedependsonly i ndir e c t l yonAlice\u2019s\ufb01nishingtimeviaBob\u2019s.\nIfwealreadyknowBob\u2019s\ufb01nishingtime,wewillnotbeabletoestimateCarol\u2019s\n\ufb01nishingtimebetterby\ufb01ndingoutwhatAlice\u2019s\ufb01nishingtimewas.Thismeans\nwecanmodeltherelayraceusingonlytwointeractions: Alice\u2019se\ufb00ectonBoband\nBob\u2019se\ufb00ectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\nandCarolfromourmodel.\nStructuredprobabilisticmodelsprovideaformalframeworkformodelingonly\ndirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\nsigni\ufb01cantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\nThesesmallermodelsalsohavedramatically reducedcomputational costinterms\nofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\nthemodel.\n16.2UsingGraphstoDescribeModelStructure\nStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof\u201cnodes\u201dor\n\u201cvertices\u201dconnectedbyedges)torepresentinteractionsbetweenrandomvariables.\nEachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\nThesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\ninteractionsneedtobeexplicitlymodeled.\nThereismore\u00a0thanone\u00a0wayto\u00a0describe\u00a0theinteractionsin\u00a0aprobability\ndistributionusingagraph.Inthefollowingsectionswedescribesomeofthemost\npopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\ntwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\nundirectedgraphs.\n1 6 . 2 . 1 D i rect ed Mo d el s\nOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,\notherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,).\nDirectedgraphicalmodelsarecalled\u201cdirected\u201dbecausetheiredgesaredirected,\n2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm \u201c B a y e s i a n n e t wo rk \u201d wh e n o n e wis h e s t o \u201c e m p h a s i z e\nt h e j u d g m e n t a l \u201d n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y\nre p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s .\n5 6 3", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nt 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol\nFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice\u2019s\ufb01nishing\ntimet 0in\ufb02uencesBob\u2019s\ufb01nishingtimet 1,becauseBobdoesnotgettostartrunninguntil\nAlice\ufb01nishes.Likewise,CarolonlygetstostartrunningafterBob\ufb01nishes,soBob\u2019s\n\ufb01nishingtimet 1directlyin\ufb02uencesCarol\u2019s\ufb01nishingtimet 2.\nthatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin\nthedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable\u2019s\nprobabilitydistributionisde\ufb01nedintermsoftheother\u2019s.Drawinganarrowfrom\natobmeansthatwede\ufb01netheprobabilitydistributionoverbviaaconditional\ndistribution,withaasoneofthevariablesontherightsideoftheconditioning\nbar.Inotherwords,thedistributionoverbdependsonthevalueofa.\nContinuingwiththerelayraceexamplefromsection,supposewename 16.1\nAlice\u2019s\ufb01nishingtimet 0,Bob\u2019s\ufb01nishingtimet 1,andCarol\u2019s\ufb01nishingtimet 2.\nAswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends\ndirectlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected\ngraphicalmodel,illustratedin\ufb01gure.16.2\nFormally,adirectedgraphicalmodelde\ufb01nedonvariables xisde\ufb01nedbya\ndirectedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,\nandasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where\nP aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven\nby\np() = \u03a0x i p(x i| P aG(x i)) . (16.1)\nInourrelayraceexample,thismeansthat,usingthegraphdrawnin\ufb01gure,16.2\np(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2)\nThisisour\ufb01rsttimeseeingastructuredprobabilisticmodelinaction.We\ncanexaminethecostofusingit,inordertoobservehowstructuredmodelinghas\nmanyadvantagesrelativetounstructuredmodeling.\nSupposewerepresentedtimebydiscretizingtimerangingfromminute0to\nminute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete\nvariablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha\ntable,itwouldneedtostore999,999values(100valuesoft 0\u00d7100valuesoft 1\u00d7\n100valuesoft 2,minus1,sincetheprobabilityofoneofthecon\ufb01gurations ismade\n5 6 4", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\nonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe\ndistributionovert 0requires99values,thetablede\ufb01ningt 1givent 0requires9900\nvalues,andsodoesthetablede\ufb01ningt 2givent 1.Thiscomestoatotalof19,899\nvalues.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\nparametersbyafactorofmorethan50!\nIngeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe\nsingletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose\nwebuildadirectedgraphicalmodeloverthesevariables.\u00a0If misthemaximum\nnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\nconditionalprobabilitydistribution,thenthecostofthetablesforthedirected\nmodelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we\ngetverydramaticsavings.\nInotherwords,solongaseachvariablehasfewparentsinthegraph,the\ndistributioncanberepresentedwithveryfewparameters.\u00a0Somerestrictionson\nthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\noperationslikecomputingmarginalorconditionaldistributionsoversubsetsof\nvariablesaree\ufb03cient.\nItisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\nthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables\nareconditionallyindependentfromeachother.Itisalsopossibletomakeother\nkindsofsimplifyingassumptions.\u00a0Forexample,supposeweassumeBobalways\nrunsthesameregardlessofhowAliceperformed.(Inreality,Alice\u2019sperformance\nprobablyin\ufb02uencesBob\u2019sperformance\u2014dependingonBob\u2019spersonality,ifAlice\nrunsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\nmatchherexceptionalperformance,oritmightmakehimovercon\ufb01dentandlazy).\nThentheonlye\ufb00ectAlicehasonBob\u2019s\ufb01nishingtimeisthatwemustaddAlice\u2019s\n\ufb01nishingtimetothetotalamountoftimewethinkBobneedstorun.This\nobservationallowsustode\ufb01neamodelwith O( k)parametersinsteadof O( k2).\nHowever,notethatt 0andt 1arestilldirectlydependentwiththisassumption,\nbecauset 1representstheabsolutetimeatwhichBob\ufb01nishes,notthetotaltime\nhehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom\nt 0tot 1.TheassumptionthatBob\u2019spersonalrunningtimeisindependentfrom\nallotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we\nencodethisinformationinthede\ufb01nitionoftheconditionaldistributionitself.The\nconditionaldistributionisnolongera k k\u00d7\u22121elementtableindexedbyt 0andt 1\nbutisnowaslightlymorecomplicatedformulausingonly k\u22121parameters.The\ndirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwede\ufb01ne\n5 6 5", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nourconditionaldistributions.Itonlyde\ufb01neswhichvariablestheyareallowedto\ntakeinasarguments.\n1 6 . 2 . 2 Un d i rec t ed Mo d el s\nDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\nticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise\nknownasMarkovrandom\ufb01elds(MRFs)orMarkovnetworks(Kinder-\nmann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\nareundirected.\nDirectedmodelsaremostnaturallyapplicabletosituationswherethereis\naclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\nsituationswhereweunderstandthecausalityandthecausalityonly\ufb02owsinone\ndirection.Onesuchsituationistherelayraceexample.Earlierrunnersa\ufb00ectthe\n\ufb01nishingtimesoflaterrunners;laterrunnersdonota\ufb00ectthe\ufb01nishingtimesof\nearlierrunners.\nNotallsituationswemightwanttomodelhavesuchacleardirectiontotheir\ninteractions.Whentheinteractionsseemtohavenointrinsicdirection,orto\noperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\nAsanexampleofsuchasituation,supposewewanttomodeladistribution\noverthreebinaryvariables:whetherornotyouaresick,whetherornotyour\ncoworkerissick,andwhetherornotyourroommateissick.Asintherelayrace\nexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\ntakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach\nother,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\ncolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\nit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\nthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\nacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe\ncoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\nroommate.\nInthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas\nitisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional\nnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\nAswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\nedge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\notherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\narrow,andisnotassociatedwithaconditionalprobabilitydistribution.\n5 6 6", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh r h r h y h y h c h c\nFigure16.3:Anundirectedgraphrepresentinghowyourroommate\u2019shealthh r,your\nhealthh y,andyourworkcolleague\u2019s healthh ca\ufb00ecteachother.Youandyourroommate\nmightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,\nbutassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan\nonlyinfecteachotherindirectlyviayou.\nWedenotetherandomvariablerepresentingyourhealthash y,therandom\nvariablerepresentingyourroommate\u2019shealthash r,andtherandomvariable\nrepresentingyourcolleague\u2019shealthash c.See\ufb01gureforadrawingofthe 16.3\ngraphrepresentingthisscenario.\nFormally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\nde\ufb01nedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor \u03c6(C)\n(alsocalledacliquepotential)\u00a0measuresthea\ufb03nityofthevariablesinthatclique\nforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe\nnon-negative.Togethertheyde\ufb01neanunnormalizedprobabilitydistribution\n\u02dc p() = \u03a0x C\u2208G \u03c6 .()C (16.3)\nTheunnormalized probabilitydistributionise\ufb03cienttoworkwithsolongas\nallthecliquesaresmall.Itencodestheideathatstateswithhighera\ufb03nityare\nmorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\nde\ufb01nitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\ntogetherwillyieldavalidprobabilitydistribution.See\ufb01gureforanexample 16.4\nofreadingfactorizationinformationfromanundirectedgraph.\nOurexampleofthecoldspreadingbetweenyou,yourroommate,andyour\ncolleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis\ncliquecanbede\ufb01nedbyatable,andmighthavevaluesresemblingthese:\nh y= 0h y= 1\nh c= 021\nh c= 1110\n3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f\nt h e g ra p h .\n5 6 7", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\n(havingbeen\u00a0infectedwith\u00a0acold).Both\u00a0ofyou\u00a0areusuallyhealthy,\u00a0sothe\ncorrespondingstatehasthehighesta\ufb03nity.Thestatewhereonlyoneofyouis\nsickhasthelowesta\ufb03nity,becausethisisararestate.Thestatewherebothof\nyouaresick(becauseoneofyouhasinfectedtheother)isahighera\ufb03nitystate,\nthoughstillnotascommonasthestatewherebotharehealthy.\nTocompletethemodel,wewouldneedtoalsode\ufb01neasimilarfactorforthe\ncliquecontainingh yandh r.\n1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n\nWhiletheunnormalized probabilitydistributionisguaranteedtobenon-negative\neverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\nprobabilitydistribution,wemustusethecorrespondingnormalizedprobability\ndistribution:4\np() =x1\nZ\u02dc p()x (16.4)\nwhere Zisthevalue\u00a0thatresultsintheprobability\u00a0distributionsummingor\nintegratingto1:\nZ=\ue05a\n\u02dc p d . ()xx (16.5)\nYoucanthinkof Zasaconstantwhenthe \u03c6functionsareheldconstant.Note\nthatifthe \u03c6functionshaveparameters,then Zisafunctionofthoseparameters.\nItiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace.\nThenormalizingconstant Zisknownasthepartitionfunction,atermborrowed\nfromstatisticalphysics.\nSince Zisanintegralorsumoverallpossiblejointassignmentsofthestatex\nitisoftenintractabletocompute.\u00a0Inordertobeabletoobtainthenormalized\nprobabilitydistributionofanundirectedmodel,\u00a0themodelstructureandthe\nde\ufb01nitionsofthe \u03c6functionsmustbeconducivetocomputing Ze\ufb03ciently.In\nthecontextofdeeplearning, Zisusuallyintractable.\u00a0Due totheintractability\nofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate\nalgorithmsarethetopicofchapter.18\nOneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\nisthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist.\nThishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\n4A d i s t rib u t i o n d e \ufb01 n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s\nd is t rib u t i on .\n5 6 8", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nof\u02dc povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\nscalarvariablexwithasinglecliquepotential \u2208 R \u03c6 x x () = 2.Inthiscase,\nZ=\ue05a\nx2d x . (16.6)\nSincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\nthischoiceof \u03c6( x).\u00a0Sometimes thechoiceofsomeparameterofthe \u03c6functions\ndetermineswhetherthe\u00a0probabilit ydistribution\u00a0isde\ufb01ned.For\u00a0example,\u00a0for\n\u03c6( x; \u03b2) =exp\ue000\u2212 \u03b2 x2\ue001\n,the \u03b2parameterdetermineswhether Zexists.Positive \u03b2\nresultsinaGaussiandistributionoverxbutallothervaluesof \u03b2make \u03c6impossible\ntonormalize.\nOnekeydi\ufb00erencebetweendirectedmodelingandundirectedmodelingisthat\ndirectedmodelsarede\ufb01neddirectlyintermsofprobabilitydistributionsfrom\nthestart,whileundirectedmodelsarede\ufb01nedmorelooselyby \u03c6functionsthat\narethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone\nmustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind\nwhileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\nhasdramatice\ufb00ectonthekindofprobabilitydistributionthatagivensetof \u03c6\nfunctionscorrespondsto.Forexample,consideran n-dimensionalvector-valued\nrandomvariable xandanundirectedmodelparametrized byavectorofbiases\nb.Supposewehaveonecliqueforeachelementofx, \u03c6( ) i(x i) =exp( b ix i).What\nkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\nnothaveenoughinformation,becausewehavenotyetspeci\ufb01edthedomainofx.\nIfx \u2208 Rn,thentheintegralde\ufb01ning Zdivergesandnoprobabilitydistribution\nexists.Ifx\u2208{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with\np(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors\n({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge\nvalueof b iactuallyreduces p(x j=1)for j\ue036= i.\u00a0Often,itispossibletoleverage\nthee\ufb00ectofacarefullychosendomainofavariableinordertoobtaincomplicated\nbehaviorfromarelativelysimplesetof \u03c6functions.Wewillexploreapractical\napplicationofthisidealater,insection.20.6\n1 6 . 2 . 4 E n erg y-B a s ed Mo d el s\nManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\nsumptionthat\u2200x ,\u02dc p(x) >0.Aconvenientwaytoenforcethisconditionistouse\nan (EBM)where energy-basedmodel\n\u02dc p E () = exp( x \u2212())x (16.7)\n5 6 9", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas\n1\nZ\u03c6 a b ,(ab ,) \u03c6 b c ,(bc ,) \u03c6 a d ,(ad ,) \u03c6 b e ,(be ,) \u03c6 e f ,(ef ,)foranappropriatechoiceofthe \u03c6func-\ntions.\nand E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall\nz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\nforanystatex.Beingcompletely\u00a0free to\u00a0choose\u00a0theenergyfunction\u00a0makes\nlearningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\nconstrainedoptimization toarbitrarilyimposesomespeci\ufb01cminimalprobability\nvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\nTheprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\nbutneverreachit.\nAnydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\nmann\u00a0distribution.For\u00a0this\u00a0reason,\u00a0manyenergy-based\u00a0models\u00a0are\u00a0called\nBoltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,\n1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\namodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\ntermBoltzmannmachinewas\ufb01rstintroducedtodescribeamodelwithexclusively\nbinaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\nBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann\nmachineswereoriginallyde\ufb01nedtoencompassbothmodelswithandwithoutla-\ntentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\nmodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\naremoreoftencalledMarkovrandom\ufb01eldsorlog-linearmodels.\nCliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\nprobabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdi\ufb00erent\ncliquesintheundirectedgraphcorrespondtothedi\ufb00erenttermsoftheenergy\nfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov\nnetwork:theexponentiationmakeseachtermintheenergyfunctioncorrespond\ntoafactorforadi\ufb00erentclique.See\ufb01gureforanexampleofhowtoreadthe 16.5\n5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z\n5 7 0", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure\u00a016.5:Thisgraph\u00a0impliesthat E(abcdef , , , , ,)can\u00a0be\u00a0writtenas E a b ,(ab ,)+\nE b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique\nenergyfunctions.Notethatwecanobtainthe \u03c6functionsin\ufb01gurebysettingeach 16.4 \u03c6\ntotheexponentialofthecorrespondingnegativeenergy,e.g., \u03c6 a b ,(ab ,) =exp(()) \u2212 Eab ,.\nformoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan\nenergy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\nofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\nanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\nbethoughtofasan\u201cexpert\u201dthatdetermineswhetheraparticularsoftconstraint\nissatis\ufb01ed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\nalow-dimensionalprojectionoftherandomvariables,butwhencombinedby\nmultiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-\ndimensionalconstraint.\nOnepartofthede\ufb01nitionofanenergy-basedmodelservesnofunctionalpurpose\nfromamachinelearningpointofview:the\u2212signinequation.This16.7 \u2212sign\ncouldbeincorporatedintothede\ufb01nitionof E.Formanychoicesofthefunction\nE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\n\u2212signispresentprimarilytopreservecompatibilitybetweenthemachinelearning\nliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodeling\nwereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,\nphysicalenergyanddoesnothavearbitrarysign.\u00a0Terminologysuchas\u201cenergy\u201d\nand\u201cpartitionfunction\u201dremainsassociatedwiththesetechniques,eventhough\ntheirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey\nweredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986\nreferredtonegativeenergyasharmony)havechosentoemitthenegation,but\nthisisnotthestandardconvention.\nManyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute\np m o de l( x)butonly log \u02dc p m o de l( x).Forenergy-basedmodelswithlatentvariables h,\nthesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\n5 7 1", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b a s b\n(a) (b)\nFigure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis\nactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres\nisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\nthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.\ncalledthe :freeenergy\nF \u2212 () = x log\ue058\nhexp(( )) \u2212 E x h , . (16.8)\nInthisbook,weusuallypreferthemoregeneral log \u02dc p m o de l() xformulation.\n1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n\nTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\nneedtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions\ncanbeenabledordisabledbyobservingothervariables.Moreformally,wewould\nliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach\nother,giventhevaluesofothersubsetsofvariables.\nIdentifyingtheconditionalindependencesinagraphisverysimpleinthecase\nofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph\niscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat\nAisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath\ninvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno\npathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\nseparated.Werefertopathsinvolvingonlyunobservedvariablesas\u201cactive\u201dand\npathsincludinganobservedvariableas\u201cinactive.\u201d\nWhenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\nSee\ufb01gureforadepictionofhowactiveandinactivepathsinanundirected 16.6\nmodellookwhendrawninthisway.See\ufb01gureforanexampleofreading 16.7\nseparationfromanundirectedgraph.\nSimilar\u00a0concepts apply\u00a0todirected\u00a0models ,except\u00a0that\u00a0inthe\u00a0context\u00a0of\ndirectedmodels,theseconceptsarereferredtoasd-separation.The\u201cd\u201dstands\nfor\u201cdependence.\u201d\u00a0D-separati onfordirectedgraphsisde\ufb01nedthesameasseparation\n5 7 2", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na\nb c\nd\nFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\nbisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom\natoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb\nalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.\nTherefore,aanddarenotseparatedgivenb.\nforundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies\nthatisindependentfromgiven. A B S\nAswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\ngraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\naredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch\npathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\nmorecomplicated. See\ufb01gureforaguidetoidentifyingactivepathsina 16.8\ndirectedmodel.See\ufb01gureforanexampleofreadingsomepropertiesfroma 16.9\ngraph.\nItisimportanttorememberthatseparationandd-separationtellusonly\naboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno\nrequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\nitisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\ntorepresentanydistribution.Infact,somedistributionscontainindependences\nthatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\nspeci\ufb01cindependencesareindependencesthatarepresentdependentonthe\nvalueofsomevariablesinthenetwork.\u00a0Forexample,consideramodelofthree\nbinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,\nbutwhenais1,bisdeterministicallyequaltoc.\u00a0Encodingthebehaviorwhen\na= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb\nandcareindependentwhena.= 0\nIngeneral,agraphwillneverimplythatanindependenceexistswhenitdoes\nnot.However,agraphmayfailtoencodeanindependence.\n5 7 3", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b\na s b\na\nsb a s ba s b\nc( a ) ( b )\n( c ) ( d )\nFigure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\nvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa.\nThiskindofpathbecomesblockedifsisobserved.\u00a0Wehavealreadyseenthiskindof\npathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For\nexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaand\nbmeasurethewindspeedattwodi\ufb00erentnearbyweathermonitoringoutposts.Ifwe\nobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This\nkindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we\nexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected\nwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing\nthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the\npathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe\ncollidercase.\u00a0TheV-structurecausesaandbtoberelatedbytheexplainingaway\ne\ufb00ect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose\nsisavariableindicatingthatyourcolleagueisnotatwork.\u00a0Thevariablearepresents\nherbeingsick,whilebrepresentsherbeingonvacation.\u00a0Ifyouobservethatsheisnot\natwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially\nlikelythatbothhavehappenedatthesametime.Ifyou\ufb01ndoutthatsheisonvacation,\nthisfactissu\ufb03cienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n\nsick.Theexplainingawaye\ufb00ecthappensevenifanydescendantof ( d ) sisobserved!For\nexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareport\nfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases\nyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit\nmorelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha\nV-structureistoobservenoneofthedescendantsofthesharedchild.\n5 7 4", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nc\nd e\nFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\ninclude:\n\u2022aandbared-separatedgiventheemptyset.\n\u2022aandeared-separatedgivenc.\n\u2022dandeared-separatedgivenc.\nWecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\nvariables:\n\u2022aandbarenotd-separatedgivenc.\n\u2022aandbarenotd-separatedgivend.\n5 7 5", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s\nWeoftenrefertoaspeci\ufb01cmachinelearningmodelasbeingundirectedordirected.\nForexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\nThischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\nisinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d\nusingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\nDirectedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\nvantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\nweshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\ndependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\nuseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\ncapturethemostindependencesintheprobabilitydistributionorwhichapproach\nusesthefewestedgestodescribethedistribution.Thereareotherfactorsthat\ncana\ufb00ectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle\nprobabilitydistribution,wemaysometimesswitchbetweendi\ufb00erentmodeling\nlanguages.Sometimesadi\ufb00erentlanguagebecomesmoreappropriateifweobserve\nacertainsubsetofvariables,orifwewishtoperformadi\ufb00erentcomputational\ntask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward\napproachtoe\ufb03cientlydrawsamplesfromthemodel(describedinsection)16.3\nwhiletheundirectedmodelformulationisoftenusefulforderivingapproximate\ninferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19\nmodelsishighlightedinequation).19.56\nEveryprobabilitydistributioncanberepresentedbyeitheradirectedmodel\norbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany\ndistributionbyusinga\u201ccompletegraph.\u201dInthecaseofadirectedmodel,the\ncompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingon\ntherandomvariables,andeachvariablehasallothervariablesthatprecedeitin\ntheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete\ngraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.\nSee\ufb01gureforanexample. 16.10\nOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\nvariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\ndoesnotimplyanyindependences.\nWhenwerepresentaprobabilitydistributionwithagraph,wewanttochoose\nagraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\nindependencesthatdonotactuallyexist.\nFromthispointofview,somedistributionscanberepresentedmoree\ufb03ciently\n5 7 6", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\nHereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph.\nIntheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t )\nInthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\nvariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe\nordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\nvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.\nusingdirectedmodels,whileotherdistributionscanberepresentedmoree\ufb03ciently\nusing\u00a0undirectedmodels.In\u00a0other\u00a0words,directed\u00a0models\u00a0canencode\u00a0some\nindependencesthatundirectedmodelscannotencode,andviceversa.\nDirectedmodelsareabletouseonespeci\ufb01ckindofsubstructurethatundirected\nmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.\nThestructureoccurswhentworandomvariablesaandbarebothparentsofa\nthirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither\ndirection.(Thename\u201cimmorality\u201dmayseemstrange;itwascoinedinthegraphical\nmodelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\nwithgraph Dintoanundirectedmodel,weneedtocreateanewgraph U.\u00a0For\neverypairofvariablesxandy,weaddanundirectededgeconnectingxandyto\nUifthereisadirectededge(ineitherdirection)connectingxandyinDorifx\nandyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa\nmoralizedgraph.See\ufb01gureforexamplesofconvertingdirectedmodelsto 16.11\nundirectedmodelsviamoralization.\nLikewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\ncanrepresentperfectly.Speci\ufb01cally,adirectedgraphcannotcaptureallofthe D\nconditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop\noflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis\nasequenceofvariablesconnectedbyundirectededges,withthelastvariablein\nthesequenceconnectedbacktothe\ufb01rstvariableinthesequence.\u00a0Achordisa\nconnectionbetweenanytwonon-consecutivevariablesinthesequencede\ufb01ninga\nloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese\nloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\n5 7 7", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nFigure16.11:\u00a0Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\n(bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted\ntoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\nresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional\nindependences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )\ntoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely\nofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive\npathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude\nacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab\u22a5.\n( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany\nimpliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\nedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\ndirectdependences.\n5 7 8", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nd ca b\nd ca b\nd c\nFigure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected\nmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour\nwithnochords.Speci\ufb01cally,theundirectedmodelencodestwodi\ufb00erentindependencesthat\nnodirectedmodelcancapturesimultaneously:acbd \u22a5|{ ,}andbdac \u22a5|{ ,}.To ( C e n t e r )\nconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by\nensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither\naddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis\nexample,wechoosetoaddtheedgeconnectingaandc.To\ufb01nishtheconversion ( R i g h t )\nprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\ndirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\nandalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode\nthatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\nalphabeticalorder.\nthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.\nThegraphformedbyaddingchordstoUisknownasachordalortriangulated\ngraph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\nloops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign\ndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein\nD,ortheresultdoesnotde\ufb01neavaliddirectedprobabilisticmodel.Oneway\ntoassigndirectionstotheedgesinDistoimposeanorderingontherandom\nvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingto\nthenodethatcomeslaterintheordering.See\ufb01gureforademonstration. 16.12\n1 6 . 2 . 7 F a ct o r G ra p h s\nFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean\nambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\nanundirectedmodel,thescopeofevery \u03c6functionmustbeaofsomeclique s u b s e t\ninthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\nacorrespondingfactorwhosescopeencompassestheentireclique\u2014forexample,\nacliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\normaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\n5 7 9", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach \u03c6\nfunction.Speci\ufb01cally,afactorgraphisagraphicalrepresentationofanundirected\nmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\nascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected\nmodel.\u00a0Therestofthenodesaredrawnassquares.\u00a0Thesenodescorrespondto\nthefactors \u03c6oftheunnormalized probabilitydistribution.Variablesandfactors\nmaybeconnectedwithundirectededges.Avariableandafactorareconnected\ninthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\ntheunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother\nfactorinthegraph,norcanavariablebeconnectedtoavariable.See\ufb01gure16.13\nforanexampleofhowfactorgraphscanresolveambiguityintheinterpretation of\nundirectednetworks.\na b\nca b\ncf 1 f 1a b\ncf 1 f 1f 2 f 2\nf 3 f 3\nFigure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation\nofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:\na,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )\nfactorgraphhasonefactoroverallthreevariables.\u00a0Anothervalidfactorgraph ( R i g h t )\nforthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo\nvariables.Representation,inference,andlearningareallasymptoticallycheaperinthis\nfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe\nsameundirectedgraphtorepresent.\n16.3SamplingfromGraphicalModels\nGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\nOneadvantageofdirectedgraphicalmodelsisthatasimpleande\ufb03cientproce-\ndurecalledancestralsamplingcanproduceasamplefromthejointdistribution\nrepresentedbythemodel.\nThebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,\nsothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables\n5 8 0", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ncanthenbesampledinthisorder.Inotherwords,we\ufb01rstsamplex 1\u223c P(x 1),\nthensample P(x 2| P aG(x 2)),andsoon,until\ufb01nallywesample P(x n| P aG(x n)).\nSolongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,\nthenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\nguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\nsamplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto\nsampleavariablebeforeitsparentsareavailable.\nForsomegraphs,morethanonetopologicalorderingispossible.Ancestral\nsamplingmaybeusedwithanyofthesetopologicalorderings.\nAncestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\ntionaliseasy)andconvenient.\nOnedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\nmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\noperation.Whenwewishtosamplefromasubsetofthevariablesinadirected\ngraphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\ningvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\nInthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\nspeci\ufb01edbythemodeldistribution.Otherwise,theconditionaldistributionswe\nneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.\nTheseposteriordistributionsareusuallynotexplicitlyspeci\ufb01edandparametrized\ninthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\nthisisthecase,ancestralsamplingisnolongere\ufb03cient.\nUnfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\ncansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\noftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\ndistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\nsomanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling\nfromanundirectedmodelwithout\ufb01rstconvertingittoadirectedmodelseemsto\nrequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\nvariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\ndrawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass\nprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewe\nhaveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We\niterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother\nvariables,from p(x i|x\u2212 i).Duetotheseparationpropertiesofthegraphical\nmodel,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,\nafterwehavemadeonepassthroughthegraphicalmodelandsampledall n\nvariables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe\n5 8 1", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nprocessandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors.\nAsymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\nthecorrectdistribution.Itcanbedi\ufb03culttodeterminewhenthesampleshave\nreachedasu\ufb03cientlyaccurateapproximationofthedesireddistribution.Sampling\ntechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\nchapter.17\n16.4AdvantagesofStructuredModeling\nTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\nustodramatically reducethecostofrepresentingprobabilitydistributionsaswell\naslearningandinference.Samplingisalsoacceleratedinthecaseofdirected\nmodels,whilethesituationcanbecomplicatedwithundirectedmodels.The\nprimarymechanismthatallowsalloftheseoperationstouselessruntimeand\nmemoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey\ninformationbyleavingedgesout.Anywherethereisnotanedge,themodel\nspeci\ufb01estheassumptionthatwedonotneedtomodeladirectinteraction.\nAlessquanti\ufb01ablebene\ufb01tofusingstructuredprobabilisticmodelsisthat\ntheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\nknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\ndevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\ninferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,\nwecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\ndata.Wecanthencombinethesedi\ufb00erentalgorithmsandstructuresandobtain\naCartesianproductofdi\ufb00erentpossibilities.Itwouldbemuchmoredi\ufb03cultto\ndesignend-to-endalgorithmsforeverypossiblesituation.\n16.5LearningaboutDependencies\nAgoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\nobservedor\u201cvisible\u201d\u00a0variables v.Oftenthedi\ufb00erentelementsofvarehighly\ndependentoneachother.Inthecontextofdeeplearning,theapproachmost\ncommonlyusedtomodelthesedependenciesistointroduceseverallatentor\n\u201chidden\u201dvariables,h.Themodelcanthencapturedependenciesbetweenanypair\nofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and\ndirectdependenciesbetweenandv h j.\nAgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto\n5 8 2", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\ncliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis\ncostly\u2014bothinacomputational sense,becausethenumberofparametersthat\nmustbestoredinmemoryscalesexponentiallywiththenumberofmembersina\nclique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\nrequiresawealthofdatatoestimateaccurately.\nWhenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\nwithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\ngraphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\nomitedgesbetweenothervariables.Anentire\ufb01eldofmachinelearningcalled\nstructurelearningisdevotedtothisproblemForagoodreferenceonstructure\nlearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare\naformofgreedysearch.Astructureisproposed,amodelwiththatstructure\nistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\npenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\naddedorremovedarethenproposedasthenextstepofthesearch.Thesearch\nproceedstoanewstructurethatisexpectedtoincreasethescore.\nUsinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\ndiscretesearchesandmultipleroundsoftraining.A\ufb01xedstructureovervisible\nandhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits\ntoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter\nlearningtechniqueswecanlearnamodelwitha\ufb01xedstructurethatimputesthe\nrightstructureonthemarginal . p()v\nLatentvariableshaveadvantagesbeyondtheirroleine\ufb03cientlycapturing p(v).\nThenewvariables halsoprovideanalternativerepresentationforv.Forexample,\nasdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6\nthatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This\nmeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo\nclassi\ufb01cation.\u00a0Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14\ncodinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassi\ufb01er,\norascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,\nbutdeepermodelsandmodelswithdi\ufb00erentkindsofinteractionscancreateeven\nricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning\nbylearninglatentvariables.Often,givensomemodelofvandh,experimental\nobservationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor\nv.\n5 8 3", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n16.6InferenceandApproximateInference\nOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\nhowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\nwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\nextractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed\ntosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\nusingtheprincipleofmaximumlikelihood.Because\nlog()= p v E h h\u223c p (| v )[log( )log( )] p h v ,\u2212 p h v| ,(16.9)\nweoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof\ntheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof\nsomevariablesgivenothervariables,orpredicttheprobabilitydistributionover\nsomevariablesgiventhevalueofothervariables.\nUnfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\nintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The\ngraphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\nwithareasonablenumberofparameters,butthegraphsusedfordeeplearningare\nusuallynotrestrictiveenoughtoalsoallowe\ufb03cientinference.\nItisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\ngraphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe\ncomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\nhasasolutionand\ufb01ndingasolutionifoneexists.Problemsin#Prequirecounting\nthenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\nwede\ufb01neagraphicalmodeloverthebinaryvariablesina3-SATproblem.\u00a0We\ncanimposeauniformdistributionoverthesevariables.Wecanthenaddone\nbinarylatentvariableperclausethatindicateswhethereachclauseissatis\ufb01ed.\nWecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare\nsatis\ufb01ed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\ntreeoflatentvariables,witheachnodeinthetreereportingwhethertwoother\nvariablesaresatis\ufb01ed.Theleavesofthistreearethevariablesforeachclause.\nTherootofthetreereportswhethertheentireproblemissatis\ufb01ed.\u00a0Duetothe\nuniformdistributionovertheliterals,themarginaldistributionovertherootofthe\nreductiontreespeci\ufb01eswhatfractionofassignmentssatisfytheproblem.While\nthisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical\nreal-worldscenarios.\nThismotivatestheuseofapproximate inference.In\u00a0thecontextof\u00a0deep\nlearning,thisusuallyreferstovariationalinference,inwhichweapproximate the\n5 8 4", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntruedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas\nclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\ninchapter.19\n16.7TheDeepLearningApproachtoStructuredProb-\nabilisticModels\nDeeplearningpractitioners generallyusethesamebasiccomputational toolsas\nothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\nHowever,inthecontextofdeeplearning,weusuallymakedi\ufb00erentdesigndecisions\nabouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\nhaveaverydi\ufb00erent\ufb02avorfrommoretraditionalgraphicalmodels.\nDeeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\ncontextofgraphicalmodels,wecande\ufb01nethedepthofamodelintermsofthe\ngraphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa\nlatentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved\nvariableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest\ndepthofanysuch h i.Thiskindofdepthisdi\ufb00erentfromthedepthinducedby\nthecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno\nlatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational\ngraphstode\ufb01netheconditionaldistributionswithinamodel.\nDeeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\ntations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\nshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways\nhaveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave\nmorelatentvariablesthanobservedvariables.Complicated nonlinearinteractions\nbetweenvariablesareaccomplishedviaindirectconnectionsthat\ufb02owthrough\nmultiplelatentvariables.\nBycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\nareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\nrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order\ntermsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\nvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.\nThewaythatlatentvariablesaredesignedalsodi\ufb00ersindeeplearning.The\ndeeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\ntakeonanyspeci\ufb01csemanticsaheadoftime\u2014thetrainingalgorithmisfreeto\ninventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\n5 8 5", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\ntechniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When\nlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\noftendesignedwithsomespeci\ufb01csemanticsinmind\u2014thetopicofadocument,\ntheintelligenceofastudent,thediseasecausingapatient\u2019ssymptoms,etc.These\nmodelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave\nmoretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare\nnotreusableinasmanydi\ufb00erentcontextsasdeepmodels.\nAnotherobviousdi\ufb00erenceisthekindofconnectivitytypicallyusedinthe\ndeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits\nthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween\ntwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels\nhaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe\nindividuallydesigned.Thedesignofthemodelstructureistightlylinkedwith\nthechoiceofinferencealgorithm.Traditionalapproachestographicalmodels\ntypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint\nistoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled\nloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery\nsparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto\nconnecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea\ndistributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo).\nDistributedrepresentationshavemanyadvantages,butfromthepointofview\nofgraphicalmodelsandcomputational complexity,distributedrepresentations\nhavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor\nthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe\nrelevant.Asaconsequence,oneofthemoststrikingdi\ufb00erencesbetweenthelarger\ngraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat\nloopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels\nareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms\ne\ufb03cient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge\nnumberoflatentvariables,makinge\ufb03cientnumericalcodeessential.Thisprovides\nanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for\ngroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween\ntwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented\nwithe\ufb03cientmatrixproductoperations,orsparselyconnectedgeneralizations ,like\nblockdiagonalmatrixproductsorconvolutions.\nFinally,thedeeplearningapproachtographicalmodelingischaracterizedby\namarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\nallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\n5 8 6", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\nwhosemarginaldistributionscannotbecomputed,andaresatis\ufb01edsimplytodraw\napproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\nobjectivefunctionthatwecannotevenapproximate inareasonableamountof\ntime,butwearestillabletoapproximately trainthemodelifwecane\ufb03ciently\nobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach\nisoftento\ufb01gureoutwhattheminimumamountofinformationweabsolutely\nneedis,andthento\ufb01gureouthowtogetareasonableapproximation ofthat\ninformationasquicklyaspossible.\n1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e\nTherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium\nisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.\nTheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables\nthatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20\nseehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe\nRBMexempli\ufb01esmanyofthepracticesusedinawidevarietyofdeepgraphical\nmodels:\u00a0itsunitsareorganizedintolargegroupscalledlayers,theconnectivity\nbetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the\nmodelisdesignedtoallowe\ufb03cientGibbssampling,andtheemphasisofthemodel\ndesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics\nwerenotspeci\ufb01edbythedesigner.Later,insection,wewillrevisittheRBM 20.2\ninmoredetail.\nThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\nunits.Itsenergyfunctionis\nE ,( v h b ) = \u2212\ue03ev c\u2212\ue03eh v\u2212\ue03eW h , (16.10)\nwhere b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan\nseethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction\nbetweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically\nin\ufb01gure.Asthis\ufb01guremakesclear,animportantaspectofthismodelis 16.14\nthattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany\ntwohiddenunits(hencethe\u201crestricted,\u201dageneralBoltzmannmachinemayhave\narbitraryconnections).\nTherestrictionsontheRBMstructureyieldtheniceproperties\np( ) = \u03a0 hv| i p(h i|v) (16.11)\n5 8 7", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\nFigure16.14:AnRBMdrawnasaMarkovnetwork.\nand\np( ) = \u03a0 vh| i p(v i|h) . (16.12)\nTheindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\nweobtain:\nP(h i= 1 ) = |v \u03c3\ue010\nv\ue03eW : , i+ b i\ue011\n, (16.13)\nP(h i= 0 ) = 1 |v \u2212 \u03c3\ue010\nv\ue03eW : , i+ b i\ue011\n. (16.14)\nTogetherthesepropertiesallowfore\ufb03cientblockGibbssampling,whichalter-\nnatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-\nously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\n\ufb01gure.16.15\nSincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\neasytotakeitsderivatives.Forexample,\n\u2202\n\u2202 W i , jE ,(vh) = \u2212v ih j . (16.15)\nThesetwoproperties\u2014e\ufb03cientGibbssamplingande\ufb03cientderivatives\u2014make\ntrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\ntrainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\nTrainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse\nE h h\u223c p (| v )[] hasasetoffeaturestodescribe. v\nOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\nicalmodels:\u00a0representationlearningaccomplishedvialayersoflatentvariables,\ncombinedwithe\ufb03cientinteractionsbetweenlayersparametrized bymatrices.\nThelanguageofgraphicalmodelsprovidesanelegant,\ufb02exibleandclearlanguage\nfordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\namongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\n5 8 8", "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith\npermissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn\nusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow\nrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare\nhighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )\nthistothesamplesandweightsofalinearfactormodel,shownin\ufb01gure.Thesamples 13.2\nherearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The\nRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,\ntheRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|\nsothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable\ntohavebothanon-factorialandanon-factorial. p() h p( ) h v|\n5 8 9", "C h a p t e r 1 7\nMon t e C arl o Me t h o d s\nRandomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsand\nMonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrect\nanswer(orreportthattheyfailed).Thesealgorithmsconsumearandomamount\nofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturn\nanswerswitharandomamountoferror.Theamountoferrorcantypicallybe\nreducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Forany\n\ufb01xedcomputational budget,aMonteCarloalgorithmcanprovideanapproximate\nanswer.\nManyproblemsinmachinelearningaresodi\ufb03cultthatwecanneverexpectto\nobtainpreciseanswerstothem.Thisexcludesprecisedeterministicalgorithmsand\nLasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithms\norMonteCarloapproximations.Bothapproachesareubiquitousinmachine\nlearning.Inthischapter,wefocusonMonteCarlomethods.\n17.1SamplingandMonteCarloMethods\nManyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebased\nondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplesto\nformaMonteCarloestimateofsomedesiredquantity.\n1 7 . 1 . 1 Wh y S a m p l i n g ?\nTherearemanyreasonsthatwemaywishtodrawsamplesfromaprobability\ndistribution.Samplingprovidesa\ufb02exiblewaytoapproximatemanysumsand\n590", "CHAPTER17.MONTECARLOMETHODS\nintegralsatreducedcost.Sometimesweusethistoprovideasigni\ufb01cantspeedupto\nacostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcost\nwithminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximate\nanintractablesumorintegral,suchasthegradientofthelogpartitionfunctionof\nanundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthe\nsensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution.\n1 7 . 1 . 2 B a s i cs o f Mo n t e Ca rl o S a m p l i n g\nWhenasumoranintegralcannotbecomputedexactly(forexamplethesum\nhasanexponentialnumberoftermsandnoexactsimpli\ufb01cationisknown)itis\noftenpossibletoapproximate itusingMonteCarlosampling.Theideaistoview\nthesumorintegralasifitwasanexpectationundersomedistributionandto\na p p r o x i m a t e t h e e x p e c t a t i o n b y a c o r r e s p o nding a v e r a g e.Let\ns=\ue058\nxp f E () x() = x p[()] f x (17.1)\nor\ns=\ue05a\np f d E () x() x x= p[()] f x (17.2)\nbethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraint\nthat pisaprobabilitydistribution(forthesum)oraprobabilitydensity(forthe\nintegral)overrandomvariable. x\nWecanapproximate sbydrawing nsamples x( 1 ), . . . , x( ) nfrom pandthen\nformingtheempiricalaverage\n\u02c6 s n=1\nnn \ue058\ni = 1f( x( ) i) . (17.3)\nThisapproximation isjusti\ufb01edbyafewdi\ufb00erentproperties.The\ufb01rsttrivial\nobservationisthattheestimator \u02c6 sisunbiased,since\nE[\u02c6 s n] =1\nnn \ue058\ni = 1E[( f x( ) i)] =1\nnn \ue058\ni = 1s s .= (17.4)\nButinaddition,thelawoflargenumbersstatesthatifthesamples x( ) iare\ni.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:\nlimn\u2192\u221e\u02c6 s n= s , (17.5)\n5 9 1", "CHAPTER17.MONTECARLOMETHODS\nprovidedthatthevarianceoftheindividualterms,Var[ f( x( ) i)],isbounded.Tosee\nthismoreclearly,considerthevarianceof\u02c6 s nas nincreases.Thevariance Var[\u02c6 s n]\ndecreasesandconvergesto0,solongasVar[( f x( ) i)] <\u221e:\nVar[\u02c6 s n] =1\nn2n\ue058\ni = 1Var[()] f x (17.6)\n=Var[()] f x\nn. (17.7)\nThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonte\nCarloaverageorequivalentlytheamountofexpectederroroftheMonteCarlo\napproximation.Wecomputeboththeempiricalaverageofthe f( x( ) i)andtheir\nempiricalvariance,1andthendividetheestimatedvariancebythenumberof\nsamples ntoobtainanestimatorofVar[\u02c6 s n].\u00a0Thecentrallimittheoremtells\nusthatthedistributionoftheaverage, \u02c6 s n,convergestoanormaldistribution\nwithmean sandvarianceV a r [ ( ) ] f x\nn.Thisallowsustoestimatecon\ufb01denceintervals\naroundtheestimate \u02c6 s n,usingthecumulativedistributionofthenormaldensity.\nHowever,allthisreliesonourabilitytoeasilysamplefromthebasedistribution\np( x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefrom\np,analternativeistouseimportancesampling,presentedinsection.A17.2\nmoregeneralapproachistoformasequenceofestimatorsthatconvergetowards\nthedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains\n(section).17.3\n17.2ImportanceSampling\nAnimportantstepinthedecompositionoftheintegrand(orsummand)usedbythe\nMonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould 17.2\nplaytheroletheprobability p( x)andwhichpartoftheintegrandshouldplaythe\nroleofthequantity f( x) whoseexpectedvalue(underthatprobabilitydistribution)\nistobeestimated.Thereisnouniquedecompositionbecause p( x) f( x)canalways\nberewrittenas\np f q () x() = x () xp f() x() x\nq() x, (17.8)\nwherewenowsamplefrom qandaveragep f\nq.Inmanycases,wewishtocompute\nanexpectationforagiven pandan f,andthefactthattheproblemisspeci\ufb01ed\n1Th e u n b i a s e d e s t i m a t o r o f t h e v a ria n c e i s o f t e n p re f e rre d , i n wh i c h t h e s u m o f s q u a re d\nd i \ufb00 e re n c e s i s d i v i d e d b y i n s t e a d o f . n \u2212 1 n\n5 9 2", "CHAPTER17.MONTECARLOMETHODS\nfromthestartasanexpectationsuggeststhatthis pand fwouldbeanatural\nchoiceofdecomposition.However,theoriginalspeci\ufb01cationoftheproblemmay\nnotbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtain\nagivenlevelofaccuracy.\u00a0Fortunately,theformoftheoptimalchoice q\u2217canbe\nderivedeasily.Theoptimal q\u2217correspondstowhatiscalledoptimalimportance\nsampling.\nBecauseoftheidentityshowninequation,anyMonteCarloestimator 17.8\n\u02c6 s p=1\nnn \ue058\ni , = 1 x( ) i\u223c pf( x( ) i) (17.9)\ncanbetransformedintoanimportancesamplingestimator\n\u02c6 s q=1\nnn \ue058\ni , = 1 x( ) i\u223c qp( x( ) i)( f x( ) i)\nq( x( ) i). (17.10)\nWeseereadilythattheexpectedvalueoftheestimatordoesnotdependon: q\nE q[\u02c6 s q] = E q[\u02c6 s p] = s . (17.11)\nHowever,thevarianceofanimportancesamplingestimatorcanbegreatlysensitive\ntothechoiceof.Thevarianceisgivenby q\nVar[\u02c6 s q] = Var[p f() x() x\nq() x] /n . (17.12)\nTheminimumvarianceoccurswhenis q\nq\u2217() = xp f() x|() x|\nZ, (17.13)\nwhere Zisthenormalization constant,chosensothat q\u2217( x)sumsorintegratesto\n1asappropriate.Betterimportancesamplingdistributionsputmoreweightwhere\ntheintegrandislarger.Infact,when f( x)doesnotchangesign,Var[\u02c6 s q\u2217]=0,\nmeaningthat whentheoptimaldistributionisused. a s i ng l e s a m p l e i s s u \ufb03 c i e nt\nOfcourse,thisisonlybecausethecomputationof q\u2217hasessentiallysolvedthe\noriginalproblem,soitisusuallynotpracticaltousethisapproachofdrawinga\nsinglesamplefromtheoptimaldistribution.\nAnychoiceofsamplingdistribution qisvalid(inthesenseofyieldingthe\ncorrectexpectedvalue)and q\u2217istheoptimalone(inthesenseofyieldingminimum\nvariance).Samplingfrom q\u2217isusuallyinfeasible,butotherchoicesof qcanbe\nfeasiblewhilestillreducingthevariancesomewhat.\n5 9 3", "CHAPTER17.MONTECARLOMETHODS\nAnotherapproachistousebiasedimportancesampling,whichhasthe\nadvantageofnotrequiringnormalized por q.Inthecaseofdiscretevariables,the\nbiasedimportancesamplingestimatorisgivenby\n\u02c6 s B I S=\ue050n\ni = 1p ( x( ) i)\nq ( x( ) i )f( x( ) i)\n\ue050n\ni = 1p ( x( ) i )\nq ( x( ) i )(17.14)\n=\ue050n\ni = 1p ( x( ) i)\n\u02dc q ( x( ) i)f( x( ) i)\n\ue050n\ni = 1p ( x( ) i)\n\u02dc q ( x( ) i)(17.15)\n=\ue050n\ni = 1\u02dc p ( x( ) i)\n\u02dc q ( x( ) i )f( x( ) i)\n\ue050n\ni = 1\u02dc p ( x( ) i )\n\u02dc q ( x( ) i ), (17.16)\nwhere \u02dc pand\u02dc qaretheunnormalized formsof pand qandthe x( ) iarethesamples\nfrom q.Thisestimatorisbiasedbecause E[\u02c6 s B I S]\ue036= s,exceptasymptoticallywhen\nn\u2192\u221eandthedenominator ofequationconvergesto1.Hencethisestimator 17.14\niscalledasymptoticallyunbiased.\nAlthoughagoodchoiceof qcangreatlyimprovethee\ufb03ciencyofMonteCarlo\nestimation,apoorchoiceof qcanmakethee\ufb03ciencymuchworse.Goingbackto\nequation,weseethatiftherearesamplesof 17.12 qforwhichp f ( ) x| ( ) x|\nq ( ) xislarge,\nthenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhen\nq( x)istinywhileneither p( x)nor f( x)aresmallenoughtocancelit.The q\ndistributionisusuallychosentobeaverysimpledistributionsothatitiseasy\ntosamplefrom.When xishigh-dimensional,thissimplicityin qcausesitto\nmatch por p f||poorly.When q( x( ) i)\ue01d p( x( ) i)| f( x( ) i)|,importancesampling\ncollectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,when\nq( x( ) i)\ue01c p( x( ) i)| f( x( ) i)|,whichwillhappenmorerarely,theratiocanbehuge.\nBecausetheselattereventsarerare,theymaynotshowupinatypicalsample,\nyieldingtypicalunderestimationof s,compensatedrarelybygrossoverestimation.\nSuchverylargeorverysmallnumbersaretypicalwhen xishighdimensional,\nbecauseinhighdimensionthedynamicrangeofjointprobabilities canbevery\nlarge.\nInspiteofthisdanger,importancesamplinganditsvariantshavebeenfound\nveryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms.\nForexample,seetheuseofimportancesamplingtoacceleratetraininginneural\nlanguagemodelswithalargevocabulary(section)orotherneuralnets 12.4.3.3\nwithalargenumberofoutputs.Seealsohowimportancesamplinghasbeen\nusedtoestimateapartitionfunction(thenormalization constantofaprobability\n5 9 4", "CHAPTER17.MONTECARLOMETHODS\ndistribution)insection,andtoestimatethelog-likelihoodindeepdirected 18.7\nmodelssuchasthevariationalautoencoder,insection.Importancesampling 20.10.3\nmayalsobeusedtoimprovetheestimateofthegradientofthecostfunctionused\ntotrainmodelparameterswithstochasticgradientdescent,particularlyformodels\nsuchasclassi\ufb01erswheremostofthetotalvalueofthecostfunctioncomesfroma\nsmallnumberofmisclassi\ufb01edexamples.Samplingmoredi\ufb03cultexamplesmore\nfrequentlycanreducethevarianceofthegradientinsuchcases(,). Hinton2006\n17.3MarkovChainMonteCarloMethods\nInmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractable\nmethodfordrawingexactsamplesfromthedistribution p m o de l( x)orfromagood\n(lowvariance)importancesamplingdistribution q( x).Inthecontextofdeep\nlearning,thismostoftenhappenswhen p m o de l( x)isrepresentedbyanundirected\nmodel.Inthesecases,weintroduceamathematical toolcalledaMarkovchain\ntoapproximately samplefrom p m o de l( x).ThefamilyofalgorithmsthatuseMarkov\nchainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlo\nmethods(MCMC).MarkovchainMonteCarlomethodsformachinelearningare\ndescribedatgreaterlengthinKollerandFriedman2009().\u00a0Themoststandard,\ngenericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodel\ndoesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenient\nto\u00a0present\u00a0these\u00a0techniques\u00a0assampling\u00a0froman\u00a0energy-basedmodel\u00a0(EBM)\np( x)\u221d \u2212exp( E()) xasdescribedinsection.IntheEBMformulation,every 16.2.4\nstateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfact\nmorebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthat\ncontainzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthe\nbehaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordi\ufb00erent\nfamiliesofsuchdistributions.Inthecontextofdeeplearning,itismostcommon\ntorelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoall\nenergy-basedmodels.\nTounderstandwhydrawingsamplesfromanenergy-basedmodelisdi\ufb03cult,\nconsideranEBMoverjusttwovariables,de\ufb01ningadistributionab.Inorder p( ,)\ntosamplea,wemustdrawafrom p(ab|),andinordertosampleb,wemust\ndrawitfrom p(ba|).Itseemstobeanintractablechicken-and-eggproblem.\nDirectedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperform\nancestralsamplingonesimplysampleseachofthevariablesintopologicalorder,\nconditioningoneachvariable\u2019sparents,whichareguaranteedtohavealreadybeen\nsampled(section).Ancestralsamplingde\ufb01nesane\ufb03cient,single-passmethod 16.3\n5 9 5", "CHAPTER17.MONTECARLOMETHODS\nofobtainingasample.\nInanEBM,wecanavoidthischickenandeggproblembysamplingusinga\nMarkovchain.ThecoreideaofaMarkovchainistohaveastate xthatbegins\nasanarbitraryvalue.Overtime,werandomlyupdate xrepeatedly.Eventually\nxbecomes(verynearly)afairsamplefrom p( x).Formally,aMarkovchainis\nde\ufb01nedbyarandomstate xandatransitiondistribution T( x\ue030| x)specifying\ntheprobabilitythatarandomupdatewillgotostate x\ue030ifitstartsinstate x.\nRunningtheMarkovchainmeansrepeatedlyupdatingthestate xtoavalue x\ue030\nsampledfrom T( x\ue030| x).\nTogainsometheoreticalunderstandingofhowMCMCmethodswork,itis\nusefultoreparametrizetheproblem.First,werestrictourattentiontothecase\nwheretherandomvariable xhascountablymanystates.Wecanthenrepresent\nthestateasjustapositiveinteger x.\u00a0Di\ufb00erentintegervaluesof xmapbackto\ndi\ufb00erentstatesintheoriginalproblem. x\nConsiderwhathappenswhenwerunin\ufb01nitelymanyMarkovchainsinparallel.\nAllofthestatesofthedi\ufb00erentMarkovchainsaredrawnfromsomedistribution\nq( ) t( x),where tindicatesthenumberoftimestepsthathaveelapsed.Atthe\nbeginning, q( 0 )issomedistributionthatweusedtoarbitrarilyinitialize xforeach\nMarkovchain.Later, q( ) tisin\ufb02uencedbyalloftheMarkovchainstepsthathave\nrunsofar.Ourgoalisfor q( ) t() xtoconvergeto. p x()\nBecausewehavereparametrized theproblemintermsofpositiveinteger x,we\ncandescribetheprobabilitydistributionusingavector,with q v\nq i v (= x ) = i . (17.17)\nConsiderwhathappenswhenweupdateasingleMarkovchain\u2019sstate xtoa\nnewstate x\ue030.Theprobabilityofasinglestatelandinginstate x\ue030isgivenby\nq( + 1 ) t( x\ue030) =\ue058\nxq( ) t()( x T x\ue030| x .) (17.18)\nUsingourintegerparametrization, wecanrepresentthee\ufb00ectofthetransition\noperatorusingamatrix.Wede\ufb01nesothat T A A\nA i , j= ( T x\ue030= = ) i| x j . (17.19)\nUsingthisde\ufb01nition,wecannowrewriteequation.Ratherthanwritingitin 17.18\ntermsof qand Ttounderstandhowasinglestateisupdated,wemaynowuse v\nand Atodescribehowtheentiredistributionoverallthedi\ufb00erentMarkovchains\n(runninginparallel)shiftsasweapplyanupdate:\nv( ) t= A v( 1 ) t\u2212. (17.20)\n5 9 6", "CHAPTER17.MONTECARLOMETHODS\nApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythe\nmatrix Arepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiating\nthematrix: A\nv( ) t= Atv( 0 ). (17.21)\nThematrix Ahasspecialstructurebecauseeachofitscolumnsrepresentsa\nprobabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthere\nisanon-zeroprobabilityoftransitioningfromanystate xtoanyotherstate x\ue030for\nsomepower t,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,)\nguaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan 1\nseethatalloftheeigenvaluesareexponentiated:\nv( ) t=\ue000\nV \u03bb Vdiag()\u2212 1\ue001tv( 0 )= () Vdiag \u03bbtV\u2212 1v( 0 ).(17.22)\nThisprocesscausesalloftheeigenvaluesthatarenotequaltotodecaytozero. 1\nUndersomeadditionalmildconditions, Aisguaranteedtohaveonlyoneeigenvector\nwitheigenvalue.Theprocessthusconvergestoa 1 stationarydistribution,\nsometimesalsocalledthe .Atconvergence, equilibriumdistribution\nv\ue030= = A v v , (17.23)\nandthissameconditionholdsforeveryadditionalstep.Thisisaneigenvector\nequation.Tobeastationarypoint, vmustbeaneigenvectorwithcorresponding\neigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary 1\ndistribution,repeatedapplicationsofthetransitionsamplingproceduredonot\nchangethe overthestatesofallthevariousMarkovchains(although d i s t r i b u t i o n\ntransitionoperatordoeschangeeachindividualstate,ofcourse).\nIfwehavechosen Tcorrectly,thenthestationarydistribution qwillbeequal\ntothedistribution pwewishtosamplefrom.Wewilldescribehowtochoose T\nshortly,insection.17.4\nMostpropertiesofMarkovChainswithcountablestatescanbegeneralized\ntocontinuousvariables.Inthissituation,someauthorscalltheMarkovChain\naHarrischainbutweusethetermMarkovChaintodescribebothconditions.\nIngeneral,aMarkovchainwithtransitionoperator Twillconverge,undermild\nconditions,toa\ufb01xedpointdescribedbytheequation\nq\ue030( x\ue030) = E x\u223c q T( x\ue030| x) , (17.24)\nwhichinthediscretecaseisjustrewritingequation.When17.23 xisdiscrete,\ntheexpectationcorrespondstoasum,andwhen xiscontinuous,theexpectation\ncorrespondstoanintegral.\n5 9 7", "CHAPTER17.MONTECARLOMETHODS\nRegardlessofwhetherthestateiscontinuousordiscrete,allMarkovchain\nmethodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythe\nstatebeginstoyieldsamplesfromtheequilibriumdistribution.Runningthe\nMarkovchainuntilitreachesitsequilibriumdistributioniscalled\u201cburningin\u201d\ntheMarkovchain.Afterthechainhasreachedequilibrium,asequenceofin\ufb01nitely\nmanysamplesmaybedrawnfromfromtheequilibriumdistribution.\u00a0Theyare\nidenticallydistributedbutanytwosuccessivesampleswillbehighlycorrelated\nwitheachother.A\ufb01nitesequenceofsamplesmaythusnotbeveryrepresentative\noftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturn\nonlyevery nsuccessivesamples,\u00a0sothatourestimateofthestatisticsofthe\nequilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMC\nsampleandthenextseveralsamples.Markovchainsarethusexpensivetouse\nbecauseofthetimerequiredtoburnintotheequilibriumdistributionandthetime\nrequiredtotransitionfromonesampletoanotherreasonablydecorrelatedsample\nafterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrun\nmultipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputation\ntoeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerate\nallsamplesandthestrategyofusingoneMarkovchainforeachdesiredsampleare\ntwoextremes;deeplearningpractitioners usuallyuseanumberofchainsthatis\nsimilartothenumberofexamplesinaminibatchandthendrawasmanysamples\nasareneededfromthis\ufb01xedsetofMarkovchains.Acommonlyusednumberof\nMarkovchainsis100.\nAnotherdi\ufb03cultyisthatwedonotknowinadvancehowmanystepsthe\nMarkovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthof\ntimeiscalledthemixingtime.Itisalsoverydi\ufb03culttotestwhetheraMarkov\nchainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguiding\nusinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnot\nmuchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrix A\nactingonavectorofprobabilities v,thenweknowthatthechainmixeswhen At\nhase\ufb00ectivelylostalloftheeigenvaluesfrom Abesidestheuniqueeigenvalueof.1\nThismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethe\nmixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchain\nintermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisit\nisexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresent\nv, A,ortheeigenvaluesof A.\u00a0Duetotheseandotherobstacles,weusuallydo\nnotknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkov\nchainforanamountoftimethatweroughlyestimatetobesu\ufb03cient,anduse\nheuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristic\nmethodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween\n5 9 8", "CHAPTER17.MONTECARLOMETHODS\nsuccessivesamples.\n17.4GibbsSampling\nSofarwehavedescribedhowtodrawsamplesfromadistribution q( x)byrepeatedly\nupdating x x\u2190\ue030\u223c T( x\ue030| x).However,wehavenotdescribedhowtoensurethat\nq( x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook.\nThe\ufb01rstoneistoderive Tfromagivenlearned p m o de l,describedbelowwiththe\ncaseofsamplingfromEBMs.Thesecondoneistodirectlyparametrize Tand\nlearnit,sothatitsstationarydistributionimplicitlyde\ufb01nesthe p m o de lofinterest.\nExamplesofthissecondapproacharediscussedinsectionsand. 20.1220.13\nInthecontextofdeeplearning,wecommonlyuseMarkovchainstodraw\nsamplesfromanenergy-basedmodelde\ufb01ningadistribution p m o de l( x).Inthiscase,\nwewantthe q( x)fortheMarkovchaintobe p m o de l( x).Toobtainthedesired\nq() x,wemustchooseanappropriate T( x\ue030| x).\nAconceptuallysimpleande\ufb00ectiveapproachtobuildingaMarkovchain\nthatsamplesfrom p m o de l( x)istouseGibbssampling,inwhichsamplingfrom\nT( x\ue030| x)isaccomplishedbyselectingonevariablex iandsamplingitfrom p m o de l\nconditionedonitsneighborsintheundirectedgraph Gde\ufb01ningthestructureof\ntheenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesame\ntimesolongastheyareconditionallyindependentgivenalloftheirneighbors.\nAsshownintheRBMexampleinsection,allofthehiddenunitsofan 16.7.1\nRBMmaybesampledsimultaneouslybecausetheyareconditionallyindependent\nfromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmay\nbesampledsimultaneouslybecausetheyareconditionallyindependentfromeach\nothergivenallofthehiddenunits.Gibbssamplingapproachesthatupdatemany\nvariablessimultaneouslyinthiswayarecalledblockGibbssampling.\nAlternateapproachestodesigningMarkovchainstosamplefrom p m o de lare\npossible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinother\ndisciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,\nitisraretouseanyapproachotherthanGibbssampling.Improvedsampling\ntechniquesareonepossibleresearchfrontier.\n17.5TheChallengeofMixingbetweenSeparatedModes\nTheprimarydi\ufb03cultyinvolvedwithMCMCmethodsisthattheyhaveatendency\ntomixpoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample\n5 9 9", "CHAPTER17.MONTECARLOMETHODS\nfrom p( x)wouldbecompletelyindependentfromeachotherandwouldvisitmany\ndi\ufb00erentregionsin xspaceproportionaltotheirprobability.Instead,especially\ninhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer\ntosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswith\nslowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisy\ngradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingonthe\nprobability,withrespecttothestateofthechain(therandomvariablesbeing\nsampled).\u00a0Thechaintendstotakesmallsteps(inthespaceofthestateofthe\nMarkovchain),fromacon\ufb01guration x( 1 ) t\u2212toacon\ufb01guration x( ) t,withtheenergy\nE( x( ) t)generallylowerorapproximately equaltotheenergy E( x( 1 ) t\u2212),witha\npreferenceformovesthatyieldlowerenergycon\ufb01gurations. Whenstartingfroma\nratherimprobablecon\ufb01guration(higherenergythanthetypicalonesfrom p( x)),\nthechaintendstograduallyreducetheenergyofthestateandonlyoccasionally\nmovetoanothermode.Oncethechainhasfoundaregionoflowenergy(for\nexample,ifthevariablesarepixelsinanimage,aregionoflowenergymightbe\naconnectedmanifoldofimagesofthesameobject),whichwecallamode,the\nchainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Once\ninawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifit\ufb01nds\nanescaperoute)movetowardsanothermode.Theproblemisthatsuccessful\nescaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwill\ncontinuetosamplethesamemodelongerthanitshould.\nThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4\nInthiscontext,considertheprobabilityofgoingfromonemodetoanearbymode\nwithinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshape\nofthe\u201cenergybarrier\u201d\u00a0betweenthesemodes.Transitionsbetweentwomodes\nthatareseparatedbyahighenergybarrier(aregionoflowprobability)are\nexponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisis\nillustratedin\ufb01gure.Theproblemariseswhentherearemultiplemodeswith 17.1\nhighprobabilitythatareseparatedbyregionsoflowprobability,especiallywhen\neachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhose\nvaluesarelargelydeterminedbytheothervariables.\nAsasimpleexample,consideranenergy-basedmodelovertwovariablesaand\nb,whicharebothbinarywithasign,takingonvalues \u22121 1and.If E(ab ,) =\u2212 wab\nforsomelargepositivenumber w,thenthemodelexpressesastrongbeliefthata\nandbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwith\na= 1.\u00a0Theconditionaldistributionoverbisgivenby P(b= 1|a= 1)= \u03c3( w).\nIf wislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe\n1iscloseto1.Likewise,ifa=\u22121,theprobabilityofassigningbtobe\u22121is\ncloseto1.Accordingto P m o de l(ab ,),bothsignsofbothvariablesareequallylikely.\n6 0 0", "CHAPTER17.MONTECARLOMETHODS\nFigure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkov\nchaininitializedatthemodeinbothcases. ( L e f t )Amultivariatenormaldistribution\nwithtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesare\nindependent.Amultivariatenormaldistributionwithhighlycorrelatedvariables. ( C e n t e r )\nThecorrelationbetweenvariablesmakesitdi\ufb03cultfortheMarkovchaintomix.Because\ntheupdateforeachvariablemustbeconditionedontheothervariable,thecorrelation\nreducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint.\n( R i g h t )AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned.\nGibbssamplingmixesveryslowlybecauseitisdi\ufb03culttochangemodeswhilealtering\nonlyonevariableatatime.\nAccordingto P m o de l(ab|),bothvariablesshouldhavethesamesign.Thismeans\nthatGibbssamplingwillonlyveryrarely\ufb02ipthesignsofthesevariables.\nInmorepracticalscenarios,thechallengeisevengreaterbecausewecarenot\nonlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetween\nallthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitions\naredi\ufb03cultbecauseofthedi\ufb03cultyofmixingbetweenmodes,thenitbecomes\nveryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,and\nconvergenceofthechaintoitsstationarydistributionisveryslow.\nSometimesthisproblemcanberesolvedby\ufb01ndinggroupsofhighlydependent\nunitsandupdatingallofthemsimultaneouslyinablock.\u00a0Unfortunately,when\nthedependenciesarecomplicated,itcanbecomputationally intractabletodrawa\nsamplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginally\nintroducedtosolveisthisproblemofsamplingfromalargegroupofvariables.\nInthecontextofmodelswithlatentvariables,whichde\ufb01neajointdistribution\np m o de l( x h ,),weoftendrawsamplesof xbyalternatingbetweensamplingfrom\np m o de l( x h|)andsamplingfrom p m o de l( h x|).Fromthepointofviewofmixing\n6 0 1", "CHAPTER17.MONTECARLOMETHODS\nFigure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels.\nEachpanelshouldbereadlefttoright,toptobottom. ( L e f t )Consecutivesamplesfrom\nGibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset.\nConsecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformed\ninadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisual\nfeatures,butitisstilldi\ufb03cultfortheGibbschaintotransitionfromonemodeofthe\ndistributiontoanother,forexamplebychangingthedigitidentity.Consecutive ( R i g h t )\nancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsampling\ngenerateseachsampleindependentlyfromtheothers,thereisnomixingproblem.\nrapidly,wewouldlike p m o de l( h x|)tohaveveryhighentropy.However,fromthe\npointofviewoflearningausefulrepresentationof h,wewouldlike htoencode\nenoughinformationabout xtoreconstructitwell,whichimpliesthat hand x\nshouldhaveveryhighmutualinformation. Thesetwogoalsareatoddswitheach\nother.Weoftenlearngenerativemodelsthatverypreciselyencode xinto hbut\narenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmann\nmachines\u2014thesharperthedistributionaBoltzmannmachinelearns,theharder\nitisforaMarkovchainsamplingfromthemodeldistributiontomixwell.This\nproblemisillustratedin\ufb01gure.17.2\nAllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinterest\nhasamanifoldstructurewithaseparatemanifoldforeachclass:thedistribution\nisconcentratedaroundmanymodesandthesemodesareseparatedbyvastregions\nofhighenergy.Thistypeofdistributioniswhatweexpectinmanyclassi\ufb01cation\nproblemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoor\nmixingbetweenmodes.\n6 0 2", "CHAPTER17.MONTECARLOMETHODS\n1 7 . 5 . 1 T em p eri n g t o Mi x b et w een Mo d es\nWhenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsof\nlowprobability,itisdi\ufb03culttomixbetweenthedi\ufb00erentmodesofthedistribution.\nSeveraltechniquesforfastermixingarebasedonconstructingalternativeversions\nofthetargetdistributioninwhichthepeaksarenotashighandthesurrounding\nvalleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewayto\ndoso.Sofar,wehavedescribedanenergy-basedmodelasde\ufb01ningaprobability\ndistribution\np E . () exp( x\u221d \u2212()) x (17.25)\nEnergy-basedmodelsmaybeaugmentedwithanextraparameter \u03b2controlling\nhowsharplypeakedthedistributionis:\np \u03b2() exp( ()) x\u221d \u2212 \u03b2 E x . (17.26)\nThe \u03b2parameterisoftendescribedasbeingthereciprocalofthetemperature,\nre\ufb02ectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthe\ntemperaturefallstozeroandrisestoin\ufb01nity,theenergy-basedmodelbecomes \u03b2\ndeterministic.Whenthetemperaturerisestoin\ufb01nityand \u03b2fallstozero,the\ndistribution(fordiscrete)becomesuniform. x\nTypically,amodelistrainedtobeevaluatedat \u03b2= 1.However,wecanmake\nuseofothertemperatures,particularlythosewhere \u03b2 <1.Temperingisageneral\nstrategyofmixingbetweenmodesof p 1rapidlybydrawingsampleswith. \u03b2 <1\nMarkovchainsbasedontemperedtransitions(,)temporarily Neal1994\nsamplefromhigher-temperaturedistributionsinordertomixtodi\ufb00erentmodes,\nthenresumesamplingfromtheunittemperaturedistribution.Thesetechniques\nhavebeenappliedtomodelssuchasRBMs\u00a0(Salakhutdinov2010,).Another\napproachistouseparalleltempering(,),inwhichtheMarkovchain Iba2001\nsimulatesmanydi\ufb00erentstatesinparallel,atdi\ufb00erenttemperatures.Thehighest\ntemperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature\n1,provideaccuratesamplesfromthemodel.Thetransitionoperatorincludes\nstochasticallyswappingstatesbetweentwodi\ufb00erenttemperaturelevels,sothata\nsu\ufb03cientlyhigh-probabilit ysamplefromahigh-temperatureslotcanjumpintoa\nlowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(Desjardins\ne t a l . e t a l . ,;2010Cho,).\u00a0Althoughtemperingisapromisingapproach,at 2010\nthispointithasnotallowedresearcherstomakeastrongadvanceinsolvingthe\nchallengeofsamplingfromcomplexEBMs.Onepossiblereasonisthatthere\narecriticaltemperaturesaroundwhichthetemperaturetransitionmustbe\nveryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobe\ne\ufb00ective.\n6 0 3", "CHAPTER17.MONTECARLOMETHODS\n1 7 . 5 . 2 D ep t h Ma y Hel p Mi xi n g\nWhendrawingsamplesfromalatentvariablemodel p( h x ,),wehaveseenthatif\np( h x|)encodes xtoowell,thensamplingfrom p( x h|)willnotchange xvery\nmuchandmixingwillbepoor.Onewaytoresolvethisproblemistomake hbea\ndeeprepresentation,thatencodesintoinsuchawaythataMarkovchainin x h\nthespaceof hcanmixmoreeasily.Manyrepresentationlearningalgorithms,such\nasautoencodersandRBMs,tendtoyieldamarginaldistributionover hthatis\nmoreuniformandmoreunimodalthantheoriginaldatadistributionover x.Itcan\nbearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusing\nalloftheavailablerepresentationspace,becauseminimizingreconstructionerror\noverthetrainingexampleswillbebetterachievedwhendi\ufb00erenttrainingexamples\nareeasilydistinguishablefromeachotherin h-space,andthuswellseparated.\nBengio2013a e t a l .()observedthatdeeperstacksofregularizedautoencodersor\nRBMsyieldmarginaldistributionsinthetop-level h-spacethatappearedmore\nspreadoutandmoreuniform,withlessofagapbetweentheregionscorresponding\ntodi\ufb00erentmodes(categories,intheexperiments).TraininganRBMinthat\nhigher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremains\nhoweverunclearhowtoexploitthisobservationtohelpbettertrainandsample\nfromdeepgenerativemodels.\nDespitethedi\ufb03cultyofmixing,MonteCarlotechniquesareusefulandare\noftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfront\ntheintractablepartitionfunctionofundirectedmodels,discussednext.\n6 0 4", "C h a p t e r 1 8\nC on f ron t i n g t h e P art i t i on\nF u n ct i on\nInsectionwesawthatmanyprobabilisticmodels(commonlyknownasundi- 16.2.2\nrectedgraphicalmodels)arede\ufb01nedbyanunnormalized probabilitydistribution\n\u02dc p(x; \u03b8).Wemustnormalize \u02dc pbydividingbyapartitionfunction Z( \u03b8)inorderto\nobtainavalidprobabilitydistribution:\np(;) =x \u03b81\nZ() \u03b8\u02dc p . (;)x \u03b8 (18.1)\nThepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscrete\nvariables)overtheunnormalized probabilityofallstates:\n\ue05a\n\u02dc p d() x x (18.2)\nor \ue058\nx\u02dc p .() x (18.3)\nThisoperationisintractableformanyinterestingmodels.\nAswewillseeinchapter,severaldeeplearningmodelsaredesignedto 20\nhaveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdo\nnotinvolvecomputing p(x)atall.\u00a0However,othermodelsdirectlyconfrontthe\nchallengeofintractablepartitionfunctions.Inthischapter,wedescribetechniques\nusedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions.\n605", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\n18.1TheLog-LikelihoodGradient\nWhat\u00a0makes\u00a0learning\u00a0undirectedmodels\u00a0bymaximumlikelihood\u00a0particularly\ndi\ufb03cultisthatthepartitionfunctiondependsontheparameters.Thegradientof\nthelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothe\ngradientofthepartitionfunction:\n\u2207 \u03b8log(;) = px \u03b8 \u2207 \u03b8log \u02dc p(;)x \u03b8\u2212\u2207 \u03b8log() Z \u03b8 .(18.4)\nThisisawell-knowndecompositionintothe p o si t i v e phaseand negat i v e\nphaseoflearning.\nFormostundirectedmodelsofinterest,thenegativephaseisdi\ufb03cult.Models\nwithnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypically\nhaveatractablepositivephase.Thequintessentialexampleofamodelwitha\nstraightforwardpositivephaseanddi\ufb03cultnegativephaseistheRBM,whichhas\nhiddenunitsthatareconditionallyindependentfromeachothergiventhevisible\nunits.Thecasewherethepositivephaseisdi\ufb03cult,withcomplicatedinteractions\nbetweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses 19\nonthedi\ufb03cultiesofthenegativephase.\nLetuslookmorecloselyatthegradientof: log Z\n\u2207 \u03b8log Z (18.5)\n=\u2207 \u03b8 Z\nZ(18.6)\n=\u2207 \u03b8\ue050\nx\u02dc p()x\nZ(18.7)\n=\ue050\nx\u2207 \u03b8\u02dc p()x\nZ. (18.8)\nFormodelsthatguarantee p(x) >0forallx,wecansubstitute exp(log \u02dc p())x\nfor\u02dc p()x:\ue050\nx\u2207 \u03b8exp(log \u02dc p())x\nZ(18.9)\n=\ue050\nxexp(log \u02dc p())x\u2207 \u03b8log \u02dc p()x\nZ(18.10)\n=\ue050\nx\u02dc p()x\u2207 \u03b8log \u02dc p()x\nZ(18.11)\n=\ue058\nxp()x\u2207 \u03b8log \u02dc p()x (18.12)\n606", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\n= E x x \u223c p ( )\u2207 \u03b8log \u02dc p .()x (18.13)\nThisderivationmadeuseofsummationoverdiscrete x,butasimilarresult\nappliesusingintegrationovercontinuous x.Inthecontinuousversionofthe\nderivation,weuseLeibniz\u2019srulefordi\ufb00erentiationundertheintegralsigntoobtain\ntheidentity\n\u2207 \u03b8\ue05a\n\u02dc p d()x x=\ue05a\n\u2207 \u03b8\u02dc p d . ()x x (18.14)\nThisidentityisapplicableonlyundercertainregularityconditionson\u02dc pand\u2207 \u03b8\u02dc p(x).\nInmeasuretheoreticterms,theconditionsare:(i)Theunnormalized distribution\u02dc p\nmustbeaLebesgue-integrablefunctionof xforeveryvalueof \u03b8;(ii)Thegradient\n\u2207 \u03b8\u02dc p(x)mustexistforall \u03b8andalmostall x;(iii)Theremustexistanintegrable\nfunction R( x)thatbounds \u2207 \u03b8\u02dc p(x)inthesensethatmax i|\u2202\n\u2202 \u03b8 i\u02dc p(x)|\u2264 R( x)forall\n\u03b8andalmostall x.Fortunately,mostmachinelearningmodelsofinteresthave\ntheseproperties.\nThisidentity\n\u2207 \u03b8log = Z E x x \u223c p ( )\u2207 \u03b8log \u02dc p()x (18.15)\nisthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizing\nthelikelihoodofmodelswithintractablepartitionfunctions.\nTheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitive\nframeworkinwhichwecanthinkofboththepositivephaseandthenegative\nphase.Inthepositivephase,weincreaselog \u02dc p(x)for xdrawnfromthedata.In\nthenegativephase,wedecreasethepartitionfunctionbydecreasinglog \u02dc p(x) drawn\nfromthemodeldistribution.\nInthedeeplearningliterature,itiscommontoparametrize log \u02dc pintermsof\nanenergyfunction(equation).Inthiscase,wecaninterpretthepositive 16.7\nphaseaspushingdownontheenergyoftrainingexamplesandthenegativephase\naspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedin\n\ufb01gure.18.1\n18.2StochasticMaximumLikelihoodandContrastive\nDivergence\nThenaivewayofimplementing equation istocomputeitbyburningin 18.15\nasetofMarkovchainsfromarandominitialization everytimethegradientis\nneeded.Whenlearningisperformedusingstochasticgradientdescent,thismeans\nthechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe\n607", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\ntrainingprocedurepresentedinalgorithm .Thehighcostofburninginthe 18.1\nMarkovchainsintheinnerloopmakesthisprocedurecomputationally infeasible,\nbutthisprocedureisthestartingpointthatothermorepracticalalgorithmsaim\ntoapproximate.\nAl g o r i t hm 1 8 . 1AnaiveMCMCalgorithmformaximizingthelog-likelihood\nwithanintractablepartitionfunctionusinggradientascent.\nSet,thestepsize,toasmallpositivenumber. \ue00f\nSet k,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100to\ntrainanRBMonasmallimagepatch.\nwhi l enotconverged do\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\ng\u21901\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(x( ) i;) \u03b8.\nInitializeasetof msamples {\u02dcx( 1 ), . . . ,\u02dcx( ) m}torandomvalues(e.g.,from\nauniformornormaldistribution,orpossiblyadistributionwithmarginals\nmatchedtothemodel\u2019smarginals).\nf o r do i k = 1to\nf o r do j m = 1to\n\u02dcx( ) j\u2190gibbs_update(\u02dcx( ) j) .\ne nd f o r\ne nd f o r\ngg\u2190\u22121\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(\u02dcx( ) i;) \u03b8 .\n\u03b8 \u03b8\u2190 + \ue00f .g\ne nd whi l e\nWecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachieve\nbalancebetweentwoforces,onepushinguponthemodeldistributionwherethe\ndataoccurs,andanotherpushingdownonthemodeldistributionwherethemodel\nsamplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto 18.1\nmaximizing log \u02dc pandminimizing log Z.Severalapproximations tothenegative\nphasearepossible.Eachoftheseapproximationscanbeunderstoodasmaking\nthenegativephasecomputationally cheaperbutalsomakingitpushdowninthe\nwronglocations.\nBecausethenegativephaseinvolvesdrawingsamplesfromthemodel\u2019sdistri-\nbution,wecanthinkofitas\ufb01ndingpointsthatthemodelbelievesinstrongly.\nBecausethenegativephaseactstoreducetheprobabilityofthosepoints,they\naregenerallyconsideredtorepresentthemodel\u2019sincorrectbeliefsabouttheworld.\nTheyarefrequentlyreferredtointheliteratureas\u201challucinations\u201d or\u201cfantasy\nparticles.\u201dInfact,thenegativephasehasbeenproposedasapossibleexplanation\n608", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nxp(x )The p o s i t i v e p h a s e\np mo d e l ( ) x\np d a t a ( ) x\nxp(x )The n eg a t i v e p h a s e\np mo d e l ( ) x\np d a t a ( ) x\nFigure18.1:Theviewofalgorithmashavinga\u201cpositivephase\u201dand\u201cnegativephase.\u201d 18.1\n( L e f t )Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupon\ntheirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpushed\nuponmore. ( R i g h t )Inthenegativephase,wesamplepointsfromthemodeldistribution,\nandpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase\u2019s\ntendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.When\nthedatadistributionandthemodeldistributionareequal,thepositivephasehasthe\nsamechancetopushupatapointasthenegativephasehastopushdown.Whenthis\noccurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate.\nfordreaminginhumansandotheranimals(CrickandMitchison1983,),theidea\nbeingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollows\nthegradientoflog \u02dc pwhileexperiencingrealeventswhileawakeandfollowsthe\nnegativegradientoflog \u02dc ptominimize log Zwhilesleepingandexperiencingevents\nsampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedto\ndescribealgorithmswithapositiveandnegativephase,butithasnotbeenproven\ntobecorrectwithneuroscienti\ufb01cexperiments.Inmachinelearningmodels,itis\nusuallynecessarytousethepositiveandnegativephasesimultaneously,rather\nthaninseparatetimeperiodsofwakefulnessandREMsleep.\u00a0Aswewillseein\nsection,othermachinelearningalgorithmsdrawsamplesfromthemodel 19.5\ndistributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccount\nforthefunctionofdreamsleep.\nGiventhisunderstandingoftheroleofthepositiveandnegativephaseof\nlearning,wecanattempttodesignalessexpensivealternativetoalgorithm .18.1\nThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkov\nchainsfromarandominitialization ateachstep.\u00a0Anaturalsolutionistoinitialize\ntheMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,\n609", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nsothattheburninoperationdoesnottakeasmanysteps.\nThe c o n t r ast i v e di v e r g e nc e(CD,orCD- ktoindicateCDwith kGibbssteps)\nalgorithminitializestheMarkovchainateachstepwithsamplesfromthedata\ndistribution(Hinton20002010,,).Thisapproachispresentedasalgorithm .18.2\nObtainingsamplesfromthedatadistributionisfree,becausetheyarealready\navailableinthedataset.Initially,thedatadistributionisnotclosetothemodel\ndistribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositive\nphasecanstillaccuratelyincreasethemodel\u2019sprobabilityofthedata.Afterthe\npositivephasehashadsometimetoact,themodeldistributionisclosertothe\ndatadistribution,andthenegativephasestartstobecomeaccurate.\nAl g o r i t hm 1 8 . 2Thecontrastivedivergencealgorithm,usinggradientascentas\ntheoptimization procedure.\nSet,thestepsize,toasmallpositivenumber. \ue00f\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\nfrom p(x; \u03b8)tomixwheninitializedfrom pdata.Perhaps1-20totrainanRBM\nonasmallimagepatch.\nwhi l enotconverged do\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\ng\u21901\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(x( ) i;) \u03b8 .\nf o r do i m = 1to\n\u02dcx( ) i\u2190x( ) i.\ne nd f o r\nf o r do i k = 1to\nf o r do j m = 1to\n\u02dcx( ) j\u2190gibbs_update(\u02dcx( ) j) .\ne nd f o r\ne nd f o r\ngg\u2190\u22121\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(\u02dcx( ) i;) \u03b8 .\n\u03b8 \u03b8\u2190 + \ue00f .g\ne nd whi l e\nOfcourse,CDisstillanapproximation tothecorrectnegativephase.The\nmainwaythatCDqualitativelyfailstoimplementthecorrectnegativephase\nisthatitfailstosuppressregionsofhighprobabilitythatarefarfromactual\ntrainingexamples.Theseregionsthathavehighprobabilityunderthemodelbut\nlowprobabilityunderthedatageneratingdistributionarecalled spur i o us m o des.\nFigureillustrateswhythishappens.Essentially,itisbecausemodesinthe 18.2\nmodeldistributionthatarefarfromthedatadistributionwillnotbevisitedby\n610", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nxp(x )p mo d e l ( ) x\np d a t a ( ) x\nFigure18.2:\u00a0Anillustrationofhowthenegativephaseofcontrastivedivergence(algo-\nrithm)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatis 18.2\npresentinthemodeldistributionbutabsentinthedatadistribution.Becausecontrastive\ndivergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainfor\nonlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedata\npoints.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamples\nthatdonotresemblethedata.Italsomeansthatduetowastingsomeofitsprobability\nmassonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrect\nmodes.Forthepurposeofvisualization,this\ufb01gureusesasomewhatsimpli\ufb01edconcept\nofdistance\u2014thespuriousmodeisfarfromthecorrectmodealongthenumberlinein\nR.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasingle x\nvariablein R.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbs\nsamplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallof\nthevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheedit\ndistancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceina\nhighdimensionalspaceisdi\ufb03culttodepictina2-Dplot.\nMarkovchainsinitializedattrainingpoints,unlessisverylarge. k\nCarreira-Perpi\u00f1anandHinton2005()showed\u00a0experimentallythatthe\u00a0CD\nestimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatit\nconvergestodi\ufb00erentpointsthanthemaximumlikelihoodestimator.Theyargue\nthatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitialize\namodelthatcouldlaterbe\ufb01ne-tunedviamoreexpensiveMCMCmethods.Bengio\nandDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallest\ntermsofthecorrectMCMCupdategradient,whichexplainsthebias.\nCDisusefulfortrainingshallowmodelslikeRBMs.Thesecaninturnbe\nstackedtoinitializedeepermodelslikeDBNsorDBMs.\u00a0However,CDdoesnot\nprovidemuchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdi\ufb03cult\n611", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\ntoobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethe\nhiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannot\nsolvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstill\nneedtoburninaMarkovchainsamplingfromthedistributionoverthehidden\nunitsconditionedonthosevisiblesamples.\nTheCDalgorithmcanbethoughtofaspenalizingthemodelforhavinga\nMarkovchainthatchangestheinputrapidlywhentheinputcomesfromthedata.\nThismeanstrainingwithCDsomewhatresemblesautoencodertraining.Even\nthoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbe\nusefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecause\ntheearliestmodelsinthestackareencouragedtocopymoreinformationupto\ntheirlatentvariables,therebymakingitavailabletothelatermodels.Thisshould\nbethoughtofmoreofasanoften-exploitable sidee\ufb00ectofCDtrainingratherthan\naprincipleddesignadvantage.\nSutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthe\ngradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,\nbutinpracticethisisnotaseriousproblem.\nAdi\ufb00erentstrategythatresolvesmanyoftheproblemswithCDistoinitial-\nizetheMarkovchainsateachgradientstepwiththeirstatesfromtheprevious\ngradientstep.Thisapproachwas\ufb01rstdiscoveredunderthename st o c hast i c m ax -\ni m um l i k e l i ho o d(SML)intheappliedmathematics andstatisticscommunity\n(Younes1998,)andlaterindependently rediscoveredunderthename p e r si st e n t\nc o n t r ast i v e di v e r g e n c e(PCD,orPCD- ktoindicatetheuseof kGibbssteps\nperupdate)inthedeeplearningcommunity(,).Seealgorithm . Tieleman2008 18.3\nThebasicideaofthisapproachisthat,solongasthestepstakenbythestochastic\ngradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilar\ntothemodelfromthecurrentstep.Itfollowsthatthesamplesfromtheprevious\nmodel\u2019sdistributionwillbeveryclosetobeingfairsamplesfromthecurrent\nmodel\u2019sdistribution,soaMarkovchaininitializedwiththesesampleswillnot\nrequiremuchtimetomix.\nBecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearning\nprocess,ratherthanrestartedateachgradientstep,thechainsarefreetowander\nfarenoughto\ufb01ndallofthemodel\u2019smodes.SMListhusconsiderablymore\nresistanttoformingmodelswithspuriousmodesthanCDis.Moreover,because\nitispossibletostorethestateofallofthesampledvariables,whethervisibleor\nlatent,SMLprovidesaninitialization pointforboththehiddenandvisibleunits.\nCDisonlyabletoprovideaninitialization forthevisibleunits,andtherefore\nrequiresburn-infordeepmodels.SMLisabletotraindeepmodelse\ufb03ciently.\n612", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nMarlin 2010 e t a l .()comparedSMLtomanyoftheothercriteriapresentedin\nthischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodfor\nanRBM,andthatiftheRBM\u2019shiddenunitsareusedasfeaturesforanSVM\nclassi\ufb01er,SMLresultsinthebestclassi\ufb01cationaccuracy.\nSMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithm\ncanmovethemodelfasterthantheMarkovchaincanmixbetweensteps.This\ncanhappenif kistoosmallor \ue00fistoolarge.Thepermissiblerangeofvaluesis\nunfortunately highlyproblem-dependent.Thereisnoknownwaytotestformally\nwhetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearning\nrateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeable\ntoobservethatthereismuchmorevarianceinthenegativephasesamplesacross\ngradientstepsratherthanacrossdi\ufb00erentMarkovchains.Forexample,amodel\ntrainedonMNISTmightsampleexclusively7sononestep.Thelearningprocess\nwillthenpushdownstronglyonthemodecorrespondingto7s,andthemodel\nmightsampleexclusively9sonthenextstep.\nAl g o r i t hm 1 8 . 3Thestochasticmaximumlikelihood/persistentcontrastive\ndivergencealgorithmusinggradientascentastheoptimization procedure.\nSet,thestepsize,toasmallpositivenumber. \ue00f\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\nfrom p(x; \u03b8+ \ue00fg)toburnin,startingfromsamplesfrom p(x; \u03b8).Perhaps1for\nRBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM.\nInitializeasetof msamples {\u02dcx( 1 ), . . . ,\u02dcx( ) m}torandomvalues(e.g.,froma\nuniformornormaldistribution,orpossiblyadistributionwithmarginalsmatched\ntothemodel\u2019smarginals).\nwhi l enotconverged do\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\ng\u21901\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(x( ) i;) \u03b8 .\nf o r do i k = 1to\nf o r do j m = 1to\n\u02dcx( ) j\u2190gibbs_update(\u02dcx( ) j) .\ne nd f o r\ne nd f o r\ngg\u2190\u22121\nm\ue050m\ni = 1\u2207 \u03b8log \u02dc p(\u02dcx( ) i;) \u03b8 .\n\u03b8 \u03b8\u2190 + \ue00f .g\ne nd whi l e\nCaremustbetakenwhenevaluatingthesamplesfromamodeltrainedwith\nSML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain\n613", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\ninitializedfromarandomstartingpointafterthemodelisdonetraining.The\nsamplespresentinthepersistentnegativechainsusedfortraininghavebeen\nin\ufb02uencedbyseveralrecentversionsofthemodel,andthuscanmakethemodel\nappeartohavegreatercapacitythanitactuallydoes.\nBerglundandRaiko2013()performedexperimentstoexaminethebiasand\nvarianceintheestimateofthegradientprovidedbyCDandSML.CDprovesto\nhavelowervariancethantheestimatorbasedonexactsampling.SMLhashigher\nvariance.ThecauseofCD\u2019slowvarianceisitsuseofthesametrainingpoints\ninboththepositiveandnegativephase.Ifthenegativephaseisinitializedfrom\ndi\ufb00erenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedon\nexactsampling.\nAllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodel\ncaninprinciplebeusedwithalmostanyvariantofMCMC.Thismeansthat\ntechniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMC\ntechniquesdescribedinchapter,suchasparalleltempering( , 17 Desjardins e t a l .\n2010Cho2010; e t a l .,).\nOneapproachtoacceleratingmixingduringlearningreliesnotonchanging\ntheMonteCarlosamplingtechnologybutratheronchangingtheparametrization\nofthemodelandthecostfunction. F ast P CDorFPCD( , TielemanandHinton\n2009)involvesreplacingtheparameters \u03b8ofatraditionalmodelwithanexpression\n\u03b8 \u03b8= ( )slow+ \u03b8( )fast. (18.16)\nTherearenowtwiceasmanyparametersasbefore,andtheyareaddedtogether\nelement-wisetoprovidetheparametersusedbytheoriginalmodelde\ufb01nition.The\nfastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowing\nittoadaptrapidlyinresponsetothenegativephaseoflearningandpushthe\nMarkovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,though\nthise\ufb00ectonlyoccursduringlearningwhilethefastweightsarefreetochange.\nTypicallyonealsoappliessigni\ufb01cantweightdecaytothefastweights,encouraging\nthemtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslong\nenoughtoencouragetheMarkovchaintochangemodes.\nOnekeybene\ufb01ttotheMCMC-basedmethodsdescribedinthissectionisthat\ntheyprovideanestimateofthegradientoflog Z,andthuswecanessentially\ndecomposetheproblemintothelog \u02dc pcontributionandthelog Zcontribution.\nWecanthenuseanyothermethodtotacklelog \u02dc p(x),andjustaddournegative\nphasegradientontotheothermethod\u2019sgradient.Inparticular,thismeansthat\nourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon\n\u02dc p.Mostoftheothermethodsofdealingwith log Zpresentedinthischapterare\n614", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nincompatible withbound-basedpositivephasemethods.\n18.3Pseudolikelihood\nMonteCarloapproximations tothepartitionfunctionanditsgradientdirectly\nconfrontthepartitionfunction.Otherapproachessidesteptheissue,bytraining\nthemodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesare\nbasedontheobservationthatitiseasytocomputeratiosofprobabilities inan\nundirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsin\nboththenumeratorandthedenominator oftheratioandcancelsout:\np()x\np()y=1\nZ\u02dc p()x\n1\nZ\u02dc p()y=\u02dc p()x\n\u02dc p()y. (18.17)\nThepseudolikelihoodisbasedontheobservationthatconditionalprobabilities\ntakethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthe\npartitionfunction.Supposethatwepartition xintoa,bandc,where acontains\nthevariableswewantto\ufb01ndtheconditionaldistributionover,bcontainsthe\nvariableswewanttoconditionon,andccontainsthevariablesthatarenotpart\nofourquery.\np( ) = ab|p ,(ab)\np()b=p ,(ab)\ue050\na c , p , ,(abc)=\u02dc p ,(ab)\ue050\na c ,\u02dc p , ,(abc).(18.18)\nThisquantityrequiresmarginalizing outa,whichcanbeaverye\ufb03cientoperation\nprovidedthataandcdonotcontainverymanyvariables.Intheextremecase,a\ncanbeasinglevariableandccanbeempty,makingthisoperationrequireonlyas\nmanyevaluationsof\u02dc pastherearevaluesofasinglerandomvariable.\nUnfortunately,inordertocomputethelog-likelihood,weneedtomarginalize\noutlargesetsofvariables.Ifthereare nvariablestotal,wemustmarginalizeaset\nofsize.Bythechainruleofprobability, n\u22121\nlog() = log( px p x 1)+log( p x 2| x 1)+ +( \u00b7\u00b7\u00b7 p x n|x 1 : 1 n \u2212) .(18.19)\nInthiscase,wehavemade amaximallysmall,butccanbeaslargeasx 2 : n.What\nifwesimplymovecintobtoreducethecomputational cost?Thisyieldsthe\npseudolik e l i ho o d(,)objectivefunction,basedonpredictingthevalue Besag1975\noffeature x igivenalloftheotherfeatures x \u2212 i:\nn \ue058\ni = 1log( p x i| x \u2212 i) . (18.20)\n615", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nIfeachrandomvariablehas kdi\ufb00erentvalues,thisrequiresonly k n\u00d7evaluations\nof\u02dc ptocompute,asopposedtothe knevaluationsneededtocomputethepartition\nfunction.\nThismaylooklikeanunprincipled hack,butitcanbeproventhatestimation\nbymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995\nOfcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,\npseudolikelihoodmaydisplaydi\ufb00erentbehaviorfromthemaximumlikelihood\nestimator.\nItispossibletotradecomputational complexityfordeviationfrommaximum\nlikelihoodbehaviorbyusingthe g e ner al i z e d pseudolikel i h o o destimator(Huang\nandOgata2002,).Thegeneralizedpseudolikelihoodestimatoruses mdi\ufb00erentsets\nS( ) i, i= 1 , . . . , mofindicesofvariablesthatappeartogetherontheleftsideofthe\nconditioningbar.Intheextremecaseof m= 1and S( 1 )=1 , . . . , nthegeneralized\npseudolikelihoodrecoversthelog-likelihood.\u00a0Intheextremecaseof m= nand\nS( ) i={} i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.The\ngeneralizedpseudolikelihoodobjectivefunctionisgivenby\nm \ue058\ni = 1log( pxS() i|x\u2212 S() i) . (18.21)\nTheperformanceofpseudolikelihood-basedapproachesdependslargelyonhow\nthemodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthat\nrequireagoodmodelofthefulljoint p(x),suchasdensityestimationandsampling.\nHowever,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonly\ntheconditionaldistributionsusedduringtraining,suchas\ufb01llinginsmallamounts\nofmissingvalues.Generalizedpseudolikelihoodtechniquesareespeciallypowerfulif\nthedatahasregularstructurethatallowsthe Sindexsetstobedesignedtocapture\nthemostimportantcorrelationswhileleavingoutgroupsofvariablesthatonly\nhavenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidely\nseparatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihood\ncanbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow. S\nOneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwith\notherapproximationsthatprovideonlyalowerboundon\u02dc p(x),suchasvariational\ninference,whichwillbecoveredinchapter.Thisisbecause 19 \u02dc pappearsinthe\ndenominator. Alowerboundonthedenominator providesonlyanupperboundon\ntheexpressionasawhole,andthereisnobene\ufb01ttomaximizinganupperbound.\nThismakesitdi\ufb03culttoapplypseudolikelihoodapproachestodeepmodelssuch\nasdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominant\napproachestoapproximately marginalizing outthemanylayersofhiddenvariables\n616", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nthatinteractwitheachother.\u00a0However,pseudolikelihoodisstillusefulfordeep\nlearning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusing\napproximateinferencemethodsthatarenotbasedonlowerbounds.\nPseudolikelihoodhasamuchgreatercostpergradientstepthanSML,dueto\nitsexplicitcomputationofalloftheconditionals.However,generalizedpseudo-\nlikelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselected\nconditionaliscomputedperexample(Goodfellow2013b e t a l .,),therebybringing\nthecomputational costdowntomatchthatofSML.\nThoughthepseudolikelihoodestimatordoesnotexplicitlyminimize log Z,it\ncanstillbethoughtofashavingsomethingresemblinganegativephase.The\ndenominators ofeachconditionaldistributionresultinthelearningalgorithm\nsuppressingtheprobabilityofallstatesthathaveonlyonevariabledi\ufb00eringfrom\natrainingexample.\nSeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\ne\ufb03ciencyofpseudolikelihood.\n18.4ScoreMatchingandRatioMatching\nScorematching(,)providesanotherconsistentmeansoftraininga Hyv\u00e4rinen2005\nmodelwithoutestimating Zoritsderivatives.Thenamescorematchingcomes\nfromterminologyinwhichthederivativesofalogdensitywithrespecttoits\nargument,\u2207 xlog p( x),arecalledits sc o r e.Thestrategyusedbyscorematching\nistominimizetheexpectedsquareddi\ufb00erencebetweenthederivativesofthe\nmodel\u2019slogdensitywithrespecttotheinputandthederivativesofthedata\u2019slog\ndensitywithrespecttotheinput:\nL ,( x \u03b8) =1\n2||\u2207 xlog p m o de l(;) x \u03b8\u2212\u2207 xlog pdata() x||2\n2(18.22)\nJ() = \u03b81\n2E pdata ( ) x L ,( x \u03b8) (18.23)\n\u03b8\u2217= min\n\u03b8J() \u03b8 (18.24)\nThisobjectivefunctionavoidsthedi\ufb03cultiesassociatedwithdi\ufb00erentiating\nthepartitionfunction Zbecause Zisnotafunctionof xandtherefore \u2207 x Z= 0.\nInitially,scorematchingappearstohaveanewdi\ufb03culty:\u00a0comput ingthescore\nofthedatadistributionrequiresknowledgeofthetruedistributiongenerating\nthetrainingdata, pdata.Fortunately,minimizingtheexpectedvalueofis L ,( x \u03b8)\n617", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nequivalenttominimizingtheexpectedvalueof\n\u02dc L ,( x \u03b8) =n \ue058\nj = 1\ue020\n\u22022\n\u2202 x2\njlog p m o de l(;)+ x \u03b81\n2\ue012\u2202\n\u2202 x jlog p m o de l(;) x \u03b8\ue0132\ue021\n(18.25)\nwhereisthedimensionalityof. n x\nBecausescorematchingrequirestakingderivativeswithrespecttox,itisnot\napplicabletomodelsofdiscretedata.However,thelatentvariablesinthemodel\nmaybediscrete.\nLikethepseudolikelihood,scorematchingonlyworkswhenweareableto\nevaluate log \u02dc p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethods\nthatonlyprovidealowerboundonlog \u02dc p(x),becausescorematchingrequires\nthederivativesandsecondderivativesoflog \u02dc p(x)andalowerboundconveysno\ninformationaboutitsderivatives.Thismeansthatscorematchingcannotbe\nappliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehidden\nunits,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescore\nmatchingcanbeusedtopretrainthe\ufb01rsthiddenlayerofalargermodel,ithas\nnotbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel.\nThisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsome\ndiscretevariables.\nWhilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbe\nviewedasaversionofcontrastivedivergenceusingaspeci\ufb01ckindofMarkovchain\n(,).TheMarkovchaininthiscaseisnotGibbssampling,but Hyv\u00e4rinen2007a\nratheradi\ufb00erentapproachthatmakeslocalmovesguidedbythegradient.Score\nmatchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthe\nlocalmovesapproacheszero.\nLyu2009()generalizedscorematchingtothediscretecase(butmadeanerror\nintheirderivationthatwascorrectedby ()). Marlin e t a l .2010Marlin e t a l .\n()foundthat 2010 g e ner al i z e d sc o r e m at c hi ng(GSM)doesnotworkinhigh\ndimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0.\nAmoresuccessfulapproachtoextendingthebasicideasofscorematching\ntodiscretedatais r at i o m at c hi ng(,).Ratiomatchingapplies Hyv\u00e4rinen2007b\nspeci\ufb01callytobinarydata.Ratiomatchingconsistsofminimizingtheaverageover\nexamplesofthefollowingobjectivefunction:\nL( )RM() = x \u03b8 ,n \ue058\nj = 1\uf8eb\n\uf8ed1\n1+pmodel ( ; ) x \u03b8\npmodel ( ( ) ) ; ) f x , j \u03b8\uf8f6\n\uf8f82\n,(18.26)\n618", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nwhere returnswiththebitatposition\ufb02ipped.Ratiomatchingavoids f , j( x) x j\nthepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:ina\nratiooftwoprobabilities, thepartitionfunctioncancelsout. () Marlin e t a l .2010\nfoundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMinterms\noftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages.\nLikethepseudolikelihoodestimator,ratiomatchingrequires nevaluationsof\u02dc p\nperdatapoint,makingitscomputational costperupdateroughly ntimeshigher\nthanthatofSML.\nAswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofas\npushingdownonallfantasystatesthathaveonlyonevariabledi\ufb00erentfroma\ntrainingexample.Sinceratiomatchingappliesspeci\ufb01callytobinarydata,this\nmeansthatitactsonallfantasystateswithinHammingdistance1ofthedata.\nRatiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensional\nsparsedata,suchaswordcountvectors.Thiskindofdataposesachallengefor\nMCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentin\ndenseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodel\nhaslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio\n()overcamethisissuebydesigninganunbiasedstochasticapproximation to 2013\nratiomatching.Theapproximation evaluatesonlyarandomlyselectedsubsetof\nthetermsoftheobjective,anddoesnotrequirethemodeltogeneratecomplete\nfantasysamples.\nSeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\ne\ufb03ciencyofratiomatching.\n18.5DenoisingScoreMatching\nInsomecaseswemaywishtoregularizescorematching,by\ufb01ttingadistribution\npsmoothed() = x\ue05a\npdata()( ) y q x y| d y (18.27)\nratherthanthetrue pdata.Thedistribution q( x y|) isacorruptionprocess,usually\nonethatformsbyaddingasmallamountofnoiseto. x y\nDenoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydo\nnothaveaccesstothetrue pdatabutratheronlyanempiricaldistributionde\ufb01ned\nbysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,make\np m o de lintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothing\nby qhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty\n619", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\ndescribedinsection. ()introducedaprocedurefor 5.4.5KingmaandLeCun2010\nperformingregularizedscorematchingwiththesmoothingdistribution qbeing\nnormallydistributednoise.\nRecallfromsectionthatseveralautoencodertrainingalgorithmsare 14.5.1\nequivalenttoscorematchingordenoisingscorematching.Theseautoencoder\ntrainingalgorithmsare\u00a0therefore a\u00a0wayof\u00a0overcomingthe\u00a0partition function\nproblem.\n18.6Noise-ContrastiveEstimation\nMosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonot\nprovideanestimateofthepartitionfunction.SMLandCDestimateonlythe\ngradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself.\nScorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothe\npartitionfunctionaltogether.\nNoi se - c o n t r ast i v e \u00a0 e st i m a t i o n \u00a0 ( N C E )(Gutmann\u00a0and\u00a0Hy varinen2010,\u00a0)\ntakesadi\ufb00erentstrategy.Inthisapproach,theprobabilitydistributionestimated\nbythemodelisrepresentedexplicitlyas\nlog p m o de l() = log \u02dc x pmodel(;)+x \u03b8 c , (18.28)\nwhere cisexplicitlyintroducedasanapproximationof\u2212log Z( \u03b8).Ratherthan\nestimatingonly \u03b8,thenoisecontrastiveestimationproceduretreats casjust\nanotherparameterandestimates \u03b8and csimultaneously,usingthesamealgorithm\nforboth.Theresulting log p m o de l(x)thusmaynotcorrespondexactlytoavalid\nprobabilitydistribution,butwillbecomecloserandclosertobeingvalidasthe\nestimateofimproves. c1\nSuchanapproachwouldnotbepossibleusingmaximumlikelihoodasthe\ncriterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetoset\nc c arbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution.\nNCEworksbyreducingtheunsupervisedlearningproblemofestimating p(x)\ntothatoflearningaprobabilisticbinaryclassi\ufb01erinwhichoneofthecategories\ncorrespondstothedatageneratedbythemodel.Thissupervisedlearningproblem\nisconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised\n1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisno\nneedtointroducetheextraparameter c.However,ithasgeneratedthemostinterestasameans\nofestimatingmodelswithdi\ufb03cultpartitionfunctions.\n620", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nlearningproblemde\ufb01nesanasymptoticallyconsistentestimatoroftheoriginal\nproblem.\nSpeci\ufb01cally,weintroduceaseconddistribution,the noi se di st r i but i o n pnoise(x).\nThenoisedistributionshouldbetractabletoevaluateandtosamplefrom.\u00a0We\ncannowconstructamodeloverbothxandanew,binaryclassvariable y.Inthe\nnewjointmodel,wespecifythat\npjoint(= 1) = y1\n2, (18.29)\npjoint( = 1) = x| y p m o de l()x , (18.30)\nand\npjoint( = 0) = x| y pnoise()x . (18.31)\nInotherwords, yisaswitchvariablethatdetermineswhetherwewillgenerate x\nfromthemodelorfromthenoisedistribution.\nWecanconstructasimilarjointmodeloftrainingdata.Inthiscase,the\nswitchvariabledetermineswhetherwedraw xfromthe dat aorfromthenoise\ndistribution.Formally, ptrain( y=1)=1\n2, ptrain(x| y=1)= pdata(x),\u00a0and\nptrain( = 0) = x| y pnoise()x.\nWecannowjustusestandardmaximumlikelihoodlearningonthe sup e r v i se d\nlearningproblemof\ufb01tting pjointto ptrain:\n\u03b8 , c= argmax\n\u03b8 , cE x , py \u223ctrainlog pjoint( ) y|x . (18.32)\nThedistribution pjointisessentiallyalogisticregressionmodelappliedtothe\ndi\ufb00erenceinlogprobabilities ofthemodelandthenoisedistribution:\npjoint(= 1 ) = y |xp m o de l()x\np m o de l()+x pnoise()x(18.33)\n=1\n1+pnoise ( ) x\npmodel ( ) x(18.34)\n=1\n1+exp\ue010\nlogpnoise ( ) x\npmodel ( ) x\ue011 (18.35)\n= \u03c3\ue012\n\u2212logpnoise()x\np m o de l()x\ue013\n(18.36)\n= (log \u03c3 p m o de l()log x\u2212 pnoise())x . (18.37)\n621", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nNCEisthussimpletoapplysolongaslog \u02dc pmodeliseasytoback-propagate\nthrough,and,asspeci\ufb01edabove, pnoiseiseasytoevaluate(inordertoevaluate\npjoint)andsamplefrom(inordertogeneratethetrainingdata).\nNCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,\nbutcanworkwellevenifthoserandomvariablescantakeonahighnumberof\nvalues.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditional\ndistributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,\n2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyone\nword.\nWhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomesless\ne\ufb03cient.Thelogisticregressionclassi\ufb01ercanrejectanoisesamplebyidentifying\nanyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdown\ngreatlyafter p m o de lhaslearnedthebasicmarginalstatistics.Imaginelearninga\nmodelofimagesoffaces,usingunstructuredGaussiannoiseas pnoise.If p m o de l\nlearnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithout\nhavinglearnedanythingaboutotherfacialfeatures,suchasmouths.\nTheconstraintthat pnoisemustbeeasytoevaluateandeasytosamplefrom\ncanbeoverlyrestrictive.When pnoiseissimple,mostsamplesarelikelytobetoo\nobviouslydistinctfromthedatatoforce p m o de ltoimprovenoticeably.\nLikescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalower\nboundon\u02dc pisavailable.Suchalowerboundcouldbeusedtoconstructalower\nboundon pjoint( y= 1|x),butitcanonlybeusedtoconstructanupperboundon\npjoint( y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise,\nalowerboundon pnoiseisnotuseful,becauseitprovidesonlyanupperboundon\npjoint(= 1 ) y |x.\nWhenthemodeldistributioniscopiedtode\ufb01neanewnoisedistributionbefore\neachgradientstep,NCEde\ufb01nesaprocedurecalled se l f - c o n t r ast i v e e st i m at i o n,\nwhose\u00a0expected\u00a0gradientis\u00a0equivalentto\u00a0the\u00a0expected\u00a0gradientofmaximum\nlikelihood(,).ThespecialcaseofNCEwherethenoisesamples Goodfellow2014\nare\u00a0thosegenerated\u00a0by\u00a0themodel\u00a0suggests\u00a0thatmaximumlikelihood\u00a0can\u00a0be\ninterpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguish\nrealityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachieves\nsomereducedcomputational costbyonlyforcingthemodeltodistinguishreality\nfroma\ufb01xedbaseline(thenoisemodel).\nUsingthesupervisedtaskofclassifyingbetweentrainingsamplesandgenerated\nsamples(withthemodelenergyfunctionusedinde\ufb01ningtheclassi\ufb01er)toprovide\nagradientonthemodelwasintroducedearlierinvariousforms(Welling e t a l .,\n2003bBengio2009;,).\n622", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nNoise\u00a0contrastiveestimation\u00a0is\u00a0basedon\u00a0the\u00a0idea that\u00a0agood\u00a0generative\nmodelshould\u00a0be\u00a0abletodistinguish\u00a0datafromnoise.Aclosely\u00a0relatedidea\nisthat\u00a0agood\u00a0generativemodelshould\u00a0beabletogenerate\u00a0samples thatno\nclassi\ufb01ercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks\n(section).20.10.4\n18.7EstimatingthePartitionFunction\nWhilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneeding\ntocomputetheintractablepartitionfunction Z( \u03b8)associatedwithanundirected\ngraphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimating\nthepartitionfunction.\nEstimatingthepartitionfunctioncanbeimportantbecausewerequireitif\nwewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantin\ne v a l u a t i ngthemodel,monitoringtrainingperformance,andcomparingmodelsto\neachother.\nForexample,imaginewehavetwomodels:model M Ade\ufb01ningaprobabil-\nitydistribution p A(x; \u03b8 A)=1\nZ A\u02dc p A(x; \u03b8 A)andmodelM Bde\ufb01ningaprobability\ndistribution p B(x; \u03b8 B)=1\nZ B\u02dc p B(x; \u03b8 B).Acommonwaytocomparethemodels\nistoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d.\ntestdataset.Supposethetestsetconsistsof mexamples { x( 1 ), . . . , x( ) m}.If\ue051\ni p A(x( ) i; \u03b8 A) >\ue051\ni p B(x( ) i; \u03b8 B)orequivalentlyif\n\ue058\nilog p A(x( ) i; \u03b8 A)\u2212\ue058\nilog p B(x( ) i; \u03b8 B) 0 > ,(18.38)\nthenwesaythatM AisabettermodelthanM B(or,atleast,itisabettermodel\nofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,\ntestingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction.\nUnfortunately,equationseemstorequireevaluatingthelogprobabilitythat 18.38\nthemodelassignstoeachpoint,whichinturnrequiresevaluatingthepartition\nfunction.Wecansimplifythesituationslightlybyre-arrangingequation18.38\nintoaformwhereweneedtoknowonlythe r at i oofthetwomodel\u2019spartition\nfunctions:\n\ue058\nilog p A(x( ) i; \u03b8 A)\u2212\ue058\nilog p B(x( ) i; \u03b8 B) =\ue058\ni\ue020\nlog\u02dc p A(x( ) i; \u03b8 A)\n\u02dc p B(x( ) i; \u03b8 B)\ue021\n\u2212 mlogZ( \u03b8 A)\nZ( \u03b8 B).\n(18.39)\n623", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nWecanthusdeterminewhether M AisabettermodelthanM Bwithoutknowing\nthepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,\nwecanestimatethisratiousingimportancesampling,providedthatthetwomodels\naresimilar.\nIf,however,wewantedtocomputetheactualprobabilityofthetestdataunder\neither M AorM B,wewouldneedtocomputetheactualvalueofthepartition\nfunctions.Thatsaid,ifweknewtheratiooftwopartitionfunctions, r=Z ( \u03b8 B )\nZ ( \u03b8 A ),\nandweknewtheactualvalueofjustoneofthetwo,say Z( \u03b8 A),wecouldcompute\nthevalueoftheother:\nZ( \u03b8 B) = ( r Z \u03b8 A) =Z( \u03b8 B)\nZ( \u03b8 A)Z( \u03b8 A) . (18.40)\nAsimplewaytoestimatethe\u00a0partition functionistouse\u00a0aMonteCarlo\nmethodsuchassimpleimportancesampling.Wepresenttheapproachinterms\nofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscrete\nvariablesbyreplacingtheintegralswithsummation.Weuseaproposaldistribution\np 0(x)=1\nZ0\u02dc p 0(x)whichsupportstractablesamplingandtractableevaluationof\nboththepartitionfunction Z 0andtheunnormalized distribution\u02dc p 0()x.\nZ 1=\ue05a\n\u02dc p 1()x dx (18.41)\n=\ue05ap 0()x\np 0()x\u02dc p 1()x dx (18.42)\n= Z 0\ue05a\np 0()x\u02dc p 1()x\n\u02dc p 0()xdx (18.43)\n\u02c6 Z 1=Z 0\nKK\ue058\nk = 1\u02dc p 1(x( ) k)\n\u02dc p 0(x( ) k)st: . .x( ) k\u223c p 0 (18.44)\nInthelastline,wemakeaMonteCarloestimator,\u02c6 Z 1,oftheintegralusingsamples\ndrawnfrom p 0(x)andthenweighteachsamplewiththeratiooftheunnormalized\n\u02dc p 1andtheproposal p 0.\nWeseealsothatthisapproachallowsustoestimatetheratiobetweenthe\npartitionfunctionsas\n1\nKK \ue058\nk = 1\u02dc p 1(x( ) k)\n\u02dc p 0(x( ) k)st: . .x( ) k\u223c p 0 . (18.45)\nThisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedin\nequation.18.39\n624", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nIfthedistribution p 0iscloseto p 1,equationcanbeane\ufb00ectivewayof 18.44\nestimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime\np 1isbothcomplicated(usuallymultimodal)andde\ufb01nedoverahighdimensional\nspace.Itisdi\ufb03cultto\ufb01ndatractable p 0thatissimpleenoughtoevaluatewhile\nstillbeingcloseenoughto p 1toresultinahighqualityapproximation.If p 0and\np 1arenotclose,mostsamplesfrom p 0willhavelowprobabilityunder p 1and\nthereforemake(relatively)negligiblecontributiontothesuminequation.18.44\nHavingfewsamples\u00a0withsigni\ufb01cantweightsinthis\u00a0sumwillresultinan\nestimatorthatisofpoorqualityduetohighvariance.\u00a0This canbeunderstood\nquantitativelythroughanestimateofthevarianceofourestimate \u02c6 Z 1:\n\u02c6Var\ue010\n\u02c6 Z 1\ue011\n=Z 0\nK2K \ue058\nk = 1\ue020\n\u02dc p 1(x( ) k)\n\u02dc p 0(x( ) k)\u2212\u02c6 Z 1\ue0212\n. (18.46)\nThisquantityislargestwhenthereissigni\ufb01cantdeviationinthevaluesofthe\nimportanceweights\u02dc p1 ( x() k)\n\u02dc p0 ( x() k ).\nWenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-\ningtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-\ndimensionalspaces:\u00a0annealedimportancesamplingandbridgesampling.Both\nstartwiththesimpleimportancesamplingstrategyintroducedaboveandboth\nattempttoovercometheproblemoftheproposal p 0beingtoofarfrom p 1by\nintroducingintermediatedistributionsthatattemptto between b r i d g e t h e g a p p 0\nand p 1.\n1 8 . 7 . 1 A n n ea l ed Im p o rt a n ce S a m p l i n g\nInsituationswhere D K L( p 0\ue06b p 1)islarge(i.e.,wherethereislittleoverlapbetween\np 0and p 1),astrategycalled annealed i m p o r t anc e sampling(AIS)attempts\ntobridgethegapbyintroducingintermediate distributions(,;, Jarzynski1997Neal\n2001).Considerasequenceofdistributions p \u03b70 , . . . , p \u03b7 n,with 0 = \u03b7 0 < \u03b7 1 < <\u00b7\u00b7\u00b7\n\u03b7 n \u2212 1 < \u03b7 n= 1sothatthe\ufb01rstandlastdistributionsinthesequenceare p 0and p 1\nrespectively.\nThisapproachallowsustoestimatethepartitionfunctionofamultimodal\ndistributionde\ufb01nedoverahigh-dimensionalspace(suchasthedistributionde\ufb01ned\nbyatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction\n(suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwo\nmodel\u2019spartitionfunctions.\u00a0Theestimateofthisratioisbasedontheestimate\noftheratiosofasequenceofmanysimilardistributions,suchasthesequenceof\nRBMswithweightsinterpolatingbetweenzeroandthelearnedweights.\n625", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nWecannowwritetheratioZ1\nZ0as\nZ 1\nZ 0=Z 1\nZ 0Z \u03b71\nZ \u03b71\u00b7\u00b7\u00b7Z \u03b7 n \u22121\nZ \u03b7 n \u22121(18.47)\n=Z \u03b71\nZ 0Z \u03b72\nZ \u03b71\u00b7\u00b7\u00b7Z \u03b7 n \u22121\nZ \u03b7 n \u22122Z 1\nZ \u03b7 n \u22121(18.48)\n=n \u2212 1\ue059\nj = 0Z \u03b7 j+1\nZ \u03b7 j(18.49)\nProvidedthedistributions p \u03b7 jand p \u03b7 j + 1,forall0\u2264\u2264\u2212 j n1,aresu\ufb03ciently\nclose,wecanreliablyestimateeachofthefactorsZ \u03b7 j+1\nZ \u03b7 jusingsimpleimportance\nsamplingandthenusethesetoobtainanestimateofZ1\nZ0.\nWheredotheseintermediatedistributionscomefrom?Justastheoriginal\nproposaldistribution p 0isadesignchoice,soisthesequenceofdistributions\np \u03b71 . . . p \u03b7 n \u22121.Thatis,itcanbespeci\ufb01callyconstructedtosuittheproblemdomain.\nOnegeneral-purposeandpopularchoicefortheintermediate distributionsisto\nusetheweightedgeometricaverageofthetargetdistribution p 1andthestarting\nproposaldistribution(forwhichthepartitionfunctionisknown) p 0:\np \u03b7 j\u221d p\u03b7 j\n1 p1 \u2212 \u03b7 j\n0 (18.50)\nInordertosamplefromtheseintermediate distributions,wede\ufb01neaseriesof\nMarkovchaintransitionfunctions T \u03b7 j( x\ue030| x) thatde\ufb01netheconditionalprobability\ndistributionoftransitioningto x\ue030givenwearecurrentlyat x.Thetransition\noperator T \u03b7 j( x\ue030| x)isde\ufb01nedtoleave p \u03b7 j() xinvariant:\np \u03b7 j() = x\ue05a\np \u03b7 j( x\ue030) T \u03b7 j( x x|\ue030) d x\ue030(18.51)\nThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod\n(e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepasses\nthroughalloftherandomvariablesorotherkindsofiterations.\nTheAISsamplingstrategyisthentogeneratesamplesfrom p 0andthenuse\nthetransitionoperatorstosequentiallygeneratesamplesfromtheintermediate\ndistributionsuntilwearriveatsamplesfromthetargetdistribution p 1:\n\u2022for k . . . K = 1\n\u2013Sample x( ) k\n\u03b71\u223c p 0()x\n626", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\n\u2013Sample x( ) k\n\u03b72\u223c T \u03b71(x( ) k\n\u03b72| x( ) k\n\u03b71)\n\u2013...\n\u2013Sample x( ) k\n\u03b7 n \u22121\u223c T \u03b7 n \u22122(x( ) k\n\u03b7 n \u22121| x( ) k\n\u03b7 n \u22122)\n\u2013Sample x( ) k\n\u03b7 n\u223c T \u03b7 n \u22121(x( ) k\n\u03b7 n| x( ) k\n\u03b7 n \u22121)\n\u2022end\nForsample k,wecanderivetheimportanceweightbychainingtogetherthe\nimportanceweightsforthejumpsbetweentheintermediatedistributionsgivenin\nequation:18.49\nw( ) k=\u02dc p \u03b71( x( ) k\n\u03b71)\n\u02dc p 0( x( ) k\n\u03b71)\u02dc p \u03b72( x( ) k\n\u03b72)\n\u02dc p \u03b71( x( ) k\n\u03b72). . .\u02dc p 1( x( ) k\n1)\n\u02dc p \u03b7 n \u22121( x( ) k\n\u03b7 n). (18.52)\nToavoidnumericalissuessuchasover\ufb02ow,itisprobablybesttocompute log w( ) kby\naddingandsubtractinglogprobabilities, ratherthancomputing w( ) kbymultiplying\nanddividingprobabilities.\nWiththesamplingprocedurethusde\ufb01nedandtheimportanceweightsgiven\ninequation,theestimateoftheratioofpartitionfunctionsisgivenby: 18.52\nZ 1\nZ 0\u22481\nKK\ue058\nk = 1w( ) k(18.53)\nInordertoverifythatthisprocedurede\ufb01nesavalidimportancesampling\nscheme,wecanshow(,)thattheAISprocedurecorrespondstosimple Neal2001\nimportancesamplingonanextendedstatespacewithpointssampledoverthe\nproductspace [ x \u03b71 , . . . , x \u03b7 n \u22121 , x 1].Todothis,wede\ufb01nethedistributionoverthe\nextendedspaceas:\n\u02dc p( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1) (18.54)\n=\u02dc p 1( x 1)\u02dc T \u03b7 n \u22121( x \u03b7 n \u22121| x 1)\u02dc T \u03b7 n \u22122( x \u03b7 n \u22122| x \u03b7 n \u22121) . . .\u02dc T \u03b71( x \u03b71| x \u03b72) ,(18.55)\nwhere \u02dc T aisthereverseofthetransitionoperatorde\ufb01nedby T a(viaanapplication\nofBayes\u2019rule):\n\u02dc T a( x\ue030| x) =p a( x\ue030)\np a() xT a( x x|\ue030) =\u02dc p a( x\ue030)\n\u02dc p a() xT a( x x|\ue030) .(18.56)\nPluggingtheaboveintotheexpressionforthejointdistributionontheextended\nstatespacegiveninequation,weget:18.55\n\u02dc p( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1) (18.57)\n627", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\n=\u02dc p 1( x 1)\u02dc p \u03b7 n \u22121( x \u03b7 n \u22121)\n\u02dc p \u03b7 n \u22121( x 1)T \u03b7 n \u22121( x 1| x \u03b7 n \u22121)n \u2212 2\ue059\ni = 1\u02dc p \u03b7 i( x \u03b7 i)\n\u02dc p \u03b7 i( x \u03b7 i+1)T \u03b7 i( x \u03b7 i+1| x \u03b7 i)\n(18.58)\n=\u02dc p 1( x 1)\n\u02dc p \u03b7 n \u22121( x 1)T \u03b7 n \u22121( x 1| x \u03b7 n \u22121)\u02dc p \u03b71( x \u03b71)n \u2212 2 \ue059\ni = 1\u02dc p \u03b7 i+1( x \u03b7 i+1)\n\u02dc p \u03b7 i( x \u03b7 i+1)T \u03b7 i( x \u03b7 i+1| x \u03b7 i) .\n(18.59)\nWenowhavemeansofgeneratingsamplesfromthejointproposaldistribution\nqovertheextendedsampleviaasamplingschemegivenabove,withthejoint\ndistributiongivenby:\nq( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1) = p 0( x \u03b71) T \u03b71( x \u03b72| x \u03b71) . . . T \u03b7 n \u22121( x 1| x \u03b7 n \u22121) .(18.60)\nWehaveajointdistributionontheextendedspacegivenbyequation.Taking18.59\nq( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1)astheproposaldistributionontheextendedstatespacefrom\nwhichwewilldrawsamples,itremainstodeterminetheimportanceweights:\nw( ) k=\u02dc p( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1)\nq( x \u03b71 , . . . , x \u03b7 n \u22121 , x 1)=\u02dc p 1( x( ) k\n1)\n\u02dc p \u03b7 n \u22121( x( ) k\n\u03b7 n \u22121). . .\u02dc p \u03b72( x( ) k\n\u03b72)\n\u02dc p 1( x( ) k\n\u03b71)\u02dc p \u03b71( x( ) k\n\u03b71)\n\u02dc p 0( x( ) k\n0).(18.61)\nTheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISas\nsimpleimportancesamplingappliedtoanextendedstateanditsvalidityfollows\nimmediatelyfromthevalidityofimportancesampling.\nAnnealedimportancesampling(AIS)was\ufb01rstdiscoveredby () Jarzynski1997\nandthenagain,independently,by().Itiscurrentlythemostcommon Neal2001\nwayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.The\nreasonsforthismayhavemoretodowiththepublicationofanin\ufb02uentialpaper\n(SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthe\npartitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthan\nwithanyinherentadvantagethemethodhasovertheothermethoddescribed\nbelow.\nAdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceand\ne\ufb03ciency)canbefoundin().Neal2001\n1 8 . 7 . 2 B ri d g e S a m p l i n g\nBridgesampling ()isanothermethodthat,likeAIS,addressesthe Bennett1976\nshortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof\n628", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nintermediatedistributions,bridgesamplingreliesonasingledistribution p \u2217,known\nasthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,\np 0,andadistribution p 1forwhichwearetryingtoestimatethepartitionfunction\nZ 1.\nBridgesamplingestimatestheratio Z 1 / Z 0astheratiooftheexpectedimpor-\ntanceweightsbetween\u02dc p 0and\u02dc p \u2217andbetween\u02dc p 1and\u02dc p \u2217:\nZ 1\nZ 0\u2248K \ue058\nk = 1\u02dc p \u2217( x( ) k\n0)\n\u02dc p 0( x( ) k\n0)\ue02cK \ue058\nk = 1\u02dc p \u2217( x( ) k\n1)\n\u02dc p 1( x( ) k\n1)(18.62)\nIfthebridgedistribution p \u2217ischosencarefullytohavealargeoverlapofsupport\nwithboth p 0and p 1,thenbridgesamplingcanallowthedistancebetweentwo\ndistributions(ormoreformally, D K L( p 0\ue06b p 1))tobemuchlargerthanwithstandard\nimportancesampling.\nItcanbeshownthattheoptimalbridgingdistributionisgivenby p( ) op t\n\u2217(x)\u221d\n\u02dc p0 ( ) \u02dc x p1 ( ) x\nr \u02dc p0 ( ) + \u02dc x p1 ( ) xwhere r= Z 1 / Z 0.At\ufb01rst,thisappearstobeanunworkablesolution\nasitwouldseemtorequiretheveryquantitywearetryingtoestimate, Z 1 / Z 0.\nHowever,itispossibletostartwithacoarseestimateof randusetheresulting\nbridgedistributiontore\ufb01neourestimateiteratively(,).Thatis,we Neal2005\niterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof. r\nL i nk e d i m p o r t anc e samplingBothAISandbridgesamplinghavetheirad-\nvantages.If D K L( p 0\ue06b p 1)isnottoolarge(because p 0and p 1aresu\ufb03cientlyclose)\nbridgesamplingcanbeamoree\ufb00ectivemeansofestimatingtheratioofpartition\nfunctionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingle\ndistribution p \u2217tobridgethegapthenonecanatleastuseAISwithpotentially\nmanyintermediate distributionstospanthedistancebetween p 0and p 1.Neal\n()showedhowhislinkedimportancesamplingmethodleveragedthepowerof 2005\nthebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIS\ntosigni\ufb01cantlyimprovetheoverallpartitionfunctionestimates.\nE st i m at i n g t he par t i t i o n f unc t i o n whi l e t r ai ni ngWhileAIShasbecome\nacceptedasthestandardmethodforestimatingthepartitionfunctionformany\nundirectedmodels,\u00a0itissu\ufb03cientlycomputationally intensivethatitremains\ninfeasibletouseduringtraining.However,alternativestrategiesthathavebeen\nexploredtomaintainanestimateofthepartitionfunctionthroughouttraining\nUsingacombinationofbridgesampling,short-chainAISandparalleltempering,\nDesjardins2011 e t a l .()devisedaschemetotrackthepartitionfunctionofan\n629", "CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\nRBMthroughoutthetrainingprocess.Thestrategyisbasedonthemaintenanceof\nindependentestimatesofthepartitionfunctionsoftheRBMateverytemperature\noperatingintheparalleltemperingscheme.Theauthorscombinedbridgesampling\nestimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.from\nparalleltempering)withAISestimatesacrosstimetocomeupwithalowvariance\nestimateofthepartitionfunctionsateveryiterationoflearning.\nThetoolsdescribedinthischapterprovidemanydi\ufb00erentwaysofovercoming\ntheproblemofintractablepartitionfunctions,buttherecanbeseveralother\ndi\ufb03cultiesinvolvedintrainingandusinggenerativemodels.Foremostamongthese\nistheproblemofintractableinference,whichweconfrontnext.\n630", "C h a p t e r 1 9\nApproximateInference\nManyprobabilisticmodelsaredi\ufb03culttotrainbecauseitisdi\ufb03culttoperform\ninferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisible\nvariables vandasetoflatentvariables h.Thechallengeofinferenceusually\nreferstothedi\ufb03cultproblemofcomputing p( h v|)ortakingexpectationswith\nrespecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihood\nlearning.\nManysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestricted\nBoltzmannmachinesandprobabilisticPCA,arede\ufb01nedinawaythatmakes\ninferenceoperationslikecomputing p( h v|),ortakingexpectationswithrespect\ntoit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhidden\nvariableshaveintractableposteriordistributions.Exactinferencerequiresan\nexponentialamountoftimeinthesemodels.Evensomemodelswithonlyasingle\nlayer,suchassparsecoding,havethisproblem.\nInthischapter,weintroduceseveralofthetechniquesforconfrontingthese\nintractableinferenceproblems.Later,inchapter,wewilldescribehowtouse 20\nthesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,\nsuchasdeepbeliefnetworksanddeepBoltzmannmachines.\nIntractableinferenceproblemsindeeplearningusuallyarisefrominteractions\nbetweenlatentvariablesinastructuredgraphicalmodel.See\ufb01gureforsome19.1\nexamples.Theseinteractionsmaybeduetodirectinteractionsinundirected\nmodelsor\u201cexplainingaway\u201dinteractionsbetweenmutualancestorsofthesame\nvisibleunitindirectedmodels.\n631", "CHAPTER19.APPROXIMATEINFERENCE\nFigure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultof\ninteractionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbe\nduetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpaths\nthatareactivatedwhenthechildofaV-structureisobserved. ( L e f t )Asemi-restricted\nBoltzmannmachine( ,)withconnectionsbetweenhidden OsinderoandHinton2008\nunits.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistribution\nintractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine, ( C e n t e r )\norganizedintolayersofvariableswithoutintra-layerconnections,stillhasanintractable\nposteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodel ( R i g h t )\nhasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,because\neverytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovide\ntractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructures\ndepictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosento\nintroduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample,\nprobabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinference\nduetospecialpropertiesofthespeci\ufb01cconditionaldistributionsituses(linear-Gaussian\nconditionalswithmutuallyorthogonalbasisvectors).\n6 3 2", "CHAPTER19.APPROXIMATEINFERENCE\n19.1InferenceasOptimization\nManyapproachestoconfrontingtheproblemofdi\ufb03cultinferencemakeuseof\ntheobservationthatexactinferencecanbedescribedasanoptimization problem.\nApproximateinferencealgorithmsmaythenbederivedbyapproximatingthe\nunderlyingoptimization problem.\nToconstructtheoptimization problem,assumewehaveaprobabilisticmodel\nconsistingofobservedvariables vandlatentvariables h.Wewouldliketocompute\nthelogprobabilityoftheobserveddata, log p( v; \u03b8).Sometimesitistoodi\ufb03cult\ntocompute log p( v; \u03b8)ifitiscostlytomarginalizeout h.Instead,wecancompute\nalowerbound L( v \u03b8 , , q)onlog p( v; \u03b8).Thisboundiscalledtheevidencelower\nbound(ELBO).Anothercommonlyusednameforthislowerboundisthenegative\nvariationalfreeenergy.Speci\ufb01cally,theevidencelowerboundisde\ufb01nedtobe\nL \u2212 ( ) = log(;) v \u03b8 , , q p v \u03b8 D K L(( )( ;)) q h v|\ue06b p h v| \u03b8(19.1)\nwhereisanarbitraryprobabilitydistributionover. q h\nBecausethedi\ufb00erencebetweenlog p( v)and L( v \u03b8 , , q)isgivenbytheKL\ndivergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethat\nLalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoare\nequalifandonlyifisthesamedistributionas. q p( ) h v|\nSurprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributions q.\nSimplealgebrashowsthatwecanrearrange Lintoamuchmoreconvenientform:\nL \u2212 ( ) =log(;) v \u03b8 , , q p v \u03b8 D K L(( )( ;)) q h v|\ue06b p h v| \u03b8 (19.2)\n=log(;) p v \u03b8\u2212 E h\u223c qlogq( ) h v|\np( ) h v|(19.3)\n=log(;) p v \u03b8\u2212 E h\u223c qlogq( ) h v|\np , ( h v \u03b8 ; )\np ( ; ) v \u03b8(19.4)\n=log(;) p v \u03b8\u2212 E h\u223c q[log( )log(;)+log(;)] q h v|\u2212 p h v , \u03b8 p v \u03b8(19.5)\n=\u2212 E h\u223c q[log( )log(;)] q h v|\u2212 p h v , \u03b8 . (19.6)\nThisyieldsthemorecanonicalde\ufb01nitionoftheevidencelowerbound,\nL( ) = v \u03b8 , , q E h\u223c q[log( )]+() p h v , H q . (19.7)\nForanappropriatechoiceof q,Listractabletocompute.Foranychoice\nof q,Lprovidesalowerboundonthelikelihood.For q( h v|)thatarebetter\n6 3 3", "CHAPTER19.APPROXIMATEINFERENCE\napproximationsof p( h v|),thelowerbound Lwillbetighter,inotherwords,\nclosertolog p( v).\u00a0When q( h v|)= p( h v|),theapproximation isperfect,and\nL( ) = log(;) v \u03b8 , , q p v \u03b8 .\nWecanthusthinkofinferenceastheprocedurefor\ufb01ndingthe qthatmaximizes\nL.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctions\nqthatincludes p( h v|).Throughoutthischapter,wewillshowhowtoderive\ndi\ufb00erentformsofapproximateinferencebyusingapproximateoptimization to\n\ufb01nd q.Wecanmaketheoptimization procedurelessexpensivebutapproximate\nbyrestrictingthefamilyofdistributions qtheoptimization isallowedtosearch\noverorbyusinganimperfectoptimization procedurethatmaynotcompletely\nmaximizebutmerelyincreaseitbyasigni\ufb01cantamount. L\nNomatterwhatchoiceof qweuse,Lisalowerbound.Wecangettighter\norlooserboundsthatarecheaperormoreexpensivetocomputedependingon\nhowwechoosetoapproachthisoptimization problem.\u00a0Wecanobtainapoorly\nmatched qbutreducethecomputational costbyusinganimperfectoptimization\nprocedure,orbyusingaperfectoptimization procedureoverarestrictedfamilyof\nqdistributions.\n19.2ExpectationMaximization\nThe\ufb01rstalgorithmweintroducebasedonmaximizingalowerbound Listhe\nexpectationmaximization(EM)algorithm,apopulartrainingalgorithmfor\nmodelswithlatentvariables.WedescribehereaviewontheEMalgorithm\ndevelopedby ().Unlikemostoftheotheralgorithmswe NealandHinton1999\ndescribeinthischapter,EMisnotanapproachtoapproximateinference,but\nratheranapproachtolearningwithanapproximate posterior.\nTheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:\n\u2022TheE-step(Expectationstep):Let \u03b8( 0 )denotethevalueoftheparameters\natthebeginningofthestep.Set q( h( ) i| v)= p( h( ) i| v( ) i; \u03b8( 0 ))forall\nindices iofthetrainingexamples v( ) iwewanttotrainon(bothbatchand\nminibatchvariantsarevalid).Bythiswemean qisde\ufb01nedintermsofthe\nc u r r e ntparametervalueof \u03b8( 0 );ifwevary \u03b8then p( h v|; \u03b8)willchangebut\nq p ( ) h v|willremainequalto( ; h v| \u03b8( 0 )).\n\u2022The (Maximization step):Completelyorpartiallymaximize M-step\n\ue058\niL( v( ) i, , q \u03b8) (19.8)\n6 3 4", "CHAPTER19.APPROXIMATEINFERENCE\nwithrespecttousingyouroptimization algorithmofchoice. \u03b8\nThiscanbeviewedasacoordinateascentalgorithmtomaximize L.Onone\nstep,wemaximize Lwithrespectto q,andontheother,wemaximize Lwith\nrespectto. \u03b8\nStochasticgradientascentonlatentvariablemodelscanbeseenasaspecial\ncaseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradient\nstep.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsome\nmodelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthe\nwaytotheoptimalsolutionforgiventhecurrent. \u03b8 q\nEventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEM\nalgorithmasusingapproximate inferenceinsomesense.Speci\ufb01cally,theM-step\nassumesthatthesamevalueof qcanbeusedforallvaluesof \u03b8.Thiswillintroduce\nagapbetweenLandthetruelog p( v)astheM-stepmovesfurtherandfurther\nawayfromthevalue \u03b8( 0 )usedintheE-step.Fortunately,theE-stepreducesthe\ngaptozeroagainasweentertheloopforthenexttime.\nTheEMalgorithmcontainsafewdi\ufb00erentinsights.First,thereisthebasic\nstructureofthelearningprocess,inwhichweupdatethemodelparametersto\nimprovethelikelihoodofacompleteddataset,whereallmissingvariableshave\ntheirvaluesprovidedbyanestimateoftheposteriordistribution.Thisparticular\ninsightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescentto\nmaximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradient\ncomputationsrequiretakingexpectationswithrespecttotheposteriordistribution\noverthehiddenunits.\u00a0AnotherkeyinsightintheEMalgorithmisthatwecan\ncontinuetouseonevalueof qevenafterwehavemovedtoadi\ufb00erentvalueof \u03b8.\nThisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelarge\nM-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplex\ntoadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecond\ninsightwhichismoreuniquetotheEMalgorithmisrarelyused.\n19.3MAPInferenceandSparseCoding\nWeusuallyusetheterminferencetorefertocomputingtheprobabilitydistribution\noveronesetofvariablesgivenanother.Whentrainingprobabilisticmodelswith\nlatentvariables,weareusuallyinterestedincomputing p( h v|).Analternative\nformofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,\nratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext\n6 3 5", "CHAPTER19.APPROXIMATEINFERENCE\noflatentvariablemodels,thismeanscomputing\nh\u2217= argmax\nhp . ( ) h v| (19.9)\nThisisknownasmaximumaposterioriinference,abbreviatedMAPinference.\nMAPinferenceisusuallynotthoughtofasapproximate inference\u2014it does\ncomputetheexactmostlikelyvalueof h\u2217.However,ifwewishtodevelopa\nlearningprocessbasedonmaximizing L( v h , , q),thenitishelpfultothinkofMAP\ninferenceasaprocedurethatprovidesavalueof q.Inthissense,wecanthinkof\nMAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimal\nq.\nRecallfromsectionthatexactinferenceconsistsofmaximizing 19.1\nL( ) = v \u03b8 , , q E h\u223c q[log( )]+() p h v , H q (19.10)\nwithrespectto qoveranunrestrictedfamilyofprobabilitydistributions,using\nanexactoptimization algorithm.WecanderiveMAPinferenceasaformof\napproximateinferencebyrestrictingthefamilyofdistributions qmaybedrawn\nfrom.Speci\ufb01cally,werequiretotakeonaDiracdistribution: q\nq \u03b4 . ( ) = h v| ( ) h \u00b5\u2212 (19.11)\nThismeansthatwecannowcontrol qentirelyvia \u00b5.DroppingtermsofLthat\ndonotvarywith,weareleftwiththeoptimization problem \u00b5\n\u00b5\u2217= argmax\n\u00b5log(= ) p h \u00b5 v , , (19.12)\nwhichisequivalenttotheMAPinferenceproblem\nh\u2217= argmax\nhp . ( ) h v| (19.13)\nWecanthusjustifyalearningproceduresimilartoEM,inwhichwealternate\nbetweenperformingMAPinferencetoinfer h\u2217andthenupdate \u03b8toincrease\nlog p( h\u2217, v).AswithEM,thisisaformofcoordinateascentonL,wherewe\nalternatebetweenusing\u00a0inference to\u00a0optimize Lwithrespect\u00a0to qandusing\nparameterupdatestooptimize Lwithrespectto \u03b8.Theprocedureasawholecan\nbejusti\ufb01edbythefactthatLisalowerboundonlog p( v).InthecaseofMAP\ninference,thisjusti\ufb01cationisrathervacuous,becausetheboundisin\ufb01nitelyloose,\nduetotheDiracdistribution\u2019sdi\ufb00erentialentropyofnegativein\ufb01nity.However,\naddingnoisetowouldmaketheboundmeaningfulagain. \u00b5\n6 3 6", "CHAPTER19.APPROXIMATEINFERENCE\nMAPinferenceiscommonlyusedindeeplearningasbothafeatureextractor\nandalearningmechanism.Itisprimarilyusedforsparsecodingmodels.\nRecallfromsectionthatsparsecodingisalinearfactormodelthatimposes 13.4\nasparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplace\nprior,with\np h( i) =\u03bb\n2e\u2212| \u03bb h i|. (19.14)\nThevisibleunitsarethengeneratedbyperformingalineartransformationand\naddingnoise:\np , \u03b2 ( ) = (; + x h| N v W h b\u2212 1I) . (19.15)\nComputingorevenrepresenting p( h v|)isdi\ufb03cult.Everypairofvariables h i\nand h jarebothparentsof v.Thismeansthatwhen visobserved,thegraphical\nmodelcontainsanactivepathconnecting h iand h j.Allofthehiddenunitsthus\nparticipateinonemassivecliquein p( h v|).IfthemodelwereGaussianthen\ntheseinteractionscouldbemodelede\ufb03cientlyviathecovariancematrix,butthe\nsparsepriormakestheseinteractionsnon-Gaussian.\nBecause p( h v|)isintractable,soisthecomputationofthelog-likelihoodand\nitsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,we\nuseMAPinferenceandlearntheparametersbymaximizingtheELBOde\ufb01nedby\ntheDiracdistributionaroundtheMAPestimateof. h\nIfweconcatenateallofthe hvectorsinthetrainingsetintoamatrix H,and\nconcatenateallofthevectorsintoamatrix,thenthesparsecodinglearning v V\nprocessconsistsofminimizing\nJ ,( H W) =\ue058\ni , j| H i , j|+\ue058\ni , j\ue010\nV H W\u2212\ue03e\ue0112\ni , j.(19.16)\nMostapplicationsofsparsecodingalsoinvolveweightdecayoraconstrainton\nthenormsofthecolumnsof W,inordertopreventthepathologicalsolutionwith\nextremelysmallandlarge. H W\nWecanminimize Jbyalternatingbetweenminimization withrespectto H\nandminimization withrespectto W.Bothsub-problemsareconvex.Infact,\ntheminimization withrespectto Wisjustalinearregressionproblem.However,\nminimization of Jwithrespecttobothargumentsisusuallynotaconvexproblem.\nMinimization withrespectto Hrequiresspecializedalgorithmssuchasthe\nfeature-signsearchalgorithm(,). Lee e t a l .2007\n6 3 7", "CHAPTER19.APPROXIMATEINFERENCE\n19.4VariationalInferenceandLearning\nWe\u00a0have\u00a0seen\u00a0how\u00a0the\u00a0evidence\u00a0lo wer\u00a0bound L( v \u03b8 , , q)is\u00a0a\u00a0lower\u00a0bound\u00a0on\nlog p( v; \u03b8),howinferencecanbeviewedasmaximizing Lwithrespectto q,and\nhowlearningcanbeviewedasmaximizing Lwithrespectto \u03b8.Wehaveseen\nthattheEMalgorithmallowsustomakelargelearningstepswitha\ufb01xed qand\nthatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapoint\nestimateof p( h v|)ratherthaninferringtheentiredistribution.Nowwedevelop\nthemoregeneralapproachtovariationallearning.\nThecoreideabehindvariationallearningisthatwecanmaximize Lovera\nrestrictedfamilyofdistributions q.Thisfamilyshouldbechosensothatitiseasy\ntocompute E qlog p( h v ,).\u00a0Atypicalwaytodothisistointroduceassumptions\nabouthowfactorizes. q\nAcommonapproachtovariationallearningistoimposetherestrictionthat q\nisafactorialdistribution:\nq( ) = h v|\ue059\niq h( i| v) . (19.17)\nThisiscalledthemean\ufb01eldapproach.Moregenerally,wecanimposeanygraphi-\ncalmodelstructurewechooseon q,to\ufb02exiblydeterminehowmanyinteractionswe\nwantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproach\niscalledstructuredvariationalinference( ,). SaulandJordan1996\nThebeautyofthevariationalapproachisthatwedonotneedtospecifya\nspeci\ufb01cparametricformfor q.Wespecifyhowitshouldfactorize,butthenthe\noptimization problemdeterminestheoptimalprobabilitydistributionwithinthose\nfactorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatwe\nusetraditionaloptimization techniquestooptimizea\ufb01nitenumberofvariables\ndescribingthe qdistribution.Forcontinuouslatentvariables,thismeansthatwe\nuseabranchofmathematics calledcalculusofvariationstoperformoptimization\noveraspaceoffunctions,andactuallydeterminewhichfunctionshouldbeused\ntorepresent q.Calculusof\u00a0variations\u00a0istheorigin\u00a0ofthenames\u00a0\u201cvariational\nlearning\u201dand\u201cvariationalinference,\u201dthoughthesenamesapplyevenwhenthe\nlatentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecase\nofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethat\nremovesmuchoftheresponsibilityfromthehumandesignerofthemodel,who\nnowmustspecifyonlyhow qfactorizes,ratherthanneedingtoguesshowtodesign\naspeci\ufb01cthatcanaccuratelyapproximate theposterior. q\nBecauseL( v \u03b8 , , q)isde\ufb01nedtobelog p( v; \u03b8)\u2212 D K L( q( h v|)\ue06b p( h v|; \u03b8)),we\ncanthinkofmaximizing Lwithrespectto qasminimizing D K L( q( h v|)\ue06b p( h v|)).\n6 3 8", "CHAPTER19.APPROXIMATEINFERENCE\nInthissense,weare\ufb01tting qto p.\u00a0However,wearedoingsowiththeopposite\ndirectionoftheKLdivergencethanweareusedtousingfor\ufb01ttinganapproximation.\nWhenweusemaximumlikelihoodlearningto\ufb01tamodeltodata,weminimize\nD K L( p da t a\ue06b p m o de l).Asillustratedin\ufb01gure,thismeansthatmaximumlikelihood 3.6\nencouragesthemodeltohavehighprobabilityeverywherethatthedatahashigh\nprobability,\u00a0whileouroptimization-based inferenceprocedureencourages qto\nhavelowprobabilityeverywherethetrueposteriorhaslowprobability.Both\ndirectionsoftheKLdivergencecanhavedesirableandundesirableproperties.The\nchoiceofwhichtousedependsonwhichpropertiesarethehighestpriorityfor\neachapplication. Inthecaseoftheinferenceoptimization problem,wechoose\ntouse D K L( q( h v|)\ue06b p( h v|))forcomputational reasons.Speci\ufb01cally,computing\nD K L( q( h v|)\ue06b p( h v|))involvesevaluatingexpectationswithrespectto q,soby\ndesigning qtobesimple,wecansimplifytherequiredexpectations.Theopposite\ndirectionoftheKLdivergencewouldrequirecomputingexpectationswithrespect\ntothetrueposterior.Becausetheformofthetrueposteriorisdeterminedby\nthechoiceofmodel,wecannotdesignareduced-costapproachtocomputing\nD K L(( )( )) p h v|\ue06b q h v|exactly.\n19.4.1DiscreteLatentVariables\nVariationalinferencewithdiscretelatentvariablesisrelativelystraightforward.\nWede\ufb01neadistribution q,typicallyonewhereeachfactorof qisjustde\ufb01ned\nbyalookuptableoverdiscretestates.\u00a0Inthesimplestcase, hisbinaryandwe\nmakethemean\ufb01eldassumptionthatfactorizesovereachindividual q h i.Inthis\ncasewecanparametrize qwithavector\u02c6 hwhoseentriesareprobabilities. Then\nq h( i= 1 ) =| v \u02c6h i.\nAfterdetermininghowtorepresent q,wesimplyoptimizeitsparameters.In\nthecaseofdiscretelatentvariables,thisisjustastandardoptimization problem.\nInprincipletheselectionof qcouldbedonewithanyoptimization algorithm,such\nasgradientdescent.\nBecausethisoptimization mustoccurintheinnerloopofalearningalgorithm,\nitmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimization\nalgorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsin\nveryfewiterations.Apopularchoiceistoiterate\ufb01xedpointequations,inother\nwords,tosolve\n\u2202\n\u2202\u02c6h iL= 0 (19.18)\nfor\u02c6h i.Werepeatedlyupdatedi\ufb00erentelementsof\u02c6huntilwesatisfyaconvergence\n6 3 9", "CHAPTER19.APPROXIMATEINFERENCE\ncriterion.\nTomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothe\nbinarysparsecodingmodel(wepresentherethemodeldevelopedbyHenniges\ne t a l .()butdemonstratetraditional,genericmean\ufb01eldappliedtothemodel, 2010\nwhiletheyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderable\nmathematical detailandisintendedforthereaderwhowishestofullyresolve\nanyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceand\nlearningwehavepresentedsofar.Readerswhodonotplantoderiveorimplement\nvariationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissing\nanynewhigh-levelconcepts.Readerswhoproceedwiththebinarysparsecoding\nexampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthat\ncommonlyariseinprobabilisticmodelsinsection.Weusetheseproperties 3.10\nliberallythroughoutthefollowingderivationswithouthighlightingexactlywhere\nweuseeachone.\nInthebinarysparsecodingmodel,theinput v\u2208 Rnisgeneratedfromthe\nmodelbyaddingGaussiannoisetothesumof mdi\ufb00erentcomponentswhich\ncaneachbepresentorabsent.Eachcomponentisswitchedonoro\ufb00bythe\ncorrespondinghiddenunitin h\u2208{}01 ,m:\np h( i= 1) = ( \u03c3 b i) (19.19)\np , ( ) = (; v h| N v W h \u03b2\u2212 1) (19.20)\nwhere bisalearnablesetofbiases, Wisalearnableweightmatrix,and \u03b2isa\nlearnable,diagonalprecisionmatrix.\nTrainingthismodelwithmaximumlikelihoodrequirestakingthederivative\nwithrespecttotheparameters.Considerthederivativewithrespecttooneofthe\nbiases:\n\u2202\n\u2202 b ilog() p v (19.21)\n=\u2202\n\u2202 b ip() v\np() v(19.22)\n=\u2202\n\u2202 b i\ue050\nhp ,( h v)\np() v(19.23)\n=\u2202\n\u2202 b i\ue050\nhp p() h( ) v h|\np() v(19.24)\n6 4 0", "CHAPTER19.APPROXIMATEINFERENCE\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\nh 1 h 1\nh 2 h 2h 3 h 3\nh 4 h 4\nFigure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits.\n( L e f t )Thegraphstructureof p( h v ,).Notethattheedgesaredirected,andthateverytwo\nhiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof ( R i g h t ) p( h v|).\nInordertoaccountfortheactivepathsbetweenco-parents,theposteriordistribution\nneedsanedgebetweenallofthehiddenunits.\n=\ue050\nh p( ) v h|\u2202\n\u2202 b ip() h\np() v(19.25)\n=\ue058\nhp( ) h v|\u2202\n\u2202 b ip() h\np() h(19.26)\n= E h\u223c | p ( h v )\u2202\n\u2202 b ilog() p h . (19.27)\nThisrequirescomputingexpectationswithrespectto p( h v|).Unfortunately,\np( h v|)isacomplicateddistribution.See\ufb01gureforthegraphstructureof 19.2\np( h v ,)and p( h v|).Theposteriordistributioncorrespondstothecompletegraph\noverthehiddenunits,sovariableeliminationalgorithmsdonothelpustocompute\ntherequiredexpectationsanyfasterthanbruteforce.\nWecanresolvethisdi\ufb03cultybyusingvariationalinferenceandvariational\nlearninginstead.\nWecanmakeamean\ufb01eldapproximation:\nq( ) = h v|\ue059\niq h( i| v) . (19.28)\nThelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresent\nafactorial qwesimplyneedtomodel mBernoullidistributions q( h i| v).Anatural\nwaytorepresentthemeansoftheBernoullidistributionsiswithavector\u02c6hof\nprobabilities, with q( h i=1| v)=\u02c6h i.Weimposearestrictionthat\u02c6h iisnever\nequalto0orto1,inordertoavoiderrorswhencomputing,forexample, log\u02c6h i.\nWewillseethatthevariationalinferenceequationsneverassignorto 01 \u02c6h i\n6 4 1", "CHAPTER19.APPROXIMATEINFERENCE\nanalytically.However,inasoftwareimplementation,machineroundingerrorcould\nresultinorvalues.Insoftware,wemaywishtoimplementbinarysparse 0 1\ncodingusinganunrestrictedvectorofvariationalparameters zandobtain \u02c6 hvia\ntherelation \u02c6h= \u03c3( z).Wecanthussafelycompute log\u02c6h ionacomputerbyusing\ntheidentitylog( \u03c3 z i) = (\u2212 \u03b6\u2212 z i)relatingthesigmoidandthesoftplus.\nTobeginourderivationofvariationallearninginthebinarysparsecoding\nmodel,weshowthattheuseofthismean\ufb01eldapproximationmakeslearning\ntractable.\nTheevidencelowerboundisgivenby\nL( ) v \u03b8 , , q (19.29)\n= E h\u223c q[log( )]+() p h v , H q (19.30)\n= E h\u223c q[log()+log( )log( )] p h p v h|\u2212 q h v| (19.31)\n= E h\u223c q\ue022m\ue058\ni = 1log( p h i)+n\ue058\ni = 1log( p v i|\u2212 h)m\ue058\ni = 1log( q h i| v)\ue023\n(19.32)\n=m\ue058\ni = 1\ue068\n\u02c6 h i(log( \u03c3 b i)log\u2212 \u02c6 h i)+(1\u2212\u02c6 h i)(log( \u03c3\u2212 b i)log(1 \u2212 \u2212\u02c6 h i))\ue069\n(19.33)\n+ E h\u223c q\ue022n\ue058\ni = 1log\ue072\n\u03b2 i\n2 \u03c0exp\ue012\n\u2212\u03b2 i\n2( v i\u2212 W i , : h)2\ue013\ue023\n(19.34)\n=m\ue058\ni = 1\ue068\n\u02c6 h i(log( \u03c3 b i)log\u2212 \u02c6 h i)+(1\u2212\u02c6 h i)(log( \u03c3\u2212 b i)log(1 \u2212 \u2212\u02c6 h i))\ue069\n(19.35)\n+1\n2n\ue058\ni = 1\uf8ee\n\uf8f0log\u03b2 i\n2 \u03c0\u2212 \u03b2 i\uf8eb\n\uf8ed v2\ni\u22122 v i W i , :\u02c6h+\ue058\nj\uf8ee\n\uf8f0 W2\ni , j\u02c6 h j+\ue058\nk j\ue036=W i , j W i , k\u02c6h j\u02c6h k\uf8f9\n\uf8fb\uf8f6\n\uf8f8\uf8f9\n\uf8fb .\n(19.36)\nWhiletheseequationsaresomewhatunappealingaesthetically,theyshowthatL\ncanbeexpressedinasmallnumberofsimplearithmeticoperations.Theevidence\nlowerbound Listhereforetractable.WecanuseLasareplacementforthe\nintractablelog-likelihood.\nInprinciple,wecouldsimplyrungradientascentonboth vand handthis\nwouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm.\nUsually,however,wedonotdothis,fortworeasons.First,thiswouldrequire\nstoring \u02c6 hforeach v.Wetypicallypreferalgorithmsthatdonotrequireper-\nexamplememory.Itisdi\ufb03culttoscalelearningalgorithmstobillionsofexamples\nifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample.\n6 4 2", "CHAPTER19.APPROXIMATEINFERENCE\nSecond,wewouldliketobeabletoextractthefeatures \u02c6hveryquickly,inorderto\nrecognizethecontentof v.\u00a0Inarealisticdeployedsetting,wewouldneedtobe\nabletocompute \u02c6hinrealtime.\nForboththesereasons,wetypicallydonotusegradientdescenttocompute\nthemean\ufb01eldparameters \u02c6 h.Instead,werapidlyestimatethemwith\ufb01xedpoint\nequations.\nTheideabehind\ufb01xedpointequationsisthatweareseekingalocalmaximum\nwithrespectto\u02c6h,\u00a0where \u2207 hL( v \u03b8 , ,\u02c6h)= 0.Wecannote\ufb03cientlysolvethis\nequationwithrespecttoallof\u02c6hsimultaneously.However,wecansolveforasingle\nvariable:\u2202\n\u2202\u02c6h iL( v \u03b8 , ,\u02c6h) = 0 . (19.37)\nWecantheniterativelyapplythesolutiontotheequationfor i=1 , . . . , m,\nandrepeatthecycleuntilwesatisfyaconvergecriterion.Commonconvergence\ncriteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymore\nthansometoleranceamount,orwhenthecycledoesnotchange \u02c6hbymorethan\nsomeamount.\nIteratingmean\ufb01eld\ufb01xedpointequationsisageneraltechniquethatcan\nprovidefastvariationalinferenceinabroadvarietyofmodels.Tomakethismore\nconcrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelin\nparticular.\nFirst,wemustwriteanexpressionforthederivativeswithrespectto\u02c6h i.To\ndoso,wesubstituteequation intotheleftsideofequation: 19.36 19.37\n\u2202\n\u2202\u02c6h iL( v \u03b8 , ,\u02c6 h) (19.38)\n=\u2202\n\u2202\u02c6h i\uf8ee\n\uf8f0m\ue058\nj = 1\ue068\n\u02c6 h j(log( \u03c3 b j)log\u2212 \u02c6 h j)+(1\u2212\u02c6 h j)(log( \u03c3\u2212 b j)log(1 \u2212 \u2212\u02c6 h j))\ue069\n(19.39)\n+1\n2n\ue058\nj = 1\uf8ee\n\uf8f0log\u03b2 j\n2 \u03c0\u2212 \u03b2 j\uf8eb\n\uf8ed v2\nj\u22122 v j W j , :\u02c6h+\ue058\nk\uf8ee\n\uf8f0 W2\nj , k\u02c6h k+\ue058\nl k\ue036=W j , k W j , l\u02c6h k\u02c6h l\uf8f9\n\uf8fb\uf8f6\n\uf8f8\uf8f9\n\uf8fb\uf8f9\n\uf8fb\n(19.40)\n=log( \u03c3 b i)log\u2212 \u02c6h i\u2212 \u2212 1+log(1\u02c6h i)+1log( \u2212 \u03c3\u2212 b i) (19.41)\n+n\ue058\nj = 1\uf8ee\n\uf8f0 \u03b2 j\uf8eb\n\uf8ed v j W j , i\u22121\n2W2\nj , i\u2212\ue058\nk i\ue036=W j , k W j , i\u02c6h k\uf8f6\n\uf8f8\uf8f9\n\uf8fb (19.42)\n6 4 3", "CHAPTER19.APPROXIMATEINFERENCE\n= b i\u2212log\u02c6 h i+log(1\u2212\u02c6h i)+ v\ue03e\u03b2 W : , i\u22121\n2W\ue03e\n: , i \u03b2 W : , i\u2212\ue058\nj i\ue036=W\ue03e\n: , j \u03b2 W : , i\u02c6h j .(19.43)\nToapplythe\ufb01xedpointupdateinferencerule,wesolveforthe\u02c6 h ithatsets\nequationto0:19.43\n\u02c6h i= \u03c3\uf8eb\n\uf8ed b i+ v\ue03e\u03b2 W : , i\u22121\n2W\ue03e\n: , i \u03b2 W : , i\u2212\ue058\nj i\ue036=W\ue03e\n: , j \u03b2 W : , i\u02c6h j\uf8f6\n\uf8f8 .(19.44)\nAtthispoint,wecanseethatthereisacloseconnectionbetweenrecurrent\nneuralnetworksandinferenceingraphicalmodels.Speci\ufb01cally,themean\ufb01eld\n\ufb01xedpointequationsde\ufb01nedarecurrentneuralnetwork.Thetaskofthisnetwork\nistoperforminference.Wehavedescribedhowtoderivethisnetworkfroma\nmodeldescription,butitisalsopossibletotraintheinferencenetworkdirectly.\nSeveralideasbasedonthisthemearedescribedinchapter.20\nInthecaseofbinarysparsecoding,wecanseethattherecurrentnetwork\nconnectionspeci\ufb01edbyequationconsistsofrepeatedlyupdatingthehidden 19.44\nunitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinput\nalwayssendsa\ufb01xedmessageof v\ue03e\u03b2 Wtothehiddenunits,butthehiddenunits\nconstantlyupdatethemessagetheysendtoeachother.Speci\ufb01cally,twounits \u02c6h i\nand\u02c6 h jinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformof\ncompetition\u2014betweentwohiddenunitsthatbothexplaintheinput,onlytheone\nthatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionis\nthemean\ufb01eldapproximation\u2019sattempttocapturetheexplainingawayinteractions\ninthebinarysparsecodingposterior.Theexplainingawaye\ufb00ectactuallyshould\ncauseamulti-modalposterior,sothatifwedrawsamplesfromtheposterior,\nsomesampleswillhaveoneunitactive,othersampleswillhavetheotherunit\nactive,butveryfewsampleshavebothactive.Unfortunately,explainingaway\ninteractionscannotbemodeledbythefactorial qusedformean\ufb01eld,sothemean\n\ufb01eldapproximation isforcedtochooseonemodetomodel.Thisisaninstanceof\nthebehaviorillustratedin\ufb01gure.3.6\nWecanrewriteequationintoanequivalentformthatrevealssomefurther 19.44\ninsights:\n\u02c6h i= \u03c3\uf8eb\n\uf8ec\uf8ed b i+\uf8eb\n\uf8ed v\u2212\ue058\nj i\ue036=W : , j\u02c6h j\uf8f6\n\uf8f8\ue03e\n\u03b2 W : , i\u22121\n2W\ue03e\n: , i \u03b2 W : , i\uf8f6\n\uf8f7\uf8f8 .(19.45)\nInthisreformulation,weseetheinputateachstepasconsistingof v\u2212\ue050\nj i\ue036=W : , j\u02c6h j\nratherthan v.Wecanthusthinkofunit iasattemptingtoencodetheresidual\n6 4 4", "CHAPTER19.APPROXIMATEINFERENCE\nerrorin vgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingas\naniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attempting\nto\ufb01xmistakesinthereconstructionaftereachiteration.\nInthisexample,wehavederivedanupdaterulethatupdatesasingleunitat\natime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously.\nSomegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsucha\nwaythatwecansolveformanyentriesof\u02c6hsimultaneously.Unfortunately,binary\nsparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristic\ntechniquecalleddampingtoperformblockupdates.Inthedampingapproach,\nwesolvefortheindividuallyoptimalvaluesofeveryelementof\u02c6h,thenmoveallof\nthevaluesinasmallstepinthatdirection.Thisapproachisnolongerguaranteed\ntoincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKoller\nandFriedman2009()formoreinformationaboutchoosingthedegreeofsynchrony\nanddampingstrategiesinmessagepassingalgorithms.\n19.4.2CalculusofVariations\nBeforecontinuingwithourpresentationofvariationallearning,wemustbrie\ufb02y\nintroduceanimportantsetofmathematical toolsusedinvariationallearning:\ncalculusofvariations.\nManymachinelearningtechniquesarebasedonminimizingafunction J( \u03b8)by\n\ufb01ndingtheinputvector \u03b8\u2208 Rnforwhichittakesonitsminimalvalue.Thiscan\nbeaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthe\ncriticalpointswhere \u2207 \u03b8 J( \u03b8) = 0.Insomecases,weactuallywanttosolvefora\nfunction f( x),suchaswhenwewantto\ufb01ndtheprobabilitydensityfunctionover\nsomerandomvariable.Thisiswhatcalculusofvariationsenablesustodo.\nAfunction\u00a0ofa\u00a0function fisknown\u00a0asafunctional J[ f].Muchas\u00a0we\ncantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-\nvaluedargument,wecantakefunctionalderivatives,alsoknownasvariational\nderivatives,ofafunctional J[ f]withrespecttoindividualvaluesofthefunction\nf( x)atanyspeci\ufb01cvalueof x.Thefunctionalderivativeofthefunctional Jwith\nrespecttothevalueofthefunctionatpointisdenoted f x\u03b4\n\u03b4 f x ( )J.\nAcompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeof\nthisbook.Forourpurposes,itissu\ufb03cienttostatethatfordi\ufb00erentiablefunctions\nf g y , () xanddi\ufb00erentiable functions ( x)withcontinuousderivatives,that\n\u03b4\n\u03b4 f() x\ue05a\ng f , d (() x x) x=\u2202\n\u2202 yg f , . (() x x) (19.46)\n6 4 5", "CHAPTER19.APPROXIMATEINFERENCE\nTogainsomeintuitionforthisidentity,onecanthinkof f( x)asbeingavector\nwithuncountablymanyelements,indexedbyarealvector x.Inthis(somewhat\nincompleteview),theidentityprovidingthefunctionalderivativesisthesameas\nwewouldobtainforavector \u03b8\u2208 Rnindexedbypositiveintegers:\n\u2202\n\u2202 \u03b8 i\ue058\njg \u03b8( j , j) =\u2202\n\u2202 \u03b8 ig \u03b8( i , i .) (19.47)\nManyresultsinothermachinelearningpublicationsarepresentedusingthemore\ngeneralEuler-Lagrangeequationwhichallows gtodependonthederivatives\nof faswellasthevalueof f,butwedonotneedthisfullygeneralformforthe\nresultspresentedinthisbook.\nTooptimizeafunctionwithrespecttoavector,wetakethegradientofthe\nfunctionwithrespecttothevectorandsolveforthepointwhereeveryelementof\nthegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingfor\nthefunctionwherethefunctionalderivativeateverypointisequaltozero.\nAsanexampleofhowthisprocessworks,considertheproblemof\ufb01ndingthe\nprobabilitydistributionfunctionover x\u2208 Rthathasmaximaldi\ufb00erentialentropy.\nRecallthattheentropyofaprobabilitydistributionisde\ufb01nedas p x()\nH p[] = \u2212 E xlog() p x . (19.48)\nForcontinuousvalues,theexpectationisanintegral:\nH p[] = \u2212\ue05a\np x p x d x . ()log() (19.49)\nWecannotsimplymaximize H[ p] withrespecttothefunction p( x),becausethe\nresultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrange\nmultipliers\u00a0toadd\u00a0aconstraint\u00a0that p( x)integratesto\u00a01.Also,theentropy\nincreaseswithoutboundasthevarianceincreases.Thismakesthequestionof\nwhichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhich\ndistributionhasmaximalentropyfor\ufb01xedvariance \u03c32.Finally,theproblem\nisunderdetermined becausethedistributioncanbeshiftedarbitrarilywithout\nchangingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthe\nmeanofthedistributionbe \u00b5.\u00a0TheLagrangianfunctionalforthisoptimization\nproblemis\nL[] = p \u03bb 1\ue012\ue05a\np x d x()\u22121\ue013\n+ \u03bb 2([] )+ E x\u2212 \u00b5 \u03bb 3\ue000\nE[( ) x \u00b5\u22122]\u2212 \u03c32\ue001\n+[] H p(19.50)\n6 4 6", "CHAPTER19.APPROXIMATEINFERENCE\n=\ue05a\ue000\n\u03bb 1 p x \u03bb ()+ 2 p x x \u03bb ()+ 3 p x x \u00b5 ()(\u2212)2\u2212 p x p x ()log()\ue001d x \u03bb\u2212 1\u2212 \u00b5 \u03bb 2\u2212 \u03c32\u03bb 3 .\n(19.51)\nTominimizetheLagrangianwithrespectto p,wesetthefunctionalderivatives\nequalto0:\n\u2200 x ,\u03b4\n\u03b4 p x()L= \u03bb 1+ \u03bb 2 x \u03bb+ 3( ) x \u00b5\u22122\u2212\u22121log() = 0 p x .(19.52)\nThisconditionnowtellsusthefunctionalformof p( x).Byalgebraically\nre-arrangingtheequation,weobtain\np x() = exp\ue000\n\u03bb 1+ \u03bb 2 x \u03bb+ 3( ) x \u00b5\u22122\u22121\ue001\n. (19.53)\nWeneverassumeddirectlythat p( x)wouldtakethisfunctionalform;we\nobtainedtheexpressionitselfbyanalyticallyminimizingafunctional.To\ufb01nish\ntheminimization problem,wemustchoosethe \u03bbvaluestoensurethatallofour\nconstraintsaresatis\ufb01ed.Wearefreetochooseany \u03bbvalues,becausethegradient\noftheLagrangianwithrespecttothe \u03bbvariablesiszerosolongastheconstraints\naresatis\ufb01ed.Tosatisfyalloftheconstraints,wemayset \u03bb 1=1\u2212log \u03c3\u221a\n2 \u03c0,\n\u03bb 2= 0,and \u03bb 3= \u22121\n2 \u03c32toobtain\np x x \u00b5 , \u03c3 () = (N;2) . (19.54)\nThisisonereasonforusingthenormaldistributionwhenwedonotknowthe\ntruedistribution.Becausethenormaldistributionhasthemaximumentropy,we\nimposetheleastpossibleamountofstructurebymakingthisassumption.\nWhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,\nwefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyfor\n\ufb01xedvariance.Whatabouttheprobabilitydistributionfunctionthat m i nim i z e s\ntheentropy?Whydidwenot\ufb01ndasecondcriticalpointcorrespondingtothe\nminimum?Thereasonisthatthereisnospeci\ufb01cfunctionthatachievesminimal\nentropy.Asfunctionsplacemoreprobabilitydensityonthetwopoints x= \u00b5+ \u03c3\nand x= \u00b5 \u03c3\u2212,andplacelessprobabilitydensityonallothervaluesof x,theylose\nentropywhilemaintainingthedesiredvariance.However,anyfunctionplacing\nexactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnota\nvalidprobabilitydistribution.Therethusisnosingleminimalentropyprobability\ndistributionfunction,muchasthereisnosingleminimalpositiverealnumber.\nInstead,wecansaythatthereisasequenceofprobabilitydistributionsconverging\ntowardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe\n6 4 7", "CHAPTER19.APPROXIMATEINFERENCE\ndescribedasamixtureofDiracdistributions.BecauseDiracdistributionsare\nnotdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureof\nDiracdistributioncorrespondstoasinglespeci\ufb01cpointinfunctionspace.These\ndistributionsarethusinvisibletoourmethodofsolvingforaspeci\ufb01cpointwhere\nthefunctionalderivativesarezero.Thisisalimitationofthemethod.Distributions\nsuchastheDiracmustbefoundbyothermethods,suchasguessingthesolution\nandthenprovingthatitiscorrect.\n19.4.3ContinuousLatentVariables\nWhenourgraphicalmodelcontainscontinuouslatentvariables,\u00a0wemaystill\nperformvariationalinferenceandlearningbymaximizing L.However,wemust\nnowusecalculusofvariationswhenmaximizing withrespectto. L q( ) h v|\nInmostcases,practitioners neednotsolveanycalculusofvariationsproblems\nthemselves.Instead,thereisageneralequationforthemean\ufb01eld\ufb01xedpoint\nupdates.Ifwemakethemean\ufb01eldapproximation\nq( ) = h v|\ue059\niq h( i| v) , (19.55)\nand\ufb01x q( h j| v)forall j\ue036= i,thentheoptimal q( h i| v)maybeobtainedby\nnormalizingtheunnormalized distribution\n\u02dc q h( i| v) = exp\ue000\nE h \u2212 i\u223c q ( h \u2212 i| v )log \u02dc p ,( v h)\ue001\n(19.56)\nsolongas pdoesnotassignprobabilitytoanyjointcon\ufb01gurationofvariables. 0\nCarryingouttheexpectationinsidetheequationwillyieldthecorrectfunctional\nformof q( h i| v).Itisonlynecessarytoderivefunctionalformsof qdirectlyusing\ncalculusofvariationsifonewishestodevelopanewformofvariationallearning;\nequationyieldsthemean\ufb01eldapproximation foranyprobabilisticmodel. 19.56\nEquationisa\ufb01xedpointequation,designedtobeiterativelyappliedfor 19.56\neachvalueof irepeatedlyuntilconvergence.However,italsotellsusmorethan\nthat.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whether\nwearrivethereby\ufb01xedpointequationsornot.Thismeanswecantakethe\nfunctionalformfromthatequationbutregardsomeofthevaluesthatappearinit\nasparameters,thatwecanoptimizewithanyoptimization algorithmwelike.\nAsanexample,consideraverysimpleprobabilisticmodel,withlatentvariables\nh\u2208 R2andjustonevisiblevariable, v.Supposethat p( h)=N( h;0 , I)and\np( v| h)=N( v; w\ue03eh;1).Wecouldactuallysimplifythismodelbyintegrating\nout h;theresultisjustaGaussiandistributionover v.\u00a0Themodelitselfisnot\n6 4 8", "CHAPTER19.APPROXIMATEINFERENCE\ninteresting;wehaveconstructeditonlytoprovideasimpledemonstrationofhow\ncalculusofvariationsmaybeappliedtoprobabilisticmodeling.\nThetrueposteriorisgiven,uptoanormalizingconstant,by\np( ) h v| (19.57)\n\u221d p ,( h v) (19.58)\n=( p h 1)( p h 2)( ) p v h| (19.59)\n\u221dexp\ue012\n\u22121\n2\ue002\nh2\n1+ h2\n2+( v h\u2212 1 w 1\u2212 h 2 w 2)2\ue003\ue013\n(19.60)\n=exp\ue012\n\u22121\n2\ue002\nh2\n1+ h2\n2+ v2+ h2\n1 w2\n1+ h2\n2 w2\n2\u22122 v h 1 w 1\u22122 v h 2 w 2+2 h 1 w 1 h 2 w 2\ue003\ue013\n.\n(19.61)\nDuetothepresenceofthetermsmultiplying h 1and h 2together,wecanseethat\nthetrueposteriordoesnotfactorizeover h 1and h 2.\nApplyingequation,we\ufb01ndthat 19.56\n\u02dc q h( 1| v) (19.62)\n=exp\ue000\nE h 2\u223c q ( h 2| v )log \u02dc p ,( v h)\ue001\n(19.63)\n=exp\ue012\n\u22121\n2Eh 2\u223c q ( h 2| v )\ue002\nh2\n1+ h2\n2+ v2+ h2\n1 w2\n1+ h2\n2 w2\n2(19.64)\n\u22122 v h 1 w 1\u22122 v h 2 w 2+2 h 1 w 1 h 2 w 2]\ue013\n. (19.65)\nFromthis,wecanseethattherearee\ufb00ectivelyonlytwovaluesweneedtoobtain\nfrom q( h 2| v): E h 2\u223c | q ( h v )[ h 2]and E h 2\u223c | q ( h v )[ h2\n2].\u00a0Writingtheseas\ue068 h 2\ue069and\ue068 h2\n2\ue069,\nweobtain\n\u02dc q h( 1| v) = exp\ue012\n\u22121\n2\ue002\nh2\n1+\ue068 h2\n2\ue069+ v2+ h2\n1 w2\n1+\ue068 h2\n2\ue069 w2\n2(19.66)\n\u22122 v h 1 w 1\u2212\ue0682 v h 2\ue069 w 2+2 h 1 w 1\ue068 h 2\ue069 w 2]\ue013\n.(19.67)\nFromthis,wecanseethat\u02dc qhasthefunctionalformofaGaussian.Wecan\nthusconclude q( h v|)=N( h; \u00b5 \u03b2 ,\u2212 1)where \u00b5anddiagonal \u03b2arevariational\nparametersthatwecanoptimizeusinganytechniquewechoose.Itisimportant\ntorecallthatwedidnoteverassumethat qwouldbeGaussian;itsGaussian\nformwasderivedautomatically byusingcalculusofvariationstomaximize qwith\n6 4 9", "CHAPTER19.APPROXIMATEINFERENCE\nrespecttoL.Usingthesameapproachonadi\ufb00erentmodelcouldyieldadi\ufb00erent\nfunctionalformof. q\nThiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes.\nForexamplesofrealapplicationsofvariationallearningwithcontinuousvariables\ninthecontextofdeeplearning,see (). Goodfellow e t a l .2013d\n19.4.4InteractionsbetweenLearningandInference\nUsingapproximate inferenceaspartofalearningalgorithma\ufb00ectsthelearning\nprocess,andthisinturna\ufb00ectstheaccuracyoftheinferencealgorithm.\nSpeci\ufb01cally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakes\ntheapproximating assumptionsunderlyingtheapproximate inferencealgorithm\nbecomemoretrue.Whentrainingtheparameters,variationallearningincreases\nE h\u223c qlog( ) p v h , . (19.68)\nForaspeci\ufb01c v,thisincreases p( h v|)forvaluesof hthathavehighprobability\nunder q( h v|)anddecreases p( h v|)forvaluesof hthathavelowprobability\nunder . q( ) h v|\nThisbehaviorcausesourapproximating assumptionstobecomeself-ful\ufb01lling\nprophecies.Ifwetrainthemodelwithaunimodalapproximate posterior,wewill\nobtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewould\nhaveobtainedbytrainingthemodelwithexactinference.\nComputingthetrueamountofharmimposedonamodelbyavariational\napproximationisthusverydi\ufb03cult.Thereexistseveralmethodsforestimating\nlog p( v).Weoftenestimate log p( v; \u03b8)aftertrainingthemodel,and\ufb01ndthat\nthegapwith L( v \u03b8 , , q)issmall.Fromthis,wecanconcludethatourvariational\napproximationisaccurateforthespeci\ufb01cvalueof \u03b8thatweobtainedfromthe\nlearningprocess.Weshouldnotconcludethatourvariationalapproximation is\naccurateingeneralorthatthevariationalapproximation didlittleharmtothe\nlearningprocess.Tomeasurethetrueamountofharminducedbythevariational\napproximation,wewouldneedtoknow \u03b8\u2217=max \u03b8log p( v; \u03b8).\u00a0Itispossiblefor\nL( v \u03b8 , , q)\u2248log p( v; \u03b8)andlog p( v; \u03b8)\ue01clog p( v; \u03b8\u2217)toholdsimultaneously.If\nmax qL( v \u03b8 ,\u2217, q)\ue01clog p( v; \u03b8\u2217),because \u03b8\u2217inducestoocomplicatedofaposterior\ndistributionforour qfamilytocapture,\u00a0then thelearningprocesswillnever\napproach \u03b8\u2217.Suchaproblemisverydi\ufb03culttodetect,becausewecanonlyknow\nforsurethatithappenedifwehaveasuperiorlearningalgorithmthatcan\ufb01nd \u03b8\u2217\nforcomparison.\n6 5 0", "CHAPTER19.APPROXIMATEINFERENCE\n19.5LearnedApproximateInference\nWehaveseenthatinferencecanbethoughtofasanoptimization procedure\nthatincreasesthevalueofafunction L.Explicitlyperformingoptimization via\niterativeproceduressuchas\ufb01xedpointequationsorgradient-basedoptimization\nisoftenveryexpensiveandtime-consuming. Manyapproachestoinferenceavoid\nthisexpensebylearningtoperformapproximateinference.\u00a0Speci\ufb01cally ,wecan\nthinkoftheoptimization processasafunction fthatmapsaninput vtoan\napproximatedistribution q\u2217=argmaxqL( v , q).Oncewethinkofthemulti-step\niterativeoptimization processasjustbeingafunction,wecanapproximateitwith\naneuralnetworkthatimplementsanapproximation \u02c6 f(;) v \u03b8.\n19.5.1Wake-Sleep\nOneofthemaindi\ufb03cultieswithtrainingamodeltoinfer hfrom visthatwe\ndonothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givena v,\nwedonotknowtheappropriate h.Themappingfrom vto hdependsonthe\nchoiceofmodelfamily,andevolvesthroughoutthelearningprocessas \u03b8changes.\nThewake-sleepalgorithm(Hinton1995bFrey1996 e t a l .,; e t a l .,)resolvesthis\nproblembydrawingsamplesofboth hand vfromthemodeldistribution.\u00a0For\nexample,inadirectedmodel,thiscanbedonecheaplybyperformingancestral\nsamplingbeginningat handendingat v.Theinferencenetworkcanthenbe\ntrainedtoperformthereversemapping:\u00a0predicting which hcausedthepresent\nv.Themaindrawbacktothisapproachisthatwewillonlybeabletotrainthe\ninferencenetworkonvaluesof vthathavehighprobabilityunderthemodel.Early\ninlearning,themodeldistributionwillnotresemblethedatadistribution,sothe\ninferencenetworkwillnothaveanopportunitytolearnonsamplesthatresemble\ndata.\nInsectionwesawthatonepossibleexplanationfortheroleofdreamsleep 18.2\ninhumanbeingsandanimalsisthatdreamscouldprovidethenegativephase\nsamplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegative\ngradientofthelogpartitionfunctionofundirectedmodels.Anotherpossible\nexplanationforbiologicaldreamingisthatitisprovidingsamplesfrom p( h v ,)\nwhichcanbeusedtotrainaninferencenetworktopredict hgiven v.Insome\nsenses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation.\nMonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonly\nthepositivephaseofthegradientforseveralstepsthenwithonlythenegative\nphaseofthegradientforseveralsteps.Humanbeingsandanimalsareusually\nawakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis\n6 5 1", "CHAPTER19.APPROXIMATEINFERENCE\nnotreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofan\nundirectedmodel.Learningalgorithmsbasedonmaximizing Lcanberunwith\nprolongedperiodsofimproving qandprolongedperiodsofimproving \u03b8,however.\nIftheroleofbiologicaldreamingistotrainnetworksforpredicting q,thenthis\nexplainshowanimalsareabletoremainawakeforseveralhours(thelongerthey\nareawake,thegreaterthegapbetweenLandlog p( v),butLwillremainalower\nbound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnot\nmodi\ufb01edduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,these\nideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreaming\naccomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearning\nratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromthe\nanimal\u2019stransitionmodel,onwhichtotraintheanimal\u2019spolicy.Orsleepmay\nservesomeotherpurposenotyetanticipatedbythemachinelearningcommunity.\n19.5.2OtherFormsofLearnedInference\nThisstrategyoflearnedapproximateinferencehasalsobeenappliedtoother\nmodels.SalakhutdinovandLarochelle2010()showedthatasinglepassina\nlearnedinferencenetworkcouldyieldfasterinferencethaniteratingthemean\ufb01eld\n\ufb01xedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningthe\ninferencenetwork,thenapplyingonestepofmean\ufb01eldtoimproveitsestimates,\nandtrainingtheinferencenetworktooutputthisre\ufb01nedestimateinsteadofits\noriginalestimate.\nWehavealreadyseeninsectionthatthepredictivesparsedecomposition 14.8\nmodeltrainsashallowencodernetworktopredictasparsecodefortheinput.\nThiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itis\npossibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencoder\nmaybeviewedasperforminglearnedapproximate MAPinference.Duetoits\nshallowencoder,PSDisnotabletoimplementthekindofcompetitionbetween\nunitsthatwehaveseeninmean\ufb01eldinference.However,thatproblemcanbe\nremediedbytrainingadeepencodertoperformlearnedapproximateinference,as\nintheISTAtechnique( ,). GregorandLeCun2010b\nLearned\u00a0approximate\u00a0inference\u00a0hasrecently\u00a0become\u00a0one\u00a0of\u00a0the\u00a0dominant\napproachestogenerativemodeling,intheformofthevariationalautoencoder\n(,; ,).Inthiselegantapproach,thereisnoneedto Kingma2013Rezende e t a l .2014\nconstructexplicittargetsfortheinferencenetwork.Instead,theinferencenetwork\nissimplyusedtode\ufb01ne L,andthentheparametersoftheinferencenetworkare\nadaptedtoincrease.Thismodelisdescribedindepthlater,insection. L 20.10.3\n6 5 2", "CHAPTER19.APPROXIMATEINFERENCE\nUsingapproximateinference,itispossibletotrainanduseawidevarietyof\nmodels.Manyofthesemodelsaredescribedinthenextchapter.\n6 5 3", "C h a p t e r 2 0\nD e e p Ge n e rat i v e Mo d e l s\nInthischapter,wepresentseveralofthespeci\ufb01ckindsofgenerativemodelsthat\ncanbebuiltandtrainedusingthetechniquespresentedinchapters\u2013.Allof1619\nthesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsome\nway.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly.\nOthersdonotallowtheevaluationoftheprobabilitydistributionfunction,but\nsupportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamples\nfromthedistribution.Someofthesemodelsarestructuredprobabilisticmodels\ndescribedintermsofgraphsandfactors,usingthelanguageofgraphicalmodels\npresentedinchapter.\u00a0Otherscannoteasilybedescribedintermsoffactors, 16\nbutrepresentprobabilitydistributionsnonetheless.\n20.1BoltzmannMachines\nBoltzmannmachineswereoriginallyintroducedasageneral\u201cconnectionist\u201d ap-\nproachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlman\ne t a l .,;1983Ackley1985Hinton1984HintonandSejnowski1986 e t a l .,; e t a l .,; ,).\nVariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelong\nagosurpassedthepopularityoftheoriginal.Inthissectionwebrie\ufb02yintroduce\nthebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingto\ntrainandperforminferenceinthemodel.\nWede\ufb01netheBoltzmannmachineovera d-dimensionalbinaryrandomvector\nx \u2208{0 ,1}d.\u00a0TheBoltzmannmachineisanenergy-basedmodel(section),16.2.4\n654", "CHAPTER20.DEEPGENERATIVEMODELS\nmeaningwede\ufb01nethejointprobabilitydistributionusinganenergyfunction:\nP() = xexp(()) \u2212 E x\nZ, (20.1)\nwhere E( x)istheenergyfunctionand Zisthepartitionfunctionthatensures\nthat\ue050\nx P() = 1 x.TheenergyfunctionoftheBoltzmannmachineisgivenby\nE() = x \u2212 x\ue03eU x b\u2212\ue03ex , (20.2)\nwhere Uisthe\u201cweight\u201dmatrixofmodelparametersand bisthevectorofbias\nparameters.\nInthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftraining\nexamples,eachofwhichare n-dimensional.Equationdescribesthejoint 20.1\nprobabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainly\nviable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablesto\nthosedescribedbytheweightmatrix.Speci\ufb01cally,itmeansthattheprobabilityof\noneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesof\ntheotherunits.\nTheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesare\nobserved.Inthiscase,thelatentvariables,canactsimilarlytohiddenunitsina\nmulti-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits.\nJustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresults\nintheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachine\nwithhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetween\nvariables.Instead,theBoltzmannmachinebecomesauniversalapproximator of\nprobabilitymassfunctionsoverdiscretevariables( ,). LeRouxandBengio2008\nFormally,wedecomposetheunits xintotwosubsets:thevisibleunits vand\nthelatent(orhidden)units.Theenergyfunctionbecomes h\nE ,( v h v ) = \u2212\ue03eR v v\u2212\ue03eW h h\u2212\ue03eS h b\u2212\ue03ev c\u2212\ue03eh .(20.3)\nBoltzmannMachineLearningLearningalgorithmsforBoltzmannmachines\nareusuallybasedonmaximumlikelihood.AllBoltzmannmachineshavean\nintractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-\nproximatedusingthetechniquesdescribedinchapter.18\nOneinterestingpropertyofBoltzmannmachineswhentrainedwithlearning\nrulesbasedonmaximumlikelihoodisthattheupdateforaparticularweight\nconnectingtwounitsdependsonlythestatisticsofthosetwounits,collected\nunderdi\ufb00erentdistributions: P m o de l( v)and \u02c6 P da t a( v) P m o de l( h v|).Therestofthe\n6 5 5", "CHAPTER20.DEEPGENERATIVEMODELS\nnetworkparticipatesinshapingthosestatistics,buttheweightcanbeupdated\nwithoutknowinganythingabouttherestofthenetworkorhowthosestatisticswere\nproduced.Thismeansthatthelearningruleis\u201clocal,\u201dwhichmakesBoltzmann\nmachinelearningsomewhatbiologicallyplausible.\u00a0Itisconceivablethatifeach\nneuronwerearandomvariableinaBoltzmannmachine,thentheaxonsand\ndendritesconnectingtworandomvariablescouldlearnonlybyobservingthe\ufb01ring\npatternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthe\npositivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnection\nstrengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949\nsummarizedwiththemnemonic\u201c\ufb01retogether,wiretogether.\u201d\u00a0Hebbian learning\nrulesareamongtheoldesthypothesizedexplanationsforlearninginbiological\nsystemsandremainrelevanttoday( ,). Giudice e t a l .2009\nOtherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseem\ntorequireustohypothesizetheexistenceofmoremachinerythanthis.For\nexample,forthebraintoimplementback-propagation inamultilayerperceptron,\nitseemsnecessaryforthebraintomaintainasecondarycommunication networkfor\ntransmittinggradientinformationbackwardsthroughthenetwork.Proposalsfor\nbiologicallyplausibleimplementations(andapproximations)ofback-propagation\nhavebeenmade(,;,)butremaintobevalidated,and Hinton2007aBengio2015\nBengio2015()linksback-propagationofgradientstoinferenceinenergy-based\nmodelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables).\nThenegativephaseofBoltzmannmachinelearningissomewhatharderto\nexplainfromabiologicalpointofview.Asarguedinsection,dreamsleep 18.2\nmaybeaformofnegativephasesampling.Thisideaismorespeculativethough.\n20.2RestrictedBoltzmannMachines\nInventedunderthenameharmonium(,),restrictedBoltzmann Smolensky1986\nmachinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels.\nWehavebrie\ufb02ydescribedRBMspreviously,insection.Herewereviewthe 16.7.1\npreviousinformationandgointomoredetail.RBMsareundirectedprobabilistic\ngraphicalmodelscontainingalayerofobservablevariablesandasinglelayerof\nlatentvariables.RBMsmaybestacked(oneontopoftheother)toformdeeper\nmodels.See\ufb01gureforsomeexamples.Inparticular,\ufb01gureashowsthe 20.1 20.1\ngraphstructureoftheRBMitself.Itisabipartitegraph,withnoconnections\npermittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthe\nlatentlayer.\n6 5 6", "CHAPTER20.DEEPGENERATIVEMODELS\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h( 1 )\n1 h( 1 )\n1 h( 1 )\n2 h( 1 )\n2 h( 1 )\n3 h( 1 )\n3\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n1 h( 2 )\n1 h( 2 )\n2 h( 2 )\n2h( 2 )\n3h( 2 )\n3\nh( 1 )\n4 h( 1 )\n4\n(a) (b)\nh( 1 )\n1h( 1 )\n1h( 1 )\n2h( 1 )\n2h( 1 )\n3 h( 1 )\n3\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n1 h( 2 )\n1 h( 2 )\n2 h( 2 )\n2 h( 2 )\n3 h( 2 )\n3\nh( 1 )\n4h( 1 )\n4\n(c)\nFigure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines.\n( a )TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedon\nabipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsinthe\notherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamong\nthehiddenunits.\u00a0Typicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutit\nispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A ( b )\ndeepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirected\nconnections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiple\nhiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparate\nlayers.Allofthelocalconditionalprobabilitydistributionsneededbythedeepbelief\nnetworkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofits\nconstituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwith\nacompletelyundirectedgraph,butitwouldneedintralayerconnectionstocapturethe\ndependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical ( c )\nmodelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayer\nconnections.\u00a0DBMsarelesscloselytiedtoRBMsthanDBNsare.\u00a0Wheninitializinga\nDBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.Some\nkindsofDBMsmaybetrainedwithout\ufb01rsttrainingasetofRBMs.\n6 5 7", "CHAPTER20.DEEPGENERATIVEMODELS\nWebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butas\nweseelaterthereareextensionstoothertypesofvisibleandhiddenunits.\nMoreformally,lettheobservedlayerconsistofasetof n vbinaryrandom\nvariableswhichwerefertocollectivelywiththevectorv.Werefertothelatentor\nhiddenlayerof n hbinaryrandomvariablesas. h\nLikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisan\nenergy-basedmodelwiththejointprobabilitydistributionspeci\ufb01edbyitsenergy\nfunction:\nP , (= v vh= ) = h1\nZexp(( )) \u2212 E v h , . (20.4)\nTheenergyfunctionforanRBMisgivenby\nE ,( v h b ) = \u2212\ue03ev c\u2212\ue03eh v\u2212\ue03eW h , (20.5)\nandisthenormalizingconstantknownasthepartitionfunction: Z\nZ=\ue058\nv\ue058\nhexp ( ) {\u2212 E v h ,} . (20.6)\nItisapparentfromthede\ufb01nitionofthepartitionfunction Zthatthenaivemethod\nofcomputing Z(exhaustivelysummingoverallstates)couldbecomputationally\nintractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesinthe\nprobabilitydistributiontocompute Zfaster.InthecaseofrestrictedBoltzmann\nmachines, ()formallyprovedthatthepartitionfunction LongandServedio2010 Z\nisintractable.Theintractablepartitionfunction Zimpliesthatthenormalized\njointprobabilitydistributionisalsointractabletoevaluate. P() v\n20.2.1ConditionalDistributions\nThough P( v)isintractable,thebipartitegraphstructureoftheRBMhasthe\nveryspecialpropertythatitsconditionaldistributions P(hv|)and P(vh|)are\nfactorialandrelativelysimpletocomputeandtosamplefrom.\nDerivingtheconditionaldistributionsfromthejointdistributionisstraightfor-\nward:\nP( ) = h v|P ,( h v)\nP() v(20.7)\n=1\nP() v1\nZexp\ue06e\nb\ue03ev c+\ue03eh v+\ue03eW h\ue06f\n(20.8)\n=1\nZ\ue030exp\ue06e\nc\ue03eh v+\ue03eW h\ue06f\n(20.9)\n6 5 8", "CHAPTER20.DEEPGENERATIVEMODELS\n=1\nZ\ue030exp\uf8f1\n\uf8f2\n\uf8f3n h\ue058\nj = 1c j h j+n h\ue058\nj = 1v\ue03eW : , j h j\uf8fc\n\uf8fd\n\uf8fe(20.10)\n=1\nZ\ue030n h\ue059\nj = 1exp\ue06e\nc j h j+ v\ue03eW : , j h j\ue06f\n(20.11)\nSinceweareconditioningonthevisibleunits v,wecantreattheseasconstant\nwithrespecttothedistribution P(hv|).Thefactorialnatureoftheconditional\nP(hv|)followsimmediately fromourabilitytowritethejointprobabilityover\nthevector hastheproductof(unnormalized) distributionsovertheindividual\nelements, h j.Itisnowasimplematterofnormalizingthedistributionsoverthe\nindividualbinary h j.\nP h( j= 1 ) =| v\u02dc P h( j= 1 )| v\n\u02dc P h( j= 0 )+| v \u02dc P h( j= 1 )| v(20.12)\n=exp\ue008\nc j+ v\ue03eW : , j\ue009\nexp0+exp {} { c j+ v\ue03e W : , j}(20.13)\n= \u03c3\ue010\nc j+ v\ue03eW : , j\ue011\n. (20.14)\nWecannowexpressthefullconditionaloverthehiddenlayerasthefactorial\ndistribution:\nP( ) = h v|n h\ue059\nj = 1\u03c3\ue010\n(2 1) (+ h\u2212 \ue00c c W\ue03ev)\ue011\nj.(20.15)\nAsimilarderivationwillshowthattheotherconditionofinteresttous, P( v h|),\nisalsoafactorialdistribution:\nP( ) = v h|n v\ue059\ni = 1\u03c3((2 1) (+ )) v\u2212 \ue00c b W hi . (20.16)\n20.2.2TrainingRestrictedBoltzmannMachines\nBecausetheRBMadmitse\ufb03cientevaluationanddi\ufb00erentiation of\u02dc P( v)and\ne\ufb03cientMCMCsamplingintheformofblockGibbssampling,itcanreadilybe\ntrainedwithanyofthetechniquesdescribedinchapterfortrainingmodels 18\nthathaveintractablepartitionfunctions.\u00a0ThisincludesCD,SML(PCD),ratio\nmatchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning,\ntheRBMisrelativelystraightforwardtotrainbecausewecancompute P(h| v)\n6 5 9", "CHAPTER20.DEEPGENERATIVEMODELS\nexactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmann\nmachine,combineboththedi\ufb03cultyofanintractablepartitionfunctionandthe\ndi\ufb03cultyofintractableinference.\n20.3DeepBeliefNetworks\nDeepbeliefnetworks(DBNs)wereoneofthe\ufb01rstnon-convolutionalmodels\ntosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hinton e t a l .,;,\n2007b).Theintroduction ofdeepbeliefnetworksin2006beganthecurrentdeep\nlearningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodels\nwereconsideredtoodi\ufb03culttooptimize.Kernelmachineswithconvexobjective\nfunctionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstrated\nthatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupport\nvectormachinesontheMNISTdataset( ,).Today,deepbelief Hinton e t a l .2006\nnetworkshavemostlyfallenoutoffavorandarerarelyused,evencomparedto\notherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedly\nrecognizedfortheirimportantroleindeeplearninghistory.\nDeepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables.\nThelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinary\norreal.Therearenointralayerconnections.Usually,everyunitineachlayeris\nconnectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstruct\nmoresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersare\nundirected.Theconnectionsbetweenallotherlayersaredirected,withthearrows\npointedtowardthelayerthatisclosesttothedata.See\ufb01gurebforanexample. 20.1\nADBNwith lhiddenlayerscontains lweightmatrices: W( 1 ), . . . , W( ) l.It\nalsocontains l+1biasvectors: b( 0 ), . . . , b( ) l,with b( 0 )providingthebiasesforthe\nvisiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenby\nP( h( ) l, h( 1 ) l\u2212) exp\u221d\ue010\nb( ) l\ue03eh( ) l+ b( 1 ) l\u2212\ue03eh( 1 ) l\u2212+ h( 1 ) l\u2212\ue03eW( ) lh( ) l\ue011\n,(20.17)\nP h(( ) k\ni= 1 | h( + 1 ) k) = \u03c3\ue010\nb( ) k\ni+ W( + 1 ) k \ue03e\n: , i h( + 1 ) k\ue011\n\u2200\u2200\u2208 \u2212 i , k1 , . . . , l2 ,(20.18)\nP v( i= 1 | h( 1 )) = \u03c3\ue010\nb( 0 )\ni+ W( 1 )\ue03e\n: , i h( 1 )\ue011\n\u2200 i .(20.19)\nInthecaseofreal-valuedvisibleunits,substitute\nv\u223cN\ue010\nv b;( 0 )+ W( 1 )\ue03eh( 1 ), \u03b2\u2212 1\ue011\n(20.20)\n6 6 0", "CHAPTER20.DEEPGENERATIVEMODELS\nwith \u03b2diagonalfortractability.Generalizations tootherexponentialfamilyvisible\nunitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayeris\njustanRBM.\nTogenerateasamplefromaDBN,we\ufb01rstrunseveralstepsofGibbssampling\nonthetoptwohiddenlayers.Thisstageisessentiallydrawingasamplefrom\ntheRBMde\ufb01nedbythetoptwohiddenlayers.Wecanthenuseasinglepassof\nancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisible\nunits.\nDeepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirected\nmodelsandundirectedmodels.\nInferenceinadeepbeliefnetworkisintractableduetotheexplainingaway\ne\ufb00ectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohidden\nlayersthathaveundirectedconnections.Evaluatingormaximizingthestandard\nevidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidence\nlowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetwork\nwidth.\nEvaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingthe\nproblemofintractableinferencetomarginalizeoutthelatentvariables,butalso\ntheproblemofanintractablepartitionfunctionwithintheundirectedmodelof\nthetoptwolayers.\nTotrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximize\nE v\u223c pdatalog p( v)usingcontrastivedivergenceorstochasticmaximumlikelihood.\nTheparametersoftheRBMthende\ufb01netheparametersofthe\ufb01rstlayerofthe\nDBN.Next,asecondRBMistrainedtoapproximatelymaximize\nE v\u223c pdata Eh( 1 )\u223c p( 1 ) ( h( 1 )| v )log p( 2 )( h( 1 )) (20.21)\nwhere p( 1 )istheprobabilitydistributionrepresentedbythe\ufb01rstRBMand p( 2 )\nistheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,\nthesecondRBMistrainedtomodelthedistributionde\ufb01nedbysamplingthe\nhiddenunitsofthe\ufb01rstRBM,whenthe\ufb01rstRBMisdrivenbythedata.\u00a0This\nprocedurecanberepeatedinde\ufb01nitely,toaddasmanylayerstotheDBNas\ndesired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBM\nde\ufb01nesanotherlayeroftheDBN.Thisprocedurecanbejusti\ufb01edasincreasinga\nvariationallowerboundonthelog-likelihoodofthedataundertheDBN(Hinton\ne t a l .,).2006\nInmostapplications,noe\ufb00ortismadetojointlytraintheDBNafterthegreedy\nlayer-wiseprocedureiscomplete.However,itispossibletoperformgenerative\n\ufb01ne-tuningusingthewake-sleepalgorithm.\n6 6 1", "CHAPTER20.DEEPGENERATIVEMODELS\nThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostofthe\ninterestinDBNsarosefromtheirabilitytoimproveclassi\ufb01cationmodels.Wecan\ntaketheweightsfromtheDBNandusethemtode\ufb01neanMLP:\nh( 1 )= \u03c3\ue010\nb( 1 )+ v\ue03eW( 1 )\ue011\n. (20.22)\nh( ) l= \u03c3\ue010\nb( ) l\ni+ h( 1 ) l\u2212\ue03eW( ) l\ue011\n\u2200\u2208 l2 , . . . , m ,(20.23)\nAfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerative\ntrainingoftheDBN,wemaytraintheMLPtoperformaclassi\ufb01cationtask.This\nadditionaltrainingoftheMLPisanexampleofdiscriminative\ufb01ne-tuning.\nThisspeci\ufb01cchoiceofMLPissomewhatarbitrary,comparedtomanyofthe\ninferenceequationsinchapterthatarederivedfrom\ufb01rstprinciples.ThisMLP 19\nisaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistently\nintheliterature.Manyapproximate inferencetechniquesaremotivatedbytheir\nabilityto\ufb01ndamaximally variationallowerboundonthelog-likelihood t i g h t\nundersomesetofconstraints.Onecanconstructavariationallowerboundonthe\nlog-likelihoodusingthehiddenunitexpectationsde\ufb01nedbytheDBN\u2019sMLP,but\nthisistrueofprobabilitydistributionoverthehiddenunits,andthereisno a ny\nreasontobelievethatthisMLPprovidesaparticularlytightbound.\u00a0Inparticular,\ntheMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.The\nMLPpropagatesinformationupwardfromthevisibleunitstothedeepesthidden\nunits,butdoesnotpropagateanyinformationdownwardorsideways.TheDBN\ngraphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunits\nwithinthesamelayeraswellastop-downinteractionsbetweenlayers.\nWhilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwith\nAIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasa\ngenerativemodel.\nTheterm\u201cdeepbeliefnetwork\u201discommonlyusedincorrectlytorefertoany\nkindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics.\nTheterm\u201cdeepbeliefnetwork\u201dshouldreferspeci\ufb01callytomodelswithundirected\nconnectionsinthedeepestlayeranddirectedconnectionspointingdownward\nbetweenallotherpairsofconsecutivelayers.\nTheterm\u201cdeepbeliefnetwork\u201dmayalsocausesomeconfusionbecausethe\nterm\u201cbeliefnetwork\u201dissometimesusedtorefertopurelydirectedmodels,while\ndeepbeliefnetworkscontainanundirectedlayer.Deepbeliefnetworksalsoshare\ntheacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,),\nwhichareBayesiannetworksforrepresentingMarkovchains.\n6 6 2", "CHAPTER20.DEEPGENERATIVEMODELS\nh( 1 )\n1 h( 1 )\n1 h( 1 )\n2 h( 1 )\n2 h( 1 )\n3 h( 1 )\n3\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\n1 h( 2 )\n1 h( 2 )\n2 h( 2 )\n2h( 2 )\n3h( 2 )\n3\nh( 1 )\n4 h( 1 )\n4\nFigure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer\n(bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers.\nTherearenointralayerlayerconnections.\n20.4DeepBoltzmannMachines\nAdeepBoltzmannmachineorDBM(Salakhutdino vandHinton2009a,)is\nanotherkindofdeep,generativemodel.\u00a0Unlikethedeepbeliefnetwork(DBN),\nitisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayers\noflatentvariables(RBMshavejustone).\u00a0ButliketheRBM,withineachlayer,\neachofthevariablesaremutuallyindependent,conditionedonthevariablesin\ntheneighboringlayers.See\ufb01gureforthegraphstructure.DeepBoltzmann 20.2\nmachineshavebeenappliedtoavarietyoftasksincludingdocumentmodeling\n(Srivastava2013 e t a l .,).\nLikeRBMsandDBNs,\u00a0DBMstypicallycontainonlybinaryunits\u2014aswe\nassumeforsimplicityofourpresentationofthemodel\u2014butitisstraightforward\ntoincludereal-valuedvisibleunits.\nADBMisanenergy-basedmodel,meaningthatthethejointprobability\ndistributionoverthemodelvariablesisparametrized byanenergyfunction E.In\nthecaseofadeepBoltzmannmachinewithonevisiblelayer, v,andthreehidden\nlayers, h( 1 ), h( 2 )and h( 3 ),thejointprobabilityisgivenby:\nP\ue010\nv h ,( 1 ), h( 2 ), h( 3 )\ue011\n=1\nZ() \u03b8exp\ue010\n\u2212 E ,( v h( 1 ), h( 2 ), h( 3 );) \u03b8\ue011\n.(20.24)\nTosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergy\nfunctionisthende\ufb01nedasfollows:\nE ,( v h( 1 ), h( 2 ), h( 3 );) = \u03b8 \u2212 v\ue03eW( 1 )h( 1 )\u2212 h( 1 )\ue03eW( 2 )h( 2 )\u2212 h( 2 )\ue03eW( 3 )h( 3 ).\n(20.25)\n6 6 3", "CHAPTER20.DEEPGENERATIVEMODELS\nh( 1 )\n1 h( 1 )\n1 h( 1 )\n2 h( 1 )\n2h( 1 )\n3h( 1 )\n3\nv 1 v 1 v 2 v 2h( 2 )\n1h( 2 )\n1h( 2 )\n2h( 2 )\n2h( 2 )\n3 h( 2 )\n3h( 3 )\n1 h( 3 )\n1 h( 3 )\n2 h( 3 )\n2\nv1\nv2h( 2 )\n1 h( 2 )\n1\nh( 2 )\n2 h( 2 )\n2\nh( 2 )\n3h( 2 )\n3\nh( 1 )\n1 h( 1 )\n1\nh( 1 )\n2 h( 1 )\n2\nh( 1 )\n3 h( 1 )\n3h( 3 )\n1 h( 3 )\n1\nh( 3 )\n2 h( 3 )\n2\nFigure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure.\nIncomparisontotheRBMenergyfunction(equation),theDBMenergy 20.5\nfunctionincludesconnectionsbetweenthehiddenunits(latentvariables)inthe\nformoftheweightmatrices( W( 2 )and W( 3 )).Aswewillsee,theseconnections\nhavesigni\ufb01cantconsequencesforboththemodelbehavioraswellashowwego\naboutperforminginferenceinthemodel.\nIncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-\nnectedtoeveryotherunit),theDBMo\ufb00erssomeadvantagesthataresimilar\ntothoseo\ufb00eredbytheRBM.Speci\ufb01cally,asillustratedin\ufb01gure,theDBM20.3\nlayerscanbeorganizedintoabipartitegraph,withoddlayersononesideand\nevenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthe\nvariablesintheevenlayer,thevariablesintheoddlayersbecomeconditionally\nindependent.Ofcourse,whenweconditiononthevariablesintheoddlayers,the\nvariablesintheevenlayersalsobecomeconditionallyindependent.\nThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-\ntionswehavepreviouslyusedfortheconditionaldistributionsofanRBMto\ndeterminetheconditionaldistributionsinaDBM.Theunitswithinalayerare\nconditionallyindependentfromeachothergiventhevaluesoftheneighboring\nlayers,sothedistributionsoverbinaryvariablescanbefullydescribedbythe\nBernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inour\nexamplewithtwohiddenlayers,theactivationprobabilities aregivenby:\nP v( i= 1 | h( 1 )) = \u03c3\ue010\nW( 1 )\ni , : h( 1 )\ue011\n, (20.26)\n6 6 4", "CHAPTER20.DEEPGENERATIVEMODELS\nP h(( 1 )\ni= 1 | v h ,( 2 )) = \u03c3\ue010\nv\ue03eW( 1 )\n: , i+ W( 2 )\ni , : h( 2 )\ue011\n(20.27)\nand\nP h(( 2 )\nk= 1 | h( 1 )) = \u03c3\ue010\nh( 1 )\ue03eW( 2 )\n: , k\ue011\n. (20.28)\nThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachine\ne\ufb03cient.\u00a0ThenaiveapproachtoGibbssamplingistoupdateonlyonevariable\natatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandall\nofthehiddenunitstobeupdatedinasecondblock.Onemightnaivelyassume\nthataDBMwith llayersrequires l+1updates,witheachiterationupdatinga\nblockconsistingofonelayerofunits.Instead,itispossibletoupdateallofthe\nunitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksof\nupdates,oneincludingallevenlayers(includingthevisiblelayer)andtheother\nincludingalloddlayers.DuetothebipartiteDBMconnectionpattern,given\ntheevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbe\nsampledsimultaneouslyandindependentlyasablock.Likewise,giventheodd\nlayers,theevenlayerscanbesampledsimultaneouslyandindependentlyasa\nblock.E\ufb03cientsamplingisespeciallyimportantfortrainingwiththestochastic\nmaximumlikelihoodalgorithm.\n20.4.1InterestingProperties\nDeepBoltzmannmachineshavemanyinterestingproperties.\nDBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-\ntion P( h v|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityof\nthisposteriordistributionallowsricherapproximationsoftheposterior.Inthecase\noftheDBN,weperformclassi\ufb01cationusingaheuristicallymotivatedapproximate\ninferenceprocedure,inwhichweguessthatareasonablevalueforthemean\ufb01eld\nexpectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthe\nnetworkinanMLPthatusessigmoidactivationfunctionsandthesameweightsas\ntheoriginalDBN.distribution A ny Q( h)maybeusedtoobtainavariationallower\nboundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtain\nsuchabound.However,theboundisnotexplicitlyoptimizedinanyway,sothe\nboundmaybefarfromtight.Inparticular,theheuristicestimateof Qignores\ninteractionsbetweenhiddenunitswithinthesamelayeraswellasthetop-down\nfeedbackin\ufb02uenceofhiddenunitsindeeperlayersonhiddenunitsthatarecloser\ntotheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBN\nisnotabletoaccountfortheseinteractions, theresulting Qispresumablyfar\n6 6 5", "CHAPTER20.DEEPGENERATIVEMODELS\nfromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionally\nindependentgiventheotherlayers.\u00a0Thislackofintralayerinteractionmakesit\npossibletouse\ufb01xedpointequationstoactuallyoptimizethevariationallower\nboundand\ufb01ndthetrueoptimalmean\ufb01eldexpectations(towithinsomenumerical\ntolerance).\nTheuseofpropermean\ufb01eldallowstheapproximate inferenceprocedurefor\nDBMstocapturethein\ufb02uenceoftop-downfeedbackinteractions. Thismakes\nDBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrain\nisknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,\nDBMshavebeenusedascomputational modelsofrealneuroscienti\ufb01cphenomena\n(,; Series e t a l .2010Reichert2011 e t a l .,).\nOneunfortunatepropertyofDBMsisthatsamplingfromthemisrelatively\ndi\ufb03cult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.The\notherlayersareusedonlyattheendofthesamplingprocess,inonee\ufb03cient\nancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessaryto\nuseMCMCacrossalllayers,witheverylayerofthemodelparticipating inevery\nMarkovchaintransition.\n20.4.2DBMMeanFieldInference\nTheconditionaldistributionoveroneDBMlayergiventheneighboringlayersis\nfactorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributions\nare P( v h|( 1 )), P( h( 1 )| v h ,( 2 ))and P( h( 2 )| h( 1 )).Thedistributionover a l l\nhiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers.\nIntheexamplewithtwohiddenlayers, P( h( 1 ), h( 2 )| v)doesnotfactorizeduedue\ntotheinteractionweights W( 2 )between h( 1 )and h( 2 )whichrenderthesevariables\nmutuallydependent.\nAswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximate\ntheDBMposteriordistribution.\u00a0However,unliketheDBN,theDBMposterior\ndistributionovertheirhiddenunits\u2014whilecomplicated\u2014is easytoapproximate\nwithavariationalapproximation(asdiscussedinsection),\u00a0speci\ufb01callya 19.4\nmean\ufb01eldapproximation. Themean\ufb01eldapproximation isasimpleformof\nvariationalinference,wherewerestricttheapproximatingdistributiontofully\nfactorialdistributions.InthecontextofDBMs,themean\ufb01eldequationscapture\nthebidirectionalinteractionsbetweenlayers.Inthissectionwederivetheiterative\napproximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton\n().2009a\nInvariationalapproximations toinference,weapproachthetaskofapproxi-\n6 6 6", "CHAPTER20.DEEPGENERATIVEMODELS\nmatingaparticulartargetdistribution\u2014inourcase,theposteriordistributionover\nthehiddenunitsgiventhevisibleunits\u2014bysomereasonablysimplefamilyofdis-\ntributions.Inthecaseofthemean\ufb01eldapproximation, theapproximating family\nisthesetofdistributionswherethehiddenunitsareconditionallyindependent.\nWenowdevelopthemean\ufb01eldapproachfortheexamplewithtwohidden\nlayers.Let Q( h( 1 ), h( 2 )| v)betheapproximation of P( h( 1 ), h( 2 )| v).Themean\n\ufb01eldassumptionimpliesthat\nQ( h( 1 ), h( 2 )| v) =\ue059\njQ h(( 1 )\nj| v)\ue059\nkQ h(( 2 )\nk| v) .(20.29)\nThemean\ufb01eldapproximationattemptsto\ufb01ndamemberofthisfamilyof\ndistributionsthatbest\ufb01tsthetrueposterior P( h( 1 ), h( 2 )| v).\u00a0Importantly ,the\ninferenceprocessmustberunagainto\ufb01ndadi\ufb00erentdistribution Qeverytime\nweuseanewvalueof. v\nOnecanconceiveofmanywaysofmeasuringhowwell Q( h v|)\ufb01ts P( h v|).\nThemean\ufb01eldapproachistominimize\nKL( ) = Q P\ue06b\ue058\nhQ( h( 1 ), h( 2 )| v)log\ue020\nQ( h( 1 ), h( 2 )| v)\nP( h( 1 ) , h( 2 )| v)\ue021\n.(20.30)\nIngeneral,wedonothavetoprovideaparametricformoftheapproximating\ndistributionbeyondenforcingtheindependenceassumptions.Thevariational\napproximationprocedureisgenerallyabletorecoverafunctionalformofthe\napproximatedistribution.However,inthecaseofamean\ufb01eldassumptionon\nbinaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgenerality\nresultingfrom\ufb01xingaparametrization ofthemodelinadvance.\nWeparametrize QasaproductofBernoullidistributions,thatisweassociate\ntheprobabilityofeachelementof h( 1 )withaparameter.Speci\ufb01cally,foreach j,\n\u02c6h( 1 )\nj= Q( h( 1 )\nj= 1| v),where \u02c6h( 1 )\nj\u2208[0 ,1]andforeach k,\u02c6h( 2 )\nk= Q( h( 2 )\nk= 1| v),\nwhere \u02c6 h( 2 )\nk\u2208[01] ,.Thuswehavethefollowingapproximationtotheposterior:\nQ( h( 1 ), h( 2 )| v) =\ue059\njQ h(( 1 )\nj| v)\ue059\nkQ h(( 2 )\nk| v) (20.31)\n=\ue059\nj(\u02c6 h( 1 )\nj)h( 1 )\nj(1\u2212\u02c6h( 1 )\nj)( 1\u2212 h( 1 )\nj )\u00d7\ue059\nk(\u02c6h( 2 )\nk)h( 2 )\nk(1\u2212\u02c6h( 2 )\nk)( 1\u2212 h( 2 )\nk).\n(20.32)\nOfcourse,forDBMswithmorelayerstheapproximateposteriorparametrization\ncanbeextendedintheobviousway,exploitingthebipartitestructureofthegraph\n6 6 7", "CHAPTER20.DEEPGENERATIVEMODELS\ntoupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheodd\nlayerssimultaneously,followingthesamescheduleasGibbssampling.\nNowthatwehavespeci\ufb01edourfamilyofapproximating distributions Q,it\nremainstospecifyaprocedureforchoosingthememberofthisfamilythatbest\n\ufb01ts P.Themoststraightforwardwaytodothisistousethemean\ufb01eldequations\nspeci\ufb01edbyequation.Theseequationswerederivedbysolvingforwherethe 19.56\nderivativesofthevariationallowerboundarezero.Theydescribeinanabstract\nmannerhowtooptimizethevariationallowerboundforanymodel,simplyby\ntakingexpectationswithrespectto. Q\nApplyingthesegeneralequations,weobtaintheupdaterules(again,ignoring\nbiasterms):\n\u02c6h( 1 )\nj= \u03c3\ue020\ue058\niv i W( 1 )\ni , j+\ue058\nk\ue030W( 2 )\nj , k\ue030\u02c6 h( 2 )\nk\ue030\ue021\n, j\u2200 (20.33)\n\u02c6h( 2 )\nk= \u03c3\uf8eb\n\uf8ed\ue058\nj\ue030W( 2 )\nj\ue030 , k\u02c6h( 1 )\nj\ue030\uf8f6\n\uf8f8 , k .\u2200 (20.34)\nAta\ufb01xedpointofthissystemofequations,wehavealocalmaximumofthe\nvariationallowerbound L( Q).Thusthese\ufb01xedpointupdateequationsde\ufb01nean\niterativealgorithmwherewealternateupdatesof\u02c6h( 1 )\nj(usingequation)and20.33\nupdatesof\u02c6h( 2 )\nk(usingequation).OnsmallproblemssuchasMNIST,asfew 20.34\nasteniterationscanbesu\ufb03cientto\ufb01ndanapproximate positivephasegradient\nforlearning,and\ufb01ftyusuallysu\ufb03cetoobtainahighqualityrepresentationof\nasinglespeci\ufb01cexampletobeusedforhigh-accuracy classi\ufb01cation.Extending\napproximatevariationalinferencetodeeperDBMsisstraightforward.\n20.4.3DBMParameterLearning\nLearningintheDBMmustconfrontboththechallengeofanintractablepartition\nfunction,usingthetechniquesfromchapter,andthechallengeofanintractable 18\nposteriordistribution,usingthetechniquesfromchapter.19\nAsdescribedinsection,variationalinferenceallowstheconstructionof 20.4.2\nadistribution Q( h v|)thatapproximates theintractable P( h v|).Learningthen\nproceedsbymaximizing L( v \u03b8 , Q ,),thevariationallowerboundontheintractable\nlog-likelihood, . log(;) P v \u03b8\n6 6 8", "CHAPTER20.DEEPGENERATIVEMODELS\nForadeepBoltzmannmachinewithtwohiddenlayers,isgivenby L\nL( ) = Q , \u03b8\ue058\ni\ue058\nj\ue030v i W( 1 )\ni , j\ue030\u02c6h( 1 )\nj\ue030+\ue058\nj\ue030\ue058\nk\ue030\u02c6h( 1 )\nj\ue030 W( 2 )\nj\ue030 , k\ue030\u02c6h( 2 )\nk\ue030\u2212 H log()+ Z \u03b8 () Q .(20.35)\nThisexpressionstillcontainsthelogpartitionfunction, log Z( \u03b8).Becauseadeep\nBoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,the\nhardnessresultsforcomputingthepartitionfunctionandsamplingthatapplyto\nrestrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.Thismeans\nthatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequires\napproximatemethodssuchasannealedimportancesampling.Likewise,training\nthemodelrequiresapproximationstothegradientofthelogpartitionfunction.See\nchapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained 18\nusingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedin\nchapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe 18\nabilitytoevaluatetheunnormalized probabilities, ratherthanmerelyobtaina\nvariationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmann\nmachinesbecausetheydonotallowe\ufb03cientsamplingofthehiddenunitsgiventhe\nvisibleunits\u2014instead,contrastivedivergencewouldrequireburninginaMarkov\nchaineverytimeanewnegativephasesampleisneeded.\nThenon-variationalversionofstochasticmaximumlikelihoodalgorithmwas\ndiscussedearlier,insection.\u00a0Variationalstochasticmaximumlikelihoodas 18.2\nappliedtotheDBMisgiveninalgorithm .Recallthatwedescribeasimpli\ufb01ed 20.1\nvarientoftheDBMthatlacksbiasparameters;includingthemistrivial.\n20.4.4Layer-WisePretraining\nUnfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribed\nabove)fromarandominitialization usuallyresultsinfailure.Insomecases,the\nmodelfailstolearntorepresentthedistributionadequately.Inothercases,the\nDBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancould\nbeobtainedwithjustanRBM.ADBMwithverysmallweightsinallbutthe\ufb01rst\nlayerrepresentsapproximatelythesamedistributionasanRBM.\nVarioustechniquesthatpermitjointtraininghavebeendevelopedandare\ndescribedinsection.However,theoriginalandmostpopularmethodfor 20.4.5\novercomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining.\nInthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.The\n\ufb01rstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedto\nmodelsamplesfromthepreviousRBM\u2019sposteriordistribution.\u00a0Afterallofthe\n6 6 9", "CHAPTER20.DEEPGENERATIVEMODELS\nAlgorithm20.1Thevariationalstochasticmaximumlikelihoodalgorithmfor\ntrainingaDBMwithtwohiddenlayers.\nSet,thestepsize,toasmallpositivenumber \ue00f\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainof\np( v h ,( 1 ), h( 2 ); \u03b8+ \ue00f\u2206 \u03b8)toburnin,startingfromsamplesfrom p( v h ,( 1 ), h( 2 ); \u03b8).\nInitializethreematrices,\u02dc V,\u02dc H( 1 )and \u02dc H( 2 )eachwith mrowssettorandom\nvalues(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedto\nthemodel\u2019smarginals).\nwhilenotconverged(learningloop)do\nSampleaminibatchof mexamplesfromthetrainingdataandarrangethem\nastherowsofadesignmatrix. V\nInitializematrices \u02c6 H( 1 )and \u02c6 H( 2 ),possiblytothemodel\u2019smarginals.\nwhilenotconverged(mean\ufb01eldinferenceloop)do\n\u02c6 H( 1 )\u2190 \u03c3\ue010\nV W( 1 )+\u02c6 H( 2 )W( 2 )\ue03e\ue011\n.\n\u02c6 H( 2 )\u2190 \u03c3\ue010\n\u02c6 H( 1 )W( 2 )\ue011\n.\nendwhile\n\u2206W( 1 )\u21901\nmV\ue03e\u02c6 H( 1 )\n\u2206W( 2 )\u21901\nm\u02c6 H( 1 )\ue03e\u02c6 H( 2 )\nfor do l k = 1to(Gibbssampling)\nGibbsblock1:\n\u2200 i , j ,\u02dc V i , jsampledfrom P(\u02dc V i , j= 1) = \u03c3\ue012\nW( 1 )\nj , :\ue010\n\u02dc H( 1 )\ni , :\ue011\ue03e\ue013\n.\n\u2200 i , j ,\u02dc H( 2 )\ni , jsampledfrom P(\u02dc H( 2 )\ni , j= 1) = \u03c3\ue010\n\u02dc H( 1 )\ni , : W( 2 )\n: , j\ue011\n.\nGibbsblock2:\n\u2200 i , j ,\u02dc H( 1 )\ni , jsampledfrom P(\u02dc H( 1 )\ni , j= 1) = \u03c3\ue010\n\u02dc V i , : W( 1 )\n: , j+\u02dc H( 2 )\ni , : W( 2 )\ue03e\nj , :\ue011\n.\nendfor\n\u2206W( 1 )\u2190\u2206W( 1 )\u22121\nmV\ue03e\u02dc H( 1 )\n\u2206W( 2 )\u2190\u2206W( 2 )\u22121\nm\u02dc H( 1 )\ue03e\u02dc H( 2 )\nW( 1 )\u2190 W( 1 )+ \ue00f\u2206W( 1 )(thisisacartoonillustration,inpracticeuseamore\ne\ufb00ectivealgorithm,suchasmomentumwithadecayinglearningrate)\nW( 2 )\u2190 W( 2 )+\u2206 \ue00fW( 2 )\nendwhile\n6 7 0", "CHAPTER20.DEEPGENERATIVEMODELS\nRBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.The\nDBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlya\nsmallchangeinthemodel\u2019sparametersanditsperformanceasmeasuredbythe\nlog-likelihooditassignstothedata,oritsabilitytoclassifyinputs.See\ufb01gure20.4\nforanillustrationofthetrainingprocedure.\nThisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbears\nsomepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetof\ntheparametersateachstep.Thetwomethodsdi\ufb00erbecausethegreedylayer-wise\ntrainingprocedureusesadi\ufb00erentobjectivefunctionateachstep.\nGreedylayer-wisepretrainingofaDBMdi\ufb00ersfromgreedylayer-wisepre-\ntrainingofaDBN.TheparametersofeachindividualRBMmaybecopiedto\nthecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparameters\nmustbemodi\ufb01edbeforeinclusionintheDBM.Alayerinthemiddleofthestack\nofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombined\ntoformtheDBM,thelayerwillhavebothbottom-upandtop-downinput.\u00a0To\naccountforthise\ufb00ect,SalakhutdinovandHinton2009a()advocatedividingthe\nweightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintothe\nDBM.Additionally,thebottomRBMmustbetrainedusingtwo\u201ccopies\u201dofeach\nvisibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeans\nthattheweightsaree\ufb00ectivelydoubledduringtheupwardpass.Similarly,thetop\nRBMshouldbetrainedwithtwocopiesofthetopmostlayer.\nObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequires\namodi\ufb01cationofthestandardSMLalgorithm,whichistouseasmallamountof\nmean\ufb01eldduringthenegativephaseofthejointPCDtrainingstep(Salakhutdinov\nandHinton2009a,).\u00a0Speci\ufb01cally,theexpectationoftheenergygradientshould\nbecomputedwithrespecttothemean\ufb01elddistributioninwhichalloftheunits\nareindependentfromeachother.Theparametersofthismean\ufb01elddistribution\nshouldbeobtainedbyrunningthemean\ufb01eld\ufb01xedpointequationsforjustone\nstep.See ()foracomparisonoftheperformanceofcentered Goodfellow e t a l .2013b\nDBMswithandwithouttheuseofpartialmean\ufb01eldinthenegativephase.\n20.4.5JointlyTrainingDeepBoltzmannMachines\nClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassi\ufb01cation\nwell,requireaseparateMLP-basedclassi\ufb01erontopofthehiddenfeaturesthey\nextract.Thishassomeundesirableproperties.Itishardtotrackperformance\nduringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhile\ntrainingthe\ufb01rstRBM.Thus,itishardtotellhowwellourhyperparameters\n6 7 1", "CHAPTER20.DEEPGENERATIVEMODELS\nd)a) b)\nc )\nFigure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNIST\ndataset(SalakhutdinovandHinton2009aSrivastava2014 ,; e t a l .,).TrainanRBM ( a )\nbyusingCDtoapproximatelymaximizelog P( v).TrainasecondRBMthatmodels ( b )\nh( 1 )andtargetclassybyusingCD- ktoapproximatelymaximizelog P( h( 1 ),y)where\nh( 1 )isdrawnfromthe\ufb01rstRBM\u2019sposteriorconditionedonthedata.Increase kfrom1\nto20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately ( c )\nmaximizelog P(v ,y)usingstochasticmaximumlikelihoodwith k= 5.Delete ( d )yfrom\nthemodel.De\ufb01neanewsetoffeatures h( 1 )and h( 2 )thatareobtainedbyrunningmean\n\ufb01eldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhose\nstructureisthesameasanadditionalpassofmean\ufb01eld,withanadditionaloutputlayer\nfortheestimateofy.InitializetheMLP\u2019sweightstobethesameastheDBM\u2019sweights.\nTraintheMLPtoapproximatelymaximizelog P(y|v)usingstochasticgradientdescent\nanddropout.Figurereprintedfrom( ,). Goodfellow e t a l .2013b\n6 7 2", "CHAPTER20.DEEPGENERATIVEMODELS\nareworkinguntilquitelateinthetrainingprocess.Softwareimplementations\nofDBMsneedtohavemanydi\ufb00erentcomponentsforCDtrainingofindividual\nRBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagation\nthroughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmany\noftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeing\nabletoperforminferencewhensomeinputvaluesaremissing.\nTherearetwomainwaystoresolvethejointtrainingproblemofthedeep\nBoltzmannmachine.The\u00a0\ufb01rstisthecentereddeepBoltzmann\u00a0machine\n(MontavonandMuller2012,),whichreparametrizes themodelinordertomake\ntheHessianofthecostfunctionbetter-conditionedatthebeginningofthelearning\nprocess.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wise\npretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihood\nandproduceshighqualitysamples.Unfortunately,itremainsunabletocompete\nwithappropriately regularizedMLPsasaclassi\ufb01er.Thesecondwaytojointly\ntrainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmann\nmachine(Goodfellow2013b e t a l .,).Thismodelusesanalternativetraining\ncriterionthatallowstheuseoftheback-propagationalgorithminordertoavoid\ntheproblemswithMCMCestimatesofthegradient.Unfortunately,\u00a0thenew\ncriteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMC\napproach,itdoesleadtosuperiorclassi\ufb01cationperformanceandabilitytoreason\nwellaboutmissinginputs.\nThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwe\nreturntothegeneralviewofaBoltzmannmachineasconsistingofasetofunits\nxwithaweightmatrix Uandbiases b.Recallfromequationthatheenergy 20.2\nfunctionisgivenby\nE() = x \u2212 x\ue03eU x b\u2212\ue03ex . (20.36)\nUsing\u00a0di\ufb00erent\u00a0sparsity\u00a0patternsin\u00a0theweight\u00a0matrix U,\u00a0wecan\u00a0implemen t\nstructuresofBoltzmannmachines,suchasRBMs,orDBMswithdi\ufb00erentnumbers\noflayers.Thisisaccomplishedbypartitioning xintovisibleandhiddenunitsand\nzeroingoutelementsof Uforunitsthatdonotinteract.ThecenteredBoltzmann\nmachineintroducesavectorthatissubtractedfromallofthestates: \u00b5\nE\ue030(; ) = ( ) x U b , \u2212 x \u00b5\u2212\ue03eU x \u00b5 x \u00b5 (\u2212)(\u2212 \u2212)\ue03eb .(20.37)\nTypically \u00b5isahyperparameter\ufb01xedatthebeginningoftraining.Itisusu-\nallychosentomakesurethat x \u00b5\u2212 \u22480whenthemodelisinitialized.This\nreparametrization doesnotchangethesetofprobabilitydistributionsthatthe\nmodelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescent\nappliedtothelikelihood.Speci\ufb01cally,inmanycases,thisreparametrization results\n6 7 3", "CHAPTER20.DEEPGENERATIVEMODELS\ninaHessianmatrixthatisbetterconditioned. ()experimentally Melchior e t a l .2013\ncon\ufb01rmedthattheconditioningoftheHessianmatriximproves,andobservedthat\nthecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,\ntheenhancedgradient(,).Theimprovedconditioningofthe Cho e t a l .2011\nHessianmatrixallowslearningtosucceed,evenindi\ufb03cultcasesliketraininga\ndeepBoltzmannmachinewithmultiplelayers.\nTheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-\npredictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemean\n\ufb01eldequationsasde\ufb01ningafamilyofrecurrentnetworksforapproximately solving\neverypossibleinferenceproblem( ,).Ratherthantraining Goodfellow e t a l .2013b\nthemodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrent\nnetworkobtainanaccurateanswertothecorrespondinginferenceproblem.The\ntrainingprocessisillustratedin\ufb01gure.\u00a0Itconsistsofrandomlysamplinga 20.5\ntrainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,\nandthentrainingtheinferencenetworktopredictthevaluesoftheremaining\nunits.\nThisgeneralprincipleofback-propagating throughthecomputational graph\nforapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011 e t a l .,;\nBrakel2013 e t a l .,).InthesemodelsandintheMP-DBM,the\ufb01nallossisnot\nthelowerboundonthelikelihood.Instead,the\ufb01nallossistypicallybasedon\ntheapproximateconditionaldistributionthattheapproximate inferencenetwork\nimposesoverthemissingvalues.Thismeansthatthetrainingofthesemodels\nissomewhatheuristicallymotivated.Ifweinspectthe p( v)representedbythe\nBoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,\ninthesensethatGibbssamplingyieldspoorsamples.\nBack-propagationthroughtheinferencegraphhastwomainadvantages.First,\nittrainsthemodelasitisreallyused\u2014withapproximate inference.Thismeans\nthatapproximateinference,forexample,to\ufb01llinmissinginputs,ortoperform\nclassi\ufb01cationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-\nDBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurate\nclassi\ufb01eronitsown;thebestclassi\ufb01cationresultswiththeoriginalDBMwere\nbasedontrainingaseparateclassi\ufb01ertousefeaturesextractedbytheDBM,\nratherthanbyusinginferenceintheDBMtocomputethedistributionoverthe\nclasslabels.Mean\ufb01eldinferenceintheMP-DBMperformswellasaclassi\ufb01er\nwithoutspecialmodi\ufb01cations.Theotheradvantageofback-propagating through\napproximateinferenceisthatback-propagationcomputestheexactgradientof\ntheloss.Thisisbetterforoptimization thantheapproximate gradientsofSML\ntraining,whichsu\ufb00erfrombothbiasandvariance.ThisprobablyexplainswhyMP-\n6 7 4", "CHAPTER20.DEEPGENERATIVEMODELS\nFigure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmann\nmachine.Eachrowindicatesadi\ufb00erentexamplewithinaminibatchforthesametraining\nstep.\u00a0Eachcolumnrepresentsatimestepwithinthemean\ufb01eldinferenceprocess.\u00a0For\neachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinference\nprocess.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthe\nmean\ufb01eldinferenceprocess,witharrowsindicatingwhichvariablesin\ufb02uencewhichother\nvariablesintheprocess.Inpracticalapplications,weunrollmean\ufb01eldforseveralsteps.\nInthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocess\ncouldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstothe\ninferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessfor\neachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationto\ntraintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.This\ntrainsthemean\ufb01eldprocessfortheMP-DBMtoproduceaccurateestimates.Figure\nadaptedfrom (). Goodfellow e t a l .2013b\n6 7 5", "CHAPTER20.DEEPGENERATIVEMODELS\nDBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining.\nThedisadvantageofback-propagatingthroughtheapproximate inferencegraphis\nthatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristic\napproximationofthegeneralizedpseudolikelihood.\nTheMP-DBMinspiredtheNADE- k(Raiko2014 e t a l .,)extensiontothe\nNADEframework,whichisdescribedinsection.20.10.10\nTheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-\nrametersamongmanydi\ufb00erentcomputational graphs,withthedi\ufb00erencebetween\neachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalso\nsharesparametersacrossmanycomputational graphs.InthecaseoftheMP-DBM,\nthedi\ufb00erencebetweenthegraphsiswhethereachinputunitisobservedornot.\nWhenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropout\ndoes.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.Onecould\nimagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunits\nratherthanmakingthemlatent.\n20.5BoltzmannMachinesforReal-ValuedData\nWhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,\nmanyapplicationssuchasimageandaudiomodelingseemtorequiretheability\ntorepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossible\ntotreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofa\nbinaryvariable.Forexample, ()treatsgrayscaleimagesinthetraining Hinton2000\nsetasde\ufb01ning[0,1]probabilityvalues.Eachpixelde\ufb01nestheprobabilityofa\nbinaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfrom\neachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscale\nimagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach,\nandbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.In\nthissection,wepresentBoltzmannmachinesthatde\ufb01neaprobabilitydensityover\nreal-valueddata.\n20.5.1Gaussian-BernoulliRBMs\nRestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamily\nconditionaldistributions(Welling2005 e t a l .,).Ofthese,themostcommonisthe\nRBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditional\ndistributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisa\nfunctionofthehiddenunits.\n6 7 6", "CHAPTER20.DEEPGENERATIVEMODELS\nTherearemanywaysofparametrizing Gaussian-Bernoulli RBMs.Onechoice\niswhethertouseacovariancematrixoraprecisionmatrixfortheGaussian\ndistribution.Herewepresenttheprecisionformulation.Themodi\ufb01cationtoobtain\nthecovarianceformulationisstraightforward.\u00a0Wewishtohavetheconditional\ndistribution\np , ( ) = (; v h| N v W h \u03b2\u2212 1) . (20.38)\nWecan\ufb01ndthetermsweneedtoaddtotheenergyfunctionbyexpandingthe\nunnormalized logconditionaldistribution:\nlog (;N v W h \u03b2 ,\u2212 1) = \u22121\n2( ) v W h \u2212\ue03e\u03b2 v W h \u03b2 (\u2212 )+( f) .(20.39)\nHere fencapsulatesallthetermsthatareafunctiononlyoftheparameters\nandnottherandomvariablesinthemodel.Wecandiscard fbecauseitsonly\nroleistonormalizethedistribution,andthepartitionfunctionofwhateverenergy\nfunctionwechoosewillcarryoutthatrole.\nIfweincludealloftheterms(withtheirsign\ufb02ipped)involving vfromequa-\ntioninourenergyfunctionanddonotaddanyothertermsinvolving 20.39 v,then\nourenergyfunctionwillrepresentthedesiredconditional . p( ) v h|\nWehavesomefreedomregardingtheotherconditionaldistribution, p( h v|).\nNotethatequationcontainsaterm 20.39\n1\n2h\ue03eW\ue03e\u03b2 W h . (20.40)\nThistermcannotbeincludedinitsentiretybecauseitincludes h i h jterms.These\ncorrespondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,we\nwouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.When\ndesigningourBoltzmannmachine,wesimplyomitthese h i h jcrossterms.Omitting\nthemdoesnotchangetheconditional p( v h|)soequationisstillrespected. 20.39\nHowever,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonly\nasingle h i.Ifweassumeadiagonalprecisionmatrix,we\ufb01ndthatforeachhidden\nunit h iwehaveaterm\n1\n2h i\ue058\nj\u03b2 j W2\nj , i . (20.41)\nIntheabove,weusedthefactthat h2\ni= h ibecause h i\u2208{0 ,1}.Ifweincludethis\nterm(withitssign\ufb02ipped)intheenergyfunction,thenitwillnaturallybias h i\ntobeturnedo\ufb00whentheweightsforthatunitarelargeandconnectedtovisible\nunitswithhighprecision.Thechoiceofwhetherornottoincludethisbiasterm\ndoesnota\ufb00ectthefamilyofdistributionsthemodelcanrepresent(assumingthat\n6 7 7", "CHAPTER20.DEEPGENERATIVEMODELS\nweincludebiasparametersforthehiddenunits)butitdoesa\ufb00ectthelearning\ndynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivations\nremainreasonableevenwhentheweightsrapidlyincreaseinmagnitude.\nOnewaytode\ufb01netheenergyfunctiononaGaussian-Bernoulli RBMisthus\nE ,( v h) =1\n2v\ue03e( )( ) \u03b2 v\ue00c \u2212 v \u03b2\ue00c\ue03eW h b\u2212\ue03eh(20.42)\nbutwemayalsoaddextratermsorparametrizetheenergyintermsofthevariance\nratherthanprecisionifwechoose.\nInthisderivation,wehavenotincludedabiastermonthevisibleunits,butone\ncouldeasilybeadded.One\ufb01nalsourceofvariabilityintheparametrization ofa\nGaussian-Bernoulli RBMisthechoiceofhowtotreattheprecisionmatrix.Itmay\neitherbe\ufb01xedtoaconstant(perhapsestimatedbasedonthemarginalprecision\nofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,orit\nmaybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobe\nnon-diagonal inthiscontext,becausesomeoperationsontheGaussiandistribution\nrequireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.In\nthesectionsahead,wewillseethatotherformsofBoltzmannmachinespermit\nmodelingthecovariancestructure,usingvarioustechniquestoavoidinvertingthe\nprecisionmatrix.\n20.5.2UndirectedModelsofConditionalCovariance\nWhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valued\ndata, ()arguethattheGaussianRBMinductivebiasisnot Ranzato e t a l .2010a\nwellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,\nespeciallynaturalimages.Theproblemisthatmuchoftheinformationcontent\npresentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthan\nintherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsand\nnottheirabsolutevalueswheremostoftheusefulinformationinimagesresides.\nSincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhe\nhiddenunits,itcannotcaptureconditionalcovarianceinformation. Inresponse\ntothesecriticisms,alternativemodelshavebeenproposedthatattempttobetter\naccountforthecovarianceofreal-valueddata.Thesemodelsincludethemeanand\ncovarianceRBM(mcRBM1),themean-productof t-distribution(mPoT)model\nandthespikeandslabRBM(ssRBM).\n1Th e t e rm \u201c m c R B M \u201d i s p ro n o u n c e d b y s a y i n g t h e n a m e o f t h e l e t t e rs M - C- R - B - M ; t h e \u201c m c \u201d\ni s n o t p ro n o u n c e d l i k e t h e \u201c M c \u201d i n \u201c M c D o n a l d \u2019 s . \u201d\n6 7 8", "CHAPTER20.DEEPGENERATIVEMODELS\nMeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen-\ndentlyencodetheconditionalmeanandcovarianceofallobservedunits.The\nmcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovariance\nunits.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM.\nTheotherhalfisacovarianceRBM( ,),alsocalledacRBM, Ranzato e t a l .2010a\nwhosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow.\nSpeci\ufb01cally,withbinarymeanunits h( ) mandbinarycovarianceunits h( ) c,the\nmcRBMmodelisde\ufb01nedasthecombinationoftwoenergyfunctions:\nE m c( x h ,( ) m, h( ) c) = E m( x h ,( ) m)+ E c( x h ,( ) c) ,(20.43)\nwhere E misthestandardGaussian-Bernoulli RBMenergyfunction:2\nE m( x h ,( ) m) =1\n2x\ue03ex\u2212\ue058\njx\ue03eW : , j h( ) m\nj\u2212\ue058\njb( ) m\nj h( ) m\nj ,(20.44)\nand E cisthecRBMenergy\u00a0function that\u00a0models\u00a0the conditionalcovariance\ninformation:\nE c( x h ,( ) c) =1\n2\ue058\njh( ) c\nj\ue010\nx\ue03er( ) j\ue0112\n\u2212\ue058\njb( ) c\nj h( ) c\nj .(20.45)\nTheparameter r( ) jcorrespondstothecovarianceweightvectorassociatedwith\nh( ) c\njand b( ) cisavectorofcovarianceo\ufb00sets.Thecombinedenergyfunctionde\ufb01nes\najointdistribution:\np m c( x h ,( ) m, h( ) c) =1\nZexp\ue06e\n\u2212 E m c( x h ,( ) m, h( ) c)\ue06f\n,(20.46)\nandacorrespondingconditionaldistributionovertheobservationsgiven h( ) mand\nh( ) casamultivariateGaussiandistribution:\np m c( x h|( ) m, h( ) c) = N\uf8eb\n\uf8ed x C;m c\nx h|\uf8eb\n\uf8ed\ue058\njW : , j h( ) m\nj\uf8f6\n\uf8f8 , Cm c\nx h|\uf8f6\n\uf8f8 .(20.47)\nNotethatthecovariancematrix Cm c\nx h|=\ue010\ue050\nj h( ) c\nj r( ) jr( ) j\ue03e+ I\ue011\u2212 1\nisnon-diagonal\nandthat WistheweightmatrixassociatedwiththeGaussianRBMmodelingthe\n2Th i s v e rs i o n o f t h e Ga u s s i a n - B e rn o u l l i R B M e n e rg y f u n c t i o n a s s u m e s t h e i m a g e d a t a h a s\nz e ro m e a n , p e r p i x e l . P i x e l o \ufb00 s e t s c a n e a s i l y b e a d d e d t o t h e m o d e l t o a c c o u n t f o r n o n z e ro p i x e l\nm e a n s .\n6 7 9", "CHAPTER20.DEEPGENERATIVEMODELS\nconditionalmeans.Itisdi\ufb03culttotrainthemcRBMviacontrastivedivergenceor\npersistentcontrastivedivergencebecauseofitsnon-diagonal conditionalcovariance\nstructure.CDandPCDrequiresamplingfromthejointdistributionof x h ,( ) m, h( ) c\nwhich,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals.\nHowever,inthemcRBM,samplingfrom p m c( x h|( ) m, h( ) c)requirescomputing\n( Cm c)\u2212 1ateveryiterationoflearning.Thiscanbeanimpracticalcomputational\nburdenforlargerobservations. ()avoiddirectsampling RanzatoandHinton2010\nfromtheconditional p m c( x h|( ) m, h( ) c)bysamplingdirectlyfromthemarginal\np( x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfree Neal1993\nenergy.\nMean-ProductofStudent\u2019s-distributions t Themean-productofStudent\u2019s\nt-distribution(mPoT)model( ,)extendsthePoTmodel( Ranzato e t a l .2010b Welling\ne t a l .,)inamannersimilartohowthemcRBMextendsthecRBM.This 2003a\nisachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussian\nRBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionoverthe\nobservationisamultivariateGaussian(withnon-diagonal covariance)distribution;\nhowever,unlikethemcRBM,thecomplementaryconditionaldistributionoverthe\nhiddenvariablesisgivenbyconditionallyindependentGammadistributions.The\nGammadistributionG( k , \u03b8) isaprobabilitydistributionoverpositiverealnumbers,\nwithmean k \u03b8.Itisnotnecessarytohaveamoredetailedunderstandingofthe\nGammadistributiontounderstandthebasicideasunderlyingthemPoTmodel.\nThemPoTenergyfunctionis:\nE m P o T( x h ,( ) m, h( ) c) (20.48)\n= E m( x h ,( ) m)+\ue058\nj\ue012\nh( ) c\nj\ue012\n1+1\n2\ue010\nr( ) j\ue03ex\ue0112\ue013\n+(1\u2212 \u03b3 j)log h( ) c\nj\ue013\n(20.49)\nwhere r( ) jisthecovarianceweightvectorassociatedwithunit h( ) c\njand E m( x h ,( ) m)\nisasde\ufb01nedinequation.20.44\nJustaswiththemcRBM,themPoTmodelenergyfunctionspeci\ufb01esamul-\ntivariateGaussian,withaconditionaldistributionover xthathasnon-diagonal\ncovariance.LearninginthemPoTmodel\u2014again,likethemcRBM\u2014iscompli-\ncatedbytheinabilityto\u00a0samplefromthenon-diagonal Gaussianconditional\np m P o T( x h|( ) m, h( ) c),so ()alsoadvocatedirectsamplingof Ranzato e t a l .2010b\np() xviaHamiltonian(hybrid)MonteCarlo.\n6 8 0", "CHAPTER20.DEEPGENERATIVEMODELS\nSpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestricted\nBoltzmannmachines( ,)orssRBMsprovideanothermeans Courville e t a l .2011\nofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,\nssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonian\nMonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM\u2019sbinary\nhiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseof\nauxiliaryreal-valuedvariables.\nThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunits h,\nandreal-valuedslabunits s.Themeanofthevisibleunitsconditionedonthe\nhiddenunitsisgivenby( h s\ue00c) W\ue03e.Inotherwords,eachcolumn W : , ide\ufb01nesa\ncomponentthatcanappearintheinputwhen h i= 1.Thecorrespondingspike\nvariableh idetermineswhetherthatcomponentispresentatall.Thecorresponding\nslabvariables ideterminestheintensityofthatcomponent,ifitispresent.When\naspikevariableisactive,thecorrespondingslabvariableaddsvariancetothe\ninputalongtheaxisde\ufb01nedby W : , i.Thisallowsustomodelthecovarianceofthe\ninputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergence\nwithGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix.\nFormally,thessRBMmodelisde\ufb01nedviaitsenergyfunction:\nE s s( ) = x s h , , \u2212\ue058\nix\ue03eW : , i s i h i+1\n2x\ue03e\ue020\n\u039b+\ue058\ni\u03a6 i h i\ue021\nx (20.50)\n+1\n2\ue058\ni\u03b1 i s2\ni\u2212\ue058\ni\u03b1 i \u00b5 i s i h i\u2212\ue058\nib i h i+\ue058\ni\u03b1 i \u00b52\ni h i ,(20.51)\nwhere b iistheo\ufb00setofthespike h iand\u039bisadiagonalprecisionmatrixonthe\nobservations x.Theparameter \u03b1 i >0isascalarprecisionparameterforthe\nreal-valuedslabvariable s i.Theparameter \u03a6 iisanon-negativediagonalmatrix\nthatde\ufb01nesan h-modulatedquadraticpenaltyon x.Each \u00b5 iisameanparameter\nfortheslabvariable s i.\nWiththejointdistributionde\ufb01nedviatheenergyfunction,itisrelatively\nstraightforwardto\u00a0derivethessRBM\u00a0conditionaldistributions.For\u00a0example,\nbymarginalizing outtheslabvariables s,theconditionaldistributionoverthe\nobservationsgiventhebinaryspikevariablesisgivenby: h\np s s( )= x h|1\nP() h1\nZ\ue05a\nexp ( ) {\u2212 E x s h , ,} d s(20.52)\n=N\ue020\nx C;s s\nx h|\ue058\niW : , i \u00b5 i h i , Cs s\nx h|\ue021\n(20.53)\n6 8 1", "CHAPTER20.DEEPGENERATIVEMODELS\nwhere Cs s\nx h|=\ue000\n\u039b+\ue050\ni\u03a6 i h i\u2212\ue050\ni\u03b1\u2212 1\ni h i W : , i W\ue03e\n: , i\ue001\u2212 1.Thelastequalityholdsonlyif\nthecovariancematrix Cs s\nx h|ispositivede\ufb01nite.\nGatingbythespikevariablesmeansthatthetruemarginaldistributionover\nhs\ue00cissparse.Thisisdi\ufb00erentfromsparsecoding,wheresamplesfromthemodel\n\u201calmostnever\u201d(inthemeasuretheoreticsense)containzerosinthecode,andMAP\ninferenceisrequiredtoimposesparsity.\nComparingthessRBMtothemcRBMandthemPoTmodels,thessRBM\nparametrizes theconditionalcovarianceoftheobservationinasigni\ufb01cantlydi\ufb00erent\nway.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservation\nas\ue010\ue050\nj h( ) c\nj r( ) jr( ) j\ue03e+ I\ue011\u2212 1\n,usingtheactivationofthehiddenunits h j >0to\nenforceconstraintsontheconditionalcovarianceinthedirection r( ) j.Incontrast,\nthessRBMspeci\ufb01estheconditionalcovarianceoftheobservationsusingthehidden\nspikeactivations h i= 1topinchtheprecisionmatrixalongthedirectionspeci\ufb01ed\nbythecorrespondingweightvector.\u00a0ThessRBMconditionalcovarianceisvery\nsimilartothatgivenbyadi\ufb00erentmodel:theproductofprobabilisticprincipal\ncomponentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercomplete\nsetting,sparseactivationswiththessRBMparametrization permitsigni\ufb01cant\nvariance(abovethenominalvariancegivenby\u039b\u2212 1)onlyintheselecteddirections\nofthesparselyactivated h i.\u00a0InthemcRBMormPoTmodels,anovercomplete\nrepresentationwouldmeanthattocapturevariationinaparticulardirectionin\ntheobservationspacerequiresremovingpotentiallyallconstraintswithpositive\nprojectioninthatdirection.\u00a0This wouldsuggestthatthesemodelsarelesswell\nsuitedtotheovercompletesetting.\nTheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachine\nisthatsomesettingsoftheparameterscancorrespondtoacovariancematrix\nthatisnotpositivede\ufb01nite.Suchacovariancematrixplacesmoreunnormalized\nprobabilityonvaluesthatarefartherfromthemean,causingtheintegralover\nallpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimple\nheuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Using\nconstrainedoptimization toexplicitlyavoidtheregionswheretheprobabilityis\nunde\ufb01nedisdi\ufb03culttodowithoutbeingoverlyconservativeandalsopreventing\nthemodelfromaccessinghigh-performingregionsofparameterspace.\nQualitatively,convolutionalvariantsofthessRBMproduceexcellentsamples\nofnaturalimages.Someexamplesareshownin\ufb01gure.16.1\nThessRBMallowsforseveralextensions.Includinghigher-orderinteractions\nandaverage-poolingoftheslabvariables( ,)enablesthemodel Courville e t a l .2014\ntolearnexcellentfeaturesforaclassi\ufb01erwhenlabeleddataisscarce.\u00a0Addinga\n6 8 2", "CHAPTER20.DEEPGENERATIVEMODELS\ntermtotheenergyfunctionthatpreventsthepartitionfunctionfrombecoming\nunde\ufb01nedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellow\ne t a l .,),alsoknownasS3C. 2013d\n20.6ConvolutionalBoltzmannMachines\nAsseeninchapter,extremelyhighdimensionalinputssuchasimagesplace 9\ngreatstrainonthecomputation,memoryandstatisticalrequirementsofmachine\nlearningmodels.Replacingmatrixmultiplication bydiscreteconvolutionwitha\nsmallkernelisthestandardwayofsolvingtheseproblemsforinputsthathave\ntranslationinvariantspatialortemporalstructure. () DesjardinsandBengio2008\nshowedthatthisapproachworkswellwhenappliedtoRBMs.\nDeepconvolutionalnetworksusuallyrequireapoolingoperationsothatthe\nspatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworks\noftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled.\nItisunclearhowtogeneralizethistothesettingofenergy-basedmodels.We\ncouldintroduceabinarypoolingunitpover nbinarydetectorunits dandenforce\np=max i d ibysettingtheenergyfunctiontobe\u221ewheneverthatconstraintis\nviolated.Thisdoesnotscalewellthough,asitrequiresevaluating 2ndi\ufb00erent\nenergycon\ufb01gurations tocomputethenormalization constant.Forasmall 3\u00d73\npoolingregionthisrequires 29= 512energyfunctionevaluationsperpoolingunit!\nLee2009 e t a l .()developedasolutiontothisproblemcalledprobabilistic\nmaxpooling(nottobeconfusedwith\u201cstochasticpooling,\u201dwhichisatechnique\nforimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).The\nstrategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitsso\natmostonemaybeactiveatatime.Thismeansthereareonly n+ 1total\nstates(onestateforeachofthe ndetectorunitsbeingon,andanadditionalstate\ncorrespondingtoallofthedetectorunitsbeingo\ufb00).Thepoolingunitisonif\nandonlyifoneofthedetectorunitsison.Thestatewithallunitso\ufb00isassigned\nenergyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethat\nhas n+1states,orequivalentlyasamodelthathas n+1variablesthatassigns\nenergytoallbutjointassignmentsofvariables. \u221e n+1\nWhilee\ufb03cient,probabilisticmaxpoolingdoesforcethedetectorunitstobe\nmutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontexts\noraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupport\noverlappingpoolingregions.Overlapping poolingregionsareusuallyrequired\ntoobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothis\nconstraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann\n6 8 3", "CHAPTER20.DEEPGENERATIVEMODELS\nmachines.\nLee2009 e t a l .()demonstratedthatprobabilisticmaxpoolingcouldbeused\ntobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperform\noperationssuchas\ufb01llinginmissingportionsofitsinput.Whileintellectually\nappealing,thismodelischallengingtomakeworkinpractice,andusuallydoes\nnotperformaswellasaclassi\ufb01erastraditionalconvolutionalnetworkstrained\nwithsupervisedlearning.\nManyconvolutionalmodelsworkequallywellwithinputsofmanydi\ufb00erent\nspatialsizes.ForBoltzmannmachines,itisdi\ufb03culttochangetheinputsize\nforavarietyofreasons.\u00a0Thepartitionfunctionchangesasthesizeoftheinput\nchanges.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscaling\nupthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscaling\nBoltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneural\nnetworkscanusea\ufb01xednumberofpoolingunitsanddynamicallyincreasethe\nsizeoftheirpoolingregionsinordertoobtaina\ufb01xed-sizerepresentationofa\nvariable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometoo\nexpensiveforthenaiveapproach.\u00a0The approachof()ofmaking Lee e t a l .2009\neachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolves\nthecomputational problems,butstilldoesnotallowvariable-sizepoolingregions.\nForexample,supposewelearnamodelwith 2\u00d72probabilisticmaxpoolingover\ndetectorunitsthatlearnedgedetectors.\u00a0Thisenforcestheconstraintthatonly\noneoftheseedgesmayappearineach2\u00d72region.Ifwethenincreasethesizeof\ntheinputimageby50%ineachdirection,wewouldexpectthenumberofedgesto\nincreasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby\n50%ineachdirectionto3\u00d73,thenthemutualexclusivityconstraintnowspeci\ufb01es\nthateachoftheseedgesmayonlyappearonceina3\u00d73region.Aswegrow\namodel\u2019sinputimageinthisway,themodelgeneratesedgeswithlessdensity.\nOfcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsof\npoolinginordertoemita\ufb01xed-sizeoutputvector.Modelsthatuseprobabilistic\nmaxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputof\nthemodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage.\nPixelsattheboundaryoftheimagealsoposesomedi\ufb03culty,whichisexac-\nerbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.If\nwedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthan\nvisibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled\n3Th e p u b l i c a t i o n d e s c rib e s t h e m o d e l a s a \u201c d e e p b e l i e f n e t w o rk \u201d b u t b e c a u s e i t c a n b e d e s c rib e d\na s a p u re l y u n d i re c t e d m o d e l with t ra c t a b l e l a y e r- wis e m e a n \ufb01 e l d \ufb01 x e d p o i n t u p d a t e s , i t b e s t \ufb01 t s\nt h e d e \ufb01 n i t i o n o f a d e e p B o l t z m a n n m a c h i n e .\n6 8 4", "CHAPTER20.DEEPGENERATIVEMODELS\nwellbecausetheylieinthereceptive\ufb01eldoffewerhiddenunits.However,ifwedo\nimplicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenby\nfewerinputpixels,andmayfailtoactivatewhenneeded.\n20.7BoltzmannMachinesforStructuredorSequential\nOutputs\nInthestructuredoutputscenario,wewishtotrainamodelthatcanmapfrom\nsomeinput xtosomeoutput y,andthedi\ufb00erententriesof yarerelatedtoeach\notherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,\nyisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance.\nAnaturalwaytorepresenttherelationshipsbetweentheentriesin yisto\nuseaprobabilitydistribution p(y| x).Boltzmannmachines,extendedtomodel\nconditionaldistributions,cansupplythisprobabilisticmodel.\nThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeused\nnotjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelatter\ncase,ratherthanmappinganinput xtoanoutput y,themodelmustestimatea\nprobabilitydistributionoverasequenceofvariables, p(x( 1 ), . . . ,x( ) \u03c4).Conditional\nBoltzmannmachinescanrepresentfactorsoftheform p(x( ) t|x( 1 ), . . . ,x( 1 ) t\u2212)in\nordertoaccomplishthistask.\nAnimportantsequencemodelingtaskforthevideogameand\ufb01lmindustry\nismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters.\nThesesequencesareoftencollectedusingmotioncapturesystemstorecordthe\nmovementsofactors.Aprobabilisticmodelofacharacter\u2019smovementallows\nthegenerationofnew,\u00a0previouslyunseen,\u00a0but\u00a0realisticanimations.Tosolve\nthissequencemodelingtask,Taylor2007 e t a l .()introducedaconditionalRBM\nmodeling p( x( ) t| x( 1 ) t\u2212, . . . , x( ) t m\u2212)forsmall m.ThemodelisanRBMover\np( x( ) t)whosebiasparametersarealinearfunctionofthepreceding mvaluesof x.\nWhenweconditionondi\ufb00erentvaluesof x( 1 ) t\u2212andearliervariables,wegetanew\nRBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningon\ndi\ufb00erentpastvalues,wecanchangetheprobabilityofdi\ufb00erenthiddenunitsinthe\nRBMbeingactive.Byactivatinganddeactivatingdi\ufb00erentsubsetsofhiddenunits,\nwecanmakelargechangestotheprobabilitydistributioninducedonx.\u00a0Other\nvariantsofconditionalRBM(,)andothervariantsofsequence Mnih e t a l .2011\nmodelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever ,;\ne t a l .,;2009Boulanger-Lewandowski2012 e t a l .,).\nAnothersequencemodelingtaskistomodelthedistributionoversequences\n6 8 5", "CHAPTER20.DEEPGENERATIVEMODELS\nofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012 e t a l .()\nintroducedtheRNN-RBM sequencemodelandappliedittothistask.The\nRNN-RBMisagenerativemodelofasequenceofframes x( ) tconsistingofanRNN\nthatemitstheRBMparametersforeachtimestep.Unlikepreviousapproaches\ninwhichonlythebiasparametersoftheRBMvariedfromonetimesteptothe\nnext,theRNN-RBMusestheRNNtoemitalloftheparametersoftheRBM,\nincludingtheweights.Totrainthemodel,weneedtobeabletoback-propagate\nthegradientofthelossfunctionthroughtheRNN.Thelossfunctionisnotapplied\ndirectlytotheRNNoutputs.Instead,itisappliedtotheRBM.Thismeansthat\nwemustapproximately di\ufb00erentiatethelosswithrespecttotheRBMparameters\nusingcontrastivedivergenceorarelatedalgorithm.\u00a0This approximate gradient\nmaythenbeback-propagated throughtheRNNusingtheusualback-propagation\nthroughtimealgorithm.\n20.8OtherBoltzmannMachines\nManyothervariantsofBoltzmannmachinesarepossible.\nBoltzmannmachinesmaybeextendedwithdi\ufb00erenttrainingcriteria.Wehave\nfocusedonBoltzmannmachinestrainedtoapproximately maximizethegenerative\ncriterion log p( v).ItisalsopossibletotraindiscriminativeRBMsthataimto\nmaximize log p( y| v)instead( ,).Thisapproachoften LarochelleandBengio2008\nperformsthebestwhenusingalinearcombinationofboththegenerativeand\nthediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerful\nsupervisedlearnersasMLPs,atleastusingexistingmethodology.\nMostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractions\nintheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmany\ntermsandeachindividualtermonlyincludestheproductbetweentworandom\nvariables.Anexampleofsuchatermis v i W i , j h j.Itisalsopossibletotrain\nhigher-orderBoltzmannmachines(,)whoseenergyfunctionterms Sejnowski1987\ninvolvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweena\nhiddenunitandtwodi\ufb00erentimagescanmodelspatialtransformationsfromone\nframeofvideotothenext(MemisevicandHinton20072010,,).Multiplication bya\none-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunits\ndependingonwhichclassispresent( ,).Onerecentexample NairandHinton2009\noftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsof\nhiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisible\nunits vandtheclasslabel y,andanothergroupofhiddenunitsthatinteractonly\nwiththe vinputvalues(,).Thiscanbeinterpretedasencouraging Luo e t a l .2011\n6 8 6", "CHAPTER20.DEEPGENERATIVEMODELS\nsomehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevantto\ntheclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthat\narenecessaryforthesamplesof vtoberealisticbutdonotdeterminetheclass\noftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures.\nSohn2013 e t a l .()introducedaBoltzmannmachinewiththird-orderinteractions\nwithbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemasking\nvariablesaresettozero,theyremovethein\ufb02uenceofavisibleunitonthehidden\nunits.Thisallowsvisibleunitsthatarenotrelevanttotheclassi\ufb01cationproblem\ntoberemovedfromtheinferencepathwaythatestimatestheclass.\nMoregenerally,theBoltzmannmachineframeworkisarichspaceofmodels\npermittingmanymoremodelstructuresthanhavebeenexploredsofar.Developing\nanewformofBoltzmannmachinerequiressomemorecareandcreativitythan\ndevelopinganewneuralnetworklayer,becauseitisoftendi\ufb03cultto\ufb01ndanenergy\nfunctionthatmaintainstractabilityofallofthedi\ufb00erentconditionaldistributions\nneededtousetheBoltzmannmachine,butdespitethisrequirede\ufb00ortthe\ufb01eld\nremainsopentoinnovation.\n20.9Back-PropagationthroughRandomOperations\nTraditionalneuralnetworksimplementadeterministictransformationofsome\ninputvariables x.Whendevelopinggenerativemodels,weoftenwishtoextend\nneuralnetworkstoimplementstochastictransformationsof x.Onestraightforward\nwaytodothisistoaugmenttheneuralnetworkwithextrainputs zthatare\nsampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussian\ndistribution.Theneuralnetworkcanthencontinuetoperformdeterministic\ncomputationinternally,\u00a0butthefunction f( x z ,)willappearstochasticto\u00a0an\nobserverwhodoesnothaveaccessto z.Providedthat fiscontinuousand\ndi\ufb00erentiable,wecanthencomputethegradientsnecessaryfortrainingusing\nback-propagationasusual.\nAsanexample,letusconsidertheoperationconsistingofdrawingsamplesy\nfromaGaussiandistributionwithmeanandvariance \u00b5 \u03c32:\ny\u223cN( \u00b5 , \u03c32) . (20.54)\nBecauseanindividualsampleofyisnotproducedbyafunction,butratherby\nasamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseem\ncounterintuitivetotakethederivativesof ywithrespecttotheparametersof\nitsdistribution, \u00b5and \u03c32.However,\u00a0wecanrewritethesamplingprocessas\n6 8 7", "CHAPTER20.DEEPGENERATIVEMODELS\ntransforminganunderlyingrandomvaluez\u223cN( z;0 ,1)toobtainasamplefrom\nthedesireddistribution:\ny \u00b5 \u03c3 z = + (20.55)\nWearenowabletoback-propagatethroughthesamplingoperation,byregard-\ningitasadeterministicoperationwithanextrainputz.Crucially,theextrainput\nisarandomvariablewhosedistributionisnotafunctionofanyofthevariables\nwhosederivativeswewanttocalculate.\u00a0The resulttellsushowanin\ufb01nitesimal\nchangein \u00b5or \u03c3wouldchangetheoutputifwecouldrepeatthesamplingoperation\nagainwiththesamevalueofz.\nBeingabletoback-propagate throughthissamplingoperationallowsusto\nincorporateitintoalargergraph.Wecanbuildelementsofthegraphontopofthe\noutputofthesamplingdistribution.Forexample,wecancomputethederivatives\nofsomelossfunction J( y).Wecanalsobuildelementsofthegraphwhoseoutputs\naretheinputsortheparametersofthesamplingoperation.Forexample,wecould\nbuildalargergraphwith \u00b5= f( x; \u03b8)and \u03c3= g( x; \u03b8).Inthisaugmentedgraph,\nwecanuseback-propagationthroughthesefunctionstoderive\u2207 \u03b8 J y().\nTheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-\ncable.Wecanexpressanyprobabilitydistributionoftheform p(y; \u03b8)or p(y| x; \u03b8)\nas p(y| \u03c9),where \u03c9isavariablecontainingbothparameters \u03b8,andifapplicable,\ntheinputs x.Givenavalue ysampledfromdistribution p(y| \u03c9),where \u03c9mayin\nturnbeafunctionofothervariables,wecanrewrite\ny y \u223c p(| \u03c9) (20.56)\nas\ny z \u03c9 = ( f;) , (20.57)\nwhere zisasourceofrandomness.Wemaythencomputethederivativesof ywith\nrespectto \u03c9usingtraditionaltoolssuchastheback-propagation algorithmapplied\nto f,solongas fiscontinuousanddi\ufb00erentiable almosteverywhere.Crucially, \u03c9\nmustnotbeafunctionof z,and zmustnotbeafunctionof \u03c9.Thistechnique\nisoftencalledthereparametrizationtrick,stochasticback-propagationor\nperturbationanalysis.\nTherequirementthat fbecontinuousanddi\ufb00erentiableofcourserequires y\ntobecontinuous.Ifwewishtoback-propagate throughasamplingprocessthat\nproducesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradienton\n\u03c9,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCE\nalgorithm(,),discussedinsection. Williams1992 20.9.1\n6 8 8", "CHAPTER20.DEEPGENERATIVEMODELS\nInneuralnetworkapplications,wetypicallychoose ztobedrawnfromsome\nsimpledistribution,suchasaunituniformorunitGaussiandistribution,and\nachievemorecomplexdistributionsbyallowingthedeterministicportionofthe\nnetworktoreshapeitsinput.\nTheideaofpropagatinggradientsoroptimizingthroughstochasticoperations\ndatesbacktothemid-twentiethcentury(,;,)andwas Price1958Bonnet1964\n\ufb01rstusedformachinelearninginthecontextofreinforcementlearning(,Williams\n1992).\u00a0Morerecently,ithasbeenappliedtovariationalapproximations(Opper\nandArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengio\ne t a l .,;,; 2013bKingma2013KingmaandWelling2014baRezende2014 ,,; e t a l .,;\nGoodfellow2014c e t a l .,).Manynetworks,suchasdenoisingautoencodersor\nnetworksregularized\u00a0with dropout,\u00a0are\u00a0also naturally\u00a0designedto\u00a0take\u00a0noise\nasaninputwithoutrequiringanyspecialreparametrization tomakethenoise\nindependentfromthemodel.\n20.9.1Back-PropagatingthroughDiscreteStochasticOperations\nWhenamodelemitsadiscretevariable y,thereparametrization trickisnot\napplicable.Suppose\u00a0thatthemodel\u00a0takesinputs xandparameters \u03b8,\u00a0both\nencapsulatedinthevector \u03c9,andcombinesthemwithrandomnoise ztoproduce\ny:\ny z \u03c9 = ( f;) . (20.58)\nBecause yisdiscrete, fmustbeastepfunction.Thederivativesofastepfunction\narenotusefulatanypoint.Rightateachstepboundary,thederivativesare\nunde\ufb01ned,butthatisasmallproblem.Thelargeproblemisthatthederivatives\narezeroalmosteverywhere,ontheregionsbetweenstepboundaries.Thederivatives\nofanycostfunction J( y)thereforedonotgiveanyinformationforhowtoupdate\nthemodelparameters . \u03b8\nTheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor \u00d7\nO\ufb00setReinforcement\u00d7Characteristic Eligibility)providesaframeworkde\ufb01ninga\nfamilyofsimplebutpowerfulsolutions(,).\u00a0Thecoreideaisthat Williams1992\neventhough J( f( z; \u03c9))isastepfunctionwithuselessderivatives,theexpected\ncost E z z\u223c p ( ) J f((;)) z \u03c9isoftenasmoothfunctionamenabletogradientdescent.\nAlthoughthatexpectationistypicallynottractablewhen yishigh-dimensional\n(oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbe\nestimatedwithoutbiasusingaMonteCarloaverage.Thestochasticestimateof\nthegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimization\ntechniques.\n6 8 9", "CHAPTER20.DEEPGENERATIVEMODELS\nThesimplestversionofREINFORCEcanbederivedbysimplydi\ufb00erentiating\ntheexpectedcost:\nE z[()] = J y\ue058\nyJ p() y() y (20.59)\n\u2202 J E[()] y\n\u2202 \u03c9=\ue058\nyJ() y\u2202 p() y\n\u2202 \u03c9(20.60)\n=\ue058\nyJ p() y() y\u2202 plog() y\n\u2202 \u03c9(20.61)\n\u22481\nmm\ue058\ny( ) i\u223c p , i ( ) y = 1J( y( ) i)\u2202 plog( y( ) i)\n\u2202 \u03c9.(20.62)\nEquationreliesontheassumptionthat 20.60 Jdoesnotreference \u03c9directly.Itis\ntrivialtoextendtheapproachtorelaxthisassumption.Equationexploits20.61\nthederivativeruleforthelogarithm,\u2202 p l o g ( ) y\n\u2202 \u03c9=1\np ( ) y\u2202 p ( ) y\n\u2202 \u03c9.Equation gives20.62\nanunbiasedMonteCarloestimatorofthegradient.\nAnywherewewrite p( y)inthissection,onecouldequallywrite p( y x|).This\nisbecause p( y)isparametrized by \u03c9,and \u03c9containsboth \u03b8and x,if xispresent.\nOneissuewiththeabovesimpleREINFORCEestimatoristhatithasavery\nhighvariance,sothatmanysamplesof yneedtobedrawntoobtainagood\nestimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwill\nconvergeveryslowlyandwillrequireasmallerlearningrate.Itispossibleto\nconsiderablyreducethevarianceofthatestimatorbyusingvariancereduction\nmethods(,;,).Theideaistomodifytheestimatorso Wilson1984L\u2019Ecuyer1994\nthatitsexpectedvalueremainsunchangedbutitsvariancegetreduced.\u00a0Inthe\ncontextofREINFORCE,theproposedvariancereductionmethodsinvolvethe\ncomputationofabaselinethatisusedtoo\ufb00set J( y).Notethatanyo\ufb00set b( \u03c9)\nthatdoesnotdependon ywouldnotchangetheexpectationoftheestimated\ngradientbecause\nE p ( ) y\ue014\u2202 plog() y\n\u2202 \u03c9\ue015\n=\ue058\nyp() y\u2202 plog() y\n\u2202 \u03c9(20.63)\n=\ue058\ny\u2202 p() y\n\u2202 \u03c9(20.64)\n=\u2202\n\u2202 \u03c9\ue058\nyp() = y\u2202\n\u2202 \u03c91 = 0 ,(20.65)\n6 9 0", "CHAPTER20.DEEPGENERATIVEMODELS\nwhichmeansthat\nE p ( ) y\ue014\n(()()) J y\u2212 b \u03c9\u2202 plog() y\n\u2202 \u03c9\ue015\n= E p ( ) y\ue014\nJ() y\u2202 plog() y\n\u2202 \u03c9\ue015\n\u2212 b E() \u03c9 p ( ) y\ue014\u2202 plog() y\n\u2202 \u03c9\ue015\n(20.66)\n= E p ( ) y\ue014\nJ() y\u2202 plog() y\n\u2202 \u03c9\ue015\n. (20.67)\nFurthermore,wecanobtaintheoptimal b( \u03c9) bycomputingthevarianceof( J( y)\u2212\nb( \u03c9))\u2202 p l o g ( ) y\n\u2202 \u03c9under p( y)andminimizingwithrespectto b( \u03c9).Whatwe\ufb01ndis\nthatthisoptimalbaseline b\u2217() \u03c9 iisdi\ufb00erentforeachelement \u03c9 iofthevector: \u03c9\nb\u2217() \u03c9 i=Ep ( ) y\ue068\nJ() y\u2202 p l o g ( ) y\n\u2202 \u03c9 i2\ue069\nE p ( ) y\ue068\n\u2202 p l o g ( ) y\n\u2202 \u03c9 i2\ue069 . (20.68)\nThegradientestimatorwithrespectto \u03c9 ithenbecomes\n(()() J y\u2212 b \u03c9 i)\u2202 plog() y\n\u2202 \u03c9 i(20.69)\nwhere b( \u03c9) iestimatestheabove b\u2217( \u03c9) i.Theestimate bisusuallyobtainedby\naddingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimate\nE p ( ) y[ J( y)\u2202 p l o g ( ) y\n\u2202 \u03c9 i2]and E p ( ) y\ue068\n\u2202 p l o g ( ) y\n\u2202 \u03c9 i2\ue069\nforeachelementof \u03c9.Theseextra\noutputscanbetrainedwiththemeansquarederrorobjective,usingrespectively\nJ( y)\u2202 p l o g ( ) y\n\u2202 \u03c9 i2and\u2202 p l o g ( ) y\n\u2202 \u03c9 i2astargetswhen yissampledfrom p( y),foragiven\n\u03c9.Theestimate bmaythenberecoveredbysubstitutingtheseestimatesinto\nequation. ()preferredtouseasinglesharedoutput 20.68MnihandGregor2014\n(acrossallelements iof \u03c9)trainedwiththetarget J( y),usingasbaseline b( \u03c9)\u2248\nE p ( ) y[()] J y.\nVariancereductionmethodshavebeenintroducedinthereinforcementlearning\ncontext( ,; Sutton e t a l .2000WeaverandTao2001,),generalizingpreviouswork\nonthecaseofbinaryrewardbyDayan1990Bengio2013bMnih ().\u00a0See e t a l .(),\nandGregor2014Ba2014Mnih2014Xu2015 (), e t a l .(), e t a l .(),or e t a l .()for\nexamplesofmodernusesoftheREINFORCEalgorithmwithreducedvariancein\nthecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaseline\nb( \u03c9) ( , ()foundthatthescaleof MnihandGregor2014 J( y)\u2212 b( \u03c9))couldbe\nadjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbya\nmovingaverageduringtraining,asakindofadaptivelearningrate,tocounter\nthee\ufb00ectofimportantvariationsthatoccurduringthecourseoftraininginthe\n6 9 1", "CHAPTER20.DEEPGENERATIVEMODELS\nmagnitudeofthisquantity. ()calledthisheuristic MnihandGregor2014 variance\nnormalization.\nREINFORCE-basedestimatorscanbeunderstoodasestimatingthegradient\nbycorrelatingchoicesof ywithcorrespondingvaluesof J( y).Ifagoodvalueof y\nisunlikelyunderthecurrentparametrization, itmighttakealongtimetoobtainit\nbychance,andgettherequiredsignalthatthiscon\ufb01gurationshouldbereinforced.\n20.10DirectedGenerativeNets\nAsdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass 16\nofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopular\nwithinthegreatermachinelearningcommunity,withinthesmallerdeeplearning\ncommunitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodels\nsuchastheRBM.\nInthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthat\nhavetraditionallybeenassociatedwiththedeeplearningcommunity.\nWehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirected\nmodel.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethought\nofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearners\ninthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsample\ngenerationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirected\nmodels.\n20.10.1SigmoidBeliefNets\nSigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodel Neal1990\nwithaspeci\ufb01ckindofconditionalprobabilitydistribution.Ingeneral,wecan\nthinkofasigmoidbeliefnetworkashavingavectorofbinarystates s,witheach\nelementofthestatein\ufb02uencedbyitsancestors:\np s( i) = \u03c3\uf8eb\n\uf8ed\ue058\nj < iW j , i s j+ b i\uf8f6\n\uf8f8 . (20.70)\nThemostcommonstructureofsigmoidbeliefnetworkisonethatisdivided\nintomanylayers,withancestralsamplingproceedingthroughaseriesofmany\nhiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureis\nverysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningof\n6 9 2", "CHAPTER20.DEEPGENERATIVEMODELS\nthesamplingprocessareindependentfromeachother,ratherthansampledfrom\narestrictedBoltzmannmachine.\u00a0Suchastructureisinterestingforavarietyof\nreasons.Onereasonisthatthestructureisauniversalapproximator ofprobability\ndistributionsoverthevisibleunits,inthesensethatitcanapproximate any\nprobabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,\nevenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthe\nvisiblelayer(SutskeverandHinton2008,).\nWhilegeneratingasampleofthevisibleunitsisverye\ufb03cientinasigmoid\nbeliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiven\nthevisibleunitsisintractable.Mean\ufb01eldinferenceisalsointractablebecausethe\nvariationallowerboundinvolvestakingexpectationsofcliquesthatencompass\nentirelayers.Thisproblemhasremaineddi\ufb03cultenoughtorestrictthepopularity\nofdirecteddiscretenetworks.\nOneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstruct\nadi\ufb00erentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Saul e t a l .\n1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.Another\napproachistouselearnedinferencemechanismsasdescribedinsection.The19.5\nHelmholtzmachine(Dayan1995DayanandHinton1996 e t a l .,; ,)isasigmoidbelief\nnetworkcombinedwithaninferencenetworkthatpredictstheparametersofthe\nmean\ufb01elddistributionoverthehiddenunits.Modernapproaches( ,Gregor e t a l .\n2014MnihandGregor2014 ; ,)tosigmoidbeliefnetworksstillusethisinference\nnetworkapproach.Thesetechniquesremaindi\ufb03cultduetothediscretenatureof\nthelatentvariables.Onecannotsimplyback-propagate throughtheoutputofthe\ninferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback-\npropagatingthroughdiscretesamplingprocesses,describedinsection.Recent 20.9.1\napproachesbasedonimportancesampling,reweightedwake-sleep(Bornscheinand\nBengio2015 Bornschein2015 ,)andbidirectional Helmholtzmachines( e t a l .,)\nmakeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-art\nperformanceonbenchmarktasks.\nAspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatent\nvariables.Learninginthiscaseise\ufb03cient,becausethereisnoneedtomarginalize\nlatentvariablesoutofthelikelihood.\u00a0Afamilyofmodelscalledauto-regressive\nnetworksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariables\nbesidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog-\nlinearrelationships.Auto-regressive networksaredescribedlater,insection.20.10.7\n6 9 3", "CHAPTER20.DEEPGENERATIVEMODELS\n20.10.2Di\ufb00erentiableGeneratorNets\nManygenerativemodelsarebasedontheideaofusingadi\ufb00erentiablegenerator\nnetwork.Themodeltransformssamplesoflatentvariables ztosamples xor\ntodistributionsoversamples xusingadi\ufb00erentiablefunction g( z; \u03b8( ) g)whichis\ntypicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariational\nautoencoders,\u00a0whichpair\u00a0thegeneratornetwithaninferencenet,generative\nadversarial\u00a0networks,\u00a0which\u00a0pair the\u00a0generator\u00a0net work\u00a0witha\u00a0discriminator\nnetwork,andtechniquesthattraingeneratornetworksinisolation.\nGeneratornetworksareessentiallyjustparametrized computational procedures\nforgeneratingsamples,wherethearchitectureprovidesthefamilyofpossible\ndistributionstosamplefromandtheparametersselectadistributionfromwithin\nthatfamily.\nAsanexample,thestandardprocedurefordrawingsamplesfromanormal\ndistributionwithmean \u00b5andcovariance \u03a3istofeedsamples zfromanormal\ndistributionwithzeromeanandidentitycovarianceintoaverysimplegenerator\nnetwork.Thisgeneratornetworkcontainsjustonea\ufb03nelayer:\nx z L z = ( g) = + \u00b5 (20.71)\nwhereisgivenbytheCholeskydecompositionof. L \u03a3\nPseudorandomnumbergeneratorscanalsousenonlineartransformationsof\nsimpledistributions.Forexample,inversetransformsampling(Devroye2013,)\ndrawsascalar zfrom U(0 ,1)andappliesanonlineartransformationtoascalar\nx.Inthiscase g( z)isgivenbytheinverseofthecumulativedistributionfunction\nF( x) =\ue052x\n\u2212\u221ep( v) d v.Ifweareabletospecify p( x),integrateover x,andinvertthe\nresultingfunction,wecansamplefromwithoutusingmachinelearning. p x()\nTogeneratesamplesfrommorecomplicateddistributionsthataredi\ufb03cult\ntospecifydirectly,\u00a0di\ufb03culttointegrateover,\u00a0orwhoseresultingintegralsare\ndi\ufb03culttoinvert,weuseafeedforwardnetworktorepresentaparametricfamily\nofnonlinearfunctions g,andusetrainingdatatoinfertheparametersselecting\nthedesiredfunction.\nWecanthinkof gasprovidinganonlinearchangeofvariablesthattransforms\nthedistributionoverintothedesireddistributionover. z x\nRecallfromequationthat,forinvertible,di\ufb00erentiable,continuous, 3.47 g\np z() = z p x(()) g z\ue00c\ue00c\ue00c\ue00cdet(\u2202 g\n\u2202 z)\ue00c\ue00c\ue00c\ue00c. (20.72)\n6 9 4", "CHAPTER20.DEEPGENERATIVEMODELS\nThisimplicitlyimposesaprobabilitydistributionover:x\np x() = xp z( g\u2212 1()) x\ue00c\ue00c\ue00cdet(\u2202 g\n\u2202 z)\ue00c\ue00c\ue00c. (20.73)\nOfcourse,thisformulamaybedi\ufb03culttoevaluate,dependingonthechoiceof\ng,soweoftenuseindirectmeansoflearning g,ratherthantryingtomaximize\nlog() p xdirectly.\nInsomecases,ratherthanusing gtoprovideasampleof xdirectly,weuse g\ntode\ufb01neaconditionaldistributionover x.Forexample,wecoulduseagenerator\nnetwhose\ufb01nallayerconsistsofsigmoidoutputstoprovidethemeanparameters\nofBernoullidistributions:\np(x i= 1 ) = () | z g z i . (20.74)\nInthiscase,whenweuse gtode\ufb01ne p( x z|),weimposeadistributionover xby\nmarginalizing : z\np() = x E z p . ( ) x z| (20.75)\nBothapproachesde\ufb01neadistribution p g( x)andallowustotrainvarious\ncriteriaof p gusingthereparametrization trickofsection.20.9\nThetwodi\ufb00erentapproachestoformulatinggeneratornets\u2014emittingthe\nparametersofaconditionaldistributionversusdirectlyemittingsamples\u2014have\ncomplementarystrengthsandweaknesses.Whenthegeneratornetde\ufb01nesa\nconditionaldistributionover x,itiscapableofgeneratingdiscretedataaswellas\ncontinuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableof\ngeneratingonlycontinuousdata(wecouldintroducediscretizationintheforward\npropagation, butdoingsowouldmeanthemodelcouldnolongerbetrainedusing\nback-propagation).Theadvantagetodirectsamplingisthatwearenolonger\nforcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownand\nalgebraically manipulated byahumandesigner.\nApproachesbasedondi\ufb00erentiable generatornetworksaremotivatedbythe\nsuccessof\u00a0gradient\u00a0descentappliedtodi\ufb00erentiable feedforwardnetworksfor\nclassi\ufb01cation.\u00a0Inthecontextofsupervisedlearning,deepfeedforwardnetworks\ntrainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgiven\nenoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccess\ntransfertogenerativemodeling?\nGenerativemodelingseemstobemoredi\ufb03cultthanclassi\ufb01cationorregression\nbecausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext\n6 9 5", "CHAPTER20.DEEPGENERATIVEMODELS\nofdi\ufb00erentiablegeneratornets,thecriteriaareintractablebecausethedatadoes\nnotspecifyboththeinputs zandtheoutputs xofthegeneratornet.Inthecase\nofsupervisedlearning,boththeinputs xandtheoutputs yweregiven,andthe\noptimization procedureneedsonlytolearnhowtoproducethespeci\ufb01edmapping.\nInthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehow\ntoarrangespaceinausefulwayandadditionallyhowtomapfromto. z z x\nDosovitskiy2015 e t a l .()studiedasimpli\ufb01edproblem,wherethecorrespondence\nbetween zand xisgiven.Speci\ufb01cally,thetrainingdataiscomputer-rendered\nimageryofchairs.Thelatentvariables zareparametersgiventotherendering\nenginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,\nandothercon\ufb01gurationdetailsthata\ufb00ecttherenderingoftheimage.Usingthis\nsyntheticallygenerateddata,aconvolutionalnetworkisabletolearntomap z\ndescriptionsofthecontentofanimageto xapproximationsofrenderedimages.\nThissuggeststhatcontemporarydi\ufb00erentiablegeneratornetworkshavesu\ufb03cient\nmodelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimization\nalgorithmshavetheabilityto\ufb01tthem.Thedi\ufb03cultyliesindetermininghowto\ntraingeneratornetworkswhenthevalueof zforeach xisnot\ufb01xedandknown\naheadofeachtime.\nThefollowingsectionsdescribeseveralapproachestotrainingdi\ufb00erentiable\ngeneratornetsgivenonlytrainingsamplesof. x\n20.10.3VariationalAutoencoders\nThevariationalautoencoderorVAE(,; ,)isa Kingma2013Rezende e t a l .2014\ndirectedmodelthatuseslearnedapproximate inferenceandcanbetrainedpurely\nwithgradient-basedmethods.\nTogenerateasamplefromthemodel,theVAE\ufb01rstdrawsasample zfrom\nthecodedistribution p m o de l( z).Thesampleisthenrunthroughadi\ufb00erentiable\ngeneratornetwork g( z).Finally, xissampledfromadistribution p m o de l( x; g( z)) =\np m o de l( x z|).\u00a0However,duringtraining,theapproximate inferencenetwork(or\nencoder) q( z x|)isusedtoobtain zand p m o de l( x z|)isthenviewedasadecoder\nnetwork.\nThekeyinsightbehindvariationalautoencodersisthattheymaybetrained\nbymaximizingthevariationallowerboundassociatedwithdatapoint: L() q x\nL() = q E z z x \u223c q (| )log p m o de l()+(( )) z x , H qz| x (20.76)\n= E z z x \u223c q (| )log p m o de l( ) x z|\u2212 D K L(( ) qz| x|| p m o de l())z(20.77)\n\u2264log p m o de l() x . (20.78)\n6 9 6", "CHAPTER20.DEEPGENERATIVEMODELS\nInequation,werecognizethe\ufb01rsttermasthejointlog-likelihoodofthevisible 20.76\nandhiddenvariablesundertheapproximateposterioroverthelatentvariables\n(justlikewithEM,exceptthatweuseanapproximateratherthantheexact\nposterior).Werecognizealsoasecondterm,theentropyoftheapproximate\nposterior.\u00a0When qischosentobeaGaussiandistribution,withnoiseaddedto\napredictedmeanvalue,maximizingthisentropytermencouragesincreasingthe\nstandarddeviationofthisnoise.Moregenerally,thisentropytermencouragesthe\nvariationalposteriortoplacehighprobabilitymassonmany zvaluesthatcould\nhavegenerated x,ratherthancollapsingtoasinglepointestimateofthemost\nlikelyvalue.Inequation,werecognizethe\ufb01rsttermasthereconstruction 20.77\nlog-likelihoodfoundinotherautoencoders.Thesecondtermtriestomakethe\napproximateposteriordistribution q(z| x) andthemodelprior p m o de l( z) approach\neachother.\nTraditionalapproachestovariationalinferenceandlearninginfer qviaanopti-\nmizationalgorithm,typicallyiterated\ufb01xedpointequations(section).These19.4\napproachesareslowandoftenrequiretheabilitytocompute E z\u223c qlog p m o de l( z x ,)\ninclosedform.Themainideabehindthevariationalautoencoderistotraina\nparametricencoder(alsosometimescalledaninferencenetworkorrecognition\nmodel)thatproducestheparametersof q.Solongas zisacontinuousvariable,we\ncanthenback-propagate throughsamplesof zdrawnfrom q( z x|) = q( z; f( x; \u03b8))\ninordertoobtainagradientwithrespectto \u03b8.Learningthenconsistssolelyof\nmaximizing Lwithrespecttotheparametersoftheencoderanddecoder.Allof\ntheexpectationsinmaybeapproximatedbyMonteCarlosampling. L\nThevariationalautoencoderapproachiselegant,theoreticallypleasing,and\nsimpletoimplement.Italsoobtainsexcellentresultsandisamongthestateofthe\nartapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfrom\nvariationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecauses\nofthisphenomenon arenotyetknown.Onepossibilityisthattheblurrinessis\nanintrinsice\ufb00ectofmaximumlikelihood,whichminimizes D K L( p da t a\ue06b p m o de l).As\nillustratedin\ufb01gure,thismeansthatthemodelwillassignhighprobabilityto 3.6\npointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytoother\npoints.Theseotherpointsmayincludeblurryimages.Partofthereasonthatthe\nmodelwouldchoosetoputprobabilitymassonblurryimagesratherthansome\notherpartofthespaceisthatthevariationalautoencodersusedinpracticeusually\nhaveaGaussiandistributionfor p m o de l( x; g( z)).\u00a0Maximizing alowerboundon\nthelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoder\nwithmeansquarederror,inthesensethatithasatendencytoignorefeatures\noftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthe\nbrightnessofthepixelsthattheyoccupy.Thisissueisnotspeci\ufb01ctoVAEsand\n6 9 7", "CHAPTER20.DEEPGENERATIVEMODELS\nissharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,\nD K L( p da t a\ue06b p m o de l),asarguedby ()andby().Another Theis e t a l .2015 Huszar2015\ntroublingissuewithcontemporary VAEmodelsisthattheytendtouseonlyasmall\nsubsetofthedimensionsof z,asiftheencoderwasnotabletotransformenough\nofthelocaldirectionsininputspacetoaspacewherethemarginaldistribution\nmatchesthefactorizedprior.\nTheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodel\narchitectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequire\nextremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwith\nadiversefamilyofdi\ufb00erentiable operators.OneparticularlysophisticatedVAE\nisthedeeprecurrentattentionwriterorDRAWmodel( ,). Gregor e t a l .2015\nDRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattention\nmechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentially\nvisitingdi\ufb00erentsmallimagepatchesanddrawingthevaluesofthepixelsatthose\npoints.VAEscanalsobeextendedtogeneratesequencesbyde\ufb01ningvariational\nRNNs( ,)byusingarecurrentencoderanddecoderwithin Chung e t a l .2015b\ntheVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonly\nnon-determinis ticoperationsattheoutputspace.VariationalRNNsalsohave\nrandomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAE\nlatentvariables.\nTheVAEframeworkhasbeenextendedtomaximizenotjustthetraditional\nvariationallowerbound,butinsteadtheimportanceweightedautoencoder\n(,)objective: Burda e t a l .2015\nL k() = x , q Ez( 1 ) , . . . , z( ) k\u223c | q ( z x )\ue022\nlog1\nkk\ue058\ni = 1p m o de l( x z ,( ) i)\nq( z( ) i| x)\ue023\n.(20.79)\nThisnewobjectiveisequivalenttothetraditionallowerbound Lwhen k=1.\nHowever,itmayalsobeinterpretedasforminganestimateofthetruelog p m o de l( x)\nusingimportancesamplingof zfromproposaldistribution q( z x|).Theimportance\nweightedautoencoderobjectiveisalsoalowerboundonlog p m o de l( x) andbecomes\ntighterasincreases. k\nVariationalautoencodershavesomeinterestingconnectionstotheMP-DBM\nandotherapproachesthatinvolveback-propagationthroughtheapproximate\ninferencegraph(Goodfellow2013bStoyanov2011Brakel2013 e t a l .,; e t a l .,; e t a l .,).\nThesepreviousapproachesrequiredaninferenceproceduresuchasmean\ufb01eld\ufb01xed\npointequationstoprovidethecomputational graph.Thevariationalautoencoder\nisde\ufb01nedforarbitrarycomputational graphs,whichmakesitapplicabletoawider\nrangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice\n6 9 8", "CHAPTER20.DEEPGENERATIVEMODELS\nofmodelstothosewithtractablemean\ufb01eld\ufb01xedpointequations.Thevariational\nautoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihood\nofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremore\nheuristicandhavelittleprobabilisticinterpretation beyondmakingtheresultsof\napproximateinferenceaccurate.Onedisadvantageofthevariationalautoencoder\nisthatitlearnsaninferencenetworkforonlyoneproblem,inferring zgiven x.\nTheoldermethodsareabletoperformapproximateinferenceoveranysubsetof\nvariablesgivenanyothersubsetofvariables,becausethemean\ufb01eld\ufb01xedpoint\nequationsspecifyhowtoshareparametersbetweenthecomputational graphsfor\nallofthesedi\ufb00erentproblems.\nOneverynicepropertyofthevariationalautoencoderisthatsimultaneously\ntrainingaparametricencoderincombinationwiththegeneratornetworkforcesthe\nmodeltolearnapredictablecoordinatesystemthattheencodercancapture.This\nmakesitanexcellentmanifoldlearningalgorithm.See\ufb01gureforexamplesof 20.6\nlow-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthe\ncasesdemonstratedinthe\ufb01gure,thealgorithmdiscoveredtwoindependentfactors\nofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression.\n20.10.4GenerativeAdversarialNetworks\nGenerativeadversarialnetworksorGANs( ,)areanother Goodfellow e t a l .2014c\ngenerativemodelingapproachbasedondi\ufb00erentiablegeneratornetworks.\nGenerativeadversarialnetworksarebasedonagametheoreticscenarioin\nwhichthegeneratornetworkmustcompeteagainstanadversary.Thegenerator\nnetworkdirectlyproducessamples x= g( z; \u03b8( ) g).Itsadversary,thediscriminator\nnetwork,attemptstodistinguishbetweensamplesdrawnfromthetrainingdata\nandsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvalue\ngivenby d( x; \u03b8( ) d),indicatingtheprobabilitythat xisarealtrainingexample\nratherthanafakesampledrawnfromthemodel.\nThesimplestwaytoformulatelearningingenerativeadversarialnetworksis\nasazero-sumgame,inwhichafunction v( \u03b8( ) g, \u03b8( ) d)determinesthepayo\ufb00ofthe\ndiscriminator.Thegeneratorreceives\u2212 v( \u03b8( ) g, \u03b8( ) d)asitsownpayo\ufb00.During\nlearning,eachplayerattemptstomaximizeitsownpayo\ufb00,sothatatconvergence\ng\u2217= argmin\ngmax\ndv g , d . () (20.80)\nThedefaultchoiceforis v\nv( \u03b8( ) g, \u03b8( ) d) = E x\u223c pdatalog()+ d x E x\u223c pmodellog(1 ()) \u2212 d x .(20.81)\n6 9 9", "CHAPTER20.DEEPGENERATIVEMODELS\nFigure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani-\nfolds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensions\nmaybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingof\nhowthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievethe\nintrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenot\nexamplesfromthetrainingsetbutimages xactuallygeneratedbythemodel p( x z|),\nsimplybychangingthe2-D\u201ccode\u201d z(eachimagecorrespondstoadi\ufb00erentchoiceof\u201ccode\u201d\nzona2-Duniformgrid). ( L e f t )Thetwo-dimensionalmapoftheFreyfacesmanifold.\nOnedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationof\ntheface,whiletheother(vertical)correspondstotheemotionalexpression.The ( R i g h t )\ntwo-dimensionalmapoftheMNISTmanifold.\nThisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasreal\norfake.Simultaneous ly,thegeneratorattemptstofooltheclassi\ufb01erintobelieving\nitssamplesarereal.Atconvergence,thegenerator\u2019ssamplesareindistinguishable\nfromrealdata,andthediscriminatoroutputs1\n2everywhere.Thediscriminator\nmaythenbediscarded.\nThemainmotivationforthedesignofGANsisthatthelearningprocess\nrequiresneitherapproximateinferencenorapproximation ofapartitionfunction\ngradient.Inthecasewhere max d v( g , d)isconvexin \u03b8( ) g(suchasthecasewhere\noptimization isperformeddirectlyinthespaceofprobabilitydensityfunctions)\ntheprocedureisguaranteedtoconvergeandisasymptoticallyconsistent.\nUnfortunately,learninginGANscanbedi\ufb03cultinpracticewhen gand d\narerepresentedbyneuralnetworksandmax d v( g , d)isnotconvex.Goodfellow\n7 0 0", "CHAPTER20.DEEPGENERATIVEMODELS\n()identi\ufb01ednon-convergenceasanissuethatmaycauseGANstounder\ufb01t. 2014\nIngeneral,simultaneousgradientdescentontwoplayers\u2019costsisnotguaranteed\ntoreachanequilibrium.Considerforexamplethevaluefunction v( a , b)= a b,\nwhereoneplayercontrols aandincurscost a b,whiletheotherplayercontrols b\nandreceivesacost\u2212 a b.Ifwemodeleachplayerasmakingin\ufb01nitesimallysmall\ngradientsteps,eachplayerreducingtheirowncostattheexpenseoftheother\nplayer,then aand bgointoastable,circularorbit,ratherthanarrivingatthe\nequilibriumpointattheorigin.Notethattheequilibriaforaminimaxgameare\nnotlocalminimaof v.Instead,theyarepointsthataresimultaneouslyminima\nforbothplayers\u2019costs.Thismeansthattheyaresaddlepointsof vthatarelocal\nminimawithrespecttothe\ufb01rstplayer\u2019sparametersandlocalmaximawithrespect\ntothesecondplayer\u2019sparameters.Itispossibleforthetwoplayerstotaketurns\nincreasingthendecreasing vforever,ratherthanlandingexactlyonthesaddle\npointwhereneitherplayeriscapableofreducingitscost.Itisnotknowntowhat\nextentthisnon-convergenceproblema\ufb00ectsGANs.\nGoodfellow2014()identi\ufb01edanalternativeformulationofthepayo\ufb00s,inwhich\nthegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximum\nlikelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximum\nlikelihoodtrainingconverges,thisreformulationoftheGANgameshouldalso\nconverge,givenenoughsamples.Unfortunately,thisalternativeformulationdoes\nnotseemtoimproveconvergenceinpractice,possiblyduetosuboptimalityofthe\ndiscriminator,orpossiblyduetohighvariancearoundtheexpectedgradient.\nInrealisticexperiments,thebest-performingformulationoftheGANgame\nisadi\ufb00erentformulationthatisneitherzero-sumnorequivalenttomaximum\nlikelihood,introducedby ()withaheuristicmotivation.In Goodfellow e t a l .2014c\nthisbest-performingformulation,thegeneratoraimstoincreasethelogprobability\nthatthediscriminatormakesamistake,ratherthanaimingtodecreasethelog\nprobabilitythatthediscriminatormakesthecorrectprediction.Thisreformulation\nismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator\u2019s\ncostfunctionwithrespecttothediscriminator\u2019slogitstoremainlargeeveninthe\nsituationwherethediscriminatorcon\ufb01dentlyrejectsallgeneratorsamples.\nStabilization ofGANlearningremainsanopenproblem.\u00a0Fortunately,GAN\nlearningperformswellwhenthemodelarchitectureandhyperparametersarecare-\nfullyselected. ()craftedadeepconvolutionalGAN(DCGAN) Radford e t a l .2015\nthatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre-\nsentationspacecapturesimportantfactorsofvariation,asshownin\ufb01gure.15.9\nSee\ufb01gureforexamplesofimagesgeneratedbyaDCGANgenerator. 20.7\nTheGANlearningproblemcanalsobesimpli\ufb01edbybreakingthegeneration\n7 0 1", "CHAPTER20.DEEPGENERATIVEMODELS\nFigure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset. ( L e f t )Images\nofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadford\ne t a l .().ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith 2015 ( R i g h t )\npermissionfrom (). Denton e t a l .2015\nprocessintomanylevelsofdetail.ItispossibletotrainconditionalGANs(Mirza\nandOsindero2014,)thatlearntosamplefromadistribution p( x y|)rather\nthansimplysamplingfromamarginaldistribution p( x). () Denton e t a l .2015\nshowedthataseriesofconditionalGANscanbetrainedto\ufb01rstgenerateavery\nlow-resolutionversionofanimage,thenincrementally adddetailstotheimage.\nThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramid\ntogeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgenerators\nareabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,with\nexperimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkas\nbeingrealdata.See\ufb01gureforexamplesofimagesgeneratedbyaLAPGAN 20.7\ngenerator.\nOneunusualcapabilityoftheGANtrainingprocedureisthatitcan\ufb01tproba-\nbilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthan\nmaximizingthelogprobabilityofspeci\ufb01cpoints,thegeneratornetlearnstotrace\noutamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-\ndoxically,thismeansthatthemodelmayassignalog-likelihoodofnegativein\ufb01nity\ntothetestset,whilestillrepresentingamanifoldthatahumanobserverjudges\ntocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageor\nadisadvantage,andonemayalsoguaranteethatthegeneratornetworkassigns\nnon-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegenerator\nnetworkaddGaussiannoisetoallofthegeneratedvalues.\u00a0Generatornetworks\nthataddGaussiannoiseinthismannersamplefromthesamedistributionthatone\nobtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional\n7 0 2", "CHAPTER20.DEEPGENERATIVEMODELS\nGaussiandistribution.\nDropoutseemstobeimportantinthediscriminatornetwork.\u00a0Inparticular,\nunits\u00a0shouldbe\u00a0stochasticallydropped\u00a0whilecomputingthe\u00a0gradientfor\u00a0the\ngeneratornetworktofollow.Followingthegradientofthedeterministicversionof\nthediscriminatorwithitsweightsdividedbytwodoesnotseemtobease\ufb00ective.\nLikewise,neverusingdropoutseemstoyieldpoorresults.\nWhiletheGANframeworkisdesignedfordi\ufb00erentiablegeneratornetworks,\nsimilarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-\nsupervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogistic\nregressiondiscriminator(Welling2002 e t a l .,).\n20.10.5GenerativeMomentMatchingNetworks\nGenerativemomentmatchingnetworks(,; , Li e t a l .2015Dziugaite e t a l .\n2015)areanotherformofgenerativemodelbasedondi\ufb00erentiablegenerator\nnetworks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetwork\nwithanyothernetwork\u2014neither aninferencenetworkasusedwithVAEsnora\ndiscriminatornetworkasusedwithGANs.\nThesenetworksaretrainedwithatechniquecalledmomentmatching.The\nbasicideabehindmomentmatchingistotrainthegeneratorinsuchawaythat\nmanyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossible\ntothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,a\nmomentisanexpectationofdi\ufb00erentpowersofarandomvariable.Forexample,\nthe\ufb01rstmomentisthemean,thesecondmomentisthemeanofthesquared\nvalues,andsoon.Inmultipledimensions,eachelementoftherandomvectormay\nberaisedtodi\ufb00erentpowers,sothatamomentmaybeanyquantityoftheform\nE x\u03a0 i xn i\ni (20.82)\nwhere n= [ n 1 , n 2 , . . . , n d]\ue03eisavectorofnon-negativeintegers.\nUpon\ufb01rstexamination,thisapproachseemstobecomputationally infeasible.\nForexample,ifwewanttomatchallthemomentsoftheform x i x j,thenweneed\ntominimizethedi\ufb00erencebetweenanumberofvaluesthatisquadraticinthe\ndimensionof x.Moreover,evenmatchingallofthe\ufb01rstandsecondmoments\nwouldonlybesu\ufb03cientto\ufb01tamultivariateGaussiandistribution,whichcaptures\nonlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksareto\ncapturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments.\nGANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga\n7 0 3", "CHAPTER20.DEEPGENERATIVEMODELS\ndynamicallyupdateddiscriminatorthatautomatically focusesitsattentionon\nwhicheverstatisticthegeneratornetworkismatchingtheleaste\ufb00ectively.\nInstead,generativemomentmatchingnetworkscanbetrainedbyminimizing\nacostfunctioncalledmaximummeandiscrepancy(Sch\u00f6lkopfandSmola,\n2002Gretton2012 ; e t a l .,)orMMD.Thiscostfunctionmeasurestheerrorin\nthe\ufb01rstmomentsinanin\ufb01nite-dimens ionalspace,usinganimplicitmapping\ntofeaturespacede\ufb01nedbyakernelfunctioninordertomakecomputations on\nin\ufb01nite-dimens ionalvectorstractable.TheMMDcostiszeroifandonlyifthetwo\ndistributionsbeingcomparedareequal.\nVisually,thesamplesfromgenerativemomentmatchingnetworksaresomewhat\ndisappointing.Fortunately,theycanbeimprovedbycombiningthegenerator\nnetworkwithanautoencoder.First,anautoencoderistrainedtoreconstructthe\ntrainingset.Next,theencoderoftheautoencoderisusedtotransformtheentire\ntrainingsetintocodespace.Thegeneratornetworkisthentrainedtogenerate\ncodesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder.\nUnlikeGANs,thecostfunctionisde\ufb01nedonlywithrespecttoabatchof\nexamplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossible\ntomakeatrainingupdateasafunctionofonlyonetrainingexampleoronly\nonesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbe\ncomputedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoo\nsmall,MMDcanunderestimatethetrueamountofvariationinthedistributions\nbeingsampled.No\ufb01nitebatchsizeissu\ufb03cientlylargetoeliminatethisproblem\nentirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatch\nsizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemany\nexamplesmustbeprocessedinordertocomputeasinglesmallgradientstep.\nAswithGANs,itispossibletotrainageneratornetusingMMDevenifthat\ngeneratornetassignszeroprobabilitytothetrainingpoints.\n20.10.6ConvolutionalGenerativeNetworks\nWhengeneratingimages,itisoftenusefultouseageneratornetworkthatincludes\naconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiy e t a l .()or\ne t a l .()).Todoso,\u00a0weusethe\u201ctranspose\u201doftheconvolutionoperator, 2015\ndescribedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes 9.5\nsousingfewerparametersthanusingfullyconnectedlayerswithoutparameter\nsharing.\nConvolutionalnetworksforrecognitiontaskshaveinformation\ufb02owfromthe\nimagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel.\n7 0 4", "CHAPTER20.DEEPGENERATIVEMODELS\nAsthisimage\ufb02owsupwardthroughthenetwork,informationisdiscardedasthe\nrepresentationoftheimagebecomesmoreinvarianttonuisancetransformations.\nInageneratornetwork,\u00a0theoppositeistrue.Richdetailsmustbeaddedas\ntherepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,\nculminatinginthe\ufb01nalrepresentationoftheimage,whichisofcoursetheimage\nitself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesand\nlighting.\u00a0Theprimarymechanismfordiscardinginformationinaconvolutional\nrecognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedto\naddinformation. Wecannotputtheinverseofapoolinglayerintothegenerator\nnetworkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationis\ntomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseems\ntoperformacceptablyistousean\u201cun-pooling\u201dasintroducedbyDosovitskiy e t a l .\n().Thislayercorrespondstotheinverseofthemax-poolingoperationunder 2015\ncertainsimplifyingconditions.\u00a0Firs t,thestrideofthemax-poolingoperationis\nconstrainedtobeequaltothewidthofthepoolingregion.Second,themaximum\ninputwithineachpoolingregionisassumedtobetheinputintheupper-left\ncorner.Finally,allnon-maximal inputswithineachpoolingregionareassumedto\nbezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthe\nmax-poolingoperatortobeinverted.Theinverseun-poolingoperationallocates\natensorofzeros,thencopieseachvaluefromspatialcoordinate ioftheinput\ntospatialcoordinate i k\u00d7oftheoutput.Theintegervalue kde\ufb01nesthesize\nofthepoolingregion.Eventhoughtheassumptionsmotivatingthede\ufb01nitionof\ntheun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearnto\ncompensateforitsunusualoutput,sothesamplesgeneratedbythemodelasa\nwholearevisuallypleasing.\n20.10.7Auto-RegressiveNetworks\nAuto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandom\nvariables.Theconditionalprobabilitydistributionsinthesemodelsarerepresented\nbyneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogistic\nregression).Thegraphstructureofthesemodelsisthecompletegraph.They\ndecomposeajointprobabilityovertheobservedvariablesusingthechainruleof\nprobabilitytoobtainaproductofconditionalsoftheform P( x d| x d\u2212 1 , . . . , x 1).\nSuchmodelshavebeencalledfully-visibleBayesnetworks(FVBNs)andused\nsuccessfully\u00a0inmany\u00a0forms,\u00a0\ufb01rstwith\u00a0logistic regression\u00a0foreachconditional\ndistribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(Bengio\nandBengio2000bLarochelleandMurray2011 ,; ,).Insomeformsofauto-\nregressivenetworks,suchasNADE( ,),described LarochelleandMurray2011\n7 0 5", "CHAPTER20.DEEPGENERATIVEMODELS\ninsection below,wecanintroduceaformofparametersharingthat 20.10.10\nbringsbothastatisticaladvantage(feweruniqueparameters)andacomputational\nadvantage(lesscomputation). Thisisonemoreinstanceoftherecurringdeep\nlearningmotifof r e u s e o f f e a t u r e s.\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4\nFigure20.8:A\u00a0fullyvisiblebelief\u00a0networkpredictsthe i-thvariable\u00a0fromthe i\u22121\npreviousones. ( T o p ) ( Bottom ) ThedirectedgraphicalmodelforanFVBN. Corresponding\ncomputationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadeby\nalinearpredictor.\n20.10.8LinearAuto-RegressiveNetworks\nThesimplestformofauto-regressiv enetworkhasnohiddenunitsandnosharing\nofparametersorfeatures.Each P( x i| x i\u2212 1 , . . . , x 1)isparametrized asalinear\nmodel(linearregressionforreal-valueddata,logisticregressionforbinarydata,\nsoftmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()\nandhas O( d2)parameterswhenthereare dvariablestomodel.Itisillustratedin\n\ufb01gure.20.8\nIfthevariablesarecontinuous,alinearauto-regressive modelismerelyanother\nwaytoformulateamultivariateGaussiandistribution,capturinglinearpairwise\ninteractionsbetweentheobservedvariables.\nLinearauto-regressiv enetworksareessentiallythegeneralization oflinear\nclassi\ufb01cationmethodstogenerativemodeling.Theythereforehavethesame\n7 0 6", "CHAPTER20.DEEPGENERATIVEMODELS\nadvantagesanddisadvantagesaslinearclassi\ufb01ers.Likelinearclassi\ufb01ers,theymay\nbetrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions\n(asintheGaussiancase).Likelinearclassi\ufb01ers,themodelitselfdoesnoto\ufb00er\nawayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslike\nbasisexpansionsoftheinputorthekerneltrick.\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )\nFigure20.9:Aneuralauto-regressivenetworkpredictsthe i-thvariable x ifromthe i\u22121\npreviousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenoted h i)\nthatarefunctionsof x 1 , . . . , x icanbereusedinpredictingallofthesubsequentvariables\nx i + 1 , x i + 2 , . . . , x d.\n20.10.9NeuralAuto-RegressiveNetworks\nNeuralauto-regressiv enetworks( ,,)havethesame BengioandBengio2000ab\nleft-to-rightgraphicalmodelaslogisticauto-regressiv enetworks(\ufb01gure)but20.8\nemployadi\ufb00erentparametrization oftheconditionaldistributionswithinthat\ngraphicalmodelstructure.Thenewparametrization ismorepowerfulinthesense\nthatitscapacitycanbeincreasedasmuchasneeded,allowingapproximation of\nanyjointdistribution.Thenewparametrization canalsoimprovegeneralization\nbyintroducingaparametersharingandfeaturesharingprinciplecommontodeep\nlearningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthe\ncurseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharing\nthesamestructureas\ufb01gure.Intabulardiscreteprobabilisticmodels,each 20.8\nconditionaldistributionisrepresentedbyatableofprobabilities, withoneentry\nandoneparameterforeachpossiblecon\ufb01gurationofthevariablesinvolved.By\nusinganeuralnetworkinstead,twoadvantagesareobtained:\n7 0 7", "CHAPTER20.DEEPGENERATIVEMODELS\n1.Theparametrization ofeach P( x i| x i\u2212 1 , . . . , x 1)byaneuralnetworkwith\n( i\u22121)\u00d7 kinputsand koutputs(ifthevariablesarediscreteandtake k\nvalues,encodedone-hot)allowsonetoestimatetheconditionalprobability\nwithoutrequiringanexponentialnumberofparameters(andexamples),yet\nstillisabletocapturehigh-orderdependenciesbetweentherandomvariables.\n2.Insteadofhavingadi\ufb00erentneuralnetworkforthepredictionofeach x i,\na connectivityillustratedin\ufb01gureallowsonetomergeall l e f t - t o - r i g h t 20.9\ntheneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayer\nfeaturescomputedforpredicting x icanbereusedforpredicting x i k +( k >0).\nThehiddenunitsarethusorganizedin g r o u p sthathavetheparticularity\nthatalltheunitsinthe i-thgrouponlydependontheinputvalues x 1 , . . . , x i.\nTheparametersusedtocomputethesehiddenunitsarejointlyoptimized\nto\u00a0improvethe\u00a0prediction ofall\u00a0thevariables\u00a0inthe\u00a0sequence.This\u00a0is\naninstanceofthe r e u s e p r i nc i p l ethatrecursthroughoutdeeplearningin\nscenariosrangingfromrecurrentandconvolutionalnetworkarchitectures to\nmulti-taskandtransferlearning.\nEach P( x i| x i\u2212 1 , . . . , x 1)canrepresentaconditionaldistributionbyhaving\noutputsoftheneuralnetworkpredict p a r a m e t e r softheconditionaldistribution\nof x i,asdiscussedinsection.Althoughtheoriginalneuralauto-regressive 6.2.1.1\nnetworkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariate\ndata(withasigmoidoutputforaBernoullivariableorsoftmaxoutputfora\nmultinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesor\njointdistributionsinvolvingbothdiscreteandcontinuousvariables.\n20.10.10NADE\nTheneuralautoregressivedensityestimator(NADE)isaverysuccessful\nrecentformofneuralauto-regressive network(LarochelleandMurray2011,).The\nconnectivityisthesameasfortheoriginalneuralauto-regressive networkofBengio\nandBengio2000b()butNADEintroducesanadditionalparametersharingscheme,\nasillustratedin\ufb01gure.Theparametersofthehiddenunitsofdi\ufb00erentgroups 20.10\njareshared.\nTheweights W\ue030\nj , k, ifromthe i-thinput x itothe k-thelementofthe j-thgroup\nofhiddenunit h( ) j\nk()aresharedamongthegroups: j i\u2265\nW\ue030\nj , k, i= W k, i . (20.83)\nTheremainingweights,where,arezero. j < i\n7 0 8", "CHAPTER20.DEEPGENERATIVEMODELS\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )\nW : 1 , W : 1 , W : 1 ,\nW : 2 , W : 2 , W : 3 ,\nFigure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).The\nhiddenunitsareorganizedingroups h( ) jsothatonlytheinputs x 1 , . . . , x iparticipate\nincomputing h( ) iandpredicting P( x j| x j\u2212 1 , . . . , x 1),for j > i.NADEisdi\ufb00erentiated\nfromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharing\npattern: W\ue030\nj , k , i= W k , iisshared(indicatedinthe\ufb01gurebytheuseofthesamelinepattern\nforeveryinstanceofareplicatedweight)foralltheweightsgoingoutfrom x itothe k-th\nunitofanygroup.Recallthatthevector j i\u2265 ( W 1 , i , W 2 , i , . . . , W n , i)isdenoted W : , i.\nLarochelleandMurray2011()chosethissharingschemesothatforward\npropagationinaNADEmodellooselyresemblesthecomputations performedin\nmean\ufb01eldinferenceto\ufb01llinmissinginputsinanRBM.Thismean\ufb01eldinference\ncorrespondstorunningarecurrentnetworkwithsharedweightsandthe\ufb01rststep\nofthatinferenceisthesameasinNADE.Theonlydi\ufb00erenceisthatwithNADE,\ntheoutputweightsconnectingthehiddenunitstotheoutputareparametrized\nindependentlyfromtheweightsconnectingtheinputunitstothehiddenunits.In\ntheRBM,thehidden-to-output weightsarethetransposeoftheinput-to-hidden\nweights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestep\nofthemean\ufb01eldrecurrentinferencebuttomimic ksteps.Thisapproachiscalled\nNADE-(,). kRaiko e t a l .2014\nAsmentionedpreviously,auto-regressiv enetworksmaybeextendtoprocess\ncontinuous-valueddata.Aparticularlypowerfulandgenericwayofparametrizing\nacontinuousdensityisasaGaussianmixture(introducedinsection)with3.9.6\nmixtureweights \u03b1 i(thecoe\ufb03cientorpriorprobabilityforcomponent i),per-\ncomponentconditionalmean \u00b5 iandper-componentconditionalvariance \u03c32\ni.\u00a0A\nmodelcalledRNADE(,)usesthisparametrization toextendNADE Uria e t a l .2013\ntorealvalues.Aswithothermixturedensitynetworks,theparametersofthis\n7 0 9", "CHAPTER20.DEEPGENERATIVEMODELS\ndistributionareoutputsofthenetwork,withthemixtureweightprobabilities\nproducedbyasoftmaxunit,andthevariancesparametrized sothattheyare\npositive.\u00a0Stochasticgradientdescentcanbenumericallyill-behavedduetothe\ninteractionsbetweentheconditionalmeans \u00b5 iandtheconditionalvariances \u03c32\ni.\nToreducethisdi\ufb03culty,()useapseudo-gradientthatreplacesthe Uria e t a l .2013\ngradientonthemean,intheback-propagationphase.\nAnotherveryinterestingextensionoftheneuralauto-regressiv earchitectures\ngetsridoftheneedtochooseanarbitraryorderfortheobservedvariables(Murray\nandLarochelle2014,).Inauto-regressive networks,theideaistotrainthenetwork\ntobeabletocopewithanyorderbyrandomlysamplingordersandprovidingthe\ninformationtohiddenunitsspecifyingwhichoftheinputsareobserved(onthe\nrightsideoftheconditioningbar)andwhicharetobepredictedandarethus\nconsideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecause\nitallowsonetouseatrainedauto-regressiv enetworkto p e r f o r m a ny i nfe r e nc e\np r o b l e m(i.e.predictorsamplefromtheprobabilitydistributionoveranysubset\nofvariablesgivenanysubset)extremelye\ufb03ciently.Finally,sincemanyordersof\nvariablesarepossible( n!for nvariables)andeachorder oofvariablesyieldsa\ndi\ufb00erent,wecanformanensembleofmodelsformanyvaluesof: p o(x|) o\np e nse m bl e() =x1\nkk\ue058\ni = 1p o(x|( ) i) . (20.84)\nThisensemblemodelusuallygeneralizesbetterandassignshigherprobabilityto\nthetestsetthandoesanindividualmodelde\ufb01nedbyasingleordering.\nInthesamepaper,theauthorsproposedeepversionsofthearchitecture, but\nunfortunately thatimmediatelymakescomputationasexpensiveasintheoriginal\nneuralauto-regressiv eneuralnetwork( ,).The\ufb01rstlayer BengioandBengio2000b\nandtheoutputlayercanstillbecomputedin O( n h)multiply-addoperations,\nasintheregularNADE,where histhenumberofhiddenunits(thesizeofthe\ngroups h i,in\ufb01guresand),whereasitis 20.1020.9 O( n2h)inBengioandBengio\n().However,fortheotherhiddenlayers,thecomputationis 2000b O( n2h2)ifevery\n\u201cprevious\u201dgroupatlayer lparticipatesinpredictingthe\u201cnext\u201dgroupatlayer l+1,\nassuming ngroupsof hhiddenunitsateachlayer.Makingthe i-thgroupatlayer\nl+1onlydependonthe i-thgroup,asinMurrayandLarochelle2014()atlayer l\nreducesitto O n h(2),whichisstilltimesworsethantheregularNADE. h\n7 1 0", "CHAPTER20.DEEPGENERATIVEMODELS\n20.11DrawingSamplesfromAutoencoders\nInchapter,wesawthatmanykindsofautoencoderslearnthedatadistribution. 14\nTherearecloseconnectionsbetweenscorematching,denoisingautoencoders,and\ncontractiveautoencoders.Theseconnectionsdemonstratethatsomekindsof\nautoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhow\ntodrawsamplesfromsuchmodels.\nSomekindsofautoencoders,suchasthevariationalautoencoder,explicitly\nrepresentaprobabilitydistributionandadmitstraightforwardancestralsampling.\nMostotherkindsofautoencodersrequireMCMCsampling.\nContractiveautoencodersaredesignedtorecoveranestimateofthetangent\nplaneofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwith\ninjectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifai\ne t a l . e t a l . ,;2012Mesnil,).Thismanifolddi\ufb00usiontechniqueisakindof 2012\nMarkovchain.\nThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoising\nautoencoder.\n20.11.1MarkovChainAssociatedwithanyDenoisingAutoen-\ncoder\nTheabovediscussionleftopenthequestionofwhatnoisetoinjectandwhere,\ninordertoobtainaMarkovchainthatwouldgeneratefromthedistribution\nestimatedbytheautoencoder. ()showedhowtoconstruct Bengio e t a l .2013c\nsuchaMarkovchainforgeneralizeddenoisingautoencoders.Generalized\ndenoisingautoencodersarespeci\ufb01edbyadenoisingdistributionforsamplingan\nestimateofthecleaninputgiventhecorruptedinput.\nEachstepoftheMarkovchainthatgeneratesfromtheestimateddistribution\nconsistsofthefollowingsub-steps,illustratedin\ufb01gure:20.11\n1.Startingfromthepreviousstate x,injectcorruptionnoise,sampling \u02dc xfrom\nC(\u02dc x x|).\n2.\u00a0Encode\u02dc xinto h= ( f\u02dc x).\n3.\u00a0Decodetoobtaintheparameters of h \u03c9 h= ( g) p g p ( = x | \u03c9 ()) = h (x|\u02dc x).\n4.\u00a0Samplethenextstatefrom x p g p ( = x| \u03c9 ()) = h (x|\u02dc x).\n7 1 1", "CHAPTER20.DEEPGENERATIVEMODELS\nxx\u02dc x\u02dc xh h\n\u03c9\u03c9\n\u02c6 x \u02c6 xC ( \u02dc x x| ) p ( ) x| \u03c9fg\nFigure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-\ncoder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythe\ndenoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruption\nprocess Cinstate x,yielding\u02dc x,(b)encodingitwithfunction f,yielding h= f(\u02dc x),\n(c)decodingtheresultwithfunction g,yieldingparameters \u03c9forthereconstruction\ndistribution,and(d)given \u03c9,samplinganewstatefromthereconstructiondistribution\np(x | \u03c9= g( f(\u02dc x))).Inthetypicalsquaredreconstructionerrorcase, g( h)=\u02c6 x,which\nestimates E[ x|\u02dc x],corruptionconsistsinaddingGaussiannoiseandsamplingfrom\np(x| \u03c9)consistsinaddingGaussiannoise,asecondtime,tothereconstruction\u02c6 x.The\nlatternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereas\ntheinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellasthe\nextenttowhichtheestimatorsmoothstheempiricaldistribution(,).Inthe Vincent2011\nexampleillustratedhere,onlythe Cand pconditionalsarestochasticsteps( fand gare\ndeterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,\nasingenerativestochasticnetworks( ,). Bengio e t a l .2014\n7 1 2", "CHAPTER20.DEEPGENERATIVEMODELS\nBengio2014 e t a l .()showedthatiftheautoencoder p(x |\u02dcx)formsaconsistent\nestimatorofthecorrespondingtrueconditionaldistribution,thenthestationary\ndistributionoftheaboveMarkovchainformsaconsistentestimator(albeitan\nimplicitone)ofthedatageneratingdistributionof.x\n20.11.2ClampingandConditionalSampling\nSimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations\n(suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-\nbution p(x f|x o),simplybyclampingthe o b s e r v e dunits x fandonlyresampling\nthe f r e eunits x ogivenx fandthesampledlatentvariables(ifany).Forexample,\nMP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareable\ntosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentin\nMP-DBMstoperformthesameoperation( ,). () Bengio e t a l .2014Alain e t a l .2015\nidenti\ufb01edamissingconditionfromProposition1of (),whichis Bengio e t a l .2014\nthatthetransitionoperator(de\ufb01nedbythestochasticmappinggoingfromone\nstateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance,\nwhichspeci\ufb01esthataMarkovChainatequilibriumwillremaininequilibrium\nwhetherthetransitionoperatorisruninforwardorreverse.\nAnexperimentinclampinghalfofthepixels(therightpartoftheimage)and\nrunningtheMarkovchainontheotherhalfisshownin\ufb01gure.20.12\n7 1 3", "CHAPTER20.DEEPGENERATIVEMODELS\nFigure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkov\nChainbyresamplingonlythelefthalfateachstep.\u00a0ThesesamplescomefromaGSN\ntrainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure.\n20.11.3Walk-BackTrainingProcedure\nThewalk-backtrainingprocedurewasproposedby ()asaway Bengio e t a l .2013c\ntoacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders.\nInsteadofperformingaone-stepencode-decodereconstruction,thisprocedure\nconsistsinalternativemultiplestochasticencode-decodesteps(asinthegenerative\nMarkovchain)initializedatatrainingexample(justlikewiththecontrastive\ndivergencealgorithm,describedinsection)andpenalizingthelastprobabilistic 18.2\nreconstructions(orallofthereconstructionsalongtheway).\nTrainingwith kstepsisequivalent(inthesenseofachievingthesamestationary\ndistribution)astrainingwithonestep,butpracticallyhastheadvantagethat\nspuriousmodesfurtherfromthedatacanberemovedmoree\ufb03ciently.\n20.12GenerativeStochasticNetworks\nGenerativestochasticnetworksorGSNs( ,)aregeneraliza- Bengio e t a l .2014\ntionsofdenoisingautoencodersthatincludelatentvariables hinthegenerative\n7 1 4", "CHAPTER20.DEEPGENERATIVEMODELS\nMarkovchain,inadditiontothevisiblevariables(usuallydenoted).x\nAGSNisparametrized bytwoconditionalprobabilitydistributionswhich\nspecifyonestepoftheMarkovchain:\n1.\u00a0 p(x( ) k|h( ) k)tellshowtogeneratethenextvisiblevariablegiventhecurrent\nlatentstate.Sucha\u201creconstructiondistribution\u201disalsofoundindenoising\nautoencoders,RBMs,DBNsandDBMs.\n2.\u00a0 p(h( ) k|h( 1 ) k\u2212,x( 1 ) k\u2212)tellshowtoupdatethelatentstatevariable,given\nthepreviouslatentstateandvisiblevariable.\nDenoisingautoencodersandGSNsdi\ufb00erfromclassicalprobabilisticmodels\n(directedorundirected)inthattheyparametrizethegenerativeprocessitselfrather\nthanthemathematical speci\ufb01cationofthejointdistributionofvisibleandlatent\nvariables.Instead,thelatterisde\ufb01ned ,,asthestationary i m p l i c i t l y i f i t e x i s t s\ndistributionofthegenerativeMarkovchain.Theconditionsforexistenceofthe\nstationarydistributionaremildandarethesameconditionsrequiredbystandard\nMCMCmethods(seesection).Theseconditionsarenecessarytoguarantee 17.3\nthatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransition\ndistributions(forexample,iftheyweredeterministic).\nOnecouldimaginedi\ufb00erenttrainingcriteriaforGSNs.Theoneproposedand\nevaluatedby ()issimplyreconstructionlog-probabilit yonthe Bengio e t a l .2014\nvisibleunits,justlikefordenoisingautoencoders.Thisisachievedbyclamping\nx( 0 )= xtotheobservedexampleandmaximizingtheprobabilityofgenerating x\natsomesubsequenttimesteps,i.e.,maximizing log p(x( ) k= x|h( ) k),where h( ) k\nissampledfromthechain,givenx( 0 )= x.\u00a0Inordertoestimatethegradientof\nlog p(x( ) k= x|h( ) k)withrespecttotheotherpiecesofthemodel,Bengio e t a l .\n()usethereparametrization trick,introducedinsection. 2014 20.9\nThewalk-backtrainingprotocol(describedinsection)wasused( 20.11.3 Ben-\ngio2014 e t a l .,)toimprovetrainingconvergenceofGSNs.\n20.12.1DiscriminantGSNs\nTheoriginalformulationofGSNs( ,)wasmeantforunsupervised Bengio e t a l .2014\nlearningandimplicitlymodeling p(x)forobserveddatax,butitispossibleto\nmodifytheframeworktooptimize . p( )y| x\nForexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,by\nonlyback-propagatingthereconstructionlog-probabilit yovertheoutputvariables,\nkeepingtheinputvariables\ufb01xed.Theyappliedthissuccessfullytomodelsequences\n7 1 5", "CHAPTER20.DEEPGENERATIVEMODELS\n(proteinsecondarystructure)andintroduceda(one-dimensional) convolutional\nstructureinthetransitionoperatoroftheMarkovchain.Itisimportantto\nrememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequence\nforeachlayer,andthatsequenceistheinputforcomputingotherlayervalues(say\ntheonebelowandtheoneabove)atthenexttimestep.\nHencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher-\nlevelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain,\nwithback-propagationallowingtolearnhowtheinputsequencecanconditionthe\noutputdistributionimplicitlyrepresentedbytheMarkovchain.Itisthereforea\ncaseofusingtheGSNinthecontextofstructuredoutputs.\nZ\u00f6hrerandPernkopf2014()introducedahybridmodelthatcombinesasuper-\nvisedobjective(asintheabovework)andanunsupervisedobjective(asinthe\noriginalGSNwork),bysimplyadding(withadi\ufb00erentweight)thesupervisedand\nunsupervisedcostsi.e.,thereconstructionlog-probabilities ofyandxrespectively.\nSuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelle\nandBengio2008().Theyshowimprovedclassi\ufb01cationperformanceusingthis\nscheme.\n20.13OtherGenerationSchemes\nThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestral\nsampling,orsomemixtureofthetwotogeneratesamples.\u00a0Whilethesearethe\nmostpopularapproachestogenerativemodeling,theyarebynomeanstheonly\napproaches.\nSohl-Dickstein2015 e t a l .()developedadi\ufb00usioninversiontrainingscheme\nforlearningagenerativemodel,basedonnon-equilibrium thermodynamics.The\napproachisbasedontheideathattheprobabilitydistributionswewishtosample\nfromhavestructure.Thisstructurecangraduallybedestroyedbyadi\ufb00usion\nprocessthatincrementally changestheprobabilitydistributiontohave\u00a0more\nentropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytraining\namodelthatgraduallyrestoresthestructuretoanunstructureddistribution.\nByiterativelyapplyingaprocessthatbringsadistributionclosertothetarget\none,wecangraduallyapproachthattargetdistribution.Thisapproachresembles\nMCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample.\nHowever,themodelisde\ufb01nedtobetheprobabilitydistributionproducedby\nthe\ufb01nalstepofthechain.\u00a0Inthissense,thereisnoapproximation inducedby\ntheiterativeprocedure.Theapproachintroducedby () Sohl-Dickstein e t a l .2015\nisalsoveryclosetothegenerativeinterpretation ofthedenoisingautoencoder\n7 1 6", "CHAPTER20.DEEPGENERATIVEMODELS\n(section).Aswiththedenoisingautoencoder,di\ufb00usioninversiontrainsa 20.11.1\ntransitionoperatorthatattemptstoprobabilisticallyundothee\ufb00ectofadding\nsomenoise.Thedi\ufb00erenceisthatdi\ufb00usioninversionrequresundoingonlyonestep\nofthedi\ufb00usionprocess,ratherthantravelingallthewaybacktoacleandatapoint.\nThisaddressesthefollowingdilemmapresentwiththeordinaryreconstruction\nlog-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethe\nlearneronlyseescon\ufb01gurations nearthedatapoints,whilewithlargelevelsof\nnoiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistribution\nishighlycomplexandmulti-modal). Withthedi\ufb00usioninversionobjective,the\nlearnercanlearntheshapeofthedensityaroundthedatapointsmoreprecisely\naswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints.\nAnotherapproachtosamplegenerationistheapproximateBayesiancom-\nputation(ABC)framework(,).Inthisapproach,samplesare Rubin e t a l .1984\nrejectedormodi\ufb01edinordertomakethemomentsofselectedfunctionsofthe\nsamplesmatchthoseofthedesireddistribution.Whilethisideausesthemoments\nofthesampleslikeinmomentmatching,itisdi\ufb00erentfrommomentmatching\nbecauseitmodi\ufb01esthesamplesthemselves,ratherthantrainingthemodelto\nautomatically emitsampleswiththecorrectmoments. () BachmanandPrecup2015\nshowedhowtouseideasfromABCinthecontextofdeeplearning,byusingABC\ntoshapetheMCMCtrajectoriesofGSNs.\nWeexpectthatmanyotherpossibleapproachestogenerativemodelingawait\ndiscovery.\n20.14EvaluatingGenerativeModels\nResearchersstudyinggenerativemodelsoftenneedtocompareonegenerative\nmodeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerative\nmodelisbetteratcapturingsomedistributionthanthepre-existingmodels.\nThiscanbeadi\ufb03cultandsubtletask.Inmanycases,wecannotactually\nevaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation.\nInthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhat\nisbeingmeasured.Forexample,supposewecanevaluateastochasticestimateof\nthelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihood\nformodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwe\ncareaboutdeterminingwhichmodelhasabetterinternalrepresentationofthe\ndistribution,weactuallycannottell,unlesswehavesomewayofdetermininghow\nloosetheboundformodelBis.However,ifwecareabouthowwellwecanuse\nthemodelinpractice,forexampletoperformanomalydetection,thenitisfairto\n7 1 7", "CHAPTER20.DEEPGENERATIVEMODELS\nsaythatamodelispreferablebasedonacriterionspeci\ufb01ctothepracticaltaskof\ninterest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecision\nandrecall.\nAnothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetrics\nareoftenhardresearchproblemsinandofthemselves.Itcanbeverydi\ufb03cult\ntoestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuse\nAIStoestimate log Zinordertocompute log \u02dc p( x)\u2212log Zforanewmodelwe\nhavejustinvented.Acomputationally economicalimplementation ofAISmayfail\nto\ufb01ndseveralmodesofthemodeldistributionandunderestimate Z,whichwill\nresultinusoverestimatinglog p( x).Itcanthusbedi\ufb03culttotellwhetherahigh\nlikelihoodestimateisduetoagoodmodelorabadAISimplementation.\nOther\ufb01eldsofmachinelearningusuallyallowforsomevariationinthepre-\nprocessingofthedata.Forexample,whencomparingtheaccuracyofobject\nrecognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimages\nslightlydi\ufb00erentlyforeachalgorithmbasedonwhatkindofinputrequirements\nithas.Generativemodelingisdi\ufb00erentbecausechangesinpreprocessing,even\nverysmallandsubtleones,arecompletelyunacceptable. Anychangetotheinput\ndatachangesthedistributiontobecapturedandfundamentallyaltersthetask.\nForexample,multiplyingtheinputby0.1willarti\ufb01ciallyincreaselikelihoodbya\nfactorof10.\nIssueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodels\nontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks.\nMNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints\ninarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthe\ngrayscalevaluesasprobabilities forabinarysamples.Itisessentialtocompare\nreal-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonly\ntootherbinary-valuedmodels.\u00a0Otherwisethelikelihoodsmeasuredarenotonthe\nsamespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,while\nforreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofa\ndensity.Amongbinarymodels,itisimportanttocomparemodelsusingexactly\nthesamekindofbinarization. Forexample,wemightbinarizeagraypixelto0or1\nbythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing\n1isgivenbythegraypixelintensity.Ifweusetherandombinarization, wemight\nbinarizethewholedatasetonce,orwemightdrawadi\ufb00erentrandomexamplefor\neachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthese\nthreeschemesyieldswildlydi\ufb00erentlikelihoodnumbers,andwhencomparing\ndi\ufb00erentmodelsitisimportantthatbothmodelsusethesamebinarizationscheme\nfortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom\n7 1 8", "CHAPTER20.DEEPGENERATIVEMODELS\nbinarizationstepsharea\ufb01lecontainingtheresultsoftherandombinarization, so\nthatthereisnodi\ufb00erenceinresultsbasedondi\ufb00erentoutcomesofthebinarization\nstep.\nBecausebeingabletogeneraterealisticsamplesfromthedatadistribution\nisoneofthegoalsofagenerativemodel,practitioners oftenevaluategenerative\nmodelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbythe\nresearchersthemselves,butbyexperimentalsubjectswhodonotknowthesource\nofthesamples(Denton2015 e t a l .,).Unfortunately,itispossibleforaverypoor\nprobabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyif\nthemodelonlycopiessomeofthetrainingexamplesisillustratedin\ufb01gure.16.1\nTheideaistoshowforsomeofthegeneratedsamplestheirnearestneighborin\nthetrainingset,accordingtoEuclideandistanceinthespaceof x.\u00a0Thistestis\nintendedtodetectthecasewherethemodelover\ufb01tsthetrainingsetandjust\nreproducestraininginstances.Itisevenpossibletosimultaneouslyunder\ufb01tand\nover\ufb01tyetstillproducesamplesthatindividuallylookgood.Imagineagenerative\nmodeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethe\ntrainingimagesofdogs.Suchamodelhasclearlyover\ufb01t,becauseitdoesnot\nproducesimagesthatwerenotinthetrainingset,butithasalsounder\ufb01t,because\nitassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserver\nwouldjudgeeachindividualimageofadogtobehighquality.Inthissimple\nexample,itwouldbeeasyforahumanobserverwhocaninspectmanysamplesto\ndeterminethatthecatsareabsent.Inmorerealisticsettings,agenerativemodel\ntrainedondatawithtensofthousandsofmodesmayignoreasmallnumberof\nmodes,andahumanobserverwouldnoteasilybeabletoinspectorremember\nenoughimagestodetectthemissingvariation.\nSince\u00a0thevisual\u00a0quality\u00a0ofsamples\u00a0is\u00a0not\u00a0areliable\u00a0guide,\u00a0we\u00a0often also\nevaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisis\ncomputationally feasible.Unfortunately,insomecasesthelikelihoodseemsnot\ntomeasureanyattributeofthemodelthatwereallycareabout.Forexample,\nreal-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigning\narbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsand\nalgorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,even\nthoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacost\napproaching\u00a0negativein\ufb01nityis\u00a0present\u00a0foranykind\u00a0of\u00a0maximum\u00a0likelihood\nproblemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsof\nMNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstrongly\nsuggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels.\nTheis2015 e t a l .()reviewmanyoftheissuesinvolvedinevaluatinggenerative\n7 1 9", "CHAPTER20.DEEPGENERATIVEMODELS\nmodels,includingmanyoftheideasdescribedabove.Theyhighlightthefact\nthattherearemanydi\ufb00erentusesofgenerativemodelsandthatthechoiceof\nmetricmustmatchtheintendeduseofthemodel.Forexample,somegenerative\nmodelsarebetteratassigninghighprobabilitytomostrealisticpointswhileother\ngenerativemodelsarebetteratrarelyassigninghighprobabilitytounrealistic\npoints.Thesedi\ufb00erencescanresultfromwhetheragenerativemodelisdesigned\ntominimize D K L( p da t a\ue06b p m o de l)or D K L( p m o de l\ue06b p da t a),asillustratedin\ufb01gure.3.6\nUnfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismost\nsuitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses.\nOneofthemostimportantresearchtopicsingenerativemodelingisthereforenot\njusthowtoimprovegenerativemodels,butinfact,designingnewtechniquesto\nmeasureourprogress.\n20.15Conclusion\nTraininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodels\nunderstandtheworldrepresentedinthegiventrainingdata.Bylearningamodel\np m o de l( x)andarepresentation p m o de l( h x|),agenerativemodelcanprovide\nanswerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariables\nin xandcanprovidemanydi\ufb00erentwaysofrepresenting xbytakingexpectations\nof hatdi\ufb00erentlayersofthehierarchy.\u00a0Generativemodelsholdthepromiseto\nprovideAIsystemswithaframeworkforallofthemanydi\ufb00erentintuitiveconcepts\ntheyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthe\nfaceofuncertainty.Wehopethatourreaderswill\ufb01ndnewwaystomakethese\napproachesmorepowerfulandcontinuethejourneytounderstandingtheprinciples\nthatunderlielearningandintelligence.\n7 2 0", "Bibliograp hy\nAbadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,\nA.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,\nJia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Man\u00e9,D.,Monga,R.,\nMoore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,\nTalwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Vi\u00e9gas,F.,Vinyals,O.,Warden,\nP.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scale\nmachinelearningonheterogeneoussystems.Softwareavailablefromtensor\ufb02ow.org.,25\n214446,\nAckley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).Alearningalgorithmfor\nBoltzmannmachines.CognitiveScience,,147\u2013169. , 9 570654\nAlain,G.andBengio,Y.(2013).\u00a0Whatregularizedauto-encoderslearnfromthedata\ngeneratingdistribution.In .,,, ICLR\u20192013,arXiv:1211.4246507513514521\nAlain,G.,Bengio,Y.,Yao,L.,\u00c9ricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015).\nGSNs:Generativestochasticnetworks.arXiv:1503.05571.,510713\nAnderson,E.(1935).TheIrisesoftheGasp\u00e9Peninsula.BulletinoftheAmericanIris\nSociety,,2\u20135. 5 921\nBa,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisual\nattention. . arXiv:1412.7755691\nBachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswith\ncollaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachine\nLearning,ICML2015,Lille,France,6-11July2015,pages1964\u20131972. 717\nBacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationin\nneuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConference\nonReinforcementLearningandDecisionMaking(RLDM2015).450\nBagnell,J.A.andBradley,D.M.(2009).Di\ufb00erentiablesparsecoding.InD.Koller,\nD.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformation\nProcessingSystems21(NIPS\u201908),pages113\u2013120.498\n721", "BIBLIOGRAPHY\nBahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate.In .,,,,, ICLR\u20192015,arXiv:1409.047325101397418420\n465475476,,\nBahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987).\u00a0Speechrecognition\nwithcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage, 2,\n219\u2013234.458\nBaldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:\nLearningfromexampleswithoutlocalminima.NeuralNetworks,,53\u201358. 2286\nBaldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthe\npastandthefutureinproteinsecondarystructureprediction. , Bioinformatics 1 5(11),\n937\u2013946.395\nBaldi,\u00a0P.,\u00a0Sadowski,\u00a0P.,\u00a0andWhiteson,\u00a0D.(2014).Searchingforexoticparticlesin\nhigh-energyphysicswithdeeplearning.Naturecommunications,. 526\nBallard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation.\nNature.452\nBarlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295\u2013311. 1 147\nBarron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidal\nfunction.IEEETrans.onInformationTheory,,930\u2013945. 3 9 199\nBartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversity\nPress.490\nBasilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:Theoryand\nApplications.Wiley.490\nBastien,F.,Lamblin,P.,\u00a0Pascanu,R.,Bergstra,J.,\u00a0Goodfellow,I.J.,Bergeron,A.,\nBouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements.\nDeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582214\n222446,\nBasu,S.andChristensen,J.(2013).\u00a0Teachingclassi\ufb01cationboundariestohumans.\u00a0In\nAAAI\u20192013.329\nBaxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternational\nConferenceonComputationalLearningTheory(COLT\u201995),pages311\u2013320,SantaCruz,\nCalifornia.ACMPress.245\nBayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXiv\ne-prints.265\nBecker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfaces\ninrandom-dotstereograms.Nature,,161\u2013163. 3 5 5 541\n7 2 2", "BIBLIOGRAPHY\nBehnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstraction\npyramid.Int.J.ComputationalIntelligenceandApplications,(4),427\u2013438. 1 515\nBeiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthreshold\nlogic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson, 1 4(5),1217\u2013\n1243.451\nBelkin,\u00a0M.and\u00a0Niyogi,\u00a0P.(2002).Laplacianeigenmapsandspectraltechniquesfor\nembeddingandclustering.\u00a0InT.Dietterich,S.Becker,andZ.Ghahramani,editors,\nAdvancesinNeuralInformationProcessingSystems14(NIPS\u201901),Cambridge,MA.\nMITPress.244\nBelkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionand\ndatarepresentation.NeuralComputation,(6),1373\u20131396. , 1 5 164518\nBengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationin\nneuralnetworksforfastermodels.arXiv:1511.06297.450\nBengio,\u00a0S.\u00a0andBengio,\u00a0Y.\u00a0(2000a).Taking\u00a0onthecurseofdimensionalityinjoint\ndistributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,special\nissueonDataMiningandKnowledgeDiscovery,(3),550\u2013557. 1 1 707\nBengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingfor\nsequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099.\n384\nBengio,Y.(1991).Arti\ufb01cialNeuralNetworksandtheirApplicationtoSequenceRecognition.\nPh.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.407\nBengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,\n1 2(8),1889\u20131900. 435\nBengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,\nDept.IRO,Universit\u00e9deMontr\u00e9al.467\nBengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,201622\nBengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatistical\nLanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,\npages1\u201337.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.448\nBengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation.\nTechnicalReportarXiv:1510.02777,UniversitedeMontreal.656\nBengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-\nlayerneuralnetworks.In,pages400\u2013406.MITPress.,,, NIPS12 705707708710\nBengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence.\nNeuralComputation,(6),1601\u20131621. , 2 1 513611\n7 2 3", "BIBLIOGRAPHY\nBengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-fold\ncross-validation.InS.Thrun,L.Saul,andB.Sch\u00f6lkopf,editors,AdvancesinNeural\nInformationProcessingSystems16(NIPS\u201903),Cambridge,MA.MITPress,Cambridge.\n122\nBengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScale\nKernelMachines.19\nBengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,\nY.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems\n17(NIPS\u201904),pages129\u2013136.MITPress.,160519\nBengio,Y.andS\u00e9n\u00e9cal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsby\nimportancesampling.InProceedingsofAISTATS2003.470\nBengio,Y.andS\u00e9n\u00e9cal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetraining\nofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks, 1 9(4),713\u2013722.\n470\nBengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivated\nacousticparametersforcontinuousspeechrecognitionusingarti\ufb01cialneuralnetworks.\nInProceedingsofEuroSpeech\u201991.,27459\nBengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussian\nmixturehybridforspeechrecognitionordensityestimation.In,pages175\u2013182. NIPS4\nMorganKaufmann.459\nBengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-term\ndependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeural\nNetworks,pages1183\u20131195, SanFrancisco.IEEEPress.(invitedpaper).403\nBengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswith\ngradientdescentisdi\ufb03cult.IEEETr.NeuralNets.,,, 18401403411\nBengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-\nparameters.LearningConference,Snowbird.435\nBengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel.\nInT.K.Leen,T.G.Dietterich,andV.Tresp,editors, ,pages932\u2013938.MIT NIPS\u20192000\nPress.,,,,,, 18447464466472477482\nBengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilistic\nlanguagemodel.,,1137\u20131155. , JMLR 3 466472\nBengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convex\nneuralnetworks.In ,pages123\u2013130. NIPS\u20192005 258\nBengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctions\nforlocalkernelmachines.In . NIPS\u20192005158\n7 2 4", "BIBLIOGRAPHY\nBengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows.\nIn .MITPress., NIPS\u20192005 160520\nBengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wise\ntrainingofdeepnetworks.In .,,,,,, NIPS\u201920061419201323324528530\nBengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.In\nICML\u201909.328\nBengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeep\nrepresentations.In . ICML\u20192013604\nBengio,Y.,L\u00e9onard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradients\nthroughstochasticneuronsforconditionalcomputation.\u00a0arXiv:1308.3432.,,448450\n689691,\nBengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-\nencodersasgenerativemodels.In .,, NIPS\u20192013507711714\nBengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewand\nnewperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),\n3 5(8),1798\u20131828. 555\nBengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014).\u00a0Deepgenerative\nstochasticnetworkstrainablebybackprop.In .,,,, ICML\u20192014711712713714715\nBennett,C.(1976).E\ufb03cientestimationoffreeenergydi\ufb00erencesfromMonteCarlodata.\nJournalofComputationalPhysics,(2),245\u2013268. 2 2 628\nBennett,J.andLanning,S.(2007).TheNet\ufb02ixprize.479\nBerger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropy\napproachtonaturallanguageprocessing. ,,39\u201371. ComputationalLinguistics 2 2473\nBerglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastive\ndivergenceandpersistentcontrastivedivergence., . CoRR a b s/ 1 3 1 2 .6 0 0 2614\nBergstra,\u00a0J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor\u00a0Pattern\nClassi\ufb01cation.Ph.D.thesis,Universit\u00e9deMontr\u00e9al.255\nBergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplex\ncell-likenetworks.In . NIPS\u20192009494\nBergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J.\nMachineLearningRes.,,281\u2013305. ,, 1 3 433434435\nBergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,\nJ.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpression\ncompiler.InProc.SciPy.,,,, 2582214222446\n7 2 5", "BIBLIOGRAPHY\nBergstra,J.,Bardenet,R.,Bengio,Y.,andK\u00e9gl,B.(2011).Algorithmsforhyper-parameter\noptimization.In . NIPS\u20192011436\nBerkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplex\ncellproperties. ,(6),579\u2013602. JournalofVision 5 495\nBertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScienti\ufb01c.\n106\nBesag,J.(1975).Statisticalanalysisofnon-latticedata. , TheStatistician 2 4(3),179\u2013195.\n615\nBishop,C.M.(1994).Mixturedensitynetworks.189\nBishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks.\nInProceedingsInternationalConferenceonArti\ufb01cialNeuralNetworksICANN\u201995,\nvolume1,page141\u2013148. ,242250\nBishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization.\nNeuralComputation,(1),108\u2013116. 7 242\nBishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,98146\nBlum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete.\n293\nBlumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).Learnabilityand\ntheVapnik\u2013Chervonenkisdimension. ,(4),929\u2013\u2013865. JournaloftheACM 3 6 114\nBonnet,G.(1964).Transformationsdessignauxal\u00e9atoires\u00e0traverslessyst\u00e8mesnon\nlin\u00e9airessansm\u00e9moire.AnnalesdesT\u00e9l\u00e9communications,(9\u201310),203\u2013220. 1 9 689\nBordes,\u00a0A.,\u00a0Weston,\u00a0J.,\u00a0Collobert,\u00a0R.,\u00a0andBengio,\u00a0Y.(2011).Learningstructured\nembeddingsofknowledgebases.In . AAAI2011484\nBordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsand\nmeaningrepresentationsforopen-textsemanticparsing.AISTATS\u20192012.,,401484485\nBordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergy\nfunctionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueon\nLearningSemantics.483\nBordes,A.,Usunier,\u00a0N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b).\nTranslatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,\nM.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformation\nProcessingSystems26,pages2787\u20132795. CurranAssociates,Inc.484\nBornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR\u20192015,\narXiv:1406.2751.693\n7 2 6", "BIBLIOGRAPHY\nBornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).Trainingbidirectional\nHelmholtzmachines.Technicalreport,arXiv:1506.03877.693\nBoser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-\nmalmarginclassi\ufb01ers.InCOLT\u201992:Proceedingsofthe\ufb01fthannualworkshopon\nComputationallearningtheory,pages144\u2013152,NewYork,NY,USA.ACM.,18141\nBottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,\nOnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.296\nBottou,\u00a0L.(2011).Frommachinelearning\u00a0tomachinereasoning.Technicalreport,\narXiv.1102.1808.401\nBottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.440\nBottou,L.andBousquet,O.(2008).Thetradeo\ufb00soflargescalelearning.In . NIPS\u20192008\n282295,\nBoulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporal\ndependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgeneration\nandtranscription.In ., ICML\u201912685686\nBoureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolingin\nvisionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML\u201910).\n345\nBoureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011).\u00a0Askthelocals:\nmulti-waylocalpoolingforimagerecognition.InProc.InternationalConferenceon\nComputerVision(ICCV\u201911).IEEE.345\nBourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsand\nsingularvaluedecomposition.BiologicalCybernetics,,291\u2013294. 5 9 502\nBourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layered\nperceptrons.ComputerSpeechandLanguage,,1\u201319. 3459\nBoyd,S.andVandenberghe,L.(2004). .CambridgeUniversity ConvexOptimization\nPress,NewYork,NY,USA.93\nBrady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparate\nwhereperceptronssucceed.IEEETransactionsonCircuitsandSystems, 3 6,665\u2013674.\n284\nBrakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfor\ntime-seriesimputation.JournalofMachineLearningResearch, 1 4,2771\u20132797. ,674\n698\nBrand,M.(2003).Chartingamanifold.In ,pages961\u2013968.MITPress., NIPS\u20192002 164\n518\n7 2 7", "BIBLIOGRAPHY\nBreiman,L.(1994).Baggingpredictors.MachineLearning,(2),123\u2013140. 2 4 256\nBreiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).Classi\ufb01cationand\nRegressionTrees.WadsworthInternationalGroup,Belmont,CA.146\nBridle,J.S.(1990).Alphanets:arecurrent\u2018neural\u2019networkarchitecturewithahidden\nMarkovmodelinterpretation.SpeechCommunication,(1),83\u201392. 9 186\nBriggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009).\nMaximina\ufb03nitylearningofimagesegmentation.In ,pages1865\u20131873. NIPS\u20192009 360\nBrown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,La\ufb00erty,J.D.,\nMercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation.\nComputationallinguistics,(2),79\u201385. 1 6 21\nBrown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-\nbased-grammodelsofnaturallanguage. , n ComputationalLinguistics 1 8,467\u2013479.\n463\nBryson,A.andHo,Y.(1969).\u00a0Appliedoptimalcontrol:\u00a0optimization,estimation,and\ncontrol.BlaisdellPub.Co.225\nBryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolving\noptimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,\nMissleandSpaceDivision.225\nBucilu\u02c7a,\u00a0C.,Caruana,R.,\u00a0and\u00a0Niculescu-Mizil,\u00a0A.\u00a0(2006).Model\u00a0compression.In\nProceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscovery\nanddatamining,pages535\u2013541.ACM.448\nBurda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders.\narXivpreprintarXiv:1509.00519.698\nCai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition.\nInAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshop\non,pages291\u2013296.IEEE.194\nCarreira-Perpi\u00f1an,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning.\nInR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternational\nWorkshoponArti\ufb01cialIntelligenceandStatistics(AISTATS\u201905),pages33\u201340.Society\nforArti\ufb01cialIntelligenceandStatistics.611\nCaruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModels\nSummerSchool,pages372\u2013379.244\nCauchy,A.(1847).M\u00e9thodeg\u00e9n\u00e9ralepourlar\u00e9solutiondesyst\u00e8mesd\u2019\u00e9quationssimul-\ntan\u00e9es.InCompterendudess\u00e9ancesdel\u2019acad\u00e9miedessciences,pages536\u2013538. ,83\n225\n7 2 8", "BIBLIOGRAPHY\nCayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,\nUCSD.164\nChandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACM\ncomputingsurveys(CSUR),(3),15. 4 1102\nChapelle,O.,Weston,J.,andSch\u00f6lkopf,B.(2003).Clusterkernelsforsemi-supervised\nlearning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeural\nInformationProcessingSystems15(NIPS\u201902),pages585\u2013592,Cambridge,MA.MIT\nPress.244\nChapelle,O.,Sch\u00f6lkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MIT\nPress,Cambridge,MA.,244541\nChellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeural\nNetworks\u00a0forDocumentProcessing.In\u00a0GuyLorette,\u00a0editor,\u00a0Tenth\u00a0International\nWorkshoponFrontiersinHandwritingRecognition,LaBaule(France).Universit\u00e9de\nRennes1,Suvisoft.http://www.suvisoft.com.,,2427445\nChen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariant\nspatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervised\nFeatureLearningWorkshop.360\nChen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesfor\nlanguagemodeling.Computer,SpeechandLanguage,(4),359\u2013393. ,, 1 3 462463473\nChen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:\nAsmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-\nceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramming\nlanguagesandoperatingsystems,pages269\u2013284.ACM.451\nChen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,\nandZhang,Z.(2015).MXNet:\u00a0A \ufb02exibleande\ufb03cientmachinelearninglibraryfor\nheterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25\nChen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,\netal. Microarchitecture (2014b).DaDianNao:Amachine-learningsupercomputer.In\n(MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609\u2013622.\nIEEE.451\nChilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:\nBuildingane\ufb03cientandscalabledeeplearningtrainingsystem.In11thUSENIX\nSymposiumonOperatingSystemsDesignandImplementation(OSDI\u201914).447\nCho,K.,Raiko,T.,andIlin,A.(2010).Paralleltemperingise\ufb03cientforlearningrestricted\nBoltzmannmachines.In ., IJCNN\u20192010603614\n7 2 9", "BIBLIOGRAPHY\nCho,K.,Raiko,T.,andIlin,A.(2011).Enhancedgradientandadaptivelearningratefor\ntrainingrestrictedBoltzmannmachines.In ,pages105\u2013112. ICML\u20192011 674\nCho,K.,vanMerri\u00ebnboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y.\n(2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatistical\nmachinetranslation.\u00a0InProceedingsoftheEmpiricialMethodsinNaturalLanguage\nProcessing(EMNLP2014).,,397474475\nCho,K.,VanMerri\u00ebnboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-\nertiesofneuralmachinetranslation:Encoder-decoderapproaches. , ArXive-prints\na b s/ 1 4 0 9 .1 2 5 9.412\nChoromanska,A.,Hena\ufb00,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014).\u00a0The\nlosssurfaceofmultilayernetworks.,285286\nChorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014).\u00a0End-to-endcontinuous\nspeechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412.1602.\n461\nChristianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalof\nNumericalAnalysis,(2),135\u2013150. 1 2 224\nChrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures.\narXiv1506.03694. 412\nChung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgated\nrecurrentneuralnetworksonsequencemodeling.NIPS\u20192014DeepLearningworkshop,\narXiv1412.3555. ,412460\nChung,J.,G\u00fcl\u00e7ehre,\u00c7.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrent\nneuralnetworks.In . ICML\u201915412\nChung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).A\nrecurrentlatentvariablemodelforsequentialdata.In . NIPS\u20192015698\nCiresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneural\nnetworkfortra\ufb03csignclassi\ufb01cation.NeuralNetworks,,333\u2013338. , 3 2 23201\nCiresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010).\u00a0Deepbig\nsimpleneuralnetsforhandwrittendigitrecognition.NeuralComputation, 2 2,1\u201314.\n2427446,,\nCoates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparse\ncodingandvectorquantization.In .,, ICML\u2019201127256498\nCoates,\u00a0A.,Lee,\u00a0H.,andNg,A.Y.\u00a0(2011).Ananalysisofsingle-layernetworksin\nunsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConference\nonArti\ufb01cialIntelligenceandStatistics(AISTATS2011).,,363364455\n7 3 0", "BIBLIOGRAPHY\nCoates,A.,Huval,B.,Wang,\u00a0T.,Wu,\u00a0D.,Catanzaro,\u00a0B.,and\u00a0Andrew,N.\u00a0(2013).\nDeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,\nProceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),\nvolume28(3),pages1337\u20131345. JMLRWorkshopandConferenceProceedings.,,2427\n364447,\nCohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:\nAtensoranalysis.arXiv:1509.05009.554\nCollobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,Universit\u00e9deParisVI,\nLIP6.197\nCollobert,R.(2011).Deeplearningfore\ufb03cientdiscriminativeparsing.InAISTATS\u20192011.\n101477,\nCollobert,R.andWeston,J.(2008a).Auni\ufb01edarchitecturefornaturallanguageprocessing:\nDeepneuralnetworkswithmultitasklearning.In ., ICML\u20192008471477\nCollobert,\u00a0R.\u00a0and\u00a0Weston,J.\u00a0(2008b).A\u00a0uni\ufb01ed\u00a0architecture\u00a0fornatural\u00a0language\nprocessing:Deepneuralnetworkswithmultitasklearning.In . ICML\u20192008535\nCollobert,R.,Bengio,S.,andBengio,Y.(2001).\u00a0AparallelmixtureofSVMsforvery\nlargescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.450\nCollobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylarge\nscaleproblems.NeuralComputation,(5),1105\u20131114. 1 4 450\nCollobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a).\nNaturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearning\nResearch,,2493\u20132537. ,,, 1 2 328477535536\nCollobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-\nmentformachinelearning.InBigLearn,NIPSWorkshop.,,25214446\nComon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,\n3 6,287\u2013314.491\nCortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning, 2 0,\n273\u2013297. ,18141\nCouprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentation\nusingdepthinformation.InInternationalConferenceonLearningRepresentations\n(ICLR2013).,23201\nCourbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeep\nlearning.InArxiv:1412.7024,ICLR\u20192015Workshop.452\nCourville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesby\nspike-and-slabRBMs.In ., ICML\u201911561681\n7 3 1", "BIBLIOGRAPHY\nCourville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slab\nRBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisand\nMachineIntelligence,IEEETransactionson,(9),1874\u20131887. 3 6 682\nCover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition.\nWiley-Interscience.73\nCox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearch\napproachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognition\nandWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8\u201315.IEEE.\n363\nCram\u00e9r,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,135\n295\nCrick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature, 3 0 4,\n111\u2013114.609\nCybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.Mathematics\nofControl,Signals,andSystems,,303\u2013314. 2 198\nDahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognition\nwiththemean-covariancerestrictedBoltzmannmachine.In . NIPS\u2019201023\nDahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeep\nneuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,\nSpeech,andLanguageProcessing,(1),33\u201342. 2 0 459\nDahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).Improvingdeepneuralnetworks\nforLVCSRusingrecti\ufb01edlinearunitsanddropout.In . ICASSP\u20192013460\nDahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksfor\nQSARpredictions.arXiv:1406.1231.26\nDauphin,\u00a0Y.andBengio,\u00a0Y.(2013).Stochasticratiomatchingof\u00a0RBMsforsparse\nhigh-dimensionalinputs.In.NIPSFoundation. NIPS26 619\nDauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswith\nreconstructionsampling.In . ICML\u20192011471\nDauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014).\nIdentifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convex\noptimization.In .,, NIPS\u20192014285286288\nDavis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T.\n(2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactions\nonGraphics(Proc.SIGGRAPH),(4),79:1\u201379:10. 3 3 452\n7 3 2", "BIBLIOGRAPHY\nDayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsof\nthe1990ConnectionistSummerSchool,SanMateo,CA.691\nDayan,P.andHinton,G.E.(1996).VarietiesofHelmholtzmachine.NeuralNetworks,\n9(8),1385\u20131403. 693\nDayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtzmachine.\nNeuralcomputation,(5),889\u2013904. 7 693\nDean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,\nSenior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeep\nnetworks.In ., NIPS\u2019201225447\nDean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation.\nComputationalIntelligence,(3),142\u2013150. 5 662\nDeerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990).\nIndexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformation\nScience,(6),391\u2013407. , 4 1 477482\nDelalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS\n19554,\nDeng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet:\u00a0A\nLarge-ScaleHierarchicalImageDatabase.In . CVPR0921\nDeng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan\n10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceon\nComputerVision:PartV,ECCV\u201910,pages71\u201384,Berlin,Heidelberg.Springer-Verlag.\n21\nDeng,L.andYu,D.(2014).Deeplearning\u2013methodsandapplications.Foundationsand\nTrendsinSignalProcessing.460\nDeng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binary\ncodingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,\nChiba,Japan.23\nDenil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattend\nwithdeeparchitecturesforimagetracking.NeuralComputation, 2 4(8),2151\u20132184. 367\nDenton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).Deepgenerativeimagemodels\nusingaLaplacianpyramidofadversarialnetworks.., NIPS702719\nDesjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsfor\nvision.\u00a0TechnicalReport1327,D\u00e9partementd\u2019InformatiqueetdeRechercheOp\u00e9ra-\ntionnelle,Universit\u00e9deMontr\u00e9al.683\n7 3 3", "BIBLIOGRAPHY\nDesjardins,\u00a0G.,\u00a0Courville,\u00a0A.C.,\u00a0Bengio,\u00a0Y.,\u00a0Vincen t,\u00a0P.,\u00a0andDelalleau,\u00a0O.(2010).\nTemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.In\nInternationalConferenceonArti\ufb01cialIntelligenceandStatistics,pages145\u2013152. ,603\n614\nDesjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction.\nIn . NIPS\u20192011629\nDesjardins,G.,Simonyan,K.,Pascanu,R.,(2015).\u00a0Naturalneuralnetworks.\u00a0In etal.\nAdvancesinNeuralInformationProcessingSystems,pages2062\u20132070. 320\nDevlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fast\nandrobustneuralnetworkjointmodelsforstatisticalmachinetranslation.\u00a0InProc.\nACL\u20192014.473\nDevroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:B\u00fccher.\nSpringerNewYork.694\nDiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs.\nneuronsvs.machines.NIPSTutorial.,26366\nDinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponents\nestimation.arXiv:1410.8516.493\nDonahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,\nK.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisual\nrecognitionanddescription.arXiv:1411.4389.102\nDonoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembedding\ntechniquesforhigh-dimensional\u00a0data.TechnicalReport2003-08,\u00a0Dept.Statistics,\nStanfordUniversity.,164519\nDosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswith\nconvolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputer\nVisionandPatternRecognition,pages1538\u20131546. ,,696704705\nDoya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning.\nIEEETransactionsonNeuralNetworks,,75\u201380., 1401403\nDreyfus,\u00a0S.\u00a0E.(1962).The\u00a0numerical\u00a0solutionofvariational\u00a0problems.Journalof\nMathematicalAnalysisandApplications,,30\u201345. 5 ( 1 )225\nDreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtime\nlag.IEEETransactionsonAutomaticControl,,383\u2013385. 1 8 ( 4 ) 225\nDrucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdouble\nback-propagation.IEEETransactionsonNeuralNetworks,(6),991\u2013997. 3 271\n7 3 4", "BIBLIOGRAPHY\nDuchi,J.,Hazan,E.,andSinger,Y.(2011).\u00a0Adaptivesubgradientmethodsforonline\nlearningandstochasticoptimization.JournalofMachineLearningResearch.307\nDudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning.\nInProceedingsofthe28thInternationalConferenceonMachinelearning,ICML\u201911.\n482\nDugas,C.,Bengio,Y.,B\u00e9lisle,F.,andNadeau,C.(2001).\u00a0Incorporatingsecond-order\nfunctionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,\neditors,\u00a0AdvancesinNeural\u00a0InformationProcessingSystems\u00a013(NIPS\u201900),\u00a0pages\n472\u2013478.MITPress.,68197\nDziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-\nworksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906.\n703\nElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-term\ndependencies.In .,, NIPS\u20191995398407408\nElkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachfor\ncrossdomainusermodelinginrecommendationsystems.\u00a0InProceedingsofthe24th\nInternationalConferenceonWorldWideWeb,pages278\u2013288.480\nElman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceof\nstartingsmall.Cognition,,781\u2013799. 4 8 328\nErhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thedi\ufb03culty\noftrainingdeeparchitecturesandthee\ufb00ectofunsupervisedpre-training.InProceedings\nofAISTATS\u20192009.201\nErhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010).\nWhydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes.\n529533534,,\nFahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitectures\nfor\u00a0AI:NETL,thistle,\u00a0andBoltzmann\u00a0machines.In\u00a0Proceedings\u00a0ofthe\u00a0National\nConferenceonArti\ufb01cialIntelligenceAAAI-83.,570654\nFang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Doll\u00e1r,P.,Gao,J.,He,X.,\nMitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisual\nconceptsandback.arXiv:1411.4952.102\nFarabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,and\nTalay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,\nM.Bilenko,andJ.Langford,\u00a0editors,ScalingupMachineLearning:Paralleland\nDistributedApproaches.CambridgeUniversityPress.523\n7 3 5", "BIBLIOGRAPHY\nFarabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeatures\nforscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,\n3 5(8),1915\u20131929. ,,23201360\nFei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories.\nIEEETransactionsonPatternAnalysisandMachineIntelligence, 2 8(4),594\u2013611.538\nFinn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learning\nvisualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXiv\npreprintarXiv:1509.06113.25\nFisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.Annals\nofEugenics,,179\u2013188. , 7 21105\nF\u00f6ldi\u00e1k,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternational\nJointConferenceonNeuralNetworks(IJCNN),volume1,pages401\u2013405,Washington\n1989.IEEE,NewYork.494\nFranzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,\nhead-direction,andspatial-viewcells.495\nFranzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslow\nfeatureanalysis.InArti\ufb01cialNeuralNetworks-ICANN2008,pages961\u2013970.Springer.\n496\nFrasconi,P.,Gori,M.,andSperduti,A.(1997).Onthee\ufb03cientclassi\ufb01cationofdata\nstructuresbyneuralnetworks.InProc.Int.JointConf.onArti\ufb01cialIntelligence.401\nFrasconi,\u00a0P.,\u00a0Gori,\u00a0M.,\u00a0andSperduti,\u00a0A.(1998).Ageneralframeworkforadaptive\nprocessingofdatastructures.IEEETransactionsonNeuralNetworks, 9(5),768\u2013786.\n401\nFreund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.In\nMachineLearning:ProceedingsofThirteenthInternationalConference,pages148\u2013156,\nUSA.ACM.258\nFreund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.In\nProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages\n325\u2013332.258\nFrey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication.\nMITPress.,705706\nFrey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngood\ndensityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advances\ninNeuralInformationProcessingSystems8(NIPS\u201995),pages661\u2013670.MITPress,\nCambridge,MA.651\n7 3 6", "BIBLIOGRAPHY\nFrobenius,G.(1908).\u00dcbermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss.\nBerlin,Germany.597\nFukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.Biological\nCybernetics,,121\u2013136. ,, 2 0 16226528\nFukushima,\u00a0K.(1980).Neocognitron:Aself-organizingneuralnetworkmodelfora\nmechanismofpatternrecognitionuna\ufb00ectedbyshiftinposition.BiologicalCybernetics,\n3 6,193\u2013202. ,,,, 162427226367\nGal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulli\napproximatevariationalinference.arXivpreprintarXiv:1506.02158.264\nGallinari,P.,LeCun,Y.,Thiria,S.,andFogelman-Soulie,F.(1987).Memoiresassociatives\ndistribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.515\nGarcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwo\nandthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprint\narXiv:1506.00999.484\nGarofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993).\nDarpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1.\nNASASTI/ReconTechnicalReportN,,27403. 9 3459\nGarson,J.(1900).Themetricsystemofidenti\ufb01cationofcriminals,asusedinGreat\nBritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainand\nIreland,(2),177\u2013227.21\nGers,F.A.,Schmidhuber,J.,andCummins,F.(2000).\u00a0Learningtoforget:Continual\npredictionwithLSTM.Neuralcomputation,(10),2451\u20132471. , 1 2 410412\nGhahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactor\nanalyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.489\nGillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguage\nprocessingfrombytes.arXivpreprintarXiv:1512.00103.477\nGirshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutional\nnetworksforaccurateobjectdetectionandsegmentation.426\nGiudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogeny\nofmirrorneurons.,(2),350\u2013\u2013363. Dev.Sci. 1 2 656\nGlorot,X.andBengio,Y.(2010).Understandingthedi\ufb03cultyoftrainingdeepfeedforward\nneuralnetworks.InAISTATS\u20192010.303\nGlorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserecti\ufb01erneuralnetworks.In\nAISTATS\u20192011.,,,, 16174197226227\n7 3 7", "BIBLIOGRAPHY\nGlorot,\u00a0X.,Bordes,\u00a0A.,andBengio,\u00a0Y.(2011b).Domainadaptationforlarge-scale\nsentimentclassi\ufb01cation:Adeeplearningapproach.In ., ICML\u20192011507537\nGoldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhood\ncomponentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeural\nInformationProcessingSystems17(NIPS\u201904).MITPress.115\nGong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFace\nRecognition.ImperialCollegePress.,165519\nGoodfellow,I.,Le,Q.,Saxe,A.,\u00a0andNg,A.(2009).Measuringinvariancesindeep\nnetworks.In ,pages646\u2013654. NIPS\u20192009 255\nGoodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010).\nHelpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction\n(HRI),Osaka,Japan.ACMPress,ACMPress.100\nGoodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolution\nforautoencoders.Technicalreport,Universit\u00e9deMontr\u00e9al.357\nGoodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels.\nInInternationalConferenceonLearningRepresentations,WorkshopsTrack.,,622700\n701\nGoodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecoding\nforunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearning\nHierarchicalModels.,532538\nGoodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a).\nMaxoutnetworks.InS.DasguptaandD.McAllester,editors,,pages1319\u2013 ICML\u201913\n1327.,,,, 193264344365455\nGoodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeep\nBoltzmannmachines.In.NIPSFoundation.,,,,,,, NIPS26 100617671672673674675\n698\nGoodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,\nBergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearch\nlibrary.arXivpreprintarXiv:1308.4214.,25446\nGoodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodels\nforunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachine\nIntelligence,(8),1902\u20131914. ,,,, 3 5 497498499650683\nGoodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempirical\ninvestigationofcatastrophicforgetingingradient-basedneuralnetworks.In . ICLR\u20192014\n194\n7 3 8", "BIBLIOGRAPHY\nGoodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-\nsarialexamples., .,,,, CoRR a b s/ 1 4 1 2 .6 5 7 2268269271555556\nGoodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,\nCourville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In . NIPS\u20192014\n544689699701704 ,,,,\nGoodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digit\nnumberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks.\nInInternationalConferenceonLearningRepresentations.,,,,,, 25101201202203391\n422449,\nGoodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneural\nnetworkoptimizationproblems.InInternationalConferenceonLearningRepresenta-\ntions.,,, 285286287291\nGoodman,J.(2001).Classes\u00a0forfast\u00a0maximumentropytraining.InInternational\nConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.467\nGori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEE\nTransactionsonPatternAnalysisandMachineIntelligence, P A M I - 1 4(1),76\u201386.284\nGosset,W.S.(1908).Theprobableerrorofamean. , Biometrika 6(1),1\u201325.Originally\npublishedunderthepseudonym\u201cStudent\u201d.21\nGouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributed\nrepresentationswithoutwordalignments.Technicalreport,arXiv:1410.2455.,476539\nGraf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.Circuits\nandDevicesMagazine,IEEE,(4),44\u201349. 5 451\nGraves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In . NIPS\u20192011242\nGraves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.Studies\ninComputationalIntelligence.Springer.,,, 374395411460\nGraves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,\narXiv:1308.0850.,,, 190410415420\nGraves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrent\nneuralnetworks.In . ICML\u20192014410\nGraves,A.andSchmidhuber,J.(2005).Framewisephonemeclassi\ufb01cationwithbidirec-\ntionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks, 1 8(5),602\u2013610.\n395\nGraves,A.andSchmidhuber,J.(2009).O\ufb04inehandwritingrecognitionwithmultidi-\nmensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,and\nL.Bottou,editors, ,pages545\u2013552. NIPS\u20192008 395\n7 3 9", "BIBLIOGRAPHY\nGraves,A.,Fern\u00e1ndez,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporal\nclassi\ufb01cation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In\nICML\u20192006,pages369\u2013376,Pittsburgh,USA.460\nGraves,A.,Liwicki,M.,Bunke,H.,Schmidhuber,J.,andFern\u00e1ndez,S.(2008).Uncon-\nstrainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,\nD.Koller,Y.Singer,andS.Roweis,editors, ,pages577\u2013584. NIPS\u20192007 395\nGraves,A.,Liwicki,M.,Fern\u00e1ndez,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J.\n(2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.Pattern\nAnalysisandMachineIntelligence,IEEETransactionson,(5),855\u2013868. 3 1 410\nGraves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrent\nneuralnetworks.In ,pages6645\u20136649. ,,,, ICASSP\u20192013 395398410411460\nGraves,A.,Wayne,G.,andDanihelka,I.(2014a).NeuralTuringmachines.\narXiv:1410.5401.25\nGraves,A.,Wayne,G.,andDanihelka,I.(2014b).NeuralTuringmachines.arXivpreprint\narXiv:1410.5401.418\nGrefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningto\ntransducewithunboundedmemory.In . NIPS\u20192015418\nGre\ufb00,K.,Srivastava,R.K.,Koutn\u00edk,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015).\nLSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.412\nGregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproduct\nnetworkwithlocalreceptive\ufb01elds.Technicalreport,arXiv:1006.0448.352\nGregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.In\nL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternational\nConferenceonMachineLearning(ICML-10).ACM.652\nGregor,\u00a0K.,Danihelka,\u00a0I.,Mnih,A.,\u00a0Blundell,C.,and\u00a0Wierstra,\u00a0D.\u00a0(2014).Deep\nautoregressivenetworks.InInternationalConferenceonMachineLearning(ICML\u20192014).\n693\nGregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneural\nnetworkforimagegeneration.arXivpreprintarXiv:1502.04623.698\nGretton,A.,Borgwardt,K.M.,Rasch,M.J.,Sch\u00f6lkopf,B.,andSmola,A.(2012).A\nkerneltwo-sampletest.TheJournalofMachineLearningResearch, 1 3(1),723\u2013773.\n704\nG\u00fcl\u00e7ehre,\u00c7.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformation\nforoptimization.InInternationalConferenceonLearningRepresentations(ICLR\u20192013).\n25\n7 4 0", "BIBLIOGRAPHY\nGuo,H.andGelfand,S.B.(1992).Classi\ufb01cationtreeswithneuralnetworkfeature\nextraction.NeuralNetworks,IEEETransactionson,(6),923\u2013933. 3 450\nGupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearning\nwithlimitednumericalprecision., . CoRR a b s/ 1 5 0 2 .0 2 5 5 1452\nGutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation:\u00a0Anewestima-\ntionprincipleforunnormalizedstatisticalmodels.\u00a0InProceedingsofTheThirteenth\nInternationalConferenceonArti\ufb01cialIntelligenceandStatistics(AISTATS\u201910).620\nHadsell,\u00a0R.,\u00a0Sermanet,\u00a0P.,\u00a0Ben,\u00a0J.,Erkan,A.,\u00a0Han,\u00a0J.,\u00a0Muller, U.,\u00a0andLeCun,\u00a0Y.\n(2007).Onlinelearningforo\ufb00roadrobots:Spatiallabelpropagationtolearnlong-range\ntraversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.453\nHajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuits\nofboundeddepth. ,,129\u2013154. J.Comput.System.Sci. 4 6 199\nH\u00e5stad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedings\nofthe18thannualACMSymposiumonTheoryofComputing,pages6\u201320,Berkeley,\nCalifornia.ACMPress.199\nH\u00e5stad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits.\nComputationalComplexity,,113\u2013129. 1 199\nHastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:\ndatamining,inferenceandprediction.\u00a0SpringerSeriesinStatistics.SpringerVerlag.\n146\nHe,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorecti\ufb01ers:Surpassing\nhuman-levelperformanceonImageNetclassi\ufb01cation.arXivpreprintarXiv:1502.01852.\n28193,\nHebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1417656\nHena\ufb00,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearning\nofsparsefeaturesforscalableaudioclassi\ufb01cation.In . ISMIR\u201911523\nHenderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatistical\nparsing.InHLT-NAACL,pages103\u2013110.477\nHenderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.In\nProceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,\npage95.477\nHenniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andL\u00fccke,J.(2010).Binarysparse\ncoding.InLatentVariableAnalysisandSignalSeparation,pages450\u2013457.Springer.\n640\n7 4 1", "BIBLIOGRAPHY\nHerault,J.andAns,B.(1984).Circuitsneuronaux\u00e0synapsesmodi\ufb01ables:D\u00e9codagede\nmessagescompositesparapprentissagenonsupervis\u00e9.ComptesRendusdel\u2019Acad\u00e9mie\ndesSciences, ,525\u2013\u2013528. 2 9 9 ( I I I - 1 3 ) 491\nHinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.307\nHinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,\nNguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacoustic\nmodelinginspeechrecognition.IEEESignalProcessingMagazine, 2 9(6),82\u201397.,23\n460\nHinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork.\narXivpreprintarXiv:1503.02531.448\nHinton,G.E.(1989).Connectionistlearningprocedures.Arti\ufb01cialIntelligence, 4 0,\n185\u2013234.494\nHinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.Arti\ufb01cial\nIntelligence,(1),47\u201375. 4 6 418\nHinton,G.E.(1999).Productsofexperts.In . ICANN\u20191999571\nHinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence.\nTechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,610\n676\nHinton,G.E.(2006).Torecognizeshapes,\ufb01rstlearntogenerateimages.TechnicalReport\nUTMLTR2006-003,UniversityofToronto.,528595\nHinton,G.E.(2007a).Howtodobackpropagationinabrain.Invitedtalkatthe\nNIPS\u20192007DeepLearningWorkshop.656\nHinton,G.E.(2007b).\u00a0Learningmultiplelayersofrepresentation.\u00a0Trendsincognitive\nsciences,(10),428\u2013434. 1 1 660\nHinton,\u00a0G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines.\nTechnicalReportUTMLTR2010-003,DepartmentofComputerScience,Universityof\nToronto.610\nHinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparse\ndistributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon.\n147\nHinton,G.E.andMcClelland, J.L.(1988).Learningrepresentationsbyrecirculation.In\nNIPS\u20191987,pages358\u2013366.502\nHinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In . NIPS\u20192002519\n7 4 2", "BIBLIOGRAPHY\nHinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawith\nneuralnetworks.Science,(5786),504\u2013507. ,,,, 3 1 3 509524528529534\nHinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines.\nInD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing,\nvolume1,chapter7,pages282\u2013317.MITPress,Cambridge.,570654\nHinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneural\ncomputation.MITpress.541\nHinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsof\nacquireddyslexia.Psychologicalreview,(1),74. 9 813\nHinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,and\nHelmholtzfreeenergy.In . NIPS\u20191993502\nHinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraint\nsatisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-Mellon\nUniversity,Dept.ofComputerScience.,570654\nHinton,G.E.,McClelland, J.,andRumelhart,D.(1986).\u00a0Distributedrepresentations.\nInD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing:\nExplorationsintheMicrostructureofCognition,volume1,pages77\u2013109.MITPress,\nCambridge.,,17225526\nHinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusing\nmixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,Advances\ninNeuralInformationProcessingSystems7(NIPS\u201994),pages1015\u20131022. MITPress,\nCambridge,MA.489\nHinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithm\nforunsupervisedneuralnetworks.Science,,1558\u20131161. , 2 6 8 504651\nHinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesof\nhandwrittendigits.IEEETransactionsonNeuralNetworks,,65\u201374. 8499\nHinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.In\nProceedingsof3rdInternationalConferenceonIndependentComponentAnalysisand\nBlindSignalSeparation(ICA\u201901),pages746\u2013751,SanDiego,CA.491\nHinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbelief\nnets.NeuralComputation,,1527\u20131554. ,,,,,,, 1 8 141927143528529660661\nHinton,G.E.,\u00a0Deng,L.,Yu,\u00a0D.,Dahl,G.E.,Mohamed,\u00a0A.,Jaitly,\u00a0N.,Senior,A.,\nVanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneural\nnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearch\ngroups.IEEESignalProcess.Mag.,(6),82\u201397. 2 9 101\n7 4 3", "BIBLIOGRAPHY\nHinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c).\nImprovingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technical\nreport,arXiv:1207.0580.,,238263267\nHinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge.\u00a0Invitedtalkatthe\nBayLearnBayAreaMachineLearningSymposium.448\nHochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diploma\nthesis,T.U.M\u00fcnchen.,,18401403\nHochreiter,S.andSchmidhuber,J.(1995).\u00a0Simplifyingneuralnetsbydiscovering\ufb02at\nminima.InAdvancesinNeuralInformationProcessingSystems7,pages529\u2013536.MIT\nPress.243\nHochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,\n9(8),1735\u20131780. ,,18410411\nHochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradient\ufb02owinrecurrentnets:the\ndi\ufb03cultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,Field\nGuidetoDynamicalRecurrentNetworks.IEEEPress.411\nHoli,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetwork\nhardwareimplementations.Computers,IEEETransactionson,(3),281\u2013290. 4 2 451\nHolt,J.L.andBaker,T.E.(1991).\u00a0Backpropagationsimulationsusinglimitedpreci-\nsioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJoint\nConferenceon,volume2,pages121\u2013126.IEEE.451\nHornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksare\nuniversalapproximators.NeuralNetworks,,359\u2013366. 2 198\nHornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofan\nunknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neural\nnetworks,(5),551\u2013560. 3 198\nHsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorld\nChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2\nHuang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkov\nrandom\ufb01eldsonlattice.AnnalsoftheInstituteofStatisticalMathematics, 5 4(1),1\u201318.\n616\nHuang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeep\nstructuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsof\nthe22ndACMinternationalconferenceonConferenceoninformation&knowledge\nmanagement,pages2333\u20132338. ACM.480\nHubel,D.andWiesel,T.(1968).Receptive\ufb01eldsandfunctionalarchitectureofmonkey\nstriatecortex.JournalofPhysiology(London),,215\u2013243. 1 9 5 364\n7 4 4", "BIBLIOGRAPHY\nHubel,D.H.andWiesel,T.N.(1959).Receptive\ufb01eldsofsingleneuronsinthecat\u2019s\nstriatecortex.JournalofPhysiology,,574\u2013591. 1 4 8 364\nHubel,D.H.andWiesel,\u00a0T.N.(1962).Receptive\ufb01elds,\u00a0binocularinteraction,and\nfunctionalarchitectureinthecat\u2019svisualcortex.JournalofPhysiology(London), 1 6 0,\n106\u2013154.364\nHuszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,\nadversary? . arXiv:1511.05101698\nHutter,F.,Hoos,H.,andLeyton-Brown,K.(2011).\u00a0Sequentialmodel-basedoptimization\nforgeneralalgorithmcon\ufb01guration.In.ExtendedversionasUBCTechreport LION-5\nTR-2010-10.436\nHyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP\u201996,pages\n13\u201324.379\nHyv\u00e4rinen,A.(1999).\u00a0Surveyonindependentcomponentanalysis.NeuralComputing\nSurveys,,94\u2013128. 2 491\nHyv\u00e4rinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching.\nJournalofMachineLearningResearch,,695\u2013709. , 6 513617\nHyv\u00e4rinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,\nandpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeural\nNetworks,,1529\u20131531. 1 8 618\nHyv\u00e4rinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsand\nDataAnalysis,,2499\u20132512. 5 1 618\nHyv\u00e4rinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcell\npropertiesfromnaturalimagesusingextensionsofica.In,pages827\u2013833. NIPS 493\nHyv\u00e4rinen,\u00a0A.andPajunen,\u00a0P.(1999).Nonlinearindependentcomponentanalysis:\nExistenceanduniquenessresults.NeuralNetworks,(3),429\u2013439. 1 2 493\nHyv\u00e4rinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis.\nWiley-Interscience.491\nHyv\u00e4rinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponent\nanalysis.NeuralComputation,(7),1527\u20131558. 1 3 493\nHyv\u00e4rinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilistic\napproachtoearlycomputationalvision.Springer-Verlag.370\nIba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,\nC 1 2,623\u2013656.603\n7 4 5", "BIBLIOGRAPHY\nInayoshi,\u00a0H.\u00a0and\u00a0Kurita,\u00a0T.\u00a0(2005).Improved\u00a0generalizationbyadding\u00a0both\u00a0auto-\nassociationandhidden-layernoisetoneural-network-based-classi\ufb01ers.IEEEWorkshop\nonMachineLearningforSignalProcessing,pages141\u2014-146.515\nIo\ufb00e,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktraining\nbyreducinginternalcovariateshift.,,100317320\nJacobs,R.A.(1988).\u00a0Increasedratesofconvergencethroughlearningrateadaptation.\nNeuralnetworks,(4),295\u2013307. 1 307\nJacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.(1991).Adaptivemixtures\noflocalexperts.NeuralComputation,,79\u201387., 3189450\nJaeger,H.(2003).Adaptivenonlinearsystemidenti\ufb01cationwithechostatenetworks.In\nAdvancesinNeuralInformationProcessingSystems15.404\nJaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostate\nnetworks.Technicalreport,JacobsUniversity.398\nJaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330. 2 404\nJaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulation\nstudy.Technicalreport,Technicalreport,JacobsUniversityBremen.405\nJaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsand\nsavingenergyinwirelesscommunication.Science,(5667),78\u201380., 3 0 4 27404\nJaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationand\napplicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,\n2 0(3),335\u2013352.407\nJain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,\nM.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestoration\nwithconvolutionalnetworks.InComputer\u00a0Vision,2007.ICCV2007.IEEE11th\nInternationalConferenceon,pages1\u20138.IEEE.359\nJaitly,N.andHinton,G.(2011).Learningabetterrepresentationofspeechsoundwaves\nusingrestrictedBoltzmannmachines.InAcoustics,\u00a0SpeechandSignalProcessing\n(ICASSP),2011IEEEInternationalConferenceon,pages5884\u20135887. IEEE.458\nJaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improves\nspeechrecognition.In . ICML\u20192013241\nJarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebest\nmulti-stagearchitectureforobjectrecognition?In.,,,,,, ICCV\u201909162427174193226\n363364523,,\nJarzynski,C.(1997).Nonequilibriumequalityforfreeenergydi\ufb00erences.Phys.Rev.Lett.,\n7 8,2690\u20132693. ,625628\n7 4 6", "BIBLIOGRAPHY\nJaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversity\nPress.53\nJean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetarget\nvocabularyforneuralmachinetranslation.arXiv:1412.2007.,474475\nJelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparameters\nfromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitionin\nPractice.North-Holland,Amsterdam.,462473\nJia,Y.(2013).Ca\ufb00e:Anopensourceconvolutionalarchitectureforfastfeatureembedding.\nhttp://caffe.berkeleyvision.org/.,25214\nJia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptive\ufb01eld\nlearningforpooledimagefeatures.InComputerVisionandPatternRecognition\n(CVPR),2012IEEEConferenceon,pages3370\u20133377. IEEE.345\nJim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneural\nnetworks:\u00a0convergenceandgeneralization.IEEETransactionsonNeuralNetworks,\n7(6),1424\u20131438. 242\nJordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.18\nJoulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmented\nrecurrentnets.arXivpreprintarXiv:1503.01007.418\nJozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrent\nnetworkarchitectures.In ., ICML\u20192015306412\nJudd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress.\n293\nJutten,\u00a0C.andHerault,\u00a0J.(1991).Blindseparationofsources,\u00a0partI:anadaptive\nalgorithmbasedonneuromimeticarchitecture.SignalProcessing,,1\u201310. 2 4491\nKahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,G\u00fcl\u00e7ehre,c.,Memisevic,R.,Vincent,\nP.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,\nY.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer,\u00a0J.,Lamblin,P.,Raymond,\nJ.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,\nE.,C\u00f4t\u00e9,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeci\ufb01cdeep\nneuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMon\nInternationalConferenceonMultimodalInteraction.201\nKalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.In\nEMNLP\u20192013.,474475\nKalchbrenner,N.,Danihelka,I.,andGraves,A.(2015).\u00a0Gridlongshort-termmemory.\narXivpreprintarXiv:1507.01526.395\n7 4 7", "BIBLIOGRAPHY\nKamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder.\nIEEETransactionsonPatternAnalysisandMachineIntelligence.515\nKarpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimage\ndescriptions.In .arXiv:1412.2306. CVPR\u20192015 102\nKarpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014).\nLarge-scalevideoclassi\ufb01cationwithconvolutionalneuralnetworks.In.CVPR21\nKarush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSide\nConstraints.Master\u2019sthesis,Dept.ofMathematics,Univ.ofChicago.95\nKatz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodel\ncomponentofaspeechrecognizer. IEEETransactionsonAcoustics,Speech,andSignal\nProcessing, (3),400\u2013401. , A S S P-3 5 462473\nKavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008).\u00a0Fastinferenceinsparsecoding\nalgorithmswithapplicationstoobjectrecognition.Technicalreport,Computationaland\nBiologicalLearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01.\n523\nKavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariant\nfeaturesthroughtopographic\ufb01ltermaps.In . CVPR\u20192009523\nKavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y.\n(2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In . NIPS\u20192010\n364523,\nKelley,H.J.(1960).Gradienttheoryofoptimal\ufb02ightpaths. , ARSJournal 3 0(10),\n947\u2013954.225\nKhan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearning\nandteachingdimension.InAdvancesinNeuralInformationProcessingSystems24\n(NIPS\u201911),pages1449\u20131457. 328\nKim,S.K.,McAfee,L.C.,McMahon, P.L.,andOlukotun,K.(2009).Ahighlyscalable\nrestrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogic\nandApplications,2009.FPL2009.InternationalConferenceon,pages367\u2013372.IEEE.\n451\nKindermann,R.(1980).MarkovRandomFieldsandTheirApplications(Contemporary\nMathematics;V.1).AmericanMathematicalSociety.566\nKingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXiv\npreprintarXiv:1412.6980.308\nKingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscore\nmatching.In ., NIPS\u20192010513620\n7 4 8", "BIBLIOGRAPHY\nKingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearning\nwithdeepgenerativemodels.In . NIPS\u20192014426\nKingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariable\nmodelsinauxiliaryform.Technicalreport,arxiv:1306.0733.,,652689696\nKingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedings\noftheInternationalConferenceonLearningRepresentations(ICLR).,689700\nKingma,\u00a0D.P.andWelling,\u00a0M.(2014b).E\ufb03cientgradient-basedinferencethrough\ntransformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480.\n689\nKirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulated\nannealing.Science,,671\u2013680. 2 2 0 327\nKiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels.\nIn . ICML\u20192014102\nKiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddings\nwithmultimodalneurallanguagemodels. ., arXiv:1411.2539[cs.LG]102410\nKlementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributed\nrepresentationsofwords.InProceedingsofCOLING2012.,476539\nKnowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,and\nP\ufb01ster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26\nKoller,D.andFriedman,\u00a0N.(2009).ProbabilisticGraphicalModels:Principlesand\nTechniques.MITPress.,,583595645\nKonig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationand\nmaximizationofaposterioriprobabilities\u2013applicationtotransition-basedconnectionist\nspeechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advancesin\nNeuralInformationProcessingSystems8(NIPS\u201995).MITPress,Cambridge,MA.459\nKoren,Y.(2009).TheBellKorsolutiontotheNet\ufb02ixgrandprize.,258480\nKotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividual\nlabelsusingdeepfeatures.In . ACMSIGKDD106\nKoutnik,J.,Gre\ufb00,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.In\nICML\u20192014.408\nKo\u010disk\u00fd,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-\nsentationsbyMarginalizingAlignments.InProceedingsofACL.476\nKrause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).Approximationproperties\nofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In . ICML\u20192013553\n7 4 9", "BIBLIOGRAPHY\nKrizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,\nUniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-\ncifar10-aug2010.pdf.446\nKrizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtiny\nimages.Technicalreport,UniversityofToronto.,21561\nKrizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-based\nimageretrieval.In.ESANN525\nKrizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassi\ufb01cationwithdeep\nconvolutionalneuralnetworks.In .,,,,,,, NIPS\u20192012232427100201371454458\nKrueger,K.A.andDayan,P.(2009).Flexibleshaping:howlearninginsmallstepshelps.\nCognition,,380\u2013394. 1 1 0 328\nKuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsofthe\nSecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481\u2013492,\nBerkeley,Calif.UniversityofCaliforniaPress.95\nKumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,\nM.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworks\nfornaturallanguageprocessing. ., arXiv:1506.07285418485\nKumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariable\nmodels.In . NIPS\u20192010328\nLang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetwork\narchitectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-Mellon\nUniversity.,,367374407\nLang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetwork\narchitectureforisolatedwordrecognition.Neuralnetworks,(1),23\u201343. 3 374\nLangford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armed\nbandits.In ,pages1096\u2013\u20131103. NIPS\u20192008 480\nLappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinear\nindependentcomponentanalysisusingensemblelearning:Experimentsanddiscussion.\nInProc.ICA.Citeseer.493\nLarochelle,\u00a0H.\u00a0and\u00a0Bengi o,\u00a0Y.(2008).Classi\ufb01cation\u00a0usingdiscriminative\u00a0restricted\nBoltzmannmachines.In .,,,, ICML\u20192008244255530686716\nLarochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswitha\nthird-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems\n23,pages1243\u20131251. 367\n7 5 0", "BIBLIOGRAPHY\nLarochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator.\nInAISTATS\u20192011.,,705708709\nLarochelle,H.,Erhan,D.,andBengio,Y.(2008).\u00a0Zero-datalearningofnewtasks.In\nAAAIConferenceonArti\ufb01cialIntelligence.539\nLarochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfor\ntrainingdeepneuralnetworks.JournalofMachineLearningResearch,,1\u201340. 1 0535\nLasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeand\ndiscriminativemodels.InProceedingsoftheComputerVisionandPatternRecognition\nConference(CVPR\u201906),pages87\u201394,Washington,DC,USA.IEEEComputerSociety.\n244253,\nLe,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiled\nconvolutionalneuralnetworks.InJ.La\ufb00erty,C.K.I.Williams,J.Shawe-Taylor,\nR.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems\n23(NIPS\u201910),pages1279\u20131287. 352\nLe,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimization\nmethodsfordeeplearning.InProc.ICML\u20192011.ACM.316\nLe,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,\nA.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.In\nICML\u20192012.,2427\nLeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmann\nmachinesanddeepbeliefnetworks.NeuralComputation,(6),1631\u20131649. , 2 0 553655\nLeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-\nmators.NeuralComputation,(8),2192\u20132207. 2 2 553\nLeCun,Y.(1985).Uneproc\u00e9dured\u2019apprentissagepourR\u00e9seau\u00e0seuilassym\u00e9trique.In\nCognitiva85:AlaFronti\u00e8redel\u2019IntelligenceArti\ufb01cielle,desSciencesdelaConnaissance\netdesNeurosciences,pages599\u2013604,Paris1985.CESTA,Paris.225\nLeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-\nSouli\u00e9,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiological\nOrganization,pages233\u2013240.Springer-Verlag,LesHouches,France.352\nLeCun,Y.(1987).Mod\u00e8lesconnexionistesdel\u2019apprentissage.Ph.D.thesis,Universit\u00e9de\nParisVI.,,18502515\nLeCun,\u00a0Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReport\nCRG-TR-89-4,UniversityofToronto.,330352\n7 5 1", "BIBLIOGRAPHY\nLeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,\nHoward,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applications\nofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,\n2 7(11),41\u201346.368\nLeCun,Y.,Bottou,L.,Orr,G.B.,andM\u00fcller,K.-R.(1998a).E\ufb03cientbackprop.In\nNeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524.\nSpringerVerlag.,310429\nLeCun,Y.,Bottou,L.,Bengio,Y.,andHa\ufb00ner,P.(1998b).Gradientbasedlearning\nappliedtodocumentrecognition.Proc.IEEE.,,,,,, 16182127371458460\nLeCun,\u00a0Y.,\u00a0Kavukcuoglu,\u00a0K.,\u00a0andFarabet,\u00a0C.(2010).Convolutionalnetworksand\napplicationsinvision.\u00a0InCircuitsandSystems(ISCAS),Proceedingsof2010IEEE\nInternationalSymposiumon,pages253\u2013256.IEEE.371\nL\u2019Ecuyer,P.(1994).E\ufb03ciencyimprovementandvariancereduction.InProceedingsof\nthe1994WinterSimulationConference,pages122\u2013\u2013132.690\nLee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets.\narXivpreprintarXiv:1409.5185.326\nLee,H.,Battle,A.,Raina,R.,andNg,A.(2007).E\ufb03cientsparsecodingalgorithms.\nInB.Sch\u00f6lkopf,J.Platt,andT.Ho\ufb00man,editors,AdvancesinNeuralInformation\nProcessingSystems19(NIPS\u201906),pages801\u2013808.MITPress.637\nLee,H.,Ekanadham,C.,andNg,A.(2008).Sparsedeepbeliefnetmodelforvisualarea\nV2.In.NIPS\u201907255\nLee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbelief\nnetworksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.Bottou\nandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceon\nMachineLearning(ICML\u201909).ACM,Montreal,Canada.,,363683684\nLee,Y.J.andGrauman,K.(2011).Learningtheeasythings\ufb01rst:self-pacedvisual\ncategorydiscovery.In . CVPR\u20192011328\nLeibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,\n2010).225\nLenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-\ntionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc.\n2\nLeshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforward\nnetworkswithanonpolynomialactivationfunctioncanapproximateanyfunction.\nNeuralNetworks,,861\u2013\u2013867. , 6 198199\n7 5 2", "BIBLIOGRAPHY\nLevenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleast\nsquares.QuarterlyJournalofAppliedMathematics,(2),164\u2013168. I I 312\nL\u2019H\u00f4pital,G.F.A.(1696).Analysedesin\ufb01nimentpetits,pourl\u2019intelligencedeslignes\ncourbes.Paris:L\u2019ImprimerieRoyale.225\nLi,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks.\nCoRR, . a b s/ 1 5 0 2 .0 2 7 6 1703\nLin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependencies\nisnotasdi\ufb03cultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeural\nNetworks,(6),1329\u20131338. 7 407\nLin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelation\nembeddingsforknowledgegraphcompletion.InProc.AAAI\u201915.484\nLinde,N.(1992).Themachinethatchangedtheworld,episode3.Documentaryminiseries.\n2\nLindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser\u2019s\nperspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHigh\nEnergyPhysics,pages195\u2013\u2013202,Isolad\u2019Elba,Italy.451\nLinnainmaa,\u00a0S.\u00a0(1976).Taylorexpansionofthe\u00a0accumulated\u00a0roundingerror.BIT\nNumericalMathematics,(2),146\u2013160. 1 6 225\nLISA(2008).Deeplearningtutorials:RestrictedBoltzmannmachines.Technicalreport,\nLISALab,Universit\u00e9deMontr\u00e9al.589\nLong,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardto\napproximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConference\nonMachineLearning(ICML\u201910).658\nLotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructure\nusingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,544545\nLovelace,A.(1842).NotesuponL.F.Menabrea\u2019s\u201cSketchoftheAnalyticalEngine\ninventedbyCharlesBabbage\u201d.1\nLu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetwork\nencoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.461\nLu,T.,P\u00e1l,D.,andP\u00e1l,M.(2010).Contextualmulti-armedbandits.InInternational\nConferenceonArti\ufb01cialIntelligenceandStatistics,pages485\u2013492.480\nLuenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.316\nLuko\u0161evi\u010dius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrent\nneuralnetworktraining.ComputerScienceReview,(3),127\u2013149. 3 404\n7 5 3", "BIBLIOGRAPHY\nLuo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesand\nclass-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceon\nArti\ufb01cialIntelligenceandStatistics,pages470\u2013478.686\nLuo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwith\nconvolutionalspike-and-slabRBMsanddeepextensions.InAISTATS\u20192013.102\nLyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsofthe\nTwenty-\ufb01fthConferenceinUncertaintyinArti\ufb01cialIntelligence(UAI\u201909).618\nMa,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnets\nasamethodforquantitativestructure\u2013activityrelationships.J.Chemicalinformation\nandmodeling.530\nMaas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Recti\ufb01ernonlinearitiesimproveneural\nnetworkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,and\nLanguageProcessing.193\nMaass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalog\nneuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,\npages335\u2013344.199\nMaass,W.,Schnitger,G.,andSontag,E.D.(1994).Acomparisonofthecomputational\npowerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeural\nComputationandLearning,pages127\u2013151.199\nMaass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithout\nstablestates:Anewframeworkforneuralcomputationbasedonperturbations.Neural\nComputation,(11),2531\u20132560. 1 4 404\nMacKay,D.(2003).\u00a0InformationTheory,InferenceandLearningAlgorithms.Cambridge\nUniversityPress.73\nMaclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameter\noptimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.435\nMao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioning\nwithmultimodalrecurrentneuralnetworks.In .arXiv:1410.1090. ICLR\u20192015 102\nMarcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem.\nZeitschriftf\u00fcrOperationsResearch(Theory),,517\u2013545. 3 6 276\nMarlin,B.anddeFreitas,N.(2011).Asymptotice\ufb03ciencyofdeterministicestimatorsfor\ndiscreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In ., UAI\u20192011617\n619\n7 5 4", "BIBLIOGRAPHY\nMarlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).Inductiveprinciplesfor\nrestrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternational\nConferenceonArti\ufb01cialIntelligenceandStatistics(AISTATS\u201910),volume9,pages\n509\u2013516. ,,613618619\nMarquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-\neters.JournaloftheSocietyofIndustrialandAppliedMathematics, 1 1(2),431\u2013441.\n312\nMarr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,\n1 9 4.367\nMartens,\u00a0J.(2010).DeeplearningviaHessian-freeoptimization.InL.Bottouand\nM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceon\nMachineLearning(ICML-10),pages735\u2013742.ACM.304\nMartens,J.andMedabalimi,V.(2014).Ontheexpressivee\ufb03ciencyofsumproduct\nnetworks. . arXiv:1411.7717554\nMartens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-free\noptimization.InProc.ICML\u20192011.ACM.413\nMase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuous\nstatespaceGibbsianprocesses.TheAnnalsofAppliedProbability, 5(3),pp.603\u2013612.\n616\nMcClelland, J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributed\nprocessing.InComputation&intelligence,pages305\u2013341.AmericanAssociationfor\nArti\ufb01cialIntelligence.17\nMcCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervous\nactivity.BulletinofMathematicalBiophysics,,115\u2013133. , 5 1415\nMead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80.\nSpringerScience&BusinessMedia.451\nMelchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmann\nmachines.arXivpreprintarXiv:1311.1354.674\nMemisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations.\nInProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR\u201907).\n686\nMemisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformations\nwithfactoredhigher-orderBoltzmannmachines.NeuralComputation, 2 2(6),1473\u20131492.\n686\n7 5 5", "BIBLIOGRAPHY\nMesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,\nMuller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,\nJ.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.In\nJMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,201532538\nMesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Sur\ufb01ngonthe\nmanifold.LearningWorkshop,Snowbird.711\nMiikkulainen,R.andDyer,M.G.(1991).Naturallanguageprocessingwithmodular\nPDPnetworksanddistributedlexicon.CognitiveScience,,343\u2013399. 1 5 477\nMikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,\nBrnoUniversityofTechnology.414\nMikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empirical\nevaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-\nnualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH\n2011).472\nMikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfor\ntraininglargescaleneuralnetworklanguagemodels.InProc.ASRU\u20192011.,328472\nMikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).E\ufb03cientestimationofwordrep-\nresentationsinvectorspace.InInternationalConferenceonLearningRepresentations:\nWorkshopsTrack.536\nMikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguages\nformachinetranslation.Technicalreport,arXiv:1309.4168.539\nMinka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridge\nUKTechRepMSRTR2005173,(TR-2005-173). 7 2 625\nMinsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.15\nMirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprint\narXiv:1411.1784.702\nMishkin,D.and\u00a0Matas,J.(2015).Allyouneedisagoodinit.arXivpreprint\narXiv:1511.06422.305\nMisra,J.andSaha,I.(2010).\u00a0Arti\ufb01cialneuralnetworksinhardware:Asurveyoftwo\ndecadesofprogress.Neurocomputing,(1),239\u2013255. 7 4 451\nMitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.99\nMiyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributional\nsmoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507.00677. ICLR 269\n7 5 6", "BIBLIOGRAPHY\nMnih,A.andGregor,\u00a0K.(2014).Neuralvariationalinferenceandlearninginbelief\nnetworks.In .,, ICML\u20192014691692693\nMnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguage\nmodelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternational\nConferenceonMachineLearning(ICML\u201907),pages641\u2013648.ACM.465\nMnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.\nInD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeural\nInformationProcessingSystems21(NIPS\u201908),pages1081\u20131088. 467\nMnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingse\ufb03cientlywithnoise-\ncontrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,and\nK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages\n2265\u20132273. CurranAssociates,Inc.,472622\nMnih,\u00a0A.andTeh,\u00a0Y.\u00a0W.(2012).Afastandsimple\u00a0algorithmfortrainingneural\nprobabilisticlanguagemodels.In ,pages1751\u20131758. ICML\u20192012 472\nMnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages.\nInProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).102\nMnih,V.,Larochelle,H.,\u00a0andHinton,G.(2011).ConditionalrestrictedBoltzmann\nmachinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArti\ufb01cial\nIntelligence(UAI).685\nMnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013).\nPlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312.5602.106\nMnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisual\nattention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,\neditors, ,pages2204\u20132212. NIPS\u20192014 691\nMnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,\nA.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,\nAntonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015).\nHuman-levelcontrolthroughdeepreinforcementlearning.Nature,,529\u2013533. 5 1 8 25\nMobahi,H.andFisher,\u00a0III,J.W.(2015).Atheoreticalanalysisofoptimizationby\nGaussiancontinuation.In . AAAI\u20192015327\nMobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherence\ninvideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternational\nConferenceonMachineLearning,pages737\u2013744,Montreal.Omnipress.494\nMohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition.\n459\n7 5 7", "BIBLIOGRAPHY\nMohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,\nM.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.In\nAcoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConference\non,pages5060\u20135063. IEEE.459\nMohamed,A.,Dahl,G.,andHinton,G.(2012a).\u00a0Acousticmodelingusingdeepbelief\nnetworks.IEEETrans.onAudio,SpeechandLanguageProcessing, 2 0(1),14\u201322.459\nMohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworks\nperformacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),\n2012IEEEInternationalConferenceon,pages4273\u20134276. IEEE.459\nMoller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning.\nNeuralNetworks,,525\u2013533. 6 316\nMontavon,G.andMuller,K.-R.(2012).\u00a0DeepBoltzmannmachinesandthecentering\ntrick.InG.Montavon,G.Orr,andK.-R.M\u00fcller,editors,NeuralNetworks:Tricksof\ntheTrade,volume7700ofLectureNotesinComputerScience,pages621\u2013637.Preprint:\nhttp://arxiv.org/abs/1203.3783.673\nMont\u00fafar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworks\nwithdiscreteunits.NeuralComputation,. 2 6553\nMont\u00fafar,G.andAy,N.(2011).Re\ufb01nementsofuniversalapproximationresultsfor\ndeepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation, 2 3(5),\n1306\u20131319. 553\nMontufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinear\nregionsofdeepneuralnetworks.In .,, NIPS\u2019201419199200\nMor-Yosef,S.,Samuelo\ufb00,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Ranking\ntheriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.Obstet\nGynecol,(6),944\u20137. 7 5 3\nMorin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguage\nmodel.InAISTATS\u20192005.,467469\nMozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.Hanson\nandR.Lippmann,\u00a0editors,\u00a0AdvancesinNeural\u00a0InformationProcessingSystems4\n(NIPS\u201991),pages275\u2013282,SanMateo,CA.MorganKaufmann.,407408\nMurphy,K.\u00a0P.(2012).MachineLearning:a\u00a0Probabilistic\u00a0Perspective.MIT\u00a0Press,\nCambridge,MA,USA.,,6298146\nMurray,B.U.I.andLarochelle,H.(2014).Adeepandtractabledensityestimator.In\nICML\u20192014.,190710\nNair,V.andHinton,G.(2010).Recti\ufb01edlinearunitsimproverestrictedBoltzmann\nmachines.In .,, ICML\u2019201016174197\n7 5 8", "BIBLIOGRAPHY\nNair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,\nD.Schuurmans,J.D.La\ufb00erty,C.K.I.Williams,andA.Culotta,editors,Advancesin\nNeuralInformationProcessingSystems22,pages1339\u20131347. CurranAssociates,Inc.\n686\nNarayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis.\nIn . NIPS\u20192010164\nNaumann,U.(2008).OptimalJacobianaccumulationisNP-complete.Mathematical\nProgramming,(2),427\u2013441. 1 1 2 222\nNavigli,R.andVelardi,P.(2005).\u00a0Structuralsemanticinterconnections:aknowledge-\nbasedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisand\nMachineIntelligence,(7),1075\u2013\u20131086. 2 7 485\nNeal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjusti\ufb01esincremental,\nsparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MIT\nPress,Cambridge,MA.634\nNeal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.692\nNeal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods.\nTechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.680\nNeal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions.\nTechnicalReport9421,Dept.ofStatistics,UniversityofToronto.603\nNeal,R.M.(1996).Bayesian LearningforNeuralNetworks.LectureNotesinStatistics.\nSpringer.265\nNeal,R.M.(2001).Annealedimportancesampling. , StatisticsandComputing 1 1(2),\n125\u2013139. ,,625627628\nNeal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportance\nsampling.629\nNesterov,Y.(1983).Amethodofsolvingaconvexprogrammingproblemwithconvergence\nrate O /k ( 12). ,,372\u2013376. SovietMathematicsDoklady 2 7 300\nNesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Applied\noptimization.KluwerAcademicPubl.,Boston,Dordrecht,London.300\nNetzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Reading\ndigits\u00a0in\u00a0naturalimages\u00a0withunsupervised\u00a0feature\u00a0learning.Deep\u00a0Learning\u00a0and\nUnsupervisedFeatureLearningWorkshop,NIPS.21\nNey,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatistical\nlanguagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology\n(Eurospeech),pages973\u2013976,Berlin.463\n7 5 9", "BIBLIOGRAPHY\nNg,A.(2015). Adviceforapplyingmachinelearning.\nhttps://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.421\nNiesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-\nspeechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition.\nInInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),\npages177\u2013180.463\nNing,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005).\nTowardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,\nIEEETransactionson,(9),1360\u20131371. 1 4 360\nNocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9296\nNorouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.In\nICML\u20192011.525\nNowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociative\nmixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.450\nNowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing.\nNeuralComputation,(4),473\u2013493. 4 139\nOlshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?Neural\nComputation,,1665\u20131699. 1 7 16\nOlshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptive\ufb01eldproperties\nbylearningasparsecodefornaturalimages.\u00a0Nature, 3 8 1,607\u2013609. ,,, 147255370496\nOlshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiological\nmodelofvisualattentionandinvariantpatternrecognitionbasedondynamicrouting\nofinformation.J.Neurosci.,(11),4700\u20134719. 1 3 450\nOpper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited.\nNeuralcomputation,(3),786\u2013792. 2 1 689\nOquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-level\nimagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionand\nPatternRecognition(CVPR),2014IEEEConferenceon,pages1717\u20131724. IEEE.536\nOsindero,S.andHinton,G.E.(2008).Modelingimagepatcheswithadirectedhierarchy\nofMarkovrandom\ufb01elds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\nAdvancesinNeuralInformationProcessingSystems20(NIPS\u201907),pages1121\u20131128,\nCambridge,MA.MITPress.632\nOvidandMartin,C.(2004). .W.W.Norton. Metamorphoses 1\n7 6 0", "BIBLIOGRAPHY\nPaccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconcepts\nandrelationsfrompositiveandnegativepropositions.InInternationalJointConference\nonNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.484\nPaine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervised\npre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.532\nPalatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shot\nlearningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.La\ufb00erty,\nC.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessing\nSystems22,pages1410\u20131418. CurranAssociates,Inc.539\nParker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.Research\ninEconomicsandManagementSci.,MIT.225\nPascanu,R.,Mikolov,T.,andBengio,Y.(2013).Onthedi\ufb03cultyoftrainingrecurrent\nneuralnetworks.In .,,,,, ICML\u20192013289402403408414416\nPascanu,R.,G\u00fcl\u00e7ehre,\u00c7.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeep\nrecurrentneuralnetworks.In .,,,,, ICLR\u2019201419265398399410460\nPascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregions\nofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In . ICLR\u20192014550\nPati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:\nRecursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-\nceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,\npages40\u201344.255\nPearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidential\nreasoning.In\u00a0Proceedingsofthe7thConferenceofthe\u00a0CognitiveScience\u00a0Society,\nUniversityofCalifornia,Irvine,pages329\u2013334.563\nPearl,J.(1988).\u00a0ProbabilisticReasoninginIntelligentSystems:\u00a0NetworksofPlausible\nInference.MorganKaufmann.54\nPerron,O.(1907).Zurtheoriedermatrices.MathematischeAnnalen, 6 4(2),248\u2013263.597\nPetersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003.31\nPeterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner\u2019sdiscoveryofshaping.\nJournaloftheExperimentalAnalysisofBehavior,(3),317\u2013328. 8 2 328\nPham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependent\nsourcesthroughamaximumlikelihoodapproach.In ,pages771\u2013774. EUSIPCO 491\n7 6 1", "BIBLIOGRAPHY\nPham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012).\nNeuFlow:data\ufb02owvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-\nCAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044\u20131047. IEEE.\n451\nPinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksfor\nscenelabeling.In . ICML\u20192014359\nPinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwith\nconvolutionalnetworks.InConferenceonComputerVisionandPatternRecognition\n(CVPR).359\nPinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognition\nhard?PLoSComputBiol,. 4456\nPinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologically-inspired\ncomputervision:Acasestudyinunconstrainedfacerecognition onfacebook.In\nComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputer\nSocietyConferenceon,pages35\u201342.IEEE.363\nPollack,J.B.(1990).Recursivedistributedrepresentations.Arti\ufb01cialIntelligence, 4 6(1),\n77\u2013105.401\nPolyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging.\nSIAMJ.ControlandOptimization,,838\u2013855. 3 0 ( 4 ) 322\nPolyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods.\nUSSRComputationalMathematicsandMathematicalPhysics,(5),1\u201317. 4 296\nPoole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014).\u00a0Analyzingnoiseinautoencoders\nanddeepnetworks., . CoRR a b s/ 1 4 0 6 .1 8 3 1241\nPoon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.In\nProceedingsoftheTwenty-seventhConferenceinUncertaintyinArti\ufb01cialIntelligence\n(UAI),Barcelona,Spain.554\nPresley,R.K.andHaggard,R.L.(1994).A\ufb01xedpointimplementationofthebackpropa-\ngationlearningalgorithm.InSoutheastcon\u201994.CreativeTechnologyTransfer-AGlobal\nA\ufb00air.,Proceedingsofthe1994IEEE,pages136\u2013138.IEEE.451\nPrice,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEE\nTransactionsonInformationTheory,(2),69\u201372. 4 689\nQuiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisual\nrepresentationbysingleneuronsinthehumanbrain.Nature, 4 3 5(7045),1102\u20131107.\n366\n7 6 2", "BIBLIOGRAPHY\nRadford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwith\ndeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434.\n552701702,,\nRaiko,T.,Yao,L.,Cho,K.,andBengio,\u00a0Y.(2014).Iterativeneuralautoregressive\ndistributionestimator(NADE-k).Technicalreport,arXiv:1406.1485.,676709\nRaina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearning\nusinggraphicsprocessors.\u00a0InL.BottouandM.Littman,editors,Proceedingsofthe\nTwenty-sixthInternationalConferenceonMachineLearning(ICML\u201909),pages873\u2013880,\nNewYork,NY,USA.ACM.,27446\nRamsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundations\nofMathematicsandotherLogicalEssays,chapter7,pages156\u2013198.McMasterUniversity\nArchivefortheHistoryofEconomicThought.56\nRanzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusing\nfactorizedthird-orderBoltzmannmachines.In ,pages2551\u20132558. CVPR\u20192010 680\nRanzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).E\ufb03cientlearningofsparse\nrepresentationswithanenergy-basedmodel.In .,,,, NIPS\u201920061419507528530\nRanzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningof\ninvariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsof\ntheComputerVisionandPatternRecognitionConference(CVPR\u201907).IEEEPress.364\nRanzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbelief\nnetworks.In . NIPS\u20192007507\nRanzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestricted\nBoltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010.\n678679,\nRanzato,M.,Mnih,V.,andHinton,G.(2010b).Generatingmorerealisticimagesusing\ngatedMRFs.In . NIPS\u20192010680\nRao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatistical\nparameters.BulletinoftheCalcuttaMathematicalSociety,,81\u201389., 3 7135295\nRasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervised\nlearningwithladdernetwork.arXivpreprintarXiv:1507.02672.,426530\nRecht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachto\nparallelizingstochasticgradientdescent.In . NIPS\u20192011447\nReichert,D.P.,Seri\u00e8s,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-\nbasedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformation\nProcessingSystems,pages2357\u20132365. 666\n7 6 3", "BIBLIOGRAPHY\nRezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagation\nand\u00a0approximateinferencein\u00a0deepgenerative\u00a0models.In\u00a0 .Preprint: ICML\u20192014\narXiv:1401.4082.,,652689696\nRifai,\u00a0S.,\u00a0Vincent,\u00a0P.,\u00a0Muller,X.,\u00a0Glorot,\u00a0X.,andBengio,Y.(2011a).Contractive\nauto-encoders:Explicitinvarianceduringfeatureextraction.In .,, ICML\u20192011521522\n523\nRifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X.\n(2011b).Higherordercontractiveauto-encoder.In ., ECMLPKDD521522\nRifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c).\u00a0Themanifold\ntangentclassi\ufb01er.In .,, NIPS\u20192011271272523\nRifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessfor\nsamplingcontractiveauto-encoders.In . ICML\u20192012711\nRingach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.Cognitive\nScience,(2),147\u2013166. 2 8 368\nRoberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesand\npractice.CambridgeUniversityPress.493\nRobinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeech\nrecognitionsystem.ComputerSpeechandLanguage,(3),259\u2013274. , 5 27459\nRockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.93\nRomero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y.\n(2015).Fitnets:Hintsforthindeepnets.In . ICLR\u20192015,arXiv:1412.6550325\nRosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti.\nlinearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics, 8(1),\npp.181\u2013217.93\nRosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageand\norganizationinthebrain.PsychologicalReview,,386\u2013408. ,, 6 5 141527\nRosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1527\nRoweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinear\nembedding.Science,(5500)., 2 9 0164518\nRoweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.In\nT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformation\nProcessingSystems14(NIPS\u201901),Cambridge,MA.MITPress.489\nRubin,D.B.(1984).Bayesianlyjusti\ufb01ableandrelevantfrequencycalculationsfor etal.\ntheappliedstatistician. ,(4),1151\u20131172. TheAnnalsofStatistics 1 2 717\n7 6 4", "BIBLIOGRAPHY\nRumelhart,\u00a0D.,\u00a0Hinton,\u00a0G.,\u00a0andWilliams,\u00a0R. (1986a).Learningrepresentationsby\nback-propagatingerrors.Nature,,533\u2013536. ,,,,,,, 3 2 3 141823204225373476482\nRumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.(1986b).Learninginternalrepresen-\ntationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland, editors,Parallel\nDistributedProcessing,volume1,chapter8,pages318\u2013362.MITPress,Cambridge.,21\n27225,\nRumelhart,D.E.,McClelland, J.L.,andthePDPResearchGroup(1986c).Parallel\nDistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,\nCambridge.17\nRussakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\nA.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLarge\nScaleVisualRecognitionChallenge.21\nRussakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\nA.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognition etal.\nchallenge.arXivpreprintarXiv:1409.0575.28\nRussel,S.J.andNorvig,P.(2003).Arti\ufb01cialIntelligence:aModernApproach.Prentice\nHall.86\nRust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).Spatiotemporal\nelementsofmacaqueV1receptive\ufb01elds.Neuron,(6),945\u2013956. 4 6 367\nSainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-\ntionalneuralnetworksforLVCSR.In . ICASSP2013460\nSalakhutdinov,R.(2010).LearninginMarkovrandom\ufb01eldsusingtemperedtransitions.In\nY.Bengio,D.Schuurmans,C.Williams,J.La\ufb00erty,andA.Culotta,editors,Advances\ninNeuralInformationProcessingSystems22(NIPS\u201909).603\nSalakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsof\ntheInternationalConferenceonArti\ufb01cialIntelligenceandStatistics,volume5,pages\n448\u2013455. ,,,,,, 2427529663666671672\nSalakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalof\nApproximateReasoning.525\nSalakhutdinov,\u00a0R.\u00a0andHinton,\u00a0G.\u00a0E.(2007a).Learning\u00a0a\u00a0nonlinearembedding\u00a0by\npreservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternational\nConferenceonArti\ufb01cialIntelligenceandStatistics(AISTATS\u201907),SanJuan,Porto\nRico.Omnipress.527\nSalakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In . SIGIR\u20192007525\n7 6 5", "BIBLIOGRAPHY\nSalakhutdinov,R.andHinton,G.E.(2008).Usingdeepbeliefnetstolearncovariance\nkernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\nAdvancesinNeuralInformationProcessingSystems20(NIPS\u201907),pages1249\u20131256,\nCambridge,MA.MITPress.244\nSalakhutdinov,R.andLarochelle,H.(2010).E\ufb03cientlearningofdeepBoltzmannmachines.\nInProceedingsoftheThirteenthInternationalConferenceonArti\ufb01cialIntelligenceand\nStatistics(AISTATS2010),JMLRW&CP,volume9,pages693\u2013700.652\nSalakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In . NIPS\u20192008\n480\nSalakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbelief\nnetworks.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Proceedingsof\ntheTwenty-\ufb01fthInternationalConferenceonMachineLearning(ICML\u201908),volume25,\npages872\u2013879.ACM.,628662\nSalakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesfor\ncollaborative\ufb01ltering.In.ICML480\nSanger,\u00a0T.D.\u00a0(1994).Neuralnetworklearningcontrolofrobotmanipulatorsusing\ngraduallyincreasingtaskdi\ufb03culty.IEEETransactionsonRoboticsandAutomation,\n1 0(3).328\nSaul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractable\nnetworks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeural\nInformationProcessingSystems8(NIPS\u201995).MITPress,Cambridge,MA.638\nSaul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Mean\ufb01eldtheoryforsigmoidbelief\nnetworks.JournalofArti\ufb01cialIntelligenceResearch,,61\u201376., 427693\nSavich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentation\nonimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,\n1 8(1),240\u2013252.451\nSaxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandom\nweightsandunsupervisedfeaturelearning.InProc.ICML\u20192011.ACM.363\nSaxe,A.M.,McClelland, J.L.,andGanguli,S.(2013).Exactsolutionstothenonlinear\ndynamicsoflearningindeeplinearneuralnetworks.In.,, ICLR285286303\nSchaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization.\nInInternationalConferenceonLearningRepresentations.309\nSchmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleof\nhistorycompression.NeuralComputation,(2),234\u2013242. 4 398\nSchmidhuber,J.(1996).Sequentialneuraltextcompression.IEEETransactionsonNeural\nNetworks,(1),142\u2013146. 7 477\n7 6 6", "BIBLIOGRAPHY\nSchmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118.\n390\nSch\u00f6lkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,\nregularization,optimization,andbeyond.MITpress.704\nSch\u00f6lkopf,B.,Smola,A.,andM\u00fcller,K.-R.(1998).Nonlinearcomponentanalysisasa\nkerneleigenvalueproblem.NeuralComputation,,1299\u20131319. , 1 0 164518\nSch\u00f6lkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods\u2014\nSupportVectorLearning.MITPress,Cambridge,MA.,18142\nSch\u00f6lkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).On\ncausalandanticausallearning.In ,pages1255\u20131262. ICML\u20192012 545\nSchuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsfor\nspeechrecognition.190\nSchuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEE\nTransactionsonSignalProcessing,(11),2673\u20132681. 4 5 395\nSchwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,\n2 1,492\u2013518.466\nSchwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation.\nThePragueBulletinofMathematicalLinguistics,,137\u2013146. 9 3 473\nSchwenk,H.(2014).CleanedsubsetofWMT\u201914dataset.21\nSchwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-\nworks.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformation\nProcessingSystems10(NIPS\u201997),pages647\u2013653.MITPress.258\nSchwenk,\u00a0H.andGauvain,\u00a0J.-L.(2002).Connectionistlanguagemodeling\u00a0forlarge\nvocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,\nSpeechandSignalProcessing(ICASSP),pages765\u2013768,Orlando,Florida.466\nSchwenk,H.,Costa-juss\u00e0,M.R.,andFonollosa,J.A.R.(2006).Continuousspace\nlanguagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpoken\nLanguageTranslation,pages166\u2013173.473\nSeide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-\ndependentdeepneuralnetworks.InInterspeech2011,pages437\u2013440.23\nSejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings\n151onNeuralNetworksforComputing,pages398\u2013403.AmericanInstituteofPhysics\nInc.686\n7 6 7", "BIBLIOGRAPHY\nSeries,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnet\nsyndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesin\nNeuralInformationProcessingSystems,pages2020\u20132028. 666\nSermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksapplied\ntohousenumbersdigitclassi\ufb01cation., . CoRR a b s/ 1 2 0 4 .3 9 6 8457\nSermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetection\nwithunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceon\nComputerVisionandPatternRecognition(CVPR\u201913).IEEE.,23201\nShilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications.\n31\nSiegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science, 2 6 8(5210),\n545\u2013548.379\nSiegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.Applied\nMathematicsLetters,(6),77\u201380. 4 379\nSiegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets.\nJournalofComputerandSystemsSciences,(1),132\u2013150. , 5 0 379403\nSietsma,J.andDow,R.(1991).Creatingarti\ufb01cialneuralnetworksthatgeneralize.Neural\nNetworks,(1),67\u201379. 4 241\nSimard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutional\nneuralnetworks.In . ICDAR\u20192003371\nSimard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvances\ninNeuralInformationProcessingSystems,pages232\u2013239.451\nSimard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalism\nforspecifyingselectedinvariancesinanadaptivenetwork.In .,,, NIPS\u20191991270271272\n356\nSimard,P.Y.,LeCun,Y.,andDenker,J.(1993).E\ufb03cientpatternrecognitionusinga\nnewtransformationdistance.In.NIPS\u201992270\nSimard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformation\ninvarianceinpatternrecognition\u2014tangentdistanceandtangentpropagation.Lecture\nNotesinComputerScience,. 1 5 2 4270\nSimons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringa\nreal-worldinteraction.PsychonomicBulletin&Review,(4),644\u2013649. 5 543\nSimonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scale\nimagerecognition.In.ICLR323\n7 6 8", "BIBLIOGRAPHY\nSj\u00f6berg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,\nwithapplicationtoneuralnetworks.InternationalJournalofControl, 6 2(6),1391\u20131407.\n250\nSkinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94\u201399. 1 3328\nSmolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsof\nharmonytheory.InD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributed\nProcessing,volume1,chapter6,pages194\u2013281.MITPress,Cambridge.,,571587656\nSnoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationof\nmachinelearningalgorithms.In . NIPS\u20192012436\nSocher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamic\npoolingandunfoldingrecursiveautoencodersforparaphrasedetection.In . NIPS\u20192011\n401\nSocher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-\nguagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternational\nConferenceonMachineLearning(ICML\u20192011).401\nSocher,\u00a0R.,\u00a0Pennington,\u00a0J.,\u00a0Huang,\u00a0E.H.,\u00a0Ng,\u00a0A. Y.,andManning,\u00a0C.D.(2011c).\nSemi-supervisedrecursiveautoencoders\u00a0forpredictingsentimentdistributions.In\nEMNLP\u20192011.401\nSocher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,\nC.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentiment\ntreebank.In . EMNLP\u20192013401\nSocher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthrough\ncross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessing\nSystems(NIPS2013).539\nSohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deep\nunsupervisedlearningusingnonequilibriumthermodynamics.716\nSohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywith\npoint-wisegatedBoltzmannmachines.In . ICML\u20192013687\nSolomono\ufb00,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-\nbility.328\nSontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputer\nandSystemsSciences,,69\u201396., 1 6 8547551\nSontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocal\nminimaevenfornetworkswithouthiddenlayers. ,,91\u2013106. ComplexSystems 3 284\n7 6 9", "BIBLIOGRAPHY\nSparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1\nSpitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how\n\u201clessismore\u201dinunsuperviseddependencyparsing.InHLT\u201910.328\nSquire,W.andTrapp,G.(1998).\u00a0Usingcomplexvariablestoestimatederivativesofreal\nfunctions.SIAMRev.,(1),110\u2013\u2013112. 4 0 439\nSrebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsof\nthe18thAnnualConferenceonLearningTheory,pages545\u2013560.Springer-Verlag.238\nSrivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master\u2019sthesis,U.\nToronto.535\nSrivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmann\nmachines.In . NIPS\u20192012541\nSrivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).Modelingdocumentswith\ndeepBoltzmannmachines.arXivpreprintarXiv:1309.6865.663\nSrivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014).\nDropout:Asimplewaytopreventneuralnetworksfromover\ufb01tting.JournalofMachine\nLearningResearch,,1929\u20131958. ,,, 1 5 258265267672\nSrivastava,R.K.,Gre\ufb00,K.,andSchmidhuber,J.(2015).Highwaynetworks.\narXiv:1505.00387.326\nSteinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearning\nalgorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,\n0,1115\u20131119. 445\nStoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphical\nmodelparametersgivenapproximateinference,decoding,andmodelstructure.In\nProceedingsofthe14thInternationalConferenceonArti\ufb01cialIntelligenceandStatistics\n(AISTATS) JMLRWorkshopandConferenceProceedings ,volume15of ,pages725\u2013733,\nFortLauderdale.Supplementarymaterial(4pages)alsoavailable.,674698\nSukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemory\nnetworks.arXivpreprintarXiv:1503.08895.418\nSupancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.In\nCVPR\u20192013.328\nSussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworks\nwithsmartinitialization., .,,, CoRR a b s/ 1 4 1 2 .6 5 5 8290303305403\nSutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentof\ncomputerscience,UniversityofToronto.,406413\n7 7 0", "BIBLIOGRAPHY\nSutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversal\napproximators.NeuralComputation,(11),2629\u20132636. 2 0 693\nSutskever,I.andTieleman,T.(2010).OntheConvergencePropertiesofContrastive\nDivergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternational\nConferenceonArti\ufb01cialIntelligenceandStatistics(AISTATS),volume9,pages789\u2013795.\n612\nSutskever,I.,Hinton,G.,andTaylor,G.(2009).Therecurrenttemporalrestricted\nBoltzmannmachine.In . NIPS\u20192008685\nSutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrent\nneuralnetworks.In ,pages1017\u20131024. ICML\u20192011 477\nSutskever,\u00a0I.,Martens,J.,Dahl,\u00a0G.,andHinton,G.(2013).Ontheimportanceof\ninitializationandmomentumindeeplearning.In.,, ICML300406413\nSutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwith\nneuralnetworks.In .,,,,,, NIPS\u20192014,arXiv:1409.321525101397410411474475\nSutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress.\n106\nSutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethods\nforreinforcementlearningwithfunctionapproximation.In ,pages1057\u2013 NIPS\u20191999\n\u20131063.MITPress.691\nSwersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).On\nautoencodersandscorematchingforenergybasedmodels.In .ACM. ICML\u20192011513\nSwersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization.\narXivpreprintarXiv:1406.3896.436\nSzegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,\nV.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,\narXiv:1409.4842.,,,,,, 2427201258269326347\nSzegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,and\nFergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLR a b s/ 1 3 1 2 .6 1 9 9.\n268271,\nSzegedy,C.,Vanhoucke,V.,Io\ufb00e,S.,Shlens,J.,andWojna,Z.(2015).Rethinkingthe\nInceptionArchitectureforComputerVision. ., ArXive-prints243322\nTaigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegapto\nhuman-levelperformanceinfaceveri\ufb01cation.In . CVPR\u20192014100\nTandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocial\nSciences.UniversityofCaliforniaPress.1\n7 7 1", "BIBLIOGRAPHY\nTang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.In\nProceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,\n2010,Haifa,Israel.241\nTang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers.\narXivpreprintarXiv:1206.4635.489\nTaylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachines\nformodelingmotionstyle.InL.BottouandM.Littman,\u00a0editors,Proceedingsof\ntheTwenty-sixthInternationalConferenceonMachineLearning(ICML\u201909),pages\n1025\u20131032, Montreal,Quebec,Canada.ACM.685\nTaylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinary\nlatentvariables.InB.Sch\u00f6lkopf,J.Platt,andT.Ho\ufb00man,editors,AdvancesinNeural\nInformationProcessingSystems19(NIPS\u201906),pages1345\u20131352. MITPress,Cambridge,\nMA.685\nTeh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodels\nforsparseovercompleterepresentations.JournalofMachineLearningResearch, 4,\n1235\u20131260. 491\nTenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframework\nfornonlineardimensionalityreduction.Science,(5500),2319\u20132323. ,, 2 9 0 164518533\nTheis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerative\nmodels.arXiv:1511.01844.,698719\nThompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutional\nnetworkandagraphicalmodelforhumanposeestimation.In . NIPS\u20192014360\nThrun,S.(1995).Learningtoplaythegameofchess.In . NIPS\u20191994271\nTibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.Journalofthe\nRoyalStatisticalSocietyB,,267\u2013288. 5 8 236\nTieleman,T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationsto\nthelikelihoodgradient.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Pro-\nceedingsoftheTwenty-\ufb01fthInternationalConferenceonMachineLearning(ICML\u201908),\npages1064\u20131071. ACM.612\nTieleman,T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastive\ndivergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixth\nInternationalConferenceonMachineLearning(ICML\u201909),pages1033\u20131040. ACM.\n614\nTipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis.\nJournaloftheRoyalStatisticalSocietyB,(3),611\u2013622. 6 1 491\n7 7 2", "BIBLIOGRAPHY\nTorralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesfor\nrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference\n(CVPR\u201908),pages1\u20138.525\nTouretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsof\naconnectionistinferencearchitecture.\u00a0InProceedingsofthe9thInternationalJoint\nConferenceonArti\ufb01cialIntelligence-Volume1,IJCAI\u201985,pages238\u2013243,SanFrancisco,\nCA,USA.MorganKaufmannPublishersInc.17\nTu,K.andHonavar,V.(2011).\u00a0Ontheutilityofcurriculainunsupervisedlearningof\nprobabilisticgrammars.In . IJCAI\u20192011328\nTuraga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,\nW.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogeneratea\ufb03nity\ngraphsforimagesegmentation.NeuralComputation,(2),511\u2013538. 2 2 360\nTurian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleand\ngeneralmethodforsemi-supervisedlearning.InProc.ACL\u20192010,pages384\u2013394.535\nT\u00f6scher,A.,Jahrer,M.,andBell,R.M.(2009).\u00a0TheBigChaossolutiontotheNet\ufb02ix\ngrandprize.480\nUria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-\nsivedensity-estimator.In ., NIPS\u20192013709710\nvandenO\u00f6rd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusic\nrecommendation.In . NIPS\u20192013480\nvanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.Machine\nLearningRes.,., 9477519\nVanhoucke,V.,Senior,A.,andMao,M.Z.(2011).Improvingthespeedofneuralnetworks\nonCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop.\n444452,\nVapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-\nVerlag,Berlin.114\nVapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork.\n114\nVapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelative\nfrequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,\n1 6,264\u2013280.114\nVincent,P.(2011).\u00a0Aconnectionbetweenscorematchinganddenoisingautoencoders.\nNeuralComputation,(7).,, 2 3513515712\n7 7 3", "BIBLIOGRAPHY\nVincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In .MITPress. NIPS\u20192002\n520\nVincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingand\ncomposingrobustfeatureswithdenoisingautoencoders.In ., ICML2008241515\nVincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stacked\ndenoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocal\ndenoisingcriterion.J.MachineLearningRes.,. 1 1515\nVincent,P.,deBr\u00e9bisson,A.,andBouthillier,X.(2015).E\ufb03cientexactgradientupdate\nfortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,\nD.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformation\nProcessingSystems28,pages1108\u20131116. CurranAssociates,Inc.466\nVinyals,\u00a0O.,\u00a0Kaiser,\u00a0L.,\u00a0Koo,\u00a0T.,\u00a0Petrov,\u00a0S.,\u00a0Sutskever,\u00a0I.,\u00a0andHinton,\u00a0G.(2014a).\nGrammarasaforeignlanguage.Technicalreport,arXiv:1412.7449.410\nVinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimage\ncaptiongenerator.arXiv1411.4555. 410\nVinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprint\narXiv:1506.03134.418\nVinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimage\ncaptiongenerator.In .arXiv:1411.4555. CVPR\u20192015 102\nViola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternational\nJournalofComputerVision.449\nVisin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015).\nReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXiv\npreprintarXiv:1505.00393.395\nVonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinal\nprojectionsdirectedtotheauditorypathway.Nature,(6780),871\u2013876. 4 0 4 16\nWager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization.\nInAdvancesinNeuralInformationProcessingSystems26,pages351\u2013359.265\nWaibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phoneme\nrecognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,\nandSignalProcessing,,328\u2013339. ,, 3 7 374453459\nWan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneural\nnetworksusingdropconnect.In . ICML\u20192013266\nWang,S.andManning,C.(2013).Fastdropouttraining.In . ICML\u20192013266\n7 7 4", "BIBLIOGRAPHY\nWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointly\nembedding.InProc.EMNLP\u20192014.484\nWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b).\u00a0Knowledgegraphembeddingby\ntranslatingonhyperplanes.InProc.AAAI\u20192014.484\nWarde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempirical\nanalysisofdropoutinpiecewiselinearnetworks.In .,, ICLR\u20192014262266267\nWawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N.\n(1996).Spert-II:Avectormicroprocessorsystem. ,(3),79\u201386. Computer 2 9 451\nWeaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-\nmentlearning.InProc.UAI\u20192001,pages538\u2013545.691\nWeinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsby\nsemide\ufb01niteprogramming.In ,pages988\u2013995. , CVPR\u20192004 164519\nWeiss,\u00a0Y.,\u00a0Torralba,\u00a0A.,\u00a0andFergus,\u00a0R.(2008).Spectral\u00a0hashing.In,pagesNIPS\n1753\u20131760. 525\nWelling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvances\ninNeuralInformationProcessingSystems,pages665\u2013672.703\nWelling,\u00a0M.,Hinton,G.E.,\u00a0andOsindero,\u00a0S.(2003a).Learningsparsetopographic\nrepresentationswithproductsofStudent-tdistributions.In . NIPS\u20192002680\nWelling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,\nS.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessing\nSystems15(NIPS\u201902),pages665\u2013672.MITPress.622\nWelling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniums\nwithanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,\neditors,AdvancesinNeuralInformationProcessingSystems17(NIPS\u201904),volume17,\nCambridge,MA.MITPress.676\nWerbos,\u00a0P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.In\nProceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762\u2013770.225\nWeston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningto\nrankwithjointword-imageembeddings.MachineLearning,(1),21\u201335. 8 1 401\nWeston,\u00a0J.,\u00a0Chopra,\u00a0S.,\u00a0andBordes,\u00a0A.(2014).Memorynetworks.arXivpreprint\narXiv:1410.3916.,418485\nWidrow,B.andHo\ufb00,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCON\nConventionRecord,volume4,pages96\u2013104.IRE,NewYork.,,,15212427\n7 7 5", "BIBLIOGRAPHY\nWikipedia(2015).Listofanimalsbynumberofneurons\u2014Wikipedia,thefreeencyclopedia.\n[Online;accessed4-March-2015].,2427\nWilliams,C.K.I.andAgakov,F.V.(2002).\u00a0ProductsofGaussiansandProbabilistic\nMinorComponentAnalysis.NeuralComputation,,1169\u20131182. 1 4 ( 5 ) 682\nWilliams,C.K.I.andRasmussen,C.E.(1996).\u00a0Gaussianprocessesforregression.\u00a0In\nD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformation\nProcessingSystems8(NIPS\u201995),pages514\u2013520.MITPress,Cambridge,MA.142\nWilliams,R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionist\nreinforcementlearning.MachineLearning,,229\u2013256. , 8 688689\nWilliams,R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfully\nrecurrentneuralnetworks.NeuralComputation,,270\u2013280. 1 223\nWilson,D.R.andMartinez,T.R.(2003).Thegeneraline\ufb03ciencyofbatchtrainingfor\ngradientdescentlearning.NeuralNetworks,(10),1429\u20131451. 1 6 279\nWilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.American\nJournalofMathematicalandManagementSciences,(3),277\u2013\u2013312. 4 690\nWiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningof\ninvariances.NeuralComputation,(4),715\u2013770. 1 4 494\nWolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEE\nTransactionsonEvolutionaryComputation,,67\u201382. 1293\nWolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.Neural\nComputation,(7),1341\u20131390. 8 116\nWu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimage\nrecognition.arXiv:1501.02876.447\nWu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalof\nOptimization,,814\u2013836. 7 327\nXiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulated\nsplicingusingRNAsequenceandcellularcontext. , Bioinformatics 2 7(18),2554\u20132562.\n265\nXu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,and\nBengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisual\nattention.In .,, ICML\u20192015,arXiv:1502.03044102410691\nYildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty.\nNeuralnetworks,,1\u20139. 3 5405\n7 7 6", "BIBLIOGRAPHY\nYosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeatures\nindeepneuralnetworks?In ., NIPS\u20192014325536\nYounes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidly\ndecreasingergodicityrates.InStochasticsandStochasticsModels,pages177\u2013228.612\nYu,D.,Wang,S.,and\u00a0Deng,\u00a0L.\u00a0(2010).Sequential\u00a0labeling\u00a0using\u00a0deep-structured\nconditionalrandom\ufb01elds.IEEEJournalofSelectedTopicsinSignalProcessing.323\nZaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615. 329\nZaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines.\narXiv:1505.00521.419\nZaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitions\nofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematical\nSociety.AmericanMathematicalSociety.550\nZeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks.\nIn . ECCV\u2019146\nZeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,\nA.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013).\u00a0Onrecti\ufb01edlinearunitsfor\nspeechprocessing.In . ICASSP2013460\nZhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015).\u00a0Objectdetectors\nemergeindeepsceneCNNs.ICLR\u20192015,arXiv:1412.6856.551\nZhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerative\nstochasticnetworkforproteinsecondarystructureprediction.In . ICML\u20192014715\nZhou,Y.andChellappa,R.(1988).Computationofoptical\ufb02owusinganeuralnetwork.\nInNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71\u201378.IEEE.339\nZ\u00f6hrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassi\ufb01cation.In\nNIPS\u20192014.716\n7 7 7", "I n d e x\n0-1loss,, 1 0 2274\nAbsolutevaluerecti\ufb01cation,191\nAccuracy,420\nActivationfunction,169\nActiveconstraint,94\nAdaGrad,305\nADALINE, s e eadaptivelinearelement\nAdam,,307422\nAdaptivelinearelement,,,152326\nAdversarialexample,265\nAdversarialtraining,,,266268526\nA\ufb03ne,109\nAIS, s e eannealedimportancesampling\nAlmosteverywhere,70\nAlmostsureconvergence,128\nAncestralsampling,,576591\nANN, s e eArti\ufb01cialneuralnetwork\nAnnealedimportancesampling,\u00a0,,621662\n711\nApproximateBayesiancomputation,710\nApproximateinference,579\nArti\ufb01cialintelligence,1\nArti\ufb01cialneuralnetwork,\u00a0 s e eNeuralnet-\nwork\nASR, s e eautomaticspeechrecognition\nAsymptoticallyunbiased,123\nAudio,,,101357455\nAutoencoder,,,4353 4 9 8\nAutomaticspeechrecognition,455\nBack-propagation,201\nBack-propagationthroughtime, 3 8 1\nBackprop, s e eback-propagationBagofwords,467\nBagging,252\nBatchnormalization,,264422\nBayeserror, 1 1 6\nBayes\u2019rule,69\nBayesianhyperparameteroptimization,433\nBayesian\u00a0network,\u00a0 s e edirected\u00a0graphical\nmodel\nBayesianprobability,54\nBayesianstatistics, 1 3 4\nBeliefnetwork, s e edirectedgraphicalmodel\nBernoullidistribution,61\nBFGS,314\nBias,,123227\nBiasparameter,109\nBiasedimportancesampling,589\nBigram,458\nBinaryrelation,478\nBlockGibbssampling,595\nBoltzmanndistribution,566\nBoltzmannmachine,,566648\nBPTT, s e eback-propagationthroughtime\nBroadcasting,33\nBurn-in,593\nCAE, s e econtractiveautoencoder\nCalculusofvariations,178\nCategoricaldistribution, s e emultinoullidis-\ntribution\nCD, s e econtrastivedivergence\nCenteringtrick(DBM),667\nCentrallimittheorem,63\nChainrule(calculus),203\nChainruleofprobability,58\n778", "INDEX\nChess,2\nChord,575\nChordalgraph,575\nClass-basedlanguagemodels,460\nClassicaldynamicalsystem,372\nClassi\ufb01cation,99\nCliquepotential, s e efactor(graphicalmodel)\nCNN, s e econvolutionalneuralnetwork\nCollaborativeFiltering,474\nCollider, s e eexplainingaway\nColorimages,357\nComplexcell,362\nComputationalgraph,202\nComputervision,449\nConceptdrift,533\nConditionnumber,277\nConditionalcomputation, s e edynamicstruc-\nture\nConditionalindependence,,xiii59\nConditionalprobability,58\nConditionalRBM,679\nConnectionism,,17440\nConnectionisttemporalclassi\ufb01cation,457\nConsistency,,128509\nConstrainedoptimization,,92235\nContent-basedaddressing,416\nContent-basedrecommendersystems,475\nContext-speci\ufb01cindependence,569\nContextualbandits,476\nContinuationmethods,324\nContractiveautoencoder,516\nContrast,451\nContrastivedivergence,,,289606666\nConvexoptimization,140\nConvolution,,327677\nConvolutionalnetwork,16\nConvolutionalneuralnetwork,,250 3 2 7,,422\n456\nCoordinatedescent,,319665\nCorrelation,60\nCostfunction, s e eobjectivefunction\nCovariance,,xiii60\nCovariancematrix,61\nCoverage,421Criticaltemperature,599\nCross-correlation,329\nCross-entropy,, 7 4131\nCross-validation,121\nCTC, s e econnectionisttemporalclassi\ufb01ca-\ntion\nCurriculumlearning,326\nCurseofdimensionality,153\nCyc,2\nD-separation,568\nDAE, s e edenoisingautoencoder\nDatageneratingdistribution,, 1 1 0130\nDatageneratingprocess,110\nDataparallelism,444\nDataset,103\nDatasetaugmentation,,268454\nDBM, s e edeepBoltzmannmachine\nDCGAN,,,547548695\nDecisiontree,, 1 4 4544\nDecoder,4\nDeepbeliefnetwork,,,,,, 26525626651654\n678686,\nDeepBlue,2\nDeepBoltzmannmachine,,,,, 2326525626\n647651657666678 ,,,,\nDeepfeedforwardnetwork,,166422\nDeeplearning,,25\nDenoisingautoencoder,,506683\nDenoisingscorematching,615\nDensityestimation,102\nDerivative,,xiii82\nDesignmatrix, 1 0 5\nDetectorlayer,336\nDeterminant,xii\nDiagonalmatrix,40\nDi\ufb00erentialentropy,,73641\nDiracdeltafunction,64\nDirectedgraphicalmodel,,,, 76503559685\nDirectionalderivative,84\nDiscriminative\ufb01ne-tuning, s e esupervised\n\ufb01ne-tuning\nDiscriminativeRBM,680\nDistributedrepresentation,,,17149542\nDomainadaptation,532\n7 7 9", "INDEX\nDotproduct,,33139\nDoublebackprop,268\nDoublyblockcirculantmatrix,330\nDreamsleep,,605647\nDropConnect,263\nDropout,,,,,, 2 5 5422427428666683\nDynamicstructure,445\nE-step,629\nEarlystopping,,,,, 244246270271422\nEBM, s e eenergy-basedmodel\nEchostatenetwork,,,2326401\nE\ufb00ectivecapacity,113\nEigendecomposition,41\nEigenvalue,41\nEigenvector,41\nELBO, s e eevidencelowerbound\nElement-wiseproduct, s e eHadamardprod-\nuct, s e eHadamardproduct\nEM, s e eexpectationmaximization\nEmbedding,512\nEmpiricaldistribution,65\nEmpiricalrisk,274\nEmpiricalriskminimization,274\nEncoder,4\nEnergyfunction,565\nEnergy-basedmodel,,,, 565591648657\nEnsemblemethods,252\nEpoch,244\nEqualityconstraint,93\nEquivariance,335\nErrorfunction, s e eobjectivefunction\nESN, s e eechostatenetwork\nEuclideannorm,38\nEuler-Lagrangeequation,641\nEvidencelowerbound,,628655\nExample,98\nExpectation,59\nExpectationmaximization,629\nExpectedvalue, s e eexpectation\nExplainingaway,,,570626639\nExploitation,477\nExploration,477\nExponentialdistribution, 6 4F-score,420\nFactor(graphicalmodel),563\nFactoranalysis,486\nFactorgraph,575\nFactorsofvariation,4\nFeature,98\nFeatureselection,234\nFeedforwardneuralnetwork,166\nFine-tuning,321\nFinitedi\ufb00erences,436\nForgetgate,304\nForwardpropagation,201\nFouriertransform,,357359\nFovea,363\nFPCD,610\nFreeenergy,, 5 6 7674\nFreebase,479\nFrequentistprobability,54\nFrequentiststatistics, 1 3 4\nFrobeniusnorm,45\nFully-visibleBayesnetwork,699\nFunctionalderivatives,640\nFVBN, s e efully-visibleBayesnetwork\nGaborfunction,365\nGANs, s e egenerativeadversarialnetworks\nGatedrecurrentunit,422\nGaussiandistribution, s e enormaldistribu-\ntion\nGaussiankernel,140\nGaussianmixture,,66187\nGCN, s e eglobalcontrastnormalization\nGeneOntology,479\nGeneralization,109\nGeneralizedLagrangefunction, s e egeneral-\nizedLagrangian\nGeneralizedLagrangian,93\nGenerativeadversarialnetworks,,683693\nGenerativemomentmatchingnetworks,696\nGeneratornetwork,687\nGibbsdistribution,564\nGibbssampling,,577595\nGlobalcontrastnormalization,451\nGPU, s e egraphicsprocessingunit\nGradient,83\n7 8 0", "INDEX\nGradientclipping,,287411\nGradientdescent,,8284\nGraph,xii\nGraphicalmodel, s e estructuredprobabilis-\nticmodel\nGraphicsprocessingunit,441\nGreedyalgorithm,321\nGreedylayer-wiseunsupervisedpretraining,\n524\nGreedysupervisedpretraining,321\nGridsearch,429\nHadamardproduct,,xii33\nHard,tanh195\nHarmonium, s e erestrictedBoltzmannma-\nchine\nHarmonytheory,567\nHelmholtz freeenergy, s e eevidencelower\nbound\nHessian,221\nHessianmatrix,,xiii86\nHeteroscedastic,186\nHiddenlayer,,6166\nHillclimbing,85\nHyperparameteroptimization,429\nHyperparameters,,119427\nHypothesisspace,,111117\ni.i.d.assumptions,,,110121265\nIdentitymatrix,35\nILSVRC, s e eImageNetLargeScaleVisual\nRecognitionChallenge\nImageNetLargeScaleVisualRecognition\nChallenge,22\nImmorality,573\nImportancesampling,,,588620691\nImportanceweightedautoencoder,691\nIndependence,,xiii59\nIndependentandidenticallydistributed, s e e\ni.i.d.assumptions\nIndependentcomponentanalysis,487\nIndependentsubspaceanalysis,489\nInequalityconstraint,93\nInference,,,,,,,, 558579626628630633643\n646Informationretrieval,520\nInitialization,298\nIntegral,xiii\nInvariance,339\nIsotropic,64\nJacobianmatrix,,,xiii7185\nJointprobability,56\nk-means,,361542\nk-nearestneighbors,, 1 4 1544\nKarush-Kuhn-Tuckerconditions,,94235\nKarush\u2013Kuhn\u2013Tucker,93\nKernel(convolution),,328329\nKernelmachine,544\nKerneltrick,139\nKKT, s e eKarush\u2013Kuhn\u2013Tucker\nKKTconditions, s e eKarush-Kuhn-Tucker\nconditions\nKLdivergence, s e eKullback-Leiblerdiver-\ngence\nKnowledgebase,,2479\nKrylovmethods,222\nKullback-Leiblerdivergence,,xiii 7 3\nLabelsmoothing,241\nLagrangemultipliers,,93641\nLagrangian, s e egeneralizedLagrangian\nLAPGAN,695\nLaplacedistribution,, 6 4492\nLatentvariable,66\nLayer(neuralnetwork),166\nLCN, s e elocalcontrastnormalization\nLeakyReLU,191\nLeakyunits,404\nLearningrate,84\nLinesearch,,,848592\nLinearcombination,36\nLineardependence,37\nLinearfactormodels,485\nLinearregression,,, 1 0 6109138\nLinkprediction,480\nLipschitzconstant,91\nLipschitzcontinuous,91\nLiquidstatemachine,401\n7 8 1", "INDEX\nLocalconditionalprobabilitydistribution,\n560\nLocalcontrastnormalization,452\nLogisticregression,,,3 1 3 8139\nLogisticsigmoid,,766\nLongshort-termmemory,,,,1824304 4 0 7,\n422\nLoop,575\nLoopybeliefpropagation,581\nLossfunction, s e eobjectivefunction\nLpnorm,38\nLSTM, s e elongshort-termmemory\nM-step,629\nMachinelearning,2\nMachinetranslation,100\nMaindiagonal,32\nManifold,159\nManifoldhypothesis,160\nManifoldlearning,160\nManifoldtangentclassi\ufb01er,268\nMAPapproximation,,137501\nMarginalprobability,57\nMarkovchain,591\nMarkovchainMonteCarlo,591\nMarkovnetwork, s e eundirectedmodel\nMarkovrandom\ufb01eld, s e eundirectedmodel\nMatrix,,,xixii31\nMatrixinverse,35\nMatrixproduct,33\nMaxnorm,39\nMaxpooling,336\nMaximumlikelihood, 1 3 0\nMaxout,,191422\nMCMC, s e eMarkovchainMonteCarlo\nMean\ufb01eld,,,633634666\nMeansquarederror,107\nMeasuretheory,70\nMeasurezero,70\nMemorynetwork,,413415\nMethodof\u00a0steepestdescent,\u00a0 s e egradient\ndescent\nMinibatch,277\nMissinginputs,99\nMixing(Markovchain),597Mixturedensitynetworks,187\nMixturedistribution,65\nMixturemodel,,187506\nMixtureofexperts,,446544\nMLP, s e emultilayerperception\nMNIST,,,2021666\nModelaveraging,252\nModelcompression,444\nModelidenti\ufb01ability,282\nModelparallelism,444\nMomentmatching,696\nMoore-Penrosepseudoinverse,,44237\nMoralizedgraph,573\nMP-DBM, s e emulti-predictionDBM\nMRF(Markov\u00a0RandomField), s e eundi-\nrectedmodel\nMSE, s e emeansquarederror\nMulti-modallearning,535\nMulti-predictionDBM,668\nMulti-tasklearning,,242533\nMultilayerperception,5\nMultilayerperceptron,26\nMultinomialdistribution,61\nMultinoullidistribution,61\nn-gram, 4 5 8\nNADE,702\nNaiveBayes,3\nNat,72\nNaturalimage,555\nNaturallanguageprocessing,457\nNearestneighborregression, 1 1 4\nNegativede\ufb01nite,88\nNegativephase,,,466602604\nNeocognitron,,,,162326364\nNesterovmomentum,298\nNet\ufb02ixGrandPrize,,255475\nNeurallanguagemodel,,460472\nNeuralnetwork,13\nNeuralTuringmachine,415\nNeuroscience,15\nNewton\u2019smethod,,88309\nNLM, s e eneurallanguagemodel\nNLP, s e enaturallanguageprocessing\nNofreelunchtheorem,115\n7 8 2", "INDEX\nNoise-contrastiveestimation,616\nNon-parametricmodel, 1 1 3\nNorm,,xiv38\nNormaldistribution,,,6263124\nNormalequations,,,, 1 0 8108111232\nNormalizedinitialization,301\nNumericaldi\ufb00erentiation, s e e\ufb01nitedi\ufb00er-\nences\nObjectdetection,449\nObjectrecognition,449\nObjectivefunction,81\nOMP-, k s e eorthogonalmatchingpursuit\nOne-shotlearning,534\nOperation,202\nOptimization,,7981\nOrthodoxstatistics, s e efrequentiststatistics\nOrthogonalmatchingpursuit,,26 2 5 2\nOrthogonalmatrix,41\nOrthogonality,40\nOutputlayer,166\nParalleldistributedprocessing,17\nParameterinitialization,,298403\nParametersharing,,,,, 249332370372386\nParametertying, s e eParametersharing\nParametricmodel, 1 1 3\nParametricReLU,191\nPartialderivative,83\nPartitionfunction,,,564601663\nPCA, s e eprincipalcomponentsanalysis\nPCD, s e estochasticmaximumlikelihood\nPerceptron,,1526\nPersistentcontrastivedivergence, s e estochas-\nticmaximumlikelihood\nPerturbationanalysis, s e ereparametrization\ntrick\nPointestimator,121\nPolicy,476\nPooling,,327677\nPositivede\ufb01nite,88\nPositivephase,,,,, 466602604650662\nPrecision,420\nPrecision(ofanormaldistribution),,6264\nPredictivesparsedecomposition,519Preprocessing,450\nPretraining,,320524\nPrimaryvisualcortex,362\nPrincipalcomponentsanalysis,,,, 47145146\n486626,\nPriorprobabilitydistribution, 1 3 4\nProbabilisticmaxpooling,677\nProbabilisticPCA,,,486487627\nProbabilitydensityfunction,57\nProbabilitydistribution,55\nProbabilitymassfunction,55\nProbabilitymassfunctionestimation,102\nProductofexperts,566\nProductruleofprobability, s e echainrule\nofprobability\nPSD, s e epredictivesparsedecomposition\nPseudolikelihood,611\nQuadraturepair,366\nQuasi-Newtonmethods,314\nRadialbasisfunction,195\nRandomsearch,431\nRandomvariable,55\nRatiomatching,614\nRBF,195\nRBM, s e erestrictedBoltzmannmachine\nRecall,420\nReceptive\ufb01eld,334\nRecommenderSystems,474\nRecti\ufb01edlinearunit,,,, 170191422503\nRecurrentnetwork,26\nRecurrentneuralnetwork,375\nRegression,99\nRegularization,,,,, 1 1 9119176226427\nRegularizer,118\nREINFORCE,683\nReinforcementlearning,,,, 24105476683\nRelationaldatabase,479\nRelations,478\nReparametrizationtrick,682\nRepresentationlearning,3\nRepresentationalcapacity,113\nRestrictedBoltzmannmachine,\u00a0,\u00a0,353456\n475583626650651666670 ,,,,,,,\n7 8 3", "INDEX\n672674677,,\nRidgeregression, s e eweightdecay\nRisk,273\nRNN-RBM,679\nSaddlepoints,283\nSamplemean,124\nScalar,,,xixii30\nScorematching,,509613\nSecondderivative,85\nSecondderivativetest,88\nSelf-information,72\nSemantichashing,521\nSemi-supervisedlearning,241\nSeparableconvolution,359\nSeparation(probabilisticmodeling),568\nSet,xii\nSGD, s e estochasticgradientdescent\nShannonentropy,,xiii73\nShortlist,462\nSigmoid,,xiv s e elogisticsigmoid\nSigmoidbeliefnetwork,26\nSimplecell,362\nSingularvalue, s e esingularvaluedecompo-\nsition\nSingularvaluedecomposition,,,43146475\nSingularvector, s e esingularvaluedecom-\nposition\nSlowfeatureanalysis,489\nSML, s e estochasticmaximumlikelihood\nSoftmax,,,182415446\nSoftplus,,,xiv67195\nSpamdetection,3\nSparsecoding,,,,, 319353492626686\nSparseinitialization,,302403\nSparserepresentation,,,,, 145224251501\n552\nSpearmint,433\nSpectralradius,401\nSpeechrecognition, s e e \u00a0automaticspeech\nrecognition\nSphering, s e ewhitening\nSpikeand\u00a0slabrestricted\u00a0Boltzmannma-\nchine,674\nSPN, s e esum-productnetworkSquarematrix,37\nssRBM, s e espikeandslabrestrictedBoltz-\nmannmachine\nStandarddeviation,60\nStandarderror,126\nStandarderrorofthemean,,126276\nStatistic,121\nStatisticallearningtheory,109\nSteepestdescent, s e egradientdescent\nStochasticback-propagation, s e ereparametriza-\ntiontrick\nStochasticgradientdescent,,,, 15149277\n2 9 2,666\nStochasticmaximumlikelihood,,608666\nStochasticpooling,263\nStructurelearning,578\nStructuredoutput,,100679\nStructuredprobabilisticmodel,,76554\nSumruleofprobability,57\nSum-productnetwork,549\nSupervised\ufb01ne-tuning,,525656\nSupervisedlearning, 1 0 4\nSupportvectormachine,139\nSurrogatelossfunction,274\nSVD, s e esingularvaluedecomposition\nSymmetricmatrix,,4042\nTangentdistance,267\nTangentplane,511\nTangentprop,267\nTDNN, s e etime-delayneuralnetwork\nTeacherforcing,,379380\nTempering,599\nTemplatematching,140\nTensor,,,xixii32\nTestset,109\nTikhonovregularization, s e eweightdecay\nTiledconvolution,349\nTime-delayneuralnetwork,,364371\nToeplitzmatrix,330\nTopographicICA,489\nTraceoperator,45\nTrainingerror,109\nTranscription,100\nTransferlearning,532\n7 8 4", "INDEX\nTranspose,,xii32\nTriangleinequality,38\nTriangulatedgraph, s e echordalgraph\nTrigram,458\nUnbiased,123\nUndirectedgraphicalmodel,,76503\nUndirectedmodel,562\nUniformdistribution,56\nUnigram,458\nUnitnorm,40\nUnitvector,40\nUniversalapproximationtheorem,196\nUniversalapproximator,549\nUnnormalizedprobabilitydistribution,563\nUnsupervisedlearning,, 1 0 4144\nUnsupervisedpretraining,,456524\nV-structure, s e eexplainingaway\nV1,362\nVAE, s e evariationalautoencoder\nVapnik-Chervonenkisdimension,113\nVariance,,,xiii60227\nVariationalautoencoder,,683 6 9 0\nVariationalderivatives, s e efunctionalderiva-\ntives\nVariationalfreeenergy, s e eevidencelower\nbound\nVCdimension, s e eVapnik-Chervonenkisdi-\nmension\nVector,,,xixii31\nVirtualadversarialexamples,266\nVisiblelayer,6\nVolumetricdata,357\nWake-sleep,,646655\nWeightdecay,,,, 1 1 7176229428\nWeightspacesymmetry,282\nWeights,,15106\nWhitening,452\nWikibase,479\nWikibase,479\nWordembedding,460\nWord-sensedisambiguation,480\nWordNet,479Zero-datalearning, s e ezero-shotlearning\nZero-shotlearning,534\n7 8 5"]