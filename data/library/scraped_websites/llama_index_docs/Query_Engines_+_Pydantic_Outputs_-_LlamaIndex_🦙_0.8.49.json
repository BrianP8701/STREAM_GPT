{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/query_modules/structured_outputs/query_engine.html",
        "title": "Query Engines + Pydantic Outputs - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nUsing `index.as_query_engine()` and it\u2019s underlying `RetrieverQueryEngine`, we can support structured pydantic outputs without an additional LLM calls (in contrast to a typical output parser.)\n\nEvery query engine has support for integrated structured responses using the following `response_mode`s in `RetrieverQueryEngine`:\n\n*   `refine`\n    \n*   `compact`\n    \n*   `tree_summarize`\n    \n*   `accumulate` (beta, requires extra parsing to convert to objects)\n    \n*   `compact_accumulate` (beta, requires extra parsing to convert to objects)\n    \n\nUnder the hood, this uses `OpenAIPydanitcProgam` or `LLMTextCompletionProgram` depending on which LLM you\u2019ve setup. If there are intermediate LLM responses (i.e. during `refine` or `tree_summarize` with multiple LLM calls), the pydantic object is injected into the next LLM prompt as a JSON object.\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\nFirst, you need to define the object you want to extract.\n\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Biography(BaseModel):\n    \"\"\"Data model for a biography.\"\"\"\n\n    name: str\n    best\\_known\\_for: List\\[str\\]\n    extra\\_info: str\n\nThen, you create your query engine.\n\nquery\\_engine \\= index.as\\_query\\_engine(response\\_mode\\=\"tree\\_summarize\", output\\_cls\\=Biography)\n\nLastly, you can get a response and inspect the output.\n\nresponse \\= query\\_engine.query(\"Who is Paul Graham?\")\n\nprint(response.name)\n\\> 'Paul Graham'\nprint(response.best\\_known\\_for)\n\\> \\['working on Bel', 'co-founding Viaweb', 'creating the programming language Arc'\\]\nprint(response.extra\\_info)\n\\> \"Paul Graham is a computer scientist, entrepreneur, and writer. He is best known for ...\"\n\n## Modules[\uf0c1](#modules \"Permalink to this heading\")\n\nDetailed usage is available in the notebooks below:\n\n*   [Query Engine with Pydantic Outputs](https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html)\n    *   [Setup](https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html#setup)\n    *   [Create the Index + Query Engine (OpenAI)](https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html#create-the-index-query-engine-openai)\n    *   [Create the Index + Query Engine (Non-OpenAI, Beta)](https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html#create-the-index-query-engine-non-openai-beta)\n    *   [Accumulate Examples (Beta)](https://docs.llamaindex.ai/en/stable/examples/query_engine/pydantic_query_engine.html#accumulate-examples-beta)\n*   [Pydantic Tree Summarize](https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html)\n    *   [Load Data](https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html#load-data)\n    *   [Summarize](https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/pydantic_tree_summarize.html#summarize)"
}