{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery.html",
        "title": "Self Correcting Query Engines - Evaluation & Retry - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nIn this notebook, we showcase several advanced, self-correcting query engines.  \nThey leverage the latest LLM\u2019s ability to evaluate its own output, and then self-correct to give better responses.\n\n\\# Uncomment to add your OpenAI API key\n\\# import os\n\\# os.environ\\['OPENAI\\_API\\_KEY'\\] = \"INSERT OPENAI KEY\"\n\n\\# Uncomment for debug level logging\n\\# import logging\n\\# import sys\n\n\\# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\\# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nFirst we ingest the document.\n\nfrom llama\\_index.indices.vector\\_store.base import VectorStoreIndex\nfrom llama\\_index.readers.file.base import SimpleDirectoryReader\n\n\\# Needed for running async functions in Jupyter Notebook\nimport nest\\_asyncio\n\nnest\\_asyncio.apply()\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham/\").load\\_data()\nindex \\= VectorStoreIndex.from\\_documents(documents)\nquery \\= \"What did the author do growing up?\"\n\nLet\u2019s what the response from the default query engine looks like\n\nbase\\_query\\_engine \\= index.as\\_query\\_engine()\nresponse \\= base\\_query\\_engine.query(query)\nprint(response)\n\nThe author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\n\n## Retry Query Engine[\uf0c1](#retry-query-engine \"Permalink to this heading\")\n\nThe retry query engine uses an evaluator to improve the response from a base query engine.\n\nIt does the following:\n\n1.  first queries the base query engine, then\n    \n2.  use the evaluator to decided if the response passes.\n    \n3.  If the response passes, then return response,\n    \n4.  Otherwise, transform the original query with the evaluation result (query, response, and feedback) into a new query,\n    \n5.  Repeat up to max\\_retries\n    \n\nfrom llama\\_index.query\\_engine import RetryQueryEngine\nfrom llama\\_index.evaluation import RelevancyEvaluator\n\nquery\\_response\\_evaluator \\= RelevancyEvaluator()\nretry\\_query\\_engine \\= RetryQueryEngine(\n    base\\_query\\_engine, query\\_response\\_evaluator\n)\nretry\\_response \\= retry\\_query\\_engine.query(query)\nprint(retry\\_response)\n\nThe author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer, a TRS-80, and started programming more extensively, including writing simple games and a word processor.\n\n## Retry Source Query Engine[\uf0c1](#retry-source-query-engine \"Permalink to this heading\")\n\nThe Source Retry modifies the query source nodes by filtering the existing source nodes for the query based on llm node evaluation.\n\nfrom llama\\_index.query\\_engine import RetrySourceQueryEngine\n\nretry\\_source\\_query\\_engine \\= RetrySourceQueryEngine(\n    base\\_query\\_engine, query\\_response\\_evaluator\n)\nretry\\_source\\_response \\= retry\\_source\\_query\\_engine.query(query)\nprint(retry\\_source\\_response)\n\nThe author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\n\n## Retry Guideline Query Engine[\uf0c1](#retry-guideline-query-engine \"Permalink to this heading\")\n\nThis module tries to use guidelines to direct the evaluator\u2019s behavior. You can customize your own guidelines.\n\nfrom llama\\_index.evaluation.guideline import (\n    GuidelineEvaluator,\n    DEFAULT\\_GUIDELINES,\n)\nfrom llama\\_index.response.schema import Response\nfrom llama\\_index.indices.query.query\\_transform.feedback\\_transform import (\n    FeedbackQueryTransformation,\n)\nfrom llama\\_index.query\\_engine.retry\\_query\\_engine import (\n    RetryGuidelineQueryEngine,\n)\n\n\\# Guideline eval\nguideline\\_eval \\= GuidelineEvaluator(\n    guidelines\\=DEFAULT\\_GUIDELINES\n    + \"\\\\nThe response should not be overly long.\\\\n\"\n    \"The response should try to summarize where possible.\\\\n\"\n)  \\# just for example\n\nLet\u2019s look like what happens under the hood.\n\ntyped\\_response \\= (\n    response if isinstance(response, Response) else response.get\\_response()\n)\neval \\= guideline\\_eval.evaluate\\_response(query, typed\\_response)\nprint(f\"Guideline eval evaluation result: {eval.feedback}\")\n\nfeedback\\_query\\_transform \\= FeedbackQueryTransformation(resynthesize\\_query\\=True)\ntransformed\\_query \\= feedback\\_query\\_transform.run(query, {\"evaluation\": eval})\nprint(f\"Transformed query: {transformed\\_query.query\\_str}\")\n\nGuideline eval evaluation result: The response partially answers the query but lacks specific statistics or numbers. It provides some details about the author's activities growing up, such as writing short stories and programming on different computers, but it could be more concise and focused. Additionally, the response does not mention any statistics or numbers to support the author's experiences.\nTransformed query: Here is a previous bad answer.\nThe author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\nHere is some feedback from the evaluator about the response given.\nThe response partially answers the query but lacks specific statistics or numbers. It provides some details about the author's activities growing up, such as writing short stories and programming on different computers, but it could be more concise and focused. Additionally, the response does not mention any statistics or numbers to support the author's experiences.\nNow answer the question.\nWhat were the author's activities and interests during their childhood and adolescence?\n\nNow let\u2019s run the full query engine\n\nretry\\_guideline\\_query\\_engine \\= RetryGuidelineQueryEngine(\n    base\\_query\\_engine, guideline\\_eval, resynthesize\\_query\\=True\n)\nretry\\_guideline\\_response \\= retry\\_guideline\\_query\\_engine.query(query)\nprint(retry\\_guideline\\_response)\n\nDuring their childhood and adolescence, the author worked on writing short stories and programming. They mentioned that their short stories were not very good, lacking plot but focusing on characters with strong feelings. In terms of programming, they tried writing programs on the IBM 1401 computer in 9th grade using an early version of Fortran. However, they mentioned being puzzled by the 1401 and not being able to do much with it due to the limited input options. They also mentioned getting a microcomputer, a TRS-80, and starting to write simple games, a program to predict rocket heights, and a word processor."
}