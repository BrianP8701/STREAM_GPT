{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama2.html",
        "title": "Llama2 + VectorStoreIndex - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Llama2 + VectorStoreIndex[\uf0c1](#llama2-vectorstoreindex \"Permalink to this heading\")\n\nThis notebook walks through the proper setup to use llama-2 with LlamaIndex. Specifically, we look at using a vector store index.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\n### Keys[\uf0c1](#keys \"Permalink to this heading\")\n\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"OPENAI\\_API\\_KEY\"\nos.environ\\[\"REPLICATE\\_API\\_TOKEN\"\\] \\= \"REPLICATE\\_API\\_TOKEN\"\n\n\\# currently needed for notebooks\nimport openai\n\nopenai.api\\_key \\= os.environ\\[\"OPENAI\\_API\\_KEY\"\\]\n\n### Load documents, build the VectorStoreIndex[\uf0c1](#load-documents-build-the-vectorstoreindex \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n)\n\nfrom IPython.display import Markdown, display\n\nINFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nNote: NumExpr detected 12 cores but \"NUMEXPR\\_MAX\\_THREADS\" not set, so enforcing safe limit of 8.\nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\nNumExpr defaulting to 8 threads.\n\nfrom llama\\_index.llms import Replicate\nfrom llama\\_index import ServiceContext, set\\_global\\_service\\_context\nfrom llama\\_index.llms.llama\\_utils import (\n    messages\\_to\\_prompt,\n    completion\\_to\\_prompt,\n)\n\n\\# The replicate endpoint\nLLAMA\\_13B\\_V2\\_CHAT \\= \"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\"\n\n\\# inject custom system prompt into llama-2\ndef custom\\_completion\\_to\\_prompt(completion: str) \\-> str:\n    return completion\\_to\\_prompt(\n        completion,\n        system\\_prompt\\=(\n            \"You are a Q&A assistant. Your goal is to answer questions as \"\n            \"accurately as possible is the instructions and context provided.\"\n        ),\n    )\n\nllm \\= Replicate(\n    model\\=LLAMA\\_13B\\_V2\\_CHAT,\n    temperature\\=0.01,\n    \\# override max tokens since it's interpreted\n    \\# as context window instead of max tokens\n    context\\_window\\=4096,\n    \\# override completion representation for llama 2\n    completion\\_to\\_prompt\\=custom\\_completion\\_to\\_prompt,\n    \\# if using llama 2 for data agents, also override the message representation\n    messages\\_to\\_prompt\\=messages\\_to\\_prompt,\n)\n\n\\# set a global service context\nctx \\= ServiceContext.from\\_defaults(llm\\=llm)\nset\\_global\\_service\\_context(ctx)\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\n    \"../../../examples/paul\\_graham\\_essay/data\"\n).load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents)\n\n## Querying[\uf0c1](#querying \"Permalink to this heading\")\n\n\\# set Logging to DEBUG for more detailed outputs\nquery\\_engine \\= index.as\\_query\\_engine()\n\nresponse \\= query\\_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**Based on the context information provided, the author\u2019s activities growing up were:**\n\n***   Writing short stories, which were \u201cawful\u201d and lacked a strong plot.\n    \n*   Programming on an IBM 1401 computer in 9th grade, using an early version of Fortran.\n    \n*   Building a microcomputer with a friend, and writing simple games, a program to predict the height of model rockets, and a word processor.\n    \n*   Studying philosophy in college, but finding it boring and switching to AI.\n    **2.  **Writing essays online, which became a turning point in their career.**\n    \n\n### Streaming Support[\uf0c1](#streaming-support \"Permalink to this heading\")\n\nquery\\_engine \\= index.as\\_query\\_engine(streaming\\=True)\nresponse \\= query\\_engine.query(\"What happened at interleaf?\")\nfor token in response.response\\_gen:\n    print(token, end\\=\"\")\n\n Based on the context information provided, it appears that the author worked at Interleaf, a company that made software for creating and managing documents. The author mentions that Interleaf was \"on the way down\" and that the company's Release Engineering group was large compared to the group that actually wrote the software. It is inferred that Interleaf was experiencing financial difficulties and that the author was nervous about money. However, there is no explicit mention of what specifically happened at Interleaf."
}