{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/output_parsing/LangchainOutputParserDemo.html",
        "title": "Langchain Output Parsing - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Langchain Output Parsing[\uf0c1](#langchain-output-parsing \"Permalink to this heading\")\n\n## Load documents, build the VectorStoreIndex[\uf0c1](#load-documents-build-the-vectorstoreindex \"Permalink to this heading\")\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader\nfrom IPython.display import Markdown, display\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../paul\\_graham\\_essay/data\").load\\_data()\n\nindex \\= VectorStoreIndex.from\\_documents(documents, chunk\\_size\\=512)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_documents\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_documents\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_documents\\] Total embedding token usage: 18579 tokens\n> \\[build\\_index\\_from\\_documents\\] Total embedding token usage: 18579 tokens\n\n## Define Query + Langchain Output Parser[\uf0c1](#define-query-langchain-output-parser \"Permalink to this heading\")\n\nfrom llama\\_index.output\\_parsers import LangchainOutputParser\nfrom llama\\_index.llm\\_predictor import StructuredLLMPredictor\nfrom langchain.output\\_parsers import StructuredOutputParser, ResponseSchema\n\nllm\\_predictor \\= StructuredLLMPredictor()\n\n**Define custom QA and Refine Prompts**\n\nfrom llama\\_index.prompts import PromptTemplate\nfrom llama\\_index.prompts.default\\_prompts import (\n    DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL,\n    DEFAULT\\_REFINE\\_PROMPT\\_TMPL,\n)\n\nresponse\\_schemas \\= \\[\n    ResponseSchema(\n        name\\=\"Education\",\n        description\\=(\n            \"Describes the author's educational experience/background.\"\n        ),\n    ),\n    ResponseSchema(\n        name\\=\"Work\",\n        description\\=\"Describes the author's work experience/background.\",\n    ),\n\\]\n\nlc\\_output\\_parser \\= StructuredOutputParser.from\\_response\\_schemas(\n    response\\_schemas\n)\noutput\\_parser \\= LangchainOutputParser(lc\\_output\\_parser)\n\n\\# NOTE: we use the same output parser for both prompts, though you can choose to use different parsers\n\\# NOTE: here we add formatting instructions to the prompts.\n\nfmt\\_qa\\_tmpl \\= output\\_parser.format(DEFAULT\\_TEXT\\_QA\\_PROMPT\\_TMPL)\nfmt\\_refine\\_tmpl \\= output\\_parser.format(DEFAULT\\_REFINE\\_PROMPT\\_TMPL)\n\nqa\\_prompt \\= PromptTemplate(fmt\\_qa\\_tmpl, output\\_parser\\=output\\_parser)\nrefine\\_prompt \\= PromptTemplate(fmt\\_refine\\_tmpl, output\\_parser\\=output\\_parser)\n\n\\# take a look at the new QA template!\nprint(fmt\\_qa\\_tmpl)\n\nContext information is below. \n---------------------\n{context\\_str}\n---------------------\nGiven the context information and not prior knowledge, answer the question: {query\\_str}\n\n\nThe output should be a markdown code snippet formatted in the following schema:\n\n\\`\\`\\`json\n{{\n\t\"Education\": string  // Describes the author's educational experience/background.\n\t\"Work\": string  // Describes the author's work experience/background.\n}}\n\\`\\`\\`\n\n## Query Index[\uf0c1](#query-index \"Permalink to this heading\")\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    text\\_qa\\_template\\=qa\\_prompt,\n    refine\\_template\\=refine\\_prompt,\n    llm\\_predictor\\=llm\\_predictor,\n)\nresponse \\= query\\_engine.query(\n    \"What are a few things the author did growing up?\",\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 609 tokens\n\n\\> \\[query\\] Total LLM token usage: 609 tokens\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 11 tokens\n\n\\> \\[query\\] Total embedding token usage: 11 tokens\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}"
}