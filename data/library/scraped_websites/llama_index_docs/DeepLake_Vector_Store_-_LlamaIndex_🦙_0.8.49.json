{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/DeepLakeIndexDemo.html",
        "title": "DeepLake Vector Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## DeepLake Vector Store[\uf0c1](#deeplake-vector-store \"Permalink to this heading\")\n\nimport os\nimport textwrap\n\nfrom llama\\_index import VectorStoreIndex, SimpleDirectoryReader, Document\nfrom llama\\_index.vector\\_stores import DeepLakeVectorStore\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"sk-\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\"\nos.environ\\[\"ACTIVELOOP\\_TOKEN\"\\] \\= \"\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\"\n\n/Users/adilkhansarsen/Documents/work/LlamaIndex/llama\\_index/GPTIndex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user\\_install.html\n  from .autonotebook import tqdm as notebook\\_tqdm\n\nif you don\u2019t export token in your environment alternativalay you can use deeplake CLI to loging to deeplake\n\n\\# !activeloop login -t <TOKEN>\n\n\\# load documents\ndocuments \\= SimpleDirectoryReader(\"../paul\\_graham\\_essay/data\").load\\_data()\nprint(\n    \"Document ID:\",\n    documents\\[0\\].doc\\_id,\n    \"Document Hash:\",\n    documents\\[0\\].doc\\_hash,\n)\n\nDocument ID: 14935662-4884-4c57-ac2e-fa62da019665 Document Hash: 77ae91ab542f3abb308c4d7c77c9bc4c9ad0ccd63144802b7cbe7e1bb3a4094e\n\n\\# dataset\\_path = \"hub://adilkhan/paul\\_graham\\_essay\" # if we comment this out and don't pass the path then GPTDeepLakeIndex will create dataset in memory\nfrom llama\\_index.storage.storage\\_context import StorageContext\n\ndataset\\_path \\= \"paul\\_graham\\_essay\"\n\n\\# Create an index over the documnts\nvector\\_store \\= DeepLakeVectorStore(dataset\\_path\\=dataset\\_path, overwrite\\=True)\nstorage\\_context \\= StorageContext.from\\_defaults(vector\\_store\\=vector\\_store)\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, storage\\_context\\=storage\\_context\n)\n\nYour Deep Lake dataset has been successfully created!\nThe dataset is private so make sure you are logged in!\n\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/adilkhan/paul\\_graham\\_essay\n\nhub://adilkhan/paul\\_graham\\_essay loaded successfully.\n\nEvaluating ingest: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 \\[00:21<00:00\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 17617 tokens\n\nDataset(path='hub://adilkhan/paul\\_graham\\_essay', tensors=\\['embedding', 'ids', 'metadata', 'text'\\])\n\n  tensor     htype     shape     dtype  compression\n  -------   -------   -------   -------  ------- \n embedding  generic  (6, 1536)   None     None   \n    ids      text     (6, 1)      str     None   \n metadata    json     (6, 1)      str     None   \n   text      text     (6, 1)      str     None   \n\nif we decide to not pass the path then GPTDeepLakeIndex will create dataset locally called llama\\_index\n\n\\# Create an index over the documnts\n\\# vector\\_store = DeepLakeVectorStore(overwrite=True)\n\\# storage\\_context = StorageContext.from\\_defaults(vector\\_store=vector\\_store)\n\\# index = VectorStoreIndex.from\\_documents(documents, storage\\_context=storage\\_context)\n\nllama\\_index loaded successfully.\n\nEvaluating ingest: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 \\[00:04<00:00\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 17617 tokens\n\nDataset(path='llama\\_index', tensors=\\['embedding', 'ids', 'metadata', 'text'\\])\n\n  tensor     htype     shape     dtype  compression\n  -------   -------   -------   -------  ------- \n embedding  generic  (6, 1536)   None     None   \n    ids      text     (6, 1)      str     None   \n metadata    json     (6, 1)      str     None   \n   text      text     (6, 1)      str     None   \n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\n    \"What did the author learn?\",\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 4028 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 6 tokens\n\nprint(textwrap.fill(str(response), 100))\n\n  The author learned that working on things that are not prestigious can be a good thing, as it can\nlead to discovering something real and avoiding the wrong track. The author also learned that\nignorance can be beneficial, as it can lead to discovering something new and unexpected. The author\nalso learned the importance of working hard, even at the parts of the job they don't like, in order\nto set an example for others. The author also learned the value of unsolicited advice, as it can be\nbeneficial in unexpected ways, such as when Robert Morris suggested that the author should make sure\nY Combinator wasn't the last cool thing they did.\n\nresponse \\= query\\_engine.query(\"What was a hard moment for the author?\")\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 4072 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 9 tokens\n\nprint(textwrap.fill(str(response), 100))\n\n A hard moment for the author was when he was dealing with urgent problems during YC and about 60%\nof them had to do with Hacker News, a news aggregator he had created. He was overwhelmed by the\namount of work he had to do to keep Hacker News running, and it was taking away from his ability to\nfocus on other projects. He was also haunted by the idea that his own work ethic set the upper bound\nfor how hard everyone else worked, so he felt he had to work very hard. He was also dealing with\ndisputes between cofounders, figuring out when people were lying to them, and fighting with people\nwho maltreated the startups. On top of this, he was given unsolicited advice from Robert Morris to\nmake sure Y Combinator wasn't the last cool thing he did, which made him consider quitting.\n\nquery\\_engine \\= index.as\\_query\\_engine()\nresponse \\= query\\_engine.query(\"What was a hard moment for the author?\")\nprint(textwrap.fill(str(response), 100))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total LLM token usage: 4072 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[query\\] Total embedding token usage: 9 tokens\n\n A hard moment for the author was when he was dealing with urgent problems during YC and about 60%\nof them had to do with Hacker News, a news aggregator he had created. He was overwhelmed by the\namount of work he had to do to keep Hacker News running, and it was taking away from his ability to\nfocus on other projects. He was also haunted by the idea that his own work ethic set the upper bound\nfor how hard everyone else worked, so he felt he had to work very hard. He was also dealing with\ndisputes between cofounders, figuring out when people were lying to them, and fighting with people\nwho maltreated the startups. On top of this, he was given unsolicited advice from Robert Morris to\nmake sure Y Combinator wasn't the last cool thing he did, which made him consider quitting.\n\n## Deleting items from the database[\uf0c1](#deleting-items-from-the-database \"Permalink to this heading\")\n\nimport deeplake as dp\n\nds \\= dp.load(\"paul\\_graham\\_essay\")\n\nidx \\= ds.ids\\[0\\].numpy().tolist()\n\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/adilkhan/paul\\_graham\\_essay\n\nhub://adilkhan/paul\\_graham\\_essay loaded successfully.\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 \\[00:00<00:00, 4501.13it/s\\]\n \n\nDataset(path='hub://adilkhan/paul\\_graham\\_essay', tensors=\\['embedding', 'ids', 'metadata', 'text'\\])\n\n  tensor     htype     shape     dtype  compression\n  -------   -------   -------   -------  ------- \n embedding  generic  (5, 1536)   None     None   \n    ids      text     (5, 1)      str     None   \n metadata    json     (5, 1)      str     None   \n   text      text     (5, 1)      str     None"
}