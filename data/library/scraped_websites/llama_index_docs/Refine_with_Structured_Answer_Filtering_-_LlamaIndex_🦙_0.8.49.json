{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/structured_refine.html",
        "title": "Refine with Structured Answer Filtering - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nWhen using our Refine response synthesizer for response synthesis, it\u2019s crucial to filter out non-answers. An issue often encountered is the propagation of a single unhelpful response like \u201cI don\u2019t have the answer\u201d, which can persist throughout the synthesis process and lead to a final answer of the same nature. This can occur even when there are actual answers present in other, more relevant sections.\n\nThese unhelpful responses can be filtered out by setting `structured_answer_filtering` to `True`. It is set to `False` by default since this currently only works best if you are using an OpenAI model that supports function calling.\n\n## Load Data[\uf0c1](#load-data \"Permalink to this heading\")\n\ntexts \\= \\[\n    \"The president in the year 2040 is John Cena.\",\n    \"The president in the year 2050 is Florence Pugh.\",\n    'The president in the year 2060 is Dwayne \"The Rock\" Johnson.',\n\\]\n\n## Summarize[\uf0c1](#summarize \"Permalink to this heading\")\n\nfrom llama\\_index.llms import OpenAI\n\nllm \\= OpenAI(model\\=\"gpt-3.5-turbo-0613\")\n\nfrom llama\\_index import ServiceContext\n\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\nfrom llama\\_index.response\\_synthesizers import get\\_response\\_synthesizer\n\nsummarizer \\= get\\_response\\_synthesizer(\n    response\\_mode\\=\"refine\", service\\_context\\=service\\_context, verbose\\=True\n)\n\nresponse \\= summarizer.get\\_response(\"who is president in the year 2050?\", texts)\n\n\\> Refine context: The president in the year 2050 is Florence Pugh...\n> Refine context: The president in the year 2060 is Dwayne \"The R...\n\n### Failed Result[\uf0c1](#failed-result \"Permalink to this heading\")\n\nAs you can see, we weren\u2019t able to get the correct answer from the input `texts` strings since the initial \u201cI don\u2019t know\u201d answer propogated through till the end of the response synthesis.\n\nI'm sorry, but I don't have access to information about the future.\n\nNow we\u2019ll try again with `structured_answer_filtering=True`\n\nfrom llama\\_index.response\\_synthesizers import get\\_response\\_synthesizer\n\nsummarizer \\= get\\_response\\_synthesizer(\n    response\\_mode\\=\"refine\",\n    service\\_context\\=service\\_context,\n    verbose\\=True,\n    structured\\_answer\\_filtering\\=True,\n)\n\nresponse \\= summarizer.get\\_response(\"who is president in the year 2050?\", texts)\n\nFunction call: StructuredRefineResponse with args: {\n  \"answer\": \"There is not enough context information to determine who is the president in the year 2050.\",\n  \"query\\_satisfied\": false\n}\n> Refine context: The president in the year 2050 is Florence Pugh...\nFunction call: StructuredRefineResponse with args: {\n  \"answer\": \"Florence Pugh\",\n  \"query\\_satisfied\": true\n}\n> Refine context: The president in the year 2060 is Dwayne \"The R...\nFunction call: StructuredRefineResponse with args: {\n  \"answer\": \"Florence Pugh\",\n  \"query\\_satisfied\": false\n}\n\n### Successful Result[\uf0c1](#successful-result \"Permalink to this heading\")\n\nAs you can see, we were able to determine the correct answer from the given context by filtering the `texts` strings for the ones that actually contained the answer to our question.\n\n## Non Function-calling LLMs[\uf0c1](#non-function-calling-llms \"Permalink to this heading\")\n\nYou may want to make use of this filtering functionality with an LLM that doesn\u2019t offer a function calling API.\n\nIn that case, the `Refine` module will automatically switch to using a structured output `Program` that doesn\u2019t rely on an external function calling API.\n\n\\# we'll stick with OpenAI but use an older model that does not support function calling\ndavinci\\_llm \\= OpenAI(model\\=\"text-davinci-003\")\n\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.response\\_synthesizers import get\\_response\\_synthesizer\n\ndavinci\\_service\\_context \\= ServiceContext.from\\_defaults(llm\\=davinci\\_llm)\n\nsummarizer \\= get\\_response\\_synthesizer(\n    response\\_mode\\=\"refine\",\n    service\\_context\\=davinci\\_service\\_context,\n    verbose\\=True,\n    structured\\_answer\\_filtering\\=True,\n)\n\nresponse \\= summarizer.get\\_response(\"who is president in the year 2050?\", texts)\nprint(response)\n\n\\> Refine context: The president in the year 2050 is Florence Pugh...\n> Refine context: The president in the year 2060 is Dwayne \"The R...\nFlorence Pugh is the president in the year 2050 and Dwayne \"The Rock\" Johnson is the president in the year 2060.\n\n### `CompactAndRefine`[\uf0c1](#compactandrefine \"Permalink to this heading\")\n\nSince `CompactAndRefine` is built on top of `Refine`, this response mode also supports structured answer filtering.\n\nfrom llama\\_index.response\\_synthesizers import get\\_response\\_synthesizer\n\nsummarizer \\= get\\_response\\_synthesizer(\n    response\\_mode\\=\"compact\",\n    service\\_context\\=service\\_context,\n    verbose\\=True,\n    structured\\_answer\\_filtering\\=True,\n)\n\nresponse \\= summarizer.get\\_response(\"who is president in the year 2050?\", texts)\nprint(response)\n\nFunction call: StructuredRefineResponse with args: {\n  \"answer\": \"Florence Pugh\",\n  \"query\\_satisfied\": true\n}\nFlorence Pugh"
}