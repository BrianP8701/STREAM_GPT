{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html",
        "title": "Neo4j Graph Store - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Neo4j Graph Store[\uf0c1](#neo4j-graph-store \"Permalink to this heading\")\n\n\\# For OpenAI\n\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"API\\_KEY\\_HERE\"\n\nimport logging\nimport sys\nfrom llama\\_index.llms import OpenAI\nfrom llama\\_index import ServiceContext\n\nlogging.basicConfig(stream\\=sys.stdout, level\\=logging.INFO)\n\n\\# define LLM\nllm \\= OpenAI(temperature\\=0, model\\=\"gpt-3.5-turbo\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm, chunk\\_size\\=512)\n\n\\# For Azure OpenAI\nimport os\nimport json\nimport openai\nfrom llama\\_index.llms import AzureOpenAI\nfrom llama\\_index.embeddings import OpenAIEmbedding\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    KnowledgeGraphIndex,\n    LLMPredictor,\n    ServiceContext,\n)\n\nimport logging\nimport sys\n\nfrom IPython.display import Markdown, display\n\nlogging.basicConfig(\n    stream\\=sys.stdout, level\\=logging.INFO\n)  \\# logging.DEBUG for more verbose output\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nopenai.api\\_type \\= \"azure\"\nopenai.api\\_base \\= \"https://<foo-bar>.openai.azure.com\"\nopenai.api\\_version \\= \"2022-12-01\"\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"<your-openai-key>\"\nopenai.api\\_key \\= os.getenv(\"OPENAI\\_API\\_KEY\")\n\nllm \\= AzureOpenAI(\n    deployment\\_name\\=\"<foo-bar-deployment>\",\n    temperature\\=0,\n    openai\\_api\\_version\\=openai.api\\_version,\n    model\\_kwargs\\={\n        \"api\\_key\": openai.api\\_key,\n        \"api\\_base\": openai.api\\_base,\n        \"api\\_type\": openai.api\\_type,\n        \"api\\_version\": openai.api\\_version,\n    },\n)\nllm\\_predictor \\= LLMPredictor(llm\\=llm)\n\n\\# You need to deploy your own embedding model as well as your own chat completion model\nembedding\\_llm \\= OpenAIEmbedding(\n    model\\=\"text-embedding-ada-002\",\n    deployment\\_name\\=\"<foo-bar-deployment>\",\n    api\\_key\\=openai.api\\_key,\n    api\\_base\\=openai.api\\_base,\n    api\\_type\\=openai.api\\_type,\n    api\\_version\\=openai.api\\_version,\n)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\_predictor\\=llm\\_predictor,\n    embed\\_model\\=embedding\\_llm,\n)\n\n## Using Knowledge Graph with Neo4jGraphStore[\uf0c1](#using-knowledge-graph-with-neo4jgraphstore \"Permalink to this heading\")\n\n### Building the Knowledge Graph[\uf0c1](#building-the-knowledge-graph \"Permalink to this heading\")\n\nfrom llama\\_index import (\n    KnowledgeGraphIndex,\n    LLMPredictor,\n    ServiceContext,\n    SimpleDirectoryReader,\n)\nfrom llama\\_index.storage.storage\\_context import StorageContext\nfrom llama\\_index.graph\\_stores import Neo4jGraphStore\n\nfrom llama\\_index.llms import OpenAI\nfrom IPython.display import Markdown, display\n\ndocuments \\= SimpleDirectoryReader(\n    \"../../../../examples/paul\\_graham\\_essay/data\"\n).load\\_data()\n\n\\# define LLM\nllm \\= OpenAI(temperature\\=0, model\\=\"gpt-3.5-turbo\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm, chunk\\_size\\=512)\n\n## Prepare for Neo4j[\uf0c1](#prepare-for-neo4j \"Permalink to this heading\")\n\n%pip install neo4j\n\nusername \\= \"neo4j\"\npassword \\= \"retractor-knot-thermocouples\"\nurl \\= \"bolt://44.211.44.239:7687\"\ndatabase \\= \"neo4j\"\n\nRequirement already satisfied: neo4j in /home/tomaz/anaconda3/envs/snakes/lib/python3.9/site-packages (5.11.0)\nRequirement already satisfied: pytz in /home/tomaz/anaconda3/envs/snakes/lib/python3.9/site-packages (from neo4j) (2023.3)\nNote: you may need to restart the kernel to use updated packages.\n\n## Instantiate Neo4jGraph KG Indexes[\uf0c1](#instantiate-neo4jgraph-kg-indexes \"Permalink to this heading\")\n\ngraph\\_store \\= Neo4jGraphStore(\n    username\\=username,\n    password\\=password,\n    url\\=url,\n    database\\=database,\n)\n\nstorage\\_context \\= StorageContext.from\\_defaults(graph\\_store\\=graph\\_store)\n\n\\# NOTE: can take a while!\nindex \\= KnowledgeGraphIndex.from\\_documents(\n    documents,\n    storage\\_context\\=storage\\_context,\n    max\\_triplets\\_per\\_chunk\\=2,\n    service\\_context\\=service\\_context,\n)\n\n### Querying the Knowledge Graph[\uf0c1](#querying-the-knowledge-graph \"Permalink to this heading\")\n\nFirst, we can query and send only the triplets to the LLM.\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    include\\_text\\=False, response\\_mode\\=\"tree\\_summarize\"\n)\n\nresponse \\= query\\_engine.query(\"Tell me more about Interleaf\")\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me more about Interleaf\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['Interleaf'\\]\nERROR:llama\\_index.indices.knowledge\\_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge sequence in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nInterleaf \\['IS\\_ABOUT', 'what not to do'\\]\nInterleaf \\['ADDED', 'scripting language'\\]\nInterleaf \\['MADE', 'software for creating documents'\\]\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**Interleaf is a subject that is related to \u201cwhat not to do\u201d and \u201cscripting language\u201d. It is also associated with the predicates \u201cADDED\u201d and \u201cMADE\u201d, with the objects being \u201cscripting language\u201d and \u201csoftware for creating documents\u201d respectively.**\n\nFor more detailed answers, we can also send the text from where the retrieved tripets were extracted.\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    include\\_text\\=True, response\\_mode\\=\"tree\\_summarize\"\n)\nresponse \\= query\\_engine.query(\n    \"Tell me more about what the author worked on at Interleaf\"\n)\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me more about what the author worked on at Interleaf\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['Interleaf', 'worked', 'author'\\]\nERROR:llama\\_index.indices.knowledge\\_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: c3fd9444-6c20-4cdc-9598-8f0e9ed0b85d: each student had. But the Accademia wasn't teaching me anything except Italia...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: f4bfad23-0cde-4425-99f9-9229ca0a5cc5: learned some useful things at Interleaf, though they were mostly about what n...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge sequence in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nInterleaf \\['IS\\_ABOUT', 'what not to do'\\]\nInterleaf \\['ADDED', 'scripting language'\\]\nInterleaf \\['MADE', 'software for creating documents'\\]\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**At Interleaf, the author worked on software for creating documents. The company had added a scripting language, inspired by Emacs, and the author was hired as a Lisp hacker to write things in it. However, the author admits to being a bad employee and not fully understanding the software, as it was primarily written in C. Despite this, the author was paid well and managed to save enough money to go back to RISD and pay off their college loans. The author also learned some valuable lessons at Interleaf, particularly about what not to do in technology companies.**\n\n### Query with embeddings[\uf0c1](#query-with-embeddings \"Permalink to this heading\")\n\n\\# Clean dataset first\ngraph\\_store.query(\n    \"\"\"\nMATCH (n) DETACH DELETE n\n\"\"\"\n)\n\n\\# NOTE: can take a while!\nindex \\= KnowledgeGraphIndex.from\\_documents(\n    documents,\n    storage\\_context\\=storage\\_context,\n    max\\_triplets\\_per\\_chunk\\=2,\n    service\\_context\\=service\\_context,\n    include\\_embeddings\\=True,\n)\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    include\\_text\\=True,\n    response\\_mode\\=\"tree\\_summarize\",\n    embedding\\_mode\\=\"hybrid\",\n    similarity\\_top\\_k\\=5,\n)\n\n\\# query using top 3 triplets plus keywords (duplicate triplets are removed)\nresponse \\= query\\_engine.query(\n    \"Tell me more about what the author worked on at Interleaf\"\n)\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me more about what the author worked on at Interleaf\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['Interleaf', 'worked', 'author'\\]\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: e0067958-8b62-4186-b78c-a07281531e40: each student had. But the Accademia wasn't teaching me anything except Italia...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 38459cd5-bc20-428d-a2db-9dc2e716bd15: learned some useful things at Interleaf, though they were mostly about what n...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 6be24830-85d5-49d1-8caa-d297cd0e8b14: It had been so long since I'd painted anything that I'd half forgotten why I ...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 2ec81827-d6d5-470d-8851-b97b8d8d80b4: Robert Morris showed it to me when I visited him in Cambridge, where he was n...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 46b8b977-4176-4622-8d4d-ee3ab16132b4: in decent shape at painting and drawing from the RISD foundation that summer,...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 71363c09-ec6b-47c8-86ac-e18be46f1cc2: as scare-quotes. At the time this bothered me, but now it seems amusingly acc...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 2dded283-d876-4014-8352-056fccace896: of my old life. Idelle was in New York at least, and there were other people ...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: de937aec-ebee-4348-9f23-c94d0a5d7436: and I had a lot of time to think on those flights. On one of them I realized ...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Querying with idx: 33936f7a-0f89-48c7-af9a-171372b4b4b0: What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked ...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge sequence in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\n('Interleaf', 'made', 'software for creating documents')\nInterleaf \\['MADE', 'software for creating documents'\\]\n('Interleaf', 'added', 'scripting language')\n('Interleaf', 'is about', 'what not to do')\nInterleaf \\['ADDED', 'scripting language'\\]\nInterleaf \\['IS\\_ABOUT', 'what not to do'\\]\n('I', 'worked on', 'programming')\n('I', 'worked on', 'writing')\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**At Interleaf, the author worked on writing scripts in a Lisp dialect for the company\u2019s software, which was used for creating documents.**\n\n### \\[Optional\\] Try building the graph and manually add triplets![\uf0c1](#optional-try-building-the-graph-and-manually-add-triplets \"Permalink to this heading\")\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\n\nnode\\_parser \\= SimpleNodeParser.from\\_defaults()\n\nnodes \\= node\\_parser.get\\_nodes\\_from\\_documents(documents)\n\n\\# initialize an empty index for now\nindex \\= KnowledgeGraphIndex.from\\_documents(\\[\\], storage\\_context\\=storage\\_context)\n\n\\# add keyword mappings and nodes manually\n\\# add triplets (subject, relationship, object)\n\n\\# for node 0\nnode\\_0\\_tups \\= \\[\n    (\"author\", \"worked on\", \"writing\"),\n    (\"author\", \"worked on\", \"programming\"),\n\\]\nfor tup in node\\_0\\_tups:\n    index.upsert\\_triplet\\_and\\_node(tup, nodes\\[0\\])\n\n\\# for node 1\nnode\\_1\\_tups \\= \\[\n    (\"Interleaf\", \"made software for\", \"creating documents\"),\n    (\"Interleaf\", \"added\", \"scripting language\"),\n    (\"software\", \"generate\", \"web sites\"),\n\\]\nfor tup in node\\_1\\_tups:\n    index.upsert\\_triplet\\_and\\_node(tup, nodes\\[1\\])\n\nquery\\_engine \\= index.as\\_query\\_engine(\n    include\\_text\\=False, response\\_mode\\=\"tree\\_summarize\"\n)\n\nresponse \\= query\\_engine.query(\"Tell me more about Interleaf\")\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me more about Interleaf\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['Solutions', 'Interleaf', 'Software', 'Information', 'Technology'\\]\nERROR:llama\\_index.indices.knowledge\\_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge sequence in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nInterleaf \\['MADE\\_SOFTWARE\\_FOR', 'creating documents'\\]\nInterleaf \\['IS\\_ABOUT', 'what not to do'\\]\nInterleaf \\['ADDED', 'scripting language'\\]\nInterleaf \\['MADE', 'software for creating documents'\\]\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n**Interleaf is a software company that specializes in creating documents. It has added a scripting language to its software to make it easier for users to create documents. It also provides advice on what not to do when creating documents.**"
}