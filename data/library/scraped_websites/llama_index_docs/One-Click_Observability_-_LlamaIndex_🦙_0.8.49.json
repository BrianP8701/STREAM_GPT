{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/one_click_observability.html",
        "title": "One-Click Observability - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## One-Click Observability[\uf0c1](#one-click-observability \"Permalink to this heading\")\n\nLlamaIndex provides **one-click observability** \ud83d\udd2d to allow you to build principled LLM applications in a production setting.\n\nA key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe, debug, and evaluate your system - both as a whole and for each component.\n\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you\u2019ll be able to do things like the following:\n\n*   View LLM/prompt inputs/outputs\n    \n*   Ensure that the outputs of any component (LLMs, embeddings) are performing as expected\n    \n*   View call traces for both indexing and querying\n    \n\nEach provider has similarities and differences. Take a look below for the full set of guides for each one!\n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\nTo toggle, you will generally just need to do the following:\n\nfrom llama\\_index import set\\_global\\_handler\n\n\\# general usage\nset\\_global\\_handler(\"<handler\\_name>\", \\*\\*kwargs)\n\n\\# W&B example\n\\# set\\_global\\_handler(\"wandb\", run\\_args={\"project\": \"llamaindex\"})\n\nNote that all `kwargs` to `set_global_handler` are passed to the underlying callback handler.\n\nAnd that\u2019s it! Executions will get seamlessly piped to downstream service (e.g. W&B Prompts) and you\u2019ll be able to access features such as viewing execution traces of your application.\n\n**NOTE**: TruLens (by TruEra) uses a different \u201cone-click\u201d experience. See below for details.\n\n## Simple (LLM Inputs/Outputs)[\uf0c1](#simple-llm-inputs-outputs \"Permalink to this heading\")\n\nThis simple observability tool prints every LLM input/output pair to the terminal. Most useful for when you need to quickly enable debug logging on your LLM application.\n\n### Usage Pattern[\uf0c1](#id2 \"Permalink to this heading\")\n\nimport llama\\_index\n\nllama\\_index.set\\_global\\_handler(\"simple\")\n\n## Partner `One-Click` Integrations[\uf0c1](#partner-one-click-integrations \"Permalink to this heading\")\n\nWe offer a rich set of integrations with our partners. A short description + usage pattern, and guide is provided for each partner.\n\n### Weights and Biases Prompts[\uf0c1](#weights-and-biases-prompts \"Permalink to this heading\")\n\nPrompts allows users to log/trace/inspect the execution flow of LlamaIndex during index construction and querying. It also allows users to version-control their indices.\n\n#### Usage Pattern[\uf0c1](#id3 \"Permalink to this heading\")\n\nfrom llama\\_index import set\\_global\\_handler\nset\\_global\\_handler(\"wandb\", run\\_args\\={\"project\": \"llamaindex\"})\n\n\\# NOTE: No need to do the following\n\\# from llama\\_index.callbacks import WandbCallbackHandler, CallbackManager\n\\# wandb\\_callback = WandbCallbackHandler(run\\_args={\"project\": \"llamaindex\"})\n\\# callback\\_manager = CallbackManager(\\[wandb\\_callback\\])\n\\# service\\_context = ServiceContext.from\\_defaults(\n\\#     callback\\_manager=callback\\_manager\n\\# )\n\n\\# access additional methods on handler to persist index + load index\nimport llama\\_index\n\n\\# persist index\nllama\\_index.global\\_handler.persist\\_index(graph, index\\_name\\=\"composable\\_graph\")\n\\# load storage context\nstorage\\_context \\= llama\\_index.global\\_handler.load\\_storage\\_context(\n    artifact\\_url\\=\"ayut/llamaindex/composable\\_graph:v0\"\n)\n\n![](https://docs.llamaindex.ai/en/stable/_images/wandb.png)\n\n### Arize Phoenix[\uf0c1](#arize-phoenix \"Permalink to this heading\")\n\nArize [Phoenix](https://github.com/Arize-ai/phoenix): LLMOps insights at lightning speed with zero-config observability. Phoenix provides a notebook-first experience for monitoring your models and LLM Applications by providing:\n\n*   LLM Traces - Trace through the execution of your LLM Application to understand the internals of your LLM Application and to troubleshoot problems related to things like retrieval and tool execution.\n    \n*   LLM Evals - Leverage the power of large language models to evaluate your generative model or application\u2019s relevance, toxicity, and more.\n    \n\n#### Usage Pattern[\uf0c1](#id4 \"Permalink to this heading\")\n\n\\# Phoenix can display in real time the traces automatically\n\\# collected from your LlamaIndex application.\nimport phoenix as px\n\\# Look for a URL in the output to open the App in a browser.\npx.launch\\_app()\n\\# The App is initially empty, but as you proceed with the steps below,\n\\# traces will appear automatically as your LlamaIndex application runs.\n\nimport llama\\_index\nllama\\_index.set\\_global\\_handler(\"arize\\_phoenix\")\n\n\\# Run all of your LlamaIndex applications as usual and traces\n\\# will be collected and displayed in Phoenix.\n...\n\n![](https://docs.llamaindex.ai/en/stable/_images/arize_phoenix.png)\n\n### OpenInference[\uf0c1](#openinference \"Permalink to this heading\")\n\n[OpenInference](https://github.com/Arize-ai/open-inference-spec) is an open standard for capturing and storing AI model inferences. It enables experimentation, visualization, and evaluation of LLM applications using LLM observability solutions such as [Phoenix](https://github.com/Arize-ai/phoenix).\n\n#### Usage Pattern[\uf0c1](#id6 \"Permalink to this heading\")\n\nimport llama\\_index\n\nllama\\_index.set\\_global\\_handler(\"openinference\")\n\n\\# NOTE: No need to do the following\n\\# from llama\\_index.callbacks import OpenInferenceCallbackHandler, CallbackManager\n\\# callback\\_handler = OpenInferenceCallbackHandler()\n\\# callback\\_manager = CallbackManager(\\[callback\\_handler\\])\n\\# service\\_context = ServiceContext.from\\_defaults(\n\\#     callback\\_manager=callback\\_manager\n\\# )\n\n\\# Run your LlamaIndex application here...\nfor query in queries:\n    query\\_engine.query(query)\n\n\\# View your LLM app data as a dataframe in OpenInference format.\nfrom llama\\_index.callbacks.open\\_inference\\_callback import as\\_dataframe\n\nquery\\_data\\_buffer \\= llama\\_index.global\\_handler.flush\\_query\\_data\\_buffer()\nquery\\_dataframe \\= as\\_dataframe(query\\_data\\_buffer)\n\n**NOTE**: To unlock capabilities of Phoenix, you will need to define additional steps to feed in query/ context dataframes. See below!\n\n#### Guides[\uf0c1](#id7 \"Permalink to this heading\")\n\n*   [OpenInference Callback Handler + Arize Phoenix](https://docs.llamaindex.ai/en/stable/examples/callbacks/OpenInferenceCallback.html)\n*   [Evaluating Search and Retrieval with Arize Phoenix](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb)\n\n### HoneyHive[\uf0c1](#honeyhive \"Permalink to this heading\")\n\nHoneyHive allows users to trace the execution flow of any LLM pipeline. Users can then debug and analyze their traces, or customize feedback on specific trace events to create evaluation or fine-tuning datasets from production.\n\n#### Usage Pattern[\uf0c1](#id9 \"Permalink to this heading\")\n\nfrom llama\\_index import set\\_global\\_handler\nset\\_global\\_handler(\n    \"honeyhive\",\n    project\\=\"My HoneyHive Project\",\n    name\\=\"My LLM Pipeline Name\",\n    api\\_key\\=\"MY HONEYHIVE API KEY\",\n)\n\n\\# NOTE: No need to do the following\n\\# from llama\\_index import ServiceContext\n\\# from llama\\_index.callbacks import CallbackManager\n\\# from honeyhive.sdk.llamaindex\\_tracer import HoneyHiveLlamaIndexTracer\n\\# hh\\_tracer = HoneyHiveLlamaIndexTracer(\n\\#     project=\"My HoneyHive Project\",\n\\#     name=\"My LLM Pipeline Name\",\n\\#     api\\_key=\"MY HONEYHIVE API KEY\",\n\\# )\n\\# callback\\_manager = CallbackManager(\\[hh\\_tracer\\])\n\\# service\\_context = ServiceContext.from\\_defaults(\n\\#     callback\\_manager=callback\\_manager\n\\# )\n\n![](https://docs.llamaindex.ai/en/stable/_images/honeyhive.png) ![](https://docs.llamaindex.ai/en/stable/_images/perfetto.png) _Use Perfetto to debug and analyze your HoneyHive traces_"
}