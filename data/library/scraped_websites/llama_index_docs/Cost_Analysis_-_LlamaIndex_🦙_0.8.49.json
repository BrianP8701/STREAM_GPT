{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/core_modules/supporting_modules/cost_analysis/root.html",
        "title": "Cost Analysis - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Cost Analysis[\uf0c1](#cost-analysis \"Permalink to this heading\")\n\n## Concept[\uf0c1](#concept \"Permalink to this heading\")\n\nEach call to an LLM will cost some amount of money - for instance, OpenAI\u2019s gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on\n\n*   the type of LLM used\n    \n*   the type of data structure used\n    \n*   parameters used during building\n    \n*   parameters used during querying\n    \n\nThe cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:\n\n1.  A high-level overview of the cost structure of the indices.\n    \n2.  A token predictor that you can use directly within LlamaIndex!\n    \n\n### Overview of Cost Structure[\uf0c1](#overview-of-cost-structure \"Permalink to this heading\")\n\n#### Indices with no LLM calls[\uf0c1](#indices-with-no-llm-calls \"Permalink to this heading\")\n\nThe following indices don\u2019t require LLM calls at all during building (0 cost):\n\n*   `SummaryIndex`\n    \n*   `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document\n    \n*   `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\n    \n\n#### Indices with LLM calls[\uf0c1](#indices-with-llm-calls \"Permalink to this heading\")\n\nThe following indices do require LLM calls during build time:\n\n*   `TreeIndex` - use LLM to hierarchically summarize the text to build the tree\n    \n*   `KeywordTableIndex` - use LLM to extract keywords from each document\n    \n\n### Query Time[\uf0c1](#query-time \"Permalink to this heading\")\n\nThere will always be >= 1 LLM call during query time, in order to synthesize the final answer. Some indices contain cost tradeoffs between index building and querying. `SummaryIndex`, for instance, is free to build, but running a query over a summary index (without filtering or embedding lookups), will call the LLM times.\n\nHere are some notes regarding each of the indices:\n\n*   `SummaryIndex`: by default requires LLM calls, where N is the number of nodes.\n    \n*   `TreeIndex`: by default requires LLM calls, where N is the number of leaf nodes.\n    \n    *   Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node.\n        \n*   `KeywordTableIndex`: by default requires an LLM call to extract query keywords.\n    \n    *   Can do `index.as_retriever(retriever_mode=\"simple\")` or `index.as_retriever(retriever_mode=\"rake\")` to also use regex/RAKE keyword extractors on your query text.\n        \n*   `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\n    \n\n## Usage Pattern[\uf0c1](#usage-pattern \"Permalink to this heading\")\n\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls. This allows you to estimate your costs during 1) index construction, and 2) index querying, before any respective LLM calls are made.\n\nTokens are counted using the `TokenCountingHandler` callback. See the [example notebook](https://docs.llamaindex.ai/en/stable/examples/callbacks/TokenCountingHandler.html) for details on the setup.\n\n### Using MockLLM[\uf0c1](#using-mockllm \"Permalink to this heading\")\n\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \u201cworst case\u201d prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.\n\nfrom llama\\_index import ServiceContext, set\\_global\\_service\\_context\nfrom llama\\_index.llms import MockLLM\n\nllm \\= MockLLM(max\\_tokens\\=256)\n\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm)\n\n\\# optionally set a global service context\nset\\_global\\_service\\_context(service\\_context)\n\nYou can then use this predictor during both index construction and querying.\n\n### Using MockEmbedding[\uf0c1](#using-mockembedding \"Permalink to this heading\")\n\nYou may also predict the token usage of embedding calls with `MockEmbedding`.\n\nfrom llama\\_index import ServiceContext, set\\_global\\_service\\_context\nfrom llama\\_index import MockEmbedding\n\n\\# specify a MockLLMPredictor\nembed\\_model \\= MockEmbedding(embed\\_dim\\=1536)\n\nservice\\_context \\= ServiceContext.from\\_defaults(embed\\_model\\=embed\\_model)\n\n\\# optionally set a global service context\nset\\_global\\_service\\_context(service\\_context)\n\n## Usage Pattern[\uf0c1](#id1 \"Permalink to this heading\")\n\nRead about the full usage pattern below!"
}