{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval.html",
        "title": "Building RAG from Scratch (Open-source only!) - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "Toggle table of contents sidebar\n\nIn this tutorial, we show you how to build a data ingestion pipeline into a vector database, and then build a retrieval pipeline from that vector database, from scratch.\n\nNotably, we use a fully open-source stack:\n\n*   Sentence Transformers as the embedding model\n    \n*   Postgres as the vector store (we support many other [vector stores](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html) too!)\n    \n*   Llama 2 as the LLM (through [llama.cpp](https://github.com/ggerganov/llama.cpp))\n    \n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe setup our open-source components.\n\n1.  Sentence Transformers\n    \n2.  Llama 2\n    \n3.  We initialize postgres and wrap it with our wrappers/abstractions.\n    \n\n### Sentence Transformers[\uf0c1](#sentence-transformers \"Permalink to this heading\")\n\n\\# sentence transformers\nfrom llama\\_index.embeddings import HuggingFaceEmbedding\n\nembed\\_model \\= HuggingFaceEmbedding(model\\_name\\=\"BAAI/bge-small-en\")\n\n### Llama CPP[\uf0c1](#llama-cpp \"Permalink to this heading\")\n\nIn this notebook, we use the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model, along with the proper prompt formatting.\n\nCheck out our [Llama CPP guide](https://gpt-index.readthedocs.io/en/stable/examples/llm/llama_2_llama_cpp.html) for full setup instructions/details.\n\n!pip install llama-cpp-python\n\nRequirement already satisfied: llama-cpp-python in /Users/jerryliu/Programming/gpt\\_index/.venv/lib/python3.10/site-packages (0.2.7)\nRequirement already satisfied: numpy>=1.20.0 in /Users/jerryliu/Programming/gpt\\_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\nRequirement already satisfied: typing-extensions>=4.5.0 in /Users/jerryliu/Programming/gpt\\_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (4.7.1)\nRequirement already satisfied: diskcache>=5.6.1 in /Users/jerryliu/Programming/gpt\\_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n\n\\[notice\\] A new release of pip available: 22.3.1 -> 23.2.1\n\\[notice\\] To update, run: pip install --upgrade pip\n\nfrom llama\\_index.llms import LlamaCPP\n\n\\# model\\_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4\\_0.bin\"\nmodel\\_url \\= \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4\\_0.gguf\"\n\nllm \\= LlamaCPP(\n    \\# You can pass in the URL to a GGML model to download it automatically\n    model\\_url\\=model\\_url,\n    \\# optionally, you can set the path to a pre-downloaded model instead of model\\_url\n    model\\_path\\=None,\n    temperature\\=0.1,\n    max\\_new\\_tokens\\=256,\n    \\# llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n    context\\_window\\=3900,\n    \\# kwargs to pass to \\_\\_call\\_\\_()\n    generate\\_kwargs\\={},\n    \\# kwargs to pass to \\_\\_init\\_\\_()\n    \\# set to at least 1 to use GPU\n    model\\_kwargs\\={\"n\\_gpu\\_layers\": 1},\n    verbose\\=True,\n)\n\n### Define Service Context[\uf0c1](#define-service-context \"Permalink to this heading\")\n\nfrom llama\\_index import ServiceContext\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm, embed\\_model\\=embed\\_model\n)\n\n### Initialize Postgres[\uf0c1](#initialize-postgres \"Permalink to this heading\")\n\nUsing an existing postgres running at localhost, create the database we\u2019ll be using.\n\n**NOTE**: Of course there are plenty of other open-source/self-hosted databases you can use! e.g. Chroma, Qdrant, Weaviate, and many more. Take a look at our [vector store guide](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html).\n\n**NOTE**: You will need to setup postgres on your local system. Here\u2019s an example of how to set it up on OSX: https://www.sqlshack.com/setting-up-a-postgresql-database-on-mac/.\n\n**NOTE**: You will also need to install pgvector (https://github.com/pgvector/pgvector).\n\nYou can add a role like the following:\n\nCREATE ROLE <user\\> WITH LOGIN PASSWORD '<password>';\nALTER ROLE <user\\> SUPERUSER;\n\n!pip install psycopg2-binary pgvector asyncpg \"sqlalchemy\\[asyncio\\]\" greenlet\n\nimport psycopg2\n\ndb\\_name \\= \"vector\\_db\"\nhost \\= \"localhost\"\npassword \\= \"password\"\nport \\= \"5432\"\nuser \\= \"jerry\"\n\\# conn = psycopg2.connect(connection\\_string)\nconn \\= psycopg2.connect(\n    dbname\\=\"postgres\",\n    host\\=host,\n    password\\=password,\n    port\\=port,\n    user\\=user,\n)\nconn.autocommit \\= True\n\nwith conn.cursor() as c:\n    c.execute(f\"DROP DATABASE IF EXISTS {db\\_name}\")\n    c.execute(f\"CREATE DATABASE {db\\_name}\")\n\nfrom sqlalchemy import make\\_url\nfrom llama\\_index.vector\\_stores import PGVectorStore\n\nvector\\_store \\= PGVectorStore.from\\_params(\n    database\\=db\\_name,\n    host\\=host,\n    password\\=password,\n    port\\=port,\n    user\\=user,\n    table\\_name\\=\"llama2\\_paper\",\n    embed\\_dim\\=384,  \\# openai embedding dimension\n)\n\n## Build an Ingestion Pipeline from Scratch[\uf0c1](#build-an-ingestion-pipeline-from-scratch \"Permalink to this heading\")\n\nWe show how to build an ingestion pipeline as mentioned in the introduction.\n\nWe fast-track the steps here (can skip metadata extraction). More details can be found [in our dedicated ingestion guide](https://gpt-index.readthedocs.io/en/latest/examples/low_level/ingestion.html).\n\n### 1\\. Load Data[\uf0c1](#load-data \"Permalink to this heading\")\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocuments \\= loader.load(file\\_path\\=\"./data/llama2.pdf\")\n\n### 2\\. Use a Text Splitter to Split Documents[\uf0c1](#use-a-text-splitter-to-split-documents \"Permalink to this heading\")\n\nfrom llama\\_index.text\\_splitter import SentenceSplitter\n\ntext\\_splitter \\= SentenceSplitter(\n    chunk\\_size\\=1024,\n    \\# separator=\" \",\n)\n\ntext\\_chunks \\= \\[\\]\n\\# maintain relationship with source doc index, to help inject doc metadata in (3)\ndoc\\_idxs \\= \\[\\]\nfor doc\\_idx, doc in enumerate(documents):\n    cur\\_text\\_chunks \\= text\\_splitter.split\\_text(doc.text)\n    text\\_chunks.extend(cur\\_text\\_chunks)\n    doc\\_idxs.extend(\\[doc\\_idx\\] \\* len(cur\\_text\\_chunks))\n\n### 3\\. Manually Construct Nodes from Text Chunks[\uf0c1](#manually-construct-nodes-from-text-chunks \"Permalink to this heading\")\n\nfrom llama\\_index.schema import TextNode\n\nnodes \\= \\[\\]\nfor idx, text\\_chunk in enumerate(text\\_chunks):\n    node \\= TextNode(\n        text\\=text\\_chunk,\n    )\n    src\\_doc \\= documents\\[doc\\_idxs\\[idx\\]\\]\n    node.metadata \\= src\\_doc.metadata\n    nodes.append(node)\n\n### 4\\. Generate Embeddings for each Node[\uf0c1](#generate-embeddings-for-each-node \"Permalink to this heading\")\n\nHere we generate embeddings for each Node using a sentence\\_transformers model.\n\nfor node in nodes:\n    node\\_embedding \\= embed\\_model.get\\_text\\_embedding(\n        node.get\\_content(metadata\\_mode\\=\"all\")\n    )\n    node.embedding \\= node\\_embedding\n\n### 5\\. Load Nodes into a Vector Store[\uf0c1](#load-nodes-into-a-vector-store \"Permalink to this heading\")\n\nWe now insert these nodes into our `PostgresVectorStore`.\n\n## Build Retrieval Pipeline from Scratch[\uf0c1](#build-retrieval-pipeline-from-scratch \"Permalink to this heading\")\n\nWe show how to build a retrieval pipeline. Similar to ingestion, we fast-track the steps. Take a look at our [retrieval guide](https://gpt-index.readthedocs.io/en/latest/examples/low_level/retrieval.html) for more details!\n\nquery\\_str \\= \"Can you tell me about the key concepts for safety finetuning\"\n\n### 1\\. Generate a Query Embedding[\uf0c1](#generate-a-query-embedding \"Permalink to this heading\")\n\nquery\\_embedding \\= embed\\_model.get\\_query\\_embedding(query\\_str)\n\n### 2\\. Query the Vector Database[\uf0c1](#query-the-vector-database \"Permalink to this heading\")\n\n\\# construct vector store query\nfrom llama\\_index.vector\\_stores import VectorStoreQuery\n\nquery\\_mode \\= \"default\"\n\\# query\\_mode = \"sparse\"\n\\# query\\_mode = \"hybrid\"\n\nvector\\_store\\_query \\= VectorStoreQuery(\n    query\\_embedding\\=query\\_embedding, similarity\\_top\\_k\\=2, mode\\=query\\_mode\n)\n\n\\# returns a VectorStoreQueryResult\nquery\\_result \\= vector\\_store.query(vector\\_store\\_query)\nprint(query\\_result.nodes\\[0\\].get\\_content())\n\n### 3\\. Parse Result into a Set of Nodes[\uf0c1](#parse-result-into-a-set-of-nodes \"Permalink to this heading\")\n\nfrom llama\\_index.schema import NodeWithScore\nfrom typing import Optional\n\nnodes\\_with\\_scores \\= \\[\\]\nfor index, node in enumerate(query\\_result.nodes):\n    score: Optional\\[float\\] \\= None\n    if query\\_result.similarities is not None:\n        score \\= query\\_result.similarities\\[index\\]\n    nodes\\_with\\_scores.append(NodeWithScore(node\\=node, score\\=score))\n\n### 4\\. Put into a Retriever[\uf0c1](#put-into-a-retriever \"Permalink to this heading\")\n\nfrom llama\\_index import QueryBundle\nfrom llama\\_index.retrievers import BaseRetriever\nfrom typing import Any, List\n\nclass VectorDBRetriever(BaseRetriever):\n    \"\"\"Retriever over a postgres vector store.\"\"\"\n\n    def \\_\\_init\\_\\_(\n        self,\n        vector\\_store: PGVectorStore,\n        embed\\_model: Any,\n        query\\_mode: str \\= \"default\",\n        similarity\\_top\\_k: int \\= 2,\n    ) \\-> None:\n        \"\"\"Init params.\"\"\"\n        self.\\_vector\\_store \\= vector\\_store\n        self.\\_embed\\_model \\= embed\\_model\n        self.\\_query\\_mode \\= query\\_mode\n        self.\\_similarity\\_top\\_k \\= similarity\\_top\\_k\n\n    def \\_retrieve(self, query\\_bundle: QueryBundle) \\-> List\\[NodeWithScore\\]:\n        \"\"\"Retrieve.\"\"\"\n        query\\_embedding \\= embed\\_model.get\\_query\\_embedding(query\\_str)\n        vector\\_store\\_query \\= VectorStoreQuery(\n            query\\_embedding\\=query\\_embedding,\n            similarity\\_top\\_k\\=self.\\_similarity\\_top\\_k,\n            mode\\=self.\\_query\\_mode,\n        )\n        query\\_result \\= vector\\_store.query(vector\\_store\\_query)\n\n        nodes\\_with\\_scores \\= \\[\\]\n        for index, node in enumerate(query\\_result.nodes):\n            score: Optional\\[float\\] \\= None\n            if query\\_result.similarities is not None:\n                score \\= query\\_result.similarities\\[index\\]\n            nodes\\_with\\_scores.append(NodeWithScore(node\\=node, score\\=score))\n\n        return nodes\\_with\\_scores\n\nretriever \\= VectorDBRetriever(\n    vector\\_store, embed\\_model, query\\_mode\\=\"default\", similarity\\_top\\_k\\=2\n)\n\n## Plug this into our RetrieverQueryEngine to synthesize a response[\uf0c1](#plug-this-into-our-retrieverqueryengine-to-synthesize-a-response \"Permalink to this heading\")\n\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\nquery\\_engine \\= RetrieverQueryEngine.from\\_args(\n    retriever, service\\_context\\=service\\_context\n)\n\nquery\\_str \\= \"How does Llama 2 perform compared to other open-source models?\"\n\nresponse \\= query\\_engine.query(query\\_str)\n\nLlama.generate: prefix-match hit\n\nllama\\_print\\_timings:        load time = 15473.66 ms\nllama\\_print\\_timings:      sample time =    35.20 ms /    53 runs   (    0.66 ms per token,  1505.85 tokens per second)\nllama\\_print\\_timings: prompt eval time = 16132.70 ms /  1816 tokens (    8.88 ms per token,   112.57 tokens per second)\nllama\\_print\\_timings:        eval time =  3149.79 ms /    52 runs   (   60.57 ms per token,    16.51 tokens per second)\nllama\\_print\\_timings:       total time = 19380.78 ms\n\n Based on the results shown in Table 3, Llama 2 outperforms all open-source models on most of the benchmarks, with an average improvement of around 5 points over the next best model (GPT-3.5).\n\nprint(response.source\\_nodes\\[0\\].get\\_content())"
}