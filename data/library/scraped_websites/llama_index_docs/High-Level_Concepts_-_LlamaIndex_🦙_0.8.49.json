{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/getting_started/concepts.html",
        "title": "High-Level Concepts - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## High-Level Concepts[\uf0c1](#high-level-concepts \"Permalink to this heading\")\n\nLlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.\n\nIn this high-level concepts guide, you will learn:\n\n*   the retrieval augmented generation (RAG) paradigm for combining LLM with custom data,\n    \n*   key concepts and modules in LlamaIndex for composing your own RAG pipeline.\n    \n\n## Retrieval Augmented Generation (RAG)[\uf0c1](#retrieval-augmented-generation-rag \"Permalink to this heading\")\n\nRetrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data. It generally consists of two stages:\n\n1.  **indexing stage**: preparing a knowledge base, and\n    \n2.  **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n    \n\n![](https://docs.llamaindex.ai/en/stable/_images/rag.jpg)\n\nLlamaIndex provides the essential toolkit for making both steps super easy. Let\u2019s explore each stage in detail.\n\n### Indexing Stage[\uf0c1](#indexing-stage \"Permalink to this heading\")\n\nLlamaIndex helps you prepare the knowledge base with a suite of data connectors and indexes. ![](https://docs.llamaindex.ai/en/stable/_images/indexing.jpg)\n\n[**Data Connectors**](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/connector/root.html): A data connector (i.e. `Reader`) ingests data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n[**Documents / Nodes**](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/documents_and_nodes/root.html): A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \u201cchunk\u201d of a source `Document`. It\u2019s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\n\n[**Data Indexes**](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/root.html): Once you\u2019ve ingested your data, LlamaIndex will help you index the data into a format that\u2019s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/vector_store_guide.html)\n\n### Querying Stage[\uf0c1](#querying-stage \"Permalink to this heading\")\n\nIn the querying stage, the RAG pipeline retrieves the most relevant context given a user query, and pass that to the LLM (along with the query) to synthesize a response. This gives the LLM up-to-date knowledge that is not in its original training data, (also reducing hallucination). The key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.\n\nLlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent. These building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.\n\n![](https://docs.llamaindex.ai/en/stable/_images/querying.jpg)\n\n#### Building Blocks[\uf0c1](#building-blocks \"Permalink to this heading\")\n\n[**Retrievers**](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/retriever/root.html): A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.\n\n[**Node Postprocessors**](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/node_postprocessors/root.html): A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.\n\n[**Response Synthesizers**](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/response_synthesizers/root.html): A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n\n#### Pipelines[\uf0c1](#pipelines \"Permalink to this heading\")\n\n[**Query Engines**](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/query_engine/root.html): A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n\n[**Chat Engines**](https://docs.llamaindex.ai/en/stable/core_modules/query_modules/chat_engines/root.html): A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer).\n\n[**Agents**](https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/agents/root.html): An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines. The main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic. This gives it additional flexibility to tackle more complex tasks.\n\nNext Steps\n\n*   tell me how to [customize things](https://docs.llamaindex.ai/en/stable/getting_started/customization.html).\n    \n*   curious about a specific module? Check out the module guides \ud83d\udc48\n    \n*   have a use case in mind? Check out the [end-to-end tutorials](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/use_cases.html)"
}