{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html",
        "title": "Custom Retriever combining KG Index and VectorStore Index - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nNow let\u2019s demo how KG Index could be used. We will create a VectorStore Index, KG Index and a Custom Index combining the two.\n\nBelow digrams are showing how in-context learning works:\n\n          in-context learning with Llama Index\n                  \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510                  \n                  \u2502 1  \u2502 2  \u2502 3  \u2502 4  \u2502                  \n                  \u251c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2524                  \n                  \u2502  Docs/Knowledge   \u2502                  \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502        ...        \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       \u2502         \u251c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2524       \u2502         \u2502\n\u2502       \u2502         \u2502 95 \u2502 96 \u2502    \u2502    \u2502       \u2502         \u2502\n\u2502       \u2502         \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518       \u2502         \u2502\n\u2502 User  \u2502\u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500\u25b6   LLM   \u2502\n\u2502       \u2502                                     \u2502         \u2502\n\u2502       \u2502                                     \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u250c \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2510  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u25b2     \n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u25b6\u2502  Tell me ....., please   \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \n             \u2502 \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2510               \u2502             \n               \u2502 3  \u2502 \u2502 96 \u2502                             \n             \u2502 \u2514\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2518               \u2502             \n              \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \n\nWith VectorStoreIndex, we create embeddings of each node(chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes, used to help answer the user query.\n\nWith KG Index, we will extract relationships between entities, representing concise facts from each node. It would look something like this:\n\nNode Split and Embedding\n\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n\u2502 1  \u2502 2  \u2502 3  \u2502 4  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2524\n\u2502  Docs/Knowledge   \u2502\n\u2502        ...        \u2502\n\u251c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2524\n\u2502 95 \u2502 96 \u2502    \u2502    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\n\nThen, if we zoomed in of it:\n\n       Node Split and Embedding, with Knowledge Graph being extracted\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 .\u2500.       .\u2500.    \u2502  .\u2500.       .\u2500.   \u2502            .\u2500.   \u2502  .\u2500.       .\u2500.   \u2502\n\u2502( x )\u2500\u2500\u2500\u2500\u2500\u25b6 y )   \u2502 ( x )\u2500\u2500\u2500\u2500\u2500\u25b6 a )  \u2502           ( j )  \u2502 ( m )\u25c0\u2500\u2500\u2500\u2500( x )  \u2502\n\u2502 \\`\u25b2'       \\`\u2500'    \u2502  \\`\u2500'       \\`\u2500'   \u2502            \\`\u2500'   \u2502  \\`\u2500'       \\`\u2500'   \u2502\n\u2502  \u2502     1         \u2502        2         \u2502        3    \u2502    \u2502        4         \u2502\n\u2502 .\u2500.              \u2502                  \u2502            .\u25bc.   \u2502                  \u2502\n\u2502( z )\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6( i )\u2500\u2510\u2502                  \u2502\n\u2502 \\`\u25c0\u2500\u2500\u2500\u2500\u2510          \u2502                  \u2502            \\`\u2500'  \u2502\u2502                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       \u2502                      Docs/Knowledge           \u2502                   \u2502\n\u2502       \u2502                            ...                \u2502                   \u2502\n\u2502       \u2502                                               \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  .\u2500.  \u2514\u2500\u2500\u2500\u2500\u2500\u2500.   \u2502  .\u2500.             \u2502                 \u2502\u2502  .\u2500.             \u2502\n\u2502 ( x \u25c0\u2500\u2500\u2500\u2500\u2500( b )  \u2502 ( x )            \u2502                 \u2514\u253c\u25b6( n )            \u2502\n\u2502  \\`\u2500'       \\`\u2500'   \u2502  \\`\u2500'             \u2502                  \u2502  \\`\u2500'             \u2502\n\u2502        95   \u2502    \u2502   \u2502    96        \u2502                  \u2502   \u2502    98        \u2502\n\u2502            .\u25bc.   \u2502  .\u25bc.             \u2502                  \u2502   \u25bc              \u2502\n\u2502           ( c )  \u2502 ( d )            \u2502                  \u2502  .\u2500.             \u2502\n\u2502            \\`\u2500'   \u2502  \\`\u2500'             \u2502                  \u2502 ( x )            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\\`\u2500'\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWhere, knowledge, the more granular spliting and information with higher density, optionally multi-hop of `x -> y`, `i -> j -> z -> x` etc\u2026 across many more nodes(chunks) than K(in TopK search) could be inlucded in Retrievers. And we believe there are cases that this additional work matters.\n\nLet\u2019s show examples of that now.\n\n\\# For OpenAI\n\nimport os\n\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"INSERT OPENAI KEY\"\n\nimport logging\nimport sys\n\nlogging.basicConfig(\n    stream\\=sys.stdout, level\\=logging.INFO\n)  \\# logging.DEBUG for more verbose output\n\nfrom llama\\_index import (\n    KnowledgeGraphIndex,\n    ServiceContext,\n    SimpleDirectoryReader,\n)\nfrom llama\\_index.storage.storage\\_context import StorageContext\nfrom llama\\_index.graph\\_stores import NebulaGraphStore\n\nfrom llama\\_index.llms import OpenAI\nfrom IPython.display import Markdown, display\n\n\\# define LLM\n\\# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\nllm \\= OpenAI(temperature\\=0, model\\=\"text-davinci-002\")\nservice\\_context \\= ServiceContext.from\\_defaults(llm\\=llm, chunk\\_size\\_limit\\=512)\n\n\\# For Azure OpenAI\n\nimport os\nimport json\nimport openai\nfrom llama\\_index.llms import AzureOpenAI\nfrom llama\\_index.embeddings import OpenAIEmbedding\nfrom llama\\_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    KnowledgeGraphIndex,\n    ServiceContext,\n)\nfrom llama\\_index import set\\_global\\_service\\_context\n\nfrom llama\\_index.storage.storage\\_context import StorageContext\nfrom llama\\_index.graph\\_stores import NebulaGraphStore\n\nimport logging\nimport sys\n\nfrom IPython.display import Markdown, display\n\nlogging.basicConfig(\n    stream\\=sys.stdout, level\\=logging.INFO\n)  \\# logging.DEBUG for more verbose output\nlogging.getLogger().addHandler(logging.StreamHandler(stream\\=sys.stdout))\n\nopenai.api\\_type \\= \"azure\"\nopenai.api\\_base \\= \"https://<foo-bar>.openai.azure.com\"\nopenai.api\\_version \\= \"2022-12-01\"\nos.environ\\[\"OPENAI\\_API\\_KEY\"\\] \\= \"youcannottellanyone\"\nopenai.api\\_key \\= os.getenv(\"OPENAI\\_API\\_KEY\")\n\nllm \\= AzureOpenAI(\n    engine\\=\"<foo-bar-deployment>\",\n    temperature\\=0,\n    openai\\_api\\_version\\=openai.api\\_version,\n    model\\_kwargs\\={\n        \"api\\_key\": openai.api\\_key,\n        \"api\\_base\": openai.api\\_base,\n        \"api\\_type\": openai.api\\_type,\n        \"api\\_version\": openai.api\\_version,\n    },\n)\n\n\\# You need to deploy your own embedding model as well as your own chat completion model\nembedding\\_llm \\= OpenAIEmbedding(\n    model\\=\"text-embedding-ada-002\",\n    deployment\\_name\\=\"<foo-bar-deployment>\",\n    api\\_key\\=openai.api\\_key,\n    api\\_base\\=openai.api\\_base,\n    api\\_type\\=openai.api\\_type,\n    api\\_version\\=openai.api\\_version,\n)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm,\n    embed\\_model\\=embedding\\_llm,\n)\n\nset\\_global\\_service\\_context(service\\_context)\n\n## Prepare for NebulaGraph[\uf0c1](#prepare-for-nebulagraph \"Permalink to this heading\")\n\n%pip install nebula3-python\n\nos.environ\\[\"NEBULA\\_USER\"\\] \\= \"root\"\nos.environ\\[\"NEBULA\\_PASSWORD\"\\] \\= \"nebula\"\nos.environ\\[\n    \"NEBULA\\_ADDRESS\"\n\\] \\= (  \\# assumed we have NebulaGraph 3.5.0 or newer installed locally\n    \"127.0.0.1:9669\"\n)\n\n\\# Assume that the graph has already been created\n\\# Create a NebulaGraph cluster with:\n\\# Option 0: \\`curl -fsSL nebula-up.siwei.io/install.sh | bash\\`\n\\# Option 1: NebulaGraph Docker Extension https://hub.docker.com/extensions/weygu/nebulagraph-dd-ext\n\\# and that the graph space is called \"llamaindex\"\n\\# If not, create it with the following commands from NebulaGraph's console:\n\\# CREATE SPACE llamaindex(vid\\_type=FIXED\\_STRING(256), partition\\_num=1, replica\\_factor=1);\n\\# :sleep 10;\n\\# USE llamaindex;\n\\# CREATE TAG entity(name string);\n\\# CREATE EDGE relationship(relationship string);\n\\# CREATE TAG INDEX entity\\_index ON entity(name(256));\n\nspace\\_name \\= \"llamaindex\"\nedge\\_types, rel\\_prop\\_names \\= \\[\"relationship\"\\], \\[\n    \"relationship\"\n\\]  \\# default, could be omit if create from an empty kg\ntags \\= \\[\"entity\"\\]  \\# default, could be omit if create from an empty kg\n\n## Load Data from Wikipedia[\uf0c1](#load-data-from-wikipedia \"Permalink to this heading\")\n\nfrom llama\\_index import download\\_loader\n\nWikipediaReader \\= download\\_loader(\"WikipediaReader\")\n\nloader \\= WikipediaReader()\n\ndocuments \\= loader.load\\_data(pages\\=\\[\"2023 in science\"\\], auto\\_suggest\\=False)\n\n## Create KnowledgeGraphIndex Index[\uf0c1](#create-knowledgegraphindex-index \"Permalink to this heading\")\n\ngraph\\_store \\= NebulaGraphStore(\n    space\\_name\\=space\\_name,\n    edge\\_types\\=edge\\_types,\n    rel\\_prop\\_names\\=rel\\_prop\\_names,\n    tags\\=tags,\n)\nstorage\\_context \\= StorageContext.from\\_defaults(graph\\_store\\=graph\\_store)\n\nkg\\_index \\= KnowledgeGraphIndex.from\\_documents(\n    documents,\n    storage\\_context\\=storage\\_context,\n    max\\_triplets\\_per\\_chunk\\=10,\n    space\\_name\\=space\\_name,\n    edge\\_types\\=edge\\_types,\n    rel\\_prop\\_names\\=rel\\_prop\\_names,\n    tags\\=tags,\n    include\\_embeddings\\=True,\n)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 21204 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 21204 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 21204 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 3953 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 3953 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 3953 tokens\n\n## Create VectorStoreIndex Index[\uf0c1](#create-vectorstoreindex-index \"Permalink to this heading\")\n\nvector\\_index \\= VectorStoreIndex.from\\_documents(documents)\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 15419 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 15419 tokens\n> \\[build\\_index\\_from\\_nodes\\] Total embedding token usage: 15419 tokens\n\n## Define a CustomRetriever[\uf0c1](#define-a-customretriever \"Permalink to this heading\")\n\nThe purpose of this demo was to test the effectiveness of using Knowledge Graph queries for retrieving information that is distributed across multiple nodes in small pieces. To achieve this, we adopted a simple approach: performing retrieval on both sources and then combining them into a single context to be sent to LLM.\n\nThanks to the flexible abstraction provided by Llama Index Retriever, implementing this approach was relatively straightforward. We created a new class called `CustomRetriever` which retrieves data from both `VectorIndexRetriever` and `KGTableRetriever`.\n\n\\# import QueryBundle\nfrom llama\\_index import QueryBundle\n\n\\# import NodeWithScore\nfrom llama\\_index.schema import NodeWithScore\n\n\\# Retrievers\nfrom llama\\_index.retrievers import (\n    BaseRetriever,\n    VectorIndexRetriever,\n    KGTableRetriever,\n)\n\nfrom typing import List\n\nclass CustomRetriever(BaseRetriever):\n    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n\n    def \\_\\_init\\_\\_(\n        self,\n        vector\\_retriever: VectorIndexRetriever,\n        kg\\_retriever: KGTableRetriever,\n        mode: str \\= \"OR\",\n    ) \\-> None:\n        \"\"\"Init params.\"\"\"\n\n        self.\\_vector\\_retriever \\= vector\\_retriever\n        self.\\_kg\\_retriever \\= kg\\_retriever\n        if mode not in (\"AND\", \"OR\"):\n            raise ValueError(\"Invalid mode.\")\n        self.\\_mode \\= mode\n\n    def \\_retrieve(self, query\\_bundle: QueryBundle) \\-> List\\[NodeWithScore\\]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n\n        vector\\_nodes \\= self.\\_vector\\_retriever.retrieve(query\\_bundle)\n        kg\\_nodes \\= self.\\_kg\\_retriever.retrieve(query\\_bundle)\n\n        vector\\_ids \\= {n.node.node\\_id for n in vector\\_nodes}\n        kg\\_ids \\= {n.node.node\\_id for n in kg\\_nodes}\n\n        combined\\_dict \\= {n.node.node\\_id: n for n in vector\\_nodes}\n        combined\\_dict.update({n.node.node\\_id: n for n in kg\\_nodes})\n\n        if self.\\_mode \\== \"AND\":\n            retrieve\\_ids \\= vector\\_ids.intersection(kg\\_ids)\n        else:\n            retrieve\\_ids \\= vector\\_ids.union(kg\\_ids)\n\n        retrieve\\_nodes \\= \\[combined\\_dict\\[rid\\] for rid in retrieve\\_ids\\]\n        return retrieve\\_nodes\n\nNext, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever.\n\nfrom llama\\_index import get\\_response\\_synthesizer\nfrom llama\\_index.query\\_engine import RetrieverQueryEngine\n\n\\# create custom retriever\nvector\\_retriever \\= VectorIndexRetriever(index\\=vector\\_index)\nkg\\_retriever \\= KGTableRetriever(\n    index\\=kg\\_index, retriever\\_mode\\=\"keyword\", include\\_text\\=False\n)\ncustom\\_retriever \\= CustomRetriever(vector\\_retriever, kg\\_retriever)\n\n\\# create response synthesizer\nresponse\\_synthesizer \\= get\\_response\\_synthesizer(\n    service\\_context\\=service\\_context,\n    response\\_mode\\=\"tree\\_summarize\",\n)\n\n## Create Query Engines[\uf0c1](#create-query-engines \"Permalink to this heading\")\n\nTo enable comparsion, we also create `vector_query_engine`, `kg_keyword_query_engine` together with our `custom_query_engine`.\n\ncustom\\_query\\_engine \\= RetrieverQueryEngine(\n    retriever\\=custom\\_retriever,\n    response\\_synthesizer\\=response\\_synthesizer,\n)\n\nvector\\_query\\_engine \\= vector\\_index.as\\_query\\_engine()\n\nkg\\_keyword\\_query\\_engine \\= kg\\_index.as\\_query\\_engine(\n    \\# setting to false uses the raw triplets instead of adding the text from the corresponding nodes\n    include\\_text\\=False,\n    retriever\\_mode\\=\"keyword\",\n    response\\_mode\\=\"tree\\_summarize\",\n)\n\n## Query with different retrievers[\uf0c1](#query-with-different-retrievers \"Permalink to this heading\")\n\nWith the above query engines created for corresponding retrievers, let\u2019s see how they perform.\n\nFirst, we go with the pure knowledge graph.\n\nresponse \\= kg\\_keyword\\_query\\_engine.query(\"Tell me events about NASA\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me events about NASA\n> Starting query: Tell me events about NASA\n> Starting query: Tell me events about NASA\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['NASA', 'events'\\]\n> Query keywords: \\['NASA', 'events'\\]\n> Query keywords: \\['NASA', 'events'\\]\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 159 tokens\n> \\[get\\_response\\] Total LLM token usage: 159 tokens\n> \\[get\\_response\\] Total LLM token usage: 159 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 159 tokens\n> \\[get\\_response\\] Total LLM token usage: 159 tokens\n> \\[get\\_response\\] Total LLM token usage: 159 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**NASA announced future space telescope programs in mid-2023, published images of a debris disk, and discovered an exoplanet called LHS 475 b.**\n\nThen the vector store approach.\n\nresponse \\= vector\\_query\\_engine.query(\"Tell me events about NASA\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 5 tokens\n> \\[retrieve\\] Total embedding token usage: 5 tokens\n> \\[retrieve\\] Total embedding token usage: 5 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1892 tokens\n> \\[get\\_response\\] Total LLM token usage: 1892 tokens\n> \\[get\\_response\\] Total LLM token usage: 1892 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**NASA scientists report evidence for the existence of a second Kuiper Belt, which the New Horizons spacecraft could potentially visit during the late 2020s or early 2030s. NASA is expected to release the first study on UAP in mid-2023. NASA's Venus probe is scheduled to be launched and to arrive on Venus in October, partly to search for signs of life on Venus. NASA is expected to start the Vera Rubin Observatory, the Qitai Radio Telescope, the European Spallation Source and the Jiangmen Underground Neutrino. NASA scientists suggest that a space sunshade could be created by mining the lunar soil and launching it towards the Sun to form a shield against global warming.**\n\nFinally, let\u2019s do with the one with both vector store and knowledge graph.\n\nresponse \\= custom\\_query\\_engine.query(\"Tell me events about NASA\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 5 tokens\n> \\[retrieve\\] Total embedding token usage: 5 tokens\n> \\[retrieve\\] Total embedding token usage: 5 tokens\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me events about NASA\n> Starting query: Tell me events about NASA\n> Starting query: Tell me events about NASA\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['NASA', 'events'\\]\n> Query keywords: \\['NASA', 'events'\\]\n> Query keywords: \\['NASA', 'events'\\]\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nnasa \\['public release date', 'mid-2023'\\]\nnasa \\['announces', 'future space telescope programs'\\]\nnasa \\['publishes images of', 'debris disk'\\]\nnasa \\['discovers', 'exoplanet lhs 475 b'\\]\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 2046 tokens\n> \\[get\\_response\\] Total LLM token usage: 2046 tokens\n> \\[get\\_response\\] Total LLM token usage: 2046 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 2046 tokens\n> \\[get\\_response\\] Total LLM token usage: 2046 tokens\n> \\[get\\_response\\] Total LLM token usage: 2046 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**NASA announces future space telescope programs on May 21. NASA publishes images of debris disk on May 23. NASA discovers exoplanet LHS 475 b on May 25. NASA scientists present evidence for the existence of a second Kuiper Belt on May 29. NASA confirms the start of the next El Ni\u00f1o on June 8. NASA produces the first X-ray of a single atom on May 31. NASA reports the first successful beaming of solar energy from space down to a receiver on the ground on June 1. NASA scientists report evidence that Earth may have formed in just three million years on June 14. NASA scientists report the presence of phosphates on Enceladus, moon of the planet Saturn, on June 14. NASA's Venus probe is scheduled to be launched and to arrive on Venus in October. NASA's MBR Explorer is announced by the United Arab Emirates Space Agency on May 29. NASA's Vera Rubin Observatory is expected to start in 2023.**\n\n## Comparison of results[\uf0c1](#comparison-of-results \"Permalink to this heading\")\n\nLet\u2019s put results together with their LLM tokens during the query process:\n\n> Tell me events about NASA.\n\n|     | VectorStore | Knowledge Graph + VectorStore | Knowledge Graph |\n| --- | --- | --- | --- |\n| Answer | NASA scientists report evidence for the existence of a second Kuiper Belt, which the New Horizons spacecraft could potentially visit during the late 2020s or early 2030s. NASA is expected to release the first study on UAP in mid-2023. NASA\u2019s Venus probe is scheduled to be launched and to arrive on Venus in October, partly to search for signs of life on Venus. NASA is expected to start the Vera Rubin Observatory, the Qitai Radio Telescope, the European Spallation Source and the Jiangmen Underground Neutrino. NASA scientists suggest that a space sunshade could be created by mining the lunar soil and launching it towards the Sun to form a shield against global warming. | NASA announces future space telescope programs on May 21. **NASA publishes images of debris disk on May 23. NASA discovers exoplanet LHS 475 b on May 25.** NASA scientists present evidence for the existence of a second Kuiper Belt on May 29. NASA confirms the start of the next El Ni\u00f1o on June 8. NASA produces the first X-ray of a single atom on May 31. NASA reports the first successful beaming of solar energy from space down to a receiver on the ground on June 1. NASA scientists report evidence that Earth may have formed in just three million years on June 14. NASA scientists report the presence of phosphates on Enceladus, moon of the planet Saturn, on June 14. NASA\u2019s Venus probe is scheduled to be launched and to arrive on Venus in October. NASA\u2019s MBR Explorer is announced by the United Arab Emirates Space Agency on May 29. NASA\u2019s Vera Rubin Observatory is expected to start in 2023. | NASA announced future space telescope programs in mid-2023, published images of a debris disk, and discovered an exoplanet called LHS 475 b. |\n| Cost | 1897 tokens | 2046 Tokens | 159 Tokens |\n\nAnd we could see there are indeed some knowledges added with the help of Knowledge Graph retriever:\n\n*   NASA publishes images of debris disk on May 23.\n    \n*   NASA discovers exoplanet LHS 475 b on May 25.\n    \n\nThe additional cost, however, does not seem to be very significant, at `7.28%`: `(2046-1897)/2046`.\n\nFurthermore, the answer from the knowledge graph is extremely concise (only 159 tokens used!), but is still informative.\n\n## Not all cases are advantageous[\uf0c1](#not-all-cases-are-advantageous \"Permalink to this heading\")\n\nWhile, of course, many other questions do not contain small-grained pieces of knowledges in chunks. In these cases, the extra Knowledge Graph retriever may not that helpful. Let\u2019s see this question: \u201cTell me events about ChatGPT\u201d.\n\nresponse \\= custom\\_query\\_engine.query(\"Tell me events about ChatGPT\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 7 tokens\n> \\[retrieve\\] Total embedding token usage: 7 tokens\n> \\[retrieve\\] Total embedding token usage: 7 tokens\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me events about ChatGPT\n> Starting query: Tell me events about ChatGPT\n> Starting query: Tell me events about ChatGPT\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['events', 'ChatGPT'\\]\n> Query keywords: \\['events', 'ChatGPT'\\]\n> Query keywords: \\['events', 'ChatGPT'\\]\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 2045 tokens\n> \\[get\\_response\\] Total LLM token usage: 2045 tokens\n> \\[get\\_response\\] Total LLM token usage: 2045 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 2045 tokens\n> \\[get\\_response\\] Total LLM token usage: 2045 tokens\n> \\[get\\_response\\] Total LLM token usage: 2045 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**ChatGPT is a chatbot and text-generating AI released on 30 November 2022. It quickly became highly popular, with some estimating that only two months after its launch, it had 100 million active users. Potential applications of ChatGPT include solving or supporting school writing assignments, malicious social bots (e.g. for misinformation, propaganda, and scams), and providing inspiration (e.g. for artistic writing or in design or ideation in general). There was extensive media coverage of views that regard ChatGPT as a potential step towards AGI or sentient machines, also extending to some academic works. Google released chatbot Bard due to effects of the ChatGPT release, with potential for integration into its Web search and, like ChatGPT software, also as a software development helper tool (21 Mar). DuckDuckGo released the DuckAssist feature integrated into its search engine that summarizes information from Wikipedia to answer search queries that are questions (8 Mar). The experimental feature was shut down without explanation on 12 April. Around the same time, a proprietary feature by scite.ai was released that delivers answers that use research papers and provide citations for the quoted paper(s). An open letter \"Pause Giant AI Experiments\" by the Future of Life**\n\nresponse \\= kg\\_keyword\\_query\\_engine.query(\"Tell me events about ChatGPT\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Starting query: Tell me events about ChatGPT\n> Starting query: Tell me events about ChatGPT\n> Starting query: Tell me events about ChatGPT\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Query keywords: \\['events', 'ChatGPT'\\]\n> Query keywords: \\['events', 'ChatGPT'\\]\n> Query keywords: \\['events', 'ChatGPT'\\]\nINFO:llama\\_index.indices.knowledge\\_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\n> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of \\`subject \\[predicate, object, predicate\\_next\\_hop, object\\_next\\_hop ...\\]\\`\nchatgpt \\['is', 'language model'\\]\nchatgpt \\['outperform', 'human doctors'\\]\nchatgpt \\['has', '100 million active users'\\]\nchatgpt \\['released on', '30 nov 2022'\\]\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 150 tokens\n> \\[get\\_response\\] Total LLM token usage: 150 tokens\n> \\[get\\_response\\] Total LLM token usage: 150 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 150 tokens\n> \\[get\\_response\\] Total LLM token usage: 150 tokens\n> \\[get\\_response\\] Total LLM token usage: 150 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**ChatGPT is a language model that outperforms human doctors and has 100 million active users. It was released on 30 November 2022.**\n\nresponse \\= vector\\_query\\_engine.query(\"Tell me events about ChatGPT\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\n> \\[retrieve\\] Total LLM token usage: 0 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[retrieve\\] Total embedding token usage: 7 tokens\n> \\[retrieve\\] Total embedding token usage: 7 tokens\n> \\[retrieve\\] Total embedding token usage: 7 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total LLM token usage: 1956 tokens\n> \\[get\\_response\\] Total LLM token usage: 1956 tokens\n> \\[get\\_response\\] Total LLM token usage: 1956 tokens\nINFO:llama\\_index.token\\_counter.token\\_counter:> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n> \\[get\\_response\\] Total embedding token usage: 0 tokens\n\n**ChatGPT (released on 30 Nov 2022) is a chatbot and text-generating AI, and a large language model that quickly became highly popular. It is estimated that only two months after its launch, it had 100 million active users. Applications may include solving or supporting school writing assignments, malicious social bots (e.g. for misinformation, propaganda, and scams), and providing inspiration (e.g. for artistic writing or in design or ideation in general). In response to the ChatGPT release, Google released chatbot Bard (21 Mar) with potential for integration into its Web search and, like ChatGPT software, also as a software development helper tool. DuckDuckGo released the DuckAssist feature integrated into its search engine that summarizes information from Wikipedia to answer search queries that are questions (8 Mar). The experimental feature was shut down without explanation on 12 April. Around the time, a proprietary feature by scite.ai was released that delivers answers that use research papers and provide citations for the quoted paper(s). An open letter \"Pause Giant AI Experiments\" by the Future of Life Institute calls for \"AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-**\n\n## Comparison of results[\uf0c1](#id1 \"Permalink to this heading\")\n\nWe can see that being w/ vs. w/o Knowledge Graph has no unique advantage under this question.\n\n> Question: Tell me events about ChatGPT.\n\n|     | VectorStore | Knowledge Graph + VectorStore | Knowledge Graph |\n| --- | --- | --- | --- |\n| Answer | ChatGPT (released on 30 Nov 2022) is a chatbot and text-generating AI, and a large language model that quickly became highly popular. It is estimated that only two months after its launch, it had 100 million active users. Applications may include solving or supporting school writing assignments, malicious social bots (e.g. for misinformation, propaganda, and scams), and providing inspiration (e.g. for artistic writing or in design or ideation in general). In response to the ChatGPT release, Google released chatbot Bard (21 Mar) with potential for integration into its Web search and, like ChatGPT software, also as a software development helper tool. DuckDuckGo released the DuckAssist feature integrated into its search engine that summarizes information from Wikipedia to answer search queries that are questions (8 Mar). The experimental feature was shut down without explanation on 12 April. Around the time, a proprietary feature by scite.ai was released that delivers answers that use research papers and provide citations for the quoted paper(s). An open letter \u201cPause Giant AI Experiments\u201d by the Future of Life Institute calls for \u201cAI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT- | ChatGPT is a chatbot and text-generating AI released on 30 November 2022. It quickly became highly popular, with some estimating that only two months after its launch, it had 100 million active users. Potential applications of ChatGPT include solving or supporting school writing assignments, malicious social bots (e.g. for misinformation, propaganda, and scams), and providing inspiration (e.g. for artistic writing or in design or ideation in general). There was extensive media coverage of views that regard ChatGPT as a potential step towards AGI or sentient machines, also extending to some academic works. Google released chatbot Bard due to effects of the ChatGPT release, with potential for integration into its Web search and, like ChatGPT software, also as a software development helper tool (21 Mar). DuckDuckGo released the DuckAssist feature integrated into its search engine that summarizes information from Wikipedia to answer search queries that are questions (8 Mar). The experimental feature was shut down without explanation on 12 April. Around the same time, a proprietary feature by scite.ai was released that delivers answers that use research papers and provide citations for the quoted paper(s). An open letter \u201cPause Giant AI Experiments\u201d by the Future of Life | ChatGPT is a language model that outperforms human doctors and has 100 million active users. It was released on 30 November 2022. |\n| Cost | 1963 Tokens | 2045 Tokens | 150 Tokens |\n\n\\## create graph\nfrom pyvis.network import Network\n\ng \\= kg\\_index.get\\_networkx\\_graph(200)\nnet \\= Network(notebook\\=True, cdn\\_resources\\=\"in\\_line\", directed\\=True)\nnet.from\\_nx(g)\nnet.show(\"2023\\_Science\\_Wikipedia\\_KnowledgeGraph.html\")\n\n2023\\_Science\\_Wikipedia\\_KnowledgeGraph.html"
}