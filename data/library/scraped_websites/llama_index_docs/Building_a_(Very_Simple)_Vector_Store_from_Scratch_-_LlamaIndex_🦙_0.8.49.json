{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/low_level/vector_store.html",
        "title": "Building a (Very Simple) Vector Store from Scratch - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nIn this tutorial, we show you how to build a simple in-memory vector store that can store documents along with metadata. It will also expose a query interface that can support a variety of queries:\n\n*   semantic search (with embedding similarity)\n    \n*   metadata filtering\n    \n\n**NOTE**: Obviously this is not supposed to be a replacement for any actual vector store (e.g. Pinecone, Weaviate, Chroma, Qdrant, Milvus, or others within our wide range of vector store integrations). This is more to teach some key retrieval concepts, like top-k embedding search + metadata filtering.\n\nWe won\u2019t be covering advanced query/retrieval concepts such as approximate nearest neighbors, sparse/hybrid search, or any of the system concepts that would be required for building an actual database.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nWe load in some documents, and parse them into Node objects - chunks that are ready to be inserted into a vector store.\n\n### Load in Documents[\uf0c1](#load-in-documents \"Permalink to this heading\")\n\n!mkdir data\n!wget \\--user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" \\-O \"data/llama2.pdf\"\n\nfrom pathlib import Path\nfrom llama\\_hub.file.pymu\\_pdf.base import PyMuPDFReader\n\nloader \\= PyMuPDFReader()\ndocuments \\= loader.load(file\\_path\\=\"./data/llama2.pdf\")\n\n### Parse into Nodes[\uf0c1](#parse-into-nodes \"Permalink to this heading\")\n\nfrom llama\\_index.node\\_parser import SimpleNodeParser\n\nnode\\_parser \\= SimpleNodeParser.from\\_defaults(chunk\\_size\\=256)\nnodes \\= node\\_parser.get\\_nodes\\_from\\_documents(documents)\n\n### Generate Embeddings for each Node[\uf0c1](#generate-embeddings-for-each-node \"Permalink to this heading\")\n\nfrom llama\\_index.embeddings import OpenAIEmbedding\n\nembed\\_model \\= OpenAIEmbedding()\nfor node in nodes:\n    node\\_embedding \\= embed\\_model.get\\_text\\_embedding(\n        node.get\\_content(metadata\\_mode\\=\"all\")\n    )\n    node.embedding \\= node\\_embedding\n\n## Build a Simple In-Memory Vector Store[\uf0c1](#build-a-simple-in-memory-vector-store \"Permalink to this heading\")\n\nNow we\u2019ll build our in-memory vector store. We\u2019ll store Nodes within a simple Python dictionary. We\u2019ll start off implementing embedding search, and add metadata filters.\n\n### 1\\. Defining the Interface[\uf0c1](#defining-the-interface \"Permalink to this heading\")\n\nWe\u2019ll first define the interface for building a vector store. It contains the following items:\n\n*   `get`\n    \n*   `add`\n    \n*   `delete`\n    \n*   `query`\n    \n*   `persist` (which we will not implement)\n    \n\nfrom llama\\_index.vector\\_stores.types import (\n    VectorStore,\n    VectorStoreQuery,\n    VectorStoreQueryResult,\n)\nfrom typing import List, Any, Optional, Dict\nfrom llama\\_index.schema import TextNode, BaseNode\nimport os\n\nclass BaseVectorStore(VectorStore):\n    \"\"\"Simple custom Vector Store.\n\n    Stores documents in a simple in-memory dict.\n\n    \"\"\"\n\n    stores\\_text: bool \\= True\n\n    def get(self, text\\_id: str) \\-> List\\[float\\]:\n        \"\"\"Get embedding.\"\"\"\n        pass\n\n    def add(\n        self,\n        nodes: List\\[BaseNode\\],\n    ) \\-> List\\[str\\]:\n        \"\"\"Add nodes to index.\"\"\"\n        pass\n\n    def delete(self, ref\\_doc\\_id: str, \\*\\*delete\\_kwargs: Any) \\-> None:\n        \"\"\"\n        Delete nodes using with ref\\_doc\\_id.\n\n        Args:\n            ref\\_doc\\_id (str): The doc\\_id of the document to delete.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query: VectorStoreQuery,\n        \\*\\*kwargs: Any,\n    ) \\-> VectorStoreQueryResult:\n        \"\"\"Get nodes for response.\"\"\"\n        pass\n\n    def persist(self, persist\\_path, fs\\=None) \\-> None:\n        \"\"\"Persist the SimpleVectorStore to a directory.\n\n        NOTE: we are not implementing this for now.\n\n        \"\"\"\n        pass\n\nAt a high-level, we subclass our base `VectorStore` abstraction. There\u2019s no inherent reason to do this if you\u2019re just building a vector store from scratch. We do it because it makes it easy to plug into our downstream abstractions later.\n\nLet\u2019s look at some of the classes defined here.\n\n*   `BaseNode` is simply the parent class of our core Node modules. Each Node represents a text chunk + associated metadata.\n    \n*   We also use some lower-level constructs, for instance our `VectorStoreQuery` and `VectorStoreQueryResult`. These are just lightweight dataclass containers to represent queries and results. We look at the dataclass fields below.\n    \n\nfrom dataclasses import fields\n\n{f.name: f.type for f in fields(VectorStoreQuery)}\n\n{'query\\_embedding': typing.Optional\\[typing.List\\[float\\]\\],\n 'similarity\\_top\\_k': int,\n 'doc\\_ids': typing.Optional\\[typing.List\\[str\\]\\],\n 'node\\_ids': typing.Optional\\[typing.List\\[str\\]\\],\n 'query\\_str': typing.Optional\\[str\\],\n 'output\\_fields': typing.Optional\\[typing.List\\[str\\]\\],\n 'embedding\\_field': typing.Optional\\[str\\],\n 'mode': <enum 'VectorStoreQueryMode'>,\n 'alpha': typing.Optional\\[float\\],\n 'filters': typing.Optional\\[llama\\_index.vector\\_stores.types.MetadataFilters\\],\n 'mmr\\_threshold': typing.Optional\\[float\\],\n 'sparse\\_top\\_k': typing.Optional\\[int\\]}\n\n{f.name: f.type for f in fields(VectorStoreQueryResult)}\n\n{'nodes': typing.Optional\\[typing.Sequence\\[llama\\_index.schema.BaseNode\\]\\],\n 'similarities': typing.Optional\\[typing.List\\[float\\]\\],\n 'ids': typing.Optional\\[typing.List\\[str\\]\\]}\n\n### 2\\. Defining `add`, `get`, and `delete`[\uf0c1](#defining-add-get-and-delete \"Permalink to this heading\")\n\nWe add some basic capabilities to add, get, and delete from a vector store.\n\nThe implementation is very simple (everything is just stored in a python dictionary).\n\nclass VectorStore2(BaseVectorStore):\n    \"\"\"VectorStore2 (add/get/delete implemented).\"\"\"\n\n    stores\\_text: bool \\= True\n\n    def \\_\\_init\\_\\_(self) \\-> None:\n        \"\"\"Init params.\"\"\"\n        self.node\\_dict: Dict\\[str, BaseNode\\] \\= {}\n\n    def get(self, text\\_id: str) \\-> List\\[float\\]:\n        \"\"\"Get embedding.\"\"\"\n        return self.node\\_dict\\[text\\_id\\]\n\n    def add(\n        self,\n        nodes: List\\[BaseNode\\],\n    ) \\-> List\\[str\\]:\n        \"\"\"Add nodes to index.\"\"\"\n        for node in nodes:\n            self.node\\_dict\\[node.node\\_id\\] \\= node\n\n    def delete(self, node\\_id: str, \\*\\*delete\\_kwargs: Any) \\-> None:\n        \"\"\"\n        Delete nodes using with node\\_id.\n\n        Args:\n            node\\_id: str\n\n        \"\"\"\n        del self.node\\_dict\\[node\\_id\\]\n\nWe run some basic tests just to show it works well.\n\ntest\\_node \\= TextNode(id\\_\\=\"id1\", text\\=\"hello world\")\ntest\\_node2 \\= TextNode(id\\_\\=\"id2\", text\\=\"foo bar\")\ntest\\_nodes \\= \\[test\\_node, test\\_node2\\]\n\nvector\\_store \\= VectorStore2()\n\nvector\\_store.add(test\\_nodes)\n\nnode \\= vector\\_store.get(\"id1\")\nprint(str(node))\n\nNode ID: id1\nText: hello world\n\n### 3.a Defining `query` (semantic search)[\uf0c1](#a-defining-query-semantic-search \"Permalink to this heading\")\n\nWe implement a basic version of top-k semantic search. This simply iterates through all document embeddings, and compute cosine-similarity with the query embedding. The top-k documents by cosine similarity are returned.\n\nCosine similarity: $\\\\dfrac{\\\\vec{d}\\\\vec{q}}{|\\\\vec{d}||\\\\vec{q}|}$ for every document, query embedding pair $\\\\vec{d}$, $\\\\vec{p}$.\n\n**NOTE**: The top-k value is contained in the `VectorStoreQuery` container.\n\n**NOTE**: Similar to the above, we define another subclass just so we don\u2019t have to reimplement the above functions (not because this is actually good code practice).\n\nfrom typing import Tuple\nimport numpy as np\n\ndef get\\_top\\_k\\_embeddings(\n    query\\_embedding: List\\[float\\],\n    doc\\_embeddings: List\\[List\\[float\\]\\],\n    doc\\_ids: List\\[str\\],\n    similarity\\_top\\_k: int \\= 5,\n) \\-> Tuple\\[List\\[float\\], List\\]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    \\# dimensions: D\n    qembed\\_np \\= np.array(query\\_embedding)\n    \\# dimensions: N x D\n    dembed\\_np \\= np.array(doc\\_embeddings)\n    \\# dimensions: N\n    dproduct\\_arr \\= np.dot(dembed\\_np, qembed\\_np)\n    \\# dimensions: N\n    norm\\_arr \\= np.linalg.norm(qembed\\_np) \\* np.linalg.norm(\n        dembed\\_np, axis\\=1, keepdims\\=False\n    )\n    \\# dimensions: N\n    cos\\_sim\\_arr \\= dproduct\\_arr / norm\\_arr\n\n    \\# now we have the N cosine similarities for each document\n    \\# sort by top k cosine similarity, and return ids\n    tups \\= \\[(cos\\_sim\\_arr\\[i\\], doc\\_ids\\[i\\]) for i in range(len(doc\\_ids))\\]\n    sorted\\_tups \\= sorted(tups, key\\=lambda t: t\\[0\\], reverse\\=True)\n\n    sorted\\_tups \\= sorted\\_tups\\[:similarity\\_top\\_k\\]\n\n    result\\_similarities \\= \\[s for s, \\_ in sorted\\_tups\\]\n    result\\_ids \\= \\[n for \\_, n in sorted\\_tups\\]\n    return result\\_similarities, result\\_ids\n\nclass VectorStore3A(VectorStore2):\n    \"\"\"Implements semantic/dense search.\"\"\"\n\n    def query(\n        self,\n        query: VectorStoreQuery,\n        \\*\\*kwargs: Any,\n    ) \\-> VectorStoreQueryResult:\n        \"\"\"Get nodes for response.\"\"\"\n\n        query\\_embedding \\= cast(List\\[float\\], query.query\\_embedding)\n        doc\\_embeddings \\= \\[n.embedding for n in self.node\\_dict.values()\\]\n        doc\\_ids \\= \\[n.node\\_id for n in self.node\\_dict.values()\\]\n\n        similarities, node\\_ids \\= get\\_top\\_k\\_embeddings(\n            query\\_embedding,\n            embeddings,\n            doc\\_ids,\n            similarity\\_top\\_k\\=query.similarity\\_top\\_k,\n        )\n        result\\_nodes \\= \\[self.node\\_dict\\[node\\_id\\] for node\\_id in node\\_ids\\]\n\n        return VectorStoreQueryResult(\n            nodes\\=result\\_nodes, similarities\\=similarities, ids\\=node\\_ids\n        )\n\n### 4\\. Load Data into our Vector Store[\uf0c1](#load-data-into-our-vector-store \"Permalink to this heading\")\n\nLet\u2019s load our text chunks into the vector store, and run it on different types of queries: dense search, w/ metadata filters, and more.\n\nvector\\_store \\= VectorStore3B()\n\\# load data into the vector stores\nvector\\_store.add(nodes)\n\nDefine an example question and embed it.\n\nquery\\_str \\= \"Can you tell me about the key concepts for safety finetuning\"\nquery\\_embedding \\= embed\\_model.get\\_query\\_embedding(query\\_str)\n\n#### Query the vector store with dense search.[\uf0c1](#query-the-vector-store-with-dense-search \"Permalink to this heading\")\n\nquery\\_obj \\= VectorStoreQuery(\n    query\\_embedding\\=query\\_embedding, similarity\\_top\\_k\\=2\n)\n\nquery\\_result \\= vector\\_store.query(query\\_obj)\nfor similarity, node in zip(query\\_result.similarities, query\\_result.nodes):\n    print(\n        \"\\\\n\\----------------\\\\n\"\n        f\"\\[Node ID {node.node\\_id}\\] Similarity: {similarity}\\\\n\\\\n\"\n        f\"{node.get\\_content(metadata\\_mode\\='all')}\"\n        \"\\\\n\\----------------\\\\n\\\\n\"\n    )\n\n\\----------------\n\\[Node ID 3f74fdf4-0e2e-473e-9b07-10c51eb62794\\] Similarity: 0.835677131511819\n\ntotal\\_pages: 77\nfile\\_path: ./data/llama2.pdf\nsource: 23\n\nSpecifically, we use the following techniques in safety fine-tuning:\n1. Supervised Safety Fine-Tuning: We initialize by gathering adversarial prompts and safe demonstra-\ntions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\nthe model to align with our safety guidelines even before RLHF, and thus lays the foundation for\nhigh-quality human preference data annotation.\n2. Safety RLHF: Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\ntion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\nadversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n3. Safety Context Distillation: Finally, we refine our RLHF pipeline with context distillation (Askell\net al., 2021b).\n----------------\n\n\n\n----------------\n\\[Node ID 5ad5efb3-8442-4e8a-b35a-cc3a10551dc9\\] Similarity: 0.827877930608312\n\ntotal\\_pages: 77\nfile\\_path: ./data/llama2.pdf\nsource: 23\n\nBenchmarks give a summary view of model capabilities and behaviors that allow us to understand general\npatterns in the model, but they do not provide a fully comprehensive view of the impact the model may have\non people or real-world outcomes; that would require study of end-to-end product deployments. Further\ntesting and mitigation should be done to understand bias and other social issues for the specific context\nin which a system may be deployed. For this, it may be necessary to test beyond the groups available in\nthe BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\ncontinuing research that will amplify their potential for positive impact on these important social issues.\n4.2\nSafety Fine-Tuning\nIn this section, we describe our approach to safety fine-tuning, including safety categories, annotation\nguidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\nfine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n----------------\n\n## Build a RAG System with the Vector Store[\uf0c1](#build-a-rag-system-with-the-vector-store \"Permalink to this heading\")\n\nNow that we\u2019ve built the RAG system, it\u2019s time to plug it into our downstream system!\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex.from\\_vector\\_store(vector\\_store)\n\nquery\\_engine \\= index.as\\_query\\_engine()\n\nquery\\_str \\= \"Can you tell me about the key concepts for safety finetuning\"\n\nresponse \\= query\\_engine.query(query\\_str)\n\nThe key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to align the model with safety guidelines before RLHF. Safety RLHF integrates safety into the RLHF pipeline by training a safety-specific reward model and gathering more challenging adversarial prompts for fine-tuning and optimization. Finally, safety context distillation is used to refine the RLHF pipeline. These techniques aim to mitigate safety risks and ensure that the model aligns with safety guidelines.\n\n## Conclusion[\uf0c1](#conclusion \"Permalink to this heading\")\n\nThat\u2019s it! We\u2019ve built a simple in-memory vector store that supports very simple inserts, gets, deletes, and supports dense search and metadata filtering. This can then be plugged into the rest of LlamaIndex abstractions.\n\nIt doesn\u2019t support sparse search yet and is obviously not meant to be used in any sort of actual app. But this should expose some of what\u2019s going on under the hood!"
}