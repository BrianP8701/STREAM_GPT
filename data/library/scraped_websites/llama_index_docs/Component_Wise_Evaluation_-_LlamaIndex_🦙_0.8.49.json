{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/component_wise_evaluation.html",
        "title": "Component Wise Evaluation - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Component Wise Evaluation[\uf0c1](#component-wise-evaluation \"Permalink to this heading\")\n\nTo do more in-depth evaluation of your pipeline, it helps to break it down into an evaluation of individual components.\n\nFor instance, a particular failure case may be due to a combination of not retrieving the right documents and also the LLM misunderstanding the context and hallucinating an incorrect result. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\n\n## Utilizing public benchmarks[\uf0c1](#utilizing-public-benchmarks \"Permalink to this heading\")\n\nWhen doing initial model selection, it helps to look at how well the model is performing on a standardized, diverse set of domains or tasks.\n\nA useful benchmark for embeddings is the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n## Evaluating Retrieval[\uf0c1](#evaluating-retrieval \"Permalink to this heading\")\n\n### BEIR dataset[\uf0c1](#beir-dataset \"Permalink to this heading\")\n\nBEIR is useful for benchmarking if a particular retrieval model generalize well to niche domains in a zero-shot setting.\n\nSince most publically-available embedding and retrieval models are already benchmarked against BEIR (e.g. through the MTEB benchmark), utilizing BEIR is more helpful when you have a unique model that you want to evaluate.\n\nFor instance, after fine-tuning an embedding model on your dataset, it may be helpful to view whether and by how much its performance degrades on a diverse set of domains. This can be an indication of how much data drift may affect your retrieval accuracy, such as if you add documents to your RAG system outside of your fine-tuning training distribution.\n\nHere is a notebook showing how the BEIR dataset can be used with your retrieval pipeline.\n\n*   [BEIR Out of Domain Benchmark](https://docs.llamaindex.ai/en/stable/examples/evaluation/BeirEvaluation.html)\n\nWe will be adding more methods to evaluate retrieval soon. This includes evaluating retrieval on your own dataset.\n\n## Evaluating the Query Engine Components (e.g. Without Retrieval)[\uf0c1](#evaluating-the-query-engine-components-e-g-without-retrieval \"Permalink to this heading\")\n\nIn this case, we may want to evaluate how specific components of a query engine (one which may generate sub-questions or follow-up questions) may perform on a standard benchmark. It can help give an indication of how far behind or ahead your retrieval pipeline is compared to alternate pipelines or models.\n\n### HotpotQA Dataset[\uf0c1](#hotpotqa-dataset \"Permalink to this heading\")\n\nThe HotpotQA dataset is useful for evaluating queries that require multiple retrieval steps.\n\nExample:\n\n*   [HotpotQADistractor Demo](https://docs.llamaindex.ai/en/stable/examples/evaluation/HotpotQADistractor.html)\n\nLimitations:\n\n1.  HotpotQA is evaluated on a Wikipedia corpus. LLMs, especially GPT4, tend to have memorized information from Wikipedia relatively well. Hence, the benchmark is not particularly good for evaluating retrieval + rerank systems with knowledgeable models like GPT4."
}