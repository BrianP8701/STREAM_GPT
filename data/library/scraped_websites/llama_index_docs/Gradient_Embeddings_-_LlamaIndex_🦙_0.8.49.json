{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/examples/embeddings/gradient.html",
        "title": "Gradient Embeddings - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\n## Gradient Embeddings[\uf0c1](#gradient-embeddings \"Permalink to this heading\")\n\n[Gradient](https://gradient.ai/) offers embeddings model that can be easily integrated with LlamaIndex. Below is an example of how to use it with LlamaIndex.\n\n\\# Install the required packages\n\n%pip install llama-index --quiet\n%pip install gradientai --quiet\n\nGradient needs an access token and workspaces id for authorization. They can be obtained from:\n\n*   [Gradient UI](https://auth.gradient.ai/login), or\n    \n*   [Gradient CLI](https://docs.gradient.ai/docs/cli-quickstart) with `gradient env` command.\n    \n\nimport os\n\nos.environ\\[\"GRADIENT\\_ACCESS\\_TOKEN\"\\] \\= \"{GRADIENT\\_ACCESS\\_TOKEN}\"\nos.environ\\[\"GRADIENT\\_WORKSPACE\\_ID\"\\] \\= \"{GRADIENT\\_WORKSPACE\\_ID}\"\n\nfrom llama\\_index.llms import GradientBaseModelLLM\n\n\\# NOTE: we use a base model here, you can as well insert your fine-tuned model.\nllm \\= GradientBaseModelLLM(\n    base\\_model\\_slug\\=\"llama2-7b-chat\",\n    max\\_tokens\\=400,\n)\n\n## Load Documents[\uf0c1](#load-documents \"Permalink to this heading\")\n\nfrom llama\\_index import SimpleDirectoryReader\n\ndocuments \\= SimpleDirectoryReader(\"../data/paul\\_graham\").load\\_data()\nprint(f\"Loaded {len(documents)} document(s).\")\n\n## Configure Gradient embeddings[\uf0c1](#configure-gradient-embeddings \"Permalink to this heading\")\n\nfrom llama\\_index import ServiceContext\nfrom llama\\_index.embeddings import GradientEmbedding\n\nembed\\_model \\= GradientEmbedding(\n    gradient\\_access\\_token\\=os.environ\\[\"GRADIENT\\_ACCESS\\_TOKEN\"\\],\n    gradient\\_workspace\\_id\\=os.environ\\[\"GRADIENT\\_WORKSPACE\\_ID\"\\],\n    gradient\\_model\\_slug\\=\"bge-large\",\n)\n\nservice\\_context \\= ServiceContext.from\\_defaults(\n    chunk\\_size\\=1024, llm\\=llm, embed\\_model\\=embed\\_model\n)\n\n## Setup and Query Index[\uf0c1](#setup-and-query-index \"Permalink to this heading\")\n\nfrom llama\\_index import VectorStoreIndex\n\nindex \\= VectorStoreIndex.from\\_documents(\n    documents, service\\_context\\=service\\_context\n)\nquery\\_engine \\= index.as\\_query\\_engine()\n\nresponse \\= query\\_engine.query(\n    \"What did the author do after his time at Y Combinator?\"\n)\nprint(response)"
}