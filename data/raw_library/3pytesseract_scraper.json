["STREAM AI 1 1\n\nReal-Time, Multithreaded Anomaly Detection System with Autonomous\n\nSynchronization & Self-Correction\n\nBrian Przezdziecki\n\nUniversity of Buffalo (SUNY)\n", "STREAM AI 1 2\n\nAbstract\n\nReal-time anomaly detection is crucial during the printing process, especially when\nlooking for issues like over or under extrusion. We employ a MobileNet model to classify these\nanomalies. Our system has two main components: Tip Tracking and Anomaly Detection. In the\nTip Tracking phase, we estimate the locations of the tips using gcode data. Then, we use the\nYOLO model to align these predictions with live video footage, ensuring synchronization and\ncorrecting any prediction inaccuracies. Given the need for real-time performance on a Raspberry\nPi, our system is optimized for computational efficiency. It's designed to operate in real-time,\nrunning alongside video feeds, and strikes a balance between accuracy, autonomy, robustness and\n\nspeed.\n", "STREAM AI 1\n\nProblem\n\nWe need to spot errors in 3D printing as they happen. The main mistakes are tied to how\n\nmuch material comes out - too much or too little (Under/Over Extrusion).\n\n|\n\nD\n2\n|\n\nClasses:\n\nNormal\n\nnr\n\nOver Under\n\nAnother problem arises: Given a frame, the only relevant information is the area around the tip in\n\nthe direction of recently extruded material. This is why we need to track the tip.\n", "STREAM AI 1 4\n\nSolution\n\nInitialization\nThread f}\n\nThe given diagram displays how the system is divided into distinct threads and illustrates\n\nthe process flow. All threads access some shared global variables. The tracker thread initiates and\nmanages all other threads. This system primarily revolves around three key components:\n\n1. Initialization: Predicting the path of tip tracking and aligning with the video.\n\n2. Error Correction: Correcting errors through a feedback loop.\n\n3. Analytics: Identifying anomalies.\nFor the first two steps, we utilize a fine-tuned Yolov8. For the anomaly identification, we rely on\n\na fine-tuned MobileNetv3 model.\n\nWhy we need Tip Tracking & Examples of Video and Anomalies\n", "STREAM AI 1 5\n\nChallenges\n\nThis system is designed with simplicity and versatility in mind. Our goal is to make it\nreadily adoptable for anyone, ensuring it can seamlessly integrate with various 3D printers in\ndiverse settings. There's no need for a precise camera placement, giving users flexibility. The\n\nsystem adjusts to different resolutions, simplifying setup processes.\n\nIt's built to work with any printer that operates on g-code, even if the g-code\ninterpretations vary. With edge deployment capabilities, it can run on a Raspberry Pi, streaming\nin real time. Given that the YOLO model may occasionally err in object detection, our system is\nmade to account for these discrepancies. Moreover, users will benefit from a clear visualization\n\nof the tip tracking procedure.\n\nGiven these constraints and requirements, the ensuing challenges and our tailored\n\nsolutions will be elaborated below.\n", "STREAM AI 1 6\nModel Training\n\nhttps://github.com/BrianP8701/Anomaly_Classification\nObject Detection for Tip Tracking\n\nWe utilized the YOLOv8s model, a prominent object detection system, and fine-tuned it\n\nthrough Roboflow, a platform that streamlines the computer vision data process.\n\nWe added onto our dataset and retrained multiple times, aiming to fill in gaps and\nweaknesses of the model, adding null images and wrong detections etc. We applied\n\naugmentations of blur, noise and brightness.\n\ntrain/box_loss train/cls_loss train/dfl_loss metrics/precision(B) metrics/recall(B)\nLe = results 14\n20 09\n13 te\n14\n1s\n12 os 08\n12\n10\na\n07\nA o7\nos 10\nGio | 25 O19 2 0 10) 20 0 10 20 0 10) 20\nval/box_loss valicls_loss valiafl_loss metrics/mAP5O(B) metrics/mAPS0-95(B)\n18 0.8\n22 wv\n16\nAG 0.40\n2a\n14 16 oe\n20 12 08\nAc 0.30\n10\na o7 0.25\nos 14\n18 0.20\n", "STREAM AI 1 7\n\nObject Classification for Anomalies\nWe tested various models, settings, and preprocessing methods to find the best model.\nOur data is limited because only a few frames in each video show over or under extrusion,\n\nmaking data collection slow. Here's our data pipeline:\n\nExamining the pipeline: I developed several GUIs to address the initial data collection\nbottleneck. I opted for an 85x85 crop size as it best captured material near the extrusion tip. I\nused grayscale to reduce dimensions. In some images, edges and material are barely visible.\n\nTherefore, a filter was used in preprocessing to enhance these crucial edges.\n\nl initially attempted a filter that segmented the image into\ndistinct color intensities, using the original image's color intensity\nstandard deviation. While somewhat effective, it missed the\n\nsubtlest edges.\n\nmy i\n\n", "STREAM AI 1 8\n\nsian Blur, Sharpen3, Sharpen?\n\nSharpen2\n\nDrawing inspiration from the convolutional operations in CNNs, I\n\nexperimented with various kernels and matrix filter combinations on the\n\nimages. However, no single combination consistently worked across all\n\nimages. | tested standard filters such as the Non-local Means Denoising\n\naes enhancing the faint edges.\n\n(7 from cv2 and the Unsharp Mask from PIL. Yet, neither succeeded in\n| 1\n\nAn observation was made, each image features a limited set of objects and hues: material,\ntip, bed, shadows, and gleam. Each color intensity in the image uniquely follows a normal\ndistribution with its distinct mean and variance. Hence, Gaussian Mixture Models are apt for this\n\ntask.\n\nThe x axis is pixel intensity, and the y axis is the count of how many pixels have each intensity\n\nne eS ls\n\nIogoWeighs)\n\n", "STREAM AI 1 9\n\nGMMx: Gaussian Mixture Model\nAlgorithm with x components\n\nOriginal\n\nUsing Gaussian Mixture Models, we can sharply define even the subtlest edges. I tested\nvarious component numbers, indicating the assumed Gaussians in the mixture. Next, we\nprioritize sensitivity around brighter colors since it's challenging to discern edges when material\naccumulates. This focus ignores the darker bed, which isn't of concern. This final refinement\n\nresulted in our preprocessing step, dubbed GMMS.\n\nGMMSx: Gaussian Mixture Model +\nSensitivity with x components\n\nOriginal\n\nThrough experimentation, GMMS6 is found to perform the best when training models.\n", "STREAM AI 1 10\n\nBack to examining the pipeline, the\nimages are resized to 224x224, suitable\nfor EfficientNet, MobileNet, and\n\nResNet, which have trained on the\n\n224x224-sized ImageNet dataset,\n\nmaking them adept at recognizing\nfeatures of this scale. Post-resizing, the\nimages undergo augmentation and are split into training, validation, and test datasets.\n\nDataset Summary:\n\nUnder: 436 images | Normal: 463 images | Over: 446 images\n\nExtrusion Classification Dataset\nAugmentations for each class include rotations, brightness, saturation, contrast, hue\nadjustments, and flips (both horizontal and vertical) creating 60 new images per class. The data\n\ndistribution is 75% for training, 15% for validation, and 10% for testing.\n\nTraining Summary:\n\nFine tuning, model checkpointing, 4 batches, ~12 epochs, learning rate decay, early stopping.\n\nlearning rate=0.01, momentum=0.9, step_size=5, gamma=0.1\n\nAfter trials of various models, settings, hyperparameters and preprocessing steps, the\ncombination of the described preprocessing and training procedures outlined above yielded the\nhighest accuracy. The standout was a fine tuned MobileNetv3Large model, which not only won\n\nin accuracy but also had superior computational efficiency.\n", "0.925\n\n0.900\n\nScore\n\n0.875\n\n0.850\n\n0.825\n\nscore\n\n\\\n\nscore\n\n\\\n\nscore\n\n\\\n\nscore\n\n\\\n\nScore\n\n\\\n\nscore\n\n\\\n\nas\n\na6\n\nog\n\n06\n\nas\n\na6\n\nas\n\na6\n\nog\n\na6\n\nas\n\na6\n\nSTREAM AI\n\n1\n\naccuracy\n$ @ \u2014mob_s_gmms finetune\ne \u00a9 mob_s original_finetune\n\u00b0 @ \u2014mob_s_gmms?_finetune\n@ \u2014 mob_s_original2_finetune\ne @ \u2014mob_l_gmms_finetune\ne \u00a9 mob_| original_finetune\n. e Lgmms2_finetune\n@ -mob_{ original2_finetune ~\n-0.04 =0.02 0.00 0.02 og\nIteration\nval_accuracy\n8 @ \u2014mob_s_gmms_finetune\n@ \u2014mob_s_original_finetune\n@ mob_s_gmms2_finetune\n8 @ \u2014mob_s_original2_finetune\n@ \u2014mob_L_gmms finetune\n\u00a9 mob_{original_finetune\n$ @ \u2014mob_l_gmms2_finetune\n@ mob_{ original2_finetune ~\n-0.04 -0.02 0.00 0.02 og\neration\n\nmiob_5.gnims fin\u00e9tune\n\nmob_s_original_finetune\nmob_s_gmmsz_finetune\nmob_s_original2_finetune\n\nmob_|_gmms_finetune\n\nmob_loriginal_finetune\n\nmob_l_gmms?_finetune\n\nmob_[original2_finetune\n\n25 50\n\n7s 10.0\nlteration\n\nps 15.0 ws\n\nmob_$_gmms_finetune\n\n\u2018mob_s_original_finetune\nmob_s_gmms2_finetune\nmob_s_original2_finetune\nmob_L_amms_finetune\nmob_l_original_finetune\nmob_|_gmms?_finetune\nmob_[ original2_finetune\n\n25 5.0\n\n1S 10.0\neration\n\nRs 15.0\n\nmob_\u00a7_gmms finetune\n\n\u2018mob_s_original_finetune\nmob_s_gmms2_finetune\n\u2018mob_s_original2_finetune\nmob_|_gmms_finetune\nmob_[original_finetune\nmob_|_gmms?_finetune\nmob_l_original2_finetune\n\n|\n\n25 5.0\n\n15 10.0\neration\n\n25 15.0\n\nmob_s_gmms finetune\n\n\u2018mob_s_original_finetune\nmob_s_gmms?_finetune\nmob_s_original2_finetune\nmob_|_gmms_finetune\nmob_Loriginal_finetune\nmob_|_gmms?_finetune\nmob_l original2_finetune\n\n25 5.0\n\n1S 10.0\neration\n\nws\n\nmob_s_gmms finetune\n\n\u2018mob_s_original_finetune\nmob_s_gmms?_finetune\nmob_s_original2_finetune\nmob_l_gmms_finetune\nmob_[ original_finetune\nmob_|_gmms?_finetune\nmob_[original2_finetune\n\n25 5.0\n\n1S 10.0\neration\n\nR25 15.0 75\n\nmob_s.gmms finetune\n\n\u2018mob_s_original_finetune\nmob_s_gmms2_finetune\nmob_s_original?_finetune\nmob_|_gmms_finetune\nmob _| original_finetune\nmob_l_gmms2_finetune\nmob_loriginal2_finetune\n\n|\n\n25 5.0\n\n15 10.0\neration\n\nws\n\n11\n\nTo the left shows one of the\ncomparison rounds, where we identified\nour best performing model. Here we\ncompare 3 different settings and their\ncombinations: Fine tuning\nMobileNetv3Small vs MobileNetv3Large.\nUsing no GMM preprocessing vs using\nGMM preprocessing vs using GMMS\npreprocessing with additional sensitivity\nto more intense pixels.\n\nThe results are fitting, showing that\nindeed using GMMS with extra\nsensitivity is beneficial, and training the\nlarger model results in better\nperformance. The training and validation\nmetrics are nearly indistinguishable\nindicating that it is not overfitting.\n\nBest performing model accuracy:\n\n0.9025974025974026\n", "STREAM AI 1 12\n\nImplementation\n\nIn this section I will describe the design choices and details of the system.\n\nhttps://github.com/BrianP8701/STREAM.AI\n\nInput\n1. G-Code:\n\n- Instruction set for 3D print.\n\n- Simple and sequential: it runs from start to finish.\n\n- Specifies the top speed at any moment and provides movement coordinates.\n- The printer moves directly from one point to the next without deviation.\n\n2. Video Stream:\n\n- Live footage of the 3D printer bed.\n- The angle of the camera relative to the printer is expected to be roughly consistent.\n\n3. Signal Stream:\n\n- Managed by the SKR board, which controls the 3D printer.\n\n- A hardware connection links the SKR board to our Raspberry Pi.\n\n- The Raspberry Pi gets a signal every time a movement completes. This is the signal, and\nconsists of the bed position and time. [time, x, y, z]\n\n- This data streams in real time, parallel to the printing process.\n\nInitialization\n\nFirst and foremost, the system, starting in the main Tracker Thread, begins receiving and\nrouting frames and signals through their corresponding routers. These routers will pass the data\nto the correct objects and threads through the shared global variable space based on the current\n\nstate of the system (Initializing or Tracking).\n", "STREAM AI 1 13\n\nThe duty of the initialization phase is to autonomously adapt to the camera position. It\nprimarily performs these tasks:\n1. Make initial predictions:\n\na. To transform G-code into predictions for every\nmoment in time, we first break down the G-code\ninto a sequence of motion instructions between\ncorners (or waypoints) where there are changes in\ndirection or speed.\n\nb. For each of these motions, we calculate the optimal\nspeed profile considering constraints like maximum\nspeeds and acceleration. We need to look at the future 2 moves as well, to\ndetermine what the final speed at the end of the motion should be.\n\nc. Given the optimal speed profiles for each motion, we\u2019ll calculate how long each\nmove should take using kinematics. One problem cannot be solved analytically,\n\nE4.77753\nE9.55561\nE13.29648\n\nE45.79648\nE50.57431\nE55.35281\nE60.13107\n\nand needs to be solved iteratively. The iterative method tweaks the distance over\n\nwhich we accelerate or decelerate, attempting to converge on the desired final\nspeed without exceeding the machine's constraints.\n\nd. After computing the speed profile, the script then divides the motion into frames\n\n(based on the given frames-per-second parameter) and predicts the position and\nangle of the machine's tip for each frame. By the end of this process, we have a\ndetailed account of where the machine should be and at what angle for every\nsingle frame of the motion.\n\n2. Derive millimeter to pixel ratio:\n\nFor robustness, the system will autonomously map bed predictions to screen\n\npredictions without requiring users to enter any configuring variables. By waiting\n\nfor two signals and observing their millimeter disparity on the bed, we can\ncorrelate this with the pixel difference YOLO detects, facilitating the necessary\nconversion. We slide a 640x640 window across the image to run YOLO.\n\n3. Synchronize predictions with video:\n\nTo initiate the process, we align our screen predictions using the timestamp of the\n\nfirst YOLO-signal inference from the video.\n", "STREAM AI 1 14\n\nPredictions are imperfect due to accumulated small tracking errors. Reasons include:\n\n1. Merlin firmware in our 3D printer employs Beziet\u2019s\n: . . Max jerk at corners\n6th order function for movement extrapolation, while Trapezoidal profile of trapezoid\n\nour algorithm uses a simpler 2nd order function.\n\nHowever, our model considers acceleration.\n\nVelocity\n\n2. Some printers adjust movement factors, like speed\n\nand acceleration.\n\n3. Minor deviations arise when the printer halts. S-curve profile\n\n4. Merlin Firmware has intricate mechanisms for gcode\n\nTime\n\ninterpretation.\n\nInstead of replicating Merlin's exact approach, we designed a straightforward interpreter with\nreal-time error feedback loops, enhancing system robustness and compatibility with various 3D\nprinters.\n\nError Correction\nPredictions face two error types: Temporal and Spatial, addressed by the ErrorCorrection thread.\n1. Spatial Error: Mainly arises from incorrect ratios or YOLO inference. These are rare, typically\nminor initial tracking deviations. For demonstration, an exaggerated error was intentionally\n\nintroduced in the Spatial Error Correction video.\n\nMeasure Detect Correct\n\nEverytime a signal arrives, we run | When there is a consistent spatial error | Shift all\nYOLO to see where the tip is. We | of the same magnitude and direction for | predictions by\ncompare this to our prediction and _ | sufficient time. spatial error.\nmeasure the difference in pixels.\n\n2. Temporal Error: This was a significant challenge. Without input signals, gauging temporal\n\nerror was tough. The ultimate solution simply involved immediate measurement and correction\n", "STREAM AI 1 15\n\nof temporal discrepancies. This error stems from varying interpretations of gcode, especially\n\ndiffering accelerations.\n\nEverytime a signal arrives, we We don\u2019t look for any consistent Pause or skip\n\ncompare the time of the signal, errors. When there is an error, we ahead to realign.\ncompared to our predictions. We simply perform a correction.\nmeasure error in seconds.\n\nAnalytics\nIncorporating a real-time feedback loop (autonomous corrections) results in near-flawless tip\ntracking. Using YOLO isn't constant; it's employed during signals and error measurements. This\nenables efficient pinpointing of the recently extruded material around the tip\u2014our target. Given\nour knowledge from the gcode about the printer's movement direction, we crop accordingly. This\ncropped image then enters our data preprocessing pipeline and undergoes classification by the\n\ntailored MobileNet model, determining whether extrusion is under, normal, or over.\n", "STREAM AI 1 16\n\nMetrics\n\nTo evaluate our system, we'll focus on its computational efficiency, while the accuracy is\nderived from the MobileNet model's test performance. We'll assess:\n\n- RAM Usage: We'll monitor RAM consumption over time.\n\n- Inference Intervals: Given the adaptive nature of our MobileNet model, observing the\nintervals between successive inferences provides a measure of system speed. The model\nrefrains from inferring with a full buffer.\n\nThese metrics will be gauged on both my device and a Raspberry Pi 4, with respective\nspecifications provided below.\n\n1e9 System RAM Usage Time Between Inferences\n\nRam Usage (bytes)\nTime Between Inferences (frames)\n\nrabies a\n\n200 250 300 350 400 450 1000 \u00a92000 \u00bb=\u00ab 3000S \u00ab4000.\u00bb 5000\u00bb 6000\u00bb 7000\nMilliseconds +1.69402e9 Frame Index\n\nDeployment to Raspberry Pi\nRaspberry Pi Metrics\n\nFuture Improvements\n", "STREAM AI 1\n\n1. Processor (Chipset) Details:\n- Model: Apple M1 Pro\n- Total Number of Cores: 14\n\n2. Memory:\n- Size: 16 GB\n- Type: LPDDRS\n- Manufacturer: Hynix\n\n3. Graphics/GPU:\n- Chipset Model: Apple M1 Pro\n- Metal Support: Metal 3\n\n4. Display:\n- Type: Built-in Liquid Retina XDR Display\n- Resolution: 3024 x 1964 Retina\n\n5. Storage:\n- Type: SSD (APPLE SSD AP0512R)\n- Capacity: 500.28 GB\n- Protocol: Apple Fabric (indicating it's an NVMe SSD)\n- Free Space: 120.4 GB\n\n6. File System:\n- Type: APFS\n\nhttps://gengineering.eu/install-opencv-on-raspberry-pi.html\nhttps://gengineering.eu/install-pytorch-on-raspberry-pi-4.html\nhttps://gist.github.com/wenig/8bab88dede5c838660dd05b8e5b2\u00a2e23b\n\nhttps://onnxruntime.ai/docs/tutorials/iot-edge/rasp-pi-cv.html\n\n17\n"]