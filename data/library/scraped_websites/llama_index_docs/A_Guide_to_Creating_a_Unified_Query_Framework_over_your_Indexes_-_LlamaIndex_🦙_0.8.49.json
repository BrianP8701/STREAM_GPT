{
    "metadata": {
        "type": "web",
        "url": "https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/question_and_answer/unified_query.html",
        "title": "A Guide to Creating a Unified Query Framework over your Indexes - LlamaIndex \ud83e\udd99 0.8.49",
        "description": null
    },
    "text": "[Back to top](#)\n\nToggle table of contents sidebar\n\nLlamaIndex offers a variety of different [use cases](https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/use_cases.html).\n\nFor simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `SummaryIndex` for summarization.\n\nFor more complex queries, we may want to use a composable graph.\n\nBut how do we integrate indexes and graphs into our LLM application? Different indexes and graphs may be better suited for different types of queries that you may want to run.\n\nIn this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.\n\n## Setup[\uf0c1](#setup \"Permalink to this heading\")\n\nIn this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.\n\nThe below code snippet downloads the relevant data into files.\n\nfrom pathlib import Path\nimport requests\n\nwiki\\_titles \\= \\[\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"\\]\n\nfor title in wiki\\_titles:\n    response \\= requests.get(\n        'https://en.wikipedia.org/w/api.php',\n        params\\={\n            'action': 'query',\n            'format': 'json',\n            'titles': title,\n            'prop': 'extracts',\n            \\# 'exintro': True,\n            'explaintext': True,\n        }\n    ).json()\n    page \\= next(iter(response\\['query'\\]\\['pages'\\].values()))\n    wiki\\_text \\= page\\['extract'\\]\n\n    data\\_path \\= Path('data')\n    if not data\\_path.exists():\n        Path.mkdir(data\\_path)\n\n    with open(data\\_path / f\"{title}.txt\", 'w') as fp:\n        fp.write(wiki\\_text)\n\nThe next snippet loads all files into Document objects.\n\n\\# Load all wiki documents\ncity\\_docs \\= {}\nfor wiki\\_title in wiki\\_titles:\n    city\\_docs\\[wiki\\_title\\] \\= SimpleDirectoryReader(input\\_files\\=\\[f\"data/{wiki\\_title}.txt\"\\]).load\\_data()\n\n## Defining the Set of Indexes[\uf0c1](#defining-the-set-of-indexes \"Permalink to this heading\")\n\nWe will now define a set of indexes and graphs over our data. You can think of each index/graph as a lightweight structure that solves a distinct use case.\n\nWe will first define a vector index over the documents of each city.\n\nfrom llama\\_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom llama\\_index.llms import OpenAI\n\n\\# set service context\nllm\\_gpt4 \\= OpenAI(temperature\\=0, model\\=\"gpt-4\")\nservice\\_context \\= ServiceContext.from\\_defaults(\n    llm\\=llm\\_gpt4, chunk\\_size\\=1024\n)\n\n\\# Build city document index\nvector\\_indices \\= {}\nfor wiki\\_title in wiki\\_titles:\n    storage\\_context \\= StorageContext.from\\_defaults()\n    \\# build vector index\n    vector\\_indices\\[wiki\\_title\\] \\= VectorStoreIndex.from\\_documents(\n        city\\_docs\\[wiki\\_title\\],\n        service\\_context\\=service\\_context,\n        storage\\_context\\=storage\\_context,\n    )\n    \\# set id for vector index\n    vector\\_indices\\[wiki\\_title\\].index\\_struct.index\\_id \\= wiki\\_title\n    \\# persist to disk\n    storage\\_context.persist(persist\\_dir\\=f'./storage/{wiki\\_title}')\n\nQuerying a vector index lets us easily perform semantic search over a given city\u2019s documents.\n\nresponse \\= vector\\_indices\\[\"Toronto\"\\].as\\_query\\_engine().query(\"What are the sports teams in Toronto?\")\nprint(str(response))\n\nExample response:\n\nThe sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL).\n\n## Defining a Graph for Compare/Contrast Queries[\uf0c1](#defining-a-graph-for-compare-contrast-queries \"Permalink to this heading\")\n\nWe will now define a composed graph in order to run **compare/contrast** queries (see use cases doc). This graph contains a keyword table composed on top of existing vector indexes.\n\nTo do this, we first want to set the \u201csummary text\u201d for each vector index.\n\nindex\\_summaries \\= {}\nfor wiki\\_title in wiki\\_titles:\n    \\# set summary text for city\n    index\\_summaries\\[wiki\\_title\\] \\= (\n        f\"This content contains Wikipedia articles about {wiki\\_title}. \"\n        f\"Use this index if you need to lookup specific facts about {wiki\\_title}.\\\\n\"\n        \"Do not use this index if you want to analyze multiple cities.\"\n    )\n\nNext, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.\n\nfrom llama\\_index.indices.composability import ComposableGraph\n\ngraph \\= ComposableGraph.from\\_indices(\n    SimpleKeywordTableIndex,\n    \\[index for \\_, index in vector\\_indices.items()\\],\n    \\[summary for \\_, summary in index\\_summaries.items()\\],\n    max\\_keywords\\_per\\_chunk\\=50\n)\n\n\\# get root index\nroot\\_index \\= graph.get\\_index(graph.index\\_struct.root\\_id, SimpleKeywordTableIndex)\n\\# set id of root index\nroot\\_index.set\\_index\\_id(\"compare\\_contrast\")\nroot\\_summary \\= (\n    \"This index contains Wikipedia articles about multiple cities. \"\n    \"Use this index if you want to compare multiple cities. \"\n)\n\nQuerying this graph (with a query transform module), allows us to easily compare/contrast between different cities. An example is shown below.\n\n\\# define decompose\\_transform\nfrom llama\\_index import LLMPredictor\nfrom llama\\_index.indices.query.query\\_transform.base import DecomposeQueryTransform\n\ndecompose\\_transform \\= DecomposeQueryTransform(\n    LLMPredictor(llm\\=llm\\_gpt4), verbose\\=True\n)\n\n\\# define custom query engines\nfrom llama\\_index.query\\_engine.transform\\_query\\_engine import TransformQueryEngine\ncustom\\_query\\_engines \\= {}\nfor index in vector\\_indices.values():\n    query\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context)\n    query\\_engine \\= TransformQueryEngine(\n        query\\_engine,\n        query\\_transform\\=decompose\\_transform,\n        transform\\_extra\\_info\\={'index\\_summary': index.index\\_struct.summary},\n    )\n    custom\\_query\\_engines\\[index.index\\_id\\] \\= query\\_engine\ncustom\\_query\\_engines\\[graph.root\\_id\\] \\= graph.root\\_index.as\\_query\\_engine(\n    retriever\\_mode\\='simple',\n    response\\_mode\\='tree\\_summarize',\n    service\\_context\\=service\\_context,\n)\n\n\\# define query engine\nquery\\_engine \\= graph.as\\_query\\_engine(custom\\_query\\_engines\\=custom\\_query\\_engines)\n\n\\# query the graph\nquery\\_str \\= (\n    \"Compare and contrast the arts and culture of Houston and Boston. \"\n)\nresponse\\_chatgpt \\= query\\_engine.query(query\\_str)\n\n## Defining the Unified Query Interface[\uf0c1](#defining-the-unified-query-interface \"Permalink to this heading\")\n\nNow that we\u2019ve defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface to our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph will be used for the job.\n\nThere are a few ways to do this, both within our framework as well as outside of it!\n\n*   Build a **router query engine** on top of your existing indexes/graphs\n    \n*   Define each index/graph as a Tool within an agent framework (e.g. LangChain).\n    \n\nFor the purposes of this tutorial, we follow the former approach. If you want to take a look at how the latter approach works, take a look at our example tutorial here.\n\nLet\u2019s take a look at an example of building a router query engine to automatically \u201croute\u201d any query to the set of indexes/graphs that you have define under the hood.\n\nFirst, we define the query engines for the set of indexes/graph that we want to route our query to. We also give each a description (about what data it holds and what it\u2019s useful for) to help the router choose between them depending on the specific query.\n\nfrom llama\\_index.tools.query\\_engine import QueryEngineTool\n\nquery\\_engine\\_tools \\= \\[\\]\n\n\\# add vector index tools\nfor wiki\\_title in wiki\\_titles:\n    index \\= vector\\_indices\\[wiki\\_title\\]\n    summary \\= index\\_summaries\\[wiki\\_title\\]\n\n    query\\_engine \\= index.as\\_query\\_engine(service\\_context\\=service\\_context)\n    vector\\_tool \\= QueryEngineTool.from\\_defaults(query\\_engine, description\\=summary)\n    query\\_engine\\_tools.append(vector\\_tool)\n\n\\# add graph tool\ngraph\\_description \\= (\n    \"This tool contains Wikipedia articles about multiple cities. \"\n    \"Use this tool if you want to compare multiple cities. \"\n)\ngraph\\_tool \\= QueryEngineTool.from\\_defaults(graph\\_query\\_engine, description\\=graph\\_description)\nquery\\_engine\\_tools.append(graph\\_tool)\n\nNow, we can define the routing logic and overall router query engine. Here, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.\n\nfrom llama\\_index.query\\_engine.router\\_query\\_engine import RouterQueryEngine\nfrom llama\\_index.selectors.llm\\_selectors import LLMSingleSelector\n\nrouter\\_query\\_engine \\= RouterQueryEngine(\n    selector\\=LLMSingleSelector.from\\_defaults(service\\_context\\=service\\_context),\n    query\\_engine\\_tools\\=query\\_engine\\_tools\n)\n\n## Querying our Unified Interface[\uf0c1](#querying-our-unified-interface \"Permalink to this heading\")\n\nThe advantage of a unified query interface is that it can now handle different types of queries.\n\nIt can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.\n\nLet\u2019s take a look at a few examples!\n\n**Asking a Compare/Contrast Question**\n\n\\# ask a compare/contrast question\nresponse \\= router\\_query\\_engine.query(\n    \"Compare and contrast the arts and culture of Houston and Boston.\",\n)\nprint(str(response)\n\n**Asking Questions about specific Cities**\n\nresponse \\= router\\_query\\_engine.query(\"What are the sports teams in Toronto?\")\nprint(str(response))\n\nThis \u201couter\u201d abstraction is able to handle different queries by routing to the right underlying abstractions."
}